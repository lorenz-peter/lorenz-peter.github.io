[
    {
        "date": "2025-11",
        "title": "Adversarially Robust Multitask Adaptive Control",
        "author": "Kasra Fallah, Leonardo F. Toso, and James Anderson",
        "link": "http://arxiv.org/abs/2511.05444v1",
        "abstract": "We study adversarially robust multitask adaptive linear quadratic control; a\nsetting where multiple systems collaboratively learn control policies under\nmodel uncertainty and adversarial corruption. We propose a clustered multitask\napproach that integrates clustering and system identification with resilient\naggregation to mitigate corrupted model updates. Our analysis characterizes how\nclustering accuracy, intra-cluster heterogeneity, and adversarial behavior\naffect the expected regret of certainty-equivalent (CE) control across LQR\ntasks. We establish non-asymptotic bounds demonstrating that the regret\ndecreases inversely with the number of honest systems per cluster and that this\nreduction is preserved under a bounded fraction of adversarial systems within\neach cluster."
    },
    {
        "date": "2025-11",
        "title": "Robust Neural Audio Fingerprinting using Music Foundation Models",
        "author": "Shubhr Singh, Kiran Bhat, Xavier Riley, Benjamin Resnick, John Thickstun, and Walter De Brouwer",
        "link": "http://arxiv.org/abs/2511.05399v1",
        "abstract": "The proliferation of distorted, compressed, and manipulated music on modern\nmedia platforms like TikTok motivates the development of more robust audio\nfingerprinting techniques to identify the sources of musical recordings. In\nthis paper, we develop and evaluate new neural audio fingerprinting techniques\nwith the aim of improving their robustness. We make two contributions to neural\nfingerprinting methodology: (1) we use a pretrained music foundation model as\nthe backbone of the neural architecture and (2) we expand the use of data\naugmentation to train fingerprinting models under a wide variety of audio\nmanipulations, including time streching, pitch modulation, compression, and\nfiltering. We systematically evaluate our methods in comparison to two\nstate-of-the-art neural fingerprinting models: NAFP and GraFPrint. Results show\nthat fingerprints extracted with music foundation models (e.g., MuQ, MERT)\nconsistently outperform models trained from scratch or pretrained on\nnon-musical audio. Segment-level evaluation further reveals their capability to\naccurately localize fingerprint matches, an important practical feature for\ncatalog management."
    },
    {
        "date": "2025-11",
        "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
        "author": "Yiting He, Zhishuai Liu, Weixin Wang, and Pan Xu",
        "link": "http://arxiv.org/abs/2511.05396v1",
        "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment\ntransition dynamics are different, can be formulated as learning in a robust\nMarkov decision process (RMDP) where uncertainties in transition dynamics are\nimposed. Existing literature mostly assumes access to generative models\nallowing arbitrary state-action queries or pre-collected datasets with a good\nstate coverage of the deployment environment, bypassing the challenge of\nexploration. In this work, we study a more realistic and challenging setting\nwhere the agent is limited to online interaction with the training environment.\nTo capture the intrinsic difficulty of exploration in online RMDPs, we\nintroduce the supremal visitation ratio, a novel quantity that measures the\nmismatch between the training dynamics and the deployment dynamics. We show\nthat if this ratio is unbounded, online learning becomes exponentially hard. We\npropose the first computationally efficient algorithm that achieves sublinear\nregret in online RMDPs with $f$-divergence based transition uncertainties. We\nalso establish matching regret lower bounds, demonstrating that our algorithm\nachieves optimal dependence on both the supremal visitation ratio and the\nnumber of interaction episodes. Finally, we validate our theoretical results\nthrough comprehensive numerical experiments."
    },
    {
        "date": "2025-11",
        "title": "Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval",
        "author": "Janet Jenq, and Hongda Shen",
        "link": "http://arxiv.org/abs/2511.05325v1",
        "abstract": "Multimodal product retrieval systems in e-commerce platforms rely on\neffectively combining visual and textual signals to improve search relevance\nand user experience. However, vision-language models such as CLIP are\nvulnerable to typographic attacks, where misleading or irrelevant text embedded\nin images skews model predictions. In this work, we propose a novel method that\nreverses the logic of typographic attacks by rendering relevant textual content\n(e.g., titles, descriptions) directly onto product images to perform\nvision-text compression, thereby strengthening image-text alignment and\nboosting multimodal product retrieval performance. We evaluate our method on\nthree vertical-specific e-commerce datasets (sneakers, handbags, and trading\ncards) using six state-of-the-art vision foundation models. Our experiments\ndemonstrate consistent improvements in unimodal and multimodal retrieval\naccuracy across categories and model families. Our findings suggest that\nvisually rendering product metadata is a simple yet effective enhancement for\nzero-shot multimodal retrieval in e-commerce applications."
    },
    {
        "date": "2025-11",
        "title": "Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting",
        "author": "Marius Fracarolli, Michael Staniek, and Stefan Riezler",
        "link": "http://arxiv.org/abs/2511.05289v1",
        "abstract": "Balancing strong privacy guarantees with high predictive performance is\ncritical for time series forecasting (TSF) tasks involving Electronic Health\nRecords (EHR). In this study, we explore how data augmentation can mitigate\nMembership Inference Attacks (MIA) on TSF models. We show that retraining with\nsynthetic data can substantially reduce the effectiveness of loss-based MIAs by\nreducing the attacker's true-positive to false-positive ratio. The key\nchallenge is generating synthetic samples that closely resemble the original\ntraining data to confuse the attacker, while also introducing enough novelty to\nenhance the model's ability to generalize to unseen data. We examine multiple\naugmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO\nconstrained by Principal Component Analysis (ZOO-PCA), and MixUp - to\nstrengthen model resilience without sacrificing accuracy. Our experimental\nresults show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA\nattacks without sacrificing performance on test data."
    },
    {
        "date": "2025-11",
        "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
        "author": "Ishan Kavathekar, Hemang Jain, Ameya Rathod, Ponnurangam Kumaraguru, and Tanuja Ganu",
        "link": "http://arxiv.org/abs/2511.05269v1",
        "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as\nautonomous agents through tool use, planning, and decision-making abilities,\nleading to their widespread adoption across diverse tasks. As task complexity\ngrows, multi-agent LLM systems are increasingly used to solve problems\ncollaboratively. However, safety and security of these systems remains largely\nunder-explored. Existing benchmarks and datasets predominantly focus on\nsingle-agent settings, failing to capture the unique vulnerabilities of\nmulti-agent dynamics and co-ordination. To address this gap, we introduce\n$\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent\n$\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the\nrobustness and safety of multi-agent LLM systems. TAMAS includes five distinct\nscenarios comprising 300 adversarial instances across six attack types and 211\ntools, along with 100 harmless tasks. We assess system performance across ten\nbackbone LLMs and three agent interaction configurations from Autogen and\nCrewAI frameworks, highlighting critical challenges and failure modes in\ncurrent multi-agent deployments. Furthermore, we introduce Effective Robustness\nScore (ERS) to assess the tradeoff between safety and task effectiveness of\nthese frameworks. Our findings show that multi-agent systems are highly\nvulnerable to adversarial attacks, underscoring the urgent need for stronger\ndefenses. TAMAS provides a foundation for systematically studying and improving\nthe safety of multi-agent LLM systems."
    },
    {
        "date": "2025-11",
        "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks",
        "author": "Azhar Hussain Mozumder, M. John Basha, and Chayapathi A. R",
        "link": "http://arxiv.org/abs/2511.05156v1",
        "abstract": "With more and more existing networks being transformed to Software-Defined\nNetworking (SDN), they need to be more secure and demand smarter ways of\ntraffic control. This work, SmartSecChain-SDN, is a platform that combines\nmachine learning based intrusion detection, blockchain-based storage of logs,\nand application-awareness-based priority in SDN networks. To detect network\nintrusions in a real-time, precision and low-false positives setup, the\nframework utilizes the application of advanced machine learning algorithms,\nnamely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is\nbased on the Hyperledger Fabric, which is a permissioned blockchain technology,\nto provide secure, scalable, and privacy-preserving storage and, thus,\nguarantee that the Intrusion Detection System (IDS) records cannot be altered\nand can be analyzed comprehensively. The system also has Quality of Service\n(QoS) rules and traffic shaping based on applications, which enables\nprioritization of critical services, such as VoIP, video conferencing, and\nbusiness applications, as well as de-prioritization of non-essential traffic,\nsuch as downloads and updates. Mininet can simulate real-time SDN scenarios\nbecause it is used to prototype whole architectures. It is also compatible with\ncontrollers OpenDaylight and Ryu. It has tested the framework using the InSDN\ndataset and proved that it can identify different kinds of cyberattacks and\nhandle bandwidth allocation efficiently under circumstances of resource\nconstraints. SmartSecChain-SDN comprehensively addresses SDN system protection,\nsecuring and enhancing. The proposed study offers an innovative, extensible way\nto improve cybersecurity, regulatory compliance, and the administration of\nnext-generation programmable networks."
    },
    {
        "date": "2025-11",
        "title": "A Secured Intent-Based Networking (sIBN) with Data-Driven Time-Aware Intrusion Detection",
        "author": "Urslla Uchechi Izuazu, Mounir Bensalem, and Admela Jukan",
        "link": "http://arxiv.org/abs/2511.05133v1",
        "abstract": "While Intent-Based Networking (IBN) promises operational efficiency through\nautonomous and abstraction-driven network management, a critical unaddressed\nissue lies in IBN's implicit trust in the integrity of intent ingested by the\nnetwork. This inherent assumption of data reliability creates a blind spot\nexploitable by Man-in-the-Middle (MitM) attacks, where an adversary intercepts\nand alters intent before it is enacted, compelling the network to orchestrate\nmalicious configurations. This study proposes a secured IBN (sIBN) system with\ndata driven intrusion detection method designed to secure legitimate user\nintent from adversarial tampering. The proposed intent intrusion detection\nsystem uses a ML model applied for network behavioral anomaly detection to\nreveal temporal patterns of intent tampering. This is achieved by leveraging a\nset of original behavioral metrics and newly engineered time-aware features,\nwith the model's hyperparameters fine-tuned through the randomized search\ncross-validation (RSCV) technique. Numerical results based on real-world data\nsets, show the effectiveness of sIBN, achieving the best performance across\nstandard evaluation metrics, in both binary and multi classification tasks,\nwhile maintaining low error rates."
    },
    {
        "date": "2025-11",
        "title": "PhantomFetch: Obfuscating Loads against Prefetcher Side-Channel Attacks",
        "author": "Xingzhi Zhang, Buyi Lv, Yimin Lu, and Kai Bu",
        "link": "http://arxiv.org/abs/2511.05110v1",
        "abstract": "The IP-stride prefetcher has recently been exploited to leak secrets through\nside-channel attacks. It, however, cannot be simply disabled for security with\nprefetching speedup as a sacrifice. The state-of-the-art defense tries to\nretain the prefetching effect by hardware modification. In this paper, we\npresent PhantomFetch as the first prefetching-retentive and hardware-agnostic\ndefense. It avoids potential remanufacturing cost and enriches applicability to\noff-the-shelf devices. The key idea is to directly break the exploitable\ncoupling between trained prefetcher entries and the victim's secret-dependent\nloads by obfuscating the sensitive load effects of the victim. The experiment\nresults show that PhantomFetch can secure the IP-stride prefetcher with only\nnegligible overhead."
    },
    {
        "date": "2025-11",
        "title": "SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements",
        "author": "J\u00f6rg Gamerdinger, Benedict Wetzel, Patrick Schulz, Sven Teufel, and Oliver Bringmann",
        "link": "http://arxiv.org/abs/2511.05108v1",
        "abstract": "Lane detection for autonomous driving in snow-covered environments remains a\nmajor challenge due to the frequent absence or occlusion of lane markings. In\nthis paper, we present a novel, robust and realtime capable approach that\nbypasses the reliance on traditional lane markings by detecting roadside\nfeatures,specifically vertical roadside posts called delineators, as indirect\nlane indicators. Our method first perceives these posts, then fits a smooth\nlane trajectory using a parameterized Bezier curve model, leveraging spatial\nconsistency and road geometry. To support training and evaluation in these\nchallenging scenarios, we introduce SnowyLane, a new synthetic dataset\ncontaining 80,000 annotated frames capture winter driving conditions, with\nvarying snow coverage, and lighting conditions. Compared to state-of-the-art\nlane detection systems, our approach demonstrates significantly improved\nrobustness in adverse weather, particularly in cases with heavy snow occlusion.\nThis work establishes a strong foundation for reliable lane detection in winter\nscenarios and contributes a valuable resource for future research in\nall-weather autonomous driving. The dataset is available at\nhttps://ekut-es.github.io/snowy-lane"
    },
    {
        "date": "2025-11",
        "title": "Quantifying the Risk of Transferred Black Box Attacks",
        "author": "Disesdi Susanna Cox, and Niklas Bunzel",
        "link": "http://arxiv.org/abs/2511.05102v1",
        "abstract": "Neural networks have become pervasive across various applications, including\nsecurity-related products. However, their widespread adoption has heightened\nconcerns regarding vulnerability to adversarial attacks. With emerging\nregulations and standards emphasizing security, organizations must reliably\nquantify risks associated with these attacks, particularly regarding\ntransferred adversarial attacks, which remain challenging to evaluate\naccurately. This paper investigates the complexities involved in resilience\ntesting against transferred adversarial attacks. Our analysis specifically\naddresses black-box evasion attacks, highlighting transfer-based attacks due to\ntheir practical significance and typically high transferability between neural\nnetwork models. We underline the computational infeasibility of exhaustively\nexploring high-dimensional input spaces to achieve complete test coverage. As a\nresult, comprehensive adversarial risk mapping is deemed impractical. To\nmitigate this limitation, we propose a targeted resilience testing framework\nthat employs surrogate models strategically selected based on Centered Kernel\nAlignment (CKA) similarity. By leveraging surrogate models exhibiting both high\nand low CKA similarities relative to the target model, the proposed approach\nseeks to optimize coverage of adversarial subspaces. Risk estimation is\nconducted using regression-based estimators, providing organizations with\nrealistic and actionable risk quantification."
    },
    {
        "date": "2025-11",
        "title": "TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS",
        "author": "Arslan Mumtaz, and Mridula Singh",
        "link": "http://arxiv.org/abs/2511.05100v1",
        "abstract": "Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation,\nand Timing (PNT) information to over 4 billion devices worldwide. Despite its\npervasive use in safety critical and high precision applications, GNSS remains\nvulnerable to spoofing attacks. Cryptographic enhancements, such as the use of\nTESLA protocol in Galileo, to provide navigation message authentication do not\nmitigate time of arrival manipulations. In this paper, we propose TRICK, a\nprimitive for secure positioning that closes this gap by introducing a\nfundamentally new approach that only requires two way communications with a\nsingle reference node along with multiple broadcast signals. Unlike classical\nVerifiable Multilateration (VM), which requires establishing two way\ncommunication with each reference nodes, our solution relies on only two\nmeasurements with a trusted Low Earth Orbiting (LEO) satellite and combines\nbroadcast navigation signals. We rigorously prove that combining the LEO\nsatellite based two way range measurements and multiple one way ranges such as\nfrom broadcast signals of GNSS into ellipsoidal constraint restores the same\nguarantees as offered by VM whilst using minimal infrastructure and message\nexchanges. Through detailed analysis, we show that our approach reliably\ndetects spoofing attempts while adding negligible computation overhead."
    },
    {
        "date": "2025-11",
        "title": "Deep learning models are vulnerable, but adversarial examples are even more vulnerable",
        "author": "Jun Li, Yanwei Xu, Keran Li, and Xiaoli Zhang",
        "link": "http://arxiv.org/abs/2511.05073v1",
        "abstract": "Understanding intrinsic differences between adversarial examples and clean\nsamples is key to enhancing DNN robustness and detection against adversarial\nattacks. This study first empirically finds that image-based adversarial\nexamples are notably sensitive to occlusion. Controlled experiments on CIFAR-10\nused nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,\npaired with original samples for evaluation. We introduce Sliding Mask\nConfidence Entropy (SMCE) to quantify model confidence fluctuation under\nocclusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy\nField Maps and statistical distributions show adversarial examples have\nsignificantly higher confidence volatility under occlusion than originals.\nBased on this, we propose Sliding Window Mask-based Adversarial Example\nDetection (SWM-AED), which avoids catastrophic overfitting of conventional\nadversarial training. Evaluations across classifiers and attacks on CIFAR-10\ndemonstrate robust performance, with accuracy over 62% in most cases and up to\n96.5%."
    },
    {
        "date": "2025-11",
        "title": "DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning",
        "author": "Tharindu Fernando, Clinton Fookes, and Sridha Sridharan",
        "link": "http://arxiv.org/abs/2511.04949v1",
        "abstract": "Rapid advances in generative AI have led to increasingly realistic deepfakes,\nposing growing challenges for law enforcement and public trust. Existing\npassive deepfake detectors struggle to keep pace, largely due to their\ndependence on specific forgery artifacts, which limits their ability to\ngeneralize to new deepfake types. Proactive deepfake detection using watermarks\nhas emerged to address the challenge of identifying high-quality synthetic\nmedia. However, these methods often struggle to balance robustness against\nbenign distortions with sensitivity to malicious tampering. This paper\nintroduces a novel deep learning framework that harnesses high-dimensional\nlatent space representations and the Multi-Agent Adversarial Reinforcement\nLearning (MAARL) paradigm to develop a robust and adaptive watermarking\napproach. Specifically, we develop a learnable watermark embedder that operates\nin the latent space, capturing high-level image semantics, while offering\nprecise control over message encoding and extraction. The MAARL paradigm\nempowers the learnable watermarking agent to pursue an optimal balance between\nrobustness and fragility by interacting with a dynamic curriculum of benign and\nmalicious image manipulations simulated by an adversarial attacker agent.\nComprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that\nour method consistently outperforms state-of-the-art approaches, achieving\nimprovements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under\nchallenging manipulation scenarios."
    },
    {
        "date": "2025-11",
        "title": "Zero Trust Security Model Implementation in Microservices Architectures Using Identity Federation",
        "author": "Rethish Nair Rajendran, Sathish Krishna Anumula, Dileep Kumar Rai, and Sachin Agrawal",
        "link": "http://arxiv.org/abs/2511.04925v1",
        "abstract": "The microservice bombshells that have been linked with the microservice\nexpansion have altered the application architectures, offered agility and\nscalability in terms of complexity in security trade-offs. Feeble legacy-based\nperimeter-based policies are unable to offer safeguard to distributed workloads\nand temporary interaction among and in between the services. The article itself\nis a case on the need of the Zero Trust Security Model of micro services\necosystem, particularly, the fact that human and workloads require identity\nfederation. It is proposed that the solution framework will be based on\nindustry-standard authentication and authorization and end-to-end trust\nidentity technologies, including Authorization and OpenID connect (OIDC),\nAuthorization and OAuth 2.0 token exchange, and Authorization and SPIFFE/ SPIRE\nworkload identities. Experimental evaluation is a unique demonstration of a\nsuperior security position of making use of a smaller attack surface, harmony\npolicy enforcement, as well as interoperability across multi- domain\nenvironments. The research results overlay that the federated identity combined\nwith the Zero Trust basics not only guarantee the rules relating to\nauthentication and authorization but also fully complies with the latest\nDevSecOps standards of microservice deployment, which is automated, scaled, and\nresilient. The current project offers a stringent roadmap to the organizations\nthat desire to apply Zero Trust in cloud-native technologies but will as well\nguarantee adherence and interoperability."
    },
    {
        "date": "2025-11",
        "title": "MERaLiON-SER: Robust Speech Emotion Recognition Model for English and SEA Languages",
        "author": "Hardik B. Sailor, Aw Ai Ti, Chen Fang Yih Nancy, Chiu Ying Lay, Ding Yang, He Yingxu, Jiang Ridong, Li Jingtao, Liao Jingyi, Liu Zhuohan, Lu Yanfeng, Ma Yi, Manas Gupta, Muhammad Huzaifah Bin Md Shahrin, Nabilah Binte Md Johan, Nattadaporn Lertcheva, Pan Chunlei, Pham Minh Duc, Siti Maryam Binte Ahmad Subaidi, Siti Umairah Binte Mohammad Salleh, Sun Shuo, Tarun Kumar Vangani, Wang Qiongqiong, Won Cheng Yi Lewis, Wong Heng Meng Jeremy, Wu Jinyang, Zhang Huayun, Zhang Longyin, and Zou Xunlong",
        "link": "http://arxiv.org/abs/2511.04914v1",
        "abstract": "We present MERaLiON-SER, a robust speech emotion recognition model de- signed\nfor English and Southeast Asian languages. The model is trained using a hybrid\nobjective combining weighted categorical cross-entropy and Concordance\nCorrelation Coefficient (CCC) losses for joint discrete and dimensional emotion\nmodelling. This dual approach enables the model to capture both the distinct\ncategories of emotion (like happy or angry) and the fine-grained, such as\narousal (intensity), valence (positivity/negativity), and dominance (sense of\ncontrol), lead- ing to a more comprehensive and robust representation of human\naffect. Extensive evaluations across multilingual Singaporean languages\n(English, Chinese, Malay, and Tamil ) and other public benchmarks show that\nMERaLiON-SER consistently surpasses both open-source speech encoders and large\nAudio-LLMs. These results underscore the importance of specialised speech-only\nmodels for accurate paralin- guistic understanding and cross-lingual\ngeneralisation. Furthermore, the proposed framework provides a foundation for\nintegrating emotion-aware perception into future agentic audio systems,\nenabling more empathetic and contextually adaptive multimodal reasoning."
    },
    {
        "date": "2025-11",
        "title": "Bit-Flipping Attack Exploration and Countermeasure in 5G Network",
        "author": "Joon Kim, Chengwei Duan, and Sandip Ray",
        "link": "http://arxiv.org/abs/2511.04882v1",
        "abstract": "5G communication technology has become a vital component in a wide range of\napplications due to its unique advantages such as high data rate and low\nlatency. While much of the existing research has focused on optimizing its\nefficiency and performance, security considerations have not received\ncomparable attention, potentially leaving critical vulnerabilities unexplored.\nIn this work, we investigate the vulnerability of 5G systems to bit-flipping\nattacks, which is an integrity attack where an adversary intercepts 5G network\ntraffic and modifies specific fields of an encrypted message without\ndecryption, thus mutating the message while remaining valid to the receiver.\nNotably, these attacks do not require the attacker to know the plaintext, and\nonly the semantic meaning or position of certain fields would be enough to\neffect targeted modifications. We conduct our analysis on OpenAirInterface\n(OAI), an open-source 5G platform that follows the 3GPP Technical\nSpecifications, to rigorously test the real-world feasibility and impact of\nbit-flipping attacks under current 5G encryption mechanisms. Finally, we\npropose a keystream-based shuffling defense mechanism to mitigate the effect of\nsuch attacks by raising the difficulty of manipulating specific encrypted\nfields, while introducing no additional communication overhead compared to the\nNAS Integrity Algorithm (NIA) in 5G. Our findings reveal that enhancements to\n5G security are needed to better protect against attacks that alter data during\ntransmission at the network level."
    },
    {
        "date": "2025-11",
        "title": "Security Evaluation of Quantum Circuit Split Compilation under an Oracle-Guided Attack",
        "author": "Hongyu Zhang, and Yuntao Liu",
        "link": "http://arxiv.org/abs/2511.04842v1",
        "abstract": "Quantum circuits are the fundamental representation of quantum algorithms and\nconstitute valuable intellectual property (IP). Multiple quantum circuit\nobfuscation (QCO) techniques have been proposed in prior research to protect\nquantum circuit IP against malicious compilers. However, there has not been a\nthorough security evaluation of these schemes. In this work, we investigate the\nresilience of split compilation against an oracle-guided attack. Split\ncompilation is one of the most studied QCO techniques, where the circuit to be\ncompiled is split into two disjoint partitions. Each split circuit is known to\nthe compiler, but the interconnections between them are hidden. We propose an\noracle-guided security evaluation framework in which candidate connections are\nsystematically tested against input-output observations, with iteratively\npruned inconsistent mappings. This hierarchical matching process exploits the\nreversibility of quantum gates and reduces the search space compared to\nbrute-force enumeration. Experimental evaluation in the RevLib benchmark suite\nshows that only a small number of I/O pairs are sufficient to recover the\ncorrect inter-split connections and reconstruct the entire circuits. Our study\nmarks the first thorough security evaluations in quantum IP protection and\nhighlights the necessity of such evaluations in the development of new\nprotection schemes."
    },
    {
        "date": "2025-11",
        "title": "Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose",
        "author": "Shuo Zhao, and Jianxu Chen",
        "link": "http://arxiv.org/abs/2511.04803v1",
        "abstract": "Generalist biomedical image segmentation models such as Cellpose are\nincreasingly applied across diverse imaging modalities and cell types. However,\ntwo critical challenges remain underexplored: (1) the extent of training data\nredundancy and (2) the impact of cross domain transfer on model retention. In\nthis study, we conduct a systematic empirical analysis of these challenges\nusing Cellpose as a case study. First, to assess data redundancy, we propose a\nsimple dataset quantization (DQ) strategy for constructing compact yet diverse\ntraining subsets. Experiments on the Cyto dataset show that image segmentation\nperformance saturates with only 10% of the data, revealing substantial\nredundancy and potential for training with minimal annotations. Latent space\nanalysis using MAE embeddings and t-SNE confirms that DQ selected patches\ncapture greater feature diversity than random sampling. Second, to examine\ncatastrophic forgetting, we perform cross domain finetuning experiments and\nobserve significant degradation in source domain performance, particularly when\nadapting from generalist to specialist domains. We demonstrate that selective\nDQ based replay reintroducing just 5-10% of the source data effectively\nrestores source performance, while full replay can hinder target adaptation.\nAdditionally, we find that training domain sequencing improves generalization\nand reduces forgetting in multi stage transfer. Our findings highlight the\nimportance of data centric design in biomedical image segmentation and suggest\nthat efficient training requires not only compact subsets but also retention\naware learning strategies and informed domain ordering. The code is available\nat https://github.com/MMV-Lab/biomedseg-efficiency."
    },
    {
        "date": "2025-11",
        "title": "DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation",
        "author": "Dhenenjay Yadav, and Rohan Sawai",
        "link": "http://arxiv.org/abs/2511.04766v1",
        "abstract": "Foundation models (FMs) offer powerful representations for geospatial\nanalysis, but adapting them effectively remains challenging. Standard\nadaptation methods, whether full fine-tuning or efficient frozen-backbone\napproaches, typically employ decoders with fixed regularization strategies,\nfailing to account for the significant heterogeneity in satellite imagery. We\nintroduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder\narchitecture designed to address this limitation. DARN integrates three key\ninnovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates\nper-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically\nadjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and\n(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide\ntheoretical justifications linking DARN's optimization to stationary point\nconvergence and its mechanism to adaptive information bottlenecks. Empirically,\nDARN demonstrates exceptional performance across both major adaptation\nparadigms. In full fine-tuning (unfrozen backbone), DARN achieves a new\nstate-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp\nover prior SOTA). In efficient adaptation (frozen backbone), DARN achieves\nSOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering\nsubstantial advantages crucial for real-world deployment: superior\nout-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),\nenhanced robustness (17% relative reduction in corruption error), and improved\nperformance on minority classes. DARN offers a more intelligent, robust, and\nefficient approach to leveraging FMs in critical geospatial applications."
    },
    {
        "date": "2025-11",
        "title": "Confidential Computing for Cloud Security: Exploring Hardware based Encryption Using Trusted Execution Environments",
        "author": "Dhruv Deepak Agarwal, and Aswani Kumar Cherukuri",
        "link": "http://arxiv.org/abs/2511.04550v1",
        "abstract": "The growth of cloud computing has revolutionized data processing and storage\ncapacities to another levels of scalability and flexibility. But in the\nprocess, it has created a huge challenge of security, especially in terms of\nsafeguarding sensitive data. Classical security practices, including encryption\nat rest and during transit, fail to protect data in use and expose it to\nvarious possible breaches. In response to this problem , Confidential Computing\nhas been a tool ,seeking to secure data in processing by usage of\nhardware-based Trusted Execution Environments (TEEs). TEEs, including Intel's\nSoftware Guard Extensions (SGX) and ARM's TrustZone, offers protected contexts\nwithin the processor, where data is kept confidential ,intact and secure , even\nwith malicious software or compromised operating systems. In this research, we\nhave explored the architecture and security features of TEEs like Intel SGX and\nARM TrustZone, and their effectiveness in improving cloud data security. From a\nthorough literature survey ,we have analyzed the deployment strategies,\nperformance indicators, and practical uses of these TEEs for the same purpose.\nIn addition, we have discussed the issues regarding deployment, possible\nweaknesses, scalability issues, and integration issues. Our results focuses on\nthe central position of TEEs in strengthening and advancing cloud security\ninfrastructures, pointing towards their ability to create a secure foundation\nfor Confidential Computing."
    },
    {
        "date": "2025-11",
        "title": "Large Language Models for Cyber Security",
        "author": "Raunak Somani, and Aswani Kumar Cherukuri",
        "link": "http://arxiv.org/abs/2511.04508v1",
        "abstract": "This paper studies the integration off Large Language Models into\ncybersecurity tools and protocols. The main issue discussed in this paper is\nhow traditional rule-based and signature based security systems are not enough\nto deal with modern AI powered cyber threats. Cybersecurity industry is\nchanging as threats are becoming more dangerous and adaptive in nature by\nlevering the features provided by AI tools. By integrating LLMs into these\ntools and protocols, make the systems scalable, context-aware and intelligent.\nThus helping it to mitigate these evolving cyber threats. The paper studies the\narchitecture and functioning of LLMs, its integration into Encrypted prompts to\nprevent prompt injection attacks. It also studies the integration of LLMs into\ncybersecurity tools using a four layered architecture. At last, the paper has\ntried to explain various ways of integration LLMs into traditional Intrusion\nDetection System and enhancing its original abilities in various dimensions.\nThe key findings of this paper has been (i)Encrypted Prompt with LLM is an\neffective way to mitigate prompt injection attacks, (ii) LLM enhanced cyber\nsecurity tools are more accurate, scalable and adaptable to new threats as\ncompared to traditional models, (iii) The decoupled model approach for LLM\nintegration into IDS is the best way as it is the most accurate way."
    },
    {
        "date": "2025-11",
        "title": "Exploiting Data Structures for Bypassing and Crashing Anti-Malware Solutions via Telemetry Complexity Attacks",
        "author": "Evgenios Gkritsis, Constantinos Patsakis, and George Stergiopoulos",
        "link": "http://arxiv.org/abs/2511.04472v1",
        "abstract": "Anti-malware systems rely on sandboxes, hooks, and telemetry pipelines,\nincluding collection agents, serializers, and database backends, to monitor\nprogram and system behavior. We show that these data-handling components\nconstitute an exploitable attack surface that can lead to denial-of-analysis\n(DoA) states without disabling sensors or requiring elevated privileges. As a\nresult, we present \\textit{Telemetry Complexity Attacks} (TCAs), a new class of\nvulnerabilities that exploit fundamental mismatches between unbounded\ncollection mechanisms and bounded processing capabilities. Our method\nrecursively spawns child processes to generate specially crafted, deeply\nnested, and oversized telemetry that stresses serialization and storage\nboundaries, as well as visualization layers, for example, JSON/BSON depth and\nsize limits. Depending on the product, this leads to truncated or missing\nbehavioral reports, rejected database inserts, serializer recursion and size\nerrors, and unresponsive dashboards. In all of these cases, malicious activity\nis normally executed; however, depending on the examined solution, it is not\nrecorded and/or not presented to the analysts. Therefore, instead of evading\nsensors, we break the pipeline that stores the data captured by the sensors.\n  We evaluate our technique against twelve commercial and open-source malware\nanalysis platforms and endpoint detection and response (EDR) solutions. Seven\nproducts fail in different stages of the telemetry pipeline; two vendors\nassigned CVE identifiers (CVE-2025-61301 and CVE-2025-61303), and others issued\npatches or configuration changes. We discuss root causes and propose mitigation\nstrategies to prevent DoA attacks triggered by adversarial telemetry."
    },
    {
        "date": "2025-11",
        "title": "ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting",
        "author": "Syeda Sitara Wishal Fatima, and Afshin Rahimi",
        "link": "http://arxiv.org/abs/2511.04445v1",
        "abstract": "Time series forecasting is essential across domains from finance to supply\nchain management. This paper introduces ForecastGAN, a novel decomposition\nbased adversarial framework addressing limitations in existing approaches for\nmulti-horizon predictions. Although transformer models excel in long-term\nforecasting, they often underperform in short-term scenarios and typically\nignore categorical features. ForecastGAN operates through three integrated\nmodules: a Decomposition Module that extracts seasonality and trend components;\na Model Selection Module that identifies optimal neural network configurations\nbased on forecasting horizon; and an Adversarial Training Module that enhances\nprediction robustness through Conditional Generative Adversarial Network\ntraining. Unlike conventional approaches, ForecastGAN effectively integrates\nboth numerical and categorical features. We validate our framework on eleven\nbenchmark multivariate time series datasets that span various forecasting\nhorizons. The results show that ForecastGAN consistently outperforms\nstate-of-the-art transformer models for short-term forecasting while remaining\ncompetitive for long-term horizons. This research establishes a more\ngeneralizable approach to time series forecasting that adapts to specific\ncontexts while maintaining strong performance across diverse data\ncharacteristics without extensive hyperparameter tuning."
    },
    {
        "date": "2025-11",
        "title": "Adversarially Robust and Interpretable Magecart Malware Detection",
        "author": "Pedro Pereira, Jos\u00e9 Gouveia, Jo\u00e3o Vitorino, Eva Maia, and Isabel Pra\u00e7a",
        "link": "http://arxiv.org/abs/2511.04440v1",
        "abstract": "Magecart skimming attacks have emerged as a significant threat to client-side\nsecurity and user trust in online payment systems. This paper addresses the\nchallenge of achieving robust and explainable detection of Magecart attacks\nthrough a comparative study of various Machine Learning (ML) models with a\nreal-world dataset. Tree-based, linear, and kernel-based models were applied,\nfurther enhanced through hyperparameter tuning and feature selection, to\ndistinguish between benign and malicious scripts. Such models are supported by\na Behavior Deterministic Finite Automaton (DFA) which captures structural\nbehavior patterns in scripts, helping to analyze and classify client-side\nscript execution logs. To ensure robustness against adversarial evasion\nattacks, the ML models were adversarially trained and evaluated using attacks\nfrom the Adversarial Robustness Toolbox and the Adaptative Perturbation Pattern\nMethod. In addition, concise explanations of ML model decisions are provided,\nsupporting transparency and user trust. Experimental validation demonstrated\nhigh detection performance and interpretable reasoning, demonstrating that\ntraditional ML models can be effective in real-world web security contexts."
    },
    {
        "date": "2025-11",
        "title": "Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness",
        "author": "Subeen Park, Joowang Kim, Hakyung Lee, Sunjae Yoo, and Kyungwoo Song",
        "link": "http://arxiv.org/abs/2511.04401v1",
        "abstract": "Deep learning models achieve strong performance across various domains but\noften rely on spurious correlations, making them vulnerable to distribution\nshifts. This issue is particularly severe in subpopulation shift scenarios,\nwhere models struggle in underrepresented groups. While existing methods have\nmade progress in mitigating this issue, their performance gains are still\nconstrained. They lack a rigorous theoretical framework connecting the\nembedding space representations with worst-group error. To address this\nlimitation, we propose Spurious Correlation-Aware Embedding Regularization for\nWorst-Group Robustness (SCER), a novel approach that directly regularizes\nfeature representations to suppress spurious cues. We show theoretically that\nworst-group error is influenced by how strongly the classifier relies on\nspurious versus core directions, identified from differences in group-wise mean\nembeddings across domains and classes. By imposing theoretical constraints at\nthe embedding level, SCER encourages models to focus on core features while\nreducing sensitivity to spurious patterns. Through systematic evaluation on\nmultiple vision and language, we show that SCER outperforms prior\nstate-of-the-art studies in worst-group accuracy. Our code is available at\n\\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}."
    },
    {
        "date": "2025-11",
        "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research",
        "author": "Tim Beyer, Jonas Dornbusch, Jakob Steimle, Moritz Ladenburger, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2511.04316v1",
        "abstract": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety."
    },
    {
        "date": "2025-11",
        "title": "Robustness of Minimum-Volume Nonnegative Matrix Factorization under an Expanded Sufficiently Scattered Condition",
        "author": "Giovanni Barbarino, Nicolas Gillis, and Subhayan Saha",
        "link": "http://arxiv.org/abs/2511.04291v1",
        "abstract": "Minimum-volume nonnegative matrix factorization (min-vol NMF) has been used\nsuccessfully in many applications, such as hyperspectral imaging, chemical\nkinetics, spectroscopy, topic modeling, and audio source separation. However,\nits robustness to noise has been a long-standing open problem. In this paper,\nwe prove that min-vol NMF identifies the groundtruth factors in the presence of\nnoise under a condition referred to as the expanded sufficiently scattered\ncondition which requires the data points to be sufficiently well scattered in\nthe latent simplex generated by the basis vectors."
    },
    {
        "date": "2025-11",
        "title": "Black-Box Guardrail Reverse-engineering Attack",
        "author": "Hongwei Yao, Yun Xia, Shuo Shao, Haoran Shi, Tong Qiao, and Cong Wang",
        "link": "http://arxiv.org/abs/2511.04215v1",
        "abstract": "Large language models (LLMs) increasingly employ guardrails to enforce\nethical, legal, and application-specific constraints on their outputs. While\neffective at mitigating harmful responses, these guardrails introduce a new\nclass of vulnerabilities by exposing observable decision patterns. In this\nwork, we present the first study of black-box LLM guardrail reverse-engineering\nattacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement\nlearning-based framework that leverages genetic algorithm-driven data\naugmentation to approximate the decision-making policy of victim guardrails. By\niteratively collecting input-output pairs, prioritizing divergence cases, and\napplying targeted mutations and crossovers, our method incrementally converges\ntoward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on\nthree widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,\nand demonstrate that it achieves an rule matching rate exceeding 0.92 while\nrequiring less than $85 in API costs. These findings underscore the practical\nfeasibility of guardrail extraction and highlight significant security risks\nfor current LLM safety mechanisms. Our findings expose critical vulnerabilities\nin current guardrail designs and highlight the urgent need for more robust\ndefense mechanisms in LLM deployment."
    },
    {
        "date": "2025-11",
        "title": "DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection",
        "author": "Qingfeng Chen, Haojin Zeng, Jingyi Jie, Shichao Zhang, and Debo Cheng",
        "link": "http://arxiv.org/abs/2511.04086v1",
        "abstract": "With the rapid growth of graph-structured data in critical domains,\nunsupervised graph-level anomaly detection (UGAD) has become a pivotal task.\nUGAD seeks to identify entire graphs that deviate from normal behavioral\npatterns. However, most Graph Neural Network (GNN) approaches implicitly assume\nthat the training set is clean, containing only normal graphs, which is rarely\ntrue in practice. Even modest contamination by anomalous graphs can distort\nlearned representations and sharply degrade performance. To address this\nchallenge, we propose DeNoise, a robust UGAD framework explicitly designed for\ncontaminated training data. It jointly optimizes a graph-level encoder, an\nattribute decoder, and a structure decoder via an adversarial objective to\nlearn noise-resistant embeddings. Further, DeNoise introduces an encoder\nanchor-alignment denoising mechanism that fuses high-information node\nembeddings from normal graphs into all graph embeddings, improving\nrepresentation quality while suppressing anomaly interference. A contrastive\nlearning component then compacts normal graph embeddings and repels anomalous\nones in the latent space. Extensive experiments on eight real-world datasets\ndemonstrate that DeNoise consistently learns reliable graph-level\nrepresentations under varying noise intensities and significantly outperforms\nstate-of-the-art UGAD baselines."
    },
    {
        "date": "2025-11",
        "title": "Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score",
        "author": "Abu Hanif Muhammad Syarubany",
        "link": "http://arxiv.org/abs/2511.04083v1",
        "abstract": "We study CT image denoising in the unpaired and self-supervised regimes by\nevaluating two strong, training-data-efficient paradigms: a CycleGAN-based\nresidual translator and a Noise2Score (N2S) score-matching denoiser. Under a\ncommon evaluation protocol, a configuration sweep identifies a simple standard\nU-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =\n64) as the most reliable setting; we then train it to convergence with a longer\nschedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234\nSSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an\nunseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly\nbehind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,\nhighlighting its utility when clean pairs are unavailable. Overall, CycleGAN\noffers the strongest final image quality, whereas Noise2Score provides a robust\npair-free alternative with competitive performance. Source code is available at\nhttps://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score."
    },
    {
        "date": "2025-11",
        "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals",
        "author": "Arfina Rahman, and Mahesh Banavar",
        "link": "http://arxiv.org/abs/2511.04037v1",
        "abstract": "Photoplethysmography (PPG) signals, which measure changes in blood volume in\nthe skin using light, have recently gained attention in biometric\nauthentication because of their non-invasive acquisition, inherent liveness\ndetection, and suitability for low-cost wearable devices. However, PPG signal\nquality is challenged by motion artifacts, illumination changes, and\ninter-subject physiological variability, making robust feature extraction and\nclassification crucial. This study proposes a lightweight and cost-effective\nbiometric authentication framework based on PPG signals extracted from\nlow-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings\nfrom 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The\nraw PPG signals undergo a standard preprocessing pipeline involving baseline\ndrift removal, motion artifact suppression using Principal Component Analysis\n(PCA), bandpass filtering, Fourier-based resampling, and amplitude\nnormalization. To generate robust representations, each one-dimensional PPG\nsegment is converted into a two-dimensional time-frequency scalogram via the\nContinuous Wavelet Transform (CWT), effectively capturing transient\ncardiovascular dynamics. We developed a hybrid deep learning model, termed\nCVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision\nTransformer (CVT) and ConvMixer branches with temporal features from a Long\nShort-Term Memory network (LSTM). The experimental results on 46 subjects\ndemonstrate an authentication accuracy of 98%, validating the robustness of the\nmodel to noise and variability between subjects. Due to its efficiency,\nscalability, and inherent liveness detection capability, the proposed system is\nwell-suited for real-world mobile and embedded biometric security applications."
    },
    {
        "date": "2025-11",
        "title": "P-MIA: A Profiled-Based Membership Inference Attack on Cognitive Diagnosis Models",
        "author": "Mingliang Hou, Yinuo Wang, Teng Guo, Zitao Liu, Wenzhou Dou, Jiaqi Zheng, Renqiang Luo, Mi Tian, and Weiqi Luo",
        "link": "http://arxiv.org/abs/2511.04716v1",
        "abstract": "Cognitive diagnosis models (CDMs) are pivotal for creating fine-grained\nlearner profiles in modern intelligent education platforms. However, these\nmodels are trained on sensitive student data, raising significant privacy\nconcerns. While membership inference attacks (MIA) have been studied in various\ndomains, their application to CDMs remains a critical research gap, leaving\ntheir privacy risks unquantified. This paper is the first to systematically\ninvestigate MIA against CDMs. We introduce a novel and realistic grey box\nthreat model that exploits the explainability features of these platforms,\nwhere a model's internal knowledge state vectors are exposed to users through\nvisualizations such as radar charts. We demonstrate that these vectors can be\naccurately reverse-engineered from such visualizations, creating a potent\nattack surface. Based on this threat model, we propose a profile-based MIA\n(P-MIA) framework that leverages both the model's final prediction\nprobabilities and the exposed internal knowledge state vectors as features.\nExtensive experiments on three real-world datasets against mainstream CDMs show\nthat our grey-box attack significantly outperforms standard black-box\nbaselines. Furthermore, we showcase the utility of P-MIA as an auditing tool by\nsuccessfully evaluating the efficacy of machine unlearning techniques and\nrevealing their limitations."
    },
    {
        "date": "2025-11",
        "title": "Robust inference using density-powered Stein operators",
        "author": "Shinto Eguchi",
        "link": "http://arxiv.org/abs/2511.03963v1",
        "abstract": "We introduce a density-power weighted variant for the Stein operator, called\nthe $\\gamma$-Stein operator. This is a novel class of operators derived from\nthe $\\gamma$-divergence, designed to build robust inference methods for\nunnormalized probability models. The operator's construction (weighting by the\nmodel density raised to a positive power $\\gamma$ inherently down-weights the\ninfluence of outliers, providing a principled mechanism for robustness.\nApplying this operator yields a robust generalization of score matching that\nretains the crucial property of being independent of the model's normalizing\nconstant. We extend this framework to develop two key applications: the\n$\\gamma$-kernelized Stein discrepancy for robust goodness-of-fit testing, and\n$\\gamma$-Stein variational gradient descent for robust Bayesian posterior\napproximation. Empirical results on contaminated Gaussian and quartic potential\nmodels show our methods significantly outperform standard baselines in both\nrobustness and statistical efficiency."
    },
    {
        "date": "2025-11",
        "title": "LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction",
        "author": "Sanggeon Yun, Hyunwoo Oh, Ryozo Masukawa, Pietro Mercati, Nathaniel D. Bastian, and Mohsen Imani",
        "link": "http://arxiv.org/abs/2511.03938v1",
        "abstract": "Hyperdimensional computing (HDC) suits memory, energy, and\nreliability-constrained systems, yet the standard \"one prototype per class\"\ndesign requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior\ncompaction reduces $D$ (feature axis), improving storage/compute but weakening\nrobustness. We introduce LogHD, a logarithmic class-axis reduction that\nreplaces the $C$ per-class prototypes with $n\\!\\approx\\!\\lceil\\log_k C\\rceil$\nbundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional\nactivation space, cutting memory to $O(D\\log_k C)$ while preserving $D$. LogHD\nuses a capacity-aware codebook and profile-based decoding, and composes with\nfeature-axis sparsification. Across datasets and injected bit flips, LogHD\nattains competitive accuracy with smaller models and higher resilience at\nmatched memory. Under equal memory, it sustains target accuracy at roughly\n$2.5$-$3.0\\times$ higher bit-flip rates than feature-axis compression; an ASIC\ninstantiation delivers $498\\times$ energy efficiency and $62.6\\times$ speedup\nover an AMD Ryzen 9 9950X and $24.3\\times$/$6.58\\times$ over an NVIDIA RTX\n4090, and is $4.06\\times$ more energy-efficient and $2.19\\times$ faster than a\nfeature-axis HDC ASIC baseline."
    },
    {
        "date": "2025-11",
        "title": "Secure Code Generation at Scale with Reflexion",
        "author": "Arup Datta, Ahmed Aljohani, and Hyunsook Do",
        "link": "http://arxiv.org/abs/2511.03898v1",
        "abstract": "Large language models (LLMs) are now widely used to draft and refactor code,\nbut code that works is not necessarily secure. We evaluate secure code\ngeneration using the Instruct Prime, which eliminated compliance-required\nprompts and cue contamination, and evaluate five instruction-tuned code LLMs\nusing a zero-shot baseline and a three-round reflexion prompting approach.\nSecurity is measured using the Insecure Code Detector (ICD), and results are\nreported by measuring Repair, Regression, and NetGain metrics, considering the\nprogramming language and CWE family. Our findings show that insecurity remains\ncommon at the first round: roughly 25-33% of programs are insecure at a\nzero-shot baseline (t0 ). Weak cryptography/config-dependent bugs are the\nhardest to avoid while templated ones like XSS, code injection, and hard-coded\nsecrets are handled more reliably. Python yields the highest secure rates; C\nand C# are the lowest, with Java, JS, PHP, and C++ in the middle. Reflexion\nprompting improves security for all models, improving average accuracy from\n70.74% at t0 to 79.43% at t3 , with the largest gains in the first round\nfollowed by diminishing returns. The trends with Repair, Regression, and\nNetGain metrics show that applying one to two rounds produces most of the\nbenefits. A replication package is available at\nhttps://doi.org/10.5281/zenodo.17065846."
    },
    {
        "date": "2025-11",
        "title": "Security Analysis of Agentic AI Communication Protocols: A Comparative Evaluation",
        "author": "Yedidel Louck, Ariel Stulman, and Amit Dvir",
        "link": "http://arxiv.org/abs/2511.03841v1",
        "abstract": "Multi-agent systems (MAS) powered by artificial intelligence (AI) are\nincreasingly foundational to complex, distributed workflows. Yet, the security\nof their underlying communication protocols remains critically under-examined.\nThis paper presents the first empirical, comparative security analysis of the\nofficial CORAL implementation and a high-fidelity, SDK-based ACP\nimplementation, benchmarked against a literature-based evaluation of A2A. Using\na 14 point vulnerability taxonomy, we systematically assess their defenses\nacross authentication, authorization, integrity, confidentiality, and\navailability. Our results reveal a pronounced security dichotomy: CORAL\nexhibits a robust architectural design, particularly in its transport-layer\nmessage validation and session isolation, but suffers from critical\nimplementation-level vulnerabilities, including authentication and\nauthorization failures at its SSE gateway. Conversely, ACP's architectural\nflexibility, most notably its optional JWS enforcement, translates into\nhigh-impact integrity and confidentiality flaws. We contextualize these\nfindings within current industry trends, highlighting that existing protocols\nremain insufficiently secure. As a path forward, we recommend a hybrid approach\nthat combines CORAL's integrated architecture with ACP's mandatory per-message\nintegrity guarantees, laying the groundwork for resilient, next-generation\nagent communications."
    },
    {
        "date": "2025-11",
        "title": "Whisper Leak: a side-channel attack on Large Language Models",
        "author": "Geoff McDonald, and Jonathan Bar Or",
        "link": "http://arxiv.org/abs/2511.03675v1",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in sensitive domains\nincluding healthcare, legal services, and confidential communications, where\nprivacy is paramount. This paper introduces Whisper Leak, a side-channel attack\nthat infers user prompt topics from encrypted LLM traffic by analyzing packet\nsize and timing patterns in streaming responses. Despite TLS encryption\nprotecting content, these metadata patterns leak sufficient information to\nenable topic classification. We demonstrate the attack across 28 popular LLMs\nfrom major providers, achieving near-perfect classification (often >98% AUPRC)\nand high precision even at extreme class imbalance (10,000:1 noise-to-target\nratio). For many models, we achieve 100% precision in identifying sensitive\ntopics like \"money laundering\" while recovering 5-20% of target conversations.\nThis industry-wide vulnerability poses significant risks for users under\nnetwork surveillance by ISPs, governments, or local adversaries. We evaluate\nthree mitigation strategies - random padding, token batching, and packet\ninjection - finding that while each reduces attack effectiveness, none provides\ncomplete protection. Through responsible disclosure, we have collaborated with\nproviders to implement initial countermeasures. Our findings underscore the\nneed for LLM providers to address metadata leakage as AI systems handle\nincreasingly sensitive information."
    },
    {
        "date": "2025-11",
        "title": "SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection",
        "author": "Mahek Desai, Apoorva Rumale, and Marjan Asadinia",
        "link": "http://arxiv.org/abs/2511.03661v1",
        "abstract": "The integration of IoT devices in healthcare introduces significant security\nand reliability challenges, increasing susceptibility to cyber threats and\noperational anomalies. This study proposes a machine learning-driven framework\nfor (1) detecting malicious cyberattacks and (2) identifying faulty device\nanomalies, leveraging a dataset of 200,000 records. Eight machine learning\nmodels are evaluated across three learning approaches: supervised learning\n(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative\nAdversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised\nlearning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph\nNeural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The\ncomprehensive evaluation was conducted across multiple metrics like F1-score,\nprecision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost\nachieved 99\\% accuracy with minimal computational overhead (0.04s) for anomaly\ndetection, while Isolation Forest balanced precision and recall effectively.\nLSTM Autoencoders underperformed with lower accuracy and higher latency. For\nattack detection, KNN achieved near-perfect precision, recall, and F1-score\nwith the lowest computational cost (0.05s), followed by VAE at 97% accuracy.\nGAN showed the highest computational cost with lowest accuracy and ROC-AUC.\nThese findings enhance IoT-enabled healthcare security through effective\nanomaly detection strategies. By improving early detection of cyber threats and\ndevice failures, this framework has the potential to prevent data breaches,\nminimize system downtime, and ensure the continuous and safe operation of\nmedical devices, ultimately safeguarding patient health and trust in IoT-driven\nhealthcare solutions."
    },
    {
        "date": "2025-11",
        "title": "Security and Privacy Management of IoT Using Quantum Computing",
        "author": "Jaydip Sen",
        "link": "http://arxiv.org/abs/2511.03538v1",
        "abstract": "The convergence of the Internet of Things (IoT) and quantum computing is\nredefining the security paradigm of interconnected digital systems. Classical\ncryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and\nAdvanced Encryption Standard (AES) have long provided the foundation for\nsecuring IoT communication. However, the emergence of quantum algorithms such\nas Shor's and Grover's threatens to render these techniques vulnerable,\nnecessitating the development of quantum-resilient alternatives. This chapter\nexamines the implications of quantum computing for IoT security and explores\nstrategies for building cryptographically robust systems in the post-quantum\nera. It presents an overview of Post-Quantum Cryptographic (PQC) families,\nincluding lattice-based, code-based, hash-based, and multivariate approaches,\nanalyzing their potential for deployment in resource-constrained IoT\nenvironments. In addition, quantum-based methods such as Quantum Key\nDistribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed\nfor their ability to enhance confidentiality and privacy through physics-based\nsecurity guarantees. The chapter also highlights issues of privacy management,\nregulatory compliance, and standardization, emphasizing the need for\ncollaborative efforts across academia, industry, and governance. Overall, it\nprovides a comprehensive perspective on security IoT ecosystems against quantum\nthreats and ensures resilience in the next generation of intelligent networks."
    },
    {
        "date": "2025-11",
        "title": "Byzantine-Robust Federated Learning with Learnable Aggregation Weights",
        "author": "Javad Parsa, Amir Hossein Daghestani, Andr\u00e9 M. H. Teixeira, and Mikael Johansson",
        "link": "http://arxiv.org/abs/2511.03529v1",
        "abstract": "Federated Learning (FL) enables clients to collaboratively train a global\nmodel without sharing their private data. However, the presence of malicious\n(Byzantine) clients poses significant challenges to the robustness of FL,\nparticularly when data distributions across clients are heterogeneous. In this\npaper, we propose a novel Byzantine-robust FL optimization problem that\nincorporates adaptive weighting into the aggregation process. Unlike\nconventional approaches, our formulation treats aggregation weights as\nlearnable parameters, jointly optimizing them alongside the global model\nparameters. To solve this optimization problem, we develop an alternating\nminimization algorithm with strong convergence guarantees under adversarial\nattack. We analyze the Byzantine resilience of the proposed objective. We\nevaluate the performance of our algorithm against state-of-the-art\nByzantine-robust FL approaches across various datasets and attack scenarios.\nExperimental results demonstrate that our method consistently outperforms\nexisting approaches, particularly in settings with highly heterogeneous data\nand a large proportion of malicious clients."
    },
    {
        "date": "2025-11",
        "title": "Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort",
        "author": "Nikolai Herrmann, Marcella C. Zijta, Stefan Klein, R\u00e9gine P. M. Steegers-Theunissen, Rene M. H. Wijnen, Bernadette S. de Bakker, Melek Rousian, and Wietske A. P. Bastiaansen",
        "link": "http://arxiv.org/abs/2511.03416v1",
        "abstract": "Standardized alignment of the embryo in three-dimensional (3D) ultrasound\nimages aids prenatal growth monitoring by facilitating standard plane\ndetection, improving visualization of landmarks and accentuating differences\nbetween different scans. In this work, we propose an automated method for\nstandardizing this alignment. Given a segmentation mask of the embryo,\nPrincipal Component Analysis (PCA) is applied to the mask extracting the\nembryo's principal axes, from which four candidate orientations are derived.\nThe candidate in standard orientation is selected using one of three\nstrategies: a heuristic based on Pearson's correlation assessing shape, image\nmatching to an atlas through normalized cross-correlation, and a Random Forest\nclassifier. We tested our method on 2166 images longitudinally acquired 3D\nultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional\nCohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,\nPCA correctly extracted the principal axes of the embryo. The correct candidate\nwas selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,\n95.8%, and 98.4% of images, respectively. A Majority Vote of these selection\nmethods resulted in an accuracy of 98.5%. The high accuracy of this pipeline\nenables consistent embryonic alignment in the first trimester, enabling\nscalable analysis in both clinical and research settings. The code is publicly\navailable at:\nhttps://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment."
    },
    {
        "date": "2025-11",
        "title": "Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs",
        "author": "Yize Liu, Yunyun Hou, and Aina Sui",
        "link": "http://arxiv.org/abs/2511.03271v1",
        "abstract": "Large Language Models (LLMs) have been widely deployed across various\napplications, yet their potential security and ethical risks have raised\nincreasing concerns. Existing research employs red teaming evaluations,\nutilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.\nHowever, these approaches often lack exploration of successful dialogue\ntrajectories within the attack space, and they tend to overlook the\nconsiderable overhead associated with the attack process. To address these\nlimitations, this paper first introduces a theoretical model based on\ndynamically weighted graph topology, abstracting the multi-turn attack process\nas a path planning problem. Based on this framework, we propose ABC, an\nenhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a\ncollaborative search mechanism with employed, onlooker, and scout bees. This\nalgorithm significantly improves the efficiency of optimal attack path search\nwhile substantially reducing the average number of queries required. Empirical\nevaluations on three open-source and two proprietary language models\ndemonstrate the effectiveness of our approach, achieving attack success rates\nabove 90\\% across the board, with a peak of 98\\% on GPT-3.5-Turbo, and\noutperforming existing baselines. Furthermore, it achieves comparable success\nwith only 26 queries on average, significantly reducing red teaming overhead\nand highlighting its superior efficiency."
    },
    {
        "date": "2025-11",
        "title": "RKUM: An R Package for Robust Kernel Unsupervised Methods",
        "author": "Md Ashad Alam",
        "link": "http://arxiv.org/abs/2511.03216v1",
        "abstract": "RKUM is an R package developed for implementing robust kernel-based\nunsupervised methods. It provides functions for estimating the robust kernel\ncovariance operator (CO) and the robust kernel cross-covariance operator (CCO)\nusing generalized loss functions instead of the conventional quadratic loss.\nThese operators form the foundation of robust kernel learning and enable\nreliable analysis under contaminated or noisy data conditions. The package\nincludes implementations of robust kernel canonical correlation analysis\n(Kernel CCA), as well as the influence function (IF) for both standard and\nmultiple kernel CCA frameworks. The influence function quantifies sensitivity\nand helps detect influential or outlying observations across two-view and\nmulti-view datasets. Experiments using synthesized two-view and multi-view data\ndemonstrate that the IF of the standard kernel CCA effectively identifies\noutliers, while the robust kernel methods implemented in RKUM exhibit reduced\nsensitivity to contamination. Overall, RKUM provides an efficient and\nextensible platform for robust kernel-based analysis in high-dimensional data\napplications."
    },
    {
        "date": "2025-11",
        "title": "Bayesian Advantage of Re-Identification Attack in the Shuffle Model",
        "author": "Pengcheng Su, Haibo Cheng, and Ping Wang",
        "link": "http://arxiv.org/abs/2511.03213v1",
        "abstract": "The shuffle model, which anonymizes data by randomly permuting user messages,\nhas been widely adopted in both cryptography and differential privacy. In this\nwork, we present the first systematic study of the Bayesian advantage in\nre-identifying a user's message under the shuffle model. We begin with a basic\nsetting: one sample is drawn from a distribution $P$, and $n - 1$ samples are\ndrawn from a distribution $Q$, after which all $n$ samples are randomly\nshuffled. We define $\\beta_n(P, Q)$ as the success probability of a\nBayes-optimal adversary in identifying the sample from $P$, and define the\nadditive and multiplicative Bayesian advantages as $\\mathsf{Adv}_n^{+}(P, Q) =\n\\beta_n(P,Q) - \\frac{1}{n}$ and $\\mathsf{Adv}_n^{\\times}(P, Q) = n \\cdot\n\\beta_n(P,Q)$, respectively. We derive exact analytical expressions and\nasymptotic characterizations of $\\beta_n(P, Q)$, along with evaluations in\nseveral representative scenarios. Furthermore, we establish (nearly) tight\nmutual bounds between the additive Bayesian advantage and the total variation\ndistance. Finally, we extend our analysis beyond the basic setting and present,\nfor the first time, an upper bound on the success probability of Bayesian\nattacks in shuffle differential privacy. Specifically, when the outputs of $n$\nusers -- each processed through an $\\varepsilon$-differentially private local\nrandomizer -- are shuffled, the probability that an attacker successfully\nre-identifies any target user's message is at most $e^{\\varepsilon}/n$."
    },
    {
        "date": "2025-11",
        "title": "Understanding Robustness of Model Editing in Code LLMs: An Empirical Study",
        "author": "Vinaik Chhetri, A. B Siddique, and Umar Farooq",
        "link": "http://arxiv.org/abs/2511.03182v1",
        "abstract": "Large language models (LLMs) are increasingly used in software development.\nHowever, while LLMs remain static after pretraining, programming languages and\nAPIs continue to evolve, leading to the generation of deprecated or\nincompatible code that undermines reliability. Retraining LLMs from scratch to\nreflect such changes is computationally expensive, making model editing a\npromising lightweight alternative that updates only a small subset of\nparameters. Despite its potential, it remains unclear whether model editing\nyields genuine syntactic and semantic adaptations or merely superficial fixes.\nIn this work, we present a systematic study of five state-of-the-art model\nediting methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We\napply these methods to three leading open-source code LLMs, CodeLlama,\nCodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.\nOur evaluation covers both instant and sequential editing settings, using three\ndisjoint evaluation sets designed to assess reliability, generalization, and\nspecificity. We measure model correctness at three levels: successful\ncompilation, partial test case pass, and full test pass. Our findings show that\ninstant edits consistently degrade model performance, with syntactic validity\ndropping by up to 86 percentage points and functional correctness declining by\n45 points even in the best-performing setting. Sequential edits further amplify\nthis degradation, and in some cases, model performance collapses entirely.\nAcross all models, most passing generations relied on workarounds rather than\ncorrectly adopting the intended changes, while faulty adoptions that result in\ntest failures or compilation errors were significantly more frequent. Correct\nadoptions, where the model correctly integrates the intended change, occurred\nin only about 6% of cases."
    },
    {
        "date": "2025-11",
        "title": "From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation",
        "author": "Najrin Sultana, Md Rafi Ur Rashid, Kang Gu, and Shagufta Mehnaz",
        "link": "http://arxiv.org/abs/2511.03128v1",
        "abstract": "LLMs can provide substantial zero-shot performance on diverse tasks using a\nsimple task prompt, eliminating the need for training or fine-tuning. However,\nwhen applying these models to sensitive tasks, it is crucial to thoroughly\nassess their robustness against adversarial inputs. In this work, we introduce\nStatic Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack\nframeworks designed to systematically generate dynamic and adaptive adversarial\nexamples by leveraging the understanding of the LLMs. We produce subtle and\nnatural-looking adversarial inputs that preserve semantic similarity to the\noriginal text while effectively deceiving the target LLM. By utilizing an\nautomated, LLM-driven pipeline, we eliminate the dependence on external\nheuristics. Our attacks evolve with the advancements in LLMs and demonstrate\nstrong transferability across models unknown to the attacker. Overall, this\nwork provides a systematic approach for the self-assessment of an LLM's\nrobustness. We release our code and data at\nhttps://github.com/Shukti042/AdversarialExample."
    },
    {
        "date": "2025-11",
        "title": "Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach",
        "author": "Fatemeh Ghaffari, Siddarth Sitaraman, Xutong Liu, Xuchuang Wang, and Mohammad Hajiesmaili",
        "link": "http://arxiv.org/abs/2511.03074v1",
        "abstract": "Online learning to rank (OLTR) studies how to recommend a short ranked list\nof items from a large pool and improves future rankings based on user clicks.\nThis setting is commonly modeled as cascading bandits, where the objective is\nto maximize the likelihood that the user clicks on at least one of the\npresented items across as many timesteps as possible. However, such systems are\nvulnerable to click fraud and other manipulations (i.e., corruption), where\nbots or paid click farms inject corrupted feedback that misleads the learning\nprocess and degrades user experience. In this paper, we propose MSUCB, a robust\nalgorithm that incorporates a novel mean-of-medians estimator, which to our\nknowledge is applied to bandits with corruption setting for the first time.\nThis estimator behaves like a standard mean in the absence of corruption, so no\ncost is paid for robustness. Under corruption, the median step filters out\noutliers and corrupted samples, keeping the estimate close to its true value.\nUpdating this estimate at every round further accelerates empirical convergence\nin experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence\nof corruption and degrades gracefully under corruptions, with regret increasing\nonly by an additive term tied to the total corruption. Comprehensive and\nextensive experiments on real-world datasets further demonstrate that our\napproach consistently outperforms prior methods while maintaining strong\nrobustness. In particular, it achieves a \\(97.35\\%\\) and a \\(91.60\\%\\) regret\nimprovement over two state-of-the-art methods."
    },
    {
        "date": "2025-11",
        "title": "Lightweight Session-Key Rekeying Framework for Secure IoT-Edge Communication",
        "author": "Haranath Rakshit, Rajkumar Bhandari, and Subhasis Banerjee",
        "link": "http://arxiv.org/abs/2511.02924v1",
        "abstract": "The proliferation of Internet of Things (IoT) networks demands security\nmechanisms that protect constrained devices without the computational cost of\npublic-key cryptography. Conventional Pre-Shared Key (PSK) encryption, while\nefficient, remains vulnerable due to static key reuse, replay attacks, and the\nlack of forward secrecy. This paper presents the Dynamic Session Enhanced Key\nProtocol (DSEKP) - a lightweight session-key rekeying framework, a fully\nsymmetric extension to PSK that derives per-session AES-GCM keys using the\nHMAC-based Key Derivation Function (HKDF-SHA256) and authenticates session\nestablishment through an HMAC proof in a single init-ack exchange. DSEKP was\nimplemented on an ESP32 IoT sensor node and a Raspberry Pi 5 edge server\ncommunicating through a Mosquitto MQTT broker, and benchmarked against a static\nPSK baseline over more than 6,500 encrypted packets per configuration. The\nresults demonstrate nearly identical throughput and reliability, with moderate\noverhead - mean latency increased by 27% and payload size by 10% - while\ndelivering per-session forward secrecy and built-in replay protection. These\nfindings confirm that dynamic symmetric rekeying can substantially strengthen\nIoT-Edge links with minimal computational and bandwidth cost, offering a\npractical migration path from static PSK to session-aware, scalable, and\nreproducible IoT security."
    },
    {
        "date": "2025-11",
        "title": "Optimizing AI Agent Attacks With Synthetic Data",
        "author": "Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, and Joe Benton",
        "link": "http://arxiv.org/abs/2511.02823v1",
        "abstract": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold."
    },
    {
        "date": "2025-11",
        "title": "Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning",
        "author": "Nicolas Riccieri Gardin Assumpcao, and Leandro Villas",
        "link": "http://arxiv.org/abs/2511.02797v1",
        "abstract": "Federated Learning (FL) is a distributed training paradigm wherein\nparticipants collaborate to build a global model while ensuring the privacy of\nthe involved data, which remains stored on participant devices. However,\nproposals aiming to ensure such privacy also make it challenging to protect\nagainst potential attackers seeking to compromise the training outcome. In this\ncontext, we present Fast, Private, and Protected (FPP), a novel approach that\naims to safeguard federated training while enabling secure aggregation to\npreserve data privacy. This is accomplished by evaluating rounds using\nparticipants' assessments and enabling training recovery after an attack. FPP\nalso employs a reputation-based mechanism to mitigate the participation of\nattackers. We created a dockerized environment to validate the performance of\nFPP compared to other approaches in the literature (FedAvg, Power-of-Choice,\nand aggregation via Trimmed Mean and Median). Our experiments demonstrate that\nFPP achieves a rapid convergence rate and can converge even in the presence of\nmalicious participants performing model poisoning attacks."
    },
    {
        "date": "2025-11",
        "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
        "author": "W. K. M Mithsara, Ning Yang, Ahmed Imteaj, Hussein Zangoti, and Abdur R. Shahid",
        "link": "http://arxiv.org/abs/2511.02894v1",
        "abstract": "The widespread integration of wearable sensing devices in Internet of Things\n(IoT) ecosystems, particularly in healthcare, smart homes, and industrial\napplications, has required robust human activity recognition (HAR) techniques\nto improve functionality and user experience. Although machine learning models\nhave advanced HAR, they are increasingly susceptible to data poisoning attacks\nthat compromise the data integrity and reliability of these systems.\nConventional approaches to defending against such attacks often require\nextensive task-specific training with large, labeled datasets, which limits\nadaptability in dynamic IoT environments. This work proposes a novel framework\nthat uses large language models (LLMs) to perform poisoning detection and\nsanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot\nlearning paradigms. Our approach incorporates \\textit{role play} prompting,\nwhereby the LLM assumes the role of expert to contextualize and evaluate sensor\nanomalies, and \\textit{think step-by-step} reasoning, guiding the LLM to infer\npoisoning indicators in the raw sensor data and plausible clean alternatives.\nThese strategies minimize reliance on curation of extensive datasets and enable\nrobust, adaptable defense mechanisms in real-time. We perform an extensive\nevaluation of the framework, quantifying detection accuracy, sanitization\nquality, latency, and communication cost, thus demonstrating the practicality\nand effectiveness of LLMs in improving the security and reliability of wearable\nIoT systems."
    },
    {
        "date": "2025-11",
        "title": "RL-Aided Cognitive ISAC: Robust Detection and Sensing-Communication Trade-offs",
        "author": "Adam Umra, Aya M. Ahmed, and Aydin Sezgin",
        "link": "http://arxiv.org/abs/2511.02672v1",
        "abstract": "This paper proposes a reinforcement learning (RL)-aided cognitive framework\nfor massive MIMO-based integrated sensing and communication (ISAC) systems\nemploying a uniform planar array (UPA). The focus is on enhancing radar sensing\nperformance in environments with unknown and dynamic disturbance\ncharacteristics. A Wald-type detector is employed for robust target detection\nunder non-Gaussian clutter, while a SARSA-based RL algorithm enables adaptive\nestimation of target positions without prior environmental knowledge. Based on\nthe RL-derived sensing information, a joint waveform optimization strategy is\nformulated to balance radar sensing accuracy and downlink communication\nthroughput. The resulting design provides an adaptive trade-off between\ndetection performance and achievable sum rate through an analytically derived\nclosed-form solution. Monte Carlo simulations demonstrate that the proposed\ncognitive ISAC framework achieves significantly improved detection probability\ncompared to orthogonal and non-learning adaptive baselines, while maintaining\ncompetitive communication performance. These results underline the potential of\nRL-assisted sensing for robust and spectrum-efficient ISAC in next-generation\nwireless networks."
    },
    {
        "date": "2025-11",
        "title": "Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries",
        "author": "Lihan Xu, Yanjie Dong, Gang Wang, Runhao Zeng, Xiaoyi Fan, and Xiping Hu",
        "link": "http://arxiv.org/abs/2511.02657v1",
        "abstract": "We investigate robust federated learning, where a group of workers\ncollaboratively train a shared model under the orchestration of a central\nserver in the presence of Byzantine adversaries capable of arbitrary and\npotentially malicious behaviors. To simultaneously enhance communication\nefficiency and robustness against such adversaries, we propose a\nByzantine-resilient Nesterov-Accelerated Federated Learning (Byrd-NAFL)\nalgorithm. Byrd-NAFL seamlessly integrates Nesterov's momentum into the\nfederated learning process alongside Byzantine-resilient aggregation rules to\nachieve fast and safeguarding convergence against gradient corruption. We\nestablish a finite-time convergence guarantee for Byrd-NAFL under non-convex\nand smooth loss functions with relaxed assumption on the aggregated gradients.\nExtensive numerical experiments validate the effectiveness of Byrd-NAFL and\ndemonstrate the superiority over existing benchmarks in terms of convergence\nspeed, accuracy, and resilience to diverse Byzantine attack strategies."
    },
    {
        "date": "2025-11",
        "title": "Robust Face Liveness Detection for Biometric Authentication using Single Image",
        "author": "Poulami Raha, and Yeongnam Chae",
        "link": "http://arxiv.org/abs/2511.02645v1",
        "abstract": "Biometric technologies are widely adopted in security, legal, and financial\nsystems. Face recognition can authenticate a person based on the unique facial\nfeatures such as shape and texture. However, recent works have demonstrated the\nvulnerability of Face Recognition Systems (FRS) towards presentation attacks.\nUsing spoofing (aka.,presentation attacks), a malicious actor can get\nillegitimate access to secure systems. This paper proposes a novel light-weight\nCNN framework to identify print/display, video and wrap attacks. The proposed\nrobust architecture provides seamless liveness detection ensuring faster\nbiometric authentication (1-2 seconds on CPU). Further, this also presents a\nnewly created 2D spoof attack dataset consisting of more than 500 videos\ncollected from 60 subjects. To validate the effectiveness of this architecture,\nwe provide a demonstration video depicting print/display, video and wrap attack\ndetection approaches. The demo can be viewed in the following link:\nhttps://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa"
    },
    {
        "date": "2025-11",
        "title": "A Non-Adversarial Approach to Idempotent Generative Modelling",
        "author": "Mohammed Al-Jaff, Giovanni Luca Marchetti, Michael C Welle, Jens Lundell, Mats G. Gustafsson, Gustav Eje Henter, Hossein Azizpour, and Danica Kragic",
        "link": "http://arxiv.org/abs/2511.02614v1",
        "abstract": "Idempotent Generative Networks (IGNs) are deep generative models that also\nfunction as local data manifold projectors, mapping arbitrary inputs back onto\nthe manifold. They are trained to act as identity operators on the data and as\nidempotent operators off the data manifold. However, IGNs suffer from mode\ncollapse, mode dropping, and training instability due to their objectives,\nwhich contain adversarial components and can cause the model to cover the data\nmanifold only partially -- an issue shared with generative adversarial\nnetworks. We introduce Non-Adversarial Idempotent Generative Networks (NAIGNs)\nto address these issues. Our loss function combines reconstruction with the\nnon-adversarial generative objective of Implicit Maximum Likelihood Estimation\n(IMLE). This improves on IGN's ability to restore corrupted data and generate\nnew samples that closely match the data distribution. We moreover demonstrate\nthat NAIGNs implicitly learn the distance field to the data manifold, as well\nas an energy-based model."
    },
    {
        "date": "2025-11",
        "title": "Trustworthy Quantum Machine Learning: A Roadmap for Reliability, Robustness, and Security in the NISQ Era",
        "author": "Ferhat Ozgur Catak, Jungwon Seo, and Umit Cali",
        "link": "http://arxiv.org/abs/2511.02602v1",
        "abstract": "Quantum machine learning (QML) is a promising paradigm for tackling\ncomputational problems that challenge classical AI. Yet, the inherent\nprobabilistic behavior of quantum mechanics, device noise in NISQ hardware, and\nhybrid quantum-classical execution pipelines introduce new risks that prevent\nreliable deployment of QML in real-world, safety-critical settings. This\nresearch offers a broad roadmap for Trustworthy Quantum Machine Learning\n(TQML), integrating three foundational pillars of reliability: (i) uncertainty\nquantification for calibrated and risk-aware decision making, (ii) adversarial\nrobustness against classical and quantum-native threat models, and (iii)\nprivacy preservation in distributed and delegated quantum learning scenarios.\nWe formalize quantum-specific trust metrics grounded in quantum information\ntheory, including a variance-based decomposition of predictive uncertainty,\ntrace-distance-bounded robustness, and differential privacy for hybrid learning\nchannels. To demonstrate feasibility on current NISQ devices, we validate a\nunified trust assessment pipeline on parameterized quantum classifiers,\nuncovering correlations between uncertainty and prediction risk, an asymmetry\nin attack vulnerability between classical and quantum state perturbations, and\nprivacy-utility trade-offs driven by shot noise and quantum channel noise. This\nroadmap seeks to define trustworthiness as a first-class design objective for\nquantum AI."
    },
    {
        "date": "2025-11",
        "title": "On The Dangers of Poisoned LLMs In Security Automation",
        "author": "Patrick Karlsen, and Even Eilertsen",
        "link": "http://arxiv.org/abs/2511.02600v1",
        "abstract": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications."
    },
    {
        "date": "2025-11",
        "title": "Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification",
        "author": "Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, and Hugo Proenca",
        "link": "http://arxiv.org/abs/2511.02564v1",
        "abstract": "Video-based person re-identification (ReID) in cross-view domains (for\nexample, aerial-ground surveillance) remains an open problem because of extreme\nviewpoint shifts, scale disparities, and temporal inconsistencies. To address\nthese challenges, we propose MTF-CVReID, a parameter-efficient framework that\nintroduces seven complementary modules over a ViT-B/16 backbone. Specifically,\nwe include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and\nview biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale\nstabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to\nreinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for\nmotion-aware short-term temporal encoding; (5) Inter-View Feature Alignment\n(IVFA) for perspective-invariant representation alignment; (6) Hierarchical\nTemporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;\nand (7) Multi-View Identity Consistency Learning (MVICL) that enforces\ncross-view identity coherence using a contrastive learning paradigm. Despite\nadding only about 2 million parameters and 0.7 GFLOPs over the baseline,\nMTF-CVReID maintains real-time efficiency (189 FPS) and achieves\nstate-of-the-art performance on the AG-VPReID benchmark across all altitude\nlevels, with strong cross-dataset generalization to G2A-VReID and MARS\ndatasets. These results show that carefully designed adapter-based modules can\nsubstantially enhance cross-view robustness and temporal consistency without\ncompromising computational efficiency. The source code is available at\nhttps://github.com/MdRashidunnabi/MTF-CVReID"
    },
    {
        "date": "2025-11",
        "title": "AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models",
        "author": "Aashray Reddy, Andrew Zagula, and Nicholas Saban",
        "link": "http://arxiv.org/abs/2511.02376v1",
        "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where\nadversarial prompts elicit harmful outputs, yet most evaluations focus on\nsingle-turn interactions while real-world attacks unfold through adaptive\nmulti-turn conversations. We present AutoAdv, a training-free framework for\nautomated multi-turn jailbreaking that achieves up to 95% attack success rate\non Llama-3.1-8B within six turns a 24 percent improvement over single turn\nbaselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern\nmanager that learns from successful attacks to enhance future prompts, a\ntemperature manager that dynamically adjusts sampling parameters based on\nfailure modes, and a two-phase rewriting strategy that disguises harmful\nrequests then iteratively refines them. Extensive evaluation across commercial\nand open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent\nvulnerabilities in current safety mechanisms, with multi-turn attacks\nconsistently outperforming single-turn approaches. These findings demonstrate\nthat alignment strategies optimized for single-turn interactions fail to\nmaintain robustness across extended conversations, highlighting an urgent need\nfor multi-turn-aware defenses."
    },
    {
        "date": "2025-11",
        "title": "Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice",
        "author": "Gautier-Edouard Filardo, and Thibaut Heckmann",
        "link": "http://arxiv.org/abs/2511.02365v1",
        "abstract": "This paper presents a novel framework for enhancing the quantum resistance of\nNTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal\nbounds on sampling efficiency and provide security reductions to lattice\nproblems, bridging theoretical guarantees with practical implementations. Key\ncontributions include: a new methodology for exploring private key\nvulnerabilities while maintaining quantum resistance, provable mixing time\nbounds for high-dimensional lattices, and concrete metrics linking MCMC\nparameters to lattice hardness assumptions. Numerical experiments validate our\napproach, demonstrating improved security guarantees and computational\nefficiency. These findings advance the theoretical understanding and practical\nadoption of NTRU- Encrypt in the post-quantum era."
    },
    {
        "date": "2025-11",
        "title": "An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks",
        "author": "Xu Liu, Yan Chen, Kan Ling, Yichi Zhu, Hengrun Zhang, Guisheng Fan, and Huiqun Yu",
        "link": "http://arxiv.org/abs/2511.02356v1",
        "abstract": "The widespread deployment of Large Language Models (LLMs) as public-facing\nweb services and APIs has made their security a core concern for the web\necosystem. Jailbreak attacks, as one of the significant threats to LLMs, have\nrecently attracted extensive research. In this paper, we reveal a jailbreak\nstrategy which can effectively evade current defense strategies. It can extract\nvaluable information from failed or partially successful attack attempts and\ncontains self-evolution from attack interactions, resulting in sufficient\nstrategy diversity and adaptability. Inspired by continuous learning and\nmodular design principles, we propose ASTRA, a jailbreak framework that\nautonomously discovers, retrieves, and evolves attack strategies to achieve\nmore efficient and adaptive attacks. To enable this autonomous evolution, we\ndesign a closed-loop \"attack-evaluate-distill-reuse\" core mechanism that not\nonly generates attack prompts but also automatically distills and generalizes\nreusable attack strategies from every interaction. To systematically accumulate\nand apply this attack knowledge, we introduce a three-tier strategy library\nthat categorizes strategies into Effective, Promising, and Ineffective based on\ntheir performance scores. The strategy library not only provides precise\nguidance for attack generation but also possesses exceptional extensibility and\ntransferability. We conduct extensive experiments under a black-box setting,\nand the results show that ASTRA achieves an average Attack Success Rate (ASR)\nof 82.7%, significantly outperforming baselines."
    },
    {
        "date": "2025-11",
        "title": "RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains",
        "author": "Tianle Pu, Zijie Geng, Haoyang Liu, Shixuan Liu, Jie Wang, Li Zeng, Chao Chen, and Changjun Fan",
        "link": "http://arxiv.org/abs/2511.02331v1",
        "abstract": "Mixed-Integer Linear Programming (MILP) is a fundamental and powerful\nframework for modeling complex optimization problems across diverse domains.\nRecently, learning-based methods have shown great promise in accelerating MILP\nsolvers by predicting high-quality solutions. However, most existing approaches\nare developed and evaluated in single-domain settings, limiting their ability\nto generalize to unseen problem distributions. This limitation poses a major\nobstacle to building scalable and general-purpose learning-based solvers. To\naddress this challenge, we introduce RoME, a domain-Robust Mixture-of-Experts\nframework for predicting MILP solutions across domains. RoME dynamically routes\nproblem instances to specialized experts based on learned task embeddings. The\nmodel is trained using a two-level distributionally robust optimization\nstrategy: inter-domain to mitigate global shifts across domains, and\nintra-domain to enhance local robustness by introducing perturbations on task\nembeddings. We reveal that cross-domain training not only enhances the model's\ngeneralization capability to unseen domains but also improves performance\nwithin each individual domain by encouraging the model to capture more general\nintrinsic combinatorial patterns. Specifically, a single RoME model trained on\nthree domains achieves an average improvement of 67.7% then evaluated on five\ndiverse domains. We further test the pretrained model on MIPLIB in a zero-shot\nsetting, demonstrating its ability to deliver measurable performance gains on\nchallenging real-world instances where existing learning-based approaches often\nstruggle to generalize."
    },
    {
        "date": "2025-11",
        "title": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization",
        "author": "Shaohan Li, Yunpeng Shi, and Gilad Lerman",
        "link": "http://arxiv.org/abs/2511.02329v1",
        "abstract": "We introduce Cycle-Sync, a robust and global framework for estimating camera\nposes (both rotations and locations). Our core innovation is a location solver\nthat adapts message-passing least squares (MPLS) -- originally developed for\ngroup synchronization -- to camera location estimation. We modify MPLS to\nemphasize cycle-consistent information, redefine cycle consistencies using\nestimated distances from previous iterations, and incorporate a Welsch-type\nrobust loss. We establish the strongest known deterministic exact-recovery\nguarantee for camera location estimation, showing that cycle consistency alone\n-- without access to inter-camera distances -- suffices to achieve the lowest\nsample complexity currently known. To further enhance robustness, we introduce\na plug-and-play outlier rejection module inspired by robust subspace recovery,\nand we fully integrate cycle consistency into MPLS for rotation\nsynchronization. Our global approach avoids the need for bundle adjustment.\nExperiments on synthetic and real datasets show that Cycle-Sync consistently\noutperforms leading pose estimators, including full structure-from-motion\npipelines with bundle adjustment."
    },
    {
        "date": "2025-11",
        "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
        "author": "Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen, Alvin Cheung, Joseph Gonzalez, and Ion Stoica",
        "link": "http://arxiv.org/abs/2511.02230v1",
        "abstract": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"
    },
    {
        "date": "2025-11",
        "title": "OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning",
        "author": "Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, and David Keetae Park",
        "link": "http://arxiv.org/abs/2511.02205v1",
        "abstract": "Multimodal spatiotemporal learning on real-world experimental data is\nconstrained by two challenges: within-modality measurements are sparse,\nirregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of\navailable modalities varies across space and time, shrinking the usable record\nunless models can adapt to arbitrary subsets at train and test time. We propose\nOmniField, a continuity-aware framework that learns a continuous neural field\nconditioned on available modalities and iteratively fuses cross-modal context.\nA multimodal crosstalk block architecture paired with iterative cross-modal\nrefinement aligns signals prior to the decoder, enabling unified\nreconstruction, interpolation, forecasting, and cross-modal prediction without\ngridding or surrogate preprocessing. Extensive evaluations show that OmniField\nconsistently outperforms eight strong multimodal spatiotemporal baselines.\nUnder heavy simulated sensor noise, performance remains close to clean-input\nlevels, highlighting robustness to corrupted measurements."
    },
    {
        "date": "2025-11",
        "title": "PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks",
        "author": "Fuyi Wang, Zekai Chen, Mingyuan Fan, Jianying Zhou, Lei Pan, and Leo Yu Zhang",
        "link": "http://arxiv.org/abs/2511.02185v1",
        "abstract": "Graph neural networks (GNNs) are powerful tools for analyzing and learning\nfrom graph-structured (GS) data, facilitating a wide range of services.\nDeploying such services in privacy-critical cloud environments necessitates the\ndevelopment of secure inference (SI) protocols that safeguard sensitive GS\ndata. However, existing SI solutions largely focus on convolutional models for\nimage and text data, leaving the challenge of securing GNNs and GS data\nrelatively underexplored. In this work, we design, implement, and evaluate\n$\\sysname$, a lightweight cryptographic scheme for graph-centric inference in\nthe cloud. By hybridizing additive and function secret sharings within secure\ntwo-party computation (2PC), $\\sysname$ is carefully designed based on a series\nof novel 2PC interactive protocols that achieve $1.5\\times \\sim 1.7\\times$\nspeedups for linear layers and $2\\times \\sim 15\\times$ for non-linear layers\nover state-of-the-art (SotA) solutions. A thorough theoretical analysis is\nprovided to prove $\\sysname$'s correctness, security, and lightweight nature.\nExtensive experiments across four datasets demonstrate $\\sysname$'s superior\nefficiency with $1.3\\times \\sim 4.7\\times$ faster secure predictions while\nmaintaining accuracy comparable to plaintext graph property inference."
    },
    {
        "date": "2025-11",
        "title": "Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape",
        "author": "Xinyuan Song, Jiaye Teng, and Ziye Ma",
        "link": "http://arxiv.org/abs/2511.02122v1",
        "abstract": "In this paper we study how the choice of loss functions of non-convex\noptimization problems affects their robustness and optimization landscape,\nthrough the study of noisy matrix sensing. In traditional regression tasks,\nmean squared error (MSE) loss is a common choice, but it can be unreliable for\nnon-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust\nloss based on nonparametric regression, which uses a kernel-based estimate of\nthe residual density and maximizes the estimated log-likelihood. This robust\nformulation coincides with the MSE loss under Gaussian errors but remains\nstable under more general settings. We further examine how this robust loss\nreshapes the optimization landscape by analyzing the upper-bound of restricted\nisometry property (RIP) constants for spurious local minima to disappear.\nThrough theoretical and empirical analysis, we show that this new loss excels\nat handling large noise and remains robust across diverse noise distributions.\nThis work offers initial insights into enhancing the robustness of machine\nlearning tasks through simply changing the loss, guided by an intuitive and\nbroadly applicable analytical framework."
    },
    {
        "date": "2025-11",
        "title": "The SDSC Satellite Reverse Proxy Service for Launching Secure Jupyter Notebooks on High-Performance Computing Systems",
        "author": "Mary P Thomas, Martin Kandes, James McDougall, Dmitry Mishin, Scott Sakai, Subhashini Sivagnanam, and Mahidhar Tatineni",
        "link": "http://arxiv.org/abs/2511.02116v2",
        "abstract": "Using Jupyter notebooks in an HPC environment exposes a system and its users\nto several security risks. The Satellite Proxy Service, developed at SDSC,\naddresses many of these security concerns by providing Jupyter Notebook servers\nwith a token-authenticated HTTPS reverse proxy through which end users can\naccess their notebooks securely with a single URL copied and pasted into their\nweb browser."
    },
    {
        "date": "2025-11",
        "title": "Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets",
        "author": "Sameer Wagh, Kenneth Stibler, Shubham Gupta, Lacey Strahm, Irina Bejan, Jiahao Chen, Dave Buckley, Ruchi Bhatia, Jack Bandy, Aayush Agarwal, and Andrew Trask",
        "link": "http://arxiv.org/abs/2511.02055v1",
        "abstract": "The modern AI data economy centralizes power, limits innovation, and\nmisallocates value by extracting data without control, privacy, or fair\ncompensation. We introduce Private Map-Secure Reduce (PMSR), a network-native\nparadigm that transforms data economics from extractive to participatory\nthrough cryptographically enforced markets. Extending MapReduce to\ndecentralized settings, PMSR enables computation to move to the data, ensuring\nverifiable privacy, efficient price discovery, and incentive alignment.\nDemonstrations include large-scale recommender audits, privacy-preserving LLM\nensembling (87.5\\% MMLU accuracy across six models), and distributed analytics\nover hundreds of nodes. PMSR establishes a scalable, equitable, and\nprivacy-guaranteed foundation for the next generation of AI data markets."
    },
    {
        "date": "2025-11",
        "title": "Towards Robust Mathematical Reasoning",
        "author": "Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung",
        "link": "http://arxiv.org/abs/2511.01846v1",
        "abstract": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/."
    },
    {
        "date": "2025-11",
        "title": "RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks",
        "author": "Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, and Aviral Kumar",
        "link": "http://arxiv.org/abs/2511.01758v1",
        "abstract": "Open-ended generation tasks require outputs to satisfy diverse and often\nimplicit task-specific evaluation rubrics. The sheer number of relevant rubrics\nleads to prohibitively high verification costs and incomplete assessments of a\nresponse, making reinforcement learning (RL) post-training with rubric-based\nrewards difficult to scale. This problem is exacerbated by the fact that often\nthe best way to combine these rubrics into one single reward is also highly\nprompt-specific. We propose Reinforcement Learning with Adversarial Critic\n(RLAC), a post-training approach that addresses these challenges via dynamic\nrubric verification. Our approach employs a large language model (LLM) as a\ncritic that dynamically identifies only the most likely failure modes (e.g., a\nfactual error or unhandled edge case), which are then verified by an external\nvalidator to optimize both generator and critic jointly. By training both the\ngenerator and the critic, this game enhances the critic's error detection and\nthe generator's output quality while reducing required verifications. Our\nexperiments demonstrate that RLAC improves factual accuracy in text generation\nand correctness in code generation, while also outperforming exhaustive\nverification and reward model methods. We show that dynamic critics are more\neffective than fixed critics, showcasing the potential of RLAC for scaling RL\npost-training to free-form generation tasks."
    },
    {
        "date": "2025-11",
        "title": "Scam Shield: Multi-Model Voting and Fine-Tuned LLMs Against Adversarial Attacks",
        "author": "Chen-Wei Chang, Shailik Sarkar, Hossein Salemi, Hyungmin Kim, Shutonu Mitra, Hemant Purohit, Fengxiu Zhang, Michin Hong, Jin-Hee Cho, and Chang-Tien Lu",
        "link": "http://arxiv.org/abs/2511.01746v1",
        "abstract": "Scam detection remains a critical challenge in cybersecurity as adversaries\ncraft messages that evade automated filters. We propose a Hierarchical Scam\nDetection System (HSDS) that combines a lightweight multi-model voting front\nend with a fine-tuned LLaMA 3.1 8B Instruct back end to improve accuracy and\nrobustness against adversarial attacks. An ensemble of four classifiers\nprovides preliminary predictions through majority vote, and ambiguous cases are\nescalated to the fine-tuned model, which is optimized with adversarial training\nto reduce misclassification. Experiments show that this hierarchical design\nboth improves adversarial scam detection and shortens inference time by routing\nmost cases away from the LLM, outperforming traditional machine-learning\nbaselines and proprietary LLM baselines. The findings highlight the\neffectiveness of a hybrid voting mechanism and adversarial fine-tuning in\nfortifying LLMs against evolving scam tactics, enhancing the resilience of\nautomated scam detection systems."
    },
    {
        "date": "2025-11",
        "title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark",
        "author": "Yi Zhang, Zheng Wang, Chen Zhen, Wenjie Ruan, Qing Guo, Siddartha Khastgir, Carsten Maple, and Xingyu Zhao",
        "link": "http://arxiv.org/abs/2511.01724v1",
        "abstract": "Deep learning models are notoriously vulnerable to imperceptible\nperturbations. Most existing research centers on adversarial robustness (AR),\nwhich evaluates models under worst-case scenarios by examining the existence of\ndeterministic adversarial examples (AEs). In contrast, probabilistic robustness\n(PR) adopts a statistical perspective, measuring the probability that\npredictions remain correct under stochastic perturbations. While PR is widely\nregarded as a practical complement to AR, dedicated training methods for\nimproving PR are still relatively underexplored, albeit with emerging progress.\nAmong the few PR-targeted training methods, we identify three limitations: i\nnon-comparable evaluation protocols; ii limited comparisons to strong AT\nbaselines despite anecdotal PR gains from AT; and iii no unified framework to\ncompare the generalization of these methods. Thus, we introduce PRBench, the\nfirst benchmark dedicated to evaluating improvements in PR achieved by\ndifferent robustness training methods. PRBench empirically compares most common\nAT and PR-targeted training methods using a comprehensive set of metrics,\nincluding clean accuracy, PR and AR performance, training efficiency, and\ngeneralization error (GE). We also provide theoretical analysis on the GE of PR\nperformance across different training methods. Main findings revealed by\nPRBench include: AT methods are more versatile than PR-targeted training\nmethods in terms of improving both AR and PR performance across diverse\nhyperparameter settings, while PR-targeted training methods consistently yield\nlower GE and higher clean accuracy. A leaderboard comprising 222 trained models\nacross 7 datasets and 10 model architectures is publicly available at\nhttps://tmpspace.github.io/PRBenchLeaderboard/."
    },
    {
        "date": "2025-11",
        "title": "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems",
        "author": "Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del Rio, Oleksii Sliusarenko, and Xabi Uribe-Etxebarria",
        "link": "http://arxiv.org/abs/2511.01583v1",
        "abstract": "Detecting malware, especially ransomware, is essential to securing today's\ninterconnected ecosystems, including cloud storage, enterprise file-sharing,\nand database services. Training high-performing artificial intelligence (AI)\ndetectors requires diverse datasets, which are often distributed across\nmultiple organizations, making centralization necessary. However, centralized\nlearning is often impractical due to security, privacy regulations, data\nownership issues, and legal barriers to cross-organizational sharing.\nCompounding this challenge, ransomware evolves rapidly, demanding models that\nare both robust and adaptable.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL\nplatform, which enables multiple organizations to collaboratively train a\nransomware detection model while keeping raw data local and secure. This\nparadigm is particularly relevant for cybersecurity companies (including both\nsoftware and hardware vendors) that deploy ransomware detection or firewall\nsystems across millions of endpoints. In such environments, data cannot be\ntransferred outside the customer's device due to strict security, privacy, or\nregulatory constraints. Although FL applies broadly to malware threats, we\nvalidate the approach using the Ransomware Storage Access Patterns (RanSAP)\ndataset.\n  Our experiments demonstrate that FL improves ransomware detection accuracy by\na relative 9% over server-local models and achieves performance comparable to\ncentralized training. These results indicate that FL offers a scalable,\nhigh-performing, and privacy-preserving framework for proactive ransomware\ndetection across organizational and regulatory boundaries."
    },
    {
        "date": "2025-11",
        "title": "Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images",
        "author": "Md Sumon Ali, and Muzammil Behzad",
        "link": "http://arxiv.org/abs/2511.01574v1",
        "abstract": "Compared to traditional methods, Deep Learning (DL) becomes a key technology\nfor computer vision tasks. Synthetic data generation is an interesting use case\nfor DL, especially in the field of medical imaging such as Magnetic Resonance\nImaging (MRI). The need for this task since the original MRI data is limited.\nThe generation of realistic medical images is completely difficult and\nchallenging. Generative Adversarial Networks (GANs) are useful for creating\nsynthetic medical images. In this paper, we propose a DL based methodology for\ncreating synthetic MRI data using the Deep Convolutional Generative Adversarial\nNetwork (DC-GAN) to address the problem of limited data. We also employ a\nConvolutional Neural Network (CNN) classifier to classify the brain tumor using\nsynthetic data and real MRI data. CNN is used to evaluate the quality and\nutility of the synthetic images. The classification result demonstrates\ncomparable performance on real and synthetic images, which validates the\neffectiveness of GAN-generated images for downstream tasks."
    },
    {
        "date": "2025-11",
        "title": "Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing",
        "author": "Jinhua Yin, Peiru Yang, Chen Yang, Huili Wang, Zhiyang Hu, Shangguang Wang, Yongfeng Huang, and Tao Qi",
        "link": "http://arxiv.org/abs/2511.01952v1",
        "abstract": "Large vision-language models (LVLMs) derive their capabilities from extensive\ntraining on vast corpora of visual and textual data. Empowered by large-scale\nparameters, these models often exhibit strong memorization of their training\ndata, rendering them susceptible to membership inference attacks (MIAs).\nExisting MIA methods for LVLMs typically operate under white- or gray-box\nassumptions, by extracting likelihood-based features for the suspected data\nsamples based on the target LVLMs. However, mainstream LVLMs generally only\nexpose generated outputs while concealing internal computational features\nduring inference, limiting the applicability of these methods. In this work, we\npropose the first black-box MIA framework for LVLMs, based on a prior\nknowledge-calibrated memory probing mechanism. The core idea is to assess the\nmodel memorization of the private semantic information embedded within the\nsuspected image data, which is unlikely to be inferred from general world\nknowledge alone. We conducted extensive experiments across four LVLMs and three\ndatasets. Empirical results demonstrate that our method effectively identifies\ntraining data of LVLMs in a purely black-box setting and even achieves\nperformance comparable to gray-box and white-box methods. Further analysis\nreveals the robustness of our method against potential adversarial\nmanipulations, and the effectiveness of the methodology designs. Our code and\ndata are available at https://github.com/spmede/KCMP."
    },
    {
        "date": "2025-11",
        "title": "EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance",
        "author": "Zhiyang Jia, Hongyan Cui, Ge Gao, Bo Li, Minjie Zhang, Zishuo Gao, Huiwen Huang, and Caisheng Zhuo",
        "link": "http://arxiv.org/abs/2511.01498v1",
        "abstract": "Person re-identification (ReID) plays a pivotal role in computer vision,\nparticularly in surveillance and security applications within IoT-enabled smart\nenvironments. This study introduces the Enhanced Pedestrian Alignment Network\n(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.\nEPAN employs a dual-branch architecture to mitigate the impact of perspective\nand environmental changes, extracting alignment information under varying\nscales and viewpoints. Here, we demonstrate EPAN's strong feature extraction\ncapabilities, achieving outstanding performance on the Inspection-Personnel\ndataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of\n78.82%. This highlights EPAN's potential for real-world IoT applications,\nenabling effective and reliable person ReID across diverse cameras in\nsurveillance and security systems. The code and data are available at:\nhttps://github.com/ggboy2580/EPAN"
    },
    {
        "date": "2025-11",
        "title": "SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks",
        "author": "Changyuan Zhao, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Hongyang Du, Zehui Xiong, Dong In Kim, and Ping Zhang",
        "link": "http://arxiv.org/abs/2511.01466v1",
        "abstract": "Deep joint source-channel coding (JSCC) has emerged as a promising paradigm\nfor semantic communication, delivering significant performance gains over\nconventional separate coding schemes. However, existing JSCC frameworks remain\nvulnerable to physical-layer adversarial threats, such as pilot spoofing and\nsubcarrier jamming, compromising semantic fidelity. In this paper, we propose\nSecDiff, a plug-and-play, diffusion-aided decoding framework that significantly\nenhances the security and robustness of deep JSCC under adversarial wireless\nenvironments. Different from prior diffusion-guided JSCC methods that suffer\nfrom high inference latency, SecDiff employs pseudoinverse-guided sampling and\nadaptive guidance weighting, enabling flexible step-size control and efficient\nsemantic reconstruction. To counter jamming attacks, we introduce a power-based\nsubcarrier masking strategy and recast recovery as a masked inpainting problem,\nsolved via diffusion guidance. For pilot spoofing, we formulate channel\nestimation as a blind inverse problem and develop an expectation-minimization\n(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and\na channel operator. Notably, our method alternates between pilot recovery and\nchannel estimation, enabling joint refinement of both variables throughout the\ndiffusion process. Extensive experiments over orthogonal frequency-division\nmultiplexing (OFDM) channels under adversarial conditions show that SecDiff\noutperforms existing secure and generative JSCC baselines by achieving a\nfavorable trade-off between reconstruction quality and computational cost. This\nbalance makes SecDiff a promising step toward practical, low-latency, and\nattack-resilient semantic communications."
    },
    {
        "date": "2025-11",
        "title": "Security-Aware Joint Sensing, Communication, and Computing Optimization in Low Altitude Wireless Networks",
        "author": "Jiacheng Wang, Changyuan Zhao, Jialing He, Geng Sun, Weijie Yuan, Dusit Niyato, Liehuang Zhu, and Tao Xiang",
        "link": "http://arxiv.org/abs/2511.01451v1",
        "abstract": "As terrestrial resources become increasingly saturated, the research\nattention is shifting to the low-altitude airspace, with many emerging\napplications such as urban air taxis and aerial inspection. Low-Altitude\nWireless Networks (LAWNs) are the foundation for these applications, with\nintegrated sensing, communications, and computing (ISCC) being one of the core\nparts of LAWNs. However, the openness of low-altitude airspace exposes\ncommunications to security threats, degrading ISCC performance and ultimately\ncompromising the reliability of applications supported by LAWNs. To address\nthese challenges, this paper studies joint performance optimization of ISCC\nwhile considering secrecyness of the communications. Specifically, we derive\nbeampattern error, secrecy rate, and age of information (AoI) as performance\nmetrics for sensing, secrecy communication, and computing. Building on these\nmetrics, we formulate a multi-objective optimization problem that balances\nsensing and computation performance while keeping the probability of\ncommunication being detected below a required threshold. We then propose a deep\nQ-network (DQN)-based multi-objective evolutionary algorithm, which adaptively\nselects evolutionary operators according to the evolving optimization\nobjectives, thereby leading to more effective solutions. Extensive simulations\nshow that the proposed method achieves a superior balance among sensing\naccuracy, communication secrecyness, and information freshness compared with\nbaseline algorithms, thereby safeguarding ISCC performance and LAWN-supported\nlow-altitude applications."
    },
    {
        "date": "2025-11",
        "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck",
        "author": "Huiting Huang, Tieliang Gong, Kai He, Jialun Wu, Erik Cambria, and Mengling Feng",
        "link": "http://arxiv.org/abs/2511.01444v1",
        "abstract": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively."
    },
    {
        "date": "2025-11",
        "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation",
        "author": "Yongqi Yang, Huayang Huang, Xu Peng, Xiaobin Hu, Donghao Luo, Jiangning Zhang, Chengjie Wang, and Yu Wu",
        "link": "http://arxiv.org/abs/2511.01419v1",
        "abstract": "Recent hybrid video generation models combine autoregressive temporal\ndynamics with diffusion-based spatial denoising, but their sequential,\niterative nature leads to error accumulation and long inference times. In this\nwork, we propose a distillation-based framework for efficient causal video\ngeneration that enables high-quality synthesis with extremely limited denoising\nsteps. Our approach builds upon the Distribution Matching Distillation (DMD)\nframework and proposes a novel Adversarial Self-Distillation (ASD) strategy,\nwhich aligns the outputs of the student model's n-step denoising process with\nits (n+1)-step version at the distribution level. This design provides smoother\nsupervision by bridging small intra-student gaps and more informative guidance\nby combining teacher knowledge with locally consistent student behavior,\nsubstantially improving training stability and generation quality in extremely\nfew-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame\nEnhancement (FFE) strategy, which allocates more denoising steps to the initial\nframes to mitigate error propagation while applying larger skipping steps to\nlater frames. Extensive experiments on VBench demonstrate that our method\nsurpasses state-of-the-art approaches in both one-step and two-step video\ngeneration. Notably, our framework produces a single distilled model that\nflexibly supports multiple inference-step settings, eliminating the need for\nrepeated re-distillation and enabling efficient, high-quality video synthesis."
    },
    {
        "date": "2025-11",
        "title": "ConneX: Automatically Resolving Transaction Opacity of Cross-Chain Bridges for Security Analysis",
        "author": "Hanzhong Liang, Yue Duan, Xing Su, Xiao Li, Yating Liu, Yulong Tian, Fengyuan Xu, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2511.01393v1",
        "abstract": "As the Web3 ecosystem evolves toward a multi-chain architecture, cross-chain\nbridges have become critical infrastructure for enabling interoperability\nbetween diverse blockchain networks. However, while connecting isolated\nblockchains, the lack of cross-chain transaction pairing records introduces\nsignificant challenges for security analysis like cross-chain fund tracing,\nadvanced vulnerability detection, and transaction graph-based analysis. To\naddress this gap, we introduce ConneX, an automated and general-purpose system\ndesigned to accurately identify corresponding transaction pairs across both\nends of cross-chain bridges. Our system leverages Large Language Models (LLMs)\nto efficiently prune the semantic search space by identifying semantically\nplausible key information candidates within complex transaction records.\nFurther, it deploys a novel examiner module that refines these candidates by\nvalidating them against transaction values, effectively addressing semantic\nambiguities and identifying the correct semantics. Extensive evaluations on a\ndataset of about 500,000 transactions from five major bridge platforms\ndemonstrate that ConneX achieves an average F1 score of 0.9746, surpassing\nbaselines by at least 20.05\\%, with good efficiency that reduces the semantic\nsearch space by several orders of magnitude (1e10 to less than 100). Moreover,\nits successful application in tracing illicit funds (including a cross-chain\ntransfer worth $1 million) in real-world hacking incidents underscores its\npractical utility for enhancing cross-chain security and transparency."
    },
    {
        "date": "2025-11",
        "title": "EREBUS: End-to-end Robust Event Based Underwater Simulation",
        "author": "Hitesh Kyatham, Arjun Suresh, Aadi Palnitkar, and Yiannis Aloimonos",
        "link": "http://arxiv.org/abs/2511.01381v1",
        "abstract": "The underwater domain presents a vast array of challenges for roboticists and\ncomputer vision researchers alike, such as poor lighting conditions and high\ndynamic range scenes. In these adverse conditions, traditional vision\ntechniques struggle to adapt and lead to suboptimal performance. Event-based\ncameras present an attractive solution to this problem, mitigating the issues\nof traditional cameras by tracking changes in the footage on a frame-by-frame\nbasis. In this paper, we introduce a pipeline which can be used to generate\nrealistic synthetic data of an event-based camera mounted to an AUV (Autonomous\nUnderwater Vehicle) in an underwater environment for training vision models. We\ndemonstrate the effectiveness of our pipeline using the task of rock detection\nwith poor visibility and suspended particulate matter, but the approach can be\ngeneralized to other underwater tasks."
    },
    {
        "date": "2025-11",
        "title": "Protecting the Neural Networks against FGSM Attack Using Machine Unlearning",
        "author": "Amir Hossein Khorasani, Ali Jahanian, and Maryam Rastgarpour",
        "link": "http://arxiv.org/abs/2511.01377v1",
        "abstract": "Machine learning is a powerful tool for building predictive models. However,\nit is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)\nattacks are a common type of adversarial attack that adds small perturbations\nto input data to trick a model into misclassifying it. In response to these\nattacks, researchers have developed methods for \"unlearning\" these attacks,\nwhich involves retraining a model on the original data without the added\nperturbations. Machine unlearning is a technique that tries to \"forget\"\nspecific data points from the training dataset, to improve the robustness of a\nmachine learning model against adversarial attacks like FGSM. In this paper, we\nfocus on applying unlearning techniques to the LeNet neural network, a popular\narchitecture for image classification. We evaluate the efficacy of unlearning\nFGSM attacks on the LeNet network and find that it can significantly improve\nits robustness against these types of attacks."
    },
    {
        "date": "2025-11",
        "title": "MiniFool -- Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks",
        "author": "Lucie Flek, Oliver Janik, Philipp Alexander Jung, Akbar Karimi, Timo Saala, Alexander Schmidt, Matthias Schott, Philipp Soldin, Matthias Thiesmeyer, Christopher Wiebusch, and Ulrich Willemsen",
        "link": "http://arxiv.org/abs/2511.01352v1",
        "abstract": "In this paper, we present a new algorithm, MiniFool, that implements\nphysics-inspired adversarial attacks for testing neural network-based\nclassification tasks in particle and astroparticle physics. While we initially\ndeveloped the algorithm for the search for astrophysical tau neutrinos with the\nIceCube Neutrino Observatory, we apply it to further data from other science\ndomains, thus demonstrating its general applicability. Here, we apply the\nalgorithm to the well-known MNIST data set and furthermore, to Open Data data\nfrom the CMS experiment at the Large Hadron Collider. The algorithm is based on\nminimizing a cost function that combines a $\\chi^2$ based test-statistic with\nthe deviation from the desired target score. The test statistic quantifies the\nprobability of the perturbations applied to the data based on the experimental\nuncertainties. For our studied use cases, we find that the likelihood of a\nflipped classification differs for both the initially correctly and incorrectly\nclassified events. When testing changes of the classifications as a function of\nan attack parameter that scales the experimental uncertainties, the robustness\nof the network decision can be quantified. Furthermore, this allows testing the\nrobustness of the classification of unlabeled experimental data."
    },
    {
        "date": "2025-11",
        "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
        "author": "Hongyin Zhang, Shuo Zhang, Junxi Jin, Qixin Zeng, Runze Li, and Donglin Wang",
        "link": "http://arxiv.org/abs/2511.01331v1",
        "abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful\ngeneral-purpose policies for robotic manipulation, benefiting from large-scale\nmulti-modal pre-training. However, they often fail to generalize reliably in\nout-of-distribution deployments, where unavoidable disturbances such as\nobservation noise, sensor errors, or actuation perturbations become prevalent.\nWhile recent Reinforcement Learning (RL)-based post-training provides a\npractical means to adapt pre-trained VLA models, existing methods mainly\nemphasize reward maximization and overlook robustness to environmental\nuncertainty. In this work, we introduce RobustVLA, a lightweight online RL\npost-training method designed to explicitly enhance the resilience of VLA\nmodels. Through a systematic robustness analysis, we identify two key\nregularizations: Jacobian regularization, which mitigates sensitivity to\nobservation noise, and smoothness regularization, which stabilizes policies\nunder action perturbations. Extensive experiments across diverse robotic\nenvironments demonstrate that RobustVLA significantly outperforms prior\nstate-of-the-art methods in robustness and reliability. Our results highlight\nthe importance of principled robustness-aware RL post-training as a key step\ntoward improving the reliability and robustness of VLA models."
    },
    {
        "date": "2025-11",
        "title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model",
        "author": "Sampriti Soor, Alik Pramanick, Jothiprakash K, and Arijit Sur",
        "link": "http://arxiv.org/abs/2511.01317v1",
        "abstract": "The rapid growth of deep learning has brought about powerful models that can\nhandle various tasks, like identifying images and understanding language.\nHowever, adversarial attacks, an unnoticed alteration, can deceive models,\nleading to inaccurate predictions. In this paper, a generative adversarial\nattack method is proposed that uses the CLIP model to create highly effective\nand visually imperceptible adversarial perturbations. The CLIP model's ability\nto align text and image representation helps incorporate natural language\nsemantics with a guided loss to generate effective adversarial examples that\nlook identical to the original inputs. This integration allows extensive scene\nmanipulation, creating perturbations in multi-object environments specifically\ndesigned to deceive multilabel classifiers. Our approach integrates the\nconcentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with\nthe dissimilar text embeddings similar to Generative Adversarial Multi-Object\nScene Attacks (GAMA), resulting in perturbations that both deceive\nclassification models and maintain high structural similarity to the original\nimages. The model was tested on various tasks across diverse black-box victim\nmodels. The experimental results show that our method performs competitively,\nachieving comparable or superior results to existing techniques, while\npreserving greater visual fidelity."
    },
    {
        "date": "2025-11",
        "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models",
        "author": "Tae-Young Lee, Juwon Seo, Jong Hwan Ko, and Gyeong-Moon Park",
        "link": "http://arxiv.org/abs/2511.01307v1",
        "abstract": "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM."
    },
    {
        "date": "2025-11",
        "title": "LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping",
        "author": "Guanjie Cheng, Mengzhen Yang, Xinkui Zhao, Shuyi Yu, Tianyu Du, Yangyang Wu, Mengying Zhu, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2511.01296v1",
        "abstract": "Federated learning (FL) enables collaborative model training across\ndistributed nodes without exposing raw data, but its decentralized nature makes\nit vulnerable in trust-deficient environments. Inference attacks may recover\nsensitive information from gradient updates, while poisoning attacks can\ndegrade model performance or induce malicious behaviors. Existing defenses\noften suffer from high communication and computation costs, or limited\ndetection precision. To address these issues, we propose LSHFed, a robust and\ncommunication-efficient FL framework that simultaneously enhances aggregation\nrobustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a\nnovel gradient verification mechanism that projects high-dimensional gradients\ninto compact binary representations via multi-hyperplane locally-sensitive\nhashing. This enables accurate detection and filtering of malicious gradients\nusing only their irreversible hash forms, thus mitigating privacy leakage risks\nand substantially reducing transmission overhead. Extensive experiments\ndemonstrate that LSHFed maintains high model performance even when up to 50% of\nparticipants are collusive adversaries while achieving up to a 1000x reduction\nin gradient verification communication compared to full-gradient methods."
    },
    {
        "date": "2025-11",
        "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers",
        "author": "Qin Zhou, Zhexin Zhang, Zhi Li, and Limin Sun",
        "link": "http://arxiv.org/abs/2511.01287v1",
        "abstract": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review."
    },
    {
        "date": "2025-11",
        "title": "Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting",
        "author": "Zan Li, Kyongmin Yeo, Wesley Gifford, Lara Marcuse, Madeline Fields, and B\u00fclent Yener",
        "link": "http://arxiv.org/abs/2511.01275v1",
        "abstract": "Forecasting epileptic seizures from multivariate EEG signals represents a\ncritical challenge in healthcare time series prediction, requiring high\nsensitivity, low false alarm rates, and subject-specific adaptability. We\npresent STAN, an Adversarial Spatio-Temporal Attention Network that jointly\nmodels spatial brain connectivity and temporal neural dynamics through cascaded\nattention blocks with alternating spatial and temporal modules. Unlike existing\napproaches that assume fixed preictal durations or separately process spatial\nand temporal features, STAN captures bidirectional dependencies between spatial\nand temporal patterns through a unified cascaded architecture. Adversarial\ntraining with gradient penalty enables robust discrimination between interictal\nand preictal states learned from clearly defined 15-minute preictal windows.\nContinuous 90-minute pre-seizure monitoring reveals that the learned\nspatio-temporal attention patterns enable early detection: reliable alarms\ntrigger at subject-specific times (typically 15-45 minutes before onset),\nreflecting the model's capacity to capture subtle preictal dynamics without\nrequiring individualized training. Experiments on two benchmark EEG datasets\n(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14\nevents) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011\nfalse detections per hour and 94.2% sensitivity with 0.063 false detections per\nhour, respectively, while maintaining computational efficiency (2.3M\nparameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond\nepilepsy, the proposed framework provides a general paradigm for\nspatio-temporal forecasting in healthcare and other time series domains where\nindividual heterogeneity and interpretability are crucial."
    },
    {
        "date": "2025-11",
        "title": "Rescuing the Unpoisoned: Efficient Defense against Knowledge Corruption Attacks on RAG Systems",
        "author": "Minseok Kim, Hankook Lee, and Hyungjoon Koo",
        "link": "http://arxiv.org/abs/2511.01268v1",
        "abstract": "Large language models (LLMs) are reshaping numerous facets of our daily\nlives, leading widespread adoption as web-based services. Despite their\nversatility, LLMs face notable challenges, such as generating hallucinated\ncontent and lacking access to up-to-date information. Lately, to address such\nlimitations, Retrieval-Augmented Generation (RAG) has emerged as a promising\ndirection by generating responses grounded in external knowledge sources. A\ntypical RAG system consists of i) a retriever that probes a group of relevant\npassages from a knowledge base and ii) a generator that formulates a response\nbased on the retrieved content. However, as with other AI systems, recent\nstudies demonstrate the vulnerability of RAG, such as knowledge corruption\nattacks by injecting misleading information. In response, several defense\nstrategies have been proposed, including having LLMs inspect the retrieved\npassages individually or fine-tuning robust retrievers. While effective, such\napproaches often come with substantial computational costs.\n  In this work, we introduce RAGDefender, a resource-efficient defense\nmechanism against knowledge corruption (i.e., by data poisoning) attacks in\npractical RAG deployments. RAGDefender operates during the post-retrieval\nphase, leveraging lightweight machine learning techniques to detect and filter\nout adversarial content without requiring additional model training or\ninference. Our empirical evaluations show that RAGDefender consistently\noutperforms existing state-of-the-art defenses across multiple models and\nadversarial scenarios: e.g., RAGDefender reduces the attack success rate (ASR)\nagainst the Gemini model from 0.89 to as low as 0.02, compared to 0.69 for\nRobustRAG and 0.24 for Discern-and-Answer when adversarial passages outnumber\nlegitimate ones by a factor of four (4x)."
    },
    {
        "date": "2025-11",
        "title": "A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation",
        "author": "Yiyang Yang, Xiejian Chi, Shanxing Gao, Kaidong Wang, and Yao Wang",
        "link": "http://arxiv.org/abs/2511.01267v1",
        "abstract": "Data quality is critical to Intelligent Transportation Systems (ITS), as\ncomplete and accurate traffic data underpin reliable decision-making in traffic\ncontrol and management. Recent advances in low-rank tensor recovery algorithms\nhave shown strong potential in capturing the inherent structure of\nhigh-dimensional traffic data and restoring degraded observations. However,\ntraditional batch-based methods demand substantial computational and storage\nresources, which limits their scalability in the face of continuously expanding\ntraffic data volumes. Moreover, recent online tensor recovery methods often\nsuffer from severe performance degradation in complex real-world scenarios due\nto their insufficient exploitation of the intrinsic structural properties of\ntraffic data. To address these challenges, we reformulate the traffic data\nrecovery problem within a streaming framework, and propose a novel online\nrobust tensor recovery algorithm that simultaneously leverages both the global\nspatio-temporal correlations and local consistency of traffic data, achieving\nhigh recovery accuracy and significantly improved computational efficiency in\nlarge-scale scenarios. Our method is capable of simultaneously handling missing\nand anomalous values in traffic data, and demonstrates strong adaptability\nacross diverse missing patterns. Experimental results on three real-world\ntraffic datasets demonstrate that the proposed approach achieves high recovery\naccuracy while significantly improving computational efficiency by up to three\norders of magnitude compared to state-of-the-art batch-based methods. These\nfindings highlight the potential of the proposed approach as a scalable and\neffective solution for traffic data quality enhancement in ITS."
    },
    {
        "date": "2025-11",
        "title": "Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability",
        "author": "Zhixuan Zhang, Pingyu Wang, Xingjian Zheng, Linbo Qing, and Qi Liu",
        "link": "http://arxiv.org/abs/2511.01240v1",
        "abstract": "Transferable attacks generate adversarial examples on surrogate models to\nfool unknown victim models, posing real-world threats and growing research\ninterest. Despite focusing on flat losses for transferable adversarial\nexamples, recent studies still fall into suboptimal regions, especially the\nflat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce\na novel black-box gradient-based transferable attack from a perspective of\ndual-order information. Specifically, we feasibly propose Adversarial Flatness\n(AF) to the deceptive flatness problem and a theoretical assurance for\nadversarial transferability. Based on this, using an efficient approximation of\nour objective, we instantiate our attack as Adversarial Flatness Attack (AFA),\naddressing the altered gradient sign issue. Additionally, to further improve\nthe attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by\nenhancing the inner-loop sampling efficiency. The comprehensive results on\nImageNet-compatible dataset demonstrate superiority over six baselines,\ngenerating adversarial examples in flatter regions and boosting transferability\nacross model architectures. When tested on input transformation attacks or the\nBaidu Cloud API, our method outperforms baselines."
    },
    {
        "date": "2025-11",
        "title": "A Large Scale Study of AI-based Binary Function Similarity Detection Techniques for Security Researchers and Practitioners",
        "author": "Jingyi Shi, Yufeng Chen, Yang Xiao, Yuekang Li, Zhengzi Xu, Sihao Qiu, Chi Zhang, Keyu Qi, Yeting Li, Xingchu Chen, Yanyan Zou, Yang Liu, and Wei Huo",
        "link": "http://arxiv.org/abs/2511.01180v1",
        "abstract": "Binary Function Similarity Detection (BFSD) is a foundational technique in\nsoftware security, underpinning a wide range of applications including\nvulnerability detection, malware analysis. Recent advances in AI-based BFSD\ntools have led to significant performance improvements. However, existing\nevaluations of these tools suffer from three key limitations: a lack of\nin-depth analysis of performance-influencing factors, an absence of realistic\napplication analysis, and reliance on small-scale or low-quality datasets.\n  In this paper, we present the first large-scale empirical study of AI-based\nBFSD tools to address these gaps. We construct two high-quality and diverse\ndatasets: BinAtlas, comprising 12,453 binaries and over 7 million functions for\ncapability evaluation; and BinAres, containing 12,291 binaries and 54\nreal-world 1-day vulnerabilities for evaluating vulnerability detection\nperformance in practical IoT firmware settings. Using these datasets, we\nevaluate nine representative BFSD tools, analyze the challenges and limitations\nof existing BFSD tools, and investigate the consistency among BFSD tools. We\nalso propose an actionable strategy for combining BFSD tools to enhance overall\nperformance (an improvement of 13.4%). Our study not only advances the\npractical adoption of BFSD tools but also provides valuable resources and\ninsights to guide future research in scalable and automated binary similarity\ndetection."
    },
    {
        "date": "2025-11",
        "title": "Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification",
        "author": "Ali Owfi, Amirmohammad Bamdad, Tolunay Seyfi, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2511.01172v1",
        "abstract": "Deep learning has emerged as a leading approach for Automatic Modulation\nClassification (AMC), demonstrating superior performance over traditional\nmethods. However, vulnerability to adversarial attacks and susceptibility to\ndata distribution shifts hinder their practical deployment in real-world,\ndynamic environments. To address these threats, we propose a novel, unified\nframework that integrates meta-learning with domain adaptation, making AMC\nsystems resistant to both adversarial attacks and environmental changes. Our\nframework utilizes a two-phase strategy. First, in an offline phase, we employ\na meta-learning approach to train the model on clean and adversarially\nperturbed samples from a single source domain. This method enables the model to\ngeneralize its defense, making it resistant to a combination of previously\nunseen attacks. Subsequently, in the online phase, we apply domain adaptation\nto align the model's features with a new target domain, allowing it to adapt\nwithout requiring substantial labeled data. As a result, our framework achieves\na significant improvement in modulation classification accuracy against these\ncombined threats, offering a critical solution to the deployment and\noperational challenges of modern AMC systems."
    },
    {
        "date": "2025-11",
        "title": "Verification and Attack Synthesis for Network Protocols",
        "author": "Max von Hippel",
        "link": "http://arxiv.org/abs/2511.01124v1",
        "abstract": "Network protocols are programs with inputs and outputs that follow predefined\ncommunication patterns to synchronize and exchange information. There are many\nprotocols and each serves a different purpose, e.g., routing, transport, secure\ncommunication, etc. The functional and performance requirements for a protocol\ncan be expressed using a formal specification, such as, a set of logical\npredicates over its traces. A protocol could be prevented from achieving its\nrequirements due to a bug in its design or implementation, a component failure\n(e.g., a crash), or an attack. This dissertation shows that formal methods can\nfeasibly characterize the functionality and performance of network protocols\nunder normal conditions as well as when subjected to attacks."
    },
    {
        "date": "2025-11",
        "title": "T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression",
        "author": "Nikolay I. Kalmykov, Razan Dibo, Kaiyu Shen, Xu Zhonghan, Anh-Huy Phan, Yipeng Liu, and Ivan Oseledets",
        "link": "http://arxiv.org/abs/2511.01079v1",
        "abstract": "Neural image compression (NIC) has become the state-of-the-art for\nrate-distortion performance, yet its security vulnerabilities remain\nsignificantly less understood than those of classifiers. Existing adversarial\nattacks on NICs are often naive adaptations of pixel-space methods, overlooking\nthe unique, structured nature of the compression pipeline. In this work, we\npropose a more advanced class of vulnerabilities by introducing T-MLA, the\nfirst targeted multiscale log--exponential attack framework. Our approach\ncrafts adversarial perturbations in the wavelet domain by directly targeting\nthe quality of the attacked and reconstructed images. This allows for a\nprincipled, offline attack where perturbations are strategically confined to\nspecific wavelet subbands, maximizing distortion while ensuring perceptual\nstealth. Extensive evaluation across multiple state-of-the-art NIC\narchitectures on standard image compression benchmarks reveals a large drop in\nreconstruction quality while the perturbations remain visually imperceptible.\nOur findings reveal a critical security flaw at the core of generative and\ncontent delivery pipelines."
    },
    {
        "date": "2025-11",
        "title": "Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification",
        "author": "Yangyang Zhao, Matti Kaisti, Olli Lahdenoja, and Tero Koivisto",
        "link": "http://arxiv.org/abs/2511.00949v1",
        "abstract": "Atrial fibrillation (AF) is a leading cause of stroke and mortality,\nparticularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables\nnon-invasive, continuous rhythm monitoring, yet suffers from significant\nvulnerability to motion artifacts and physiological noise. Many existing\napproaches rely solely on single-channel PPG and are limited to binary AF\ndetection, often failing to capture the broader range of arrhythmias\nencountered in clinical settings. We introduce RhythmiNet, a residual neural\nnetwork enhanced with temporal and channel attention modules that jointly\nleverage PPG and accelerometer (ACC) signals. The model performs three-class\nrhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness\nacross varying movement conditions, test data are stratified by\naccelerometer-based motion intensity percentiles without excluding any\nsegments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only\nbaseline. In addition, performance surpassed a logistic regression model based\non handcrafted HRV features by 12%, highlighting the benefit of multimodal\nfusion and attention-based learning in noisy, real-world clinical data."
    },
    {
        "date": "2025-11",
        "title": "Leakage-abuse Attack Against Substring-SSE with Partially Known Dataset",
        "author": "Xijie Ba, Qin Liu, Xiaohong Li, and Jianting Ning",
        "link": "http://arxiv.org/abs/2511.00930v1",
        "abstract": "Substring-searchable symmetric encryption (substring-SSE) has become\nincreasingly critical for privacy-preserving applications in cloud systems.\nHowever, existing schemes remain vulnerable to information leakage during\nsearch operations, particularly when adversaries possess partial knowledge of\nthe target dataset. Although leakage-abuse attacks have been widely studied for\ntraditional SSE, their applicability to substring-SSE under partially known\ndata assumptions remains unexplored. In this paper, we present the first\nleakage-abuse attack on substring-SSE under partially-known dataset conditions.\nWe develop a novel matrix-based correlation technique that extends and\noptimizes the LEAP framework for substring-SSE, enabling efficient recovery of\nplaintext data from encrypted suffix tree structures. Unlike existing\napproaches that rely on independent auxiliary datasets, our method directly\nexploits known data fragments to establish high-confidence mappings between\nciphertext tokens and plaintext substrings through iterative matrix\ntransformations. Comprehensive experiments on real-world datasets demonstrate\nthe effectiveness of the attack, with recovery rates reaching 98.32% for\nsubstrings given 50% auxiliary knowledge. Even with only 10% prior knowledge,\nthe attack achieves 74.42% substring recovery while maintaining strong\nscalability across datasets of varying sizes. The result reveals significant\nprivacy risks in current substring-SSE designs and highlights the urgent need\nfor leakage-resilient constructions."
    },
    {
        "date": "2025-11",
        "title": "Parameter Interpolation Adversarial Training for Robust Image Classification",
        "author": "Xin Liu, Yichen Yang, Kun He, and John E. Hopcroft",
        "link": "http://arxiv.org/abs/2511.00836v1",
        "abstract": "Though deep neural networks exhibit superior performance on various tasks,\nthey are still plagued by adversarial examples. Adversarial training has been\ndemonstrated to be the most effective method to defend against adversarial\nattacks. However, existing adversarial training methods show that the model\nrobustness has apparent oscillations and overfitting issues in the training\nprocess, degrading the defense efficacy. To address these issues, we propose a\nnovel framework called Parameter Interpolation Adversarial Training (PIAT).\nPIAT tunes the model parameters between each epoch by interpolating the\nparameters of the previous and current epochs. It makes the decision boundary\nof model change more moderate and alleviates the overfitting issue, helping the\nmodel converge better and achieving higher model robustness. In addition, we\nsuggest using the Normalized Mean Square Error (NMSE) to further improve the\nrobustness by aligning the relative magnitude of logits between clean and\nadversarial examples rather than the absolute magnitude. Extensive experiments\nconducted on several benchmark datasets demonstrate that our framework could\nprominently improve the robustness of both Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs)."
    },
    {
        "date": "2025-11",
        "title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack",
        "author": "Xin Liu, Aoyang Zhou, and Aoyang Zhou",
        "link": "http://arxiv.org/abs/2511.00831v1",
        "abstract": "Visual-Language Pre-training (VLP) models have achieved significant\nperformance across various downstream tasks. However, they remain vulnerable to\nadversarial examples. While prior efforts focus on improving the adversarial\ntransferability of multimodal adversarial examples through cross-modal\ninteractions, these approaches suffer from overfitting issues, due to a lack of\ninput diversity by relying excessively on information from adversarial examples\nin one modality when crafting attacks in another. To address this issue, we\ndraw inspiration from strategies in some adversarial training methods and\npropose a novel attack called Local Shuffle and Sample-based Attack (LSSA).\nLSSA randomly shuffles one of the local image blocks, thus expanding the\noriginal image-text pairs, generating adversarial images, and sampling around\nthem. Then, it utilizes both the original and sampled images to generate the\nadversarial texts. Extensive experiments on multiple models and datasets\ndemonstrate that LSSA significantly enhances the transferability of multimodal\nadversarial examples across diverse VLP models and downstream tasks. Moreover,\nLSSA outperforms other advanced attacks on Large Vision-Language Models."
    },
    {
        "date": "2025-11",
        "title": "Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift",
        "author": "Morgan Lee, Artem Frenk, Eamon Worden, Karish Gupta, Thinh Pham, Ethan Croteau, and Neil Heffernan",
        "link": "http://arxiv.org/abs/2511.00704v2",
        "abstract": "Knowledge Tracing (KT) has been an established problem in the educational\ndata mining field for decades, and it is commonly assumed that the underlying\nlearning process being modeled remains static. Given the ever-changing\nlandscape of online learning platforms (OLPs), we investigate how concept drift\nand changing student populations can impact student behavior within an OLP\nthrough testing model performance both within a single academic year and across\nmultiple academic years. Four well-studied KT models were applied to five\nacademic years of data to assess how susceptible KT models are to concept\ndrift. Through our analysis, we find that all four families of KT models can\nexhibit degraded performance, Bayesian Knowledge Tracing (BKT) remains the most\nstable KT model when applied to newer data, while more complex, attention based\nmodels lose predictive power significantly faster."
    },
    {
        "date": "2025-11",
        "title": "Stochastic Shortest Path with Sparse Adversarial Costs",
        "author": "Emmeran Johnson, Alberto Rumi, Ciara Pike-Burke, and Patrick Rebeschini",
        "link": "http://arxiv.org/abs/2511.00637v1",
        "abstract": "We study the adversarial Stochastic Shortest Path (SSP) problem with sparse\ncosts under full-information feedback. In the known transition setting,\nexisting bounds based on Online Mirror Descent (OMD) with negative-entropy\nregularization scale with $\\sqrt{\\log S A}$, where $SA$ is the size of the\nstate-action space. While we show that this is optimal in the worst-case, this\nbound fails to capture the benefits of sparsity when only a small number $M \\ll\nSA$ of state-action pairs incur cost. In fact, we also show that the\nnegative-entropy is inherently non-adaptive to sparsity: it provably incurs\nregret scaling with $\\sqrt{\\log S}$ on sparse problems. Instead, we propose a\nfamily of $\\ell_r$-norm regularizers ($r \\in (1,2)$) that adapts to the\nsparsity and achieves regret scaling with $\\sqrt{\\log M}$ instead of\n$\\sqrt{\\log SA}$. We show this is optimal via a matching lower bound,\nhighlighting that $M$ captures the effective dimension of the problem instead\nof $SA$. Finally, in the unknown transition setting the benefits of sparsity\nare limited: we prove that even on sparse problems, the minimax regret for any\nlearner scales polynomially with $SA$."
    },
    {
        "date": "2025-11",
        "title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations",
        "author": "Qiang Li, Jin Niu, and Lina Yu",
        "link": "http://arxiv.org/abs/2511.00549v1",
        "abstract": "Traffic congestion, primarily driven by intersection queuing, significantly\nimpacts urban living standards, safety, environmental quality, and economic\nefficiency. While Traffic Signal Control (TSC) systems hold potential for\ncongestion mitigation, traditional optimization models often fail to capture\nreal-world traffic complexity and dynamics. This study introduces a novel\nsingle-agent reinforcement learning (RL) framework for regional adaptive TSC,\ncircumventing the coordination complexities inherent in multi-agent systems\nthrough a centralized decision-making paradigm. The model employs an adjacency\nmatrix to unify the encoding of road network topology, real-time queue states\nderived from probe vehicle data, and current signal timing parameters.\nLeveraging the efficient learning capabilities of the DreamerV3 world model,\nthe agent learns control policies where actions sequentially select\nintersections and adjust their signal phase splits to regulate traffic\ninflow/outflow, analogous to a feedback control system. Reward design\nprioritizes queue dissipation, directly linking congestion metrics (queue\nlength) to control actions. Simulation experiments conducted in SUMO\ndemonstrate the model's effectiveness: under inference scenarios with\nmulti-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the\nframework exhibits robust anti-fluctuation capability and significantly reduces\nqueue lengths. This work establishes a new paradigm for intelligent traffic\ncontrol compatible with probe vehicle technology. Future research will focus on\nenhancing practical applicability by incorporating stochastic OD demand\nfluctuations during training and exploring regional optimization mechanisms for\ncontingency events."
    },
    {
        "date": "2025-11",
        "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training",
        "author": "Xin Yao, Haiyang Zhao, Yimin Chen, Jiawei Guo, Kecheng Huang, and Ming Zhao",
        "link": "http://arxiv.org/abs/2511.00446v1",
        "abstract": "The Contrastive Language-Image Pretraining (CLIP) model has significantly\nadvanced vision-language modeling by aligning image-text pairs from large-scale\nweb data through self-supervised contrastive learning. Yet, its reliance on\nuncurated Internet-sourced data exposes it to data poisoning and backdoor\nrisks. While existing studies primarily investigate image-based attacks, the\ntext modality, which is equally central to CLIP's training, remains\nunderexplored. In this work, we introduce ToxicTextCLIP, a framework for\ngenerating high-quality adversarial texts that target CLIP during the\npre-training phase. The framework addresses two key challenges: semantic\nmisalignment caused by background inconsistency with the target class, and the\nscarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively\napplies: 1) a background-aware selector that prioritizes texts with background\ncontent aligned to the target class, and 2) a background-driven augmenter that\ngenerates semantically coherent and diverse poisoned samples. Extensive\nexperiments on classification and retrieval tasks show that ToxicTextCLIP\nachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while\nbypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be\naccessed via https://github.com/xinyaocse/ToxicTextCLIP/."
    },
    {
        "date": "2025-11",
        "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks",
        "author": "Yiwei Zha, Rui Min, and Shanu Sushmita",
        "link": "http://arxiv.org/abs/2511.00416v1",
        "abstract": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark."
    },
    {
        "date": "2025-11",
        "title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling",
        "author": "Zenghao Niu, Weicheng Xie, Siyang Song, Zitong Yu, Feng Liu, and Linlin Shen",
        "link": "http://arxiv.org/abs/2511.00411v1",
        "abstract": "Adversarial attacks present a critical challenge to deep neural networks'\nrobustness, particularly in transfer scenarios across different model\narchitectures. However, the transferability of adversarial attacks faces a\nfundamental dilemma between Exploitation (maximizing attack potency) and\nExploration (enhancing cross-model generalization). Traditional momentum-based\nmethods over-prioritize Exploitation, i.e., higher loss maxima for attack\npotency but weakened generalization (narrow loss surface). Conversely, recent\nmethods with inner-iteration sampling over-prioritize Exploration, i.e.,\nflatter loss surfaces for cross-model generalization but weakened attack\npotency (suboptimal local maxima). To resolve this dilemma, we propose a simple\nyet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives\nthrough guiding sampling along the gradient ascent direction to improve both\nsampling efficiency and stability. Specifically, based on MI-FGSM, GGS\nintroduces inner-iteration random sampling and guides the sampling direction\nusing the gradient from the previous inner-iteration (the sampling's magnitude\nis determined by a random distribution). This mechanism encourages adversarial\nexamples to reside in balanced regions with both flatness for cross-model\ngeneralization and higher local maxima for strong attack potency. Comprehensive\nexperiments across multiple DNN architectures and multimodal large language\nmodels (MLLMs) demonstrate the superiority of our method over state-of-the-art\ntransfer attacks. Code is made available at https://github.com/anuin-cat/GGS."
    },
    {
        "date": "2025-11",
        "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping",
        "author": "Lingpeng Chen, Jiakun Tang, Apple Pui-Yi Chui, Ziyang Hong, and Junfeng Wu",
        "link": "http://arxiv.org/abs/2511.00392v1",
        "abstract": "Accurate 3D reconstruction in visually-degraded underwater environments\nremains a formidable challenge. Single-modality approaches are insufficient:\nvision-based methods fail due to poor visibility and geometric constraints,\nwhile sonar is crippled by inherent elevation ambiguity and low resolution.\nConsequently, prior fusion technique relies on heuristics and flawed geometric\nassumptions, leading to significant artifacts and an inability to model complex\nscenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep\nlearning framework that overcomes these limitations by adapting the principled\nplane sweep algorithm for cross-modal fusion between sonar and visual data.\nExtensive experiments in both high-fidelity simulation and real-world\nenvironments demonstrate that SonarSweep consistently generates dense and\naccurate depth maps, significantly outperforming state-of-the-art methods\nacross challenging conditions, particularly in high turbidity. To foster\nfurther research, we will publicly release our code and a novel dataset\nfeaturing synchronized stereo-camera and sonar data, the first of its kind."
    },
    {
        "date": "2025-11",
        "title": "Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks",
        "author": "Kayua Oleques Paim, Rodrigo Brandao Mansilha, Diego Kreutz, Muriel Figueredo Franco, and Weverton Cordeiro",
        "link": "http://arxiv.org/abs/2511.00346v1",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has raised\nsignificant concerns about their security against adversarial attacks. In this\nwork, we propose a novel approach to crafting universal jailbreaks and data\nextraction attacks by exploiting latent space discontinuities, an architectural\nvulnerability related to the sparsity of training data. Unlike previous\nmethods, our technique generalizes across various models and interfaces,\nproving highly effective in seven state-of-the-art LLMs and one image\ngeneration model. Initial results indicate that when these discontinuities are\nexploited, they can consistently and profoundly compromise model behavior, even\nin the presence of layered defenses. The findings suggest that this strategy\nhas substantial potential as a systemic attack vector."
    },
    {
        "date": "2025-11",
        "title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction",
        "author": "Yuhao Fang, Zijian Wang, Yao Lu, Ye Zhang, and Chun Li",
        "link": "http://arxiv.org/abs/2511.00338v1",
        "abstract": "This work presents a novel hybrid approach that integrates Deep Operator\nNetworks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex\ninverse problem. The method effectively addresses tasks such as source\nlocalization governed by the Navier-Stokes equations and image reconstruction,\novercoming challenges related to nonlinearity, sparsity, and noisy data. By\nincorporating physics-informed constraints and task-specific regularization\ninto the loss function, the framework ensures solutions that are both\nphysically consistent and accurate. Validation on diverse synthetic and real\ndatasets demonstrates its robustness, scalability, and precision, showcasing\nits broad potential applications in computational physics and imaging sciences."
    },
    {
        "date": "2025-11",
        "title": "Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems",
        "author": "Siva Sai, Manish Prasad, Animesh Bhargava, Vinay Chamola, and Rajkumar Buyya",
        "link": "http://arxiv.org/abs/2511.00336v1",
        "abstract": "The rapid growth of Internet of Medical Things (IoMT) devices has resulted in\nsignificant security risks, particularly the risk of malware attacks on\nresource-constrained devices. Conventional deep learning methods are\nimpractical due to resource limitations, while Federated Learning (FL) suffers\nfrom high communication overhead and vulnerability to non-IID (heterogeneous)\ndata. In this paper, we propose a split learning (SL) based framework for IoT\nmalware detection through image-based classification. By dividing the neural\nnetwork training between the clients and an edge server, the framework reduces\ncomputational burden on resource-constrained clients while ensuring data\nprivacy. We formulate a joint optimization problem that balances computation\ncost and communication efficiency by using a game-theoretic approach for\nattaining better training performance. Experimental evaluations show that the\nproposed framework outperforms popular FL methods in terms of accuracy\n(+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less\nresource consumption (33.83%). These results establish the potential of SL as a\nscalable and secure paradigm for next-generation IoT security."
    },
    {
        "date": "2025-11",
        "title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models",
        "author": "Weidong Zhang, Pak Lun Kevin Ding, and Huan Liu",
        "link": "http://arxiv.org/abs/2511.00335v1",
        "abstract": "Lightweight vision classification models such as MobileNet, ShuffleNet, and\nEfficientNet are increasingly deployed in mobile and embedded systems, yet\ntheir performance has been predominantly benchmarked on ImageNet. This raises\ncritical questions: Do models that excel on ImageNet also generalize across\nother domains? How can cross-dataset robustness be systematically quantified?\nAnd which architectural elements consistently drive generalization under tight\nresource constraints? Here, we present the first systematic evaluation of 11\nlightweight vision models (2.5M parameters), trained under a fixed 100-epoch\nschedule across 7 diverse datasets. We introduce the Cross-Dataset Score\n(xScore), a unified metric that quantifies the consistency and robustness of\nmodel performance across diverse visual domains. Our results show that (1)\nImageNet accuracy does not reliably predict performance on fine-grained or\nmedical datasets, (2) xScore provides a scalable predictor of mobile model\nperformance that can be estimated from just four datasets, and (3) certain\narchitectural components--such as isotropic convolutions with higher spatial\nresolution and channel-wise attention--promote broader generalization, while\nTransformer-based blocks yield little additional benefit, despite incurring\nhigher parameter overhead. This study provides a reproducible framework for\nevaluating lightweight vision models beyond ImageNet, highlights key design\nprinciples for mobile-friendly architectures, and guides the development of\nfuture models that generalize robustly across diverse application domains."
    },
    {
        "date": "2025-10",
        "title": "Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning",
        "author": "Michiel Straat, Thorben Markmann, Sebastian Peitz, and Barbara Hammer",
        "link": "http://arxiv.org/abs/2511.00272v1",
        "abstract": "Chaotic convective flows arise in many real-world systems, such as\nmicrofluidic devices and chemical reactors. Stabilizing these flows is highly\ndesirable but remains challenging, particularly in chaotic regimes where\nconventional control methods often fail. Reinforcement Learning (RL) has shown\npromise for control in laminar flow settings, but its ability to generalize and\nremain robust under chaotic and turbulent dynamics is not well explored,\ndespite being critical for real-world deployment. In this work, we improve the\npractical feasibility of RL-based control of such flows focusing on\nRayleigh-B\\'enard Convection (RBC), a canonical model for convective heat\ntransport. To enhance generalization and sample efficiency, we introduce\ndomain-informed RL agents that are trained using Proximal Policy Optimization\nacross diverse initial conditions and flow regimes. We incorporate domain\nknowledge in the reward function via a term that encourages B\\'enard cell\nmerging, as an example of a desirable macroscopic property. In laminar flow\nregimes, the domain-informed RL agents reduce convective heat transport by up\nto 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which\nis significantly better than the conventional controllers used in practice. We\ncompare the domain-informed to uninformed agents: Our results show that the\ndomain-informed reward design results in steady flows, faster convergence\nduring training, and generalization across flow regimes without retraining. Our\nwork demonstrates that elegant domain-informed priors can greatly enhance the\nrobustness of RL-based control of chaotic flows, bringing real-world deployment\ncloser."
    },
    {
        "date": "2025-10",
        "title": "Security Audit of intel ICE Driver for e810 Network Interface Card",
        "author": "Oisin O Sullivan",
        "link": "http://arxiv.org/abs/2511.01910v2",
        "abstract": "The security of enterprise-grade networking hardware and software is critical\nto ensuring the integrity, availability, and confidentiality of data in modern\ncloud and data center environments. Network interface controllers (NICs) play a\npivotal role in high-performance computing and virtualization, but their\nprivileged access to system resources makes them a prime target for security\nvulnerabilities. This study presents a security analysis of the Intel ICE\ndriver using the E810 Ethernet Controller, employing static analysis, fuzz\ntesting, and timing-based side-channel evaluation to assess robustness against\nexploitation. The objective is to evaluate the drivers resilience to malformed\ninputs, identify implementation weaknesses, and determine whether timing\ndiscrepancies can be exploited for unauthorized inference of system states.\nStatic code analysis reveals that insufficient bounds checking and unsafe\nstring operations may introduce security flaws. Fuzz testing targets the Admin\nQueue, debugfs interface, and virtual function (VF) management. Interface-aware\nfuzzing and command mutation confirm strong input validation that prevents\nmemory corruption and privilege escalation under normal conditions. However,\nusing principles from KernelSnitch, the driver is found to be susceptible to\ntiming-based side-channel attacks. Execution time discrepancies in hash table\nlookups allow an unprivileged attacker to infer VF occupancy states, enabling\npotential network mapping in multi-tenant environments. Further analysis shows\ninefficiencies in Read-Copy-Update (RCU) synchronization, where missing\nsynchronization leads to stale data persistence, memory leaks, and\nout-of-memory conditions. Kernel instrumentation confirms that occupied VF\nlookups complete faster than unoccupied queries, exposing timing-based\ninformation leakage."
    },
    {
        "date": "2025-10",
        "title": "Diffusion LLMs are Natural Adversaries for any LLM",
        "author": "David L\u00fcdke, Tom Wollschl\u00e4ger, Paul Ungermann, Stephan G\u00fcnnemann, and Leo Schwinn",
        "link": "http://arxiv.org/abs/2511.00203v1",
        "abstract": "We introduce a novel framework that transforms the resource-intensive\n(adversarial) prompt optimization problem into an \\emph{efficient, amortized\ninference task}. Our core insight is that pretrained, non-autoregressive\ngenerative LLMs, such as Diffusion LLMs, which model the joint distribution\nover prompt-response pairs, can serve as powerful surrogates for prompt search.\nThis approach enables the direct conditional generation of prompts, effectively\nreplacing costly, per-instance discrete optimization with a small number of\nparallelizable samples. We provide a probabilistic analysis demonstrating that\nunder mild fidelity assumptions, only a few conditional samples are required to\nrecover high-reward (harmful) prompts. Empirically, we find that the generated\nprompts are low-perplexity, diverse jailbreaks that exhibit strong\ntransferability to a wide range of black-box target models, including robustly\ntrained and proprietary LLMs. Beyond adversarial prompting, our framework opens\nnew directions for red teaming, automated prompt optimization, and leveraging\nemerging Flow- and Diffusion-based LLMs."
    },
    {
        "date": "2025-10",
        "title": "Vision Transformer for Robust Occluded Person Reidentification in Complex Surveillance Scenes",
        "author": "Bo Li, Duyuan Zheng, Xinyang Liu, Qingwen Li, Hong Li, Hongyan Cui, Ge Gao, and Chen Liu",
        "link": "http://arxiv.org/abs/2510.27677v1",
        "abstract": "Person re-identification (ReID) in surveillance is challenged by occlusion,\nviewpoint distortion, and poor image quality. Most existing methods rely on\ncomplex modules or perform well only on clear frontal images. We propose Sh-ViT\n(Shuffling Vision Transformer), a lightweight and robust model for occluded\nperson ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a\nShuffle module in the final Transformer layer to break spatial correlations and\nenhance robustness to occlusion and blur; Second, scenario-adapted augmentation\n(geometric transforms, erasing, blur, and color adjustment) to simulate\nsurveillance conditions; Third, DeiT-based knowledge distillation to improve\nlearning with limited labels.To support real-world evaluation, we construct the\nMyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base\nstation inspections, with frequent equipment occlusion and camera variations.\nExperiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,\noutperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on\nMarket1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves\nrobustness to occlusion and blur without external modules, offering a practical\nsolution for surveillance-based personnel monitoring."
    },
    {
        "date": "2025-10",
        "title": "Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration",
        "author": "Tahmid Hasan Sakib, Yago Romano Martinez, Carter Brady, Syed Rafay Hasan, and Terry N. Guo",
        "link": "http://arxiv.org/abs/2511.00140v1",
        "abstract": "This paper presents a proof-of-concept supply chain attack against the Secure\nROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle\nplatform. A Trojan-infected Debian package modifies core ROS 2 security\ncommands to exfiltrate newly generated keystore credentials via DNS in\nbase64-encoded chunks to an attacker-controlled nameserver. Possession of these\ncredentials enables the attacker to rejoin the SROS 2 network as an\nauthenticated participant and publish spoofed control or perception messages\nwithout triggering authentication failures. We evaluate this capability on a\nsecure ROS 2 Humble testbed configured for a four-stop-sign navigation routine\nusing an Intel RealSense camera for perception. Experimental results show that\ncontrol-topic injections can cause forced braking, sustained high-speed\nacceleration, and continuous turning loops, while perception-topic spoofing can\ninduce phantom stop signs or suppress real detections. The attack generalizes\nto any data distribution service (DDS)-based robotic system using SROS 2,\nhighlighting the need for both supply chain integrity controls and runtime\nsemantic validation to safeguard autonomous systems against insider and\nimpersonation threats."
    },
    {
        "date": "2025-10",
        "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning",
        "author": "Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, Liang-Yan Gui, Yu-Xiong Wang, Huan Zhang, Heng Ji, and Daniel Kang",
        "link": "http://arxiv.org/abs/2510.27623v1",
        "abstract": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment."
    },
    {
        "date": "2025-10",
        "title": "ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning",
        "author": "Samarup Bhattacharya, Anubhab Bhattacharya, and Abir Chakraborty",
        "link": "http://arxiv.org/abs/2510.27599v1",
        "abstract": "Neural networks have changed the way machines interpret the world. At their\ncore, they learn by following gradients, adjusting their parameters step by\nstep until they identify the most discriminant patterns in the data. This\nprocess gives them their strength, yet it also opens the door to a hidden flaw.\nThe very gradients that help a model learn can also be used to produce small,\nimperceptible tweaks that cause the model to completely alter its decision.\nSuch tweaks are called adversarial attacks. These attacks exploit this\nvulnerability by adding tiny, imperceptible changes to images that, while\nleaving them identical to the human eye, cause the model to make wrong\npredictions. In this work, we propose Adversarially-trained Contrastive\nHard-mining for Optimized Robustness (ANCHOR), a framework that leverages the\npower of supervised contrastive learning with explicit hard positive mining to\nenable the model to learn representations for images such that the embeddings\nfor the images, their augmentations, and their perturbed versions cluster\ntogether in the embedding space along with those for other images of the same\nclass while being separated from images of other classes. This alignment helps\nthe model focus on stable, meaningful patterns rather than fragile gradient\ncues. On CIFAR-10, our approach achieves impressive results for both clean and\nrobust accuracy under PGD-20 (epsilon = 0.031), outperforming standard\nadversarial training methods. Our results indicate that combining adversarial\nguidance with hard-mined contrastive supervision helps models learn more\nstructured and robust representations, narrowing the gap between accuracy and\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds",
        "author": "Khandoker Ashik Uz Zaman, Mohammad Zahangir Alam, Mohammed N. M. Ali, and Mahdi H. Miraz",
        "link": "http://arxiv.org/abs/2510.27533v1",
        "abstract": "The protection of intellectual property has become critical due to the rapid\ngrowth of three-dimensional content in digital media. Unlike traditional images\nor videos, 3D point clouds present unique challenges for copyright enforcement,\nas they are especially vulnerable to a range of geometric and non-geometric\nattacks that can easily degrade or remove conventional watermark signals. In\nthis paper, we address these challenges by proposing a robust deep neural\nwatermarking framework for 3D point cloud copyright protection and ownership\nverification. Our approach embeds binary watermarks into the singular values of\n3D point cloud blocks using spectral decomposition, i.e. Singular Value\nDecomposition (SVD), and leverages the extraction capabilities of Deep Learning\nusing PointNet++ neural network architecture. The network is trained to\nreliably extract watermarks even after the data undergoes various attacks such\nas rotation, scaling, noise, cropping and signal distortions. We validated our\nmethod using the publicly available ModelNet40 dataset, demonstrating that deep\nlearning-based extraction significantly outperforms traditional SVD-based\ntechniques under challenging conditions. Our experimental evaluation\ndemonstrates that the deep learning-based extraction approach significantly\noutperforms existing SVD-based methods with deep learning achieving bitwise\naccuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD\nachieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,\nwhich is the most severe geometric distortion in our experiment. This\ndemonstrates our method's ability to achieve superior watermark recovery and\nmaintain high fidelity even under severe distortions."
    },
    {
        "date": "2025-10",
        "title": "A Hybrid Deep Learning and Forensic Approach for Robust Deepfake Detection",
        "author": "Sales Aribe Jr",
        "link": "http://arxiv.org/abs/2510.27392v1",
        "abstract": "The rapid evolution of generative adversarial networks (GANs) and diffusion\nmodels has made synthetic media increasingly realistic, raising societal\nconcerns around misinformation, identity fraud, and digital trust. Existing\ndeepfake detection methods either rely on deep learning, which suffers from\npoor generalization and vulnerability to distortions, or forensic analysis,\nwhich is interpretable but limited against new manipulation techniques. This\nstudy proposes a hybrid framework that fuses forensic features, including noise\nresiduals, JPEG compression traces, and frequency-domain descriptors, with deep\nlearning representations from convolutional neural networks (CNNs) and vision\ntransformers (ViTs). Evaluated on benchmark datasets (FaceForensics++, Celeb-DF\nv2, DFDC), the proposed model consistently outperformed single-method baselines\nand demonstrated superior performance compared to existing state-of-the-art\nhybrid approaches, achieving F1-scores of 0.96, 0.82, and 0.77, respectively.\nRobustness tests demonstrated stable performance under compression (F1 = 0.87\nat QF = 50), adversarial perturbations (AUC = 0.84), and unseen manipulations\n(F1 = 0.79). Importantly, explainability analysis showed that Grad-CAM and\nforensic heatmaps overlapped with ground-truth manipulated regions in 82\npercent of cases, enhancing transparency and user trust. These findings confirm\nthat hybrid approaches provide a balanced solution, combining the adaptability\nof deep models with the interpretability of forensic cues, to develop resilient\nand trustworthy deepfake detection systems."
    },
    {
        "date": "2025-10",
        "title": "Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services",
        "author": "Wenjie Liu, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2510.27346v1",
        "abstract": "With the rise of location-based service (LBS) applications that rely on\nterrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,\nBluetooth, cellular, and IP databases) for positioning, ensuring their\nintegrity and security is paramount. However, we demonstrate that these\napplications are susceptible to low-cost attacks (less than $50), including\nWi-Fi spoofing combined with GNSS jamming, as well as more sophisticated\ncoordinated location spoofing. These attacks manipulate position data to\ncontrol or undermine LBS functionality, leading to user scams or service\nmanipulation. Therefore, we propose a countermeasure to detect and thwart such\nattacks by utilizing readily available, redundant positioning information from\noff-the-shelf platforms. Our method extends the receiver autonomous integrity\nmonitoring (RAIM) framework by incorporating opportunistic information,\nincluding data from onboard sensors and terrestrial infrastructure signals,\nand, naturally, GNSS. We theoretically show that the fusion of heterogeneous\nsignals improves resilience against sophisticated adversaries on multiple\nfronts. Experimental evaluations show the effectiveness of the proposed scheme\nin improving detection accuracy by 62% at most compared to baseline schemes and\nrestoring accurate positioning."
    },
    {
        "date": "2025-10",
        "title": "Rethinking Robust Adversarial Concept Erasure in Diffusion Models",
        "author": "Qinghong Yin, Yu Tian, and Yue Zhang",
        "link": "http://arxiv.org/abs/2510.27285v1",
        "abstract": "Concept erasure aims to selectively unlearning undesirable content in\ndiffusion models (DMs) to reduce the risk of sensitive content generation. As a\nnovel paradigm in concept erasure, most existing methods employ adversarial\ntraining to identify and suppress target concepts, thus reducing the likelihood\nof sensitive outputs. However, these methods often neglect the specificity of\nadversarial training in DMs, resulting in only partial mitigation. In this\nwork, we investigate and quantify this specificity from the perspective of\nconcept space, i.e., can adversarial samples truly fit the target concept\nspace? We observe that existing methods neglect the role of conceptual\nsemantics when generating adversarial samples, resulting in ineffective fitting\nof concept spaces. This oversight leads to the following issues: 1) when there\nare few adversarial samples, they fail to comprehensively cover the object\nconcept; 2) conversely, they will disrupt other target concept spaces.\nMotivated by the analysis of these findings, we introduce S-GRACE\n(Semantics-Guided Robust Adversarial Concept Erasure), which grace leveraging\nsemantic guidance within the concept space to generate adversarial samples and\nperform erasure training. Experiments conducted with seven state-of-the-art\nmethods and three adversarial prompt generation strategies across various DM\nunlearning scenarios demonstrate that S-GRACE significantly improves erasure\nperformance 26%, better preserves non-target concepts, and reduces training\ntime by 90%. Our code is available at https://github.com/Qhong-522/S-GRACE."
    },
    {
        "date": "2025-10",
        "title": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents",
        "author": "Kathrin Grosse, and Nico Ebert",
        "link": "http://arxiv.org/abs/2510.27275v1",
        "abstract": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings."
    },
    {
        "date": "2025-10",
        "title": "C-LEAD: Contrastive Learning for Enhanced Adversarial Defense",
        "author": "Suklav Ghosh, Sonal Kumar, and Arijit Sur",
        "link": "http://arxiv.org/abs/2510.27249v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in computer\nvision tasks such as image classification, segmentation, and object detection.\nHowever, they are vulnerable to adversarial attacks, which can cause incorrect\npredictions with small perturbations in input images. Addressing this issue is\ncrucial for deploying robust deep-learning systems. This paper presents a novel\napproach that utilizes contrastive learning for adversarial defense, a\npreviously unexplored area. Our method leverages the contrastive loss function\nto enhance the robustness of classification models by training them with both\nclean and adversarially perturbed images. By optimizing the model's parameters\nalongside the perturbations, our approach enables the network to learn robust\nrepresentations that are less susceptible to adversarial attacks. Experimental\nresults show significant improvements in the model's robustness against various\ntypes of adversarial perturbations. This suggests that contrastive loss helps\nextract more informative and resilient features, contributing to the field of\nadversarial robustness in deep learning."
    },
    {
        "date": "2025-10",
        "title": "Trans-defense: Transformer-based Denoiser for Adversarial Defense with Spatial-Frequency Domain Representation",
        "author": "Alik Pramanick, Mayank Bansal, Utkarsh Srivastava, Suklav Ghosh, and Arijit Sur",
        "link": "http://arxiv.org/abs/2510.27245v1",
        "abstract": "In recent times, deep neural networks (DNNs) have been successfully adopted\nfor various applications. Despite their notable achievements, it has become\nevident that DNNs are vulnerable to sophisticated adversarial attacks,\nrestricting their applications in security-critical systems. In this paper, we\npresent two-phase training methods to tackle the attack: first, training the\ndenoising network, and second, the deep classifier model. We propose a novel\ndenoising strategy that integrates both spatial and frequency domain approaches\nto defend against adversarial attacks on images. Our analysis reveals that\nhigh-frequency components of attacked images are more severely corrupted\ncompared to their lower-frequency counterparts. To address this, we leverage\nDiscrete Wavelet Transform (DWT) for frequency analysis and develop a denoising\nnetwork that combines spatial image features with wavelets through a\ntransformer layer. Next, we retrain the classifier using the denoised images,\nwhich enhances the classifier's robustness against adversarial attacks.\nExperimental results across the MNIST, CIFAR-10, and Fashion-MNIST datasets\nreveal that the proposed method remarkably elevates classification accuracy,\nsubstantially exceeding the performance by utilizing a denoising network and\nadversarial training approaches. The code is available at\nhttps://github.com/Mayank94/Trans-Defense."
    },
    {
        "date": "2025-10",
        "title": "FedSM: Robust Semantics-Guided Feature Mixup for Bias Reduction in Federated Learning with Long-Tail Data",
        "author": "Jingrui Zhang, Yimeng Xu, Shujie Li, Feng Liang, Haihan Duan, Yanjie Dong, Victor C. M. Leung, and Xiping Hu",
        "link": "http://arxiv.org/abs/2510.27240v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients without sharing private data. However, FL suffers from\nbiased global models due to non-IID and long-tail data distributions. We\npropose \\textbf{FedSM}, a novel client-centric framework that mitigates this\nbias through semantics-guided feature mixup and lightweight classifier\nretraining. FedSM uses a pretrained image-text-aligned model to compute\ncategory-level semantic relevance, guiding the category selection of local\nfeatures to mix-up with global prototypes to generate class-consistent\npseudo-features. These features correct classifier bias, especially when data\nare heavily skewed. To address the concern of potential domain shift between\nthe pretrained model and the data, we propose probabilistic category selection,\nenhancing feature diversity to effectively mitigate biases. All computations\nare performed locally, requiring minimal server overhead. Extensive experiments\non long-tail datasets with various imbalanced levels demonstrate that FedSM\nconsistently outperforms state-of-the-art methods in accuracy, with high\nrobustness to domain shift and computational efficiency."
    },
    {
        "date": "2025-10",
        "title": "Analysis of Line Break prediction models for detecting defensive breakthrough in football",
        "author": "Shoma Yagi, Jun Ichikawa, and Genki Ichinose",
        "link": "http://arxiv.org/abs/2511.00121v1",
        "abstract": "In football, attacking teams attempt to break through the opponent's\ndefensive line to create scoring opportunities. This action, known as a Line\nBreak, is a critical indicator of offensive effectiveness and tactical\nperformance, yet previous studies have mainly focused on shots or goal\nopportunities rather than on how teams break the defensive line. In this study,\nwe develop a machine learning model to predict Line Breaks using event and\ntracking data from the 2023 J1 League season. The model incorporates 189\nfeatures, including player positions, velocities, and spatial configurations,\nand employs an XGBoost classifier to estimate the probability of Line Breaks.\nThe proposed model achieved high predictive accuracy, with an AUC of 0.982 and\na Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such\nas offensive player speed, gaps in the defensive line, and offensive players'\nspatial distributions significantly contribute to the occurrence of Line\nBreaks. Finally, we found a moderate positive correlation between the predicted\nprobability of being Line-Broken and the number of shots and crosses conceded\nat the team level. These results suggest that Line Breaks are closely linked to\nthe creation of scoring opportunities and provide a quantitative framework for\nunderstanding tactical dynamics in football."
    },
    {
        "date": "2025-10",
        "title": "Privacy-Aware Continual Self-Supervised Learning on Multi-Window Chest Computed Tomography for Domain-Shift Robustness",
        "author": "Ren Tasai, Guang Li, Ren Togo, Takahiro Ogawa, Kenji Hirata, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Noriko Nishioka, Yukie Shimizu, Kohsuke Kudo, and Miki Haseyama",
        "link": "http://arxiv.org/abs/2510.27213v1",
        "abstract": "We propose a novel continual self-supervised learning (CSSL) framework for\nsimultaneously learning diverse features from multi-window-obtained chest\ncomputed tomography (CT) images and ensuring data privacy. Achieving a robust\nand highly generalizable model in medical image diagnosis is challenging,\nmainly because of issues, such as the scarcity of large-scale, accurately\nannotated datasets and domain shifts inherent to dynamic healthcare\nenvironments. Specifically, in chest CT, these domain shifts often arise from\ndifferences in window settings, which are optimized for distinct clinical\npurposes. Previous CSSL frameworks often mitigated domain shift by reusing past\ndata, a typically impractical approach owing to privacy constraints. Our\napproach addresses these challenges by effectively capturing the relationship\nbetween previously learned knowledge and new information across different\ntraining stages through continual pretraining on unlabeled images.\nSpecifically, by incorporating a latent replay-based mechanism into CSSL, our\nmethod mitigates catastrophic forgetting due to domain shifts during continual\npretraining while ensuring data privacy. Additionally, we introduce a feature\ndistillation technique that integrates Wasserstein distance-based knowledge\ndistillation (WKD) and batch-knowledge ensemble (BKE), enhancing the ability of\nthe model to learn meaningful, domain-shift-robust representations. Finally, we\nvalidate our approach using chest CT images obtained across two different\nwindow settings, demonstrating superior performance compared with other\napproaches."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler",
        "author": "Zixuan Hu, Li Shen, Zhenyi Wang, Yongxian Wei, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2510.27172v1",
        "abstract": "Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service\nfor large language models. Existing defense strategies preemptively build\nrobustness via attack simulation but suffer from fundamental limitations: (i)\nthe infeasibility of extending attack simulations beyond bounded threat models\ndue to the inherent difficulty of anticipating unknown attacks, and (ii)\nlimited adaptability to varying attack settings, as simulation fails to capture\ntheir variability and complexity. To address these challenges, we propose\nBayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with\nno need for attack simulation. BDS formulates harmful fine-tuning defense as a\nBayesian inference problem, learning the posterior distribution of each data\npoint's safety attribute, conditioned on the fine-tuning and alignment\ndatasets. The fine-tuning process is then constrained by weighting data with\ntheir safety attributes sampled from the posterior, thus mitigating the\ninfluence of harmful data. By leveraging the post hoc nature of Bayesian\ninference, the posterior is conditioned on the fine-tuning dataset, enabling\nBDS to tailor its defense to the specific dataset, thereby achieving adaptive\ndefense. Furthermore, we introduce a neural scheduler based on amortized\nBayesian learning, enabling efficient transfer to new data without retraining.\nComprehensive results across diverse attack and defense settings demonstrate\nthe state-of-the-art performance of our approach. Code is available at\nhttps://github.com/Egg-Hu/Bayesian-Data-Scheduler."
    },
    {
        "date": "2025-10",
        "title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels",
        "author": "Chenghao Du, Quanfeng Huang, Tingxuan Tang, Zihao Wang, Adwait Nadkarni, and Yue Xiao",
        "link": "http://arxiv.org/abs/2510.27140v2",
        "abstract": "Large Language Models (LLMs) have transformed software development, enabling\nAI-powered applications known as LLM-based agents that promise to automate\ntasks across diverse apps and workflows. Yet, the security implications of\ndeploying such agents in adversarial mobile environments remain poorly\nunderstood. In this paper, we present the first systematic study of security\nrisks in mobile LLM agents. We design and evaluate a suite of adversarial case\nstudies, ranging from opportunistic manipulations such as pop-up advertisements\nto advanced, end-to-end workflows involving malware installation and cross-app\ndata exfiltration. Our evaluation covers eight state-of-the-art mobile agents\nacross three architectures, with over 2,000 adversarial and paired benign\ntrials. The results reveal systemic vulnerabilities: low-barrier vectors such\nas fraudulent ads succeed with over 80% reliability, while even workflows\nrequiring the circumvention of operating-system warnings, such as malware\ninstallation, are consistently completed by advanced multi-app agents. By\nmapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel\nprivilege-escalation and persistence pathways unique to LLM-driven automation.\nCollectively, our findings provide the first end-to-end evidence that mobile\nLLM agents are exploitable in realistic adversarial settings, where untrusted\nthird-party channels (e.g., ads, embedded webviews, cross-app notifications)\nare an inherent part of the mobile ecosystem."
    },
    {
        "date": "2025-10",
        "title": "MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials",
        "author": "Huseyin Goksu",
        "link": "http://arxiv.org/abs/2511.00113v1",
        "abstract": "Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results\nby defining graph convolutions in the spectral domain. A common approach,\npopularized by ChebyNet, is to use polynomial filters based on continuous\northogonal polynomials (e.g., Chebyshev). This creates a theoretical\ndisconnect, as these continuous-domain filters are applied to inherently\ndiscrete graph structures. We hypothesize this mismatch can lead to suboptimal\nperformance and fragility to hyperparameter settings.\n  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture\nthat employs discrete orthogonal polynomials -- specifically, the Meixner\npolynomials $M_k(x; \\beta, c)$. Our model makes the two key shape parameters of\nthe polynomial, beta and c, learnable, allowing the filter to adapt its\npolynomial basis to the specific spectral properties of a given graph. We\novercome the significant numerical instability of these polynomials by\nintroducing a novel stabilization technique that combines Laplacian scaling\nwith per-basis LayerNorm.\n  We demonstrate experimentally that MeixnerNet achieves\ncompetitive-to-superior performance against the strong ChebyNet baseline at the\noptimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we\nshow that MeixnerNet is exceptionally robust to variations in the polynomial\ndegree K, a hyperparameter to which ChebyNet proves to be highly fragile,\ncollapsing in performance where MeixnerNet remains stable."
    },
    {
        "date": "2025-10",
        "title": "Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget",
        "author": "Zhichao Hou, Weizhi Gao, and Xiaorui Liu",
        "link": "http://arxiv.org/abs/2510.26981v1",
        "abstract": "This work tackles a critical challenge in AI safety research under limited\ncompute: given a fixed computation budget, how can one maximize the strength of\niterative adversarial attacks? Coarsely reducing the number of attack\niterations lowers cost but substantially weakens effectiveness. To fulfill the\nattainable attack efficacy within a constrained budget, we propose a\nfine-grained control mechanism that selectively recomputes layer activations\nacross both iteration-wise and layer-wise levels. Extensive experiments show\nthat our method consistently outperforms existing baselines at equal cost.\nMoreover, when integrated into adversarial training, it attains comparable\nperformance with only 30% of the original budget."
    },
    {
        "date": "2025-10",
        "title": "SYNAPSE-Net: A Unified Framework with Lesion-Aware Hierarchical Gating for Robust Segmentation of Heterogeneous Brain Lesions",
        "author": "Md. Mehedi Hassan, Shafqat Alam, Shahriar Ahmed Seam, and Maruf Ahmed",
        "link": "http://arxiv.org/abs/2510.26961v1",
        "abstract": "Automated segmentation of heterogeneous brain lesions from multi-modal MRI\nremains a critical challenge in clinical neuroimaging. Current deep learning\nmodels are typically specialized `point solutions' that lack generalization and\nhigh performance variance, limiting their clinical reliability. To address\nthese gaps, we propose the Unified Multi-Stream SYNAPSE-Net, an adaptive\nframework designed for both generalization and robustness. The framework is\nbuilt on a novel hybrid architecture integrating multi-stream CNN encoders, a\nSwin Transformer bottleneck for global context, a dynamic cross-modal attention\nfusion (CMAF) mechanism, and a hierarchical gated decoder for high-fidelity\nmask reconstruction. The architecture is trained with a variance reduction\nstrategy that combines pathology specific data augmentation and\ndifficulty-aware sampling method. The model was evaluated on three different\nchallenging public datasets: the MICCAI 2017 WMH Challenge, the ISLES 2022\nChallenge, and the BraTS 2020 Challenge. Our framework attained a\nstate-of-the-art DSC value of 0.831 with the HD95 value of 3.03 in the WMH\ndataset. For ISLES 2022, it achieved the best boundary accuracy with a\nstatistically significant difference (HD95 value of 9.69). For BraTS 2020, it\nreached the highest DSC value for the tumor core region (0.8651). These\nexperimental findings suggest that our unified adaptive framework achieves\nstate-of-the-art performance across multiple brain pathologies, providing a\nrobust and clinically feasible solution for automated segmentation. The source\ncode and the pre-trained models are available at\nhttps://github.com/mubid-01/SYNAPSE-Net-pre."
    },
    {
        "date": "2025-10",
        "title": "LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks",
        "author": "Seif Ikbarieh, Maanak Gupta, and Elmahedi Mahalal",
        "link": "http://arxiv.org/abs/2510.26941v1",
        "abstract": "The Internet of Things has expanded rapidly, transforming communication and\noperations across industries but also increasing the attack surface and\nsecurity breaches. Artificial Intelligence plays a key role in securing IoT,\nenabling attack detection, attack behavior analysis, and mitigation suggestion.\nDespite advancements, evaluations remain purely qualitative, and the lack of a\nstandardized, objective benchmark for quantitatively measuring AI-based attack\nanalysis and mitigation hinders consistent assessment of model effectiveness.\nIn this work, we propose a hybrid framework combining Machine Learning (ML) for\nmulti-class attack detection with Large Language Models (LLMs) for attack\nbehavior analysis and mitigation suggestion. After benchmarking several ML and\nDeep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we\napplied structured role-play prompt engineering with Retrieval-Augmented\nGeneration (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,\ncontext-aware responses. We introduce novel evaluation metrics for quantitative\nassessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,\nDeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon\nH1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the\nresponses. Results show that Random Forest has the best detection model, and\nChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation."
    },
    {
        "date": "2025-10",
        "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models",
        "author": "Anushka Sivakumar, Andrew Zhang, Zaber Hakim, and Chris Thomas",
        "link": "http://arxiv.org/abs/2510.26769v1",
        "abstract": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering."
    },
    {
        "date": "2025-10",
        "title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation",
        "author": "Marios Impraimakis, and Evangelia Nektaria Palkanoglou",
        "link": "http://arxiv.org/abs/2511.00099v1",
        "abstract": "The optimization-based damage detection and damage state digital twinning\ncapabilities are examined here of a novel conditional-labeled generative\nadversarial network methodology. The framework outperforms current approaches\nfor fault anomaly detection as no prior information is required for the health\nstate of the system: a topic of high significance for real-world applications.\nSpecifically, current artificial intelligence-based digital twinning approaches\nsuffer from the uncertainty related to obtaining poor predictions when a low\nnumber of measurements is available, physics knowledge is missing, or when the\ndamage state is unknown. To this end, an unsupervised framework is examined and\nvalidated rigorously on the benchmark structural health monitoring measurements\nof Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In\nimplementing the approach, firstly, different same damage-level measurements\nare used as inputs, while the model is forced to converge conditionally to two\ndifferent damage states. Secondly, the process is repeated for a different\ngroup of measurements. Finally, the convergence scores are compared to identify\nwhich one belongs to a different damage state. The process for both\nhealthy-to-healthy and damage-to-healthy input data creates, simultaneously,\nmeasurements for digital twinning purposes at different damage states, capable\nof pattern recognition and machine learning data generation. Further to this\nprocess, a support vector machine classifier and a principal component analysis\nprocedure is developed to assess the generated and real measurements of each\ndamage category, serving as a secondary new dynamics learning indicator in\ndamage scenarios. Importantly, the approach is shown to capture accurately\ndamage over healthy measurements, providing a powerful tool for vibration-based\nsystem-level monitoring and scalable infrastructure resilience."
    },
    {
        "date": "2025-10",
        "title": "Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis",
        "author": "Nicholas Pecka, Lotfi Ben Othmane, and Renee Bryce",
        "link": "http://arxiv.org/abs/2510.26620v1",
        "abstract": "Threat modeling plays a critical role in the identification and mitigation of\nsecurity risks; however, manual approaches are often labor intensive and prone\nto error. This paper investigates the automation of software threat modeling\nthrough the clustering of call graphs using density-based and community\ndetection algorithms, followed by an analysis of the threats associated with\nthe identified clusters. The proposed method was evaluated through a case study\nof the Splunk Forwarder Operator (SFO), wherein selected clustering metrics\nwere applied to the software's call graph to assess pertinent code-density\nsecurity weaknesses. The results demonstrate the viability of the approach and\nunderscore its potential to facilitate systematic threat assessment. This work\ncontributes to the advancement of scalable, semi-automated threat modeling\nframeworks tailored for modern cloud-native environments."
    },
    {
        "date": "2025-10",
        "title": "A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication",
        "author": "Weixuan Chen, and Qianqian Yang",
        "link": "http://arxiv.org/abs/2510.26610v1",
        "abstract": "Semantic communication (SemCom) aims to transmit only task-relevant\ninformation, thereby improving communication efficiency but also exposing\nsemantic information to potential eavesdropping. In this paper, we propose a\ndeep reinforcement learning (DRL)-empowered multi-level jamming approach to\nenhance the security of SemCom systems over MIMO fading wiretap channels. This\napproach combines semantic layer jamming, achieved by encoding task-irrelevant\ntext, and physical layer jamming, achieved by encoding random Gaussian noise.\nThese two-level jamming signals are superposed with task-relevant semantic\ninformation to protect the transmitted semantics from eavesdropping. A deep\ndeterministic policy gradient (DDPG) algorithm is further introduced to\ndynamically design and optimize the precoding matrices for both taskrelevant\nsemantic information and multi-level jamming signals, aiming to enhance the\nlegitimate user's image reconstruction while degrading the eavesdropper's\nperformance. To jointly train the SemCom model and the DDPG agent, we propose\nan alternating optimization strategy where the two modules are updated\niteratively. Experimental results demonstrate that, compared with both the\nencryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method\nachieves comparable security while improving the legitimate user's peak\nsignalto-noise ratio (PSNR) by up to approximately 0.6 dB."
    },
    {
        "date": "2025-10",
        "title": "Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm",
        "author": "Vin\u00edcius Ferraria, and Eurico Ruivo",
        "link": "http://arxiv.org/abs/2510.26509v1",
        "abstract": "The edge detection task is essential in image processing aiming to extract\nrelevant information from an image. One recurring problem in this task is the\nweaknesses found in some detectors, such as the difficulty in detecting loose\nedges and the lack of context to extract relevant information from specific\nproblems. To address these weaknesses and adapt the detector to the properties\nof an image, an adaptable detector described by two-dimensional cellular\nautomaton and optimized by meta-heuristic combined with transfer learning\ntechniques was developed. This study aims to analyze the impact of expanding\nthe search space of the optimization phase and the robustness of the\nadaptability of the detector in identifying edges of a set of natural images\nand specialized subsets extracted from the same image set. The results obtained\nprove that expanding the search space of the optimization phase was not\neffective for the chosen image set. The study also analyzed the adaptability of\nthe model through a series of experiments and validation techniques and found\nthat, regardless of the validation, the model was able to adapt to the input\nand the transfer learning techniques applied to the model showed no significant\nimprovements."
    },
    {
        "date": "2025-10",
        "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters",
        "author": "Mustafa Fuad Rifet Ibrahim, Maurice Meijer, Alexander Schlaefer, and Peer Stelldinger",
        "link": "http://arxiv.org/abs/2510.26501v1",
        "abstract": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables."
    },
    {
        "date": "2025-10",
        "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
        "author": "Fang Liu, Simiao Liu, Yinghao Zhu, Xiaoli Lian, and Li Zhang",
        "link": "http://arxiv.org/abs/2510.26457v1",
        "abstract": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments."
    },
    {
        "date": "2025-10",
        "title": "Robust Graph Condensation via Classification Complexity Mitigation",
        "author": "Jiayi Luo, Qingyun Sun, Beining Yang, Haonan Yuan, Xingcheng Fu, Yanbiao Ma, Jianxin Li, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2510.26451v1",
        "abstract": "Graph condensation (GC) has gained significant attention for its ability to\nsynthesize smaller yet informative graphs. However, existing studies often\noverlook the robustness of GC in scenarios where the original graph is\ncorrupted. In such cases, we observe that the performance of GC deteriorates\nsignificantly, while existing robust graph learning technologies offer only\nlimited effectiveness. Through both empirical investigation and theoretical\nanalysis, we reveal that GC is inherently an intrinsic-dimension-reducing\nprocess, synthesizing a condensed graph with lower classification complexity.\nAlthough this property is critical for effective GC performance, it remains\nhighly vulnerable to adversarial perturbations. To tackle this vulnerability\nand improve GC robustness, we adopt the geometry perspective of graph data\nmanifold and propose a novel Manifold-constrained Robust Graph Condensation\nframework named MRGC. Specifically, we introduce three graph data manifold\nlearning modules that guide the condensed graph to lie within a smooth,\nlow-dimensional manifold with minimal class ambiguity, thereby preserving the\nclassification complexity reduction capability of GC and ensuring robust\nperformance under universal adversarial attacks. Extensive experiments\ndemonstrate the robustness of \\ModelName\\ across diverse attack scenarios."
    },
    {
        "date": "2025-10",
        "title": "Multi-Output Robust and Conjugate Gaussian Processes",
        "author": "Joshua Rooijakkers, Leiv R\u00f8nneberg, Fran\u00e7ois-Xavier Briol, Jeremias Knoblauch, and Matias Altamirano",
        "link": "http://arxiv.org/abs/2510.26401v1",
        "abstract": "Multi-output Gaussian process (MOGP) regression allows modelling dependencies\namong multiple correlated response variables. Similarly to standard Gaussian\nprocesses, MOGPs are sensitive to model misspecification and outliers, which\ncan distort predictions within individual outputs. This situation can be\nfurther exacerbated by multiple anomalous response variables whose errors\npropagate due to correlations between outputs. To handle this situation, we\nextend and generalise the robust and conjugate Gaussian process (RCGP)\nframework introduced by Altamirano et al. (2024). This results in the\nmulti-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and\njointly captures correlations across outputs. We thoroughly evaluate our\napproach through applications in finance and cancer research."
    },
    {
        "date": "2025-10",
        "title": "maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition",
        "author": "Kexing Liu",
        "link": "http://arxiv.org/abs/2510.26146v1",
        "abstract": "WiFi Channel State Information (CSI)-based human activity recognition (HAR)\nprovides a privacy-preserving, device-free sensing solution for smart\nenvironments. However, its deployment on edge devices is severely constrained\nby domain shift, where recognition performance deteriorates under varying\nenvironmental and hardware conditions. This study presents maxVSTAR (maximally\nadaptive Vision-guided Sensing Technology for Activity Recognition), a\nclosed-loop, vision-guided model adaptation framework that autonomously\nmitigates domain shift for edge-deployed CSI sensing systems. The proposed\nsystem integrates a cross-modal teacher-student architecture, where a\nhigh-accuracy YOLO-based vision model serves as a dynamic supervisory signal,\ndelivering real-time activity labels for the CSI data stream. These labels\nenable autonomous, online fine-tuning of a lightweight CSI-based HAR model,\ntermed Sensing Technology for Activity Recognition (STAR), directly at the\nedge. This closed-loop retraining mechanism allows STAR to continuously adapt\nto environmental changes without manual intervention. Extensive experiments\ndemonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated\nhardware, the baseline STAR model's recognition accuracy declined from 93.52%\nto 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored\nthe accuracy to 81.51%. These results confirm the system's capacity for\ndynamic, self-supervised model adaptation in privacy-conscious IoT\nenvironments, establishing a scalable and practical paradigm for long-term\nautonomous HAR using CSI sensing at the network edge."
    },
    {
        "date": "2025-10",
        "title": "Security Risk of Misalignment between Text and Image in Multi-modal Model",
        "author": "Xiaosen Wang, Zhijin Ge, and Shaokang Wang",
        "link": "http://arxiv.org/abs/2510.26105v1",
        "abstract": "Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA."
    },
    {
        "date": "2025-10",
        "title": "Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories",
        "author": "Maximilian Schreiber, and Pascal Tippe",
        "link": "http://arxiv.org/abs/2510.26103v1",
        "abstract": "This paper presents a comprehensive empirical analysis of security\nvulnerabilities in AI-generated code across public GitHub repositories. We\ncollected and analyzed 7,703 files explicitly attributed to four major AI\ntools: ChatGPT (91.52\\%), GitHub Copilot (7.50\\%), Amazon CodeWhisperer\n(0.52\\%), and Tabnine (0.46\\%). Using CodeQL static analysis, we identified\n4,241 Common Weakness Enumeration (CWE) instances across 77 distinct\nvulnerability types. Our findings reveal that while 87.9\\% of AI-generated code\ndoes not contain identifiable CWE-mapped vulnerabilities, significant patterns\nemerge regarding language-specific vulnerabilities and tool performance. Python\nconsistently exhibited higher vulnerability rates (16.18\\%-18.50\\%) compared to\nJavaScript (8.66\\%-8.99\\%) and TypeScript (2.50\\%-7.14\\%) across all tools. We\nobserved notable differences in security performance, with GitHub Copilot\nachieving better security density for Python (1,739 LOC per CWE) and\nTypeScript, while ChatGPT performed better for JavaScript. Additionally, we\ndiscovered widespread use of AI tools for documentation generation (39\\% of\ncollected files), an understudied application with implications for software\nmaintainability. These findings extend previous work with a significantly\nlarger dataset and provide valuable insights for developing language-specific\nand context-aware security practices for the responsible integration of\nAI-generated code into software development workflows."
    },
    {
        "date": "2025-10",
        "title": "Robust Super-Capacity SRS Channel Inpainting via Diffusion Models",
        "author": "Usman Akram, Fan Zhang, Yang Li, and Haris Vikalo",
        "link": "http://arxiv.org/abs/2510.26097v1",
        "abstract": "Accurate channel state information (CSI) is essential for reliable multiuser\nMIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding\nReference Signals (SRS) face resource and coverage constraints, motivating\nsparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches\nimprove coverage but overfit to training masks and degrade under unseen\ndistortions (e.g., additional masking, interference, clipping, non-Gaussian\nnoise). We propose a diffusion-based channel inpainting framework that\nintegrates system-model knowledge at inference via a likelihood-gradient term,\nenabling a single trained model to adapt across mismatched conditions. On\nstandardized CDL channels, the score-based diffusion variant consistently\noutperforms a UNet score-model baseline and the one-step MAE under distribution\nshift, with improvements up to 14 dB NMSE in challenging settings (e.g.,\nLaplace noise, user interference), while retaining competitive accuracy under\nmatched conditions. These results demonstrate that diffusion-guided inpainting\nis a robust and generalizable approach for super-capacity SRS design in 5G NR\nsystems."
    },
    {
        "date": "2025-10",
        "title": "Message Recovery Attack in NTRU via Knapsack",
        "author": "Eirini Poimenidou, and K. A. Draziotis",
        "link": "http://arxiv.org/abs/2510.26003v1",
        "abstract": "In the present paper, we introduce a message-recovery attack based on the\nModular Knapsack Problem, applicable to all variants of the NTRU-HPS\ncryptosystem. Assuming that a fraction $\\epsilon$ of the coefficients of the\nmessage ${\\bf{m}}\\in\\{-1,0,1\\}^N$ and of the nonce vector ${\\bf\nr}\\in\\{-1,0,1\\}^N$ are known in advance at random positions, we reduce message\ndecryption to finding a short vector in a lattice that encodes an instance of a\nmodular knapsack system. This allows us to address a key question: how much\ninformation about ${\\bf m}$, or about the pair $({\\bf m},{\\bf r})$, is required\nbefore recovery becomes feasible? A FLATTER reduction successfully recovers the\nmessage, in practice when $\\epsilon\\approx 0.45$. Our implementation finds\n${\\bf m}$ within a few minutes on a commodity desktop."
    },
    {
        "date": "2025-10",
        "title": "Fixed-point graph convolutional networks against adversarial attacks",
        "author": "Shakib Khan, A. Ben Hamza, and Amr Youssef",
        "link": "http://arxiv.org/abs/2511.00083v1",
        "abstract": "Adversarial attacks present a significant risk to the integrity and\nperformance of graph neural networks, particularly in tasks where graph\nstructure and node features are vulnerable to manipulation. In this paper, we\npresent a novel model, called fixed-point iterative graph convolutional network\n(Fix-GCN), which achieves robustness against adversarial perturbations by\neffectively capturing higher-order node neighborhood information in the graph\nwithout additional memory or computational complexity. Specifically, we\nintroduce a versatile spectral modulation filter and derive the feature\npropagation rule of our model using fixed-point iteration. Unlike traditional\ndefense mechanisms that rely on additional design elements to counteract\nattacks, the proposed graph filter provides a flexible-pass filtering approach,\nallowing it to selectively attenuate high-frequency components while preserving\nlow-frequency structural information in the graph signal. By iteratively\nupdating node representations, our model offers a flexible and efficient\nframework for preserving essential graph information while mitigating the\nimpact of adversarial manipulation. We demonstrate the effectiveness of the\nproposed model through extensive experiments on various benchmark graph\ndatasets, showcasing its resilience against adversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "Robust GNN Watermarking via Implicit Perception of Topological Invariants",
        "author": "Jipeng Li, and Yannning Shen",
        "link": "http://arxiv.org/abs/2510.25934v1",
        "abstract": "Graph Neural Networks (GNNs) are valuable intellectual property, yet many\nwatermarks rely on backdoor triggers that break under common model edits and\ncreate ownership ambiguity. We present InvGNN-WM, which ties ownership to a\nmodel's implicit perception of a graph invariant, enabling trigger-free,\nblack-box verification with negligible task impact. A lightweight head predicts\nnormalized algebraic connectivity on an owner-private carrier set; a\nsign-sensitive decoder outputs bits, and a calibrated threshold controls the\nfalse-positive rate. Across diverse node and graph classification datasets and\nbackbones, InvGNN-WM matches clean accuracy while yielding higher watermark\naccuracy than trigger- and compression-based baselines. It remains strong under\nunstructured pruning, fine-tuning, and post-training quantization; plain\nknowledge distillation (KD) weakens the mark, while KD with a watermark loss\n(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,\nand we prove that exact removal is NP-complete."
    },
    {
        "date": "2025-10",
        "title": "FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring",
        "author": "Hanie Vatani, and Reza Ebrahimi Atani",
        "link": "http://arxiv.org/abs/2511.01898v1",
        "abstract": "Federated Learning (FL) enables collaborative model training without sharing\nraw data but suffers from limited scalability, high communication costs, and\nprivacy risks due to its centralized architecture. This paper proposes\nFedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,\nsecurity, and energy efficiency. Multiple edge servers distribute workloads and\nperform score-based client selection, prioritizing participants based on\nutility, energy efficiency, and data sensitivity. Secure Aggregation with\nHomomorphic Encryption and Differential Privacy protects model updates from\nexposure and manipulation. Evaluated on the eICU healthcare dataset,\nFedSelect-ME achieves higher prediction accuracy, improved fairness across\nregions, and reduced communication overhead compared to FedAvg, FedProx, and\nFedSelect. The results demonstrate that the proposed framework effectively\naddresses the bottlenecks of conventional FL, offering a secure, scalable, and\nefficient solution for large-scale, privacy-sensitive healthcare applications."
    },
    {
        "date": "2025-10",
        "title": "VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes",
        "author": "Simon Yu, Peilin Yu, Hongbo Zheng, Huajie Shao, Han Zhao, and Lui Sha",
        "link": "http://arxiv.org/abs/2510.26833v1",
        "abstract": "We present VISAT, a novel open dataset and benchmarking suite for evaluating\nmodel robustness in the task of traffic sign recognition with the presence of\nvisual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our\ndataset introduces two benchmarks that respectively emphasize robustness\nagainst adversarial attacks and distribution shifts. For our adversarial attack\nbenchmark, we employ the state-of-the-art Projected Gradient Descent (PGD)\nmethod to generate adversarial inputs and evaluate their impact on popular\nmodels. Additionally, we investigate the effect of adversarial attacks on\nattribute-specific multi-task learning (MTL) networks, revealing spurious\ncorrelations among MTL tasks. The MTL networks leverage visual attributes\n(color, shape, symbol, and text) that we have created for each traffic sign in\nour dataset. For our distribution shift benchmark, we utilize ImageNet-C's\nrealistic data corruption and natural variation techniques to perform\nevaluations on the robustness of both base and MTL models. Moreover, we further\nexplore spurious correlations among MTL tasks through synthetic alterations of\ntraffic sign colors using color quantization techniques. Our experiments focus\non two major backbones, ResNet-152 and ViT-B/32, and compare the performance\nbetween base and MTL models. The VISAT dataset and benchmarking framework\ncontribute to the understanding of model robustness for traffic sign\nrecognition, shedding light on the challenges posed by adversarial attacks and\ndistribution shifts. We believe this work will facilitate advancements in\ndeveloping more robust models for real-world applications in autonomous driving\nand cyber-physical systems."
    },
    {
        "date": "2025-10",
        "title": "Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world",
        "author": "Tobin South, Subramanya Nagabhushanaradhya, Ayesha Dissanayaka, Sarah Cecchetti, George Fletcher, Victor Lu, Aldo Pietropaolo, Dean H. Saxe, Jeff Lombardo, Abhishek Maligehalli Shivalingaiah, Stan Bounev, Alex Keisner, Andor Kesselman, Zack Proser, Ginny Fahs, Andrew Bunyea, Ben Moskowitz, Atul Tulshibagwale, Dazza Greenwood, Jiaxin Pei, and Alex Pentland",
        "link": "http://arxiv.org/abs/2510.25819v1",
        "abstract": "The rapid rise of AI agents presents urgent challenges in authentication,\nauthorization, and identity management. Current agent-centric protocols (like\nMCP) highlight the demand for clarified best practices in authentication and\nauthorization. Looking ahead, ambitions for highly autonomous agents raise\ncomplex long-term questions regarding scalable access control, agent-centric\nidentities, AI workload differentiation, and delegated authority. This OpenID\nFoundation whitepaper is for stakeholders at the intersection of AI agents and\naccess management. It outlines the resources already available for securing\ntoday's agents and presents a strategic agenda to address the foundational\nauthentication, authorization, and identity problems pivotal for tomorrow's\nwidespread autonomous systems."
    },
    {
        "date": "2025-10",
        "title": "Model Inversion Attacks Meet Cryptographic Fuzzy Extractors",
        "author": "Mallika Prabhakar, Louise Xu, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2510.25687v1",
        "abstract": "Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack."
    },
    {
        "date": "2025-10",
        "title": "Robust variable selection for spatial point processes observed with noise",
        "author": "Dominik Sturm, and Ivo F. Sbalzarini",
        "link": "http://arxiv.org/abs/2510.25550v1",
        "abstract": "We propose a method for variable selection in the intensity function of\nspatial point processes that combines sparsity-promoting estimation with\nnoise-robust model selection. As high-resolution spatial data becomes\nincreasingly available through remote sensing and automated image analysis,\nidentifying spatial covariates that influence the localization of events is\ncrucial to understand the underlying mechanism. However, results from automated\nacquisition techniques are often noisy, for example due to measurement\nuncertainties or detection errors, which leads to spurious displacements and\nmissed events. We study the impact of such noise on sparse point-process\nestimation across different models, including Poisson and Thomas processes. To\nimprove noise robustness, we propose to use stability selection based on\npoint-process subsampling and to incorporate a non-convex best-subset penalty\nto enhance model-selection performance. In extensive simulations, we\ndemonstrate that such an approach reliably recovers true covariates under\ndiverse noise scenarios and improves both selection accuracy and stability. We\nthen apply the proposed method to a forestry data set, analyzing the\ndistribution of trees in relation to elevation and soil nutrients in a tropical\nrain forest. This shows the practical utility of the method, which provides a\nsystematic framework for robust variable selection in spatial point-process\nmodels under noise, without requiring additional knowledge of the process."
    },
    {
        "date": "2025-10",
        "title": "An In-Depth Analysis of Cyber Attacks in Secured Platforms",
        "author": "Parick Ozoh, John K Omoniyi, and Bukola Ibitoye",
        "link": "http://arxiv.org/abs/2510.25470v1",
        "abstract": "There is an increase in global malware threats. To address this, an\nencryption-type ransomware has been introduced on the Android operating system.\nThe challenges associated with malicious threats in phone use have become a\npressing issue in mobile communication, disrupting user experiences and posing\nsignificant privacy threats. This study surveys commonly used machine learning\ntechniques for detecting malicious threats in phones and examines their\nperformance. The majority of past research focuses on customer feedback and\nreviews, with concerns that people might create false reviews to promote or\ndevalue products and services for personal gain. Hence, the development of\ntechniques for detecting malicious threats using machine learning has been a\nkey focus. This paper presents a comprehensive comparative study of current\nresearch on the issue of malicious threats and methods for tackling these\nchallenges. Nevertheless, a huge amount of information is required by these\nmethods, presenting a challenge for developing robust, specialized automated\nanti-malware systems. This research describes the Android Applications dataset,\nand the accuracy of the techniques is measured using the accuracy levels of the\nmetrics employed in this study."
    },
    {
        "date": "2025-10",
        "title": "From ECU to VSOC: UDS Security Monitoring Strategies",
        "author": "Ali Recai Yekta, Nicolas Loza, Jens Gramm, Michael Peter Schneider, and Stefan Katzenbeisser",
        "link": "http://arxiv.org/abs/2510.25375v1",
        "abstract": "Increasing complexity and connectivity of modern vehicles have heightened\ntheir vulnerability to cyberattacks. This paper addresses security challenges\nassociated with the Unified Diagnostic Services (UDS) protocol, a critical\ncommunication framework for vehicle diagnostics in the automotive industry. We\npresent security monitoring strategies for the UDS protocol that leverage\nin-vehicle logging and remote analysis through a Vehicle Security Operations\nCenter (VSOC). Our approach involves specifying security event logging\nrequirements, contextual data collection, and the development of detection\nstrategies aimed at identifying UDS attack scenarios. By applying these\nstrategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate\nthat our detection methods cover a wide range of potential attack vectors.\nFurthermore, we assess the adequacy of current AUTOSAR standardized security\nevents in supporting UDS attack detection, identifying gaps in the current\nstandard. This work enhances the understanding of vehicle security monitoring\nand provides an example for developing robust cybersecurity measures in\nautomotive communication protocols."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers",
        "author": "Quanliang Jing, Xinxin Fan, Yanyan Liu, and Jingping Bi",
        "link": "http://arxiv.org/abs/2510.25810v1",
        "abstract": "To date, traffic obfuscation techniques have been widely adopted to protect\nnetwork data privacy and security by obscuring the true patterns of traffic.\nNevertheless, as the pre-trained models emerge, especially transformer-based\nclassifiers, existing traffic obfuscation methods become increasingly\nvulnerable, as witnessed by current studies reporting the traffic\nclassification accuracy up to 99\\% or higher. To counter such high-performance\ntransformer-based classification models, we in this paper propose a novel and\neffective \\underline{adv}ersarial \\underline{traffic}-generating approach\n(AdvTraffic\\footnote{The code and data are available at: http://xxx}). Our\napproach has two key innovations: (i) a pre-padding strategy is proposed to\nmodify packets, which effectively overcomes the limitations of existing\nresearch against transformer-based models for network traffic classification;\nand (ii) a reinforcement learning model is employed to optimize network traffic\nperturbations, aiming to maximize adversarial effectiveness against\ntransformer-based classification models. To the best of our knowledge, this is\nthe first attempt to apply adversarial perturbation techniques to defend\nagainst transformer-based traffic classifiers. Furthermore, our method can be\neasily deployed into practical network environments. Finally, multi-faceted\nexperiments are conducted across several real-world datasets, and the\nexperimental results demonstrate that our proposed method can effectively\nundermine transformer-based classifiers, significantly reducing classification\naccuracy from 99\\% to as low as 25.68\\%."
    },
    {
        "date": "2025-10",
        "title": "MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo",
        "author": "Shiyu Qin, Zhihao Cai, Kaixuan Wang, Lin Qi, and Junyu Dong",
        "link": "http://arxiv.org/abs/2510.25221v1",
        "abstract": "Photometric stereo is a technique aimed at determining surface normals\nthrough the utilization of shading cues derived from images taken under\ndifferent lighting conditions. However, existing learning-based approaches\noften fail to accurately capture features at multiple stages and do not\nadequately promote interaction between these features. Consequently, these\nmodels tend to extract redundant features, especially in areas with intricate\ndetails such as wrinkles and edges. To tackle these issues, we propose MSF-Net,\na novel framework for extracting information at multiple stages, paired with\nselective update strategy, aiming to extract high-quality feature information,\nwhich is critical for accurate normal construction. Additionally, we have\ndeveloped a feature fusion module to improve the interplay among different\nfeatures. Experimental results on the DiLiGenT benchmark show that our proposed\nMSF-Net significantly surpasses previous state-of-the-art methods in the\naccuracy of surface normal estimation."
    },
    {
        "date": "2025-10",
        "title": "Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision",
        "author": "Yuyang Xia, Zibo Liang, Liwei Deng, Yan Zhao, Han Su, and Kai Zheng",
        "link": "http://arxiv.org/abs/2510.25205v1",
        "abstract": "Autonomous driving is an emerging technology that is expected to bring\nsignificant social, economic, and environmental benefits. However, these\nbenefits come with rising energy consumption by computation engines, limiting\nthe driving range of vehicles, especially electric ones. Perception computing\nis typically the most power-intensive component, as it relies on largescale\ndeep learning models to extract environmental features. Recently, numerous\nstudies have employed model compression techniques, such as sparsification,\nquantization, and distillation, to reduce computational consumption. However,\nthese methods often result in either a substantial model size or a significant\ndrop in perception accuracy compared to high-computation models. To address\nthese challenges, we propose an energy-efficient autonomous driving framework,\ncalled EneAD. In the adaptive perception module, a perception optimization\nstrategy is designed from the perspective of data management and tuning.\nFirstly, we manage multiple perception models with different computational\nconsumption and adjust the execution framerate dynamically. Then, we define\nthem as knobs and design a transferable tuning method based on Bayesian\noptimization to identify promising knob values that achieve low computation\nwhile maintaining desired accuracy. To adaptively switch the knob values in\nvarious traffic scenarios, a lightweight classification model is proposed to\ndistinguish the perception difficulty in different scenarios. In the robust\ndecision module, we propose a decision model based on reinforcement learning\nand design a regularization term to enhance driving stability in the face of\nperturbed perception results. Extensive experiments evidence the superiority of\nour framework in both energy consumption and driving performance. EneAD can\nreduce perception consumption by 1.9x to 3.5x and thus improve driving range by\n3.9% to 8.5%"
    },
    {
        "date": "2025-10",
        "title": "Mask-Robust Face Verification for Online Learning via YOLOv5 and Residual Networks",
        "author": "Zhifeng Wang, Minghui Wang, Chunyan Zeng, Jialong Yao, Yang Yang, and Hongmin Xu",
        "link": "http://arxiv.org/abs/2510.25184v1",
        "abstract": "In the contemporary landscape, the fusion of information technology and the\nrapid advancement of artificial intelligence have ushered school education into\na transformative phase characterized by digitization and heightened\nintelligence. Concurrently, the global paradigm shift caused by the Covid-19\npandemic has catalyzed the evolution of e-learning, accentuating its\nsignificance. Amidst these developments, one pivotal facet of the online\neducation paradigm that warrants attention is the authentication of identities\nwithin the digital learning sphere. Within this context, our study delves into\na solution for online learning authentication, utilizing an enhanced\nconvolutional neural network architecture, specifically the residual network\nmodel. By harnessing the power of deep learning, this technological approach\naims to galvanize the ongoing progress of online education, while concurrently\nbolstering its security and stability. Such fortification is imperative in\nenabling online education to seamlessly align with the swift evolution of the\neducational landscape. This paper's focal proposition involves the deployment\nof the YOLOv5 network, meticulously trained on our proprietary dataset. This\nnetwork is tasked with identifying individuals' faces culled from images\ncaptured by students' open online cameras. The resultant facial information is\nthen channeled into the residual network to extract intricate features at a\ndeeper level. Subsequently, a comparative analysis of Euclidean distances\nagainst students' face databases is performed, effectively ascertaining the\nidentity of each student."
    },
    {
        "date": "2025-10",
        "title": "Lipschitz-aware Linearity Grafting for Certified Robustness",
        "author": "Yongjin Han, and Suhyun Kim",
        "link": "http://arxiv.org/abs/2510.25130v1",
        "abstract": "Lipschitz constant is a fundamental property in certified robustness, as\nsmaller values imply robustness to adversarial examples when a model is\nconfident in its prediction. However, identifying the worst-case adversarial\nexamples is known to be an NP-complete problem. Although over-approximation\nmethods have shown success in neural network verification to address this\nchallenge, reducing approximation errors remains a significant obstacle.\nFurthermore, these approximation errors hinder the ability to obtain tight\nlocal Lipschitz constants, which are crucial for certified robustness.\nOriginally, grafting linearity into non-linear activation functions was\nproposed to reduce the number of unstable neurons, enabling scalable and\ncomplete verification. However, no prior theoretical analysis has explained how\nlinearity grafting improves certified robustness. We instead consider linearity\ngrafting primarily as a means of eliminating approximation errors rather than\nreducing the number of unstable neurons, since linear functions do not require\nrelaxation. In this paper, we provide two theoretical contributions: 1) why\nlinearity grafting improves certified robustness through the lens of the\n$l_\\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear\nactivation functions, the dominant source of approximation errors, yields a\ntighter local Lipschitz constant. Based on these theoretical contributions, we\npropose a Lipschitz-aware linearity grafting method that removes dominant\napproximation errors, which are crucial for tightening the local Lipschitz\nconstant, thereby improving certified robustness, even without certified\ntraining. Our extensive experiments demonstrate that grafting linearity into\nthese influential activations tightens the $l_\\infty$ local Lipschitz constant\nand enhances certified robustness."
    },
    {
        "date": "2025-10",
        "title": "A Unified Bilevel Model for Adversarial Learning and A Case Study",
        "author": "Yutong Zheng, and Qingna Li",
        "link": "http://arxiv.org/abs/2510.25121v1",
        "abstract": "Adversarial learning has been attracting more and more attention thanks to\nthe fast development of machine learning and artificial intelligence. However,\ndue to the complicated structure of most machine learning models, the mechanism\nof adversarial attacks is not well interpreted. How to measure the effect of\nattack is still not quite clear. In this paper, we propose a unified bilevel\nmodel for adversarial learning. We further investigate the adversarial attack\nin clustering models and interpret it from data perturbation point of view. We\nreveal that when the data perturbation is relatively small, the clustering\nmodel is robust, whereas if it is relatively large, the clustering result\nchanges, which leads to an attack. To measure the effect of attacks for\nclustering models, we analyse the well-definedness of the so-called\n$\\delta$-measure, which can be used in the proposed bilevel model for\nadversarial learning of clustering models."
    },
    {
        "date": "2025-10",
        "title": "Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios",
        "author": "Sarmad Ahmad Khan, Simon Bernatz, Zahra Moslehi, and Florian Buettner",
        "link": "http://arxiv.org/abs/2510.25026v1",
        "abstract": "Radiomics-based machine learning models show promise for clinical decision\nsupport but are vulnerable to distribution shifts caused by variations in\nimaging protocols, positioning, and segmentation. This study systematically\ninvestigates the robustness of radiomics-based machine learning models under\ndistribution shifts across five MRI sequences. We evaluated how different\nacquisition protocols and segmentation strategies affect model reliability in\nterms of predictive power and uncertainty-awareness. Using a phantom of 16\nfruits, we evaluated distribution shifts through: (1) protocol variations\nacross T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)\nsegmentation variations (full, partial, rotated); and (3) inter-observer\nvariability. We trained XGBoost classifiers on 8 consistent robust features\nversus sequence-specific features, testing model performance under in-domain\nand out-of-domain conditions. Results demonstrate that models trained on\nprotocol-invariant features maintain F1-scores >0.85 across distribution\nshifts, while models using all features showed 40% performance degradation\nunder protocol changes. Dataset augmentation substantially improved the quality\nof uncertainty estimates and reduced the expected calibration error (ECE) by\n35% without sacrificing accuracy. Temperature scaling provided minimal\ncalibration benefits, confirming XGBoost's inherent reliability. Our findings\nreveal that protocol-aware feature selection and controlled phantom studies\neffectively predict model behavior under distribution shifts, providing a\nframework for developing robust radiomics models resilient to real-world\nprotocol variations."
    },
    {
        "date": "2025-10",
        "title": "Secure Retrieval-Augmented Generation against Poisoning Attacks",
        "author": "Zirui Cheng, Jikai Sun, Anjun Gao, Yueyang Quan, Zhuqing Liu, Xiaohua Hu, and Minghong Fang",
        "link": "http://arxiv.org/abs/2510.25025v1",
        "abstract": "Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks."
    },
    {
        "date": "2025-10",
        "title": "SLIP-SEC: Formalizing Secure Protocols for Model IP Protection",
        "author": "Racchit Jain, Satya Lokam, Yehonathan Refael, Adam Hakim, Lev Greenberg, and Jay Tenenbaum",
        "link": "http://arxiv.org/abs/2510.24999v1",
        "abstract": "Large Language Models (LLMs) represent valuable intellectual property (IP),\nreflecting significant investments in training data, compute, and expertise.\nDeploying these models on partially trusted or insecure devices introduces\nsubstantial risk of model theft, making it essential to design inference\nprotocols with provable security guarantees.\n  We present the formal framework and security foundations of SLIP, a hybrid\ninference protocol that splits model computation between a trusted and an\nuntrusted resource. We define and analyze the key notions of model\ndecomposition and hybrid inference protocols, and introduce formal properties\nincluding safety, correctness, efficiency, and t-soundness. We construct secure\ninference protocols based on additive decompositions of weight matrices,\ncombined with masking and probabilistic verification techniques. We prove that\nthese protocols achieve information-theoretic security against\nhonest-but-curious adversaries, and provide robustness against malicious\nadversaries with negligible soundness error.\n  This paper focuses on the theoretical underpinnings of SLIP: precise\ndefinitions, formal protocols, and proofs of security. Empirical validation and\ndecomposition heuristics appear in the companion SLIP paper. Together, the two\nworks provide a complete account of securing LLM IP via hybrid inference,\nbridging both practice and theory."
    },
    {
        "date": "2025-10",
        "title": "FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models",
        "author": "Najmeh Nazari, Banafsheh Saber Latibari, Elahe Hosseini, Fatemeh Movafagh, Chongzhou Fang, Hosein Mohammadi Makrani, Kevin Immanuel Gubbi, Abhijit Mahalanobis, Setareh Rafatirad, Hossein Sayadi, and Houman Homayoun",
        "link": "http://arxiv.org/abs/2510.24985v1",
        "abstract": "Forget and Rewire (FaR) methodology has demonstrated strong resilience\nagainst Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating\ncritical parameters through dynamic rewiring of linear layers. However, the\napplication of FaR introduces non-negligible performance and memory overheads,\nprimarily due to the runtime modification of activation pathways and the lack\nof hardware-level optimization. To overcome these limitations, we propose\nFaRAccel, a novel hardware accelerator architecture implemented on FPGA,\nspecifically designed to offload and optimize FaR operations. FaRAccel\nintegrates reconfigurable logic for dynamic activation rerouting, and\nlightweight storage of rewiring configurations, enabling low-latency inference\nwith minimal energy overhead. We evaluate FaRAccel across a suite of\nTransformer models and demonstrate substantial reductions in FaR inference\nlatency and improvement in energy efficiency, while maintaining the robustness\ngains of the original FaR methodology. To the best of our knowledge, this is\nthe first hardware-accelerated defense against BFAs in Transformers,\neffectively bridging the gap between algorithmic resilience and efficient\ndeployment on real-world AI platforms."
    },
    {
        "date": "2025-10",
        "title": "Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging",
        "author": "Banafsheh Saber Latibari, Najmeh Nazari, Hossein Sayadi, Houman Homayoun, and Abhijit Mahalanobis",
        "link": "http://arxiv.org/abs/2510.24976v1",
        "abstract": "Vision Transformers (ViTs) have emerged as powerful architectures in medical\nimage analysis, excelling in tasks such as disease detection, segmentation, and\nclassification. However, their reliance on large, attention-driven models makes\nthem vulnerable to hardware-level attacks. In this paper, we propose a novel\nthreat model referred to as Med-Hammer that combines the Rowhammer hardware\nfault injection with neural Trojan attacks to compromise the integrity of\nViT-based medical imaging systems. Specifically, we demonstrate how malicious\nbit flips induced via Rowhammer can trigger implanted neural Trojans, leading\nto targeted misclassification or suppression of critical diagnoses (e.g.,\ntumors or lesions) in medical scans. Through extensive experiments on benchmark\nmedical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that\nsuch attacks can remain stealthy while achieving high attack success rates\nabout 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We\nfurther investigate how architectural properties, such as model sparsity,\nattention weight distribution, and the number of features of the layer, impact\nattack effectiveness. Our findings highlight a critical and underexplored\nintersection between hardware-level faults and deep learning security in\nhealthcare applications, underscoring the urgent need for robust defenses\nspanning both model architectures and underlying hardware platforms."
    },
    {
        "date": "2025-10",
        "title": "Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms",
        "author": "Bernhard Klein",
        "link": "http://arxiv.org/abs/2510.24951v1",
        "abstract": "While modern machine learning has transformed numerous application domains,\nits growing computational demands increasingly constrain scalability and\nefficiency, particularly on embedded and resource-limited platforms. In\npractice, neural networks must not only operate efficiently but also provide\nreliable predictions under distributional shifts or unseen data. Bayesian\nneural networks offer a principled framework for quantifying uncertainty, yet\ntheir computational overhead further compounds these challenges.\n  This work advances resource-efficient and robust inference for both\nconventional and Bayesian neural networks through the joint pursuit of\nalgorithmic and hardware efficiency. The former reduces computation through\nmodel compression and approximate Bayesian inference, while the latter\noptimizes deployment on digital accelerators and explores analog hardware,\nbridging algorithmic design and physical realization. The first contribution,\nGalen, performs automatic layer-specific compression guided by sensitivity\nanalysis and hardware-in-the-loop feedback. Analog accelerators offer\nefficiency gains at the cost of noise; this work models device imperfections\nand extends noisy training to nonstationary conditions, improving robustness\nand stability. A second line of work advances probabilistic inference,\ndeveloping analytic and ensemble approximations that replace costly sampling,\nintegrate into a compiler stack, and optimize embedded inference. Finally,\nprobabilistic photonic computing introduces a paradigm where controlled analog\nnoise acts as an intrinsic entropy source, enabling fast, energy-efficient\nprobabilistic inference directly in hardware.\n  Together, these studies demonstrate how efficiency and reliability can be\nadvanced jointly through algorithm-hardware co-design, laying the foundation\nfor the next generation of trustworthy, energy-efficient machine-learning\nsystems."
    },
    {
        "date": "2025-10",
        "title": "S3C2 Summit 2025-03: Industry Secure Supply Chain Summit",
        "author": "Elizabeth Lin, Jonah Ghebremichael, William Enck, Yasemin Acar, Michel Cukier, Alexandros Kapravelos, Christian Kastner, and Laurie Williams",
        "link": "http://arxiv.org/abs/2510.24920v1",
        "abstract": "Software supply chains, while providing immense economic and software\ndevelopment value, are only as strong as their weakest link. Over the past\nseveral years, there has been an exponential increase in cyberattacks\nspecifically targeting vulnerable links in critical software supply chains.\nThese attacks disrupt the day-to-day functioning and threaten the security of\nnearly everyone on the internet, from billion-dollar companies and government\nagencies to hobbyist open-source developers. The ever-evolving threat of\nsoftware supply chain attacks has garnered interest from both the software\nindustry and US government in improving software supply chain security. On\nThursday, March 6th, 2025, four researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 18 practitioners from 17 organizations. The goals of the\nSummit were: (1) to enable sharing between participants from different\nindustries regarding practical experiences and challenges with software supply\nchain security; (2) to help form new collaborations; and (3) to learn about the\nchallenges facing participants to inform our future research directions. The\nsummit consisted of discussions of six topics relevant to the government\nagencies represented, including software bill of materials (SBOMs); compliance;\nmalicious commits; build infrastructure; culture; and large language models\n(LLMs) and security. For each topic of discussion, we presented a list of\nquestions to participants to spark conversation. In this report, we provide a\nsummary of the summit. The open questions and challenges that remained after\neach topic are listed at the end of each topic's section, and the initial\ndiscussion questions for each topic are provided in the appendix."
    },
    {
        "date": "2025-10",
        "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets",
        "author": "Yujun Kim, Chaewon Moon, and Chulhee Yun",
        "link": "http://arxiv.org/abs/2510.24643v1",
        "abstract": "We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$."
    },
    {
        "date": "2025-10",
        "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries",
        "author": "Xin Zhang, Yuqi Song, and Fei Zuo",
        "link": "http://arxiv.org/abs/2510.24640v1",
        "abstract": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks."
    },
    {
        "date": "2025-10",
        "title": "Coreset for Robust Geometric Median: Eliminating Size Dependency on Outliers",
        "author": "Ziyi Fang, Lingxiao Huang, and Runkai Yang",
        "link": "http://arxiv.org/abs/2510.24621v1",
        "abstract": "We study the robust geometric median problem in Euclidean space\n$\\mathbb{R}^d$, with a focus on coreset construction.A coreset is a compact\nsummary of a dataset $P$ of size $n$ that approximates the robust cost for all\ncenters $c$ within a multiplicative error $\\varepsilon$. Given an outlier count\n$m$, we construct a coreset of size $\\tilde{O}(\\varepsilon^{-2} \\cdot\n\\min\\{\\varepsilon^{-2}, d\\})$ when $n \\geq 4m$, eliminating the $O(m)$\ndependency present in prior work [Huang et al., 2022 & 2023]. For the special\ncase of $d = 1$, we achieve an optimal coreset size of\n$\\tilde{\\Theta}(\\varepsilon^{-1/2} + \\frac{m}{n} \\varepsilon^{-1})$, revealing\na clear separation from the vanilla case studied in [Huang et al., 2023;\nAfshani and Chris, 2024]. Our results further extend to robust\n$(k,z)$-clustering in various metric spaces, eliminating the $m$-dependence\nunder mild data assumptions. The key technical contribution is a novel\nnon-component-wise error analysis, enabling substantial reduction of outlier\ninfluence, unlike prior methods that retain them.Empirically, our algorithms\nconsistently outperform existing baselines in terms of size-accuracy tradeoffs\nand runtime, even when data assumptions are violated across a wide range of\ndatasets."
    },
    {
        "date": "2025-10",
        "title": "A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity Dispersion Modeling in MaNGA Galaxies",
        "author": "Sathwik Narkedimilli, N V Saran Kumar, Aswath Babu H, Manjunath K Vanahalli, Manish M, Vinija Jain, and Aman Chadha",
        "link": "http://arxiv.org/abs/2510.24598v1",
        "abstract": "Current quantum machine learning approaches often face challenges balancing\npredictive accuracy, robustness, and interpretability. To address this, we\npropose a novel quantum adversarial framework that integrates a hybrid quantum\nneural network (QNN) with classical deep learning layers, guided by an\nevaluator model with LIME-based interpretability, and extended through quantum\nGAN and self-supervised variants. In the proposed model, an adversarial\nevaluator concurrently guides the QNN by computing feedback loss, thereby\noptimizing both prediction accuracy and model explainability. Empirical\nevaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE\n= 0.21, and R^2 = 0.59, delivering the most consistent performance across\nregression metrics compared to adversarial counterparts. These results\ndemonstrate the potential of combining quantum-inspired methods with classical\narchitectures to develop lightweight, high-performance, and interpretable\npredictive models, advancing the applicability of QML beyond current\nlimitations."
    },
    {
        "date": "2025-10",
        "title": "Online neural fusion of distortionless differential beamformers for robust speech enhancement",
        "author": "Yuanhang Qian, Kunlong Zhao, Jilu Jin, Xueqin Luo, Gongping Huang, Jingdong Chen, and Jacob Benesty",
        "link": "http://arxiv.org/abs/2510.24497v1",
        "abstract": "Fixed beamforming is widely used in practice since it does not depend on the\nestimation of noise statistics and provides relatively stable performance.\nHowever, a single beamformer cannot adapt to varying acoustic conditions, which\nlimits its interference suppression capability. To address this, adaptive\nconvex combination (ACC) algorithms have been introduced, where the outputs of\nmultiple fixed beamformers are linearly combined to improve robustness.\nNevertheless, ACC often fails in highly non-stationary scenarios, such as\nrapidly moving interference, since its adaptive updates cannot reliably track\nrapid changes. To overcome this limitation, we propose a frame-online neural\nfusion framework for multiple distortionless differential beamformers, which\nestimates the combination weights through a neural network. Compared with\nconventional ACC, the proposed method adapts more effectively to dynamic\nacoustic environments, achieving stronger interference suppression while\nmaintaining the distortionless constraint."
    },
    {
        "date": "2025-10",
        "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space",
        "author": "Viktoriia Zinkovich, Anton Antonov, Andrei Spiridonov, Denis Shepelev, Andrey Moskalenko, Daria Pugacheva, Elena Tutubalina, Andrey Kuznetsov, and Vlad Shakhuro",
        "link": "http://arxiv.org/abs/2510.24446v1",
        "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities\nin vision-language tasks such as reasoning segmentation, where models generate\nsegmentation masks based on textual queries. While prior work has primarily\nfocused on perturbing image inputs, semantically equivalent textual\nparaphrases-crucial in real-world applications where users express the same\nintent in varied ways-remain underexplored. To address this gap, we introduce a\nnovel adversarial paraphrasing task: generating grammatically correct\nparaphrases that preserve the original query meaning while degrading\nsegmentation performance. To evaluate the quality of adversarial paraphrases,\nwe develop a comprehensive automatic evaluation protocol validated with human\nstudies. Furthermore, we introduce SPARTA-a black-box, sentence-level\noptimization method that operates in the low-dimensional semantic latent space\nof a text autoencoder, guided by reinforcement learning. SPARTA achieves\nsignificantly higher success rates, outperforming prior methods by up to 2x on\nboth the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive\nbaselines to assess the robustness of advanced reasoning segmentation models.\nWe reveal that they remain vulnerable to adversarial paraphrasing-even under\nstrict semantic and grammatical constraints. All code and data will be released\npublicly upon acceptance."
    },
    {
        "date": "2025-10",
        "title": "Attack on a PUF-based Secure Binary Neural Network",
        "author": "Bijeet Basak, Nupur Patil, Kurian Polachan, and Srinivas Vivek",
        "link": "http://arxiv.org/abs/2510.24422v1",
        "abstract": "Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters."
    },
    {
        "date": "2025-10",
        "title": "Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers",
        "author": "Yan Meng, Jiachun Li, Matthew Pillari, Arjun Deopujari, Liam Brennan, Hafsah Shamsie, Haojin Zhu, and Yuan Tian",
        "link": "http://arxiv.org/abs/2510.24393v1",
        "abstract": "Though playing an essential role in smart home systems, smart speakers are\nvulnerable to voice spoofing attacks. Passive liveness detection, which\nutilizes only the collected audio rather than the deployed sensors to\ndistinguish between live-human and replayed voices, has drawn increasing\nattention. However, it faces the challenge of performance degradation under the\ndifferent environmental factors as well as the strict requirement of the fixed\nuser gestures.\n  In this study, we propose a novel liveness feature, array fingerprint, which\nutilizes the microphone array inherently adopted by the smart speaker to\ndetermine the identity of collected audios. Our theoretical analysis\ndemonstrates that by leveraging the circular layout of microphones, compared\nwith existing schemes, array fingerprint achieves a more robust performance\nunder the environmental change and user's movement. Then, to leverage such a\nfingerprint, we propose ARRAYID, a lightweight passive detection scheme, and\nelaborate a series of features working together with array fingerprint. Our\nevaluation on the dataset containing 32,780 audio samples and 14 spoofing\ndevices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to\nexisting passive liveness detection schemes."
    },
    {
        "date": "2025-10",
        "title": "What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements",
        "author": "Vishal Halder, Alexandre Reiffers-Masson, Abdeldjalil A\u00efssa-El-Bey, and Gugan Thoppe",
        "link": "http://arxiv.org/abs/2510.24215v2",
        "abstract": "Let $A \\in \\mathbb{R}^{m \\times n}$ be an arbitrary, known matrix and $e$ a\n$q$-sparse adversarial vector. Given $y = A x^\\star + e$ and $q$, we seek the\nsmallest set containing $x^\\star$ -- hence the one conveying maximal\ninformation about $x^\\star$ -- that is uniformly recoverable from $y$ without\nknowing $e$. While exact recovery of $x^\\star$ via strong (and often\nimpractical) structural assumptions on $A$ or $x^\\star$ (e.g., restricted\nisometry, sparsity) is well studied, recoverability for arbitrary $A$ and\n$x^\\star$ remains open. Our main result shows that the best that one can hope\nto recover is $x^\\star + \\ker(U)$, where $U$ is the unique projection matrix\nonto the intersection of rowspaces of all possible submatrices of $A$ obtained\nby deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the\n$\\ell_0$-norm of $y - A x$ lies in $x^\\star + \\ker(U)$, which then gives a\nconstructive approach to recover this set."
    },
    {
        "date": "2025-10",
        "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2",
        "author": "Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, and Hai Jin",
        "link": "http://arxiv.org/abs/2510.24195v1",
        "abstract": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin."
    },
    {
        "date": "2025-10",
        "title": "Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases",
        "author": "Ziyao Cui, Minxing Zhang, and Jian Pei",
        "link": "http://arxiv.org/abs/2510.24807v1",
        "abstract": "Privacy concerns have become increasingly critical in modern AI and data\nscience applications, where sensitive information is collected, analyzed, and\nshared across diverse domains such as healthcare, finance, and mobility. While\nprior research has focused on protecting privacy in a single data release, many\nreal-world systems operate under sequential or continuous data publishing,\nwhere the same or related data are released over time. Such sequential\ndisclosures introduce new vulnerabilities, as temporal correlations across\nreleases may enable adversaries to infer sensitive information that remains\nhidden in any individual release. In this paper, we investigate whether an\nattacker can compromise privacy in sequential data releases by exploiting\ndependencies between consecutive publications, even when each individual\nrelease satisfies standard privacy guarantees. To this end, we propose a novel\nattack model that captures these sequential dependencies by integrating a\nHidden Markov Model with a reinforcement learning-based bi-directional\ninference mechanism. This enables the attacker to leverage both earlier and\nlater observations in the sequence to infer private information. We instantiate\nour framework in the context of trajectory data, demonstrating how an adversary\ncan recover sensitive locations from sequential mobility datasets. Extensive\nexperiments on Geolife, Porto Taxi, and SynMob datasets show that our model\nconsistently outperforms baseline approaches that treat each release\nindependently. The results reveal a fundamental privacy risk inherent to\nsequential data publishing, where individually protected releases can\ncollectively leak sensitive information when analyzed temporally. These\nfindings underscore the need for new privacy-preserving frameworks that\nexplicitly model temporal dependencies, such as time-aware differential privacy\nor sequential data obfuscation strategies."
    },
    {
        "date": "2025-10",
        "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation",
        "author": "Xiucheng Zhang, Yang Jiang, Hongwei Qing, and Jiashuo Bai",
        "link": "http://arxiv.org/abs/2510.24055v1",
        "abstract": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation"
    },
    {
        "date": "2025-10",
        "title": "Causal-Aware Generative Adversarial Networks with Reinforcement Learning",
        "author": "Tu Anh Hoang Nguyen, Dang Nguyen, Tri-Nhan Vo, Thuc Duy Le, and Sunil Gupta",
        "link": "http://arxiv.org/abs/2510.24046v1",
        "abstract": "The utility of tabular data for tasks ranging from model training to\nlarge-scale data analysis is often constrained by privacy concerns or\nregulatory hurdles. While existing data generation methods, particularly those\nbased on Generative Adversarial Networks (GANs), have shown promise, they\nfrequently struggle with capturing complex causal relationship, maintaining\ndata utility, and providing provable privacy guarantees suitable for enterprise\ndeployment. We introduce CA-GAN, a novel generative framework specifically\nengineered to address these challenges for real-world tabular datasets. CA-GAN\nutilizes a two-step approach: causal graph extraction to learn a robust,\ncomprehensive causal relationship in the data's manifold, followed by a custom\nConditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates\nexclusively as per the structure of nodes in the causal graph. More\nimportantly, the generator is trained with a new Reinforcement Learning-based\nobjective that aligns the causal graphs constructed from real and fake data,\nensuring the causal awareness in both training and sampling phases. We\ndemonstrate CA-GAN superiority over six SOTA methods across 14 tabular\ndatasets. Our evaluations, focused on core data engineering metrics: causal\npreservation, utility preservation, and privacy preservation. Our method offers\na practical, high-performance solution for data engineers seeking to create\nhigh-quality, privacy-compliant synthetic datasets to benchmark database\nsystems, accelerate software development, and facilitate secure data-driven\nresearch."
    },
    {
        "date": "2025-10",
        "title": "Enhancing CLIP Robustness via Cross-Modality Alignment",
        "author": "Xingyu Zhu, Beier Zhu, Shuo Wang, Kesen Zhao, and Hanwang Zhang",
        "link": "http://arxiv.org/abs/2510.24038v1",
        "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples."
    },
    {
        "date": "2025-10",
        "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts",
        "author": "Yufan Liu, Wanqian Zhang, Huashan Chen, Lin Wang, Xiaojun Jia, Zheng Lin, and Weiping Wang",
        "link": "http://arxiv.org/abs/2510.24034v1",
        "abstract": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.)."
    },
    {
        "date": "2025-10",
        "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization",
        "author": "Heethanjan Kanagalingam, Thenukan Pathmanathan, Mokeeshan Vathanakumar, and Tharmakulasingam Mukunthan",
        "link": "http://arxiv.org/abs/2510.24000v1",
        "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets."
    },
    {
        "date": "2025-10",
        "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability",
        "author": "Peiyang Xu, Minzhou Pan, Zhaorun Chen, Shuang Yang, Chaowei Xiao, and Bo Li",
        "link": "http://arxiv.org/abs/2510.23960v1",
        "abstract": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats."
    },
    {
        "date": "2025-10",
        "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments",
        "author": "Miguel Fernandez-de-Retana, Unai Zulaika, Rub\u00e9n S\u00e1nchez-Corcuera, and Aitor Almeida",
        "link": "http://arxiv.org/abs/2510.23931v1",
        "abstract": "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy."
    },
    {
        "date": "2025-10",
        "title": "PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs",
        "author": "Jiaqi Xue, Yifei Zhao, Mansour Al Ghanim, Shangqian Gao, Ruimin Sun, Qian Lou, and Mengxin Zheng",
        "link": "http://arxiv.org/abs/2510.23891v1",
        "abstract": "Text watermarking for large language models (LLMs) enables model owners to\nverify text origin and protect intellectual property. While watermarking\nmethods for closed-source LLMs are relatively mature, extending them to\nopen-source models remains challenging, as developers cannot control the\ndecoding process. Consequently, owners of open-source LLMs lack practical means\nto verify whether text was generated by their models. A core difficulty lies in\nembedding watermarks directly into model weights without hurting detectability.\nA promising idea is to distill watermarks from a closed-source model into an\nopen one, but this suffers from (i) poor detectability due to mismatch between\nlearned and predefined patterns, and (ii) fragility to downstream modifications\nsuch as fine-tuning or model merging. To overcome these limitations, we propose\nPRO, a Precise and Robust text watermarking method for open-source LLMs. PRO\njointly trains a watermark policy model with the LLM, producing patterns that\nare easier for the model to learn and more consistent with detection criteria.\nA regularization term further simulates downstream perturbations and penalizes\ndegradation in watermark detectability, ensuring robustness under model edits.\nExperiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO\nsubstantially improves both watermark detectability and resilience to model\nmodifications."
    },
    {
        "date": "2025-10",
        "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges",
        "author": "Shrestha Datta, Shahriar Kabir Nahin, Anshuman Chhabra, and Prasant Mohapatra",
        "link": "http://arxiv.org/abs/2510.23883v1",
        "abstract": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems."
    },
    {
        "date": "2025-10",
        "title": "EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet",
        "author": "Joel Poncha Lemayian, Ghyslain Gagnon, Kaiwen Zhang, and Pascal Giard",
        "link": "http://arxiv.org/abs/2510.23847v1",
        "abstract": "Cryptocurrency blockchain networks safeguard digital assets using\ncryptographic keys, with wallets playing a critical role in generating,\nstoring, and managing these keys. Wallets, typically categorized as hot and\ncold, offer varying degrees of security and convenience. However, they are\ngenerally software-based applications running on microcontrollers.\nConsequently, they are vulnerable to malware and side-channel attacks, allowing\nperpetrators to extract private keys by targeting critical algorithms, such as\nECC, which processes private keys to generate public keys and authorize\ntransactions. To address these issues, this work presents EthVault, the first\nhardware architecture for an Ethereum hierarchically deterministic cold wallet,\nfeaturing hardware implementations of key algorithms for secure key generation.\nAlso, an ECC architecture resilient to side-channel and timing attacks is\nproposed. Moreover, an architecture of the child key derivation function, a\nfundamental component of cryptocurrency wallets, is proposed. The design\nminimizes resource usage, meeting market demand for small, portable\ncryptocurrency wallets. FPGA implementation results validate the feasibility of\nthe proposed approach. The ECC architecture exhibits uniform execution behavior\nacross varying inputs, while the complete design utilizes only 27%, 7%, and 6%\nof LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+\nFPGA."
    },
    {
        "date": "2025-10",
        "title": "Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation",
        "author": "Nicki Barari, Edward Kim, and Christopher MacLellan",
        "link": "http://arxiv.org/abs/2510.23756v1",
        "abstract": "Catastrophic forgetting remains a central challenge in continual learning,\nwhere models are required to integrate new knowledge over time without losing\nwhat they have previously learned. In prior work, we introduced Cobweb/4V, a\nhierarchical concept formation model that exhibited robustness to catastrophic\nforgetting in visual domains. Motivated by this robustness, we examine three\nhypotheses regarding the factors that contribute to such stability: (1)\nadaptive structural reorganization enhances knowledge retention, (2) sparse and\nselective updates reduce interference, and (3) information-theoretic learning\nbased on sufficiency statistics provides advantages over gradient-based\nbackpropagation. To test these hypotheses, we compare Cobweb/4V with neural\nbaselines, including CobwebNN, a neural implementation of the Cobweb framework\nintroduced in this work. Experiments on datasets of varying complexity (MNIST,\nFashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring\nenhances learning plasticity, sparse updates help mitigate interference, and\nthe information-theoretic learning process preserves prior knowledge without\nrevisiting past data. Together, these findings provide insight into mechanisms\nthat can mitigate catastrophic forgetting and highlight the potential of\nconcept-based, information-theoretic approaches for building stable and\nadaptive continual learning systems."
    },
    {
        "date": "2025-10",
        "title": "Lightweight Robust Direct Preference Optimization",
        "author": "Cheol Woo Kim, Shresth Verma, Mauricio Tec, and Milind Tambe",
        "link": "http://arxiv.org/abs/2510.23590v1",
        "abstract": "Direct Preference Optimization (DPO) has become a popular method for\nfine-tuning large language models (LLMs) due to its stability and simplicity.\nHowever, it is also known to be sensitive to noise in the data and prone to\noverfitting. Recent works have proposed using distributionally robust\noptimization (DRO) to address potential noise and distributional shift in the\ndata. However, these methods often suffer from excessive conservatism and high\ncomputational cost. We propose DPO-PRO (DPO with Preference Robustness), a\nrobust fine-tuning algorithm based on DPO which accounts for uncertainty in the\npreference distribution through a lightweight DRO formulation. Unlike prior\nDRO-based variants, DPO-PRO focuses solely on uncertainty in preferences,\navoiding unnecessary conservatism and incurring negligible computational\noverhead. We further show that DPO-PRO is equivalent to a regularized DPO\nobjective that penalizes model overconfidence under weak preference signals. We\nevaluate DPO-PRO on standard alignment benchmarks and a real-world public\nhealth task. Experimental results show that our method consistently improves\nrobustness to noisy preference signals compared to existing DPO variants."
    },
    {
        "date": "2025-10",
        "title": "Robust Decision Making with Partially Calibrated Forecasts",
        "author": "Shayan Kiyani, Hamed Hassani, George Pappas, and Aaron Roth",
        "link": "http://arxiv.org/abs/2510.23471v1",
        "abstract": "Calibration has emerged as a foundational goal in ``trustworthy machine\nlearning'', in part because of its strong decision theoretic semantics.\nIndependent of the underlying distribution, and independent of the decision\nmaker's utility function, calibration promises that amongst all policies\nmapping predictions to actions, the uniformly best policy is the one that\n``trusts the predictions'' and acts as if they were correct. But this is true\nonly of \\emph{fully calibrated} forecasts, which are tractable to guarantee\nonly for very low dimensional prediction problems. For higher dimensional\nprediction problems (e.g. when outcomes are multiclass), weaker forms of\ncalibration have been studied that lack these decision theoretic properties. In\nthis paper we study how a conservative decision maker should map predictions\nendowed with these weaker (``partial'') calibration guarantees to actions, in a\nway that is robust in a minimax sense: i.e. to maximize their expected utility\nin the worst case over distributions consistent with the calibration\nguarantees. We characterize their minimax optimal decision rule via a duality\nargument, and show that surprisingly, ``trusting the predictions and acting\naccordingly'' is recovered in this minimax sense by \\emph{decision calibration}\n(and any strictly stronger notion of calibration), a substantially weaker and\nmore tractable condition than full calibration. For calibration guarantees that\nfall short of decision calibration, the minimax optimal decision rule is still\nefficiently computable, and we provide an empirical evaluation of a natural one\nthat applies to any regression model solved to optimize squared error."
    },
    {
        "date": "2025-10",
        "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents",
        "author": "Gokturk Aytug Akarlar",
        "link": "http://arxiv.org/abs/2510.23682v1",
        "abstract": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility."
    },
    {
        "date": "2025-10",
        "title": "Eigen-Value: Efficient Domain-Robust Data Valuation via Eigenvalue-Based Approach",
        "author": "Youngjun Choi, Joonseong Kang, Sungjun Lim, and Kyungwoo Song",
        "link": "http://arxiv.org/abs/2510.23409v2",
        "abstract": "Data valuation has become central in the era of data-centric AI. It drives\nefficient training pipelines and enables objective pricing in data markets by\nassigning a numeric value to each data point. Most existing data valuation\nmethods estimate the effect of removing individual data points by evaluating\nchanges in model validation performance under in-distribution (ID) settings, as\nopposed to out-of-distribution (OOD) scenarios where data follow different\npatterns. Since ID and OOD data behave differently, data valuation methods\nbased on ID loss often fail to generalize to OOD settings, particularly when\nthe validation set contains no OOD data. Furthermore, although OOD-aware\nmethods exist, they involve heavy computational costs, which hinder practical\ndeployment. To address these challenges, we introduce \\emph{Eigen-Value} (EV),\na plug-and-play data valuation framework for OOD robustness that uses only an\nID data subset, including during validation. EV provides a new spectral\napproximation of domain discrepancy, which is the gap of loss between ID and\nOOD using ratios of eigenvalues of ID data's covariance matrix. EV then\nestimates the marginal contribution of each data point to this discrepancy via\nperturbation theory, alleviating the computational burden. Subsequently, EV\nplugs into ID loss-based methods by adding an EV term without any additional\ntraining loop. We demonstrate that EV achieves improved OOD robustness and\nstable value rankings across real-world datasets, while remaining\ncomputationally lightweight. These results indicate that EV is practical for\nlarge-scale settings with domain shift, offering an efficient path to\nOOD-robust data valuation."
    },
    {
        "date": "2025-10",
        "title": "Robust Non-negative Proximal Gradient Algorithm for Inverse Problems",
        "author": "Hanzhang Wang, Zonglin Liu, Jingyi Xu, Chenyang Wang, Zhiwei Zhong, and Qiangqiang Shen",
        "link": "http://arxiv.org/abs/2510.23362v1",
        "abstract": "Proximal gradient algorithms (PGA), while foundational for inverse problems\nlike image reconstruction, often yield unstable convergence and suboptimal\nsolutions by violating the critical non-negativity constraint. We identify the\ngradient descent step as the root cause of this issue, which introduces\nnegative values and induces high sensitivity to hyperparameters. To overcome\nthese limitations, we propose a novel multiplicative update proximal gradient\nalgorithm (SSO-PGA) with convergence guarantees, which is designed for\nrobustness in non-negative inverse problems. Our key innovation lies in\nsuperseding the gradient descent step with a learnable sigmoid-based operator,\nwhich inherently enforces non-negativity and boundedness by transforming\ntraditional subtractive updates into multiplicative ones. This design,\naugmented by a sliding parameter for enhanced stability and convergence, not\nonly improves robustness but also boosts expressive capacity and noise\nimmunity. We further formulate a degradation model for multi-modal restoration\nand derive its SSO-PGA-based optimization algorithm, which is then unfolded\ninto a deep network to marry the interpretability of optimization with the\npower of deep learning. Extensive numerical and real-world experiments\ndemonstrate that our method significantly surpasses traditional PGA and other\nstate-of-the-art algorithms, ensuring superior performance and stability."
    }
]