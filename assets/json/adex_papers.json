[
    {
        "date": "2025-03",
        "title": "OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation",
        "author": "Mallika Garg, Debashis Ghosh, and Pyari Mohan Pradhan",
        "link": "http://arxiv.org/abs/2503.21723v1",
        "abstract": "Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets."
    },
    {
        "date": "2025-03",
        "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
        "author": "Jiahe Qian, Yaoyu Fang, Jinkui Hao, and Bo Zhou",
        "link": "http://arxiv.org/abs/2503.21695v1",
        "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches."
    },
    {
        "date": "2025-03",
        "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
        "author": "Satvik Verma, Qun Wang, and E. Wes Bethel",
        "link": "http://arxiv.org/abs/2503.21674v1",
        "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity."
    },
    {
        "date": "2025-03",
        "title": "Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems",
        "author": "Huacheng Li, Jingyong Su, and Kai Wang",
        "link": "http://arxiv.org/abs/2503.21496v1",
        "abstract": "The rapid development of network technologies and industrial intelligence has\naugmented the connectivity and intelligence within the automotive industry.\nNotably, in the Internet of Vehicles (IoV), the Controller Area Network (CAN),\nwhich is crucial for the communication of electronic control units but lacks\ninbuilt security measures, has become extremely vulnerable to severe\ncybersecurity threats. Meanwhile, the efficacy of Intrusion Detection Systems\n(IDS) is hampered by the scarcity of sufficient attack data for robust model\ntraining. To overcome this limitation, we introduce a novel methodology\nleveraging the Restricted Boltzmann Machine (RBM) to generate synthetic CAN\nattack data, thereby producing training datasets with a more balanced sample\ndistribution. Specifically, we design a CAN Data Processing Module for\ntransforming raw CAN data into an RBM-trainable format, and a Negative Sample\nGeneration Module to generate data reflecting the distribution of CAN data\nframes denoting network intrusions. Experimental results show the generated\ndata significantly improves IDS performance, with CANet accuracy rising from\n0.6477 to 0.9725 and EfficientNet from 0.1067 to 0.1555. Code is available at\nhttps://github.com/wangkai-tech23/CANDataSynthetic."
    },
    {
        "date": "2025-03",
        "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time",
        "author": "Zhaojun Nan, Yunchu Han, Sheng Zhou, and Zhisheng Niu",
        "link": "http://arxiv.org/abs/2503.21476v1",
        "abstract": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices."
    },
    {
        "date": "2025-03",
        "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
        "author": "Ryan Marinelli, Josef Pichlmeier, and Tamas Bisztray",
        "link": "http://arxiv.org/abs/2503.21464v1",
        "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
    },
    {
        "date": "2025-03",
        "title": "AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model",
        "author": "Sen Zhang, Qingqing Ye, Haibo Hu, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2503.21426v1",
        "abstract": "The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks."
    },
    {
        "date": "2025-03",
        "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
        "author": "Cheng Wang, Yiwei Wang, Yujun Cai, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2503.21315v1",
        "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets."
    },
    {
        "date": "2025-03",
        "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
        "author": "Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2503.21305v1",
        "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings."
    },
    {
        "date": "2025-03",
        "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
        "author": "Yongxu Wang, Weiyun Yi, Xinhao Kong, and Wanting Li",
        "link": "http://arxiv.org/abs/2503.21257v1",
        "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub."
    },
    {
        "date": "2025-03",
        "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing",
        "author": "Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.21236v1",
        "abstract": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
        "author": "Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, and Seong Tae Kim",
        "link": "http://arxiv.org/abs/2503.21164v1",
        "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."
    },
    {
        "date": "2025-03",
        "title": "How to Secure Existing C and C++ Software without Memory Safety",
        "author": "\u00dalfar Erlingsson",
        "link": "http://arxiv.org/abs/2503.21145v1",
        "abstract": "The most important security benefit of software memory safety is easy to\nstate: for C and C++ software, attackers can exploit most bugs and\nvulnerabilities to gain full, unfettered control of software behavior, whereas\nthis is not true for most bugs in memory-safe software.\n  Fortunately, this security benefit -- most bugs don't give attackers full\ncontrol -- can be had for unmodified C/C++ software, without per-application\neffort.\n  This doesn't require trying to establish memory safety; instead, it is\nsufficient to eliminate most of the combinatorial ways in which software with\ncorrupted memory can execute. To eliminate these interleavings, there already\nexist practical compiler and runtime mechanisms that incur little overhead and\nneed no special hardware or platform support.\n  Each of the mechanisms described here is already in production use, at scale,\non one or more platforms. By supporting their combined use in development\ntoolchains, the security of all C and C++ software against remote code\nexecution attacks can be rapidly, and dramatically, improved."
    },
    {
        "date": "2025-03",
        "title": "Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid",
        "author": "Junfei Wang, and Pirathayini Srikantha",
        "link": "http://arxiv.org/abs/2503.20976v1",
        "abstract": "Real-time price signals and power generation levels (disaggregated or\naggregated) are commonly made available to the public by Independent System\nOperators (ISOs) to promote efficiency and transparency. However, they may\ninadvertently reveal crucial private information about the power grid, such as\nthe cost functions of generators. Adversaries can exploit these vulnerabilities\nfor strategic bidding, potentially leading to financial losses for power market\nparticipants and consumers. In this paper, we prove the existence of a\nclosed-form solution for recovering coefficients in cost functions when LMPs\nand disaggregated power generation data are available. Additionally, we\nestablish the convergence conditions for inference the quadratic coefficients\nof cost functions when LMPs and aggregated generation data are given. Our\ntheoretical analysis provides the conditions under which the algorithm is\nguaranteed to converge, and our experiments demonstrate the efficacy of this\nmethod on IEEE benchmark systems, including 14-bus and 30-bus and 118-bus\nsystems."
    },
    {
        "date": "2025-03",
        "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
        "author": "Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, and Lydia Y. Chen",
        "link": "http://arxiv.org/abs/2503.20952v1",
        "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse"
    },
    {
        "date": "2025-03",
        "title": "Prototype Guided Backdoor Defense",
        "author": "Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, and Narayanan P J",
        "link": "http://arxiv.org/abs/2503.20925v1",
        "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}."
    },
    {
        "date": "2025-03",
        "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
        "author": "Usama Zafar, Andr\u00e9 Teixeira, and Salman Toor",
        "link": "http://arxiv.org/abs/2503.20884v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems."
    },
    {
        "date": "2025-03",
        "title": "DR-PETS: Learning-Based Control With Planning in Adversarial Environments",
        "author": "Hozefa Jesawada, Antonio Acernese, Giovanni Russo, and Carmen Del Vecchio",
        "link": "http://arxiv.org/abs/2503.20660v2",
        "abstract": "Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates."
    },
    {
        "date": "2025-03",
        "title": "Robust Flower Cluster Matching Using The Unscented Transform",
        "author": "Andy Chu, Rashik Shrestha, Yu Gu, and Jason N. Gross",
        "link": "http://arxiv.org/abs/2503.20631v1",
        "abstract": "Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments."
    },
    {
        "date": "2025-03",
        "title": "$\u03b2$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
        "author": "Haci Ismail Aslan, Philipp Wiesner, Ping Xiong, and Odej Kao",
        "link": "http://arxiv.org/abs/2503.20630v1",
        "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance."
    },
    {
        "date": "2025-03",
        "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, and Yue Gao",
        "link": "http://arxiv.org/abs/2503.20844v1",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms."
    },
    {
        "date": "2025-03",
        "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.20613v1",
        "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks."
    },
    {
        "date": "2025-03",
        "title": "Feature Statistics with Uncertainty Help Adversarial Robustness",
        "author": "Ran Wang, Xinlei Zhou, Rihao Li, Meng Hu, Wenhui Wu, and Yuheng Jia",
        "link": "http://arxiv.org/abs/2503.20583v1",
        "abstract": "Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN."
    },
    {
        "date": "2025-03",
        "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling",
        "author": "Vinzenz Uhr, Ivan Diaz, Christian Rummel, and Richard McKinley",
        "link": "http://arxiv.org/abs/2503.20571v1",
        "abstract": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs)."
    },
    {
        "date": "2025-03",
        "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks",
        "author": "Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, and Xian Wei",
        "link": "http://arxiv.org/abs/2503.20454v1",
        "abstract": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates."
    },
    {
        "date": "2025-03",
        "title": "Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive Energy Forecasting with Missing Data",
        "author": "Akylas Stratigakos, and Panagiotis Andrianesis",
        "link": "http://arxiv.org/abs/2503.20410v1",
        "abstract": "Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data."
    },
    {
        "date": "2025-03",
        "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
        "author": "Francesco Micheli, Efe C. Balta, Anastasios Tsiamis, and John Lygeros",
        "link": "http://arxiv.org/abs/2503.20341v1",
        "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method."
    },
    {
        "date": "2025-03",
        "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
        "author": "Tao Wu, and Tie Luo",
        "link": "http://arxiv.org/abs/2503.20310v1",
        "abstract": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures."
    },
    {
        "date": "2025-03",
        "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
        "author": "Hongye Cao, Fan Feng, Jing Huo, Shangdong Yang, Meng Fang, Tianpei Yang, and Yang Gao",
        "link": "http://arxiv.org/abs/2503.20285v1",
        "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency."
    },
    {
        "date": "2025-03",
        "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
        "author": "Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2503.20257v1",
        "abstract": "As Machine Learning (ML) evolves, the complexity and sophistication of\nsecurity threats against this paradigm continue to grow as well, threatening\ndata privacy and model integrity. In response, Machine Unlearning (MU) is a\nrecent technology that aims to remove the influence of specific data from a\ntrained model, enabling compliance with privacy regulations and user requests.\nThis can be done for privacy compliance (e.g., GDPR's right to be forgotten) or\nmodel refinement. However, the intersection between classical threats in ML and\nMU remains largely unexplored. In this Systematization of Knowledge (SoK), we\nprovide a structured analysis of security threats in ML and their implications\nfor MU. We analyze four major attack classes, namely, Backdoor Attacks,\nMembership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks,\nwe investigate their impact on MU and propose a novel classification based on\nhow they are usually used in this context. Finally, we identify open\nchallenges, including ethical considerations, and explore promising future\nresearch directions, paving the way for future research in secure and\nprivacy-preserving Machine Unlearning."
    },
    {
        "date": "2025-03",
        "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
        "author": "Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, and Robby T. Tan",
        "link": "http://arxiv.org/abs/2503.20211v1",
        "abstract": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness of Kernel Ridge Regression Using the Cauchy Loss Function",
        "author": "Hongwei Wen, Annika Betken, and Wouter Koolen",
        "link": "http://arxiv.org/abs/2503.20120v1",
        "abstract": "Robust regression aims to develop methods for estimating an unknown\nregression function in the presence of outliers, heavy-tailed distributions, or\ncontaminated data, which can severely impact performance. Most existing\ntheoretical results in robust regression assume that the noise has a finite\nabsolute mean, an assumption violated by certain distributions, such as Cauchy\nand some Pareto noise. In this paper, we introduce a generalized Cauchy noise\nframework that accommodates all noise distributions with finite moments of any\norder, even when the absolute mean is infinite. Within this framework, we study\nthe \\textit{kernel Cauchy ridge regressor} (\\textit{KCRR}), which minimizes a\nregularized empirical Cauchy risk to achieve robustness. To derive the\n$L_2$-risk bound for KCRR, we establish a connection between the excess Cauchy\nrisk and $L_2$-risk for sufficiently large scale parameters of the Cauchy loss,\nwhich reveals that these two risks are equivalent. Furthermore, under the\nassumption that the regression function satisfies H\\\"older smoothness, we\nderive excess Cauchy risk bounds for KCRR, showing improved performance as the\nscale parameter decreases. By considering the twofold effect of the scale\nparameter on the excess Cauchy risk and its equivalence with the $L_2$-risk, we\nestablish the almost minimax-optimal convergence rate for KCRR in terms of\n$L_2$-risk, highlighting the robustness of the Cauchy loss in handling various\ntypes of noise. Finally, we validate the effectiveness of KCRR through\nexperiments on both synthetic and real-world datasets under diverse noise\ncorruption scenarios."
    },
    {
        "date": "2025-03",
        "title": "ARGO-SLSA: Software Supply Chain Security in Argo Workflows",
        "author": "Mohomed Thariq, and Indrajith Ekanayake",
        "link": "http://arxiv.org/abs/2503.20079v1",
        "abstract": "Distributed systems widely adopt microservice architecture to handle growing\ncomplexity and scale. This approach breaks applications into independent,\nloosely coupled services. Kubernetes has become the de facto standard for\nmanaging microservices, and automating complex, multi-step workflows is a\ncommon requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine\nfor managing these workflows in an automated fashion. These workflows generate\nartifacts such as executables, logs, container images, and packages, which\noften require proper management through software supply chain security.\nHowever, Argo Workflows does not include built-in functionality for frameworks\nlike Supply-chain Levels for Software Artifacts (SLSA), which is essential for\nensuring artifact integrity, traceability, and security. This gap compels\npractitioners to rely on external tools to meet software supply chain security\nstandards. In response, this paper proposes a Kubernetes-native controller\nbuilt on top of existing open-source Argo Workflows to enhance artifact\nsecurity. By generating cryptographic signing and provenance attestations, the\ncontroller enables Argo Workflows to comply with SLSA standards. We demonstrate\nthat implementations can provide such cryptographic signing and provenance\nattestations for artifacts produced by the controller, allowing software\nartifacts built with Argo Workflows to adhere to SLSA requirements. The\nproposed validation model evaluates the proof of concept of the controller,\nincluding its ability to reconcile workflows, detect pods associated with\nworkflow nodes, operate without disrupting existing operations, enforce\nintegrity, and monitor software artifacts."
    },
    {
        "date": "2025-03",
        "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
        "author": "Alejandro Ortega",
        "link": "http://arxiv.org/abs/2503.19887v1",
        "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems."
    },
    {
        "date": "2025-03",
        "title": "RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning",
        "author": "Abdulmoneam Ali, and Ahmed Arafa",
        "link": "http://arxiv.org/abs/2503.19886v1",
        "abstract": "We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction."
    },
    {
        "date": "2025-03",
        "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
        "author": "Jordan Madden, Lhamo Dorje, and Xiaohua Li",
        "link": "http://arxiv.org/abs/2503.19817v1",
        "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented."
    },
    {
        "date": "2025-03",
        "title": "SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation",
        "author": "Jingdan Kang, Haoxin Yang, Yan Cai, Huaidong Zhang, Xuemiao Xu, Yong Du, and Shengfeng He",
        "link": "http://arxiv.org/abs/2503.19791v1",
        "abstract": "Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https://github.com/A-raniy-day/SITA."
    },
    {
        "date": "2025-03",
        "title": "PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch",
        "author": "Abhishek Ghosh, Ajay Nayak, Ashish Panwar, and Arkaprava Basu",
        "link": "http://arxiv.org/abs/2503.19779v1",
        "abstract": "CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2."
    },
    {
        "date": "2025-03",
        "title": "A Managed Tokens Service for Securely Keeping and Distributing Grid Tokens",
        "author": "Shreyas Bhat, and Dave Dykstra",
        "link": "http://arxiv.org/abs/2503.19768v1",
        "abstract": "Fermilab is transitioning authentication and authorization for grid\noperations to using bearer tokens based on the WLCG Common JWT (JSON Web Token)\nProfile. One of the functionalities that Fermilab experimenters rely on is the\nability to automate batch job submission, which in turn depends on the ability\nto securely refresh and distribute the necessary credentials to experiment job\nsubmit points. Thus, with the transition to using tokens for grid operations,\nwe needed to create a service that would obtain, refresh, and distribute tokens\nfor experimenters' use. This service would avoid the need for experimenters to\nbe experts in obtaining their own tokens and would better protect the most\nsensitive long-lived credentials. Further, the service needed to be widely\nscalable, as Fermilab hosts many experiments, each of which would need their\nown credentials. To address these issues, we created and deployed a Managed\nTokens Service. The service is written in Go, taking advantage of that\nlanguage's native concurrency primitives to easily be able to scale operations\nas we onboard experiments. The service uses as its first credentials a set of\nkerberos keytabs, stored on the same secure machine that the Managed Tokens\nservice runs on. These kerberos credentials allow the service to use htgettoken\nvia condor_vault_storer to store vault tokens in the HTCondor credential\nmanagers (credds) that run on the batch system scheduler machines (HTCondor\nschedds); as well as downloading a local, shorter-lived copy of the vault\ntoken. The kerberos credentials are then also used to distribute copies of the\nlocally-stored vault tokens to experiment submit points."
    },
    {
        "date": "2025-03",
        "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion",
        "author": "Pihai Sun, Junjun Jiang, Yuanqi Yao, Youyu Chen, Wenbo Zhao, Kui Jiang, and Xianming Liu",
        "link": "http://arxiv.org/abs/2503.19739v2",
        "abstract": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
    },
    {
        "date": "2025-03",
        "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
        "author": "Francisco Mena, Diego Arenas, Miro Miranda, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2503.19719v1",
        "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data",
        "author": "Samuel Rey, Ernesto Curbelo, Luca Martino, Fernando Llorente, and Antonio G. Marques",
        "link": "http://arxiv.org/abs/2503.19651v1",
        "abstract": "This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure."
    },
    {
        "date": "2025-03",
        "title": "Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs",
        "author": "J\u00e9r\u00e9my Thibault, Joseph Lenormand, and Catalin Hritcu",
        "link": "http://arxiv.org/abs/2503.19609v1",
        "abstract": "Researchers aim to build secure compilation chains enforcing that if there is\nno attack a source context can mount against a source program then there is\nalso no attack an adversarial target context can mount against the compiled\nprogram. Proving that these compilation chains are secure is, however,\nchallenging, and involves a non-trivial back-translation step: for any attack a\ntarget context mounts against the compiled program one has to exhibit a source\ncontext mounting the same attack against the source program. We describe a\nnovel back-translation technique, which results in simpler proofs that can be\nmore easily mechanized in a proof assistant. Given a finite set of finite trace\nprefixes, capturing the interaction recorded during an attack between a target\ncontext and the compiled program, we build a call-return tree that we\nback-translate into a source context producing the same trace prefixes. We use\nstate in the generated source context to record the current location in the\ncall-return tree. The back-translation is done in several small steps, each\nadding to the tree new information describing how the location should change\ndepending on how the context regains control. To prove this back-translation\ncorrect we give semantics to every intermediate call-return tree language,\nusing ghost state to store information and explicitly enforce execution\ninvariants. We prove several small forward simulations, basically seeing the\nback-translation as a verified nanopass compiler. Thanks to this modular\nstructure, we are able to mechanize this complex back-translation and its\ncorrectness proof in the Rocq prover without too much effort."
    },
    {
        "date": "2025-03",
        "title": "Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization",
        "author": "Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2503.19591v1",
        "abstract": "With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality."
    },
    {
        "date": "2025-03",
        "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
        "author": "Dahyun Jung, Seungyoon Lee, Hyeonseok Moon, Chanjun Park, and Heuiseok Lim",
        "link": "http://arxiv.org/abs/2503.19540v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."
    },
    {
        "date": "2025-03",
        "title": "Towards Imperceptible Adversarial Attacks for Time Series Classification with Local Perturbations and Frequency Analysis",
        "author": "Wenwei Gu, Renyi Zhong, Jianping Zhang, and Michael R. Lyu",
        "link": "http://arxiv.org/abs/2503.19519v1",
        "abstract": "Adversarial attacks in time series classification (TSC) models have recently\ngained attention due to their potential to compromise model robustness.\nImperceptibility is crucial, as adversarial examples detected by the human\nvision system (HVS) can render attacks ineffective. Many existing methods fail\nto produce high-quality imperceptible examples, often generating perturbations\nwith more perceptible low-frequency components, like square waves, and global\nperturbations that reduce stealthiness. This paper aims to improve the\nimperceptibility of adversarial attacks on TSC models by addressing frequency\ncomponents and time series locality. We propose the Shapelet-based\nFrequency-domain Attack (SFAttack), which uses local perturbations focused on\ntime series shapelets to enhance discriminative information and stealthiness.\nAdditionally, we introduce a low-frequency constraint to confine perturbations\nto high-frequency components, enhancing imperceptibility."
    },
    {
        "date": "2025-03",
        "title": "SparSamp: Efficient Provably Secure Steganography Based on Sparse Sampling",
        "author": "Yaofei Wang, Gang Pei, Kejiang Chen, Jinyang Ding, Chao Pan, Weilong Pang, Donghui Hu, and Weiming Zhang",
        "link": "http://arxiv.org/abs/2503.19499v1",
        "abstract": "Steganography embeds confidential data within seemingly innocuous\ncommunications. Provable security in steganography, a long-sought goal, has\nbecome feasible with deep generative models. However, existing methods face a\ncritical trade-off between security and efficiency. This paper introduces\nSparSamp, an efficient provably secure steganography method based on sparse\nsampling. SparSamp embeds messages by combining them with pseudo-random numbers\nto obtain message-derived random numbers for sampling. It enhances extraction\naccuracy and embedding capacity by increasing the sampling intervals and making\nthe sampling process sparse. SparSamp preserves the original probability\ndistribution of the generative model, thus ensuring security. It introduces\nonly $O(1)$ additional complexity per sampling step, enabling the fastest\nembedding speed without compromising generation speed. SparSamp is designed to\nbe plug-and-play; message embedding can be achieved by simply replacing the\nsampling component of an existing generative model with SparSamp. We\nimplemented SparSamp in text, image, and audio generation models. It can\nachieve embedding speeds of up to 755 bits/second with GPT-2, 5046 bits/second\nwith DDPM, and 9,223 bits/second with WaveRNN."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model",
        "author": "Changyong He, Jin Zeng, Jiawei Zhang, and Jiajie Guo",
        "link": "http://arxiv.org/abs/2503.19448v1",
        "abstract": "Time-of-Flight (ToF) sensors efficiently capture scene depth, but the\nnonlinear depth construction procedure often results in extremely large noise\nvariance or even invalid areas. Recent methods based on deep neural networks\n(DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when\npresented with severe noise corruption due to limited prior knowledge of ToF\ndata distribution. In this paper, we propose DepthCAD, a novel ToF denoising\napproach that ensures global structural smoothness by leveraging the rich prior\nknowledge in Stable Diffusion and maintains local metric accuracy by steering\nthe diffusion process with confidence guidance. To adopt the pretrained image\ndiffusion model to ToF depth denoising, we apply the diffusion on raw ToF\ncorrelation measurements with dynamic range normalization before converting to\ndepth maps. Experimental results validate the state-of-the-art performance of\nthe proposed scheme, and the evaluation on real data further verifies its\nrobustness against real-world ToF noise."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
        "author": "Hengyu Wu, and Yang Cao",
        "link": "http://arxiv.org/abs/2503.19338v1",
        "abstract": "The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction."
    },
    {
        "date": "2025-03",
        "title": "Efficient Adversarial Detection Frameworks for Vehicle-to-Microgrid Services in Edge Computing",
        "author": "Ahmed Omara, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2503.19318v1",
        "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into\nmicrogrid control systems, the risk of malicious actors exploiting\nvulnerabilities in Machine Learning (ML) algorithms to disrupt power generation\nand distribution grows. Detection models to identify adversarial attacks need\nto meet the constraints of edge environments, where computational power and\nmemory are often limited. To address this issue, we propose a novel strategy\nthat optimizes detection models for Vehicle-to-Microgrid (V2M) edge\nenvironments without compromising performance against inference and evasion\nattacks. Our approach integrates model design and compression into a unified\nprocess and results in a highly compact detection model that maintains high\naccuracy. We evaluated our method against four benchmark evasion attacks-Fast\nGradient Sign Method (FGSM), Basic Iterative Method (BIM), Carlini & Wagner\nmethod (C&W) and Conditional Generative Adversarial Network (CGAN) method-and\ntwo knowledge-based attacks, white-box and gray-box. Our optimized model\nreduces memory usage from 20MB to 1.3MB, inference time from 3.2 seconds to 0.9\nseconds, and GPU utilization from 5% to 2.68%."
    },
    {
        "date": "2025-03",
        "title": "SoK: How Robust is Audio Watermarking in Generative AI models?",
        "author": "Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, and Qiben Yan",
        "link": "http://arxiv.org/abs/2503.19176v2",
        "abstract": "Audio watermarking is increasingly used to verify the provenance of\nAI-generated content, enabling applications such as detecting AI-generated\nspeech, protecting music IP, and defending against voice cloning. To be\neffective, audio watermarks must resist removal attacks that distort signals to\nevade detection. While many schemes claim robustness, these claims are\ntypically tested in isolation and against a limited set of attacks. A\nsystematic evaluation against diverse removal attacks is lacking, hindering\npractical deployment. In this paper, we investigate whether recent watermarking\nschemes that claim robustness can withstand a broad range of removal attacks.\nFirst, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we\nsummarize their underlying technologies and potential vulnerabilities. We then\npresent a large-scale empirical study to assess their robustness. To support\nthis, we build an evaluation framework encompassing 22 types of removal attacks\n(109 configurations) including signal-level, physical-level, and AI-induced\ndistortions. We reproduce 9 watermarking schemes using open-source code,\nidentify 8 new highly effective attacks, and highlight 11 key findings that\nexpose the fundamental limitations of these methods across 3 public datasets.\nOur results reveal that none of the surveyed schemes can withstand all tested\ndistortions. This evaluation offers a comprehensive view of how current\nwatermarking methods perform under real-world threats. Our demo and code are\navailable at https://sokaudiowm.github.io/."
    },
    {
        "date": "2025-03",
        "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
        "author": "Wenhao You, Bryan Hooi, Yiwei Wang, Youke Wang, Zong Ke, Ming-Hsuan Yang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.19134v1",
        "abstract": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats."
    },
    {
        "date": "2025-03",
        "title": "Paving the way for scientific foundation models: enhancing generalization and robustness in PDEs with constraint-aware pre-training",
        "author": "Amin Totounferoush, Serge Kotchourko, Michael W. Mahoney, and Steffen Staab",
        "link": "http://arxiv.org/abs/2503.19081v1",
        "abstract": "Partial differential equations (PDEs) govern a wide range of physical\nsystems, but solving them efficiently remains a major challenge. The idea of a\nscientific foundation model (SciFM) is emerging as a promising tool for\nlearning transferable representations across diverse domains. However, SciFMs\nrequire large amounts of solution data, which may be scarce or computationally\nexpensive to generate. To maximize generalization while reducing data\ndependence, we propose incorporating PDE residuals into pre-training either as\nthe sole learning signal or in combination with data loss to compensate for\nlimited or infeasible training data. We evaluate this constraint-aware\npre-training across three key benchmarks: (i) generalization to new physics,\nwhere material properties, e.g., the diffusion coefficient, is shifted with\nrespect to the training distribution; (ii) generalization to entirely new PDEs,\nrequiring adaptation to different operators; and (iii) robustness against noisy\nfine-tuning data, ensuring stability in real-world applications. Our results\nshow that pre-training with PDE constraints significantly enhances\ngeneralization, outperforming models trained solely on solution data across all\nbenchmarks. These findings prove the effectiveness of our proposed\nconstraint-aware pre-training as a crucial component for SciFMs, providing a\nscalable approach to data-efficient, generalizable PDE solvers."
    },
    {
        "date": "2025-03",
        "title": "Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks",
        "author": "Jiazhu Dai, and Yubing Lu",
        "link": "http://arxiv.org/abs/2503.19070v2",
        "abstract": "Graph neural networks (GNNs) are widely used for graph-structured data but\nare vulnerable to membership inference attacks (MIAs) in graph classification\ntasks, which determine if a graph was part of the training dataset, potentially\ncausing data leakage. Existing MIAs rely on prediction probability vectors, but\nthey become ineffective when only prediction labels are available. We propose a\nGraph-level Label-Only Membership Inference Attack (GLO-MIA), which is based on\nthe intuition that the target model's predictions on training data are more\nstable than those on testing data. GLO-MIA generates a set of perturbed graphs\nfor target graph by adding perturbations to its effective features and queries\nthe target model with the perturbed graphs to get their prediction labels,\nwhich are then used to calculate robustness score of the target graph. Finally,\nby comparing the robustness score with a predefined threshold, the membership\nof the target graph can be inferred correctly with high probability. Our\nevaluation on three datasets and four GNN models shows that GLO-MIA achieves an\nattack accuracy of up to 0.825, outperforming baseline work by 8.5% and closely\nmatching the performance of probability-based MIAs, even with only prediction\nlabels."
    },
    {
        "date": "2025-03",
        "title": "strideSEA: A STRIDE-centric Security Evaluation Approach",
        "author": "Alvi Jawad, Jason Jaskolka, Ashraf Matrawy, and Mohamed Ibnkahla",
        "link": "http://arxiv.org/abs/2503.19030v1",
        "abstract": "Microsoft's STRIDE methodology is at the forefront of threat modeling,\nsupporting the increasingly critical quality attribute of security in\nsoftware-intensive systems. However, in a comprehensive security evaluation\nprocess, the general consensus is that the STRIDE classification is only useful\nfor threat elicitation, isolating threat modeling from the other security\nevaluation activities involved in a secure software development life cycle\n(SDLC). We present strideSEA, a STRIDE-centric Security Evaluation Approach\nthat integrates STRIDE as the central classification scheme into the security\nactivities of threat modeling, attack scenario analysis, risk analysis, and\ncountermeasure recommendation that are conducted alongside software engineering\nactivities in secure SDLCs. The application of strideSEA is demonstrated in a\nreal-world online immunization system case study. Using STRIDE as a single\nunifying thread, we bind existing security evaluation approaches in the four\nsecurity activities of strideSEA to analyze (1) threats using Microsoft threat\nmodeling tool, (2) attack scenarios using attack trees, (3) systemic risk using\nNASA's defect detection and prevention (DDP) technique, and (4) recommend\ncountermeasures based on their effectiveness in reducing the most critical\nrisks using DDP. The results include a detailed quantitative assessment of the\nsecurity of the online immunization system with a clear definition of the role\nand advantages of integrating STRIDE in the evaluation process. Overall, the\nunified approach in strideSEA enables a more structured security evaluation\nprocess, allowing easier identification and recommendation of countermeasures,\nthus supporting the security requirements and eliciting design considerations,\ninforming the software development life cycle of future software-based\ninformation systems."
    },
    {
        "date": "2025-03",
        "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
        "author": "Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, and Sahin Albayrak",
        "link": "http://arxiv.org/abs/2503.18903v1",
        "abstract": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications."
    },
    {
        "date": "2025-03",
        "title": "Secure Edge Computing Reference Architecture for Data-driven Structural Health Monitoring: Lessons Learned from Implementation and Benchmarking",
        "author": "Sheikh Muhammad Farjad, Sandeep Reddy Patllola, Yonas Kassa, George Grispos, and Robin Gandhi",
        "link": "http://arxiv.org/abs/2503.18857v1",
        "abstract": "Structural Health Monitoring (SHM) plays a crucial role in maintaining aging\nand critical infrastructure, supporting applications such as smart cities and\ndigital twinning. These applications demand machine learning models capable of\nprocessing large volumes of real-time sensor data at the network edge. However,\nexisting approaches often neglect the challenges of deploying machine learning\nmodels at the edge or are constrained by vendor-specific platforms. This paper\nintroduces a scalable and secure edge-computing reference architecture tailored\nfor data-driven SHM. We share practical insights from deploying this\narchitecture at the Memorial Bridge in New Hampshire, US, referred to as the\nLiving Bridge project. Our solution integrates a commercial data acquisition\nsystem with off-the-shelf hardware running an open-source edge-computing\nplatform, remotely managed and scaled through cloud services. To support the\ndevelopment of data-driven SHM systems, we propose a resource consumption\nbenchmarking framework called edgeOps to evaluate the performance of machine\nlearning models on edge devices. We study this framework by collecting resource\nutilization data for machine learning models typically used in SHM applications\non two different edge computing hardware platforms. edgeOps was specifically\nstudied on off-the-shelf Linux and ARM-based edge devices. Our findings\ndemonstrate the impact of platform and model selection on system performance,\nproviding actionable guidance for edge-based SHM system design."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
        "author": "Wenxi Chen, Raymond A. Yeh, Shaoshuai Mou, and Yan Gu",
        "link": "http://arxiv.org/abs/2503.18784v1",
        "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles",
        "author": "Der-Hau Lee",
        "link": "http://arxiv.org/abs/2503.18752v1",
        "abstract": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers."
    },
    {
        "date": "2025-03",
        "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting",
        "author": "Lijiang Li, Jinglu Wang, Xiang Ming, and Yan Lu",
        "link": "http://arxiv.org/abs/2503.18718v1",
        "abstract": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time."
    },
    {
        "date": "2025-03",
        "title": "Robust face recognition based on the wing loss and the $\\ell_1$ regularization",
        "author": "Yaoyao Yun, and Jianwen Xu",
        "link": "http://arxiv.org/abs/2503.18652v1",
        "abstract": "In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition."
    },
    {
        "date": "2025-03",
        "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling",
        "author": "Kunyang Li, and Ming Hou",
        "link": "http://arxiv.org/abs/2503.18631v1",
        "abstract": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions."
    },
    {
        "date": "2025-03",
        "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
        "author": "Hadi Mohammadi, Ehsan Nazerfard, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2503.18569v1",
        "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning."
    },
    {
        "date": "2025-03",
        "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
        "author": "Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, and Luping Zhou",
        "link": "http://arxiv.org/abs/2503.18536v1",
        "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module."
    },
    {
        "date": "2025-03",
        "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
        "author": "Jiate Li, Meng Pang, Yun Dong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.18503v1",
        "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training."
    },
    {
        "date": "2025-03",
        "title": "Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study",
        "author": "Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.18487v1",
        "abstract": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
        "author": "Chenfei Liao, Kaiyu Lei, Xu Zheng, Junha Moon, Zhixiong Wang, Yixuan Wang, Danda Pani Paudel, Luc Van Gool, and Xuming Hu",
        "link": "http://arxiv.org/abs/2503.18445v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "author": "Wen Bai, Yi Wong, Xiao Qiao, and Chin Pang Ho",
        "link": "http://arxiv.org/abs/2503.18436v1",
        "abstract": "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity."
    },
    {
        "date": "2025-03",
        "title": "RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data",
        "author": "Xudong Mou, Rui Wang, Bo Li, Tianyu Wo, Jie Sun, Hui Wang, and Xudong Liu",
        "link": "http://arxiv.org/abs/2503.18385v1",
        "abstract": "The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https://github.com/ruiking04/RoCA."
    },
    {
        "date": "2025-03",
        "title": "Attacking and Improving the Tor Directory Protocol",
        "author": "Zhongtang Luo, Adithya Bhat, Kartik Nayak, and Aniket Kate",
        "link": "http://arxiv.org/abs/2503.18345v1",
        "abstract": "The Tor network enhances clients' privacy by routing traffic through an\noverlay network of volunteered intermediate relays. Tor employs a distributed\nprotocol among nine hard-coded Directory Authority (DA) servers to securely\ndisseminate information about these relays to produce a new consensus document\nevery hour. With a straightforward voting mechanism to ensure consistency, the\nprotocol is expected to be secure even when a minority of those authorities get\ncompromised. However, the current consensus protocol is flawed: it allows an\nequivocation attack that enables only a single compromised authority to create\na valid consensus document with malicious relays. Importantly the vulnerability\nis not innocuous: We demonstrate that the compromised authority can effectively\ntrick a targeted client into using the equivocated consensus document in an\nundetectable manner. Moreover, even if we have archived Tor consensus documents\navailable since its beginning, we cannot be sure that no client was ever\ntricked.\n  We propose a two-stage solution to deal with this exploit. In the short term,\nwe have developed and deployed TorEq, a monitor to detect such exploits\nreactively: the Tor clients can refer to the monitor before updating the\nconsensus to ensure no equivocation. To solve the problem proactively, we first\ndefine the Tor DA consensus problem as the interactive consistency (IC) problem\nfrom the distributed computing literature. We then design DirCast, a novel\nsecure Byzantine Broadcast protocol that requires minimal code change from the\ncurrent Tor DA code base. Our protocol has near-optimal efficiency that uses\noptimistically five rounds and at most nine rounds to reach an agreement in the\ncurrent nine-authority system. We are communicating with the Tor security team\nto incorporate the solutions into the Tor project."
    },
    {
        "date": "2025-03",
        "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
        "author": "Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, and Tsuyoshi Takatani",
        "link": "http://arxiv.org/abs/2503.18341v1",
        "abstract": "Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning."
    },
    {
        "date": "2025-03",
        "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD",
        "author": "Paul K. Mandal",
        "link": "http://arxiv.org/abs/2503.18290v1",
        "abstract": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences."
    },
    {
        "date": "2025-03",
        "title": "Enhance GNNs with Reliable Confidence Estimation via Adversarial Calibration Learning",
        "author": "Yilong Wang, Jiahao Zhang, Tianxiang Zhao, and Suhang Wang",
        "link": "http://arxiv.org/abs/2503.18235v1",
        "abstract": "Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios."
    },
    {
        "date": "2025-03",
        "title": "Literature Review: Cyber Security Monitoring in Maritime",
        "author": "Risto Vaarandi, Leonidas Tsiopoulos, Gabor Visky, Muaan Ur Rehman, and Hayretdin Bahsi",
        "link": "http://arxiv.org/abs/2503.18173v1",
        "abstract": "In recent years, many cyber incidents have happened in the maritime sector,\ntargeting the information technology (IT) and operational technology (OT)\ninfrastructure. Although several systematization-of-knowledge papers have been\npublished in the maritime field, none of the previous studies has focused on\ncyber security monitoring, which aims at timely detection of cyber attacks with\nautomated methods. The current article addresses this research gap and surveys\nthe methods, algorithms, tools and architectures used for cyber security\nmonitoring in the maritime sector. For the survey, a systematic literature\nreview of cyber security monitoring studies is conducted in this article,\nfollowing the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) protocol. The first contribution of this article is the\nbibliometric analysis of related literature and the identification of the main\nresearch themes in previous works. For that purpose, our article presents a\ntaxonomy for existing studies which highlights the main properties of maritime\ncyber security monitoring research. The second contribution of this article is\nan in-depth analysis of previous works and the identification of research gaps\nand limitations in existing literature. Based on our findings, we outline\nfuture research directions for cyber security monitoring in the maritime field."
    },
    {
        "date": "2025-03",
        "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
        "author": "Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, and An-An Liu",
        "link": "http://arxiv.org/abs/2503.17987v1",
        "abstract": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}"
    },
    {
        "date": "2025-03",
        "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
        "author": "Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, and Arthur C. Graesser",
        "link": "http://arxiv.org/abs/2503.18982v1",
        "abstract": "Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes."
    },
    {
        "date": "2025-03",
        "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
        "author": "Dong Zhao, Jinlong Li, Shuang Wang, Mengyao Wu, Qi Zang, Nicu Sebe, and Zhun Zhong",
        "link": "http://arxiv.org/abs/2503.17940v1",
        "abstract": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."
    },
    {
        "date": "2025-03",
        "title": "Understanding and Mitigating Side and Covert Channel Vulnerabilities Introduced by RowHammer Defenses",
        "author": "F. Nisa Bostanc\u0131, O\u011fuzhan Canpolat, Ataberk Olgun, \u0130smail Emir Y\u00fcksel, Mohammad Sadrosadati, A. Giray Ya\u011fl\u0131k\u00e7\u0131, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2503.17891v1",
        "abstract": "DRAM chips are vulnerable to read disturbance phenomena (e.g., RowHammer and\nRowPress), where repeatedly accessing or keeping open a DRAM row causes\nbitflips in nearby rows, due to DRAM density scaling. Attackers can leverage\nRowHammer bitflips in real systems to take over systems and leak data.\nConsequently, many prior works propose mitigations, including recent DDR\nspecifications introducing new mitigation frameworks (e.g., PRAC and RFM). For\nrobustness, it is timely and critical to analyze other security implications\nthat widely-adopted RowHammer mitigations can introduce. Unfortunately, no\nprior work analyzes the timing channel vulnerabilities introduced by RowHammer\nmitigations. In this work, we present the first analysis and evaluation of\ntiming channel vulnerabilities introduced by RowHammer mitigations. Our key\nobservation is that RowHammer mitigations' preventive actions have two features\nthat enable timing channels. First, preventive actions often reduce DRAM\nbandwidth availability because they block access to DRAM, thereby delaying\nregular memory requests and resulting in increased memory latencies. Second,\npreventive actions can be triggered on demand as they depend on memory access\npatterns. We systematically analyze two latest industry mitigations and\nintroduce LeakyHammer, a new class of attacks that leverage the RowHammer\nmitigation-induced memory latency differences to establish communication\nchannels between processes and leak secrets. First, we build two covert channel\nattacks exploiting two state-of-the-art RowHammer mitigations, providing 41.9\nKbps and 54.0 Kbps channel capacity. Second, we demonstrate a proof-of-concept\nwebsite fingerprinting attack that can identify visited websites based on the\nRowHammer mitigation behavior. We discuss 3 mitigations against LeakyHammer and\nshow that fundamentally mitigating LeakyHammer induces significant performance\noverheads."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Mitigating DDoS Attacks with AI: A Survey",
        "author": "Alexandru Apostu, Silviu Gheorghe, Andrei H\u00eeji, Nicolae Cleju, Andrei P\u0103tra\u015fcu, Cristian Rusu, Radu Ionescu, and Paul Irofti",
        "link": "http://arxiv.org/abs/2503.17867v1",
        "abstract": "Distributed Denial of Service attacks represent an active cybersecurity\nresearch problem. Recent research shifted from static rule-based defenses\ntowards AI-based detection and mitigation. This comprehensive survey covers\nseveral key topics. Preeminently, state-of-the-art AI detection methods are\ndiscussed. An in-depth taxonomy based on manual expert hierarchies and an\nAI-generated dendrogram are provided, thus settling DDoS categorization\nambiguities. An important discussion on available datasets follows, covering\ndata format options and their role in training AI detection methods together\nwith adversarial training and examples augmentation. Beyond detection, AI based\nmitigation techniques are surveyed as well. Finally, multiple open research\ndirections are proposed."
    },
    {
        "date": "2025-03",
        "title": "NVBleed: Covert and Side-Channel Attacks on NVIDIA Multi-GPU Interconnect",
        "author": "Yicheng Zhang, Ravan Nazaraliyev, Sankha Baran Dutta, Andres Marquez, Kevin Barker, and Nael Abu-Ghazaleh",
        "link": "http://arxiv.org/abs/2503.17847v1",
        "abstract": "Multi-GPU systems are becoming increasingly important in highperformance\ncomputing (HPC) and cloud infrastructure, providing acceleration for\ndata-intensive applications, including machine learning workloads. These\nsystems consist of multiple GPUs interconnected through high-speed networking\nlinks such as NVIDIA's NVLink. In this work, we explore whether the\ninterconnect on such systems can offer a novel source of leakage, enabling new\nforms of covert and side-channel attacks. Specifically, we reverse engineer the\noperations of NVlink and identify two primary sources of leakage: timing\nvariations due to contention and accessible performance counters that disclose\ncommunication patterns. The leakage is visible remotely and even across VM\ninstances in the cloud, enabling potentially dangerous attacks. Building on\nthese observations, we develop two types of covert-channel attacks across two\nGPUs, achieving a bandwidth of over 70 Kbps with an error rate of 4.78% for the\ncontention channel. We develop two end-to-end crossGPU side-channel attacks:\napplication fingerprinting (including 18 high-performance computing and deep\nlearning applications) and 3D graphics character identification within Blender,\na multi-GPU rendering application. These attacks are highly effective,\nachieving F1 scores of up to 97.78% and 91.56%, respectively. We also discover\nthat leakage surprisingly occurs across Virtual Machines on the Google Cloud\nPlatform (GCP) and demonstrate a side-channel attack on Blender, achieving F1\nscores exceeding 88%. We also explore potential defenses such as managing\naccess to counters and reducing the resolution of the clock to mitigate the two\nsources of leakage."
    },
    {
        "date": "2025-03",
        "title": "Connectedness: a dimension of security bug severity assessment for measuring uncertainty",
        "author": "Shue Long Chan",
        "link": "http://arxiv.org/abs/2503.17813v1",
        "abstract": "Current frameworks for evaluating security bug severity, such as the Common\nVulnerability Scoring System (CVSS), prioritize the ratio of exploitability to\nimpact. This paper suggests that the above approach measures the \"known knowns\"\nbut inadequately addresses the \"known unknowns\" especially when there exist\nmultiple possible exploit paths and side effects, which introduce significant\nuncertainty. This paper introduces the concept of connectedness, which measures\nhow strongly a security bug is connected with different entities, thereby\nreflecting the uncertainty of impact and the exploit potential. This work\nhighlights the critical but underappreciated role connectedness plays in\nseverity assessments."
    },
    {
        "date": "2025-03",
        "title": "Design and implementation of a novel cryptographically secure pseudorandom number generator",
        "author": "Juan Di Mauro, Eduardo Salazar, and Hugo D. Scolnik",
        "link": "http://arxiv.org/abs/2503.17767v1",
        "abstract": "The aim of this paper is to present a new design for a pseudorandom number\ngenerator (PRNG) that is cryptographically secure, passes all of the usual\nstatistical tests referenced in the literature and hence generates high quality\nrandom sequences, that is compact and easy to implement in practice, of\nportable design and offering reasonable execution times. Our procedure achieves\nthose objectives through the use of a sequence of modular exponentiations\nfollowed by the application of Feistel-like boxes that mix up bits using a\nnonlinear function. The results of extensive statistical tests on sequences of\nabout 2^40 bits in size generated by our algorithm are also presented."
    },
    {
        "date": "2025-03",
        "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "author": "Jie Zhang, Zhongqi Wang, Shiguang Shan, and Xilin Chen",
        "link": "http://arxiv.org/abs/2503.17724v1",
        "abstract": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA."
    },
    {
        "date": "2025-03",
        "title": "Measuring the Robustness of Audio Deepfake Detectors",
        "author": "Xiang Li, Pin-Yu Chen, and Wenqi Wei",
        "link": "http://arxiv.org/abs/2503.17577v1",
        "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings."
    },
    {
        "date": "2025-03",
        "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
        "author": "Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, and Hassan Mansour",
        "link": "http://arxiv.org/abs/2503.17351v1",
        "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."
    },
    {
        "date": "2025-03",
        "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
        "author": "John Naulty, Eason Chen, Joy Wang, George Digkas, and Kostas Chalkias",
        "link": "http://arxiv.org/abs/2503.17302v1",
        "abstract": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
    },
    {
        "date": "2025-03",
        "title": "UAV Resilience Against Stealthy Attacks",
        "author": "Arthur Amorim, Max Taylor, Trevor Kann, Gary T. Leavens, William L. Harrison, and Lance Joneckis",
        "link": "http://arxiv.org/abs/2503.17298v1",
        "abstract": "Unmanned aerial vehicles (UAVs) depend on untrusted software components to\nautomate dangerous or critical missions, making them a desirable target for\nattacks. Some work has been done to prevent an attacker who has either\ncompromised a ground control station or parts of a UAV's software from\nsabotaging the vehicle, but not both. We present an architecture running a UAV\nsoftware stack with runtime monitoring and seL4-based software isolation that\nprevents attackers from both exploiting software bugs and utilizing stealthy\nattacks. Our architecture retrofits legacy UAVs and secures the popular MAVLink\nprotocol, making wide adoption possible."
    },
    {
        "date": "2025-03",
        "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
        "author": "Jan Rabenseifner, Sven Klaassen, Jannis Kueck, and Philipp Bach",
        "link": "http://arxiv.org/abs/2503.17290v1",
        "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
    },
    {
        "date": "2025-03",
        "title": "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies",
        "author": "Ronan Mouchoux, and Fran\u00e7ois Moerman",
        "link": "http://arxiv.org/abs/2503.17219v1",
        "abstract": "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains."
    },
    {
        "date": "2025-03",
        "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization",
        "author": "Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, and Wang Lu",
        "link": "http://arxiv.org/abs/2503.17211v1",
        "abstract": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."
    },
    {
        "date": "2025-03",
        "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
        "author": "Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, and Ada Sedova",
        "link": "http://arxiv.org/abs/2503.17173v1",
        "abstract": "The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments."
    },
    {
        "date": "2025-03",
        "title": "Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers",
        "author": "Gaojie Jin, Tianjin Huang, Ronghui Mu, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2503.17172v1",
        "abstract": "Recent studies have identified a critical challenge in deep neural networks\n(DNNs) known as ``robust fairness\", where models exhibit significant\ndisparities in robust accuracy across different classes. While prior work has\nattempted to address this issue in adversarial robustness, the study of\nworst-class certified robustness for smoothed classifiers remains unexplored.\nOur work bridges this gap by developing a PAC-Bayesian bound for the\nworst-class error of smoothed classifiers. Through theoretical analysis, we\ndemonstrate that the largest eigenvalue of the smoothed confusion matrix\nfundamentally influences the worst-class error of smoothed classifiers. Based\non this insight, we introduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy\nof the smoothed classifier and further improve its worst-class certified\nrobustness. We provide extensive experimental validation across multiple\ndatasets and model architectures to demonstrate the effectiveness of our\napproach."
    },
    {
        "date": "2025-03",
        "title": "Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes",
        "author": "Orkun Furat, Sabrina Weber, Johannes Schubert, Ren\u00e9 Rekers, Maximilian Luczak, Erik Glatt, Andreas Wiegmann, J\u00fcrgen Janek, Anja Bielefeld, and Volker Schmidt",
        "link": "http://arxiv.org/abs/2503.17171v1",
        "abstract": "This paper presents a computational method for generating virtual 3D\nmorphologies of functional materials using low-parametric stochastic geometry\nmodels, i.e., digital twins, calibrated with 2D microscopy images. These\ndigital twins allow systematic parameter variations to simulate various\nmorphologies, that can be deployed for virtual materials testing by means of\nspatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to\ngenerate realistic 3D morphologies. However, GANs often comprise of numerous\nuninterpretable parameters make systematic variation of morphologies for\nvirtual materials testing challenging. In contrast, low-parametric stochastic\ngeometry models (e.g., based on Gaussian random fields) enable targeted\nvariation but may struggle to mimic complex morphologies. Combining GANs with\nadvanced stochastic geometry models (e.g., excursion sets of more general\nrandom fields) addresses these limitations, allowing model calibration solely\nfrom 2D image data. This approach is demonstrated by generating a digital twin\nof all-solid-state battery (ASSB) cathodes. Since the digital twins are\nparametric, they support systematic exploration of structural scenarios and\ntheir macroscopic properties. The proposed method facilitates simulation\nstudies for optimizing 3D morphologies, benefiting not only ASSB cathodes but\nalso other materials with similar structures."
    },
    {
        "date": "2025-03",
        "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving",
        "author": "Alexandra Arzberger, and Ramin Tavakoli Kolagari",
        "link": "http://arxiv.org/abs/2503.17168v2",
        "abstract": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults."
    },
    {
        "date": "2025-03",
        "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks",
        "author": "Ekaterina Dmitrieva, and Maksim Kaledin",
        "link": "http://arxiv.org/abs/2503.17141v1",
        "abstract": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software simplifying downstream speech tasks. Still, modern Deep\nLearning (DL) solutions often require high amount of computational resources\nwhat makes their usage on low-resource devices challenging. We present\nHiFi-Stream, an optimized version of recently published HiFi++ model. Our\nexperiments demonstrate that HiFiStream saves most of the qualities of the\noriginal model despite its size and computational complexity: the lightest\nversion has only around 490k parameters which is 3.5x reduction in comparison\nto the original HiFi++ making it one of the smallest and fastest models\navailable. The model is evaluated in streaming setting where it demonstrates\nits superior performance in comparison to modern baselines."
    },
    {
        "date": "2025-03",
        "title": "EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations",
        "author": "Tadeu Freitas, Erick Silva, Rehana Yasmin, Ali Shoker, Manuel E. Correia, Rolando Martins, and Paulo Esteves-Verissimo",
        "link": "http://arxiv.org/abs/2503.16984v1",
        "abstract": "Vehicle cybersecurity has emerged as a critical concern, driven by the\ninnovation in the automotive industry, e.g., automomous, electric, or\nconnnected vehicles. Current efforts to address these challenges are\nconstrained by the limited computational resources of vehicles and the reliance\non connected infrastructures. This motivated the foundation of Vehicle Security\nOperations Centers (VSOCs) that extend IT-based Security Operations Centers\n(SOCs) to cover the entire automotive ecosystem, both the in-vehicle and\noff-vehicle scopes. Security Orchestration, Automation, and Response (SOAR)\ntools are considered key for impelementing an effective cybersecurity solution.\nHowever, existing state-of-the-art solutions depend on infrastructure networks\nsuch as 4G, 5G, and WiFi, which often face scalability and congestion issues.\nTo address these limitations, we propose a novel SOAR architecture EVSOAR that\nleverages the EV charging stations for connectivity and computing to enhance\nvehicle cybersecurity. Our EV-specific SOAR architecture enables real-time\nanalysis and automated responses to cybersecurity threats closer to the EV,\nreducing the cellular latency, bandwidth, and interference limitations. Our\nexperimental results demonstrate a significant improvement in latency,\nstability, and scalability through the infrastructure and the capacity to\ndeploy computationally intensive applications, that are otherwise infeasible\nwithin the resource constraints of individual vehicles."
    },
    {
        "date": "2025-03",
        "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
        "author": "Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, and Hang Su",
        "link": "http://arxiv.org/abs/2503.16975v1",
        "abstract": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."
    },
    {
        "date": "2025-03",
        "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
        "author": "Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, and Yi Yang",
        "link": "http://arxiv.org/abs/2503.16964v1",
        "abstract": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery."
    },
    {
        "date": "2025-03",
        "title": "CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.16950v1",
        "abstract": "Stack-based memory corruption vulnerabilities have\n  long been exploited by attackers to execute arbitrary code\n  or perform unauthorized memory operations. Various defense\n  mechanisms have been introduced to mitigate stack memory\n  errors, but they typically focus on specific attack types, incur\n  substantial performance overhead, or suffer from compatibility\n  limitations.In this paper, we present CleanStack, an efficient,\n  highly compatible, and comprehensive stack protection mech anism. CleanStack\nisolates stack objects influenced by external\n  input from other safe stack objects, thereby preventing attackers\n  from modifying return addresses via controlled stack objects.\n  Additionally, by randomizing the placement of tainted stack\n  objects within the Unclean Stack, CleanStack mitigates non control data\nattacks by preventing attackers from predicting the\n  stack layout.A key component of CleanStack is the identifica tion of tainted\nstack objects. We analyze both static program\n  analysis and heuristic methods for this purpose. To maximize\n  compatibility, we adopt a heuristic approach and implement\n  CleanStack within the LLVM compiler framework, applying it to\n  SPEC CPU2017 benchmarks and a real-world application.Our\n  security evaluation demonstrates that CleanStack significantly\n  reduces the exploitability of stack-based memory errors by\n  providing a dual-stack system with isolation and randomization.\n  Performance evaluation results indicate that CleanStack incurs\n  an execution overhead of only 1.73% on the SPEC CPU2017\n  benchmark while introducing a minimal memory overhead of\n  just 0.04%. Compared to existing stack protection techniques,\n  CleanStack achieves an optimal balance between protection\n  coverage, runtime overhead, and compatibility, making it one\n  of the most comprehensive and efficient stack security solutions\n  to date."
    },
    {
        "date": "2025-03",
        "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
        "author": "Shayne Longpre, Kevin Klyman, Ruth E. Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M. Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca S. Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, and Arvind Narayanan",
        "link": "http://arxiv.org/abs/2503.16861v2",
        "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
    },
    {
        "date": "2025-03",
        "title": "Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic",
        "author": "Yali Yuan, Qianqi Niu, and Yachao Yuan",
        "link": "http://arxiv.org/abs/2503.16847v1",
        "abstract": "Flow correlation attacks is an efficient network attacks, aiming to expose\nthose who use anonymous network services, such as Tor. Conducting such attacks\nduring the early stages of network communication is particularly critical for\nscenarios demanding rapid decision-making, such as cybercrime detection or\nfinancial fraud prevention. Although recent studies have made progress in flow\ncorrelation attacks techniques, research specifically addressing flow\ncorrelation with early network traffic flow remains limited. Moreover, due to\nfactors such as model complexity, training costs, and real-time requirements,\nexisting technologies cannot be directly applied to flow correlation with early\nnetwork traffic flow. In this paper, we propose flow correlation attack with\nearly network traffic, named Early-MFC, based on multi-view triplet networks.\nThe proposed approach extracts multi-view traffic features from the payload at\nthe transport layer and the Inter-Packet Delay. It then integrates multi-view\nflow information, converting the extracted features into shared embeddings. By\nleveraging techniques such as metric learning and contrastive learning, the\nmethod optimizes the embeddings space by ensuring that similar flows are mapped\ncloser together while dissimilar flows are positioned farther apart. Finally,\nBayesian decision theory is applied to determine flow correlation, enabling\nhigh-accuracy flow correlation with early network traffic flow. Furthermore, we\ninvestigate flow correlation attacks under extra-early network traffic flow\nconditions. To address this challenge, we propose Early-MFC+, which utilizes\npayload data to construct embedded feature representations, ensuring robust\nperformance even with minimal packet availability."
    },
    {
        "date": "2025-03",
        "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
        "author": "Massa Baali, Xiang Li, Hao Chen, Rita Singh, and Bhiksha Raj",
        "link": "http://arxiv.org/abs/2503.16718v1",
        "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released."
    },
    {
        "date": "2025-03",
        "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents",
        "author": "Mihaela-Larisa Clement, M\u00f3nika Farsang, Felix Resch, and Radu Grosu",
        "link": "http://arxiv.org/abs/2503.16711v1",
        "abstract": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task."
    },
    {
        "date": "2025-03",
        "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
        "author": "Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, and Yushun Dong",
        "link": "http://arxiv.org/abs/2503.16693v1",
        "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine\nLearning as a Service (GMLaaS) platforms, yet they remain vulnerable to\ngraph-based model extraction attacks (MEAs), where adversaries reconstruct\nsurrogate models by querying the victim model. Existing defense mechanisms,\nsuch as watermarking and fingerprinting, suffer from poor real-time\nperformance, susceptibility to evasion, or reliance on post-attack\nverification, making them inadequate for handling the dynamic characteristics\nof graph-based MEA variants. To address these limitations, we propose ATOM, a\nnovel real-time MEA detection framework tailored for GNNs. ATOM integrates\nsequential modeling and reinforcement learning to dynamically detect evolving\nattack patterns, while leveraging $k$-core embedding to capture the structural\nproperties, enhancing detection precision. Furthermore, we provide theoretical\nanalysis to characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate that ATOM\noutperforms existing approaches in detection performance, maintaining stable\nacross different time steps, thereby offering a more effective defense\nmechanism for GMLaaS environments."
    },
    {
        "date": "2025-03",
        "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
        "author": "Qiankun Shi, Jie Peng, Kun Yuan, Xiao Wang, and Qing Ling",
        "link": "http://arxiv.org/abs/2503.16337v1",
        "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight."
    },
    {
        "date": "2025-03",
        "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
        "author": "Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, and Jizhao Liu",
        "link": "http://arxiv.org/abs/2503.16287v1",
        "abstract": "The rapid development of low-Earth orbit (LEO) satellite constellations and\nsatellite communication systems has elevated the importance of secure video\ntransmission, which is the key to applications such as remote sensing, disaster\nrelief, and secure information exchange. In this context, three serious issues\narise concerning real-time encryption of videos on satellite embedded devices:\n(a) the challenge of achieving real-time performance; (b) the limitations posed\nby the constrained computing performance of satellite payloads; and (c) the\npotential for excessive power consumption leading to overheating, thereby\nescalating safety risks. To overcome these challenges, this study introduced a\nnovel approach for encrypting videos by employing two 1D chaotic maps, which\nwas deployed on a satellite for the first time. The experiment on the satellite\nconfirms that our scheme is suitable for complex satellite environments. In\naddition, the proposed chaotic maps were implemented on a Field Programmable\nGate Array (FPGA) platform, and simulation results showed consistency with\nthose obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B\ndemonstrate exceptional real-time performance and low power consumption,\nvalidating both the hardware feasibility and the stability of our design.\nRigorous statistical testing also confirms the scheme's resilience against a\nvariety of attacks, underscoring its potential for secure, real-time data\ntransmission in satellite communication systems."
    },
    {
        "date": "2025-03",
        "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
        "author": "Jo\u00e3o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cin\u00e0, Carlos Cotrini, Lea Sch\u00f6nherr, and Joachim M. Buhmann",
        "link": "http://arxiv.org/abs/2503.16271v1",
        "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
    },
    {
        "date": "2025-03",
        "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "author": "Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, and Chenjun Ma",
        "link": "http://arxiv.org/abs/2503.16266v1",
        "abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from\nmodels, leading to privacy leakage, particularly in facial recognition systems.\nAlthough many studies have enhanced the effectiveness of white-box MIAs, less\nattention has been paid to improving efficiency and utility under limited\nattacker capabilities. Existing black-box MIAs necessitate an impractical\nnumber of queries, incurring significant overhead. Therefore, we analyze the\nlimitations of existing MIAs and introduce Surrogate Model-based Inversion with\nLong-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient\nMIA for the black-box setting. We begin by analyzing the initialization of MIAs\nfrom a data distribution perspective and propose a long-tailed surrogate\ntraining method to obtain high-quality initial points. We then enhance the\nattack's effectiveness by employing the gradient-free black-box optimization\nalgorithm selected by NGOpt. Our experiments show that SMILE outperforms\nexisting state-of-the-art black-box MIAs while requiring only about 5% of the\nquery overhead."
    },
    {
        "date": "2025-03",
        "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
        "author": "Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath",
        "link": "http://arxiv.org/abs/2503.16248v1",
        "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible."
    },
    {
        "date": "2025-03",
        "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2503.16179v1",
        "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."
    },
    {
        "date": "2025-03",
        "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
        "author": "Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, and Vincent Guigue",
        "link": "http://arxiv.org/abs/2503.16161v1",
        "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
    },
    {
        "date": "2025-03",
        "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
        "author": "Marek Wodzinski, and Henning M\u00fcller",
        "link": "http://arxiv.org/abs/2503.16075v1",
        "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
    },
    {
        "date": "2025-03",
        "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
        "author": "Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, and Prisca Chinazor Amajuoyi",
        "link": "http://arxiv.org/abs/2503.16047v2",
        "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
    },
    {
        "date": "2025-03",
        "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
        "author": "Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2503.16023v1",
        "abstract": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
    },
    {
        "date": "2025-03",
        "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
        "author": "Junsung Park, Hwijeong Lee, Inha Kang, and Hyunjung Shim",
        "link": "http://arxiv.org/abs/2503.15910v2",
        "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness."
    },
    {
        "date": "2025-03",
        "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
        "author": "Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, and Bo Li",
        "link": "http://arxiv.org/abs/2503.15754v1",
        "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems."
    },
    {
        "date": "2025-03",
        "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
        "author": "Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar, and Prospero C. Naval Jr",
        "link": "http://arxiv.org/abs/2503.15726v1",
        "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications."
    },
    {
        "date": "2025-03",
        "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "author": "Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, and Chao Shen",
        "link": "http://arxiv.org/abs/2503.15404v1",
        "abstract": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."
    },
    {
        "date": "2025-03",
        "title": "Robustness of Nonlinear Representation Learning",
        "author": "Simon Buchholz, and Bernhard Sch\u00f6lkopf",
        "link": "http://arxiv.org/abs/2503.15355v1",
        "abstract": "We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes."
    },
    {
        "date": "2025-03",
        "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
        "author": "Dominik Macko, Robert Moro, and Ivan Srba",
        "link": "http://arxiv.org/abs/2503.15128v1",
        "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
    },
    {
        "date": "2025-03",
        "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
        "author": "Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, and Siyuan Huang",
        "link": "http://arxiv.org/abs/2503.15082v1",
        "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs."
    },
    {
        "date": "2025-03",
        "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
        "author": "Jiazhu Dai, and Haoyu Sun",
        "link": "http://arxiv.org/abs/2503.14922v1",
        "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples."
    },
    {
        "date": "2025-03",
        "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
        "author": "Jingyi Liao, Xun Xu, Yongyi Su, Rong-Cheng Tu, Yifan Liu, Dacheng Tao, and Xulei Yang",
        "link": "http://arxiv.org/abs/2503.14910v1",
        "abstract": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition",
        "author": "Seyed Mojtaba Mohasel, and Hamidreza Koosha",
        "link": "http://arxiv.org/abs/2503.14873v1",
        "abstract": "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness Tradeoff in Fine-Tuning",
        "author": "Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak, Yohan Beugin, Eric Pauley, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.14836v1",
        "abstract": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."
    },
    {
        "date": "2025-03",
        "title": "Robust Transmission of Punctured Text with Large Language Model-based Recovery",
        "author": "Sojeong Park, Hyeonho Noh, and Hyun Jong Yang",
        "link": "http://arxiv.org/abs/2503.14831v1",
        "abstract": "With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions."
    },
    {
        "date": "2025-03",
        "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation",
        "author": "Shiyi Jiang, Farshad Firouzi, and Krishnendu Chakrabarty",
        "link": "http://arxiv.org/abs/2503.16542v1",
        "abstract": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data."
    },
    {
        "date": "2025-03",
        "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
        "author": "Prashant Kulkarni, and Assaf Namer",
        "link": "http://arxiv.org/abs/2503.15560v1",
        "abstract": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security."
    },
    {
        "date": "2025-03",
        "title": "Robust Object Detection of Underwater Robot based on Domain Generalization",
        "author": "Pinhao Song",
        "link": "http://arxiv.org/abs/2503.19929v1",
        "abstract": "Object detection aims to obtain the location and the category of specific\nobjects in a given image, which includes two tasks: classification and\nlocation. In recent years, researchers tend to apply object detection to\nunderwater robots equipped with vision systems to complete tasks including\nseafood fishing, fish farming, biodiversity monitoring and so on. However, the\ndiversity and complexity of underwater environments bring new challenges to\nobject detection. First, aquatic organisms tend to live together, which leads\nto severe occlusion. Second, theaquatic organisms are good at hiding\nthemselves, which have a similar color to the background. Third, the various\nwater quality and changeable and extreme lighting conditions lead to the\ndistorted, low contrast, blue or green images obtained by the underwater\ncamera, resulting in domain shift. And the deep model is generally vulnerable\nto facing domain shift. Fourth, the movement of the underwater robot leads to\nthe blur of the captured image and makes the water muddy, which results in low\nvisibility of the water. This paper investigates the problems brought by the\nunderwater environment mentioned above, and aims to design a high-performance\nand robust underwater object detector."
    },
    {
        "date": "2025-03",
        "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
        "author": "Rohan Menon, Nicola Franco, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.14751v1",
        "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures."
    },
    {
        "date": "2025-03",
        "title": "A Comprehensive Study of LLM Secure Code Generation",
        "author": "Shih-Chieh Dai, Jun Xu, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2503.15554v1",
        "abstract": "LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work."
    },
    {
        "date": "2025-03",
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "author": "Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat, and Huan Liu",
        "link": "http://arxiv.org/abs/2503.15552v1",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts."
    },
    {
        "date": "2025-03",
        "title": "Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection",
        "author": "Leonardo Henrique de Melo, Gustavo de Carvalho Bertoli, Michele Nogueira, Aldri Luiz dos Santos, and Louren\u00e7o Alves Pereira Junior",
        "link": "http://arxiv.org/abs/2503.14618v1",
        "abstract": "Distributed denial-of-service (DDoS) attacks remain a critical threat to\nInternet services, causing costly disruptions. While machine learning (ML) has\nshown promise in DDoS detection, current solutions struggle with multi-domain\nenvironments where attacks must be detected across heterogeneous networks and\norganizational boundaries. This limitation severely impacts the practical\ndeployment of ML-based defenses in real-world settings.\n  This paper introduces Anomaly-Flow, a novel framework that addresses this\ncritical gap by combining Federated Learning (FL) with Generative Adversarial\nNetworks (GANs) for privacy-preserving, multi-domain DDoS detection. Our\nproposal enables collaborative learning across diverse network domains while\npreserving data privacy through synthetic flow generation. Through extensive\nevaluation across three distinct network datasets, Anomaly-Flow achieves an\naverage F1-score of $0.747$, outperforming baseline models. Importantly, our\nframework enables organizations to share attack detection capabilities without\nexposing sensitive network data, making it particularly valuable for critical\ninfrastructure and privacy-sensitive sectors.\n  Beyond immediate technical contributions, this work provides insights into\nthe challenges and opportunities in multi-domain DDoS detection, establishing a\nfoundation for future research in collaborative network defense systems. Our\nfindings have important implications for academic research and industry\npractitioners working to deploy practical ML-based security solutions."
    },
    {
        "date": "2025-03",
        "title": "Doubly robust identification of treatment effects from multiple environments",
        "author": "Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, and Fanny Yang",
        "link": "http://arxiv.org/abs/2503.14459v1",
        "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods."
    },
    {
        "date": "2025-03",
        "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
        "author": "Murong Yue, and Ziyu Yao",
        "link": "http://arxiv.org/abs/2503.15551v1",
        "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
    },
    {
        "date": "2025-03",
        "title": "Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory",
        "author": "Lucas Gnecco-Heredia, Matteo Sammut, Muni Sreenivas Pydi, Rafael Pinot, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2503.14299v1",
        "abstract": "Randomization as a mean to improve the adversarial robustness of machine\nlearning models has recently attracted significant attention. Unfortunately,\nmuch of the theoretical analysis so far has focused on binary classification,\nproviding only limited insights into the more complex multiclass setting. In\nthis paper, we take a step toward closing this gap by drawing inspiration from\nthe field of graph theory. Our analysis focuses on discrete data distributions,\nallowing us to cast the adversarial risk minimization problems within the\nwell-established framework of set packing problems. By doing so, we are able to\nidentify three structural conditions on the support of the data distribution\nthat are necessary for randomization to improve robustness. Furthermore, we are\nable to construct several data distributions where (contrarily to binary\nclassification) switching from a deterministic to a randomized solution\nsignificantly reduces the optimal adversarial risk. These findings highlight\nthe crucial role randomization can play in enhancing robustness to adversarial\nattacks in multiclass classification."
    },
    {
        "date": "2025-03",
        "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
        "author": "Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa, Alexander L\u00f6ser, Erik Rodner, and Felix A. Gers",
        "link": "http://arxiv.org/abs/2503.14572v1",
        "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes."
    },
    {
        "date": "2025-03",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "author": "Adam \u0160torek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim, Prashast Srivastava, and Suman Jana",
        "link": "http://arxiv.org/abs/2503.14281v1",
        "abstract": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
    },
    {
        "date": "2025-03",
        "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
        "author": "Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2503.14247v1",
        "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM"
    },
    {
        "date": "2025-03",
        "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
        "author": "Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, and Wei-Shi Zheng",
        "link": "http://arxiv.org/abs/2503.14198v1",
        "abstract": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat."
    },
    {
        "date": "2025-03",
        "title": "Towards properties of adversarial image perturbations",
        "author": "Egor Kuznetsov, Kirill Aistov, and Maxim Koroteev",
        "link": "http://arxiv.org/abs/2503.14111v1",
        "abstract": "Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization."
    },
    {
        "date": "2025-03",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "author": "Yuchen Niu, and Siew-Kei Lam",
        "link": "http://arxiv.org/abs/2503.14006v1",
        "abstract": "Automated insulin delivery (AID) systems have emerged as a significant\ntechnological advancement in diabetes care. These systems integrate a\ncontinuous glucose monitor, an insulin pump, and control algorithms to automate\ninsulin delivery, reducing the burden of self-management and offering enhanced\nglucose control. However, the increasing reliance on wireless connectivity and\nsoftware control has exposed AID systems to critical security risks that could\nresult in life-threatening treatment errors. This review first presents a\ncomprehensive examination of the security landscape, covering technical\nvulnerabilities, legal frameworks, and commercial product considerations, and\nan analysis of existing research on attack vectors, defence mechanisms, as well\nas evaluation methods and resources for AID systems. Despite recent\nadvancements, several open challenges remain in achieving secure AID systems,\nparticularly in standardising security evaluation frameworks and developing\ncomprehensive, lightweight, and adaptive defence strategies. As one of the most\nwidely adopted and extensively studied physiologic closed-loop control systems,\nthis review serves as a valuable reference for understanding security\nchallenges and solutions applicable to analogous medical systems."
    },
    {
        "date": "2025-03",
        "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
        "author": "Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui",
        "link": "http://arxiv.org/abs/2503.13962v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."
    },
    {
        "date": "2025-03",
        "title": "Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels",
        "author": "Yujia Tong, Yuze Wang, Jingling Yuan, and Chuang Hu",
        "link": "http://arxiv.org/abs/2503.13917v1",
        "abstract": "Model quantization enables efficient deployment of deep neural networks on\nedge devices through low-bit parameter representation, yet raises critical\nchallenges for implementing machine unlearning (MU) under data privacy\nregulations. Existing MU methods designed for full-precision models fail to\naddress two fundamental limitations in quantized networks: 1) Noise\namplification from label mismatch during data processing, and 2) Gradient\nimbalance between forgotten and retained data during training. These issues are\nexacerbated by quantized models' constrained parameter space and discrete\noptimization. We propose Q-MUL, the first dedicated unlearning framework for\nquantized models. Our method introduces two key innovations: 1) Similar Labels\nassignment replaces random labels with semantically consistent alternatives to\nminimize noise injection, and 2) Adaptive Gradient Reweighting dynamically\naligns parameter update contributions from forgotten and retained data. Through\nsystematic analysis of quantized model vulnerabilities, we establish\ntheoretical foundations for these mechanisms. Extensive evaluations on\nbenchmark datasets demonstrate Q-MUL's superiority over existing approaches."
    },
    {
        "date": "2025-03",
        "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation",
        "author": "Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, and Yanye Lu",
        "link": "http://arxiv.org/abs/2503.13895v1",
        "abstract": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available."
    },
    {
        "date": "2025-03",
        "title": "Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception",
        "author": "Jinge Ma, Jiangpeng He, and Fengqing Zhu",
        "link": "http://arxiv.org/abs/2503.13869v1",
        "abstract": "3D perception plays a crucial role in real-world applications such as\nautonomous driving, robotics, and AR/VR. In practical scenarios, 3D perception\nmodels must continuously adapt to new data and emerging object categories, but\nretraining from scratch incurs prohibitive costs. Therefore, adopting\nclass-incremental learning (CIL) becomes particularly essential. However,\nreal-world 3D point cloud data often include corrupted samples, which poses\nsignificant challenges for existing CIL methods and leads to more severe\nforgetting on corrupted data. To address these challenges, we consider the\nscenario in which a CIL model can be updated using point clouds with unknown\ncorruption to better simulate real-world conditions. Inspired by Farthest Point\nSampling, we propose a novel exemplar selection strategy that effectively\npreserves intra-class diversity when selecting replay exemplars, mitigating\nforgetting induced by data corruption. Furthermore, we introduce a point cloud\ndownsampling-based replay method to utilize the limited replay buffer memory\nmore efficiently, thereby further enhancing the model's continual learning\nability. Extensive experiments demonstrate that our method improves the\nperformance of replay-based CIL baselines by 2% to 11%, proving its\neffectiveness and promising potential for real-world 3D applications."
    },
    {
        "date": "2025-03",
        "title": "Text-Guided Image Invariant Feature Learning for Robust Image Watermarking",
        "author": "Muhammad Ahtesham, and Xin Zhong",
        "link": "http://arxiv.org/abs/2503.13805v1",
        "abstract": "Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking."
    },
    {
        "date": "2025-03",
        "title": "Web Artifact Attacks Disrupt Vision Language Models",
        "author": "Maan Qraitem, Piotr Teterwak, Kate Saenko, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2503.13652v1",
        "abstract": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "author": "Johan Edstedt",
        "link": "http://arxiv.org/abs/2503.13433v1",
        "abstract": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1."
    },
    {
        "date": "2025-03",
        "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
        "author": "Nhi Pham, Bernt Schiele, Adam Kortylewski, and Jonas Fischer",
        "link": "http://arxiv.org/abs/2503.13429v1",
        "abstract": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness."
    },
    {
        "date": "2025-03",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "author": "Ripan Kumar Kundu, Matthew Denton, Genova Mongalo, Prasad Calyam, and Khaza Anuarul Hoque",
        "link": "http://arxiv.org/abs/2503.13419v1",
        "abstract": "The synergy between virtual reality (VR) and artificial intelligence (AI),\nspecifically deep learning (DL)-based cybersickness detection models, has\nushered in unprecedented advancements in immersive experiences by automatically\ndetecting cybersickness severity and adaptively various mitigation techniques,\noffering a smooth and comfortable VR experience. While this DL-enabled\ncybersickness detection method provides promising solutions for enhancing user\nexperiences, it also introduces new risks since these models are vulnerable to\nadversarial attacks; a small perturbation of the input data that is visually\nundetectable to human observers can fool the cybersickness detection model and\ntrigger unexpected mitigation, thus disrupting user immersive experiences (UIX)\nand even posing safety risks. In this paper, we present a new type of VR\nattack, i.e., a cybersickness attack, which successfully stops the triggering\nof cybersickness mitigation by fooling DL-based cybersickness detection models\nand dramatically hinders the UIX. Next, we propose a novel explainable\nartificial intelligence (XAI)-guided cybersickness attack detection framework\nto detect such attacks in VR to ensure UIX and a comfortable VR experience. We\nevaluate the proposed attack and the detection framework using two\nstate-of-the-art open-source VR cybersickness datasets: Simulation 2021 and\nGameplay dataset. Finally, to verify the effectiveness of our proposed method,\nwe implement the attack and the XAI-based detection using a testbed with a\ncustom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and\nperform a user study. Our study shows that such an attack can dramatically\nhinder the UIX. However, our proposed XAI-guided cybersickness attack detection\ncan successfully detect cybersickness attacks and trigger the proper\nmitigation, effectively reducing VR cybersickness."
    },
    {
        "date": "2025-03",
        "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
        "author": "Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, and Libo Qin",
        "link": "http://arxiv.org/abs/2503.13413v3",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
    },
    {
        "date": "2025-03",
        "title": "Follow-the-Regularized-Leader with Adversarial Constraints",
        "author": "Ricardo N. Ferreira, and Cl\u00e1udia Soares",
        "link": "http://arxiv.org/abs/2503.13366v2",
        "abstract": "Constrained Online Convex Optimization (COCO) can be seen as a generalization\nof the standard Online Convex Optimization (OCO) framework. At each round, a\ncost function and constraint function are revealed after a learner chooses an\naction. The goal is to minimize both the regret and cumulative constraint\nviolation (CCV) against an adaptive adversary. We show for the first time that\nis possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV,\nimproving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\~{O}\n\\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively."
    },
    {
        "date": "2025-03",
        "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
        "author": "Marcello Iotti, Paolo Davini, Jost von Hardenberg, and Giuseppe Zappa",
        "link": "http://arxiv.org/abs/2503.13316v1",
        "abstract": "To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature."
    },
    {
        "date": "2025-03",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "author": "Tianxing Fu, Jia Hu, Geyong Min, and Zi Wang",
        "link": "http://arxiv.org/abs/2503.13255v1",
        "abstract": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models while ensuring their data remains private and\nsecure. Blockchain technology further enhances FL by providing stronger\nsecurity, a transparent audit trail, and protection against data tampering and\nmodel manipulation. Most blockchain-secured FL systems rely on conventional\nconsensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while\nProof-of-Stake (PoS) improves energy efficiency but risks centralization as it\ninherently favors participants with larger stakes. Recently, learning-based\nconsensus has emerged as an alternative by replacing cryptographic tasks with\nmodel training to save energy. However, this approach introduces potential\nprivacy vulnerabilities, as the training process may inadvertently expose\nsensitive information through gradient sharing and model updates. To address\nthese challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT)\nconsensus mechanism. This method leverages the zero-knowledge succinct\nnon-interactive argument of knowledge proof (zk-SNARK) protocol to validate\nparticipants' contributions based on their model performance, effectively\neliminating the inefficiencies of traditional consensus methods and mitigating\nthe privacy risks posed by learning-based consensus. We analyze our system's\nsecurity, demonstrating its capacity to prevent the disclosure of sensitive\ninformation about local models or training data to untrusted parties during the\nentire FL process. Extensive experiments demonstrate that our system is robust\nagainst privacy and Byzantine attacks while maintaining accuracy and utility\nwithout trade-offs, scalable across various blockchain settings, and efficient\nin both computation and communication."
    },
    {
        "date": "2025-03",
        "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
        "author": "Tong Zhou, Shijin Duan, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Shaolei Ren, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2503.13224v1",
        "abstract": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security."
    },
    {
        "date": "2025-03",
        "title": "Robust Decision-Making Via Free Energy Minimization",
        "author": "Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, and Giovanni Russo",
        "link": "http://arxiv.org/abs/2503.13223v1",
        "abstract": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments."
    },
    {
        "date": "2025-03",
        "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
        "author": "Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, and Xin Wang",
        "link": "http://arxiv.org/abs/2503.13086v1",
        "abstract": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction."
    },
    {
        "date": "2025-03",
        "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
        "author": "Robin Zbinden, Nina van Tiel, Gencer Sumbul, Chiara Vanalli, Benjamin Kellenberger, and Devis Tuia",
        "link": "http://arxiv.org/abs/2503.13057v1",
        "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications."
    },
    {
        "date": "2025-03",
        "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
        "author": "Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, and Xi Zhang",
        "link": "http://arxiv.org/abs/2503.12931v1",
        "abstract": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness."
    },
    {
        "date": "2025-03",
        "title": "MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG",
        "author": "Pingyu Wu, Daiheng Gao, Jing Tang, Huimin Chen, Wenbo Zhou, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.13563v1",
        "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https://github.com/wpydcr/MES-RAG."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "author": "Pengcheng Zhou, Yinglun Feng, and Zhongliang Yang",
        "link": "http://arxiv.org/abs/2503.15548v1",
        "abstract": "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures."
    },
    {
        "date": "2025-03",
        "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Ke Ma, Xinfeng Li, Yihao Huang, Wei Dong, Yang Liu, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2503.12874v2",
        "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT."
    },
    {
        "date": "2025-03",
        "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
        "author": "Chen Liu, Peike Li, Liying Yang, Dadong Wang, Lincheng Li, and Xin Yu",
        "link": "http://arxiv.org/abs/2503.12847v1",
        "abstract": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation."
    },
    {
        "date": "2025-03",
        "title": "CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting",
        "author": "Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, and Sangpil Kim",
        "link": "http://arxiv.org/abs/2503.12836v3",
        "abstract": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model."
    },
    {
        "date": "2025-03",
        "title": "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
        "author": "Md Farhamdur Reza, Richeng Jin, Tianfu Wu, and Huaiyu Dai",
        "link": "http://arxiv.org/abs/2503.12827v2",
        "abstract": "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBA$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBA$^K$ can be used to attack against classifiers with $top$-$K$\nmulti-label learning. Extensive experimental results on ImageNet and PASCAL VOC\ndatasets validate the effectiveness of GSBA$^K$ in crafting $top$-$K$\nadversarial examples."
    },
    {
        "date": "2025-03",
        "title": "BLIA: Detect model memorization in binary classification model through passive Label Inference attack",
        "author": "Mohammad Wahiduzzaman Khan, Sheng Chen, Ilya Mironov, Leizhen Zhang, and Rabib Noor",
        "link": "http://arxiv.org/abs/2503.12801v1",
        "abstract": "Model memorization has implications for both the generalization capacity of\nmachine learning models and the privacy of their training data. This paper\ninvestigates label memorization in binary classification models through two\nnovel passive label inference attacks (BLIA). These attacks operate passively,\nrelying solely on the outputs of pre-trained models, such as confidence scores\nand log-loss values, without interacting with or modifying the training\nprocess. By intentionally flipping 50% of the labels in controlled subsets,\ntermed \"canaries,\" we evaluate the extent of label memorization under two\nconditions: models trained without label differential privacy (Label-DP) and\nthose trained with randomized response-based Label-DP. Despite the application\nof varying degrees of Label-DP, the proposed attacks consistently achieve\nsuccess rates exceeding 50%, surpassing the baseline of random guessing and\nconclusively demonstrating that models memorize training labels, even when\nthese labels are deliberately uncorrelated with the features."
    },
    {
        "date": "2025-03",
        "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization",
        "author": "Yechao Zhang, Yingzhe Xu, Junyu Shi, Leo Yu Zhang, Shengshan Hu, Minghui Li, and Yanjun Zhang",
        "link": "http://arxiv.org/abs/2503.12793v2",
        "abstract": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%."
    },
    {
        "date": "2025-03",
        "title": "Algebraic Adversarial Attacks on Explainability Models",
        "author": "Lachlan Simpson, Federico Costanza, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, and Hong Gunn Chew",
        "link": "http://arxiv.org/abs/2503.12683v1",
        "abstract": "Classical adversarial attacks are phrased as a constrained optimisation\nproblem. Despite the efficacy of a constrained optimisation approach to\nadversarial attacks, one cannot trace how an adversarial point was generated.\nIn this work, we propose an algebraic approach to adversarial attacks and study\nthe conditions under which one can generate adversarial examples for post-hoc\nexplainability models. Phrasing neural networks in the framework of geometric\ndeep learning, algebraic adversarial attacks are constructed through analysis\nof the symmetry groups of neural networks. Algebraic adversarial examples\nprovide a mathematically tractable approach to adversarial examples. We\nvalidate our approach of algebraic adversarial examples on two well-known and\none real-world dataset."
    },
    {
        "date": "2025-03",
        "title": "SCOOP: CoSt-effective COngestiOn Attacks in Payment Channel Networks",
        "author": "Mohammed Ababneh, Kartick Kolachala, and Roopa Vishwanathan",
        "link": "http://arxiv.org/abs/2503.12625v1",
        "abstract": "Payment channel networks (PCNs) are a promising solution to address\nblockchain scalability and throughput challenges, However, the security of PCNs\nand their vulnerability to attacks are not sufficiently studied. In this paper,\nwe introduce SCOOP, a framework that includes two novel congestion attacks on\nPCNs. These attacks consider the minimum transferable amount along a path (path\ncapacity) and the number of channels involved (path length), formulated as\nlinear optimization problems. The first attack allocates the attacker's budget\nto achieve a specific congestion threshold, while the second maximizes\ncongestion under budget constraints. Simulation results show the effectiveness\nof the proposed attack formulations in comparison to other attack strategies.\nSpecifically, the results indicate that the first attack provides around a 40\\%\nimprovement in congestion performance, while the second attack offers\napproximately a 50\\% improvement in comparison to the state-of-the-art.\nMoreover, in terms of payment to congestion efficiency, the first attack is\nabout 60\\% more efficient, and the second attack is around 90\\% more efficient\nin comparison to state-of-the-art"
    },
    {
        "date": "2025-03",
        "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack",
        "author": "Abyad Enan, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2503.12567v1",
        "abstract": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel."
    },
    {
        "date": "2025-03",
        "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry",
        "author": "Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, and Dewen Hu",
        "link": "http://arxiv.org/abs/2503.12527v1",
        "abstract": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}."
    },
    {
        "date": "2025-03",
        "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
        "author": "Jian-Ping Mei, Weibin Zhang, Jie Chen, Xuyun Zhang, and Tiantian Zhu",
        "link": "http://arxiv.org/abs/2503.12497v1",
        "abstract": "Malicious users attempt to replicate commercial models functionally at low\ncost by training a clone model with query responses. It is challenging to\ntimely prevent such model-stealing attacks to achieve strong protection and\nmaintain utility. In this paper, we propose a novel non-parametric detector\ncalled Account-aware Distribution Discrepancy (ADD) to recognize queries from\nmalicious users by leveraging account-wise local dependency. We formulate each\nclass as a Multivariate Normal distribution (MVN) in the feature space and\nmeasure the malicious score as the sum of weighted class-wise distribution\ndiscrepancy. The ADD detector is combined with random-based prediction\npoisoning to yield a plug-and-play defense module named D-ADD for image\nclassification models. Results of extensive experimental studies show that\nD-ADD achieves strong defense against different types of attacks with little\ninterference in serving benign users for both soft and hard-label settings."
    },
    {
        "date": "2025-03",
        "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation",
        "author": "Edgar Heinert, Thomas Gottwald, Annika M\u00fctze, and Matthias Rottmann",
        "link": "http://arxiv.org/abs/2503.12453v1",
        "abstract": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs."
    },
    {
        "date": "2025-03",
        "title": "Semi-Decision-Focused Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization",
        "author": "Juhyeong Kim",
        "link": "http://arxiv.org/abs/2503.13544v2",
        "abstract": "I propose Semi-Decision-Focused Learning, a practical adaptation of\nDecision-Focused Learning for portfolio optimization. Rather than directly\noptimizing complex financial metrics, I employ simple target portfolios\n(Max-Sortino or One-Hot) and train models with a convex, cross-entropy loss. I\nfurther incorporate Deep Ensemble methods to reduce variance and stabilize\nperformance. Experiments on two universes (one upward-trending and another\nrange-bound) show consistent outperformance over baseline portfolios,\ndemonstrating the effectiveness and robustness of my approach. Code is\navailable at https://github.com/sDFLwDE/sDFLwDE"
    },
    {
        "date": "2025-03",
        "title": "SCReedSolo: A Secure and Robust LSB Image Steganography Framework with Randomized Symmetric Encryption and Reed-Solomon Coding",
        "author": "Syed Rifat Raiyan, and Md. Hasanul Kabir",
        "link": "http://arxiv.org/abs/2503.12368v1",
        "abstract": "Image steganography is an information-hiding technique that involves the\nsurreptitious concealment of covert informational content within digital\nimages. In this paper, we introduce ${\\rm SCR{\\small EED}S{\\small OLO}}$, a\nnovel framework for concealing arbitrary binary data within images. Our\napproach synergistically leverages Random Shuffling, Fernet Symmetric\nEncryption, and Reed-Solomon Error Correction Codes to encode the secret\npayload, which is then discretely embedded into the carrier image using LSB\n(Least Significant Bit) Steganography. The combination of these methods\naddresses the vulnerability vectors of both security and resilience against\nbit-level corruption in the resultant stego-images. We show that our framework\nachieves a data payload of 3 bits per pixel for an RGB image, and\nmathematically assess the probability of successful transmission for the\namalgamated $n$ message bits and $k$ error correction bits. Additionally, we\nfind that ${\\rm SCR{\\small EED}S{\\small OLO}}$ yields good results upon being\nevaluated with multiple performance metrics, successfully eludes detection by\nvarious passive steganalysis tools, and is immune to simple active steganalysis\nattacks. Our code and data are available at\nhttps://github.com/Starscream-11813/SCReedSolo-Steganography."
    },
    {
        "date": "2025-03",
        "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
        "author": "Aditi Godbole",
        "link": "http://arxiv.org/abs/2503.12353v1",
        "abstract": "In today's business landscape, organizations need to find the right balance\nbetween using their customers' data ethically to power AI solutions and being\ncompliant regarding data privacy and data usage regulations. In this paper, we\ndiscuss synthetic data as a possible solution to this dilemma. Synthetic data\nis simulated data that mimics the real data. We explore how organizations in\nheavily regulated industries, such as financial institutions or healthcare\norganizations, can leverage synthetic data to build robust AI solutions while\nstaying compliant. We demonstrate that synthetic data offers two significant\nadvantages by allowing AI models to learn from more diverse data and by helping\norganizations stay compliant against data privacy laws with the use of\nsynthetic data instead of customer information. We discuss case studies to show\nhow synthetic data can be effectively used in the finance and healthcare sector\nwhile discussing the challenges of using synthetic data and some ethical\nquestions it raises. Our research finds that synthetic data could be a\ngame-changer for AI in regulated industries. The potential can be realized when\nindustry, academia, and regulators collaborate to build solutions. We aim to\ninitiate discussions on the use of synthetic data to build ethical,\nresponsible, and effective AI systems in regulated enterprise industries."
    },
    {
        "date": "2025-03",
        "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
        "author": "Wenqing Kuang, Xiongwei Zhao, Yehui Shen, Congcong Wen, Huimin Lu, Zongtan Zhou, and Xieyuanli Chen",
        "link": "http://arxiv.org/abs/2503.12350v1",
        "abstract": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR."
    },
    {
        "date": "2025-03",
        "title": "Augmented Adversarial Trigger Learning",
        "author": "Zhe Wang, and Yanjun Qi",
        "link": "http://arxiv.org/abs/2503.12339v1",
        "abstract": "Gradient optimization-based adversarial attack methods automate the learning\nof adversarial triggers to generate jailbreak prompts or leak system prompts.\nIn this work, we take a closer look at the optimization objective of\nadversarial trigger learning and propose ATLA: Adversarial Trigger Learning\nwith Augmented objectives. ATLA improves the negative log-likelihood loss used\nby previous studies into a weighted loss formulation that encourages the\nlearned adversarial triggers to optimize more towards response format tokens.\nThis enables ATLA to learn an adversarial trigger from just one query-response\npair and the learned trigger generalizes well to other similar queries. We\nfurther design a variation to augment trigger optimization with an auxiliary\nloss that suppresses evasive responses. We showcase how to use ATLA to learn\nadversarial suffixes jailbreaking LLMs and to extract hidden system prompts.\nEmpirically we demonstrate that ATLA consistently outperforms current\nstate-of-the-art techniques, achieving nearly 100% success in attacking while\nrequiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high\ngeneralization to unseen queries and transfer well to new LLMs."
    },
    {
        "date": "2025-03",
        "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise",
        "author": "Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, and Sanjay Lall",
        "link": "http://arxiv.org/abs/2503.12301v1",
        "abstract": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness."
    },
    {
        "date": "2025-03",
        "title": "FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning",
        "author": "Binghui Zhang, Luis Mares De La Cruz, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.13537v1",
        "abstract": "Federated Learning (FL) is an emerging decentralized learning paradigm that\ncan partly address the privacy concern that cannot be handled by traditional\ncentralized and distributed learning. Further, to make FL practical, it is also\nnecessary to consider constraints such as fairness and robustness. However,\nexisting robust FL methods often produce unfair models, and existing fair FL\nmethods only consider one-level (client) fairness and are not robust to\npersistent outliers (i.e., injected outliers into each training round) that are\ncommon in real-world FL settings. We propose \\texttt{FedTilt}, a novel FL that\ncan preserve multi-level fairness and be robust to outliers. In particular, we\nconsider two common levels of fairness, i.e., \\emph{client fairness} --\nuniformity of performance across clients, and \\emph{client data fairness} --\nuniformity of performance across different classes of data within a client.\n\\texttt{FedTilt} is inspired by the recently proposed tilted empirical risk\nminimization, which introduces tilt hyperparameters that can be flexibly tuned.\nTheoretically, we show how tuning tilt values can achieve the two-level\nfairness and mitigate the persistent outliers, and derive the convergence\ncondition of \\texttt{FedTilt} as well. Empirically, our evaluation results on a\nsuite of realistic federated datasets in diverse settings show the\neffectiveness and flexibility of the \\texttt{FedTilt} framework and the\nsuperiority to the state-of-the-arts."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels",
        "author": "Chengxuan Qian, Kai Han, Siqi Ma, Chongwen Lyu, Zhenlong Yuan, Jun Chen, and Zhe Liu",
        "link": "http://arxiv.org/abs/2503.12218v1",
        "abstract": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
        "author": "Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, and Jianfei Cai",
        "link": "http://arxiv.org/abs/2503.12150v2",
        "abstract": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
    },
    {
        "date": "2025-03",
        "title": "Robust Isolation Forest using Soft Sparse Random Projection and Valley Emphasis Method",
        "author": "Hun Kang, and Kyoungok Kim",
        "link": "http://arxiv.org/abs/2503.12125v1",
        "abstract": "Isolation Forest (iForest) is an unsupervised anomaly detection algorithm\ndesigned to effectively detect anomalies under the assumption that anomalies\nare ``few and different.\" Various studies have aimed to enhance iForest, but\nthe resulting algorithms often exhibited significant performance disparities\nacross datasets. Additionally, the challenge of isolating rare and widely\ndistributed anomalies persisted in research focused on improving splits. To\naddress these challenges, we introduce Robust iForest (RiForest). RiForest\nleverages both existing features and random hyperplanes obtained through soft\nsparse random projection to identify superior split features for anomaly\ndetection, independent of datasets. It utilizes the underutilized valley\nemphasis method for optimal split point determination and incorporates sparsity\nrandomization in soft sparse random projection for enhanced anomaly detection\nrobustness. Across 24 benchmark datasets, experiments demonstrate RiForest's\nconsistent outperformance of existing algorithms in anomaly detection,\nemphasizing stability and robustness to noise variables."
    },
    {
        "date": "2025-03",
        "title": "Robust Dataset Distillation by Matching Adversarial Trajectories",
        "author": "Wei Lai, Tianyu Ding, ren dongdong, Lei Wang, Jing Huo, Yang Gao, and Wenbin Li",
        "link": "http://arxiv.org/abs/2503.12069v1",
        "abstract": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
        "author": "Chenhao Lin, Chenyang Zhao, Shiwei Wang, Longtian Wang, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2503.12058v1",
        "abstract": "Backdoor attacks typically place a specific trigger on certain training data,\nsuch that the model makes prediction errors on inputs with that trigger during\ninference. Despite the core role of the trigger, existing studies have commonly\nbelieved a perfect match between training-inference triggers is optimal. In\nthis paper, for the first time, we systematically explore the\ntraining-inference trigger relation, particularly focusing on their mismatch,\nbased on a Training-Inference Trigger Intensity Manipulation (TITIM) workflow.\nTITIM specifically investigates the training-inference trigger intensity, such\nas the size or the opacity of a trigger, and reveals new insights into trigger\ngeneralization and overfitting.\n  These new insights challenge the above common belief by demonstrating that\nthe training-inference trigger mismatch can facilitate attacks in two practical\nscenarios, posing more significant security threats than previously thought.\nFirst, when the inference trigger is fixed, using training triggers with mixed\nintensities leads to stronger attacks than using any single intensity. For\nexample, on CIFAR-10 with ResNet-18, mixing training triggers with 1.0 and 0.1\nopacities improves the worst-case attack success rate (ASR) (over different\ntesting opacities) of the best single-opacity attack from 10.61\\% to 92.77\\%.\nSecond, intentionally using certain mismatched training-inference triggers can\nimprove the attack stealthiness, i.e., better bypassing defenses. For example,\ncompared to the training/inference intensity of 1.0/1.0, using 1.0/0.7\ndecreases the area under the curve (AUC) of the Scale-Up defense from 0.96 to\n0.62, while maintaining a high attack ASR (99.65\\% vs. 91.62\\%). The above new\ninsights are validated to be generalizable across different backdoor attacks,\nmodels, datasets, tasks, and (digital/physical) domains."
    },
    {
        "date": "2025-03",
        "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training",
        "author": "Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.12030v1",
        "abstract": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt."
    },
    {
        "date": "2025-03",
        "title": "Mixed-feature Logistic Regression Robust to Distribution Shifts",
        "author": "Qingshi Sun, Nathan Justin, Andres Gomez, and Phebe Vayanos",
        "link": "http://arxiv.org/abs/2503.12012v1",
        "abstract": "Logistic regression models are widely used in the social and behavioral\nsciences and in high-stakes domains, due to their simplicity and\ninterpretability properties. At the same time, such domains are permeated by\ndistribution shifts, where the distribution generating the data changes between\ntraining and deployment. In this paper, we study a distributionally robust\nlogistic regression problem that seeks the model that will perform best against\nadversarial realizations of the data distribution drawn from a suitably\nconstructed Wasserstein ambiguity set. Our model and solution approach differ\nfrom prior work in that we can capture settings where the likelihood of\ndistribution shifts can vary across features, significantly broadening the\napplicability of our model relative to the state-of-the-art. We propose a\ngraph-based solution approach that can be integrated into off-the-shelf\noptimization solvers. We evaluate the performance of our model and algorithms\non numerous publicly available datasets. Our solution achieves a 408x speed-up\nrelative to the state-of-the-art. Additionally, compared to the\nstate-of-the-art, our model reduces average calibration error by up to 36.19%\nand worst-case calibration error by up to 41.70%, while increasing the average\narea under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to\n48.37%."
    },
    {
        "date": "2025-03",
        "title": "Winning the MIDST Challenge: New Membership Inference Attacks on Diffusion Models for Tabular Data Synthesis",
        "author": "Xiaoyu Wu, Yifei Pang, Terrance Liu, and Steven Wu",
        "link": "http://arxiv.org/abs/2503.12008v1",
        "abstract": "Tabular data synthesis using diffusion models has gained significant\nattention for its potential to balance data utility and privacy. However,\nexisting privacy evaluations often rely on heuristic metrics or weak membership\ninference attacks (MIA), leaving privacy risks inadequately assessed. In this\nwork, we conduct a rigorous MIA study on diffusion-based tabular synthesis,\nrevealing that state-of-the-art attacks designed for image models fail in this\nsetting. We identify noise initialization as a key factor influencing attack\nefficacy and propose a machine-learning-driven approach that leverages loss\nfeatures across different noises and time steps. Our method, implemented with a\nlightweight MLP, effectively learns membership signals, eliminating the need\nfor manual optimization. Experimental results from the MIDST Challenge @ SaTML\n2025 demonstrate the effectiveness of our approach, securing first place across\nall tracks. Code is available at\nhttps://github.com/Nicholas0228/Tartan_Federer_MIDST."
    },
    {
        "date": "2025-03",
        "title": "Internet of Things-Based Smart Precision Farming in Soilless Agriculture:Opportunities and Challenges for Global Food Security",
        "author": "Monica Dutta, Deepali Gupta, Sumegh Tharewal, Deepam Goyal, Jasminder Kaur Sandhu, Manjit Kaur, Ahmad Ali Alzubi, and Jazem Mutared Alanazi",
        "link": "http://arxiv.org/abs/2503.13528v2",
        "abstract": "The rapid growth of the global population and the continuous decline in\ncultivable land pose significant threats to food security. This challenge\nworsens as climate change further reduces the availability of farmland.\nSoilless agriculture, such as hydroponics, aeroponics, and aquaponics, offers a\nsustainable solution by enabling efficient crop cultivation in controlled\nenvironments. The integration of the Internet of Things (IoT) with smart\nprecision farming improves resource efficiency, automates environmental\ncontrol, and ensures stable and high-yield crop production. IoT-enabled smart\nfarming systems utilize real-time monitoring, data-driven decision-making, and\nautomation to optimize water and nutrient usage while minimizing human\nintervention. This paper explores the opportunities and challenges of IoT-based\nsoilless farming, highlighting its role in sustainable agriculture, urban\nfarming, and global food security. These advanced farming methods ensure\ngreater productivity, resource conservation, and year-round cultivation.\nHowever, they also face challenges such as high initial investment,\ntechnological dependency, and energy consumption. Through a comprehensive\nstudy, bibliometric analysis, and comparative analysis, this research\nhighlights current trends and research gaps. It also outlines future directions\nfor researchers, policymakers, and industry stakeholders to drive innovation\nand scalability in IoT-driven soilless agriculture. By emphasizing the benefits\nof vertical farming and Controlled Environment Agriculture (CEA)-enabled\nsoilless techniques, this paper supports informed decision-making to address\nfood security challenges and promote sustainable agricultural innovations."
    },
    {
        "date": "2025-03",
        "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
        "author": "Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, and Yanxia Zhang",
        "link": "http://arxiv.org/abs/2503.11937v1",
        "abstract": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model."
    },
    {
        "date": "2025-03",
        "title": "Trust Under Siege: Label Spoofing Attacks against Machine Learning for Android Malware Detection",
        "author": "Tianwei Lan, Luca Demetrio, Farid Nait-Abdesselam, Yufei Han, and Simone Aonzo",
        "link": "http://arxiv.org/abs/2503.11841v1",
        "abstract": "Machine learning (ML) malware detectors rely heavily on crowd-sourced\nAntiVirus (AV) labels, with platforms like VirusTotal serving as a trusted\nsource of malware annotations. But what if attackers could manipulate these\nlabels to classify benign software as malicious? We introduce label spoofing\nattacks, a new threat that contaminates crowd-sourced datasets by embedding\nminimal and undetectable malicious patterns into benign samples. These patterns\ncoerce AV engines into misclassifying legitimate files as harmful, enabling\npoisoning attacks against ML-based malware classifiers trained on those data.\nWe demonstrate this scenario by developing AndroVenom, a methodology for\npolluting realistic data sources, causing consequent poisoning attacks against\nML malware detectors. Experiments show that not only state-of-the-art feature\nextractors are unable to filter such injection, but also various ML models\nexperience Denial of Service already with 1% poisoned samples. Additionally,\nattackers can flip decisions of specific unaltered benign samples by modifying\nonly 0.015% of the training data, threatening their reputation and market share\nand being unable to be stopped by anomaly detectors on training data. We\nconclude our manuscript by raising the alarm on the trustworthiness of the\ntraining process based on AV annotations, requiring further investigation on\nhow to produce proper labels for ML malware detectors."
    },
    {
        "date": "2025-03",
        "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
        "author": "Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, and Leonid Karlinsky",
        "link": "http://arxiv.org/abs/2503.11790v1",
        "abstract": "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Resiliency of Sketch-based Security via LSB Sharing-based Dynamic Late Merging",
        "author": "Seungsam Yang, Seyed Mohammad Mehdi Mirnajafizadeh, Sian Kim, Rhongho Jang, and DaeHun Nyang",
        "link": "http://arxiv.org/abs/2503.11777v1",
        "abstract": "With the exponentially growing Internet traffic, sketch data structure with a\nprobabilistic algorithm has been expected to be an alternative solution for\nnon-compromised (non-selective) security monitoring. While facilitating\ncounting within a confined memory space, the sketch's memory efficiency and\naccuracy were further pushed to their limit through finer-grained and dynamic\ncontrol of constrained memory space to adapt to the data stream's inherent\nskewness (i.e., Zipf distribution), namely small counters with extensions. In\nthis paper, we unveil a vulnerable factor of the small counter design by\nintroducing a new sketch-oriented attack, which threatens a stream of\nstate-of-the-art sketches and their security applications. With the root cause\nanalyses, we propose Siamese Counter with enhanced adversarial resiliency and\nverified feasibility with extensive experimental and theoretical analyses.\nUnder a sketch pollution attack, Siamese Counter delivers 47% accurate results\nthan a state-of-the-art scheme, and demonstrates up to 82% more accurate\nestimation under normal measurement scenarios."
    },
    {
        "date": "2025-03",
        "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
        "author": "Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.11650v1",
        "abstract": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels."
    },
    {
        "date": "2025-03",
        "title": "Are Deep Speech Denoising Models Robust to Adversarial Noise?",
        "author": "Will Schwarzer, Philip S. Thomas, Andrea Fanelli, and Xiaoyu Liu",
        "link": "http://arxiv.org/abs/2503.11627v1",
        "abstract": "Deep noise suppression (DNS) models enjoy widespread use throughout a variety\nof high-stakes speech applications. However, in this paper, we show that four\nrecent DNS models can each be reduced to outputting unintelligible gibberish\nthrough the addition of imperceptible adversarial noise. Furthermore, our\nresults show the near-term plausibility of targeted attacks, which could induce\nmodels to output arbitrary utterances, and over-the-air attacks. While the\nsuccess of these attacks varies by model and setting, and attacks appear to be\nstrongest when model-specific (i.e., white-box and non-transferable), our\nresults highlight a pressing need for practical countermeasures in DNS systems."
    },
    {
        "date": "2025-03",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "author": "Shuyang Hao, Yiwei Wang, Bryan Hooi, Ming-Hsuan Yang, Jun Liu, Chengcheng Tang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.11619v1",
        "abstract": "Deploying large vision-language models (LVLMs) introduces a unique\nvulnerability: susceptibility to malicious attacks via visual inputs. However,\nexisting defense methods suffer from two key limitations: (1) They solely focus\non textual defenses, fail to directly address threats in the visual domain\nwhere attacks originate, and (2) the additional processing steps often incur\nsignificant computational overhead or compromise model performance on benign\ntasks. Building on these insights, we propose ESIII (Embedding Security\nInstructions Into Images), a novel methodology for transforming the visual\nspace from a source of vulnerability into an active defense mechanism.\nInitially, we embed security instructions into defensive images through\ngradient-based optimization, obtaining security instructions in the visual\ndimension. Subsequently, we integrate security instructions from visual and\ntextual dimensions with the input query. The collaboration between security\ninstructions from different dimensions ensures comprehensive security\nprotection. Extensive experiments demonstrate that our approach effectively\nfortifies the robustness of LVLMs against such attacks while preserving their\nperformance on standard benign tasks and incurring an imperceptible increase in\ntime costs."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "author": "Thuy M. Pham, Linda Senigagliesi, Marco Baldi, Rafael F. Schaefer, Gerhard P. Fettweis, and Arsenia Chorti",
        "link": "http://arxiv.org/abs/2503.11508v1",
        "abstract": "In this paper, we investigate the utilization of the angle of arrival (AoA)\nas a feature for robust physical layer authentication (PLA). While most of the\nexisting approaches to PLA focus on common features of the physical layer of\ncommunication channels, such as channel frequency response, channel impulse\nresponse or received signal strength, the use of AoA in this domain has not yet\nbeen studied in depth, particularly regarding the ability to thwart\nimpersonation attacks. In this work, we demonstrate that an impersonation\nattack targeting AoA based PLA is only feasible under strict conditions on the\nattacker's location and hardware capabilities, which highlights the AoA's\npotential as a strong feature for PLA. We extend previous works considering a\nsingle-antenna attacker to the case of a multiple-antenna attacker, and we\ndevelop a theoretical characterization of the conditions in which a successful\nimpersonation attack can be mounted. Furthermore, we leverage extensive\nsimulations in support of theoretical analyses, to validate the robustness of\nAoA-based PLA."
    },
    {
        "date": "2025-03",
        "title": "Dynamic Obstacle Avoidance with Bounded Rationality Adversarial Reinforcement Learning",
        "author": "Jose-Luis Holgado-Alvarez, Aryaman Reddi, and Carlo D'Eramo",
        "link": "http://arxiv.org/abs/2503.11467v1",
        "abstract": "Reinforcement Learning (RL) has proven largely effective in obtaining stable\nlocomotion gaits for legged robots. However, designing control algorithms which\ncan robustly navigate unseen environments with obstacles remains an ongoing\nproblem within quadruped locomotion. To tackle this, it is convenient to solve\nnavigation tasks by means of a hierarchical approach with a low-level\nlocomotion policy and a high-level navigation policy. Crucially, the high-level\npolicy needs to be robust to dynamic obstacles along the path of the agent. In\nthis work, we propose a novel way to endow navigation policies with robustness\nby a training process that models obstacles as adversarial agents, following\nthe adversarial RL paradigm. Importantly, to improve the reliability of the\ntraining process, we bound the rationality of the adversarial agent resorting\nto quantal response equilibria, and place a curriculum over its rationality. We\ncalled this method Hierarchical policies via Quantal response Adversarial\nReinforcement Learning (Hi-QARL). We demonstrate the robustness of our method\nby benchmarking it in unseen randomized mazes with multiple obstacles. To prove\nits applicability in real scenarios, our method is applied on a Unitree GO1\nrobot in simulation."
    },
    {
        "date": "2025-03",
        "title": "In Shift and In Variance: Assessing the Robustness of HAR Deep Learning Models against Variability",
        "author": "Azhar Ali Khaked, Nobuyuki Oishi, Daniel Roggen, and Paula Lago",
        "link": "http://arxiv.org/abs/2503.11466v1",
        "abstract": "Human Activity Recognition (HAR) using wearable inertial measurement unit\n(IMU) sensors can revolutionize healthcare by enabling continual health\nmonitoring, disease prediction, and routine recognition. Despite the high\naccuracy of Deep Learning (DL) HAR models, their robustness to real-world\nvariabilities remains untested, as they have primarily been trained and tested\non limited lab-confined data. In this study, we isolate subject, device,\nposition, and orientation variability to determine their effect on DL HAR\nmodels and assess the robustness of these models in real-world conditions. We\nevaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a\ncomprehensive discussion on the impact of variability on data distribution\nshifts and changes in model performance. Our experiments measured shifts in\ndata distribution using Maximum Mean Discrepancy (MMD) and observed DL model\nperformance drops due to variability. We concur that studied variabilities\naffect DL HAR models differently, and there is an inverse relationship between\ndata distribution shifts and model performance. The compounding effect of\nvariability was analyzed, and the implications of variabilities in real-world\nscenarios were highlighted. MMD proved an effective metric for calculating data\ndistribution shifts and explained the drop in performance due to variabilities\nin HARVAR and REALDISP datasets. Combining our understanding of variability\nwith evaluating its effects will facilitate the development of more robust DL\nHAR models and optimal training techniques. Allowing Future models to not only\nbe assessed based on their maximum F1 score but also on their ability to\ngeneralize effectively"
    }
]