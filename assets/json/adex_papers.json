[
    {
        "date": "2025-09",
        "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees",
        "author": "Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, and Michael Zuzak",
        "link": "http://arxiv.org/abs/2509.07939v1",
        "abstract": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"
    },
    {
        "date": "2025-09",
        "title": "Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering",
        "author": "Shivam Sharma, Supreeth Mysore Venkatesh, and Pushkin Kachroo",
        "link": "http://arxiv.org/abs/2509.07766v1",
        "abstract": "Clustering financial assets based on return correlations is a fundamental\ntask in portfolio optimization and statistical arbitrage. However, classical\nclustering methods often fall short when dealing with signed correlation\nstructures, typically requiring lossy transformations and heuristic assumptions\nsuch as a fixed number of clusters. In this work, we apply the Graph-based\nCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,\nweighted graphs without relying on such transformations. GCS-Q formulates each\npartitioning step as a QUBO problem, enabling it to leverage quantum annealing\nfor efficient exploration of exponentially large solution spaces. We validate\nour approach on both synthetic and real-world financial data, benchmarking\nagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Our\nexperiments demonstrate that GCS-Q consistently achieves higher clustering\nquality, as measured by Adjusted Rand Index and structural balance penalties,\nwhile dynamically determining the number of clusters. These results highlight\nthe practical utility of near-term quantum computing for graph-based\nunsupervised learning in financial applications."
    },
    {
        "date": "2025-09",
        "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
        "author": "Haitao Hu, Peng Chen, Yanpeng Zhao, and Yuqi Chen",
        "link": "http://arxiv.org/abs/2509.07764v1",
        "abstract": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."
    },
    {
        "date": "2025-09",
        "title": "Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection",
        "author": "Nils Bars, Lukas Bernhard, Moritz Schloegel, and Thorsten Holz",
        "link": "http://arxiv.org/abs/2509.07757v2",
        "abstract": "We use browsers daily to access all sorts of information. Because browsers\nroutinely process scripts, media, and executable code from unknown sources,\nthey form a critical security boundary between users and adversaries. A common\nattack vector is JavaScript, which exposes a large attack surface due to the\nsheer complexity of modern JavaScript engines. To mitigate these threats,\nmodern engines increasingly adopt software-based fault isolation (SFI). A\nprominent example is Google's V8 heap sandbox, which represents the most widely\ndeployed SFI mechanism, protecting billions of users across all Chromium-based\nbrowsers and countless applications built on Node$.$js and Electron. The heap\nsandbox splits the address space into two parts: one part containing trusted,\nsecurity-sensitive metadata, and a sandboxed heap containing memory accessible\nto untrusted code. On a technical level, the sandbox enforces isolation by\nremoving raw pointers and using translation tables to resolve references to\ntrusted objects. Consequently, an attacker cannot corrupt trusted data even\nwith full control of the sandboxed data, unless there is a bug in how code\nhandles data from the sandboxed heap. Despite their widespread use, such SFI\nmechanisms have seen little security testing.\n  In this work, we propose a new testing technique that models the security\nboundary of modern SFI implementations. Following the SFI threat model, we\nassume a powerful attacker who fully controls the sandbox's memory. We\nimplement this by instrumenting memory loads originating in the trusted domain\nand accessing untrusted, attacker-controlled sandbox memory. We then inject\nfaults into the loaded data, aiming to trigger memory corruption in the trusted\ndomain. In a comprehensive evaluation, we identify 19 security bugs in V8 that\nenable an attacker to bypass the sandbox."
    },
    {
        "date": "2025-09",
        "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems",
        "author": "Kamel Kamel, Hridoy Sankar Dutta, Keshav Sood, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2509.07677v1",
        "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for\nverification. They are increasingly integrated into high-security sectors such\nas banking and healthcare. Despite their improvements using deep learning, they\nface severe vulnerabilities from sophisticated threats like deepfakes and\nadversarial attacks. The emergence of realistic voice cloning complicates\ndetection, as systems struggle to distinguish authentic from synthetic audio.\nWhile anti-spoofing countermeasures (CMs) exist to mitigate these risks, many\nrely on static detection models that can be bypassed by novel adversarial\nmethods, leaving a critical security gap. To demonstrate this vulnerability, we\npropose the Spectral Masking and Interpolation Attack (SMIA), a novel method\nthat strategically manipulates inaudible frequency regions of AI-generated\naudio. By altering the voice in imperceptible zones to the human ear, SMIA\ncreates adversarial samples that sound authentic while deceiving CMs. We\nconducted a comprehensive evaluation of our attack against state-of-the-art\n(SOTA) models across multiple tasks, under simulated real-world conditions.\nSMIA achieved a strong attack success rate (ASR) of at least 82% against\ncombined VAS/CM systems, at least 97.5% against standalone speaker verification\nsystems, and 100% against countermeasures. These findings conclusively\ndemonstrate that current security postures are insufficient against adaptive\nadversarial attacks. This work highlights the urgent need for a paradigm shift\ntoward next-generation defenses that employ dynamic, context-aware frameworks\ncapable of evolving with the threat landscape."
    },
    {
        "date": "2025-09",
        "title": "Nearest Neighbor Projection Removal Adversarial Training",
        "author": "Himanshu Singh, A. V. Subramanyam, Shivank Rajput, and Mohan Kankanhalli",
        "link": "http://arxiv.org/abs/2509.07673v2",
        "abstract": "Deep neural networks have exhibited impressive performance in image\nclassification tasks but remain vulnerable to adversarial examples. Standard\nadversarial training enhances robustness but typically fails to explicitly\naddress inter-class feature overlap, a significant contributor to adversarial\nsusceptibility. In this work, we introduce a novel adversarial training\nframework that actively mitigates inter-class proximity by projecting out\ninter-class dependencies from adversarial and clean samples in the feature\nspace. Specifically, our approach first identifies the nearest inter-class\nneighbors for each adversarial sample and subsequently removes projections onto\nthese neighbors to enforce stronger feature separability. Theoretically, we\ndemonstrate that our proposed logits correction reduces the Lipschitz constant\nof neural networks, thereby lowering the Rademacher complexity, which directly\ncontributes to improved generalization and robustness. Extensive experiments\nacross standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that\nour method demonstrates strong performance that is competitive with leading\nadversarial training techniques, highlighting significant achievements in both\nrobust and clean accuracy. Our findings reveal the importance of addressing\ninter-class feature proximity explicitly to bolster adversarial robustness in\nDNNs."
    },
    {
        "date": "2025-09",
        "title": "Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity",
        "author": "Sung Ju Lee, and Nam Ik Cho",
        "link": "http://arxiv.org/abs/2509.07647v1",
        "abstract": "Semantic watermarking techniques for latent diffusion models (LDMs) are\nrobust against regeneration attacks, but often suffer from detection\nperformance degradation due to the loss of frequency integrity. To tackle this\nproblem, we propose a novel embedding method called Hermitian Symmetric Fourier\nWatermarking (SFW), which maintains frequency integrity by enforcing Hermitian\nsymmetry. Additionally, we introduce a center-aware embedding strategy that\nreduces the vulnerability of semantic watermarking due to cropping attacks by\nensuring robust information retention. To validate our approach, we apply these\ntechniques to existing semantic watermarking schemes, enhancing their\nfrequency-domain structures for better robustness and retrieval accuracy.\nExtensive experiments demonstrate that our methods achieve state-of-the-art\nverification and identification performance, surpassing previous approaches\nacross various attack scenarios. Ablation studies confirm the impact of SFW on\ndetection capabilities, the effectiveness of the center-aware embedding against\ncropping, and how message capacity influences identification accuracy. Notably,\nour method achieves the highest detection accuracy while maintaining superior\nimage fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed\nSFW is shown to be an effective framework for balancing robustness and image\nfidelity, addressing the inherent trade-offs in semantic watermarking. Code\navailable at https://github.com/thomas11809/SFWMark"
    },
    {
        "date": "2025-09",
        "title": "RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection",
        "author": "Jad Yehya, Mansour Benbakoura, C\u00e9dric Allain, Beno\u00eet Malezieux, Matthieu Kowalski, and Thomas Moreau",
        "link": "http://arxiv.org/abs/2509.07523v2",
        "abstract": "Identifying recurring patterns and rare events in large-scale signals is a\nfundamental challenge in fields such as astronomy, physical simulations, and\nbiomedical science. Convolutional Dictionary Learning (CDL) offers a powerful\nframework for modeling local structures in signals, but its use for detecting\nrare or anomalous events remains largely unexplored. In particular, CDL faces\ntwo key challenges in this setting: high computational cost and sensitivity to\nartifacts and outliers. In this paper, we introduce RoseCDL, a scalable and\nrobust CDL algorithm designed for unsupervised rare event detection in long\nsignals. RoseCDL combines stochastic windowing for efficient training on large\ndatasets with inline outlier detection to enhance robustness and isolate\nanomalous patterns. This reframes CDL as a practical tool for event discovery\nand characterization in real-world signals, extending its role beyond\ntraditional tasks like compression or denoising."
    },
    {
        "date": "2025-09",
        "title": "Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios",
        "author": "Simon Cremer, Lydia Jehmlich, and Rainer Lenz",
        "link": "http://arxiv.org/abs/2509.07505v1",
        "abstract": "Spatial data are gaining increasing importance in many areas of research.\nParticularly spatial health data are becoming increasingly important for\nmedical research, for example, to better understand relationships between\nenvironmental factors and disease patterns. However, their use is often\nrestricted by legal data protection regulations, since georeferenced personal\ninformation carries a high risk of re-identification of individuals. To address\nthis issue, what are called geomasking methods are applied to guarantee data\nprotection through targeted displacement of individual data points, while\nsimultaneously maintaining analytical validity within a tolerable range. In the\ncurrent literature the degree of anonymity of such anonymized georeferenced\ndatasets is often measured by the so-called metric of spatial k-anonymity.\nHowever, this metric has considerable shortcomings, particularly regarding its\nresilience against realistic data attack scenarios. This article classifies the\npotential data attack scenarios in the context of anonymized georeferenced\nmicrodata and introduces appropriate metrics that enable a comprehensive\nassessment of anonymity adapted to potential data attack scenarios."
    },
    {
        "date": "2025-09",
        "title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey",
        "author": "Bilal Hussain Abbasi, Yanjun Zhang, Leo Zhang, and Shang Gao",
        "link": "http://arxiv.org/abs/2509.07504v1",
        "abstract": "Backdoor (trojan) attacks embed hidden, controllable behaviors into\nmachine-learning models so that models behave normally on benign inputs but\nproduce attacker-chosen outputs when a trigger is present. This survey reviews\nthe rapidly growing literature on backdoor attacks and defenses in the\ncomputer-vision domain. We introduce a multi-dimensional taxonomy that\norganizes attacks and defenses by injection stage (dataset poisoning,\nmodel/parameter modification, inference-time injection), trigger type (patch,\nblended/frequency, semantic, transformation), labeling strategy (dirty-label\nvs. clean-label / feature-collision), representation stage (instance-specific,\nmanifold/class-level, neuron/parameter hijacking, distributed encodings), and\ntarget task (classification, detection, segmentation, video, multimodal). For\neach axis we summarize representative methods, highlight evaluation practices,\nand discuss where defenses succeed or fail. For example, many classical\nsanitization and reverse-engineering tools are effective against reusable patch\nattacks but struggle with input-aware, sample-specific, or parameter-space\nbackdoors and with transfer via compromised pre-trained encoders or hardware\nbit-flips. We synthesize trends, identify persistent gaps (supply-chain and\nhardware threats, certifiable defenses, cross-task benchmarks), and propose\npractical guidelines for threat-aware evaluation and layered defenses. This\nsurvey aims to orient researchers and practitioners to the current threat\nlandscape and pressing research directions in secure computer vision."
    },
    {
        "date": "2025-09",
        "title": "Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition",
        "author": "Chun Liu, Hailong Wang, Bingqian Zhu, Panpan Ding, Zheng Zheng, Tao Xu, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2509.07495v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing\nsignificant security threats to their deployment in remote sensing\napplications. Research on adversarial attacks not only reveals model\nvulnerabilities but also provides critical insights for enhancing robustness.\nAlthough current mixing-based strategies have been proposed to increase the\ntransferability of adversarial examples, they either perform global blending or\ndirectly exchange a region in the images, which may destroy global semantic\nfeatures and mislead the optimization of adversarial examples. Furthermore,\ntheir reliance on cross-entropy loss for perturbation optimization leads to\ngradient diminishing during iterative updates, compromising adversarial example\nquality. To address these limitations, we focus on non-targeted attacks and\npropose a novel framework via local mixing and logits optimization. First, we\npresent a local mixing strategy to generate diverse yet semantically consistent\ninputs. Different from MixUp, which globally blends two images, and MixCut,\nwhich stitches images together, our method merely blends local regions to\npreserve global semantic information. Second, we adapt the logit loss from\ntargeted attacks to non-targeted scenarios, mitigating the gradient vanishing\nproblem of cross-entropy loss. Third, a perturbation smoothing loss is applied\nto suppress high-frequency noise and enhance transferability. Extensive\nexperiments on FGSCR-42 and MTARSI datasets demonstrate superior performance\nover 12 state-of-the-art methods across 6 surrogate models. Notably, with\nResNet as the surrogate on MTARSI, our method achieves a 17.28% average\nimprovement in black-box attack success rate."
    },
    {
        "date": "2025-09",
        "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis",
        "author": "Sven Kirchner, Nils Purschke, Ross Greer, and Alois C. Knoll",
        "link": "http://arxiv.org/abs/2509.07463v1",
        "abstract": "Ensuring reliable robot operation when visual input is degraded or\ninsufficient remains a central challenge in robotics. This letter introduces\nDepthVision, a framework for multimodal scene understanding designed to address\nthis problem. Unlike existing Vision-Language Models (VLMs), which use only\ncamera-based visual input alongside language, DepthVision synthesizes RGB\nimages from sparse LiDAR point clouds using a conditional generative\nadversarial network (GAN) with an integrated refiner network. These synthetic\nviews are then combined with real RGB data using a Luminance-Aware Modality\nAdaptation (LAMA), which blends the two types of data dynamically based on\nambient lighting conditions. This approach compensates for sensor degradation,\nsuch as darkness or motion blur, without requiring any fine-tuning of\ndownstream vision-language models. We evaluate DepthVision on real and\nsimulated datasets across various models and tasks, with particular attention\nto safety-critical tasks. The results demonstrate that our approach improves\nperformance in low-light conditions, achieving substantial gains over RGB-only\nbaselines while preserving compatibility with frozen VLMs. This work highlights\nthe potential of LiDAR-guided RGB synthesis for achieving robust robot\noperation in real-world environments."
    },
    {
        "date": "2025-09",
        "title": "EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise",
        "author": "Arslan Majal, Aamir Hussain Chughtai, and Muhammad Tahir",
        "link": "http://arxiv.org/abs/2509.07415v1",
        "abstract": "We present a learning-based outlier-robust filter for a general setup where\nthe measurement noise can be correlated. Since it is an enhanced version of\nEM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is\nequipped with an additional powerful feature to learn the outlier\ncharacteristics during inference along with outlier-detection, EMORF-II has\nimproved outlier-mitigation capability. Numerical experiments confirm\nperformance gains as compared to the state-of-the-art methods in terms of\naccuracy with an increased computational overhead. However, thankfully the\ncomputational complexity order remains at par with other practical methods\nmaking it a useful choice for diverse applications."
    },
    {
        "date": "2025-09",
        "title": "Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness",
        "author": "Ziang Yin, Hongjian Zhou, Chetan Choppali Sudarshan, Vidya Chhabria, and Jiaqi Gu",
        "link": "http://arxiv.org/abs/2509.07396v1",
        "abstract": "The relentless growth of large-scale artificial intelligence (AI) has created\nunprecedented demand for computational power, straining the energy, bandwidth,\nand scaling limits of conventional electronic platforms. Electronic-photonic\nintegrated circuits (EPICs) have emerged as a compelling platform for\nnext-generation AI systems, offering inherent advantages in ultra-high\nbandwidth, low latency, and energy efficiency for computing and\ninterconnection. Beyond performance, EPICs also hold unique promises for\nsustainability. Fabricated in relaxed process nodes with fewer metal layers and\nlower defect densities, photonic devices naturally reduce embodied carbon\nfootprint (CFP) compared to advanced digital electronic integrated circuits,\nwhile delivering orders-of-magnitude higher computing performance and\ninterconnect bandwidth. To further advance the sustainability of photonic AI\nsystems, we explore how electronic-photonic design automation (EPDA) and\ncross-layer co-design methodologies can amplify these inherent benefits. We\npresent how advanced EPDA tools enable more compact layout generation, reducing\nboth chip area and metal layer usage. We will also demonstrate how cross-layer\ndevice-circuit-architecture co-design unlocks new sustainability gains for\nphotonic hardware: ultra-compact photonic circuit designs that minimize chip\narea cost, reconfigurable hardware topology that adapts to evolving AI\nworkloads, and intelligent resilience mechanisms that prolong lifetime by\ntolerating variations and faults. By uniting intrinsic photonic efficiency with\nEPDA- and co-design-driven gains in area efficiency, reconfigurability, and\nrobustness, we outline a vision for lifelong-sustainable electronic-photonic AI\nsystems. This perspective highlights how EPIC AI systems can simultaneously\nmeet the performance demands of modern AI and the urgent imperative for\nsustainable computing."
    },
    {
        "date": "2025-09",
        "title": "When Fine-Tuning is Not Enough: Lessons from HSAD on Hybrid and Adversarial Audio Spoof Detection",
        "author": "Bin Hu, Kunyang Huang, Daehan Kwak, Meng Xu, and Kuan Huang",
        "link": "http://arxiv.org/abs/2509.07323v1",
        "abstract": "The rapid advancement of AI has enabled highly realistic speech synthesis and\nvoice cloning, posing serious risks to voice authentication, smart assistants,\nand telecom security. While most prior work frames spoof detection as a binary\ntask, real-world attacks often involve hybrid utterances that mix genuine and\nsynthetic speech, making detection substantially more challenging. To address\nthis gap, we introduce the Hybrid Spoofed Audio Dataset (HSAD), a benchmark\ncontaining 1,248 clean and 41,044 degraded utterances across four classes:\nhuman, cloned, zero-shot AI-generated, and hybrid audio. Each sample is\nannotated with spoofing method, speaker identity, and degradation metadata to\nenable fine-grained analysis. We evaluate six transformer-based models,\nincluding spectrogram encoders (MIT-AST, MattyB95-AST) and self-supervised\nwaveform models (Wav2Vec2, HuBERT). Results reveal critical lessons: pretrained\nmodels overgeneralize and collapse under hybrid conditions; spoof-specific\nfine-tuning improves separability but struggles with unseen compositions; and\ndataset-specific adaptation on HSAD yields large performance gains (AST greater\nthan 97 percent and F1 score is approximately 99 percent), though residual\nerrors persist for complex hybrids. These findings demonstrate that fine-tuning\nalone is not sufficient-robust hybrid-aware benchmarks like HSAD are essential\nto expose calibration failures, model biases, and factors affecting spoof\ndetection in adversarial environments. HSAD thus provides both a dataset and an\nanalytic framework for building resilient and trustworthy voice authentication\nsystems."
    },
    {
        "date": "2025-09",
        "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection",
        "author": "David Oprea, and Sam Powers",
        "link": "http://arxiv.org/abs/2509.07308v1",
        "abstract": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study",
        "author": "Kutub Uddin, Muhammad Umar Farooq, Awais Khan, and Khalid Mahmood Malik",
        "link": "http://arxiv.org/abs/2509.07132v1",
        "abstract": "The widespread use of generative AI has shown remarkable success in producing\nhighly realistic deepfakes, posing a serious threat to various voice biometric\napplications, including speaker verification, voice biometrics, audio\nconferencing, and criminal investigations. To counteract this, several\nstate-of-the-art (SoTA) audio deepfake detection (ADD) methods have been\nproposed to identify generative AI signatures to distinguish between real and\ndeepfake audio. However, the effectiveness of these methods is severely\nundermined by anti-forensic (AF) attacks that conceal generative signatures.\nThese AF attacks span a wide range of techniques, including statistical\nmodifications (e.g., pitch shifting, filtering, noise addition, and\nquantization) and optimization-based attacks (e.g., FGSM, PGD, C \\& W, and\nDeepFool). In this paper, we investigate the SoTA ADD methods and provide a\ncomparative analysis to highlight their effectiveness in exposing deepfake\nsignatures, as well as their vulnerabilities under adversarial conditions. We\nconducted an extensive evaluation of ADD methods on five deepfake benchmark\ndatasets using two categories: raw and spectrogram-based approaches. This\ncomparative analysis enables a deeper understanding of the strengths and\nlimitations of SoTA ADD methods against diverse AF attacks. It does not only\nhighlight vulnerabilities of ADD methods, but also informs the design of more\nrobust and generalized detectors for real-world voice biometrics. It will\nfurther guide future research in developing adaptive defense strategies that\ncan effectively counter evolving AF techniques."
    },
    {
        "date": "2025-09",
        "title": "SoK: Security and Privacy of AI Agents for Blockchain",
        "author": "Nicol\u00f2 Romandini, Carlo Mazzocca, Kai Otsuki, and Rebecca Montanari",
        "link": "http://arxiv.org/abs/2509.07131v1",
        "abstract": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions."
    },
    {
        "date": "2025-09",
        "title": "Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry",
        "author": "Soruya Saha, Md Nurul Absur, and Saptarshi Debroy",
        "link": "http://arxiv.org/abs/2509.07130v1",
        "abstract": "Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by\nfusing camera and Inertial Measurement Unit (IMU) data for real-time pose.\nHowever, current trend of offloading VIO to edge servers can lead server-side\nthreat surface where subtle pose spoofing can accumulate into substantial\ndrift, while evading heuristic checks. In this paper, we study this threat and\npresent an unsupervised, label-free detection and recovery mechanism. The\nproposed model is trained on attack-free sessions to learn temporal\nregularities of motion to detect runtime deviations and initiate recovery to\nrestore pose consistency. We evaluate the approach in a realistic offloaded-VIO\nenvironment using ILLIXR testbed across multiple spoofing intensities.\nExperimental results in terms of well-known performance metrics show\nsubstantial reductions in trajectory and pose error compared to a no-defense\nbaseline."
    },
    {
        "date": "2025-09",
        "title": "Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition",
        "author": "Tarhib Al Azad, and Shahana Ibrahim",
        "link": "http://arxiv.org/abs/2509.06918v1",
        "abstract": "Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings."
    },
    {
        "date": "2025-09",
        "title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition",
        "author": "Behnoud Shafiezadeh, Amir Mashmool, Farshad Eshghi, and Manoochehr Kelarestaghi",
        "link": "http://arxiv.org/abs/2509.06868v1",
        "abstract": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput."
    },
    {
        "date": "2025-09",
        "title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset",
        "author": "Nabeyou Tadessa, Balaji Iyangar, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2509.06835v1",
        "abstract": "Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems."
    },
    {
        "date": "2025-09",
        "title": "Imitative Membership Inference Attack",
        "author": "Yuntao Du, Yuetian Chen, Hanshen Xiao, Bruno Ribeiro, and Ninghui Li",
        "link": "http://arxiv.org/abs/2509.06796v1",
        "abstract": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches."
    },
    {
        "date": "2025-09",
        "title": "When Secure Isn't: Assessing the Security of Machine Learning Model Sharing",
        "author": "Gabriele Digregorio, Marco Di Gennaro, Stefano Zanero, Stefano Longari, and Michele Carminati",
        "link": "http://arxiv.org/abs/2509.06703v1",
        "abstract": "The rise of model-sharing through frameworks and dedicated hubs makes Machine\nLearning significantly more accessible. Despite their benefits, these tools\nexpose users to underexplored security risks, while security awareness remains\nlimited among both practitioners and developers. To enable a more\nsecurity-conscious culture in Machine Learning model sharing, in this paper we\nevaluate the security posture of frameworks and hubs, assess whether\nsecurity-oriented mechanisms offer real protection, and survey how users\nperceive the security narratives surrounding model sharing. Our evaluation\nshows that most frameworks and hubs address security risks partially at best,\noften by shifting responsibility to the user. More concerningly, our analysis\nof frameworks advertising security-oriented settings and complete model sharing\nuncovered six 0-day vulnerabilities enabling arbitrary code execution. Through\nthis analysis, we debunk the misconceptions that the model-sharing problem is\nlargely solved and that its security can be guaranteed by the file format used\nfor sharing. As expected, our survey shows that the surrounding security\nnarrative leads users to consider security-oriented settings as trustworthy,\ndespite the weaknesses shown in this work. From this, we derive takeaways and\nsuggestions to strengthen the security of model-sharing ecosystems."
    },
    {
        "date": "2025-09",
        "title": "Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives",
        "author": "Yuanyuan Wu, Zhenlin Qin, Leizhen Wang, Xiaolei Ma, and Zhenliang Ma",
        "link": "http://arxiv.org/abs/2509.06656v1",
        "abstract": "Understanding and modeling individual travel behavior responses is crucial\nfor urban mobility regulation and policy evaluation. The Markov decision\nprocess (MDP) provides a structured framework for dynamic travel behavior\nmodeling at the individual level. However, solving an MDP in this context is\nhighly data-intensive and faces challenges of data quantity, spatial-temporal\ncoverage, and situational diversity. To address these, we propose a\ngroup-effect-enhanced generative adversarial imitation learning (gcGAIL) model\nthat improves the individual behavior modeling efficiency by leveraging shared\nbehavioral patterns among passenger groups. We validate the gcGAIL model using\na public transport fare-discount case study and compare against\nstate-of-the-art benchmarks, including adversarial inverse reinforcement\nlearning (AIRL), baseline GAIL, and conditional GAIL. Experimental results\ndemonstrate that gcGAIL outperforms these methods in learning individual travel\nbehavior responses to incentives over time in terms of accuracy,\ngeneralization, and pattern demonstration efficiency. Notably, gcGAIL is robust\nto spatial variation, data sparsity, and behavioral diversity, maintaining\nstrong performance even with partial expert demonstrations and underrepresented\npassenger groups. The gcGAIL model predicts the individual behavior response at\nany time, providing the basis for personalized incentives to induce sustainable\nbehavior changes (better timing of incentive injections)."
    },
    {
        "date": "2025-09",
        "title": "Network-level Censorship Attacks in the InterPlanetary File System",
        "author": "Jan Matter, and Muoi Tran",
        "link": "http://arxiv.org/abs/2509.06626v1",
        "abstract": "The InterPlanetary File System (IPFS) has been successfully established as\nthe de facto standard for decentralized data storage in the emerging Web3.\nDespite its decentralized nature, IPFS nodes, as well as IPFS content\nproviders, have converged to centralization in large public clouds.\nCentralization introduces BGP routing-based attacks, such as passive\ninterception and BGP hijacking, as potential threats. Although this attack\nvector has been investigated for many other Web3 protocols, such as Bitcoin and\nEthereum, to the best of our knowledge, it has not been analyzed for the IPFS\nnetwork. In our work, we bridge this gap and demonstrate that BGP routing\nattacks can be effectively leveraged to censor content in IPFS. For the\nanalysis, we collected 3,000 content blocks called CIDs and conducted a\nsimulation of BGP hijacking and passive interception against them. We find that\na single malicious AS can censor 75% of the IPFS content for more than 57% of\nall requester nodes. Furthermore, we show that even with a small set of only 62\nhijacked prefixes, 70% of the full attack effectiveness can already be reached.\nWe further propose and validate countermeasures based on global collaborative\ncontent replication among all nodes in the IPFS network, together with\nadditional robust backup content provider nodes that are well-hardened against\nBGP hijacking. We hope this work raises awareness about the threat BGP\nrouting-based attacks pose to IPFS and triggers further efforts to harden the\nlive IPFS network against them."
    },
    {
        "date": "2025-09",
        "title": "A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)",
        "author": "Margarita Capretto, Mart\u00edn Ceresa, Antonio Fern\u00e1ndez Anta, Pedro Moreno-Sanchez, and C\u00e9sar S\u00e1nchez",
        "link": "http://arxiv.org/abs/2509.06614v2",
        "abstract": "Blockchains face a scalability limitation, partly due to the throughput\nlimitations of consensus protocols, especially when aiming to obtain a high\ndegree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to\nconventional blockchains. L2s perform most computations offchain using\nminimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer\nis a service that receives offchain L2 transaction requests, batches these\ntransactions, and commits compressed or hashed batches to L1. Using hashing\nneeds less L1 space, which is beneficial for gas cost, but requires a data\navailability committee (DAC) service to translate hashes into their\ncorresponding batches of transaction requests. The behavior of sequencers and\nDACs influence the evolution of the L2 blockchain, presenting a potential\nsecurity threat and delaying L2 adoption. We propose in this paper fraud-proof\nmechanisms, arbitrated by L1 contracts, to detect and generate evidence of\ndishonest behavior of the sequencer and DAC. We study how these fraud-proofs\nlimit the power of adversaries that control different number of sequencer and\nDACs members, and provide incentives for their honest behavior. We designed\nthese fraud-proof mechanisms as two player games. Unlike the generic\nfraud-proofs in current L2s (designed to guarantee the correct execution of\ntransactions), our fraud-proofs are over pred-etermined algorithms that verify\nthe properties that determine the correctness of the DAC. Arbitrating over\nconcrete algorithms makes our fraud-proofs more efficient, easier to\nunderstand, and simpler to prove correct. We provide as an artifact a\nmechanization in LEAN4 of our fraud-proof games, including (1) the verified\nstrategies that honest players should play to win all games as well as (2)\nmechanisms to detect dishonest claims."
    },
    {
        "date": "2025-09",
        "title": "Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination",
        "author": "Yian Huang, Yang Feng, and Zhiliang Ying",
        "link": "http://arxiv.org/abs/2509.06575v1",
        "abstract": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination."
    },
    {
        "date": "2025-09",
        "title": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem",
        "author": "Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, and Zhi Xue",
        "link": "http://arxiv.org/abs/2509.06572v1",
        "abstract": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments."
    },
    {
        "date": "2025-09",
        "title": "Robustness and accuracy of mean opinion scores with hard and soft outlier detection",
        "author": "Dietmar Saupe, and Tim Bleile",
        "link": "http://arxiv.org/abs/2509.06554v1",
        "abstract": "In subjective assessment of image and video quality, observers rate or\ncompare selected stimuli. Before calculating the mean opinion scores (MOS) for\nthese stimuli from the ratings, it is recommended to identify and deal with\noutliers that may have given unreliable ratings. Several methods are available\nfor this purpose, some of which have been standardized. These methods are\ntypically based on statistics and sometimes tested by introducing synthetic\nratings from artificial outliers, such as random clickers. However, a reliable\nand comprehensive approach is lacking for comparative performance analysis of\noutlier detection methods. To fill this gap, this work proposes and applies an\nempirical worst-case analysis as a general solution. Our method involves\nevolutionary optimization of an adversarial black-box attack on outlier\ndetection algorithms, where the adversary maximizes the distortion of scale\nvalues with respect to ground truth. We apply our analysis to several hard and\nsoft outlier detection methods for absolute category ratings and show their\ndiffering performance in this stress test. In addition, we propose two new\noutlier detection methods with low complexity and excellent worst-case\nperformance. Software for adversarial attacks and data analysis is available."
    },
    {
        "date": "2025-09",
        "title": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation",
        "author": "Hailong Chang, Guozhu Meng, Shuhui Xiao, Kai Chen, Kun Sun, and Yilin Li",
        "link": "http://arxiv.org/abs/2509.06504v1",
        "abstract": "With the growing demand for cross-language codebase migration, evaluating\nLLMs' security implications in translation tasks has become critical. Existing\nevaluations primarily focus on syntactic or functional correctness at the\nfunction level, neglecting the critical dimension of security.\n  To enable security evaluation, we construct STED (Security-centric\nTranslation Evaluation Dataset), the first dataset specifically designed for\nevaluating the security implications of LLM-based code translation. It\ncomprises 720 security-related code samples across five programming languages\nand nine high-impact CWE categories, sourced from CVE/NVD and manually verified\nfor translation tasks. Our evaluation framework consists of two independent\nassessment modules: (1) rigorous evaluation by security researchers, and (2)\nautomated analysis via LLM-as-a-judge. Together they evaluate three critical\naspects: functional correctness, vulnerability preservation, and vulnerability\nintroduction rates.\n  Our large-scale evaluation of five state-of-the-art LLMs across 6,000\ntranslation instances reveals significant security degradation, with 28.6-45%\nof translations introducing new vulnerabilities--particularly for web-related\nflaws like input validation, where LLMs show consistent weaknesses.\nFurthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation\nstrategy that reduces translation-induced vulnerabilities by 32.8%, showing the\npotential of knowledge-enhanced prompting."
    },
    {
        "date": "2025-09",
        "title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks",
        "author": "Sebastian-Vasile Echim, Andrei-Alexandru Preda, Dumitru-Clementin Cercel, and Florin Pop",
        "link": "http://arxiv.org/abs/2509.06459v1",
        "abstract": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation."
    },
    {
        "date": "2025-09",
        "title": "CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup",
        "author": "Xudong Mou, Rui Wang, Tiejun Wang, Renyu Yang, Shiru Chen, Jie Sun, Tianyu Wo, and Xudong Liu",
        "link": "http://arxiv.org/abs/2509.06419v1",
        "abstract": "Time series anomaly detection (TSAD) is a vital yet challenging task,\nparticularly in scenarios where labeled anomalies are scarce and temporal\ndependencies are complex. Recent anomaly assumption (AA) approaches alleviate\nthe lack of anomalies by injecting synthetic samples and training\ndiscriminative models. Despite promising results, these methods often suffer\nfrom two fundamental limitations: patchy generation, where scattered anomaly\nknowledge leads to overly simplistic or incoherent anomaly injection, and\nAnomaly Shift, where synthetic anomalies either resemble normal data too\nclosely or diverge unrealistically from real anomalies, thereby distorting\nclassification boundaries. In this paper, we propose CAPMix, a controllable\nanomaly augmentation framework that addresses both issues. First, we design a\nCutAddPaste mechanism to inject diverse and complex anomalies in a targeted\nmanner, avoiding patchy generation. Second, we introduce a label revision\nstrategy to adaptively refine anomaly labels, reducing the risk of anomaly\nshift. Finally, we employ dual-space mixup within a temporal convolutional\nnetwork to enforce smoother and more robust decision boundaries. Extensive\nexperiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and\nESA, demonstrate that CAPMix achieves significant improvements over\nstate-of-the-art baselines, with enhanced robustness against contaminated\ntraining data. The code is available at https://github.com/alsike22/CAPMix."
    },
    {
        "date": "2025-09",
        "title": "Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection",
        "author": "Hyungjoon Soh, Dongha Lee, Vipul Periwal, and Junghyo Jo",
        "link": "http://arxiv.org/abs/2509.06383v1",
        "abstract": "Selecting key variables from high-dimensional data is increasingly important\nin the era of big data. Sparse regression serves as a powerful tool for this\npurpose by promoting model simplicity and explainability. In this work, we\nrevisit a valuable yet underutilized method, the statistical physics-based\nVariational Garrote (VG), which introduces explicit feature selection spin\nvariables and leverages variational inference to derive a tractable loss\nfunction. We enhance VG by incorporating modern automatic differentiation\ntechniques, enabling scalable and efficient optimization. We evaluate VG on\nboth fully controllable synthetic datasets and complex real-world datasets. Our\nresults demonstrate that VG performs especially well in highly sparse regimes,\noffering more consistent and robust variable selection than Ridge and LASSO\nregression across varying levels of sparsity. We also uncover a sharp\ntransition: as superfluous variables are admitted, generalization degrades\nabruptly and the uncertainty of the selection variables increases. This\ntransition point provides a practical signal for estimating the correct number\nof relevant variables, an insight we successfully apply to identify key\npredictors in real-world data. We expect that VG offers strong potential for\nsparse modeling across a wide range of applications, including compressed\nsensing and model pruning in machine learning."
    },
    {
        "date": "2025-09",
        "title": "From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)",
        "author": "Kunlin Cai, Jinghuai Zhang, Ying Li, Zhiyuan Wang, Xun Chen, Tianshi Li, and Yuan Tian",
        "link": "http://arxiv.org/abs/2509.06368v1",
        "abstract": "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges."
    },
    {
        "date": "2025-09",
        "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?",
        "author": "Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, and Xiangzheng Zhang",
        "link": "http://arxiv.org/abs/2509.06350v1",
        "abstract": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks."
    },
    {
        "date": "2025-09",
        "title": "Schrodinger's Toolbox: Exploring the Quantum Rowhammer Attack",
        "author": "Devon Campbell",
        "link": "http://arxiv.org/abs/2509.06318v1",
        "abstract": "Residual cross-talk in superconducting qubit devices creates a security\nvulnerability for emerging quantum cloud services. We demonstrate a\nClifford-only Quantum Rowhammer attack-using just X and CNOT gates-that injects\nfaults on IBM's 127-qubit Eagle processors without requiring pulse-level\naccess. Experiments show that targeted hammering induces localized errors\nconfined to the attack cycle and primarily manifests as phase noise, as\nconfirmed by near 50% flip rates under Hadamard-basis probing. A full lattice\nsweep maps QR's spatial and temporal behavior, revealing reproducible\ncorruption limited to qubits within two coupling hops and rapid recovery in\nsubsequent benign cycles. Finally, we leverage these properties to outline a\nprime-and-probe covert channel, demonstrating that the clear separability\nbetween hammered and benign rounds enables highly reliable signaling without\nerror correction. These findings underscore the need for hardware-level\nisolation and scheduler-aware defenses as multi-tenant quantum computing\nbecomes standard."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition",
        "author": "Guangyu Lei, Tianhao Liang, Yuqi Ping, Xinglin Chen, Longyu Zhou, Junwei Wu, Xiyuan Zhang, Huahao Ding, Xingjian Zhang, Weijie Yuan, Tingting Zhang, and Qinyu Zhang",
        "link": "http://arxiv.org/abs/2509.06312v1",
        "abstract": "The rapid development of the low-altitude economy emphasizes the critical\nneed for effective perception and intent recognition of non-cooperative\nunmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities\nof multimodal large language models (MLLMs) present a promising approach in\nsuch tasks. In this paper, we focus on the combination of UAV intent\nrecognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV\nintent recognition architecture, where the multimodal perception system is\nutilized to obtain real-time payload and motion information of UAVs, generating\nstructured input information, and MLLM outputs intent recognition results by\nincorporating environmental information, prior knowledge, and tactical\npreferences. Subsequently, we review the related work and demonstrate their\nprogress within the proposed architecture. Then, a use case for low-altitude\nconfrontation is conducted to demonstrate the feasibility of our architecture\nand offer valuable insights for practical system design. Finally, the future\nchallenges are discussed, followed by corresponding strategic recommendations\nfor further applications."
    },
    {
        "date": "2025-09",
        "title": "Robust Analysis for Resilient AI System",
        "author": "Yu Wang, Ran Jin, and Lulu Kang",
        "link": "http://arxiv.org/abs/2509.06172v1",
        "abstract": "Operational hazards in Manufacturing Industrial Internet (MII) systems\ngenerate severe data outliers that cripple traditional statistical analysis.\nThis paper proposes a novel robust regression method, DPD-Lasso, which\nintegrates Density Power Divergence with Lasso regularization to analyze\ncontaminated data from AI resilience experiments. We develop an efficient\niterative algorithm to overcome previous computational bottlenecks. Applied to\nan MII testbed for Aerosol Jet Printing, DPD-Lasso provides reliable, stable\nperformance on both clean and outlier-contaminated data, accurately quantifying\nhazard impacts. This work establishes robust regression as an essential tool\nfor developing and validating resilient industrial AI systems."
    },
    {
        "date": "2025-09",
        "title": "Additive Distributionally Robust Ranking and Selection",
        "author": "Zaile Li, Yuchen Wan, and L. Jeff Hong",
        "link": "http://arxiv.org/abs/2509.06147v1",
        "abstract": "Ranking and selection (R&S) aims to identify the alternative with the best\nmean performance among $k$ simulated alternatives. The practical value of R&S\ndepends on accurate simulation input modeling, which often suffers from the\ncurse of input uncertainty due to limited data. Distributionally robust ranking\nand selection (DRR&S) addresses this challenge by modeling input uncertainty\nvia an ambiguity set of $m > 1$ plausible input distributions, resulting in\n$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:\nadditivity in budget allocation is essential for efficiency. However, existing\njustifications are heuristic, and fundamental properties such as consistency\nand the precise allocation pattern induced by additivity remain poorly\nunderstood. In this paper, we propose a simple additive allocation (AA)\nprocedure that aims to exclusively sample the $k + m - 1$ previously\nhypothesized critical scenarios. Leveraging boundary-crossing arguments, we\nestablish a lower bound on the probability of correct selection and\ncharacterize the procedure's budget allocation behavior. We then prove that AA\nis consistent and, surprisingly, achieves additivity in the strongest sense: as\nthe total budget increases, only $k + m - 1$ scenarios are sampled infinitely\noften. Notably, the worst-case scenarios of non-best alternatives may not be\namong them, challenging prior beliefs about their criticality. These results\noffer new and counterintuitive insights into the additive structure of DRR&S.\nTo improve practical performance while preserving this structure, we introduce\na general additive allocation (GAA) framework that flexibly incorporates\nsampling rules from traditional R&S procedures in a modular fashion. Numerical\nexperiments support our theoretical findings and demonstrate the competitive\nperformance of the proposed GAA procedures."
    },
    {
        "date": "2025-09",
        "title": "Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving",
        "author": "Yang Lou, Haibo Hu, Qun Song, Qian Xu, Yi Zhu, Rui Tan, Wei-Bin Lee, and Jianping Wang",
        "link": "http://arxiv.org/abs/2509.06071v1",
        "abstract": "High-definition maps provide precise environmental information essential for\nprediction and planning in autonomous driving systems. Due to the high cost of\nlabeling and maintenance, recent research has turned to online HD map\nconstruction using onboard sensor data, offering wider coverage and more timely\nupdates for autonomous vehicles. However, the robustness of online map\nconstruction under adversarial conditions remains underexplored. In this paper,\nwe present a systematic vulnerability analysis of online map construction\nmodels, which reveals that these models exhibit an inherent bias toward\npredicting symmetric road structures. In asymmetric scenes like forks or\nmerges, this bias often causes the model to mistakenly predict a straight\nboundary that mirrors the opposite side. We demonstrate that this vulnerability\npersists in the real-world and can be reliably triggered by obstruction or\ntargeted interference. Leveraging this vulnerability, we propose a novel\ntwo-stage attack framework capable of manipulating online constructed maps.\nFirst, our method identifies vulnerable asymmetric scenes along the victim AV's\npotential route. Then, we optimize the location and pattern of camera-blinding\nattacks and adversarial patch attacks. Evaluations on a public AD dataset\ndemonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render\nup to 44% of targeted routes unreachable, and increase unsafe planned\ntrajectory rates, colliding with real-world road boundaries, by up to 27%.\nThese attacks are also validated on a real-world testbed vehicle. We further\nanalyze root causes of the symmetry bias, attributing them to training data\nimbalance, model architecture, and map element representation. To the best of\nour knowledge, this study presents the first vulnerability assessment of online\nmap construction models and introduces the first digital and physical attack\nagainst them."
    },
    {
        "date": "2025-09",
        "title": "Empirical Study of Code Large Language Models for Binary Security Patch Detection",
        "author": "Qingyuan Li, Binchang Li, Cuiyun Gao, Shuzheng Gao, and Zongjie Li",
        "link": "http://arxiv.org/abs/2509.06052v1",
        "abstract": "Security patch detection (SPD) is crucial for maintaining software security,\nas unpatched vulnerabilities can lead to severe security risks. In recent\nyears, numerous learning-based SPD approaches have demonstrated promising\nresults on source code. However, these approaches typically cannot be applied\nto closed-source applications and proprietary systems that constitute a\nsignificant portion of real-world software, as they release patches only with\nbinary files, and the source code is inaccessible. Given the impressive\nperformance of code large language models (LLMs) in code intelligence and\nbinary analysis tasks such as decompilation and compilation optimization, their\npotential for detecting binary security patches remains unexplored, exposing a\nsignificant research gap between their demonstrated low-level code\nunderstanding capabilities and this critical security task. To address this\ngap, we construct a large-scale binary patch dataset containing \\textbf{19,448}\nsamples, with two levels of representation: assembly code and pseudo-code, and\nsystematically evaluate \\textbf{19} code LLMs of varying scales to investigate\ntheir capability in binary SPD tasks. Our initial exploration demonstrates that\ndirectly prompting vanilla code LLMs struggles to accurately identify security\npatches from binary patches, and even state-of-the-art prompting techniques\nfail to mitigate the lack of domain knowledge in binary SPD within vanilla\nmodels. Drawing on the initial findings, we further investigate the fine-tuning\nstrategy for injecting binary SPD domain knowledge into code LLMs through two\nlevels of representation. Experimental results demonstrate that fine-tuned LLMs\nachieve outstanding performance, with the best results obtained on the\npseudo-code representation."
    },
    {
        "date": "2025-09",
        "title": "DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation",
        "author": "Xinyu Gao, Xiangtao Meng, Yingkai Dong, Zheng Li, and Shanqing Guo",
        "link": "http://arxiv.org/abs/2509.06026v1",
        "abstract": "While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations\nby integrating external knowledge bases, it introduces vulnerabilities to\nmembership inference attacks (MIAs), particularly in systems handling sensitive\ndata. Existing MIAs targeting RAG's external databases often rely on model\nresponses but ignore the interference of non-member-retrieved documents on RAG\noutputs, limiting their effectiveness. To address this, we propose DCMI, a\ndifferential calibration MIA that mitigates the negative impact of\nnon-member-retrieved documents. Specifically, DCMI leverages the sensitivity\ngap between member and non-member retrieved documents under query perturbation.\nIt generates perturbed queries for calibration to isolate the contribution of\nmember-retrieved documents while minimizing the interference from\nnon-member-retrieved documents. Experiments under progressively relaxed\nassumptions show that DCMI consistently outperforms baselines--for example,\nachieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5,\nexceeding the MBA baseline by over 40%. Furthermore, on real-world RAG\nplatforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the\nbaseline. These results highlight significant privacy risks in RAG systems and\nemphasize the need for stronger protection mechanisms. We appeal to the\ncommunity's consideration of deeper investigations, like ours, against the data\nleakage risks in rapidly evolving RAG systems. Our code is available at\nhttps://github.com/Xinyu140203/RAG_MIA."
    },
    {
        "date": "2025-09",
        "title": "ConstStyle: Robust Domain Generalization with Unified Style Transformation",
        "author": "Nam Duong Tran, Nam Nguyen Phuong, Hieu H. Pham, Phi Le Nguyen, and My T. Thai",
        "link": "http://arxiv.org/abs/2509.05975v1",
        "abstract": "Deep neural networks often suffer performance drops when test data\ndistribution differs from training data. Domain Generalization (DG) aims to\naddress this by focusing on domain-invariant features or augmenting data for\ngreater diversity. However, these methods often struggle with limited training\ndomains or significant gaps between seen (training) and unseen (test) domains.\nTo enhance DG robustness, we hypothesize that it is essential for the model to\nbe trained on data from domains that closely resemble unseen test domains-an\ninherently difficult task due to the absence of prior knowledge about the\nunseen domains. Accordingly, we propose ConstStyle, a novel approach that\nleverages a unified domain to capture domain-invariant features and bridge the\ndomain gap with theoretical analysis. During training, all samples are mapped\nonto this unified domain, optimized for seen domains. During testing, unseen\ndomain samples are projected similarly before predictions. By aligning both\ntraining and testing data within this unified domain, ConstStyle effectively\nreduces the impact of domain shifts, even with large domain gaps or few seen\ndomains. Extensive experiments demonstrate that ConstStyle consistently\noutperforms existing methods across diverse scenarios. Notably, when only a\nlimited number of seen domains are available, ConstStyle can boost accuracy up\nto 19.82\\% compared to the next best approach."
    },
    {
        "date": "2025-09",
        "title": "Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms",
        "author": "Ali Zeynali, Mahsa Sahebdel, Qingsong Liu, Mohammad Hajiesmaili, and Ramesh K. Sitaraman",
        "link": "http://arxiv.org/abs/2509.05930v1",
        "abstract": "We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)\nproblem, a new framework that integrates three key objectives in online\ndecision-making under uncertainty: (1) tracking cost for following a\ndynamically moving target, (2) adversarial perturbation cost for withstanding\nunpredictable disturbances, and (3) switching cost for penalizing abrupt\nchanges in decisions. This formulation captures real-world scenarios such as\nelastic and inelastic workload scheduling in AI clusters, where operators must\nbalance long-term service-level agreements (e.g., LLM training) against sudden\ndemand spikes (e.g., real-time inference). We first present BEST, a robust\nalgorithm with provable competitive guarantees for SOOTT. To enhance practical\nperformance, we introduce CoRT, a learning-augmented variant that incorporates\nuntrusted black-box predictions (e.g., from ML models) into its decision\nprocess. Our theoretical analysis shows that CoRT strictly improves over BEST\nwhen predictions are accurate, while maintaining robustness under arbitrary\nprediction errors. We validate our approach through a case study on workload\nscheduling, demonstrating that both algorithms effectively balance trajectory\ntracking, decision smoothness, and resilience to external disturbances."
    },
    {
        "date": "2025-09",
        "title": "Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs",
        "author": "Andrew Yeo, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2509.05883v1",
        "abstract": "Large Language Models (LLMs) have seen rapid adoption in recent years, with\nindustries increasingly relying on them to maintain a competitive advantage.\nThese models excel at interpreting user instructions and generating human-like\nresponses, leading to their integration across diverse domains, including\nconsulting and information retrieval. However, their widespread deployment also\nintroduces substantial security risks, most notably in the form of prompt\ninjection and jailbreak attacks.\n  To systematically evaluate LLM vulnerabilities -- particularly to external\nprompt injection -- we conducted a series of experiments on eight commercial\nmodels. Each model was tested without supplementary sanitization, relying\nsolely on its built-in safeguards. The results exposed exploitable weaknesses\nand emphasized the need for stronger security measures. Four categories of\nattacks were examined: direct injection, indirect (external) injection,\nimage-based injection, and prompt leakage. Comparative analysis indicated that\nClaude 3 demonstrated relatively greater robustness; nevertheless, empirical\nfindings confirm that additional defenses, such as input normalization, remain\nnecessary to achieve reliable protection."
    },
    {
        "date": "2025-09",
        "title": "Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV",
        "author": "Muhammad Arif Hakimi Zamrai, and Kamaludin Mohd Yusof",
        "link": "http://arxiv.org/abs/2509.07016v1",
        "abstract": "In response to the prevalent concern of TCP SYN flood attacks within the\ncontext of Software-Defined Internet of Vehicles (SD-IoV), this study addresses\nthe significant challenge of network security in rapidly evolving vehicular\ncommunication systems. This research focuses on optimizing a Random Forest\nClassifier model to achieve maximum accuracy and minimal detection time,\nthereby enhancing vehicular network security. The methodology involves\npreprocessing a dataset containing SYN attack instances, employing feature\nscaling and label encoding techniques, and applying Stratified K-Fold\ncross-validation to target key metrics such as accuracy, precision, recall, and\nF1-score. This research achieved an average value of 0.999998 for all metrics\nwith a SYN DoS attack detection time of 0.24 seconds. Results show that the\nfine-tuned Random Forest model, configured with 20 estimators and a depth of\n10, effectively differentiates between normal and malicious traffic with high\naccuracy and minimal detection time, which is crucial for SD-IoV networks. This\napproach marks a significant advancement and introduces a state-of-the-art\nalgorithm in detecting SYN flood attacks, combining high accuracy with minimal\ndetection time. It contributes to vehicular network security by providing a\nrobust solution against TCP SYN flood attacks while maintaining network\nefficiency and reliability."
    },
    {
        "date": "2025-09",
        "title": "Yours or Mine? Overwriting Attacks against Neural Audio Watermarking",
        "author": "Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Phone Lin, Tomoaki Ohtsuki, and Miao Pan",
        "link": "http://arxiv.org/abs/2509.05835v1",
        "abstract": "As generative audio models are rapidly evolving, AI-generated audios\nincreasingly raise concerns about copyright infringement and misinformation\nspread. Audio watermarking, as a proactive defense, can embed secret messages\ninto audio for copyright protection and source verification. However, current\nneural audio watermarking methods focus primarily on the imperceptibility and\nrobustness of watermarking, while ignoring its vulnerability to security\nattacks. In this paper, we develop a simple yet powerful attack: the\noverwriting attack that overwrites the legitimate audio watermark with a forged\none and makes the original legitimate watermark undetectable. Based on the\naudio watermarking information that the adversary has, we propose three\ncategories of overwriting attacks, i.e., white-box, gray-box, and black-box\nattacks. We also thoroughly evaluate the proposed attacks on state-of-the-art\nneural audio watermarking methods. Experimental results demonstrate that the\nproposed overwriting attacks can effectively compromise existing watermarking\nschemes across various settings and achieve a nearly 100% attack success rate.\nThe practicality and effectiveness of the proposed overwriting attacks expose\nsecurity flaws in existing neural audio watermarking systems, underscoring the\nneed to enhance security in future audio watermarking designs."
    },
    {
        "date": "2025-09",
        "title": "Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces",
        "author": "Zeyu Song, Sainyam Galhotra, and Shagufta Mehnaz",
        "link": "http://arxiv.org/abs/2509.05833v1",
        "abstract": "The rise of distributed and privacy-preserving machine learning has sparked\ninterest in decentralized gradient marketplaces, where participants trade\nintermediate artifacts like gradients. However, existing Federated Learning\n(FL) benchmarks overlook critical economic and systemic factors unique to such\nmarketplaces-cost-effectiveness, fairness to sellers, and market\nstability-especially when a buyer relies on a private baseline dataset for\nevaluation.\n  We introduce a comprehensive benchmark framework to holistically evaluate\nrobust gradient aggregation methods within these buyer-baseline-reliant\nmarketplaces. Our contributions include: (1) a simulation environment modeling\nmarketplace dynamics with a variable buyer baseline and diverse seller\ndistributions; (2) an evaluation methodology augmenting standard FL metrics\nwith marketplace-centric dimensions such as Economic Efficiency, Fairness, and\nSelection Dynamics; (3) an in-depth empirical analysis of the existing\nDistributed Gradient Marketplace framework, MartFL, including the integration\nand comparative evaluation of adapted FLTrust and SkyMask as alternative\naggregation strategies within it. This benchmark spans diverse datasets, local\nattacks, and Sybil attacks targeting the marketplace selection process; and (4)\nactionable insights into the trade-offs between model performance, robustness,\ncost, fairness, and stability.\n  This benchmark equips the community with essential tools and empirical\nevidence to evaluate and design more robust, equitable, and economically viable\ndecentralized gradient marketplaces."
    },
    {
        "date": "2025-09",
        "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization",
        "author": "Ishaan Verma",
        "link": "http://arxiv.org/abs/2509.05831v1",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content."
    },
    {
        "date": "2025-09",
        "title": "Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond",
        "author": "Hai Dinh-Tuan, Sandro Rodriguez Garzon, and Jianeng Fu",
        "link": "http://arxiv.org/abs/2509.05797v1",
        "abstract": "In the evolving landscape of future mobile networks, there is a critical need\nfor secure and trustful communication modalities to support dynamic\ninteractions among core network components of different network domains. This\npaper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs)\nto establish secure and trustful communication channels among network functions\nin 5G and subsequent generations. A new communication agent is introduced that\nintegrates seamlessly with 5G-standardized network functions and utilizes a\nDID-based application layer transport protocol to ensure confidentiality,\nintegrity, and authenticity for cross-domain interactions. A comparative\nanalysis of the two different versions of the DID-based communication protocol\nfor inter network function communication reveals compatibility advantages of\nthe latest protocol iteration. Furthermore, a comprehensive evaluation of the\ncommunication overhead caused by both protocol iterations compared to\ntraditional TCP/TLS shows the benefits of using DIDs to improve communication\nsecurity, albeit with performance loses compared to TCP/TLS. These results\nuncover the potential of DID-based communication for future mobile networks but\nalso point out areas for optimization."
    },
    {
        "date": "2025-09",
        "title": "DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection",
        "author": "Arantxa Urrea-Casta\u00f1o, Nicol\u00e1s Segura-Kunsagi, Juan Luis Su\u00e1rez-D\u00edaz, Rosana Montes, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2509.05778v1",
        "abstract": "Out-of-distribution (OOD) detection plays a key role in enhancing the\nrobustness of artificial intelligence systems by identifying inputs that differ\nsignificantly from the training distribution, thereby preventing unreliable\npredictions and enabling appropriate fallback mechanisms. Developing reliable\nOOD detection methods is a significant challenge, and rigorous evaluation of\nthese techniques is essential for ensuring their effectiveness, as it allows\nresearchers to assess their performance under diverse conditions and to\nidentify potential limitations or failure modes. Cross-validation (CV) has\nproven to be a highly effective tool for providing a reasonable estimate of the\nperformance of a learning algorithm. Although OOD scenarios exhibit particular\ncharacteristics, an appropriate adaptation of CV can lead to a suitable\nevaluation framework for this setting. This work proposes a dual CV framework\nfor robust evaluation of OOD detection models, aimed at improving the\nreliability of their assessment. The proposed evaluation framework aims to\neffectively integrate in-distribution (ID) and OOD data while accounting for\ntheir differing characteristics. To achieve this, ID data are partitioned using\na conventional approach, whereas OOD data are divided by grouping samples based\non their classes. Furthermore, we analyze the context of data with class\nhierarchy to propose a data splitting that considers the entire class hierarchy\nto obtain fair ID-OOD partitions to apply the proposed evaluation framework.\nThis framework is called Dual Cross-Validation for Robust Out-of-Distribution\nDetection (DCV-ROOD). To test the validity of the evaluation framework, we\nselected a set of state-of-the-art OOD detection methods, both with and without\noutlier exposure. The results show that the method achieves very fast\nconvergence to the true performance."
    },
    {
        "date": "2025-09",
        "title": "Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting",
        "author": "Chen Shao, Yue Wang, Zhenyi Zhu, Zhanbo Huang, Sebastian P\u00fctz, Benjamin Sch\u00e4fer, Tobais K\u00e4fer, and Michael F\u00e4rber",
        "link": "http://arxiv.org/abs/2509.05768v1",
        "abstract": "Energy forecasting is vital for grid reliability and operational efficiency.\nAlthough recent advances in time series forecasting have led to progress,\nexisting benchmarks remain limited in spatial and temporal scope and lack\nmulti-energy features. This raises concerns about their reliability and\napplicability in real-world deployment. To address this, we present the Real-E\ndataset, covering over 74 power stations across 30+ European countries over a\n10-year span with rich metadata. Using Real- E, we conduct an extensive data\nanalysis and benchmark over 20 baselines across various model types. We\nintroduce a new metric to quantify shifts in correlation structures and show\nthat existing methods struggle on our dataset, which exhibits more complex and\nnon-stationary correlation dynamics. Our findings highlight key limitations of\ncurrent methods and offer a strong empirical basis for building more robust\nforecasting models"
    },
    {
        "date": "2025-09",
        "title": "Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras",
        "author": "Xinyu Zhang, Kai Huang, Junqiao Zhao, Zihan Yuan, and Tiantian Feng",
        "link": "http://arxiv.org/abs/2509.05740v1",
        "abstract": "We propose a multi-camera LiDAR-visual-inertial odometry framework,\nMulti-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and\ninertial sensors for highly accurate and robust state estimation. To enable\nefficient and consistent integration of visual information from multiple\nfisheye cameras, we introduce a panoramic visual feature model that unifies\nmulti-camera observations into a single representation. The panoramic model\nserves as a global geometric optimization framework that consolidates\nmulti-view constraints, enabling seamless loop closure and global pose\noptimization, while simplifying system design by avoiding redundant handling of\nindividual cameras. To address the triangulation inconsistency caused by the\nmisalignment between each camera's frame and the panoramic model's frame, we\npropose an extrinsic compensation method. This method improves feature\nconsistency across views and significantly reduces triangulation and\noptimization errors, leading to more accurate pose estimation. We integrate the\npanoramic visual feature model into a tightly coupled LiDAR-visual-inertial\nsystem based on a factor graph. Extensive experiments on public datasets\ndemonstrate that the panoramic visual feature model enhances the quality and\nconsistency of multi-camera constraints, resulting in higher accuracy and\nrobustness than existing multi-camera LiDAR-visual-inertial systems."
    },
    {
        "date": "2025-09",
        "title": "Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated",
        "author": "Hanna Foerster, Ilia Shumailov, Yiren Zhao, Harsh Chaudhari, Jamie Hayes, Robert Mullins, and Yarin Gal",
        "link": "http://arxiv.org/abs/2509.05739v1",
        "abstract": "Early research into data poisoning attacks against Large Language Models\n(LLMs) demonstrated the ease with which backdoors could be injected. More\nrecent LLMs add step-by-step reasoning, expanding the attack surface to include\nthe intermediate chain-of-thought (CoT) and its inherent trait of decomposing\nproblems into subproblems. Using these vectors for more stealthy poisoning, we\nintroduce ``decomposed reasoning poison'', in which the attacker modifies only\nthe reasoning path, leaving prompts and final answers clean, and splits the\ntrigger across multiple, individually harmless components.\n  Fascinatingly, while it remains possible to inject these decomposed poisons,\nreliably activating them to change final answers (rather than just the CoT) is\nsurprisingly difficult. This difficulty arises because the models can often\nrecover from backdoors that are activated within their thought processes.\nUltimately, it appears that an emergent form of backdoor robustness is\noriginating from the reasoning capabilities of these advanced LLMs, as well as\nfrom the architectural separation between reasoning and final answer\ngeneration."
    },
    {
        "date": "2025-09",
        "title": "Robust variational neural posterior estimation for simulation-based inference",
        "author": "Matthew O'Callaghan, Kaisey S. Mandel, and Gerry Gilmore",
        "link": "http://arxiv.org/abs/2509.05724v1",
        "abstract": "Recent advances in neural density estimation have enabled powerful\nsimulation-based inference (SBI) methods that can flexibly approximate Bayesian\ninference for intractable stochastic models. Although these methods have\ndemonstrated reliable posterior estimation when the simulator accurately\nrepresents the underlying data generative process (GDP), recent work has shown\nthat they perform poorly in the presence of model misspecification. This poses\na significant problem for their use on real-world problems, due to simulators\nalways misrepresenting the true DGP to a certain degree. In this paper, we\nintroduce robust variational neural posterior estimation (RVNP), a method which\naddresses the problem of misspecification in amortised SBI by bridging the\nsimulation-to-reality gap using variational inference and error modelling. We\ntest RVNP on multiple benchmark tasks, including using real data from\nastronomy, and show that it can recover robust posterior inference in a\ndata-driven manner without adopting tunable hyperparameters or priors governing\nthe misspecification."
    },
    {
        "date": "2025-09",
        "title": "Larger-scale Nakamoto-style Blockchains Offer Better Security",
        "author": "Junjie Hu, and Na Ruan",
        "link": "http://arxiv.org/abs/2509.05708v1",
        "abstract": "Traditional security models for Nakamoto-style blockchains overestimate\nadversarial coordination by assuming instantaneous synchronization among\nmalicious nodes, neglecting the critical impact of internal communication\ndelays on security. This paper introduces a dual-delay framework to revisit\nsecurity analysis, addressing this oversight through two key innovations.\nFirst, the static delay model quantifies how adversarial communication delays\n(\\(\\Delta_a\\)) constrain the effective growth rate of private chains, derived\nvia an M/D/1 queuing model as \\(\\lambda_{eff} = \\lambda_a / (1 + \\lambda_a\n\\Delta_a)\\). This model reveals that the security threshold (\\(\\beta^*\\)), the\nmaximum adversarial power the system tolerates, increases with \\(\\Delta_a\\),\neven exceeding the classic 51\\% boundary when \\(\\Delta_a \\textgreater \\Delta\\)\n(honest nodes' delay), breaking the long-standing 50\\% assumption. Second, the\ndynamic delay model integrates probabilistic corruption and scale-dependent\ndelays to characterize the total adversarial delay window (\\(\\Delta_{total} =\n\\Delta(n) e^{-k\\beta} + c \\log(1 + \\beta n)\\)), where \\(\\Delta(n) \\in\n\\Theta(\\log n)\\) captures honest nodes' logarithmic delay growth. Asymptotic\nanalysis shows adversarial power decays linearly with network scale, ensuring\nthe probability of \\(\\beta \\leq \\beta^*\\) approaches 1 as \\(n \\to \\infty\\). By\nexposing the interplay between network scale, communication delays, and power\ndilution, we provide a theoretical foundation for optimizing consensus\nprotocols and assessing robustness in large-scale Nakamoto-style blockchains."
    },
    {
        "date": "2025-09",
        "title": "SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts",
        "author": "Xng Ai, Shudan Lin, Zecheng Li, Kai Zhou, Bixin Li, and Bin Xiao",
        "link": "http://arxiv.org/abs/2509.05681v1",
        "abstract": "Decentralized Finance (DeFi) attacks have resulted in significant losses,\noften orchestrated through Adversarial Exploiter Contracts (AECs) that exploit\nvulnerabilities in victim smart contracts. To proactively identify such\nthreats, this paper targets the explainable detection of AECs.\n  Existing detection methods struggle to capture semantic dependencies and lack\ninterpretability, limiting their effectiveness and leaving critical knowledge\ngaps in AEC analysis. To address these challenges, we introduce SEASONED, an\neffective, self-explanatory, and robust framework for AEC detection.\n  SEASONED extracts semantic information from contract bytecode to construct a\nsemantic relation graph (SRG), and employs a self-counterfactual explainable\ndetector (SCFED) to classify SRGs and generate explanations that highlight the\ncore attack logic. SCFED further enhances robustness, generalizability, and\ndata efficiency by extracting representative information from these\nexplanations. Both theoretical analysis and experimental results demonstrate\nthe effectiveness of SEASONED, which showcases outstanding detection\nperformance, robustness, generalizability, and data efficiency learning\nability. To support further research, we also release a new dataset of 359\nAECs."
    },
    {
        "date": "2025-09",
        "title": "SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models",
        "author": "Kien Nguyen, Anh Tran, and Cuong Pham",
        "link": "http://arxiv.org/abs/2509.05625v1",
        "abstract": "The rapid growth of text-to-image diffusion models has raised concerns about\ntheir potential misuse in generating harmful or unauthorized contents. To\naddress these issues, several Concept Erasure methods have been proposed.\nHowever, most of them fail to achieve both robustness, i.e., the ability to\nrobustly remove the target concept., and effectiveness, i.e., maintaining image\nquality. While few recent techniques successfully achieve these goals for NSFW\nconcepts, none could handle narrow concepts such as copyrighted characters or\ncelebrities. Erasing these narrow concepts is critical in addressing copyright\nand legal concerns. However, erasing them is challenging due to their close\ndistances to non-target neighboring concepts, requiring finer-grained\nmanipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel\nmethod specifically designed to achieve both robustness and effectiveness in\neasing these narrow concepts. SuMa first derives a target subspace representing\nthe concept to be erased and then neutralizes it by mapping it to a reference\nsubspace that minimizes the distance between the two. This mapping ensures the\ntarget concept is robustly erased while preserving image quality. We conduct\nextensive experiments with SuMa across four tasks: subclass erasure, celebrity\nerasure, artistic style erasure, and instance erasure and compare the results\nwith current state-of-the-art methods. Our method achieves image quality\ncomparable to approaches focused on effectiveness, while also yielding results\nthat are on par with methods targeting completeness."
    },
    {
        "date": "2025-09",
        "title": "Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids",
        "author": "Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu, Shiqi Wang, Liam Edwards, Jessica Anz, Alex Xu, Fan Shi, Stelian Coros, and Dennis W. Hong",
        "link": "http://arxiv.org/abs/2509.05581v1",
        "abstract": "We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a\ncustom-built humanoid robot designed for entertainment applications. Unlike\ntraditional humanoids, entertainment robots present unique challenges due to\naesthetic-driven design choices. Cosmo embodies these with a disproportionately\nlarge head (16% of total mass), limited sensing, and protective shells that\nconsiderably restrict movement. To address these challenges, we apply\nAdversarial Motion Priors (AMP) to enable the robot to learn natural-looking\nmovements while maintaining physical stability. We develop tailored domain\nrandomization techniques and specialized reward structures to ensure safe\nsim-to-real, protecting valuable hardware components during deployment. Our\nexperiments demonstrate that AMP generates stable standing and walking\nbehaviors despite Cosmo's extreme mass distribution and movement constraints.\nThese results establish a promising direction for robots that balance aesthetic\nappeal with functional performance, suggesting that learning-based methods can\neffectively adapt to aesthetic-driven design constraints."
    },
    {
        "date": "2025-09",
        "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation",
        "author": "Yihong Leng, Siming Zheng, Jinwei Chen, Bo Li, Jiaojiao Li, and Peng-Tao Jiang",
        "link": "http://arxiv.org/abs/2509.05554v1",
        "abstract": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness."
    },
    {
        "date": "2025-09",
        "title": "Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications",
        "author": "Ali Arastehfard, Weiran Liu, Joshua Lee, Bingyu Liu, Xuegang Ban, and Yuan Hong",
        "link": "http://arxiv.org/abs/2509.05552v1",
        "abstract": "Secure norm computation is becoming increasingly important in many real-world\nlearning applications. However, existing cryptographic systems often lack a\ngeneral framework for securely computing the $L^p$-norm over private inputs\nheld by different parties. These systems often treat secure norm computation as\na black-box process, neglecting to design tailored cryptographic protocols that\noptimize performance. Moreover, they predominantly focus on the $L^2$-norm,\npaying little attention to other popular $L^p$-norms, such as $L^1$ and\n$L^\\infty$, which are commonly used in practice, such as machine learning tasks\nand location-based services.\n  To our best knowledge, we propose the first comprehensive framework for\nsecure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\\infty$),\ndenoted as \\mbox{Crypto-$L^p$}, designed to be versatile across various\napplications. We have designed, implemented, and thoroughly evaluated our\nframework across a wide range of benchmarking applications, state-of-the-art\n(SOTA) cryptographic protocols, and real-world datasets to validate its\neffectiveness and practical applicability. In summary, \\mbox{Crypto-$L^p$}\noutperforms prior works on secure $L^p$-norm computation, achieving $82\\times$,\n$271\\times$, and $42\\times$ improvements in runtime while reducing\ncommunication overhead by $36\\times$, $4\\times$, and $21\\times$ for $p=1$, $2$,\nand $\\infty$, respectively. Furthermore, we take the first step in adapting our\nCrypto-$L^p$ framework for secure machine learning inference, reducing\ncommunication costs by $3\\times$ compared to SOTA systems while maintaining\ncomparable runtime and accuracy."
    },
    {
        "date": "2025-09",
        "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis",
        "author": "Disha Makhija, Manoj Ghuhan Arivazhagan, Vinayshekhar Bannihatti Kumar, and Rashmi Gangadharaiah",
        "link": "http://arxiv.org/abs/2509.05449v1",
        "abstract": "Membership inference attacks (MIAs) reveal whether specific data was used to\ntrain machine learning models, serving as important tools for privacy auditing\nand compliance assessment. Recent studies have reported that MIAs perform only\nmarginally better than random guessing against large language models,\nsuggesting that modern pre-training approaches with massive datasets may be\nfree from privacy leakage risks. Our work offers a complementary perspective to\nthese findings by exploring how examining LLMs' internal representations,\nrather than just their outputs, may provide additional insights into potential\nmembership inference signals. Our framework, \\emph{memTrace}, follows what we\ncall \\enquote{neural breadcrumbs} extracting informative signals from\ntransformer hidden states and attention patterns as they process candidate\nsequences. By analyzing layer-wise representation dynamics, attention\ndistribution characteristics, and cross-layer transition patterns, we detect\npotential memorization fingerprints that traditional loss-based approaches may\nnot capture. This approach yields strong membership detection across several\nmodel families achieving average AUC scores of 0.85 on popular MIA benchmarks.\nOur findings suggest that internal model behaviors can reveal aspects of\ntraining data exposure even when output-based signals appear protected,\nhighlighting the need for further research into membership privacy and the\ndevelopment of more robust privacy-preserving training techniques for large\nlanguage models."
    },
    {
        "date": "2025-09",
        "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks",
        "author": "Jie Fu, Hong Yuan, Zhili Chen, and Wendy Hui Wang",
        "link": "http://arxiv.org/abs/2509.05429v2",
        "abstract": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is available at\nhttps://github.com/JeffffffFu/PGR."
    },
    {
        "date": "2025-09",
        "title": "On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy",
        "author": "Zijian Wang, Wei Tong, Tingxuan Han, Haoyu Chen, Tianling Zhang, Yunlong Mao, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2509.05265v1",
        "abstract": "Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack"
    },
    {
        "date": "2025-09",
        "title": "CURE: Controlled Unlearning for Robust Embeddings - Mitigating Conceptual Shortcuts in Pre-Trained Language Models",
        "author": "Aysenur Kocak, Shuo Yang, Bardh Prenkaj, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2509.05230v2",
        "abstract": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems."
    },
    {
        "date": "2025-09",
        "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers",
        "author": "Nariman Niknejad, Gokul S. Sankar, Bahare Kiumarsi, and Hamidreza Modares",
        "link": "http://arxiv.org/abs/2509.05201v1",
        "abstract": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance."
    },
    {
        "date": "2025-09",
        "title": "Reinforcing Secure Live Migration through Verifiable State Management",
        "author": "Stefanos Vasileaidis, Thanassis Giannetsos, Matthias Schunter, and Bruno Crispo",
        "link": "http://arxiv.org/abs/2509.05150v1",
        "abstract": "Live migration of applications is a fundamental capability for enabling\nresilient computing in modern distributed systems. However, extending this\nfunctionality to trusted applications (TA) -- executing within Trusted\nExecution Environments (TEEs) -- introduces unique challenges such as secure\nstate preservation, integrity verification, replay and rollback prevention, and\nmitigation of unauthorized cloning of TAs. We present TALOS, a lightweight\nframework for verifiable state management and trustworthy application\nmigration. While our implementation is prototyped and evaluated using Intel SGX\nwith the Gramine LibOS and RISC-V Keystone (evidencing the framework's\nportability across diverse TEEs), its design is agnostic to the underlying TEE\narchitecture. Such agility is a necessity in today's network service mesh\n(collaborative computing across the continuum) where application workloads must\nbe managed across domain boundaries in a harmonized fashion. TALOS is built\naround the principle of minimizing trust assumptions: TAs are treated as\nuntrusted until explicitly verified, and the migration process does not rely on\na trusted third party. To ensure both the integrity and secure launch of the\nmigrated application, TALOS integrates memory introspection and control-flow\ngraph extraction, enabling robust verification of state continuity and\nexecution flow. Thereby achieving strong security guarantees while maintaining\nefficiency, making it suitable for decentralized settings."
    },
    {
        "date": "2025-09",
        "title": "On the Learnability of Distribution Classes with Adaptive Adversaries",
        "author": "Tosca Lechner, Alex Bie, and Gautam Kamath",
        "link": "http://arxiv.org/abs/2509.05137v1",
        "abstract": "We consider the question of learnability of distribution classes in the\npresence of adaptive adversaries -- that is, adversaries capable of\nintercepting the samples requested by a learner and applying manipulations with\nfull knowledge of the samples before passing it on to the learner. This stands\nin contrast to oblivious adversaries, who can only modify the underlying\ndistribution the samples come from but not their i.i.d.\\ nature. We formulate a\ngeneral notion of learnability with respect to adaptive adversaries, taking\ninto account the budget of the adversary. We show that learnability with\nrespect to additive adaptive adversaries is a strictly stronger condition than\nlearnability with respect to additive oblivious adversaries."
    },
    {
        "date": "2025-09",
        "title": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers",
        "author": "Svetlana Pavlitska, Haixi Fan, Konstantin Ditschuneit, and J. Marius Z\u00f6llner",
        "link": "http://arxiv.org/abs/2509.05086v1",
        "abstract": "Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes."
    },
    {
        "date": "2025-09",
        "title": "Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework",
        "author": "Tianpei Zhang, Jufeng Zhao, Yiming Zhu, and Guangmang Cui",
        "link": "http://arxiv.org/abs/2509.05000v1",
        "abstract": "Most existing infrared-visible image fusion (IVIF) methods assume\nhigh-quality inputs, and therefore struggle to handle dual-source degraded\nscenarios, typically requiring manual selection and sequential application of\nmultiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion\npipeline inevitably leads to error accumulation and performance degradation. To\novercome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),\na novel framework that synergistically integrates vision-language models (VLMs)\nfor degradation perception with dual-domain (frequency/spatial) joint\noptimization. Concretely, the designed Guided Frequency Modality-Specific\nExtraction (GFMSE) module performs frequency-domain degradation perception and\nsuppression and discriminatively extracts fusion-relevant sub-band features.\nMeanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries\nout cross-modal degradation filtering and adaptive multi-source feature\naggregation in the spatial domain to enhance modality complementarity and\nstructural consistency. Extensive qualitative and quantitative experiments\ndemonstrate that GD^2Fusion achieves superior fusion performance compared with\nexisting algorithms and strategies in dual-source degraded scenarios. The code\nwill be publicly released after acceptance of this paper."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection",
        "author": "Sidahmed Benabderrahmane, and Talal Rahwan",
        "link": "http://arxiv.org/abs/2509.04999v1",
        "abstract": "Advanced Persistent Threats (APTs) present a considerable challenge to\ncybersecurity due to their stealthy, long-duration nature. Traditional\nsupervised learning methods typically require large amounts of labeled data,\nwhich is often scarce in real-world scenarios. This paper introduces a novel\napproach that combines AutoEncoders for anomaly detection with active learning\nto iteratively enhance APT detection. By selectively querying an oracle for\nlabels on uncertain or ambiguous samples, our method reduces labeling costs\nwhile improving detection accuracy, enabling the model to effectively learn\nwith minimal data and reduce reliance on extensive manual labeling. We present\na comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based\nanomaly detection framework and demonstrate how the active learning loop\nprogressively enhances the model's performance. The framework is evaluated on\nreal-world, imbalanced provenance trace data from the DARPA Transparent\nComputing program, where APT-like attacks account for just 0.004\\% of the data.\nThe datasets, which cover multiple operating systems including Android, Linux,\nBSD, and Windows, are tested in two attack scenarios. The results show\nsubstantial improvements in detection rates during active learning,\noutperforming existing methods."
    },
    {
        "date": "2025-09",
        "title": "MAIA: An Inpainting-Based Approach for Music Adversarial Attacks",
        "author": "Yuxuan Liu, Peihong Zhang, Rui Sang, Zhixin Li, and Shengchen Li",
        "link": "http://arxiv.org/abs/2509.04980v1",
        "abstract": "Music adversarial attacks have garnered significant interest in the field of\nMusic Information Retrieval (MIR). In this paper, we present Music Adversarial\nInpainting Attack (MAIA), a novel adversarial attack framework that supports\nboth white-box and black-box attack scenarios. MAIA begins with an importance\nanalysis to identify critical audio segments, which are then targeted for\nmodification. Utilizing generative inpainting models, these segments are\nreconstructed with guidance from the output of the attacked model, ensuring\nsubtle and effective adversarial perturbations. We evaluate MAIA on multiple\nMIR tasks, demonstrating high attack success rates in both white-box and\nblack-box settings while maintaining minimal perceptual distortion.\nAdditionally, subjective listening tests confirm the high audio fidelity of the\nadversarial samples. Our findings highlight vulnerabilities in current MIR\nsystems and emphasize the need for more robust and secure models."
    },
    {
        "date": "2025-09",
        "title": "RobQFL: Robust Quantum Federated Learning in Adversarial Environment",
        "author": "Walid El Maouaki, Nouhaila Innan, Alberto Marchisio, Taoufik Said, Muhammad Shafique, and Mohamed Bennai",
        "link": "http://arxiv.org/abs/2509.04914v1",
        "abstract": "Quantum Federated Learning (QFL) merges privacy-preserving federation with\nquantum computing gains, yet its resilience to adversarial noise is unknown. We\nfirst show that QFL is as fragile as centralized quantum learning. We propose\nRobust Quantum Federated Learning (RobQFL), embedding adversarial training\ndirectly into the federated loop. RobQFL exposes tunable axes: client coverage\n$\\gamma$ (0-100\\%), perturbation scheduling (fixed-$\\varepsilon$ vs\n$\\varepsilon$-mixes), and optimization (fine-tune vs scratch), and distils the\nresulting $\\gamma \\times \\varepsilon$ surface into two metrics:\nAccuracy-Robustness Area and Robustness Volume. On 15-client simulations with\nMNIST and Fashion-MNIST, IID and Non-IID conditions, training only 20-50\\%\nclients adversarially boosts $\\varepsilon \\leq 0.1$ accuracy $\\sim$15 pp at $<\n2$ pp clean-accuracy cost; fine-tuning adds 3-5 pp. With $\\geq$75\\% coverage, a\nmoderate $\\varepsilon$-mix is optimal, while high-$\\varepsilon$ schedules help\nonly at 100\\% coverage. Label-sorted non-IID splits halve robustness,\nunderscoring data heterogeneity as a dominant risk."
    },
    {
        "date": "2025-09",
        "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing",
        "author": "Hongyi Jing, Jiafu Chen, Chen Rao, Ziqiang Dang, Jiajie Teng, Tianyi Chu, Juncheng Mo, Shuo Fang, Huaizhong Lin, Rui Lv, Chenguang Ma, and Lei Zhao",
        "link": "http://arxiv.org/abs/2509.04908v1",
        "abstract": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser."
    },
    {
        "date": "2025-09",
        "title": "Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images",
        "author": "Yuanyuan Gui, Wei Li, Yinjian Wang, Xiang-Gen Xia, Mauro Marty, Christian Ginzler, and Zuyuan Wang",
        "link": "http://arxiv.org/abs/2509.04870v1",
        "abstract": "Recent advances in semantic segmentation of multi-modal remote sensing images\nhave significantly improved the accuracy of tree cover mapping, supporting\napplications in urban planning, forest monitoring, and ecological assessment.\nIntegrating data from multiple modalities-such as optical imagery, light\ndetection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown\nsuperior performance over single-modality methods. However, these data are\noften acquired days or even months apart, during which various changes may\noccur, such as vegetation disturbances (e.g., logging, and wildfires) and\nvariations in imaging quality. Such temporal misalignments introduce\ncross-modal uncertainty, especially in high-resolution imagery, which can\nseverely degrade segmentation accuracy. To address this challenge, we propose\nMURTreeFormer, a novel multi-modal segmentation framework that mitigates and\nleverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer\ntreats one modality as primary and others as auxiliary, explicitly modeling\npatch-level uncertainty in the auxiliary modalities via a probabilistic latent\nrepresentation. Uncertain patches are identified and reconstructed from the\nprimary modality's distribution through a VAE-based resampling mechanism,\nproducing enhanced auxiliary features for fusion. In the decoder, a gradient\nmagnitude attention (GMA) module and a lightweight refinement head (RH) are\nfurther integrated to guide attention toward tree-like structures and to\npreserve fine-grained spatial details. Extensive experiments on multi-modal\ndatasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly\nimproves segmentation performance and effectively reduces the impact of\ntemporally induced aleatoric uncertainty."
    },
    {
        "date": "2025-09",
        "title": "Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition",
        "author": "Erik Rye, Dave Levin, and Robert Beverly",
        "link": "http://arxiv.org/abs/2509.04792v1",
        "abstract": "IPv4 NAT has limited the spread of IoT botnets considerably by\ndefault-denying bots' incoming connection requests to in-home devices unless\nthe owner has explicitly allowed them. As the Internet transitions to majority\nIPv6, however, residential connections no longer require the use of NAT. This\npaper therefore asks: has the transition from IPv4 to IPv6 ultimately made\nresidential networks more vulnerable to attack, thereby empowering the next\ngeneration of IPv6-based IoT botnets? To answer this question, we introduce a\nlarge-scale IPv6 scanning methodology that, unlike those that rely on AI, can\nbe run on low-resource devices common in IoT botnets. We use this methodology\nto perform the largest-scale measurement of IPv6 residential networks to date,\nand compare which devices are publicly accessible to comparable IPv4 networks.\nWe were able to receive responses from 14.0M distinct IPv6 addresses inside of\nresidential networks (i.e., not the external-facing gateway), in 2,436 ASes\nacross 118 countries. These responses come from protocols commonly exploited by\nIoT botnets (including telnet and FTP), as well as protocols typically\nassociated with end-user devices (including iPhone-Sync and IPP). Comparing to\nIPv4, we show that we are able to reach more printers, iPhones, and smart\nlights over IPv6 than full IPv4-wide scans could. Collectively, our results\nshow that NAT has indeed acted as the de facto firewall of the Internet, and\nthe v4-to-v6 transition of residential networks is opening up new devices to\nattack."
    },
    {
        "date": "2025-09",
        "title": "Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs",
        "author": "Brennen Hill, Surendra Parla, Venkata Abhijeeth Balabhadruni, Atharv Prajod Padmalayam, and Sujay Chandra Shekara Sharma",
        "link": "http://arxiv.org/abs/2509.04615v1",
        "abstract": "The proliferation of Large Language Models (LLMs) has introduced critical\nsecurity challenges, where adversarial actors can manipulate input prompts to\ncause significant harm and circumvent safety alignments. These prompt-based\nattacks exploit vulnerabilities in a model's design, training, and contextual\nunderstanding, leading to intellectual property theft, misinformation\ngeneration, and erosion of user trust. A systematic understanding of these\nattack vectors is the foundational step toward developing robust\ncountermeasures. This paper presents a comprehensive literature survey of\nprompt-based attack methodologies, categorizing them to provide a clear threat\nmodel. By detailing the mechanisms and impacts of these exploits, this survey\naims to inform the research community's efforts in building the next generation\nof secure LLMs that are inherently resistant to unauthorized distillation,\nfine-tuning, and editing."
    },
    {
        "date": "2025-09",
        "title": "DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models",
        "author": "Jin Ma, Mohammed Aldeen, Christopher Salas, Feng Luo, Mashrur Chowdhury, Mert Pes\u00e9, and Long Cheng",
        "link": "http://arxiv.org/abs/2509.04597v1",
        "abstract": "Object detection is fundamental to various real-world applications, such as\nsecurity monitoring and surveillance video analysis. Despite their\nadvancements, state-of-theart object detectors are still vulnerable to\nadversarial patch attacks, which can be easily applied to real-world objects to\neither conceal actual items or create non-existent ones, leading to severe\nconsequences. Given the current diversity of adversarial patch attacks and\npotential unknown threats, an ideal defense method should be effective,\ngeneralizable, and robust against adaptive attacks. In this work, we introduce\nDISPATCH, the first diffusion-based defense framework for object detection.\nUnlike previous works that aim to \"detect and remove\" adversarial patches,\nDISPATCH adopts a \"regenerate and rectify\" strategy, leveraging generative\nmodels to disarm attack effects while preserving the integrity of the input\nimage. Specifically, we utilize the in-distribution generative power of\ndiffusion models to regenerate the entire image, aligning it with benign data.\nA rectification process is then employed to identify and replace adversarial\nregions with their regenerated benign counterparts. DISPATCH is attack-agnostic\nand requires no prior knowledge of the existing patches. Extensive experiments\nacross multiple detectors and attacks demonstrate that DISPATCH consistently\noutperforms state-of-the-art defenses on both hiding attacks and creating\nattacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and\nlowering the attack success rate to 24.8% on untargeted creating attacks.\nMoreover, it maintains strong robustness against adaptive attacks, making it a\npractical and reliable defense for object detection systems."
    },
    {
        "date": "2025-09",
        "title": "Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions",
        "author": "Faruk Alpay, and Taylan Alpay",
        "link": "http://arxiv.org/abs/2509.04549v1",
        "abstract": "Transformer-based language models excel in NLP tasks, but fine-grained\ncontrol remains challenging. This paper explores methods for manipulating\ntransformer models through principled interventions at three levels: prompts,\nactivations, and weights. We formalize controllable text generation as an\noptimization problem addressable via prompt engineering, parameter-efficient\nfine-tuning, model editing, and reinforcement learning. We introduce a unified\nframework encompassing prompt-level steering, activation interventions, and\nweight-space edits. We analyze robustness and safety implications, including\nadversarial attacks and alignment mitigations. Theoretically, we show minimal\nweight updates can achieve targeted behavior changes with limited side-effects.\nEmpirically, we demonstrate >90% success in sentiment control and factual edits\nwhile preserving base performance, though generalization-specificity trade-offs\nexist. We discuss ethical dual-use risks and the need for rigorous evaluation.\nThis work lays groundwork for designing controllable and robust language\nmodels."
    },
    {
        "date": "2025-09",
        "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation",
        "author": "Jiajun He, Naoki Sawada, Koichi Miyazaki, and Tomoki Toda",
        "link": "http://arxiv.org/abs/2509.04357v1",
        "abstract": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech."
    },
    {
        "date": "2025-09",
        "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes",
        "author": "Isidoro Tamassia, and Wendelin B\u00f6hmer",
        "link": "http://arxiv.org/abs/2509.04317v1",
        "abstract": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub."
    },
    {
        "date": "2025-09",
        "title": "A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis",
        "author": "Charles Jones, and Ben Glocker",
        "link": "http://arxiv.org/abs/2509.04295v1",
        "abstract": "Machine learning methods often fail when deployed in the real world. Worse\nstill, they fail in high-stakes situations and across socially sensitive lines.\nThese issues have a chilling effect on the adoption of machine learning methods\nin settings such as medical diagnosis, where they are arguably best-placed to\nprovide benefits if safely deployed. In this primer, we introduce the causal\nand statistical structures which induce failure in machine learning methods for\nimage analysis. We highlight two previously overlooked problems, which we call\nthe \\textit{no fair lunch} problem and the \\textit{subgroup separability}\nproblem. We elucidate why today's fair representation learning methods fail to\nadequately solve them and propose potential paths forward for the field."
    },
    {
        "date": "2025-09",
        "title": "Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification",
        "author": "Rudi Coppola, Hovsep Touloujian, Pierfrancesco Ombrini, and Manuel Mazo Jr",
        "link": "http://arxiv.org/abs/2509.04288v2",
        "abstract": "Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell."
    },
    {
        "date": "2025-09",
        "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its Implications Across Security Tasks",
        "author": "Jintao Gu, Haolang Lu, Guoshun Nan, Yihan Lin, Kun Wang, Yuchun Guo, Yigui Cao, and Yang Liu",
        "link": "http://arxiv.org/abs/2509.04091v2",
        "abstract": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
    },
    {
        "date": "2025-09",
        "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
        "author": "Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, and Kevin Roitero",
        "link": "http://arxiv.org/abs/2509.04013v1",
        "abstract": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios."
    },
    {
        "date": "2025-09",
        "title": "Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case",
        "author": "Delphine Longuet, Amira Elouazzani, Alejandro Penacho Riveiros, and Nicola Bastianello",
        "link": "http://arxiv.org/abs/2509.03948v1",
        "abstract": "Failures in satellite components are costly and challenging to address, often\nrequiring significant human and material resources. Embedding a hybrid AI-based\nsystem for fault detection directly in the satellite can greatly reduce this\nburden by allowing earlier detection. However, such systems must operate with\nextremely high reliability. To ensure this level of dependability, we employ\nthe formal verification tool Marabou to verify the local robustness of the\nneural network models used in the AI-based algorithm. This tool allows us to\nquantify how much a model's input can be perturbed before its output behavior\nbecomes unstable, thereby improving trustworthiness with respect to its\nperformance under uncertainty."
    },
    {
        "date": "2025-09",
        "title": "LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding",
        "author": "Yifan Jia, Yanbin Wang, Jianguo Sun, Ye Tian, and Peng Qian",
        "link": "http://arxiv.org/abs/2509.03939v1",
        "abstract": "Current Ethereum fraud detection methods rely on context-independent,\nnumerical transaction sequences, failing to capture semantic of account\ntransactions. Furthermore, the pervasive homogeneity in Ethereum transaction\nrecords renders it challenging to learn discriminative account embeddings.\nMoreover, current self-supervised graph learning methods primarily learn node\nrepresentations through graph reconstruction, resulting in suboptimal\nperformance for node-level tasks like fraud account detection, while these\nmethods also encounter scalability challenges. To tackle these challenges, we\npropose LMAE4Eth, a multi-view learning framework that fuses transaction\nsemantics, masked graph embedding, and expert knowledge. We first propose a\ntransaction-token contrastive language model (TxCLM) that transforms\ncontext-independent numerical transaction records into logically cohesive\nlinguistic representations. To clearly characterize the semantic differences\nbetween accounts, we also use a token-aware contrastive learning pre-training\nobjective together with the masked transaction model pre-training objective,\nlearns high-expressive account representations. We then propose a masked\naccount graph autoencoder (MAGAE) using generative self-supervised learning,\nwhich achieves superior node-level account detection by focusing on\nreconstructing account node features. To enable MAGAE to scale for large-scale\ntraining, we propose to integrate layer-neighbor sampling into the graph, which\nreduces the number of sampled vertices by several times without compromising\ntraining quality. Finally, using a cross-attention fusion network, we unify the\nembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our\nmethod against 21 baseline approaches on three datasets. Experimental results\nshow that our method outperforms the best baseline by over 10% in F1-score on\ntwo of the datasets."
    },
    {
        "date": "2025-09",
        "title": "A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks",
        "author": "Umair Amjid, M. Umar Khan, and S. A. Manan Kirmani",
        "link": "http://arxiv.org/abs/2509.05366v1",
        "abstract": "The increasing use of Internet of Things (IoT) devices has led to a rise in\nsecurity related concerns regarding IoT Networks. The surveillance cameras in\nIoT networks are vulnerable to security threats such as brute force and\nzero-day attacks which can lead to unauthorized access by hackers and potential\nspying on the users activities. Moreover, these cameras can be targeted by\nDenial of Service (DOS) attacks, which will make it unavailable for the user.\nThe proposed AI based framework will leverage machine learning algorithms to\nanalyze network traffic and detect anomalous behavior, allowing for quick\ndetection and response to potential intrusions. The framework will be trained\nand evaluated using real-world datasets to learn from past security incidents\nand improve its ability to detect potential intrusion."
    },
    {
        "date": "2025-09",
        "title": "ShieldMMU: Detecting and Defending against Controlled-Channel Attacks in Shielding Memory System",
        "author": "Gang Liu, Ningjie Li, and Cen Chen",
        "link": "http://arxiv.org/abs/2509.03879v1",
        "abstract": "Intel SGX and hypervisors isolate non-privileged programs from other\nsoftware, ensuring confidentiality and integrity. However, side-channel attacks\ncontinue to threaten Intel SGX's security, enabling malicious OS to manipulate\nPTE present bits, induce page faults, and steal memory access traces. Despite\nextensive research, existing defenses focus on detection or rely on impractical\nsolutions. This paper presents ShieldMMU, a comprehensive solution for\nmitigating controlled channel attacks, balancing compatibility, performance,\nand usability. Leveraging a Merkle Tree-inspired Defense Tree (DD-Tree),\nShieldMMU protects PTE integrity by detecting, locating, and restoring attacked\nPTEs. It identifies MMU page table lookup events and side-channel attacks,\npromptly restoring PTE parameters to prevent page fault traps and ensure secure\nnon-privileged application operation within SGX. Our experiments confirm\nShieldMMU's enhanced security and acceptable latency performance."
    },
    {
        "date": "2025-09",
        "title": "Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations",
        "author": "Hao Nie, Wei Wang, Peng Xu, Wei Chen, Laurence T. Yang, Mauro Conti, and Kaitai Liang",
        "link": "http://arxiv.org/abs/2509.03806v1",
        "abstract": "Dynamic Searchable Symmetric Encryption (DSSE) allows secure searches over a\ndynamic encrypted database but suffers from inherent information leakage.\nExisting passive attacks against DSSE rely on persistent leakage monitoring to\ninfer leakage patterns, whereas this work targets intermittent observation - a\nmore practical threat model. We propose Peekaboo - a new universal attack\nframework - and the core design relies on inferring the search pattern and\nfurther combining it with auxiliary knowledge and other leakage. We instantiate\nPeekaboo over the SOTA attacks, Sap (USENIX' 21) and Jigsaw (USENIX' 24), to\nderive their \"+\" variants (Sap+ and Jigsaw+). Extensive experiments demonstrate\nthat our design achieves >0.9 adjusted rand index for search pattern recovery\nand 90% query accuracy vs. FMA's 30% (CCS' 23). Peekaboo's accuracy scales with\nobservation rounds and the number of observed queries but also it resists SOTA\ncountermeasures, with >40% accuracy against file size padding and >80% against\nobfuscation."
    },
    {
        "date": "2025-09",
        "title": "Learning an Adversarial World Model for Automated Curriculum Generation in MARL",
        "author": "Brennen Hill",
        "link": "http://arxiv.org/abs/2509.03771v1",
        "abstract": "World models that infer and predict environmental dynamics are foundational\nto embodied intelligence. However, their potential is often limited by the\nfinite complexity and implicit biases of hand-crafted training environments. To\ndevelop truly generalizable and robust agents, we need environments that scale\nin complexity alongside the agents learning within them. In this work, we\nreframe the challenge of environment generation as the problem of learning a\ngoal-conditioned, generative world model. We propose a system where a\ngenerative **Attacker** agent learns an implicit world model to synthesize\nincreasingly difficult challenges for a team of cooperative **Defender**\nagents. The Attacker's objective is not passive prediction, but active,\ngoal-driven interaction: it models and generates world states (i.e.,\nconfigurations of enemy units) specifically to exploit the Defenders'\nweaknesses. Concurrently, the embodied Defender team learns a cooperative\npolicy to overcome these generated worlds. This co-evolutionary dynamic creates\na self-scaling curriculum where the world model continuously adapts to\nchallenge the decision-making policy of the agents, providing an effectively\ninfinite stream of novel and relevant training scenarios. We demonstrate that\nthis framework leads to the emergence of complex behaviors, such as the world\nmodel learning to generate flanking and shielding formations, and the defenders\nlearning coordinated focus-fire and spreading tactics. Our findings position\nadversarial co-evolution as a powerful method for learning instrumental world\nmodels that drive agents toward greater strategic depth and robustness."
    },
    {
        "date": "2025-09",
        "title": "Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning",
        "author": "Duy A. Nguyen, Abhi Kamboj, and Minh N. Do",
        "link": "http://arxiv.org/abs/2509.03477v1",
        "abstract": "Addressing missing modalities and limited labeled data is crucial for\nadvancing robust multimodal learning. We propose Robult, a scalable framework\ndesigned to mitigate these challenges by preserving modality-specific\ninformation and leveraging redundancy through a novel information-theoretic\napproach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled\n(PU) contrastive loss that maximizes task-relevant feature alignment while\neffectively utilizing limited labeled data in semi-supervised settings, and (2)\na latent reconstruction loss that ensures unique modality-specific information\nis retained. These strategies, embedded within a modular design, enhance\nperformance across various downstream tasks and ensure resilience to incomplete\nmodalities during inference. Experimental results across diverse datasets\nvalidate that Robult achieves superior performance over existing approaches in\nboth semi-supervised learning and missing modality contexts. Furthermore, its\nlightweight design promotes scalability and seamless integration with existing\narchitectures, making it suitable for real-world multimodal applications."
    },
    {
        "date": "2025-09",
        "title": "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information",
        "author": "Somiya Chhillar, Mary K. Righi, Rebecca E. Sutter, and Evgenios M. Kornaropoulos",
        "link": "http://arxiv.org/abs/2509.03350v1",
        "abstract": "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns."
    },
    {
        "date": "2025-09",
        "title": "Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions",
        "author": "Xizhe Zhang, and Jiayang Zhu",
        "link": "http://arxiv.org/abs/2509.03323v1",
        "abstract": "Astrocytes are critical glial cells whose altered morphology and density are\nhallmarks of many neurological disorders. However, their intricate branching\nand stain dependent variability make automated detection of histological images\na highly challenging task. To address these challenges, we propose a hybrid CNN\nTransformer detector that combines local feature extraction with global\ncontextual reasoning. A heatmap guided query mechanism generates spatially\ngrounded anchors for small and faint astrocytes, while a lightweight\nTransformer module improves discrimination in dense clusters. Evaluated on\nALDH1L1 and GFAP stained astrocyte datasets, the model consistently\noutperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with\nfewer false positives, as confirmed by FROC analysis. These results highlight\nthe potential of hybrid CNN Transformer architectures for robust astrocyte\ndetection and provide a foundation for advanced computational pathology tools."
    },
    {
        "date": "2025-09",
        "title": "Evaluating Security Properties in the Execution of Quantum Circuits",
        "author": "Paolo Bernardi, Antonio Brogi, Gian-Luigi Ferrari, and Giuseppe Bisicchia",
        "link": "http://arxiv.org/abs/2509.03306v1",
        "abstract": "Quantum computing is a disruptive technology that is expected to offer\nsignificant advantages in many critical fields (e.g. drug discovery and\ncryptography). The security of information processed by such machines is\ntherefore paramount. Currently, modest Noisy Intermediate-Scale Quantum (NISQ)\ndevices are available. The goal of this work is to identify a practical,\nheuristic methodology to evaluate security properties, such as secrecy and\nintegrity, while using quantum processors owned by potentially untrustworthy\nproviders."
    },
    {
        "date": "2025-09",
        "title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking",
        "author": "Jing Zhang, Siying Tao, Jiao Li, Tianhe Wang, Junchen Wu, Ruqian Hao, Xiaohui Du, Ruirong Tan, and Rui Li",
        "link": "http://arxiv.org/abs/2509.03221v1",
        "abstract": "Organoids replicate organ structure and function, playing a crucial role in\nfields such as tumor treatment and drug screening. Their shape and size can\nindicate their developmental status, but traditional fluorescence labeling\nmethods risk compromising their structure. Therefore, this paper proposes an\nautomated, non-destructive approach to organoid segmentation and tracking. We\nintroduced the LGBP-OrgaNet, a deep learning-based system proficient in\naccurately segmenting, tracking, and quantifying organoids. The model leverages\ncomplementary information extracted from CNN and Transformer modules and\nintroduces the innovative feature fusion module, Learnable Gaussian Band Pass\nFusion, to merge data from two branches. Additionally, in the decoder, the\nmodel proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,\nand finally completes the decoding through progressive concatenation and\nupsampling. SROrga demonstrates satisfactory segmentation accuracy and\nrobustness on organoids segmentation datasets, providing a potent tool for\norganoid research."
    },
    {
        "date": "2025-09",
        "title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images",
        "author": "Hania Ghouse, and Muzammil Behzad",
        "link": "http://arxiv.org/abs/2509.03188v1",
        "abstract": "Segmentation of small and irregularly shaped abdominal organs, such as the\nadrenal glands in CT imaging, remains a persistent challenge due to severe\nclass imbalance, poor spatial context, and limited annotated data. In this\nwork, we propose a unified framework that combines variational reconstruction,\nsupervised segmentation, and adversarial patch-based feedback to address these\nlimitations in a principled and scalable manner. Our architecture is built upon\na VAE-UNet backbone that jointly reconstructs input patches and generates\nvoxel-level segmentation masks, allowing the model to learn disentangled\nrepresentations of anatomical structure and appearance. We introduce a\npatch-based training pipeline that selectively injects synthetic patches\ngenerated from the learned latent space, and systematically study the effects\nof varying synthetic-to-real patch ratios during training. To further enhance\noutput fidelity, the framework incorporates perceptual reconstruction loss\nusing VGG features, as well as a PatchGAN-style discriminator for adversarial\nsupervision over spatial realism. Comprehensive experiments on the BTCV dataset\ndemonstrate that our approach improves segmentation accuracy, particularly in\nboundary-sensitive regions, while maintaining strong reconstruction quality.\nOur findings highlight the effectiveness of hybrid generative-discriminative\ntraining regimes for small-organ segmentation and provide new insights into\nbalancing realism, diversity, and anatomical consistency in data-scarce\nscenarios."
    },
    {
        "date": "2025-09",
        "title": "AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain",
        "author": "Alma M. Liezenga, Stefan Wijnja, Puck de Haan, Niels W. T. Brink, Jip J. van Stijn, Yori Kamphuis, and Klamer Schutte",
        "link": "http://arxiv.org/abs/2509.03179v1",
        "abstract": "Poisoning attacks pose an increasing threat to the security and robustness of\nArtificial Intelligence systems in the military domain. The widespread use of\nopen-source datasets and pretrained models exacerbates this risk. Despite the\nseverity of this threat, there is limited research on the application and\ndetection of poisoning attacks on object detection systems. This is especially\nproblematic in the military domain, where attacks can have grave consequences.\nIn this work, we both investigate the effect of poisoning attacks on military\nobject detectors in practice, and the best approach to detect these attacks. To\nsupport this research, we create a small, custom dataset featuring military\nvehicles: MilCivVeh. We explore the vulnerability of military object detectors\nfor poisoning attacks by implementing a modified version of the BadDet attack:\na patch-based poisoning attack. We then assess its impact, finding that while a\npositive attack success rate is achievable, it requires a substantial portion\nof the data to be poisoned -- raising questions about its practical\napplicability. To address the detection challenge, we test both specialized\npoisoning detection methods and anomaly detection methods from the visual\nindustrial inspection domain. Since our research shows that both classes of\nmethods are lacking, we introduce our own patch detection method: AutoDetect, a\nsimple, fast, and lightweight autoencoder-based method. Our method shows\npromising results in separating clean from poisoned samples using the\nreconstruction error of image slices, outperforming existing methods, while\nbeing less time- and memory-intensive. We urge that the availability of large,\nrepresentative datasets in the military domain is a prerequisite to further\nevaluate risks of poisoning attacks and opportunities patch detection."
    },
    {
        "date": "2025-09",
        "title": "From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models",
        "author": "Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Xiaoling Wang, and Linlin Wang",
        "link": "http://arxiv.org/abs/2509.03122v1",
        "abstract": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs."
    },
    {
        "date": "2025-09",
        "title": "Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods",
        "author": "Shota Iwamatsu, Koichi Ito, and Takafumi Aoki",
        "link": "http://arxiv.org/abs/2509.03108v1",
        "abstract": "Face recognition systems are robust against environmental changes and noise,\nand thus may be vulnerable to illegal authentication attempts using user face\nphotos, such as spoofing attacks. To prevent such spoofing attacks, it is\ncrucial to discriminate whether the input image is a live user image or a\nspoofed image prior to the face recognition process. Most existing spoofing\nattack detection methods utilize deep learning, which necessitates a\nsubstantial amount of training data. Consequently, if malicious data is\ninjected into a portion of the training dataset, a specific spoofing attack may\nbe erroneously classified as live, leading to false positives.In this paper, we\npropose a novel backdoor poisoning attack method to demonstrate the latent\nthreat of backdoor poisoning within face anti-spoofing detection. The proposed\nmethod enables certain spoofing attacks to bypass detection by embedding\nfeatures extracted from the spoofing attack's face image into a live face image\nwithout inducing any perceptible visual alterations.Through experiments\nconducted on public datasets, we demonstrate that the proposed method\nconstitutes a realistic threat to existing spoofing attack detection systems."
    },
    {
        "date": "2025-09",
        "title": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint",
        "author": "Zhenhua Xu, Meng Han, and Wenpeng Xing",
        "link": "http://arxiv.org/abs/2509.03058v1",
        "abstract": "The proliferation of large language models (LLMs) has intensified concerns\nover model theft and license violations, necessitating robust and stealthy\nownership verification. Existing fingerprinting methods either require\nimpractical white-box access or introduce detectable statistical anomalies. We\npropose EverTracer, a novel gray-box fingerprinting framework that ensures\nstealthy and robust model provenance tracing. EverTracer is the first to\nrepurpose Membership Inference Attacks (MIAs) for defensive use, embedding\nownership signals via memorization instead of artificial trigger-output\noverfitting. It consists of Fingerprint Injection, which fine-tunes the model\non any natural language data without detectable artifacts, and Verification,\nwhich leverages calibrated probability variation signal to distinguish\nfingerprinted models. This approach remains robust against adaptive\nadversaries, including input level modification, and model-level modifications.\nExtensive experiments across architectures demonstrate EverTracer's\nstate-of-the-art effectiveness, stealthness, and resilience, establishing it as\na practical solution for securing LLM intellectual property. Our code and data\nare publicly available at https://github.com/Xuzhenhua55/EverTracer."
    },
    {
        "date": "2025-09",
        "title": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum",
        "author": "Shuzheng Wang, Yue Huang, Zhuoer Xu, Yuming Huang, and Jing Tang",
        "link": "http://arxiv.org/abs/2509.03037v1",
        "abstract": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability."
    },
    {
        "date": "2025-09",
        "title": "Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification",
        "author": "Kaicong Huang, Talha Azfar, Jack M. Reilly, Thomas Guggisberg, and Ruimin Ke",
        "link": "http://arxiv.org/abs/2509.03032v1",
        "abstract": "Person re-identification faces two core challenges: precisely locating the\nforeground target while suppressing background noise and extracting\nfine-grained features from the target region. Numerous visual-only approaches\naddress these issues by partitioning an image and applying attention modules,\nyet they rely on costly manual annotations and struggle with complex\nocclusions. Recent multimodal methods, motivated by CLIP, introduce semantic\ncues to guide visual understanding. However, they focus solely on foreground\ninformation, but overlook the potential value of background cues. Inspired by\nhuman perception, we argue that background semantics are as important as the\nforeground semantics in ReID, as humans tend to eliminate background\ndistractions while focusing on target appearance. Therefore, this paper\nproposes an end-to-end framework that jointly models foreground and background\ninformation within a dual-branch cross-modal feature extraction pipeline. To\nhelp the network distinguish between the two domains, we propose an\nintra-semantic alignment and inter-semantic adversarial learning strategy.\nSpecifically, we align visual and textual features that share the same\nsemantics across domains, while simultaneously penalizing similarity between\nforeground and background features to enhance the network's discriminative\npower. This strategy drives the model to actively suppress noisy background\nregions and enhance attention toward identity-relevant foreground cues.\nComprehensive experiments on two holistic and two occluded ReID benchmarks\ndemonstrate the effectiveness and generality of the proposed method, with\nresults that match or surpass those of current state-of-the-art approaches."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers",
        "author": "Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, and Jun-Cheng Chen",
        "link": "http://arxiv.org/abs/2509.03006v1",
        "abstract": "Recent studies on deep watermarking have predominantly focused on\nin-processing watermarking, which integrates the watermarking process into\nimage generation. However, post-processing watermarking, which embeds\nwatermarks after image generation, offers more flexibility. It can be applied\nto outputs from any generative model (e.g. GANs, diffusion models) without\nneeding access to the model's internal structure. It also allows users to embed\nunique watermarks into individual images. Therefore, this study focuses on\npost-processing watermarking and enhances its robustness by incorporating an\nensemble attack network during training. We construct various versions of\nattack networks using CNN and Transformer in both spatial and frequency domains\nto investigate how each combination influences the robustness of the\nwatermarking model. Our results demonstrate that combining a CNN-based attack\nnetwork in the spatial domain with a Transformer-based attack network in the\nfrequency domain yields the highest robustness in watermarking models.\nExtensive evaluation on the WAVES benchmark, using average bit accuracy as the\nmetric, demonstrates that our ensemble attack network significantly enhances\nthe robustness of baseline watermarking methods under various stress tests. In\nparticular, for the Regeneration Attack defined in WAVES, our method improves\nStegaStamp by 18.743%. The code is released\nat:https://github.com/aiiu-lab/DeepRobustWatermark."
    },
    {
        "date": "2025-09",
        "title": "FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models",
        "author": "Kun Zhai, Siheng Chen, Xingjun Ma, and Yu-Gang Jiang",
        "link": "http://arxiv.org/abs/2509.06992v1",
        "abstract": "Federated Prompt Tuning (FPT) is an efficient method for cross-client\ncollaborative fine-tuning of large Vision-Language Models (VLMs). However,\nmodels tuned using FPT are vulnerable to adversarial attacks, leading to\nmisclassification in downstream tasks. In this work, we introduce Federated\nAdversarial Prompt Tuning (\\textbf{FedAPT}), a novel method designed to enhance\nthe adversarial robustness of FPT. We identify a key issue in FedAPT under\nnon-independent and identically distributed (non-IID) settings: a \\textit{class\ninformation gap} between clients and the global model. Clients rely solely on\nlimited local label information to generate adversarial samples for training,\nwhile the global model must defend against adversarial attacks from global\nlabels. To address this issue, we propose a \\textbf{class-aware prompt\ngenerator} that generates visual prompts from text prompts. This generator is\nguided by a \\emph{Global Label Embedding} (serving as a ``beacon\") which\nencodes cross-client label information to create more globally-aligned visual\nprompts. Additionally, we propose a \\textbf{cross-layer generator sharing}\nstrategy to enhance prompt coupling across different layers of the model,\nfurther boosting adversarial robustness. Extensive experiments on multiple\nimage classification datasets demonstrate the superiority of FedAPT in\nimproving adversarial robustness, outperforming existing methods by a large\nmargin. FedAPT also exhibits exceptional generalization in cross-domain and\ncross-dataset scenarios, indicating its effectiveness in real-world\napplications."
    },
    {
        "date": "2025-09",
        "title": "Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation",
        "author": "Kaoru Otsuka, Yuki Takezawa, and Makoto Yamada",
        "link": "http://arxiv.org/abs/2509.02970v1",
        "abstract": "Federated Learning (FL) allows distributed model training across multiple\nclients while preserving data privacy, but it remains vulnerable to Byzantine\nclients that exhibit malicious behavior. While existing Byzantine-robust FL\nmethods provide strong convergence guarantees (e.g., to a stationary point in\nexpectation) under Byzantine attacks, they typically assume full client\nparticipation, which is unrealistic due to communication constraints and client\navailability. Under partial participation, existing methods fail immediately\nafter the sampled clients contain a Byzantine majority, creating a fundamental\nchallenge for sparse communication. First, we introduce delayed momentum\naggregation, a novel principle where the server aggregates the most recently\nreceived gradients from non-participating clients alongside fresh momentum from\nactive clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with\nMomentum) implements this delayed momentum aggregation principle for\nByzantine-robust FL with partial participation. Then, we establish convergence\nguarantees that recover previous full participation results and match the\nfundamental lower bounds we prove for the partial participation setting.\nExperiments on deep learning tasks validated our theoretical findings, showing\nstable and robust training under various Byzantine attacks."
    },
    {
        "date": "2025-09",
        "title": "STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images",
        "author": "Zeyu Liu, and Shengwei Ding",
        "link": "http://arxiv.org/abs/2509.02952v1",
        "abstract": "Registration of serial whole-slide histopathological images (WSIs) is\ncritical for enabling direct comparison across diverse stains and for preparing\npaired datasets in artificial intelligence (AI) workflows such as virtual\nstaining and biomarker prediction. While existing methods often rely on complex\ndeformable or deep learning approaches that are computationally intensive and\ndifficult to reproduce, lightweight rigid frameworks-sufficient for many\nconsecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial\nTissue Alignment for Rigid registration), a fast and robust open-source\nframework for multi-WSI alignment. STAR integrates stain-conditioned\npreprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive\nkernel scaling, and built-in quality control, achieving reliable rigid\nregistration across heterogeneous tissue types and staining protocols,\nincluding hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,\nPASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).\nEvaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs\nand scanning conditions, STAR consistently produced stable alignments within\nminutes per slide, demonstrating robustness to cross-stain variability and\npartial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC\nalignment, construction of multi-IHC panels, and typical failure modes,\nunderscoring both utility and limitations. Released as an open and lightweight\ntool, STAR provides a reproducible baseline that lowers the barrier for\nclinical adoption and enables large-scale paired data preparation for\nnext-generation computational pathology."
    },
    {
        "date": "2025-09",
        "title": "A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation",
        "author": "Kesen Wang, Daulet Toibazar, and Pedro J. Moreno",
        "link": "http://arxiv.org/abs/2509.02864v1",
        "abstract": "We present an end-to-end, self-evolving adversarial workflow for long-context\nQuestion-Answer (QA) Generation in Arabic. By orchestrating multiple\nspecialized LVLMs: a question generator, an evaluator, and a swarm of answer\ngenerators, our system iteratively refines its own performance without any\nhuman intervention. Starting from raw, multi-page Arabic documents across\ndiverse domains, the question generator produces fine-grained, context-aware\nqueries to be tackled by the answer generator swarm, and the evaluator assesses\nand feeds back quality metrics. This closed-loop cycle enables continuous\nlearning: low-confidence outputs trigger automated re-generation and model\nupdates, progressively enhancing question difficulty and relevance. Moreover,\nwe set the quality metrics as a tunable hyperparameter, enabling question\ngeneration at controllable and customizable difficulty levels. We release\nAraLongBench, a large-scale Arabic benchmark of single- and multi-page\nchallenges spanning hundreds of pages, and demonstrate that our self-evolving\nworkflow substantially outperform static pipelines, markedly boosting the\nlong-context comprehension capabilities of leading Arabic Large Vision Language\nModels (LVLMs). Lastly, we also meticulously architect a fully automated\nagentic workflow for long-context Arabic document collection."
    },
    {
        "date": "2025-09",
        "title": "GPS Spoofing Attacks on Automated Frequency Coordination System in Wi-Fi 6E and Beyond",
        "author": "Yilu Dong, Tianchang Yang, Arupjyoti Bhuyan, and Syed Rafiul Hussain",
        "link": "http://arxiv.org/abs/2509.02824v1",
        "abstract": "The 6 GHz spectrum, recently opened for unlicensed use under Wi-Fi 6E and\nWi-Fi 7, overlaps with frequencies used by mission-critical incumbent systems\nsuch as public safety communications and utility infrastructure. To prevent\ninterference, the FCC mandates the use of Automated Frequency Coordination\n(AFC) systems, which assign safe frequency and power levels based on Wi-Fi\nAccess Point (AP)-reported locations. In this work, we demonstrate that\nGPS-based location reporting, which Wi-Fi APs use, can be spoofed using\ninexpensive, off-the-shelf radio equipment. This enables attackers to\nmanipulate AP behavior, gain unauthorized spectrum access, cause harmful\ninterference, or disable APs entirely by spoofing them into foreign locations.\nWe validate these attacks in a controlled lab setting against a commercial AP\nand evaluate a commercial AFC system under spoofed scenarios. Our findings\nhighlight critical gaps in the security assumptions of AFC and motivate the\nneed for stronger location integrity protections."
    },
    {
        "date": "2025-09",
        "title": "Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs",
        "author": "Naman Deep Singh, Maximilian M\u00fcller, Francesco Croce, and Matthias Hein",
        "link": "http://arxiv.org/abs/2509.02820v1",
        "abstract": "Unlearning in large language models (LLMs) involves precisely removing\nspecific information from a pre-trained model. This is crucial to ensure safety\nof LLMs by deleting private data or harmful knowledge acquired during\npre-training. However, existing unlearning methods often fall short when\nsubjected to thorough evaluation. To overcome this, we introduce JensUn, where\nwe leverage the Jensen-Shannon Divergence as the training objective for both\nforget and retain sets for more stable and effective unlearning dynamics\ncompared to commonly used loss functions. In extensive experiments, JensUn\nachieves better forget-utility trade-off than competing methods, and even\ndemonstrates strong resilience to benign relearning. Additionally, for a\nprecise unlearning evaluation, we introduce LKF, a curated dataset of\nlesser-known facts that provides a realistic unlearning scenario. Finally, to\ncomprehensively test unlearning methods, we propose (i) employing an LLM as\nsemantic judge instead of the standard ROUGE score, and (ii) using worst-case\nunlearning evaluation over various paraphrases and input formats. Our improved\nevaluation framework reveals that many existing methods are less effective than\npreviously thought."
    },
    {
        "date": "2025-09",
        "title": "Ensembling Membership Inference Attacks Against Tabular Generative Models",
        "author": "Joshua Ward, Yuxuan Yang, Chi-Hua Wang, and Guang Cheng",
        "link": "http://arxiv.org/abs/2509.05350v1",
        "abstract": "Membership Inference Attacks (MIAs) have emerged as a principled framework\nfor auditing the privacy of synthetic data generated by tabular generative\nmodels, where many diverse methods have been proposed that each exploit\ndifferent privacy leakage signals. However, in realistic threat scenarios, an\nadversary must choose a single method without a priori guarantee that it will\nbe the empirically highest performing option. We study this challenge as a\ndecision theoretic problem under uncertainty and conduct the largest synthetic\ndata privacy benchmark to date. Here, we find that no MIA constitutes a\nstrictly dominant strategy across a wide variety of model architectures and\ndataset domains under our threat model. Motivated by these findings, we propose\nensemble MIAs and show that unsupervised ensembles built on individual attacks\noffer empirically more robust, regret-minimizing strategies than individual\nattacks."
    },
    {
        "date": "2025-09",
        "title": "Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women",
        "author": "Gabriel A. B. do Nascimento, Vincent Dong, Guilherme J. Cavalcante, Alex Nguyen, Tha\u00eds G. do R\u00eago, Yuri Malheiros, Telmo M. Silva Filho, Carla R. Zeballos Torrez, James C. Gee, Anne Marie McCarthy, Andrew D. A. Maidment, and Bruno Barufaldi",
        "link": "http://arxiv.org/abs/2509.02710v1",
        "abstract": "Accurate breast MRI lesion detection is critical for early cancer diagnosis,\nespecially in high-risk populations. We present a classification pipeline that\nadapts a pretrained foundation model, the Medical Slice Transformer (MST), for\nbreast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI).\nLeveraging DINOv2-based self-supervised pretraining, MST generates robust\nper-slice feature embeddings, which are then used to train a Kolmogorov--Arnold\nNetwork (KAN) classifier. The KAN provides a flexible and interpretable\nalternative to conventional convolutional networks by enabling localized\nnonlinear transformations via adaptive B-spline activations. This enhances the\nmodel's ability to differentiate benign from malignant lesions in imbalanced\nand heterogeneous clinical datasets. Experimental results demonstrate that the\nMST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80\n\\pm 0.02 while preserving interpretability through attention-based heatmaps.\nOur findings highlight the effectiveness of combining foundation model\nembeddings with advanced classification strategies for building robust and\ngeneralizable breast MRI analysis tools."
    },
    {
        "date": "2025-09",
        "title": "Preference Robustness for DPO with Applications to Public Health",
        "author": "Cheol Woo Kim, Shresth Verma, Mauricio Tec, and Milind Tambe",
        "link": "http://arxiv.org/abs/2509.02709v1",
        "abstract": "We study an LLM fine-tuning task for designing reward functions for\nsequential resource allocation problems in public health, guided by human\npreferences expressed in natural language. This setting presents a challenging\ntestbed for alignment due to complex and ambiguous objectives and limited data\navailability. We propose DPO-PRO, a robust fine-tuning algorithm based on\nDirect Preference Optimization (DPO), which accounts for uncertainty in the\npreference distribution using a lightweight Distributionally Robust\nOptimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is\nsignificantly less conservative. We evaluate DPO-PRO on a real-world maternal\nmobile health program operated by the non-profit organization ARMMAN, as well\nas on standard alignment benchmarks. Experimental results demonstrate that our\nmethod consistently improves robustness to noisy preference signals compared to\nexisting DPO variants. Moreover, DPO-PRO achieves comparable performance to\nprior self-reflection-based baseline for reward function design, while\nrequiring significantly lower inference-time cost."
    },
    {
        "date": "2025-09",
        "title": "From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation",
        "author": "Tao Wang, Zhenxuan Zhang, Yuanbo Zhou, Xinlin Zhang, Yuanbin Chen, Tao Tan, Guang Yang, and Tong Tong",
        "link": "http://arxiv.org/abs/2509.02419v1",
        "abstract": "The effectiveness of convolutional neural networks in medical image\nsegmentation relies on large-scale, high-quality annotations, which are costly\nand time-consuming to obtain. Even expert-labeled datasets inevitably contain\nnoise arising from subjectivity and coarse delineations, which disrupt feature\nlearning and adversely impact model performance. To address these challenges,\nthis study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which\nintegrates geometric and structural cues to improve robustness against noisy\nannotations. It incorporates a Geometric Distance-Aware module that dynamically\nadjusts pixel-level weights using geometric features, thereby strengthening\nsupervision in reliable regions while suppressing noise. A Structure-Guided\nLabel Refinement module further refines labels with structural priors, and a\nKnowledge Transfer module enriches supervision and improves sensitivity to\nlocal details. To comprehensively assess its effectiveness, we evaluated\nGSD-Net on six publicly available datasets: four containing three types of\nsimulated label noise, and two with multi-expert annotations that reflect\nreal-world subjectivity and labeling inconsistencies. Experimental results\ndemonstrate that GSD-Net achieves state-of-the-art performance under noisy\nannotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,\n8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of\nthis study are available at https://github.com/ortonwang/GSD-Net."
    },
    {
        "date": "2025-09",
        "title": "A Survey: Towards Privacy and Security in Mobile Large Language Models",
        "author": "Honghui Xu, Kaiyang Li, Wei Chen, Danyang Zheng, Zhiyuan Li, and Zhipeng Cai",
        "link": "http://arxiv.org/abs/2509.02411v1",
        "abstract": "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems."
    },
    {
        "date": "2025-09",
        "title": "Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems",
        "author": "Rye Stahle-Smith, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2509.02387v1",
        "abstract": "The growing use of FPGAs in reconfigurable systems introducessecurity risks\nthrough malicious bitstreams that could cause denial-of-service (DoS), data\nleakage, or covert attacks. We investigated chip-level hardware malicious\npayload in embedded systems and proposed a supervised machine learning method\nto detect malicious bitstreams via static byte-level features. Our approach\ndiverges from existing methods by analyzing bitstreams directly at the binary\nlevel, enabling real-time detection without requiring access to source code or\nnetlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and\nre-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset\nincluded 122 samples of benign and malicious configurations. The data were\nvectorized using byte frequency analysis, compressed using TSVD, and balanced\nusing SMOTE to address class imbalance. The evaluated classifiers demonstrated\nthat Random Forest achieved a macro F1-score of 0.97, underscoring the\nviability of real-time Trojan detection on resource-constrained systems. The\nfinal model was serialized and successfully deployed via PYNQ to enable\nintegrated bitstream analysis."
    },
    {
        "date": "2025-09",
        "title": "Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers",
        "author": "Anuj Gautam, Tarun Yadav, Garrett Smith, Kent Seamons, and Scott Ruoti",
        "link": "http://arxiv.org/abs/2509.02289v1",
        "abstract": "Password managers provide significant security benefits to users. However,\nmalicious client-side scripts and browser extensions can steal passwords after\nthe manager has autofilled them into the web page. In this paper, we extend\nprior work by Stock and Johns, showing how password autofill can be hardened to\nprevent these local attacks. We implement our design in the Firefox browser and\nconduct experiments demonstrating that our defense successfully protects\npasswords from XSS attacks and malicious extensions. We also show that our\nimplementation is compatible with 97% of the Alexa top 1000 websites. Next, we\ngeneralize our design, creating a second defense that prevents recently\ndiscovered local attacks against the FIDO2 protocols. We implement this second\ndefense into Firefox, demonstrating that it protects the FIDO2 protocol against\nXSS attacks and malicious extensions. This defense is compatible with all\nwebsites, though it does require a small change (2-3 lines) to web servers\nimplementing FIDO2."
    },
    {
        "date": "2025-09",
        "title": "ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking",
        "author": "Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, and Bernard Ghanem",
        "link": "http://arxiv.org/abs/2509.02182v1",
        "abstract": "We introduce a novel tracklet-based dataset for benchmarking test-time\nadaptation (TTA) methods. The aim of this dataset is to mimic the intricate\nchallenges encountered in real-world environments such as images captured by\nhand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus\non how models face distribution shifts, when deployed, and on violations to the\ncustomary independent-and-identically-distributed (i.i.d.) assumption in\nmachine learning. Yet, these benchmarks fail to faithfully represent realistic\nscenarios that naturally display temporal dependencies, such as how consecutive\nframes from a video stream likely show the same object across time. We address\nthis shortcoming of current datasets by proposing a novel TTA benchmark we call\nthe \"Inherent Temporal Dependencies\" (ITD) dataset. We ensure the instances in\nITD naturally embody temporal dependencies by collecting them from\ntracklets-sequences of object-centric images we compile from the bounding boxes\nof an object-tracking dataset. We use ITD to conduct a thorough experimental\nanalysis of current TTA methods, and shed light on the limitations of these\nmethods when faced with the challenges of temporal dependencies. Moreover, we\nbuild upon these insights and propose a novel adversarial memory initialization\nstrategy to improve memory-based TTA methods. We find this strategy\nsubstantially boosts the performance of various methods on our challenging\nbenchmark."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety",
        "author": "Wenxiao Zhang, Xiangrui Kong, Conan Dewitt, Thomas Br\u00e4unl, and Jin B. Hong",
        "link": "http://arxiv.org/abs/2509.02163v1",
        "abstract": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/"
    },
    {
        "date": "2025-09",
        "title": "From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach",
        "author": "Refat Othman, Diaeddin Rimawi, Bruno Rossi, and Barbara Russo",
        "link": "http://arxiv.org/abs/2509.02077v2",
        "abstract": "In the domain of security, vulnerabilities frequently remain undetected even\nafter their exploitation. In this work, vulnerabilities refer to publicly\ndisclosed flaws documented in Common Vulnerabilities and Exposures (CVE)\nreports. Establishing a connection between attacks and vulnerabilities is\nessential for enabling timely incident response, as it provides defenders with\nimmediate, actionable insights. However, manually mapping attacks to CVEs is\ninfeasible, thereby motivating the need for automation. This paper evaluates 14\nstate-of-the-art (SOTA) sentence transformers for automatically identifying\nvulnerabilities from textual descriptions of attacks. Our results demonstrate\nthat the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior\nclassification performance when using attack Technique descriptions, with an\nF1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was\nobserved that, on average, 56% of the vulnerabilities identified by the MMPNet\nmodel are also represented within the CVE repository in conjunction with an\nattack, while 61% of the vulnerabilities detected by the model correspond to\nthose cataloged in the CVE repository. A manual inspection of the results\nrevealed the existence of 275 predicted links that were not documented in the\nMITRE repositories. Consequently, the automation of linking attack techniques\nto vulnerabilities not only enhances the detection and response capabilities\nrelated to software security incidents but also diminishes the duration during\nwhich vulnerabilities remain exploitable, thereby contributing to the\ndevelopment of more secure systems."
    },
    {
        "date": "2025-09",
        "title": "Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model",
        "author": "Kong Mun Yeen, Rafidah Md Noor, Wahidah Md Shah, Aslinda Hassan, and Muhammad Umair Munir",
        "link": "http://arxiv.org/abs/2509.02076v1",
        "abstract": "This paper forecasts future Distributed Denial of Service (DDoS) attacks\nusing deep learning models. Although several studies address forecasting DDoS\nattacks, they remain relatively limited compared to detection-focused research.\nBy studying the current trends and forecasting based on newer and updated\ndatasets, mitigation plans against the attacks can be planned and formulated.\nThe methodology used in this research work conforms to the Cross Industry\nStandard Process for Data Mining (CRISP-DM) model."
    },
    {
        "date": "2025-09",
        "title": "Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports",
        "author": "Jian Chen, Jiabao Dou, Jinbao Tian, Yunqi Xu, and Zhou Li",
        "link": "http://arxiv.org/abs/2509.02072v2",
        "abstract": "The automatic classification of occupational accident reports is a critical\nresearch area for enhancing workplace safety and enabling large-scale risk\nanalysis. However, the severe class imbalance inherent in these real-world\ndatasets often compromises the performance of analytical models, particularly\nfor rare but severe incident types, hindering the development of reliable\nautomated systems. To address this challenge, we propose ABEX-RAT, a novel and\nefficient framework that synergizes generative data augmentation with robust\nadversarial training. Our approach first employs a twostep\nabstractive-expansive (ABEX) pipeline, which leverages a large language model\nto distill core incident semantics and then uses a generative model to create\ndiverse, highquality synthetic samples for underrepresented classes.\nSubsequently, a lightweight classifier is trained on the augmented data using a\ncomputationally efficient random adversarial training (RAT) protocol, which\nstochastically applies perturbations to enhance model generalization and\nrobustness without significant overhead. Experimental results on the public\nOSHA dataset demonstrate that our method achieves new state-of-the-art\nperformance, reaching a macro-F1 score of 90.32% and significantly\noutperforming previous SOTA and fine-tuned large model baselines. Our work\nvalidates that this synergistic strategy is a highly effective and efficient\nalternative to brute-force fine-tuning for specialized, imbalanced\nclassification tasks. The code is publicly available\nat:https://github.com/nxcc-lab/ABEX-RAT."
    },
    {
        "date": "2025-09",
        "title": "Targeted Physical Evasion Attacks in the Near-Infrared Domain",
        "author": "Pascal Zimmer, Simon Lachnit, Alexander Jan Zielinski, and Ghassan Karame",
        "link": "http://arxiv.org/abs/2509.02042v1",
        "abstract": "A number of attacks rely on infrared light sources or heat-absorbing material\nto imperceptibly fool systems into misinterpreting visual input in various\nimage recognition applications. However, almost all existing approaches can\nonly mount untargeted attacks and require heavy optimizations due to the\nuse-case-specific constraints, such as location and shape. In this paper, we\npropose a novel, stealthy, and cost-effective attack to generate both targeted\nand untargeted adversarial infrared perturbations. By projecting perturbations\nfrom a transparent film onto the target object with an off-the-shelf infrared\nflashlight, our approach is the first to reliably mount laser-free targeted\nattacks in the infrared domain. Extensive experiments on traffic signs in the\ndigital and physical domains show that our approach is robust and yields higher\nattack success rates in various attack scenarios across bright lighting\nconditions, distances, and angles compared to prior work. Equally important,\nour attack is highly cost-effective, requiring less than US\\$50 and a few tens\nof seconds for deployment. Finally, we propose a novel segmentation-based\ndetection that thwarts our attack with an F1-score of up to 99%."
    },
    {
        "date": "2025-09",
        "title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems",
        "author": "Halima Bouzidi, Haoyu Liu, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.02028v2",
        "abstract": "Language-vision understanding has driven the development of advanced\nperception systems, most notably the emerging paradigm of Referring\nMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOT\nsystems can selectively track objects that satisfy a given semantic\ndescription, guided through Transformer-based spatial-temporal reasoning\nmodules. End-to-End (E2E) RMOT models further unify feature extraction,\ntemporal memory, and spatial reasoning within a Transformer backbone, enabling\nlong-range spatial-temporal modeling over fused textual-visual representations.\nDespite these advances, the reliability and robustness of RMOT remain\nunderexplored. In this paper, we examine the security implications of RMOT\nsystems from a design-logic perspective, identifying adversarial\nvulnerabilities that compromise both the linguistic-visual referring and\ntrack-object matching components. Additionally, we uncover a novel\nvulnerability in advanced RMOT models employing FIFO-based memory, whereby\ntargeted and consistent attacks on their spatial-temporal reasoning introduce\nerrors that persist within the history buffer over multiple subsequent frames.\nWe present VEIL, a novel adversarial framework designed to disrupt the unified\nreferring-matching mechanisms of RMOT models. We show that carefully crafted\ndigital and physical perturbations can corrupt the tracking logic reliability,\ninducing track ID switches and terminations. We conduct comprehensive\nevaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL\nand demonstrate the urgent need for security-aware RMOT designs for critical\nlarge-scale applications."
    },
    {
        "date": "2025-09",
        "title": "A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis",
        "author": "Nsengiyumva Wilberforce",
        "link": "http://arxiv.org/abs/2509.03545v1",
        "abstract": "The proliferation of mobile money in Uganda has been a cornerstone of\nfinancial inclusion, yet its security mechanisms remain a critical concern.\nThis study investigates a significant public response to perceived security\nfailures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an\nincident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a\nvictim's account, withdrew funds, and procured a loan, the campaign revealed\ndeep seated public anxiety over the safety of mobile money. This research\nemploys qualitative analysis to systematically examine the complaints raised\nduring this campaign, extracting key themes related to security vulnerabilities\nand user dissatisfaction. By synthesizing these public sentiments, the paper\nprovides crucial insights into the specific security gaps experienced by users\nand situates these findings within the larger framework of Uganda's mobile\nmoney regulatory and operational environment. The study concludes with\nimplications for providers, policymakers, and the future of secure digital\nfinance in Uganda."
    },
    {
        "date": "2025-09",
        "title": "A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection",
        "author": "Yasemin Topuz, M. Taha G\u00f6kcan, Serdar Y\u0131ld\u0131z, and Song\u00fcl Varl\u0131",
        "link": "http://arxiv.org/abs/2509.02637v1",
        "abstract": "Mitotic figure detection is a crucial task in computational pathology, as\nmitotic activity serves as a strong prognostic marker for tumor aggressiveness.\nHowever, domain variability that arises from differences in scanners, tissue\ntypes, and staining protocols poses a major challenge to the robustness of\nautomated detection methods. In this study, we introduce SDF-YOLO (Single\nDetect Focused YOLO), a lightweight yet domain-robust detection framework\ndesigned specifically for small, rare targets such as mitotic figures. The\nmodel builds on YOLOv11 with task-specific modifications, including a single\ndetection head aligned with mitotic figure scale, coordinate attention to\nenhance positional sensitivity, and improved cross-channel feature mixing.\nExperiments were conducted on three datasets that span human and canine tumors:\nMIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary\ncarcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025\nchallenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a\nprecision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of\n5.793, demonstrating both competitive accuracy and computational efficiency.\nThese results indicate that SDF-YOLO provides a reliable and efficient\nframework for robust mitotic figure detection across diverse domains."
    },
    {
        "date": "2025-09",
        "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators",
        "author": "Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, and Chenghong Wang",
        "link": "http://arxiv.org/abs/2509.01742v2",
        "abstract": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
    },
    {
        "date": "2025-09",
        "title": "Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection",
        "author": "Sara Khan, Mehmed Y\u00fcksel, and Frank Kirchner",
        "link": "http://arxiv.org/abs/2509.01719v1",
        "abstract": "Wear and tear detection in fleet and shared vehicle systems is a critical\nchallenge, particularly in rental and car-sharing services, where minor damage,\nsuch as dents, scratches, and underbody impacts, often goes unnoticed or is\ndetected too late. Currently, manual inspection methods are the default\napproach but are labour intensive and prone to human error. In contrast,\nstate-of-the-art image-based methods struggle with real-time performance and\nare less effective at detecting underbody damage due to limited visual access\nand poor spatial coverage. This work introduces a novel multi-modal\narchitecture based on anomaly detection to address these issues. Sensors such\nas IMUs and microphones are integrated into a compact device mounted on the\nvehicle's windshield. This approach supports real-time damage detection while\navoiding the need for highly resource-intensive sensors. We developed multiple\nvariants of multi-modal autoencoder-based architectures and evaluated them\nagainst unimodal and state-of-the-art methods. Our ensemble pooling multi-modal\nmodel achieved the highest performance, with a Receiver Operating\nCharacteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its\neffectiveness in real-world applications. This approach can also be extended to\nother applications, such as improving automotive safety - where it can\nintegrate with airbag systems for efficient deployment - and helping autonomous\nvehicles by complementing other sensors in collision detection."
    },
    {
        "date": "2025-09",
        "title": "Designing a Layered Framework to Secure Data via Improved Multi Stage Lightweight Cryptography in IoT Cloud Systems",
        "author": "Hojjat Farshadinia, Ali Barati, and Hamid Barati",
        "link": "http://arxiv.org/abs/2509.01717v1",
        "abstract": "This paper presents a novel multi-layered hybrid security approach aimed at\nenhancing lightweight encryption for IoT-Cloud systems. The primary goal is to\novercome limitations inherent in conventional solutions such as TPA,\nBlockchain, ECDSA and ZSS which often fall short in terms of data protection,\ncomputational efficiency and scalability. Our proposed method strategically\nrefines and integrates these technologies to address their shortcomings while\nmaximizing their individual strengths. By doing so we create a more reliable\nand high-performance framework for secure data exchange across heterogeneous\nenvironments. The model leverages the combined potential of emerging\ntechnologies, particularly Blockchain, IoT and Cloud computing which when\neffectively coordinated offer significant advancements in security\narchitecture. The proposed framework consists of three core layers: (1) the\nH.E.EZ Layer which integrates improved versions of Hyperledger Fabric,\nEnc-Block and a hybrid ECDSA-ZSS scheme to improve encryption speed,\nscalability and reduce computational cost; (2) the Credential Management Layer\nindependently verifying data integrity and authenticity; and (3) the Time and\nAuditing Layer designed to reduce traffic overhead and optimize performance\nacross dynamic workloads. Evaluation results highlight that the proposed\nsolution not only strengthens security but also significantly improves\nexecution time, communication efficiency and system responsiveness, offering a\nrobust path forward for next-generation IoT-Cloud infrastructures."
    },
    {
        "date": "2025-09",
        "title": "AmphiKey: A Dual-Mode Secure Authenticated Key Encapsulation Protocol for Smart Grid",
        "author": "Kazi Hassan Shakib, Muhammad Asfand Hafeez, and Arslan Munir",
        "link": "http://arxiv.org/abs/2509.01701v1",
        "abstract": "AmphiKey, a dual-mode post-quantum/traditional (PQ/T) hybrid authenticated\nkey exchange mechanism (AKEM) has been designed to secure smart grid\ncommunications against both classical and quantum threats. AmphiKey offers two\ndistinct operational modes within a single framework: an Authenticated Mode and\na Deniable Mode. The Authenticated Mode employs a blackbox approach, combining\nephemeral ML-KEM-768 and X25519 with long-term Raccoon DSA keys to provide\nforward secrecy and strong, non-repudiable authenticity. This design achieves\n\"OR\" confidentiality, where security holds if either of the KEMs is unbroken,\nand robust \"AND\" authenticity. For the signature operation, it leverages the\n'masking-friendly' Raccoon digital signature (DSA), which is specifically\ndesigned for side-channel attack resistance, though this protection is\nlocalized to the signing key and does not provide deniability. In contrast,\nDeniable Mode provides deniable authentication, preserving privacy. The\nprotocol used ML-KEM-768 (AKEM-1), Ephemeral X25519 (AKEM-2), Raccoon-based DSA\n(Rac) (compared performance to ML-DSA-65), and the Ascon cipher to deliver its\nsecurity guarantees. Key contributions include providing a flexible protocol\nwith enhanced security, optional deniability, and efficiency adapted to the\ndiverse needs of the smart grid infrastructure. We present a comprehensive\nperformance evaluation on a heterogeneous testbed featuring a powerful server\nand client (AMD Ryzen 5) and a resource-constrained client (Raspberry Pi). In\nefficient Deniable mode, the full handshake completes in 0.15 ms on the server\nand 0.41 ms on the Raspberry Pi client. In contrast, the Authenticated Mode is\nbottlenecked by the client-side signature generation; the handshake takes 4.8\nms for the Raspberry Pi client to initiate and 0.84 ms for the server to\nverify."
    },
    {
        "date": "2025-09",
        "title": "Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection",
        "author": "Euiseop Song, Jaeyoung Park, and Jaewoo Park",
        "link": "http://arxiv.org/abs/2509.02630v1",
        "abstract": "Mitotic figure detection remains a challenging task in computational\npathology due to domain variability and morphological complexity. This paper\ndescribes our participation in the MIDOG 2025 challenge, focusing on robust\nmitotic figure detection across diverse tissue domains. We developed a\ntwo-stage pipeline combining Faster R-CNN for candidate detection with an\nensemble of three classifiers (DenseNet-121, EfficientNet-v2,\nInceptionResNet-v2) for false positive reduction. Our best submission achieved\nF1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN\ntrained solely on MIDOG++ dataset. While our high recall demonstrates effective\nmitotic figure detection, the critically low precision (12.67%) reveals\nfundamental challenges in distinguishing true mitoses from morphologically\nsimilar imposters across diverse domains. Analysis of six submission variants\nshowed that subsequent optimization attempts were counterproductive,\nhighlighting the omplexity of domain generalization in histopathology. This\nwork provides valuable insights into the practical challenges of developing\nrobust mitotic figure detection algorithms and emphasizes the importance of\neffective false positive suppression strategies."
    },
    {
        "date": "2025-09",
        "title": "Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices",
        "author": "Einstein Rivas Pizarro, Wajiha Zaheer, Li Yang, Khalil El-Khatib, and Glenn Harvel",
        "link": "http://arxiv.org/abs/2509.01592v1",
        "abstract": "Radiation Detection Systems (RDSs) play a vital role in ensuring public\nsafety across various settings, from nuclear facilities to medical\nenvironments. However, these systems are increasingly vulnerable to\ncyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP\nfloods, botnet attacks, privilege escalation, and distributed denial-of-service\n(DDoS) attacks. Such threats could compromise the integrity and reliability of\nradiation measurements, posing significant public health and safety risks. This\npaper presents a new synthetic radiation dataset and an Intrusion Detection\nSystem (IDS) tailored for resource-constrained environments, bringing Machine\nLearning (ML) predictive capabilities closer to the sensing edge layer of\ncritical infrastructure. Leveraging TinyML techniques, the proposed IDS employs\nan optimized XGBoost model enhanced with pruning, quantization, feature\nselection, and sampling. These TinyML techniques significantly reduce the size\nof the model and computational demands, enabling real-time intrusion detection\non low-resource devices while maintaining a reasonable balance between\nefficiency and accuracy."
    },
    {
        "date": "2025-09",
        "title": "Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing",
        "author": "Zihao Wang, Enneng Yang, Lu Yin, Shiwei Liu, and Li Shen",
        "link": "http://arxiv.org/abs/2509.01548v1",
        "abstract": "Model merging leverages multiple finetuned expert models to construct a\nmulti-task model with low cost, and is gaining increasing attention. However,\nas a growing number of finetuned models become publicly available, concerns\nabout the safety of model merging have emerged. Unauthorized merging may\ninfringe on developers' rights and risk leaking sensitive personal information.\nMost existing methods focus on detecting whether a merged model originates from\na specific source model, but fail to effectively prevent illegal merging. In\nthis paper, we propose MergeLock, an active protection mechanism that disrupts\nmodel parameters to render them unmergeable, thereby directly preventing\nunauthorized model merging. Specifically, leveraging the inherent symmetry of\nthe attention mechanism in Transformer-based models, we randomly sample two\npairs of invertible matrices and apply them to the Query-Key (QK) and\nValue-Output (VO) branches. This transformation keeps the model's output\nunchanged while pushing it away from the shared parameter space of other\nfinetuned models. Extensive experiments across both vision and language tasks\ndemonstrate that MergeLock can degrade the performance of merged models by over\n95% when a protected model is involved in most cases, demonstrating its\neffectiveness. Moreover, we further demonstrate that merged models protected by\nMergeLock cannot be effectively recovered using low-cost restoration methods,\nfurther enhancing robustness against unauthorized merging. The code is\navailable at https://github.com/hetailang/Merge-Lock."
    },
    {
        "date": "2025-09",
        "title": "LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT",
        "author": "Handi Chen, Jing Deng, Xiuzhe Wu, Zhihan Jiang, Xinchen Zhang, Xianhao Chen, and Edith C. H. Ngai",
        "link": "http://arxiv.org/abs/2509.01434v1",
        "abstract": "The expansion of Internet of Things (IoT) devices constantly generates\nheterogeneous data streams, driving demand for continuous, decentralized\nintelligence. Federated Lifelong Learning (FLL) provides an ideal solution by\nincorporating federated and lifelong learning to overcome catastrophic\nforgetting. The extended lifecycle of FLL in IoT systems increases their\nvulnerability to persistent attacks, and these risks may be obscured by\nperformance degradation caused by spatial-temporal data heterogeneity.\nMoreover, this problem is exacerbated by the standard single-server\narchitecture, as its single point of failure makes it difficult to maintain a\nreliable audit trail for long-term threats. Blockchain provides a tamper-proof\nfoundation for trustworthy FLL systems. Nevertheless, directly applying\nblockchain to FLL significantly increases computational and retrieval costs\nwith the expansion of the knowledge base, slowing down the training on IoT\ndevices. To address these challenges, we propose LiFeChain, a lightweight\nblockchain for secure and efficient federated lifelong learning by providing a\ntamper-resistant ledger with minimal on-chain disclosure and bidirectional\nverification. To the best of our knowledge, LiFeChain is the first blockchain\ntailored for FLL. LiFeChain incorporates two complementary mechanisms: the\nproof-of-model-correlation (PoMC) consensus on the server, which couples\nlearning and unlearning mechanisms to mitigate negative transfer, and segmented\nzero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates\nabnormal committee behavior without compromising privacy. LiFeChain is designed\nas a plug-and-play component that can be seamlessly integrated into existing\nFLL algorithms. Experimental results demonstrate that LiFeChain not only\nenhances model performance against two long-term attacks but also sustains high\nefficiency and scalability."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning",
        "author": "Long Zhang, Peipei Song, Jianfeng Dong, Kun Li, and Xun Yang",
        "link": "http://arxiv.org/abs/2509.01383v1",
        "abstract": "Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos\npartially relevant to a given query. The core challenge lies in learning robust\nquery-video alignment against spurious semantic correlations arising from\ninherent data uncertainty: 1) query ambiguity, where the query incompletely\ncharacterizes the target video and often contains uninformative tokens, and 2)\npartial video relevance, where abundant query-irrelevant segments introduce\ncontextual noise in cross-modal alignment. Existing methods often focus on\nenhancing multi-scale clip representations and retrieving the most relevant\nclip. However, the inherent data uncertainty in PRVR renders them vulnerable to\ndistractor videos with spurious similarities, leading to suboptimal\nperformance. To fill this research gap, we propose Robust Alignment Learning\n(RAL) framework, which explicitly models the uncertainty in data. Key\ninnovations include: 1) we pioneer probabilistic modeling for PRVR by encoding\nvideos and queries as multivariate Gaussian distributions. This not only\nquantifies data uncertainty but also enables proxy-level matching to capture\nthe variability in cross-modal correspondences; 2) we consider the\nheterogeneous informativeness of query words and introduce learnable confidence\ngates to dynamically weight similarity. As a plug-and-play solution, RAL can be\nseamlessly integrated into the existing architectures. Extensive experiments\nacross diverse retrieval backbones demonstrate its effectiveness."
    },
    {
        "date": "2025-09",
        "title": "An Automated Attack Investigation Approach Leveraging Threat-Knowledge-Augmented Large Language Models",
        "author": "Rujie Dai, Peizhuo Lv, Yujiang Gui, Qiujian Lv, Yuanyuan Qiao, Yan Wang, Degang Sun, Weiqing Huang, Yingjiu Li, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2509.01271v1",
        "abstract": "Advanced Persistent Threats (APTs) are prolonged, stealthy intrusions by\nskilled adversaries that compromise high-value systems to steal data or disrupt\noperations. Reconstructing complete attack chains from massive, heterogeneous\nlogs is essential for effective attack investigation, yet existing methods\nsuffer from poor platform generality, limited generalization to evolving\ntactics, and an inability to produce analyst-ready reports. Large Language\nModels (LLMs) offer strong semantic understanding and summarization\ncapabilities, but in this domain they struggle to capture the long-range,\ncross-log dependencies critical for accurate reconstruction.\n  To solve these problems, we present an LLM-empowered attack investigation\nframework augmented with a dynamically adaptable Kill-Chain-aligned threat\nknowledge base. We organizes attack-relevant behaviors into stage-aware\nknowledge units enriched with semantic annotations, enabling the LLM to\niteratively retrieve relevant intelligence, perform causal reasoning, and\nprogressively expand the investigation context. This process reconstructs\nmulti-phase attack scenarios and generates coherent, human-readable\ninvestigation reports. Evaluated on 15 attack scenarios spanning single-host\nand multi-host environments across Windows and Linux (over 4.3M log events, 7.2\nGB of data), the system achieves an average True Positive Rate (TPR) of 97.1%\nand an average False Positive Rate (FPR) of 0.2%, significantly outperforming\nthe SOTA method ATLAS, which achieves an average TPR of 79.2% and an average\nFPR of 29.1%."
    },
    {
        "date": "2025-09",
        "title": "Geometric origin of adversarial vulnerability in deep learning",
        "author": "Yixiong Ren, Wenkang Du, Jianhui Zhou, and Haiping Huang",
        "link": "http://arxiv.org/abs/2509.01235v1",
        "abstract": "How to balance training accuracy and adversarial robustness has become a\nchallenge since the birth of deep learning. Here, we introduce a geometry-aware\ndeep learning framework that leverages layer-wise local training to sculpt the\ninternal representations of deep neural networks. This framework promotes\nintra-class compactness and inter-class separation in feature space, leading to\nmanifold smoothness and adversarial robustness against white or black box\nattacks. The performance can be explained by an energy model with Hebbian\ncoupling between elements of the hidden representation. Our results thus shed\nlight on the physics of learning in the direction of alignment between\nbiological and artificial intelligence systems. Using the current framework,\nthe deep network can assimilate new information into existing knowledge\nstructures while reducing representation interference."
    },
    {
        "date": "2025-09",
        "title": "RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations",
        "author": "Weihang Ouyang, Min Zhu, Wei Xiong, Si-Wei Liu, and Lu Lu",
        "link": "http://arxiv.org/abs/2509.01234v1",
        "abstract": "Physics-informed neural networks (PINNs) and neural operators, two leading\nscientific machine learning (SciML) paradigms, have emerged as powerful tools\nfor solving partial differential equations (PDEs). Although increasing the\ntraining sample size generally enhances network performance, it also increases\ncomputational costs for physics-informed or data-driven training. To address\nthis trade-off, different sampling strategies have been developed to sample\nmore points in regions with high PDE residuals. However, existing sampling\nmethods are computationally demanding for high-dimensional problems, such as\nhigh-dimensional PDEs or operator learning tasks. Here, we propose a\nresidual-based adversarial-gradient moving sample (RAMS) method, which moves\nsamples according to the adversarial gradient direction to maximize the PDE\nresidual via gradient-based optimization. RAMS can be easily integrated into\nexisting sampling methods. Extensive experiments, ranging from PINN applied to\nhigh-dimensional PDEs to physics-informed and data-driven operator learning\nproblems, have been conducted to demonstrate the effectiveness of RAMS.\nNotably, RAMS represents the first efficient adaptive sampling approach for\noperator learning, marking a significant advancement in the SciML field."
    },
    {
        "date": "2025-09",
        "title": "PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity",
        "author": "Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, and Cheng Jin",
        "link": "http://arxiv.org/abs/2509.01214v1",
        "abstract": "Tumor spatial heterogeneity analysis requires precise correlation between\nHematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker\nexpression, yet current methods suffer from spatial misalignment in consecutive\nsections, severely compromising in situ pathological interpretation. In order\nto obtain a more accurate virtual staining pattern, We propose PRINTER, a\nweakly-supervised framework that integrates PRototype-drIven content and\nstaiNing patTERn decoupling and deformation-aware adversarial learning\nstrategies designed to accurately learn IHC staining patterns while preserving\nH&E staining details. Our approach introduces three key innovations: (1) A\nprototype-driven staining pattern transfer with explicit content-style\ndecoupling; and (2) A cyclic registration-synthesis framework GapBridge that\nbridges H&E and IHC domains through deformable structural alignment, where\nregistered features guide cross-modal style transfer while synthesized outputs\niteratively refine the registration;(3) Deformation-Aware Adversarial Learning:\nWe propose a training framework where a generator and deformation-aware\nregistration network jointly adversarially optimize a style-focused\ndiscriminator. Extensive experiments demonstrate that PRINTER effectively\nachieves superior performance in preserving H&E staining details and virtual\nstaining fidelity, outperforming state-of-the-art methods. Our work provides a\nrobust and scalable solution for virtual staining, advancing the field of\ncomputational pathology."
    },
    {
        "date": "2025-09",
        "title": "Web Fraud Attacks Against LLM-Driven Multi-Agent Systems",
        "author": "Dezhang Kong, Hujin Peng, Yilun Zhang, Lele Zhao, Zhenhua Xu, Shi Lin, Changting Lin, and Meng Han",
        "link": "http://arxiv.org/abs/2509.01211v1",
        "abstract": "With the proliferation of applications built upon LLM-driven multi-agent\nsystems (MAS), the security of Web links has become a critical concern in\nensuring system reliability. Once an agent is induced to visit a malicious\nwebsite, attackers can use it as a springboard to conduct diverse subsequent\nattacks, which will drastically expand the attack surface. In this paper, we\npropose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to\nvisit malicious websites. We design 11 representative attack variants that\nencompass domain name tampering (homoglyph deception, character substitution,\netc.), link structure camouflage (sub-directory nesting, sub-domain grafting,\nparameter obfuscation, etc.), and other deceptive techniques tailored to\nexploit MAS's vulnerabilities in link validation. Through extensive experiments\non these crafted attack vectors, we demonstrate that Web fraud attacks not only\nexhibit significant destructive potential across different MAS architectures\nbut also possess a distinct advantage in evasion: they circumvent the need for\ncomplex input formats such as jailbreaking, which inherently carry higher\nexposure risks. These results underscore the importance of addressing Web fraud\nattacks in LLM-driven MAS, as their stealthiness and destructiveness pose\nnon-negligible threats to system security and user safety."
    },
    {
        "date": "2025-09",
        "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
        "author": "Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, and Jianya Gong",
        "link": "http://arxiv.org/abs/2509.01183v1",
        "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial\nanalysis in remote sensing, necessitating robust segmentation quality\nassessment (SQA), particularly in unsupervised settings lacking ground truth.\nAlthough recent deep learning (DL) based unsupervised SQA methods show\npotential, they often suffer from coarse evaluation granularity, incomplete\nassessments, and poor transferability. To overcome these limitations, this\npaper introduces Panoramic Quality Mapping (PQM) as a new paradigm for\ncomprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning\nframework realizing this approach. SegAssess distinctively formulates SQA as a\nfine-grained, four-class panoramic segmentation task, classifying pixels within\na segmentation mask under evaluation into true positive (TP), false positive\n(FP), true negative (TN), and false negative (FN) categories, thereby\ngenerating a complete quality map. Leveraging an enhanced Segment Anything\nModel (SAM) architecture, SegAssess uniquely employs the input mask as a prompt\nfor effective feature integration via cross-attention. Key innovations include\nan Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)\nmodule to refine predictions near challenging object edges, and an Augmented\nMixup Sampling (AMS) training strategy integrating multi-source masks to\nsignificantly boost cross-domain robustness and zero-shot transferability.\nComprehensive experiments across 32 datasets derived from 6 sources demonstrate\nthat SegAssess achieves state-of-the-art (SOTA) performance and exhibits\nremarkable zero-shot transferability to unseen masks, establishing PQM via\nSegAssess as a robust and transferable solution for unsupervised SQA. The code\nis available at https://github.com/Yangbn97/SegAssess."
    },
    {
        "date": "2025-09",
        "title": "Efficient and High-Accuracy Secure Two-Party Protocols for a Class of Functions with Real-number Inputs",
        "author": "Hao Guo, Zhaoqian Liu, Liqiang Peng, Shuaishuai Li, Ximing Fu, Weiran Liu, and Lin Qu",
        "link": "http://arxiv.org/abs/2509.01178v1",
        "abstract": "In two-party secret sharing scheme, values are typically encoded as unsigned\nintegers $\\mathsf{uint}(x)$, whereas real-world applications often require\ncomputations on signed real numbers $\\mathsf{Real}(x)$. To enable secure\nevaluation of practical functions, it is essential to computing\n$\\mathsf{Real}(x)$ from shared inputs, as protocols take shares as input. At\nUSENIX'25, Guo et al. proposed an efficient method for computing signed integer\nvalues $\\mathsf{int}(x)$ from shares, which can be extended to compute\n$\\mathsf{Real}(x)$. However, their approach imposes a restrictive input\nconstraint $|x| < \\frac{L}{3}$ for $x \\in \\mathbb{Z}_L$, limiting its\napplicability in real-world scenarios. In this work, we significantly relax\nthis constraint to $|x| < B$ for any $B \\leq \\frac{L}{2}$, where $B =\n\\frac{L}{2}$ corresponding to the natural representable range in $x \\in\n\\mathbb{Z}_L$. This relaxes the restrictions and enables the computation of\n$\\mathsf{Real}(x)$ with loose or no input constraints. Building upon this\nfoundation, we present a generalized framework for designing secure protocols\nfor a broad class of functions, including integer division ($\\lfloor\n\\frac{x}{d} \\rfloor$), trigonometric ($\\sin(x)$) and exponential ($e^{-x}$)\nfunctions. Our experimental evaluation demonstrates that the proposed protocols\nachieve both high efficiency and high accuracy. Notably, our protocol for\nevaluating $e^{-x}$ reduces communication costs to approximately 31% of those\nin SirNN (S&P 21) and Bolt (S&P 24), with runtime speedups of up to $5.53\n\\times$ and $3.09 \\times$, respectively. In terms of accuracy, our protocol\nachieves a maximum ULP error of $1.435$, compared to $2.64$ for SirNN and\n$8.681$ for Bolt."
    },
    {
        "date": "2025-09",
        "title": "RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness",
        "author": "Junghyun Park, Tuan Anh Nguyen, and Dugki Min",
        "link": "http://arxiv.org/abs/2509.05333v1",
        "abstract": "Real world deployments often expose modern object recognition models to\ndomain shifts that precipitate a severe drop in accuracy. Such shifts encompass\n(i) variations in low level image statistics, (ii) changes in object pose and\nviewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent\nclasses. To mitigate this degradation, we introduce the Re-Thinking Vision\nLanguage Model (RT-VLM) framework. The foundation of this framework is a unique\nsynthetic dataset generation pipeline that produces images annotated with\n\"4-Clues\": precise bounding boxes, class names, detailed object-level captions,\nand a comprehensive context-level caption for the entire scene. We then perform\nparameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this\nresource. At inference time, a two stage Re-Thinking scheme is executed: the\nmodel first emits its own four clues, then re examines these responses as\nevidence and iteratively corrects them. Across robustness benchmarks that\nisolate individual domain shifts, RT-VLM consistently surpasses strong\nbaselines. These findings indicate that the integration of structured\nmultimodal evidence with an explicit self critique loop constitutes a promising\nroute toward reliable and transferable visual understanding."
    },
    {
        "date": "2025-09",
        "title": "Lightening the Load: A Cluster-Based Framework for A Lower-Overhead, Provable Website Fingerprinting Defense",
        "author": "Khashayar Khajavi, and Tao Wang",
        "link": "http://arxiv.org/abs/2509.01046v1",
        "abstract": "Website fingerprinting (WF) attacks remain a significant threat to encrypted\ntraffic, prompting the development of a wide range of defenses. Among these,\ntwo prominent classes are regularization-based defenses, which shape traffic\nusing fixed padding rules, and supersequence-based approaches, which conceal\ntraces among predefined patterns. In this work, we present a unified framework\nfor designing an adaptive WF defense that combines the effectiveness of\nregularization with the provable security of supersequence-style grouping. The\nscheme first extracts behavioural patterns from traces and clusters them into\n(k,l)-diverse anonymity sets; an early-time-series classifier (adapted from\nECDIRE) then switches from a conservative global set of regularization\nparameters to the lighter, set-specific parameters. We instantiate the design\nas Adaptive Tamaraw, a variant of Tamaraw that assigns padding parameters on a\nper-cluster basis while retaining its original information-theoretic guarantee.\nComprehensive experiments on public real-world datasets confirm the benefits.\nBy tuning k, operators can trade privacy for efficiency: in its high-privacy\nmode Adaptive Tamaraw pushes the bound on any attacker's accuracy below 30%,\nwhereas in efficiency-centred settings it cuts total overhead by 99% compared\nwith classic Tamaraw."
    },
    {
        "date": "2025-08",
        "title": "Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles",
        "author": "Christos Anagnostopoulos, Ioulia Kapsali, Alexandros Gkillas, Nikos Piperigkos, and Aris S. Lalos",
        "link": "http://arxiv.org/abs/2509.05332v1",
        "abstract": "Autonomous vehicles (AVs) rely on complex perception and communication\nsystems, making them vulnerable to adversarial attacks that can compromise\nsafety. While simulation offers a scalable and safe environment for robustness\ntesting, existing frameworks typically lack comprehensive supportfor modeling\nmulti-domain adversarial scenarios. This paper introduces a novel, open-source\nintegrated simulation framework designed to generate adversarial attacks\ntargeting both perception and communication layers of AVs. The framework\nprovides high-fidelity modeling of physical environments, traffic dynamics, and\nV2X networking, orchestrating these components through a unified core that\nsynchronizes multiple simulators based on a single configuration file. Our\nimplementation supports diverse perception-level attacks on LiDAR sensor data,\nalong with communication-level threats such as V2X message manipulation and GPS\nspoofing. Furthermore, ROS 2 integration ensures seamless compatibility with\nthird-party AV software stacks. We demonstrate the framework's effectiveness by\nevaluating the impact of generated adversarial scenarios on a state-of-the-art\n3D object detector, revealing significant performance degradation under\nrealistic conditions."
    },
    {
        "date": "2025-08",
        "title": "Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play",
        "author": "Zakaria El Jaafari",
        "link": "http://arxiv.org/abs/2509.00923v1",
        "abstract": "Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a\ncornerstone algorithm for solving extensive-form games, but its integration\nwith deep neural networks introduces scale-dependent challenges that manifest\ndifferently across game complexities. This paper presents a comprehensive\nanalysis of how neural MCCFR component effectiveness varies with game scale and\nproposes an adaptive framework for selective component deployment. We identify\nthat theoretical risks such as nonstationary target distribution shifts, action\nsupport collapse, variance explosion, and warm-starting bias have\nscale-dependent manifestation patterns, requiring different mitigation\nstrategies for small versus large games. Our proposed Robust Deep MCCFR\nframework incorporates target networks with delayed updates, uniform\nexploration mixing, variance-aware training objectives, and comprehensive\ndiagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc\nPoker, we demonstrate scale-dependent component effectiveness and identify\ncritical component interactions. The best configuration achieves final\nexploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the\nclassical framework (0.156). On the more complex Leduc Poker domain, selective\ncomponent usage achieves exploitability of 0.2386, a 23.5% improvement over the\nclassical framework (0.3703) and highlighting the importance of careful\ncomponent selection over comprehensive mitigation. Our contributions include:\n(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled\nmitigation framework with convergence guarantees, (3) comprehensive multi-scale\nexperimental validation revealing scale-dependent component interactions, and\n(4) practical guidelines for deployment in larger games."
    },
    {
        "date": "2025-08",
        "title": "AI-Enhanced Intelligent NIDS Framework: Leveraging Metaheuristic Optimization for Robust Attack Detection and Prevention",
        "author": "Maryam Mahdi Alhusseini, and Mohammad Reza Feizi Derakhshi",
        "link": "http://arxiv.org/abs/2509.00896v2",
        "abstract": "In todays rapidly evolving digital landscape, safeguarding network\ninfrastructures against cyberattacks has become a critical priority. This\nresearch presents an innovative AI-driven real-time intrusion detection\nframework designed to enhance network security, particularly in Wireless Sensor\nNetworks (WSNs), Cloud Computing (CC), and Internet of Things (IoT)\nenvironments. The system employs classical machine learning models, Logistic\nRegression, decision trees, and K-Nearest Neighbors, optimized through the\nnovel Energy Valley Optimization (EVO) method using the NSL-KDD dataset.\nFeature selection significantly reduced the number of input features from 42 to\n18, while maintaining strong detection capabilities. The proposed system\nachieved 98.95 percent. accuracy with Decision Tree, 98.47 percent with\nK-Nearest Neighbors, and 88.84 percent with Logistic Regression. Moreover, high\nprecision, recall, and F1-scores were attained across all classifiers while\nsubstantially reducing training and testing times, making the framework highly\nsuitable for real-time applications. To ensure fair detection across diverse\nattack types, dataset balancing via Downsampling was applied to address class\nimbalance challenges. This investigation focuses on the significance of\nadvancing IDSs. in cloud computing and WSNs. Overall, this work advances secure\ncommunications by delivering a scalable, low-latency, and high-accuracy\nintrusion detection solution aligned with the latest trends in artificial\nintelligence, cybersecurity, and real-time digital networks."
    },
    {
        "date": "2025-08",
        "title": "Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization",
        "author": "Xinlei Liu, Tao Hu, Peng Yi, Weitao Han, Jichao Xie, and Baolin Li",
        "link": "http://arxiv.org/abs/2509.00826v1",
        "abstract": "Efficient adversarial attack methods are critical for assessing the\nrobustness of computer vision models. In this paper, we reconstruct the\noptimization objective for generating adversarial examples as \"maximizing the\ndifference between the non-true labels' probability upper bound and the true\nlabel's probability,\" and propose a gradient-based attack method termed\nSequential Difference Maximization (SDM). SDM establishes a three-layer\noptimization framework of \"cycle-stage-step.\" The processes between cycles and\nbetween iterative steps are respectively identical, while optimization stages\ndiffer in terms of loss functions: in the initial stage, the negative\nprobability of the true label is used as the loss function to compress the\nsolution space; in subsequent stages, we introduce the Directional Probability\nDifference Ratio (DPDR) loss function to gradually increase the non-true\nlabels' probability upper bound by compressing the irrelevant labels'\nprobabilities. Experiments demonstrate that compared with previous SOTA\nmethods, SDM not only exhibits stronger attack performance but also achieves\nhigher attack cost-effectiveness. Additionally, SDM can be combined with\nadversarial training methods to enhance their defensive effects. The code is\navailable at https://github.com/X-L-Liu/SDM."
    },
    {
        "date": "2025-08",
        "title": "Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance",
        "author": "Xiang Yuan, Jun Shu, Deyu meng, and Zongben Xu",
        "link": "http://arxiv.org/abs/2509.05328v1",
        "abstract": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. To remedy this,\nmost robust fine-tuning methods aim to preserve the pretrained weights,\nfeatures, or logits. However, we find that these methods cannot always improve\nOOD robustness for different model architectures. This is due to the OOD\nrobustness requiring the model function to produce stable prediction for input\ninformation of downstream tasks, while existing methods might serve as a poor\nproxy for the optimization in the function space. Based on this finding, we\npropose a novel regularization that constrains the distance of fine-tuning and\npre-trained model in the function space with the simulated OOD samples, aiming\nto preserve the OOD robustness of the pre-trained model. Besides, to further\nenhance the OOD robustness capability of the fine-tuning model, we introduce an\nadditional consistency regularization to promote stable predictions of\nperturbed samples. Extensive experiments demonstrate our approach could\nconsistently improve both downstream task ID fine-tuning performance and OOD\nrobustness across a variety of CLIP backbones, outperforming existing\nregularization-based robust fine-tuning methods."
    },
    {
        "date": "2025-08",
        "title": "MAESTROCUT: Dynamic, Noise-Adaptive, and Secure Quantum Circuit Cutting on Near-Term Hardware",
        "author": "Samuel Punch, and Krishnendu Guha",
        "link": "http://arxiv.org/abs/2509.00811v1",
        "abstract": "We present MaestroCut, a closed-loop framework for quantum circuit cutting\nthat adapts partitioning and shot allocation to device drift and workload\nvariation. MaestroCut tracks a variance proxy in real time, triggers re-cutting\nwhen accuracy degrades, and routes shots using topology-aware priors. An online\nestimator cascade (MLE, Bayesian, GP-assisted) selects the lowest-error\nreconstruction within a fixed budget. Tier-1 simulations show consistent\nvariance contraction and reduced mean-squared error versus uniform and\nproportional baselines. Tier-2 emulation with realistic queueing and noise\ndemonstrates stable latency targets, high reliability, and ~1% software\noverhead under stress scenarios. These results indicate that adaptive circuit\ncutting can provide accuracy and efficiency improvements with minimal\noperational cost on near-term hardware."
    },
    {
        "date": "2025-08",
        "title": "Secure and Scalable Face Retrieval via Cancelable Product Quantization",
        "author": "Haomiao Tang, Wenjie Li, Yixiang Qiu, Genping Wang, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2509.00781v1",
        "abstract": "Despite the ubiquity of modern face retrieval systems, their retrieval stage\nis often outsourced to third-party entities, posing significant risks to user\nportrait privacy. Although homomorphic encryption (HE) offers strong security\nguarantees by enabling arithmetic computations in the cipher space, its high\ncomputational inefficiency makes it unsuitable for real-time, real-world\napplications. To address this issue, we propose Cancelable Product\nQuantization, a highly efficient framework for secure face representation\nretrieval. Our hierarchical two-stage framework comprises: (i) a\nhigh-throughput cancelable PQ indexing module for fast candidate filtering, and\n(ii) a fine-grained cipher-space retrieval module for final precise face\nranking. A tailored protection mechanism is designed to secure the indexing\nmodule for cancelable biometric authentication while ensuring efficiency.\nExperiments on benchmark datasets demonstrate that our method achieves an\ndecent balance between effectiveness, efficiency and security."
    },
    {
        "date": "2025-08",
        "title": "Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition",
        "author": "Osama Ahmad, Lukas Wesemann, Fabian Waschkowski, and Zubair Khalid",
        "link": "http://arxiv.org/abs/2509.00703v1",
        "abstract": "Accurate spatiotemporal forecasting is critical for numerous complex systems\nbut remains challenging due to complex volatility patterns and spectral\nentanglement in conventional graph neural networks (GNNs). While\ndecomposition-integrated approaches like variational mode graph convolutional\nnetwork (VMGCN) improve accuracy through signal decomposition, they suffer from\ncomputational inefficiency and manual hyperparameter tuning. To address these\nlimitations, we propose the mode adaptive graph network (MAGN) that transforms\niterative variational mode decomposition (VMD) into a trainable neural module.\nOur key innovations include (1) an unfolded VMD (UVMD) module that replaces\niterative optimization with a fixed-depth network to reduce the decomposition\ntime (by 250x for the LargeST benchmark), and (2) mode-specific learnable\nbandwidth constraints ({\\alpha}k ) adapt spatial heterogeneity and eliminate\nmanual tuning while preventing spectral overlap. Evaluated on the LargeST\nbenchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction\nin the prediction error over VMGCN and outperforms state-of-the-art baselines."
    },
    {
        "date": "2025-08",
        "title": "Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware",
        "author": "Vamsi Shankar Simhadri, Yichang Xiong, Habiba Farrukh, and Xiaokuan Zhang",
        "link": "http://arxiv.org/abs/2509.00662v2",
        "abstract": "Virtual Reality (VR) technology is rapidly growing in recent years. VR\ndevices such as Meta Quest 3 utilize numerous sensors to collect users' data to\nprovide an immersive experience. Due to the extensive data collection and the\nimmersive nature, the security of VR devices is paramount. Leading VR devices\noften adopt and customize Android systems, which makes them susceptible to both\nAndroid-based vulnerabilities and new issues introduced by VR-specific\ncustomizations (e.g., system services to support continuous head and hand\ntracking). While prior work has extensively examined the security properties of\nthe Android software stack, how these security properties hold for VR systems\nremains unexplored. In this paper, we present the first comprehensive security\nanalysis of VR firmware. We collect over 300 versions of VR firmware from two\nmajor vendors, Quest and Pico, and perform a longitudinal analysis across the\nkernel layer, the system binary and library layer, and the application layer.\nWe have identified several security issues in these VR firmware, including\nmissing kernel-level security features, insufficient binary hardening,\ninconsistent permission enforcement, and inadequate SELinux policy enforcement.\nBased on our findings, we synthesize recommendations for VR vendors to improve\nsecurity and trust for VR devices. This paper will act as an important security\nresource for VR developers, users, and vendors, and will also direct future\nadvancements in secure VR ecosystem."
    },
    {
        "date": "2025-08",
        "title": "Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains",
        "author": "Yumeng Lin, Dong Li, Xintao Wu, Minglai Shao, Xujiang Zhao, Zhong Chen, and Chen Zhao",
        "link": "http://arxiv.org/abs/2509.00658v1",
        "abstract": "Ensuring fairness and robustness in machine learning models remains a\nchallenge, particularly under domain shifts. We present Face4FairShifts, a\nlarge-scale facial image benchmark designed to systematically evaluate\nfairness-aware learning and domain generalization. The dataset includes 100,000\nimages across four visually distinct domains with 39 annotations within 14\nattributes covering demographic and facial features. Through extensive\nexperiments, we analyze model performance under distribution shifts and\nidentify significant gaps. Our findings emphasize the limitations of existing\nrelated datasets and the need for more effective fairness-aware domain\nadaptation techniques. Face4FairShifts provides a comprehensive testbed for\nadvancing equitable and reliable AI systems. The dataset is available online at\nhttps://meviuslab.github.io/Face4FairShifts/."
    },
    {
        "date": "2025-08",
        "title": "RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models",
        "author": "Shikun Liu, Deyu Zou, Nima Shoghi, Victor Fung, Kai Liu, and Pan Li",
        "link": "http://arxiv.org/abs/2509.00614v2",
        "abstract": "In the era of foundation models, fine-tuning pre-trained models for specific\ndownstream tasks has become crucial. This drives the need for robust\nfine-tuning methods to address challenges such as model overfitting and sparse\nlabeling. Molecular graph foundation models (MGFMs) face unique difficulties\nthat complicate fine-tuning. These models are limited by smaller pre-training\ndatasets and more severe data scarcity for downstream tasks, both of which\nrequire enhanced model generalization. Moreover, MGFMs must accommodate diverse\nobjectives, including both regression and classification tasks. To better\nunderstand and improve fine-tuning techniques under these conditions, we\nclassify eight fine-tuning methods into three mechanisms: weight-based,\nrepresentation-based, and partial fine-tuning. We benchmark these methods on\ndownstream regression and classification tasks across supervised and\nself-supervised pre-trained models in diverse labeling settings. This extensive\nevaluation provides valuable insights and informs the design of a refined\nrobust fine-tuning method, ROFT-MOL. This approach combines the strengths of\nsimple post-hoc weight interpolation with more complex weight ensemble\nfine-tuning methods, delivering improved performance across both task types\nwhile maintaining the ease of use inherent in post-hoc weight interpolation."
    },
    {
        "date": "2025-08",
        "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality",
        "author": "Rongqian Chen, Shu Hong, Rifatul Islam, Mahdi Imani, G. Gary Tan, and Tian Lan",
        "link": "http://arxiv.org/abs/2509.05324v1",
        "abstract": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks."
    },
    {
        "date": "2025-08",
        "title": "FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks",
        "author": "Yuwen Pu, Zhou Feng, Chunyi Zhou, Jiahao Chen, Chunqiang Hu, Haibo Hu, and Shouling Ji",
        "link": "http://arxiv.org/abs/2509.00561v1",
        "abstract": "Recently, speech assistant and speech verification have been used in many\nfields, which brings much benefit and convenience for us. However, when we\nenjoy these speech applications, our speech may be collected by attackers for\nspeech synthesis. For example, an attacker generates some inappropriate\npolitical opinions with the characteristic of the victim's voice by obtaining a\npiece of the victim's speech, which will greatly influence the victim's\nreputation. Specifically, with the appearance of some zero-shot voice\nconversion methods, the cost of speech synthesis attacks has been further\nreduced, which also brings greater challenges to user voice security and\nprivacy. Some researchers have proposed the corresponding privacy-preserving\nmethods. However, the existing approaches have some non-negligible drawbacks:\nlow transferability and robustness, high computational overhead. These\ndeficiencies seriously limit the existing method deployed in practical\nscenarios. Therefore, in this paper, we propose a lightweight, robust,\nplug-and-play privacy preservation method against speech synthesis attacks in a\nblack-box setting. Our method generates and adds a frequency-domain\nperturbation to the original speech to achieve privacy protection and high\nspeech quality. Then, we present a data augmentation strategy and noise\nsmoothing mechanism to improve the robustness of the proposed method. Besides,\nto reduce the user's defense overhead, we also propose a novel identity-wise\nprotection mechanism. It can generate a universal perturbation for one speaker\nand support privacy preservation for speech of any length. Finally, we conduct\nextensive experiments on 5 speech synthesis models, 5 speech verification\nmodels, 1 speech recognition model, and 2 datasets. The experimental results\ndemonstrate that our method has satisfying privacy-preserving performance, high\nspeech quality, and utility."
    },
    {
        "date": "2025-08",
        "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models",
        "author": "Yuting Tan, Xuying Li, Zhuo Li, Huizhen Shu, and Peikang Hu",
        "link": "http://arxiv.org/abs/2509.00391v1",
        "abstract": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation."
    },
    {
        "date": "2025-08",
        "title": "Unifying Adversarial Perturbation for Graph Neural Networks",
        "author": "Jinluan Yang, Ruihao Zhang, Zhengyu Chen, Fei Wu, and Kun Kuang",
        "link": "http://arxiv.org/abs/2509.00387v1",
        "abstract": "This paper studies the vulnerability of Graph Neural Networks (GNNs) to\nadversarial attacks on node features and graph structure. Various methods have\nimplemented adversarial training to augment graph data, aiming to bolster the\nrobustness and generalization of GNNs. These methods typically involve applying\nperturbations to the node feature, weights, or graph structure and subsequently\nminimizing the loss by learning more robust graph model parameters under the\nadversarial perturbations. Despite the effectiveness of adversarial training in\nenhancing GNNs' robustness and generalization abilities, its application has\nbeen largely confined to specific datasets and GNN types. In this paper, we\npropose a novel method, PerturbEmbedding, that integrates adversarial\nperturbation and training, enhancing GNNs' resilience to such attacks and\nimproving their generalization ability. PerturbEmbedding performs perturbation\noperations directly on every hidden embedding of GNNs and provides a unified\nframework for most existing perturbation strategies/methods. We also offer a\nunified perspective on the forms of perturbations, namely random and\nadversarial perturbations. Through experiments on various datasets using\ndifferent backbone models, we demonstrate that PerturbEmbedding significantly\nimproves both the robustness and generalization abilities of GNNs,\noutperforming existing methods. The rejection of both random (non-targeted) and\nadversarial (targeted) perturbations further enhances the backbone model's\nperformance."
    },
    {
        "date": "2025-08",
        "title": "HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization",
        "author": "Joohyun Chang, Soyeon Hong, Hyogun Lee, Seong Jong Ha, Dongho Lee, Seong Tae Kim, and Jinwoo Choi",
        "link": "http://arxiv.org/abs/2509.00385v1",
        "abstract": "In this work, we tackle the egocentric visual query localization (VQL), where\na model should localize the query object in a long-form egocentric video.\nFrequent and abrupt viewpoint changes in egocentric videos cause significant\nobject appearance variations and partial occlusions, making it difficult for\nexisting methods to achieve accurate localization. To tackle these challenges,\nwe introduce Hierarchical, Egocentric and RObust Visual Query Localization\n(HERO-VQL), a novel method inspired by human cognitive process in object\nrecognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric\nAugmentation based Consistency Training (EgoACT). Top-down Attention Guidance\nrefines the attention mechanism by leveraging the class token for high-level\ncontext and principal component score maps for fine-grained localization. To\nenhance learning in diverse and challenging matching scenarios, EgoAug enhances\nquery diversity by replacing the query with a randomly selected corresponding\nobject from groundtruth annotations and simulates extreme viewpoint changes by\nreordering video frames. Additionally, CT loss enforces stable object\nlocalization across different augmentation scenarios. Extensive experiments on\nVQ2D dataset validate that HERO-VQL effectively handles egocentric challenges,\nsignificantly outperforming baselines."
    },
    {
        "date": "2025-08",
        "title": "Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models",
        "author": "Sihao Wu, Gaojie Jin, Wei Huang, Jianhong Wang, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.00373v1",
        "abstract": "Vision Language Models (VLMs) have demonstrated impressive capabilities in\nintegrating visual and textual information for understanding and reasoning, but\nremain highly vulnerable to adversarial attacks. While activation steering has\nemerged as a promising defence, existing approaches often rely on task-specific\ncontrastive prompts to extract harmful directions, which exhibit suboptimal\nperformance and can degrade visual grounding performance. To address these\nlimitations, we propose \\textit{Sequence-Level Preference Optimization} for VLM\n(\\textit{SPO-VLM}), a novel two-stage defense framework that combines\nactivation-level intervention with policy-level optimization to enhance model\nrobustness. In \\textit{Stage I}, we compute adaptive layer-specific steering\nvectors from diverse data sources, enabling generalized suppression of harmful\nbehaviors during inference. In \\textit{Stage II}, we refine these steering\nvectors through a sequence-level preference optimization process. This stage\nintegrates automated toxicity assessment, as well as visual-consistency rewards\nbased on caption-image alignment, to achieve safe and semantically grounded\ntext generation. The two-stage structure of SPO-VLM balances efficiency and\neffectiveness by combining a lightweight mitigation foundation in Stage I with\ndeeper policy refinement in Stage II. Extensive experiments shown SPO-VLM\nenhances safety against attacks via activation steering and preference\noptimization, while maintaining strong performance on benign tasks without\ncompromising visual understanding capabilities. We will release our code, model\nweights, and evaluation toolkit to support reproducibility and future research.\n\\textcolor{red}{Warning: This paper may contain examples of offensive or\nharmful text and images.}"
    },
    {
        "date": "2025-08",
        "title": "MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification",
        "author": "Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, and Jia Wu",
        "link": "http://arxiv.org/abs/2509.00311v1",
        "abstract": "Domain generalization in computational histopathology is hindered by\nheterogeneity in whole slide images (WSIs), caused by variations in tissue\npreparation, staining, and imaging conditions across institutions. Unlike\nmachine learning systems, pathologists rely on domain-invariant morphological\ncues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,\nchromatin texture, spatial disorganization), structural atypia (abnormal\narchitecture and gland formation), and overall morphological atypia that remain\ndiagnostic across diverse settings. Motivated by this, we hypothesize that\nexplicitly modeling biologically robust nuclear morphology and spatial\norganization will enable the learning of cancer representations that are\nresilient to domain shifts. We propose MorphGen (Morphology-Guided\nGeneralization), a method that integrates histopathology images, augmentations,\nand nuclear segmentation masks within a supervised contrastive learning\nframework. By aligning latent representations of images and nuclear masks,\nMorphGen prioritizes diagnostic features such as nuclear and morphological\natypia and spatial organization over staining artifacts and domain-specific\nfeatures. To further enhance out-of-distribution robustness, we incorporate\nstochastic weight averaging (SWA), steering optimization toward flatter minima.\nAttention map analyses revealed that MorphGen primarily relies on nuclear\nmorphology, cellular composition, and spatial cell organization within tumors\nor normal regions for final classification. Finally, we demonstrate resilience\nof the learned representations to image corruptions (such as staining\nartifacts) and adversarial attacks, showcasing not only OOD generalization but\nalso addressing critical vulnerabilities in current deep learning systems for\ndigital pathology. Code, datasets, and trained models are available at:\nhttps://github.com/hikmatkhan/MorphGen"
    },
    {
        "date": "2025-08",
        "title": "A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report",
        "author": "Qishen Sam Liang",
        "link": "http://arxiv.org/abs/2509.00266v1",
        "abstract": "Academic and research Cyber Infrastructures (CI) present unique security\nchallenges due to their collaborative nature, heterogeneous components, and the\nlack of practical, tailored security assessment frameworks. Existing standards\ncan be too generic or complex for CI administrators to apply effectively. This\nreport introduces a systematic, mission-centric approach to estimate and\nanalyze the security posture of a CI. The framework guides administrators\nthrough a top-down process: (1) defining unacceptable losses and security\nmissions, (2) identifying associated system hazards and critical assets, and\n(3) modeling the CI's components and their relationships as a security\nknowledge graph. The core of this methodology is the construction of directed\nattack graphs, which systematically map all potential paths an adversary could\ntake from an entry point to a critical asset. By visualizing these attack paths\nalongside defense mechanisms, the framework provides a clear, comprehensive\noverview of the system's vulnerabilities and security gaps. This structured\napproach enables CI operators to proactively assess risks, prioritize\nmitigation strategies, and make informed, actionable decisions to strengthen\nthe overall security posture of the CI."
    },
    {
        "date": "2025-08",
        "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction",
        "author": "Xiaoyang Wang, and Christopher C. Yang",
        "link": "http://arxiv.org/abs/2508.21793v1",
        "abstract": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability."
    },
    {
        "date": "2025-08",
        "title": "Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling",
        "author": "Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, and Lunhong Dong",
        "link": "http://arxiv.org/abs/2508.21785v1",
        "abstract": "Heart rate prediction is vital for personalized health monitoring and\nfitness, while it frequently faces a critical challenge when deploying in\nreal-world: data heterogeneity. We classify it in two key dimensions: source\nheterogeneity from fragmented device markets with varying feature sets, and\nuser heterogeneity reflecting distinct physiological patterns across\nindividuals and activities. Existing methods either discard device-specific\ninformation, or fail to model user-specific differences, limiting their\nreal-world performance. To address this, we propose a framework that learns\nlatent representations agnostic to both heterogeneity, enabling downstream\npredictors to work consistently under heterogeneous data patterns.\nSpecifically, we introduce a random feature dropout strategy to handle source\nheterogeneity, making the model robust to various feature sets. To manage user\nheterogeneity, we employ a time-aware attention module to capture long-term\nphysiological traits and use a contrastive learning objective to build a\ndiscriminative representation space. To reflect the heterogeneous nature of\nreal-world data, we created and publicly released a new benchmark dataset,\nParroTao. Evaluations on both ParroTao and the public FitRec dataset show that\nour model significantly outperforms existing baselines by 17% and 15%,\nrespectively. Furthermore, analysis of the learned representations demonstrates\ntheir strong discriminative power, and one downstream application task confirm\nthe practical value of our model."
    },
    {
        "date": "2025-08",
        "title": "RF-DETR for Robust Mitotic Figure Detection: A MIDOG 2025 Track 1 Approach",
        "author": "Piotr Giedziun, Jan So\u0142tysik, Mateusz G\u00f3rczany, Norbert Ropiak, Marcin Przymus, Piotr Krajewski, Jaros\u0142aw Kwiecie\u0144, Artur Bartczak, Izabela Wasiak, and Mateusz Maniewski",
        "link": "http://arxiv.org/abs/2509.02599v1",
        "abstract": "Mitotic figure detection in histopathology images remains challenging due to\nsignificant domain shifts across different scanners, staining protocols, and\ntissue types. This paper presents our approach for the MIDOG 2025 challenge\nTrack 1, focusing on robust mitotic figure detection across diverse\nhistological contexts. While we initially planned a two-stage approach\ncombining high-recall detection with subsequent classification refinement, time\nconstraints led us to focus on optimizing a single-stage detection pipeline. We\nemployed RF-DETR (Roboflow Detection Transformer) with hard negative mining,\ntrained on MIDOG++ dataset. On the preliminary test set, our method achieved an\nF1 score of 0.789 with a recall of 0.839 and precision of 0.746, demonstrating\neffective generalization across unseen domains. The proposed solution offers\ninsights into the importance of training data balance and hard negative mining\nfor addressing domain shift challenges in mitotic figure detection."
    },
    {
        "date": "2025-08",
        "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization",
        "author": "Jiazheng Xing, Hai Ci, Hongbin Xu, Hangjie Yuan, Yong Liu, and Mike Zheng Shou",
        "link": "http://arxiv.org/abs/2508.21727v1",
        "abstract": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks."
    },
    {
        "date": "2025-08",
        "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
        "author": "Daryna Oliynyk, Rudolf Mayer, Kathrin Grosse, and Andreas Rauber",
        "link": "http://arxiv.org/abs/2508.21654v1",
        "abstract": "Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks."
    },
    {
        "date": "2025-08",
        "title": "Detecting Stealthy Data Poisoning Attacks in AI Code Generators",
        "author": "Cristina Improta",
        "link": "http://arxiv.org/abs/2508.21636v1",
        "abstract": "Deep learning (DL) models for natural language-to-code generation have become\nintegral to modern software development pipelines. However, their heavy\nreliance on large amounts of data, often collected from unsanitized online\nsources, exposes them to data poisoning attacks, where adversaries inject\nmalicious samples to subtly bias model behavior. Recent targeted attacks\nsilently replace secure code with semantically equivalent but vulnerable\nimplementations without relying on explicit triggers to launch the attack,\nmaking it especially hard for detection methods to distinguish clean from\npoisoned samples. We present a systematic study on the effectiveness of\nexisting poisoning detection methods under this stealthy threat model.\nSpecifically, we perform targeted poisoning on three DL models (CodeBERT,\nCodeT5+, AST-T5), and evaluate spectral signatures analysis, activation\nclustering, and static analysis as defenses. Our results show that all methods\nstruggle to detect triggerless poisoning, with representation-based approaches\nfailing to isolate poisoned samples and static analysis suffering false\npositives and false negatives, highlighting the need for more robust,\ntrigger-independent defenses for AI-assisted code generation."
    },
    {
        "date": "2025-08",
        "title": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs",
        "author": "Nishant Chinnasami, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2508.21606v1",
        "abstract": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced."
    },
    {
        "date": "2025-08",
        "title": "OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories",
        "author": "Bo Li, Yingqi Feng, Ming Jin, Xin Zheng, Yufei Tang, Laurent Cherubin, Alan Wee-Chung Liew, Can Wang, Qinghua Lu, Jingwei Yao, Shirui Pan, Hong Zhang, and Xingquan Zhu",
        "link": "http://arxiv.org/abs/2508.21570v1",
        "abstract": "Ocean salinity plays a vital role in circulation, climate, and marine\necosystems, yet its measurement is often sparse, irregular, and noisy,\nespecially in drifter-based datasets. Traditional approaches, such as remote\nsensing and optimal interpolation, rely on linearity and stationarity, and are\nlimited by cloud cover, sensor drift, and low satellite revisit rates. While\nmachine learning models offer flexibility, they often fail under severe\nsparsity and lack principled ways to incorporate physical covariates without\nspecialized sensors. In this paper, we introduce the OceAn Salinity Imputation\nSystem (OASIS), a novel diffusion adversarial framework designed to address\nthese challenges."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Patch Attack for Ship Detection via Localized Augmentation",
        "author": "Chun Liu, Panpan Ding, Zheng Zheng, Hailong Wang, Bingqian Zhu, Tao Xu, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2508.21472v1",
        "abstract": "Current ship detection techniques based on remote sensing imagery primarily\nrely on the object detection capabilities of deep neural networks (DNNs).\nHowever, DNNs are vulnerable to adversarial patch attacks, which can lead to\nmisclassification by the detection model or complete evasion of the targets.\nNumerous studies have demonstrated that data transformation-based methods can\nimprove the transferability of adversarial examples. However, excessive\naugmentation of image backgrounds or irrelevant regions may introduce\nunnecessary interference, resulting in false detections of the object detection\nmodel. These errors are not caused by the adversarial patches themselves but\nrather by the over-augmentation of background and non-target areas. This paper\nproposes a localized augmentation method that applies augmentation only to the\ntarget regions, avoiding any influence on non-target areas. By reducing\nbackground interference, this approach enables the loss function to focus more\ndirectly on the impact of the adversarial patch on the detection model, thereby\nimproving the attack success rate. Experiments conducted on the HRSC2016\ndataset demonstrate that the proposed method effectively increases the success\nrate of adversarial patch attacks and enhances their transferability."
    },
    {
        "date": "2025-08",
        "title": "Robust Pan-Cancer Mitotic Figure Detection with YOLOv12",
        "author": "Rapha\u00ebl Bourgade, Guillaume Balezo, and Thomas Walter",
        "link": "http://arxiv.org/abs/2509.02593v1",
        "abstract": "Mitotic figures represent a key histoprognostic feature in tumor pathology,\nproviding crucial insights into tumor aggressiveness and proliferation.\nHowever, their identification remains challenging, subject to significant\ninter-observer variability, even among experienced pathologists. To address\nthis issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the\nthird edition of an international competition aiming to develop robust mitosis\ndetection algorithms. In this paper, we present a mitotic figures detection\napproach based on the YOLOv12 object detection architecture, achieving a\n$F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge,\nwithout relying on external data."
    },
    {
        "date": "2025-08",
        "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs",
        "author": "Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2508.21393v2",
        "abstract": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."
    },
    {
        "date": "2025-08",
        "title": "Risks and Compliance with the EU's Core Cyber Security Legislation",
        "author": "Jukka Ruohonen, Jesper L\u00f8ffler Nielsen, and Jakub Sk\u00f3rczynski",
        "link": "http://arxiv.org/abs/2508.21386v1",
        "abstract": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it."
    },
    {
        "date": "2025-08",
        "title": "DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks",
        "author": "Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, and Yining Wang",
        "link": "http://arxiv.org/abs/2508.21340v1",
        "abstract": "Time series synthesis is an effective approach to ensuring the secure\ncirculation of time series data. Existing time series synthesis methods\ntypically perform temporal modeling based on random sequences to generate\ntarget sequences, which often struggle to ensure the temporal dependencies in\nthe generated time series. Additionally, directly modeling temporal features on\nrandom sequences makes it challenging to accurately capture the feature\ninformation of the original time series. To address the above issues, we\npropose a simple but effective generative model \\textbf{D}ual-\\textbf{L}ayer\n\\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etworks, named\n\\textbf{DLGAN}. The model decomposes the time series generation process into\ntwo stages: sequence feature extraction and sequence reconstruction. First,\nthese two stages form a complete time series autoencoder, enabling supervised\nlearning on the original time series to ensure that the reconstruction process\ncan restore the temporal dependencies of the sequence. Second, a Generative\nAdversarial Network (GAN) is used to generate synthetic feature vectors that\nalign with the real-time sequence feature vectors, ensuring that the generator\ncan capture the temporal features from real time series. Extensive experiments\non four public datasets demonstrate the superiority of this model across\nvarious evaluation metrics."
    },
    {
        "date": "2025-08",
        "title": "Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning",
        "author": "Hunter Gittlin",
        "link": "http://arxiv.org/abs/2509.02592v1",
        "abstract": "Class imbalance remains a fundamental challenge in machine learning, with\ntraditional solutions often creating as many problems as they solve. We\ndemonstrate that group-aware threshold calibration--setting different decision\nthresholds for different demographic groups--provides superior robustness\ncompared to synthetic data generation methods. Through extensive experiments,\nwe show that group-specific thresholds achieve 1.5-4% higher balanced accuracy\nthan SMOTE and CT-GAN augmented models while improving worst-group balanced\naccuracy. Unlike single-threshold approaches that apply one cutoff across all\ngroups, our group-aware method optimizes the Pareto frontier between balanced\naccuracy and worst-group balanced accuracy, enabling fine-grained control over\ngroup-level performance. Critically, we find that applying group thresholds to\nsynthetically augmented data yields minimal additional benefit, suggesting\nthese approaches are fundamentally redundant. Our results span seven model\nfamilies including linear, tree-based, instance-based, and boosting methods,\nconfirming that group-aware threshold calibration offers a simpler, more\ninterpretable, and more effective solution to class imbalance."
    },
    {
        "date": "2025-08",
        "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation",
        "author": "A H M Nazmus Sakib, Mahsin Bin Akram, Joseph Spracklen, Sahan Kalutarage, Raveen Wijewickrama, Igor Bilogrevic, and Murtuza Jadliwala",
        "link": "http://arxiv.org/abs/2508.21219v1",
        "abstract": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach",
        "author": "Han Yang, Jian Lan, Yihong Liu, Hinrich Sch\u00fctze, and Thomas Seidl",
        "link": "http://arxiv.org/abs/2508.21206v1",
        "abstract": "Autoregressive language models are vulnerable to orthographic attacks, where\ninput text is perturbed with characters from multilingual alphabets, leading to\nsubstantial performance degradation. This vulnerability primarily stems from\nthe out-of-vocabulary issue inherent in subword tokenizers and their\nembeddings. To address this limitation, we propose a pixel-based generative\nlanguage model that replaces the text-based embeddings with pixel-based\nrepresentations by rendering words as individual images. This design provides\nstronger robustness to noisy inputs, while an extension of compatibility to\nmultilingual text across diverse writing systems. We evaluate the proposed\nmethod on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2\nbenchmark, demonstrating both its resilience to orthographic noise and its\neffectiveness in multilingual settings."
    },
    {
        "date": "2025-08",
        "title": "RARR : Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online",
        "author": "Dong Yoon Lee, Alyssa Weakley, Hui Wei, Blake Brown, Keyana Carrion, and Shijia Pan",
        "link": "http://arxiv.org/abs/2508.21167v1",
        "abstract": "One in four people dementia live alone, leading family members to take on\ncaregiving roles from a distance. Many researchers have developed remote\nmonitoring solutions to lessen caregiving needs; however, limitations remain\nincluding privacy preserving solutions, activity recognition, and model\ngeneralizability to new users and environments. Structural vibration sensor\nsystems are unobtrusive solutions that have been proven to accurately monitor\nhuman information, such as identification and activity recognition, in\ncontrolled settings by sensing surface vibrations generated by activities.\nHowever, when deploying in an end user's home, current solutions require a\nsubstantial amount of labeled data for accurate activity recognition. Our\nscalable solution adapts synthesized data from near-surface acoustic audio to\npretrain a model and allows fine tuning with very limited data in order to\ncreate a robust framework for daily routine tracking."
    },
    {
        "date": "2025-08",
        "title": "Privacy Auditing Synthetic Data Release through Local Likelihood Attacks",
        "author": "Joshua Ward, Chi-Hua Wang, and Guang Cheng",
        "link": "http://arxiv.org/abs/2508.21146v1",
        "abstract": "Auditing the privacy leakage of synthetic data is an important but unresolved\nproblem. Most existing privacy auditing frameworks for synthetic data rely on\nheuristics and unreasonable assumptions to attack the failure modes of\ngenerative models, exhibiting limited capability to describe and detect the\nprivacy exposure of training data through synthetic data release. In this\npaper, we study designing Membership Inference Attacks (MIAs) that specifically\nexploit the observation that tabular generative models tend to significantly\noverfit to certain regions of the training distribution. Here, we propose\nGenerative Likelihood Ratio Attack (Gen-LRA), a novel, computationally\nefficient No-Box MIA that, with no assumption of model knowledge or access,\nformulates its attack by evaluating the influence a test observation has in a\nsurrogate model's estimation of a local likelihood ratio over the synthetic\ndata. Assessed over a comprehensive benchmark spanning diverse datasets, model\narchitectures, and attack parameters, we find that Gen-LRA consistently\ndominates other MIAs for generative models across multiple performance metrics.\nThese results underscore Gen-LRA's effectiveness as a privacy auditing tool for\nthe release of synthetic data, highlighting the significant privacy risks posed\nby generative model overfitting in real-world applications."
    },
    {
        "date": "2025-08",
        "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping",
        "author": "Esha Sadia Nasir, Jiaqi Lv, Mostafa Jahanifar, and Shan E Ahmed Raza",
        "link": "http://arxiv.org/abs/2509.02586v2",
        "abstract": "Automated detection and classification of mitotic figures especially\ndistinguishing atypical from normal remain critical challenges in computational\npathology. We present MitoDetect++, a unified deep learning pipeline designed\nfor the MIDOG 2025 challenge, addressing both mitosis detection and atypical\nmitosis classification. For detection (Track 1), we employ a U-Net-based\nencoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced\nwith attention modules, and trained via combined segmentation losses. For\nclassification (Track 2), we leverage the Virchow2 vision transformer,\nfine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource\nconsumption. To improve generalization and mitigate domain shifts, we integrate\nstrong augmentations, focal loss, and group-aware stratified 5-fold\ncross-validation. At inference, we deploy test-time augmentation (TTA) to boost\nrobustness. Our method achieves a balanced accuracy of 0.892 across validation\ndomains, highlighting its clinical applicability and scalability across tasks."
    },
    {
        "date": "2025-08",
        "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
        "author": "Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, and Qinglin Lu",
        "link": "http://arxiv.org/abs/2508.21019v1",
        "abstract": "The field of video diffusion generation faces critical bottlenecks in\nsampling efficiency, especially for large-scale models and long sequences.\nExisting video acceleration methods adopt image-based techniques but suffer\nfrom fundamental limitations: they neither model the temporal coherence of\nvideo frames nor provide single-step distillation for large-scale video models.\nTo bridge this gap, we propose POSE (Phased One-Step Equilibrium), a\ndistillation framework that reduces the sampling steps of large-scale video\ndiffusion models, enabling the generation of high-quality videos in a single\nstep. POSE employs a carefully designed two-phase process to distill video\nmodels:(i) stability priming: a warm-up mechanism to stabilize adversarial\ndistillation that adapts the high-quality trajectory of the one-step generator\nfrom high to low signal-to-noise ratio regimes, optimizing the video quality of\nsingle-step mappings near the endpoints of flow trajectories. (ii) unified\nadversarial equilibrium: a flexible self-adversarial distillation mechanism\nthat promotes stable single-step adversarial training towards a Nash\nequilibrium within the Gaussian noise space, generating realistic single-step\nvideos close to real videos. For conditional video generation, we propose (iii)\nconditional adversarial consistency, a method to improve both semantic\nconsistency and frame consistency between conditional frames and generated\nframes. Comprehensive experiments demonstrate that POSE outperforms other\nacceleration methods on VBench-I2V by average 7.15% in semantic alignment,\ntemporal conference and frame quality, reducing the latency of the pre-trained\nmodel by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining\ncompetitive performance."
    },
    {
        "date": "2025-08",
        "title": "Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System",
        "author": "Hashim Ali, Surya Subramani, Lekha Bollinani, Nithin Sai Adupa, Sali El-Loh, and Hafiz Malik",
        "link": "http://arxiv.org/abs/2508.20983v1",
        "abstract": "The SAFE Challenge evaluates synthetic speech detection across three tasks:\nunmodified audio, processed audio with compression artifacts, and laundered\naudio designed to evade detection. We systematically explore self-supervised\nlearning (SSL) front-ends, training data compositions, and audio length\nconfigurations for robust deepfake detection. Our AASIST-based approach\nincorporates WavLM large frontend with RawBoost augmentation, trained on a\nmultilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS\nsystems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.\nThrough extensive experimentation with different SSL front-ends, three training\ndata versions, and two audio lengths, we achieved second place in both Task 1\n(unmodified audio detection) and Task 3 (laundered audio detection),\ndemonstrating strong generalization and robustness."
    },
    {
        "date": "2025-08",
        "title": "Learning Robust Spatial Representations from Binaural Audio through Feature Distillation",
        "author": "Holger Severin Bovbjerg, Jan \u00d8stergaard, Jesper Jensen, Shinji Watanabe, and Zheng-Hua Tan",
        "link": "http://arxiv.org/abs/2508.20914v1",
        "abstract": "Recently, deep representation learning has shown strong performance in\nmultiple audio tasks. However, its use for learning spatial representations\nfrom multichannel audio is underexplored. We investigate the use of a\npretraining stage based on feature distillation to learn a robust spatial\nrepresentation of binaural speech without the need for data labels. In this\nframework, spatial features are computed from clean binaural speech samples to\nform prediction labels. These clean features are then predicted from\ncorresponding augmented speech using a neural network. After pretraining, we\nthrow away the spatial feature predictor and use the learned encoder weights to\ninitialize a DoA estimation model which we fine-tune for DoA estimation. Our\nexperiments demonstrate that the pretrained models show improved performance in\nnoisy and reverberant environments after fine-tuning for direction-of-arrival\nestimation, when compared to fully supervised models and classic signal\nprocessing methods."
    },
    {
        "date": "2025-08",
        "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
        "author": "Huong Ngo, Matt Deitke, Martijn Bartelds, Sarah Pratt, Josh Gardner, Matt Jordan, and Ludwig Schmidt",
        "link": "http://arxiv.org/abs/2508.20869v1",
        "abstract": "Improvements in training data scale and quality have led to significant\nadvances, yet its influence in speech recognition remains underexplored. In\nthis paper, we present a large-scale dataset, OLMoASR-Pool, and series of\nmodels, OLMoASR, to study and develop robust zero-shot speech recognition\nmodels. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio\nand 17M transcripts, we design text heuristic filters to remove low-quality or\nmistranscribed data. Our curation pipeline produces a new dataset containing 1M\nhours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use\nOLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M\n(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR\nachieves comparable average performance to OpenAI's Whisper on short and\nlong-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a\n12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest\nEnglish-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and\nlong-form recognition respectively (at equivalent parameter count).\nOLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will\nbe made publicly available to further research on robust speech processing."
    },
    {
        "date": "2025-08",
        "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
        "author": "Matteo Gioele Collu, Umberto Salviati, Roberto Confalonieri, Mauro Conti, and Giovanni Apruzzese",
        "link": "http://arxiv.org/abs/2508.20863v2",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
    },
    {
        "date": "2025-08",
        "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning",
        "author": "Dongjun Lee, Changho Hwang, and Kimin Lee",
        "link": "http://arxiv.org/abs/2508.21107v1",
        "abstract": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task."
    },
    {
        "date": "2025-08",
        "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning",
        "author": "He Li, Xinyu Liu, Weihang Kong, and Xingchen Zhang",
        "link": "http://arxiv.org/abs/2508.20817v2",
        "abstract": "Visible and infrared image fusion (VIF) is an important multimedia task in\ncomputer vision. Most VIF methods focus primarily on optimizing fused image\nquality. Recent studies have begun incorporating downstream tasks, such as\nsemantic segmentation and object detection, to provide semantic guidance for\nVIF. However, semantic segmentation requires extensive annotations, while\nobject detection, despite reducing annotation efforts compared with\nsegmentation, faces challenges in highly crowded scenes due to overlapping\nbounding boxes and occlusion. Moreover, although RGB-T crowd counting has\ngained increasing attention in recent years, no studies have integrated VIF and\ncrowd counting into a unified framework. To address these challenges, we\npropose FusionCounting, a novel multi-task learning framework that integrates\ncrowd counting into the VIF process. Crowd counting provides a direct\nquantitative measure of population density with minimal annotation, making it\nparticularly suitable for dense scenes. Our framework leverages both input\nimages and population density information in a mutually beneficial multi-task\ndesign. To accelerate convergence and balance tasks contributions, we introduce\na dynamic loss function weighting strategy. Furthermore, we incorporate\nadversarial training to enhance the robustness of both VIF and crowd counting,\nimproving the model's stability and resilience to adversarial attacks.\nExperimental results on public datasets demonstrate that FusionCounting not\nonly enhances image fusion quality but also achieves superior crowd counting\nperformance."
    },
    {
        "date": "2025-08",
        "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
        "author": "Yifan Zhang",
        "link": "http://arxiv.org/abs/2508.20784v1",
        "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced."
    },
    {
        "date": "2025-08",
        "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
        "author": "Jan Erik van Woerden, Gertjan Burghouts, Lotte Nijskens, Alma M. Liezenga, Sabina van Rooij, Frank Ruis, and Hugo J. Kuijf",
        "link": "http://arxiv.org/abs/2508.20760v2",
        "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP."
    },
    {
        "date": "2025-08",
        "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
        "author": "Stefano Fumero, Kai Huang, Matteo Boffa, Danilo Giordano, Marco Mellia, Zied Ben Houidi, and Dario Rossi",
        "link": "http://arxiv.org/abs/2508.20643v1",
        "abstract": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents."
    },
    {
        "date": "2025-08",
        "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications",
        "author": "Immanuel Ro\u00dfteutscher, Klaus S. Drese, and Thorsten Uphues",
        "link": "http://arxiv.org/abs/2508.20622v1",
        "abstract": "We investigated the adaptation and performance of Masked Autoencoders (MAEs)\nwith Vision Transformer (ViT) architectures for self-supervised representation\nlearning on one-dimensional (1D) ultrasound signals. Although MAEs have\ndemonstrated significant success in computer vision and other domains, their\nuse for 1D signal analysis, especially for raw ultrasound data, remains largely\nunexplored. Ultrasound signals are vital in industrial applications such as\nnon-destructive testing (NDT) and structural health monitoring (SHM), where\nlabeled data are often scarce and signal processing is highly task-specific. We\npropose an approach that leverages MAE to pre-train on unlabeled synthetic\nultrasound signals, enabling the model to learn robust representations that\nenhance performance in downstream tasks, such as time-of-flight (ToF)\nclassification. This study systematically investigated the impact of model\nsize, patch size, and masking ratio on pre-training efficiency and downstream\naccuracy. Our results show that pre-trained models significantly outperform\nmodels trained from scratch and strong convolutional neural network (CNN)\nbaselines optimized for the downstream task. Additionally, pre-training on\nsynthetic data demonstrates superior transferability to real-world measured\nsignals compared with training solely on limited real datasets. This study\nunderscores the potential of MAEs for advancing ultrasound signal analysis\nthrough scalable, self-supervised learning."
    },
    {
        "date": "2025-08",
        "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification",
        "author": "Smriti Joshi, Lidia Garrucho, Richard Osuala, Oliver Diaz, and Karim Lekadir",
        "link": "http://arxiv.org/abs/2508.20621v1",
        "abstract": "Breast cancer is one of the leading causes of cancer-related mortality in\nwomen, and early detection is essential for improving outcomes. Magnetic\nresonance imaging (MRI) is a highly sensitive tool for breast cancer detection,\nparticularly in women at high risk or with dense breast tissue, where\nmammography is less effective. The ODELIA consortium organized a multi-center\nchallenge to foster AI-based solutions for breast cancer diagnosis and\nclassification. The dataset included 511 studies from six European centers,\nacquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study\nwas labeled for the left and right breast as no lesion, benign lesion, or\nmalignant lesion. We developed a SwinUNETR-based deep learning framework that\nincorporates breast region masking, extensive data augmentation, and ensemble\nlearning to improve robustness and generalizability. Our method achieved second\nplace on the challenge leaderboard, highlighting its potential to support\nclinical breast MRI interpretation. We publicly share our codebase at\nhttps://github.com/smriti-joshi/bcnaim-odelia-challenge.git."
    },
    {
        "date": "2025-08",
        "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization",
        "author": "Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, and Ke Xu",
        "link": "http://arxiv.org/abs/2508.20613v1",
        "abstract": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\nof Split Inference (SI), a collaborative paradigm that partitions computation\nbetween edge devices and the cloud to reduce latency and protect user privacy.\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\nintermediate features exchanged in SI can be exploited to recover sensitive\ninput data, posing significant privacy risks. Existing DRAs are typically\neffective only on shallow models and fail to fully leverage semantic priors,\nlimiting their reconstruction quality and generalizability across datasets and\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\nwith Progressive Feature Optimization (PFO), which decomposes the generator\ninto hierarchical blocks and incrementally refines intermediate representations\nto enhance the semantic fidelity of reconstructed images. To stabilize the\noptimization and improve image realism, we introduce an L1-ball constraint\nduring reconstruction. Extensive experiments show that our method outperforms\nprior attacks by a large margin, especially in high-resolution scenarios,\nout-of-distribution settings, and against deeper and more complex DNNs."
    },
    {
        "date": "2025-08",
        "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations",
        "author": "Mengxiao Huang, Minglei Shu, Shuwang Zhou, and Zhaoyang Liu",
        "link": "http://arxiv.org/abs/2508.20595v1",
        "abstract": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses\nsignificant risks to privacy and societal security. Existing detection methods\nare predominantly passive, focusing on post-event analysis without preventing\nattacks. To address this, we propose an active defense method based on\nlow-frequency perceptual perturbations to disrupt face swapping manipulation,\nreducing the performance and naturalness of generated content. Unlike prior\napproaches that used low-frequency perturbations to impact classification\naccuracy,our method directly targets the generative process of deepfake\ntechniques. We combine frequency and spatial domain features to strengthen\ndefenses. By introducing artifacts through low-frequency perturbations while\npreserving high-frequency details, we ensure the output remains visually\nplausible. Additionally, we design a complete architecture featuring an\nencoder, a perturbation generator, and a decoder, leveraging discrete wavelet\ntransform (DWT) to extract low-frequency components and generate perturbations\nthat disrupt facial manipulation models. Experiments on CelebA-HQ and LFW\ndemonstrate significant reductions in face-swapping effectiveness, improved\ndefense success rates, and preservation of visual quality."
    },
    {
        "date": "2025-08",
        "title": "SemSR: Semantics aware robust Session-based Recommendations",
        "author": "Jyoti Narwariya, Priyanka Gupta, Muskan Gupta, Jyotsana Khatri, and Lovekesh Vig",
        "link": "http://arxiv.org/abs/2508.20587v1",
        "abstract": "Session-based recommendation (SR) models aim to recommend items to anonymous\nusers based on their behavior during the current session. While various SR\nmodels in the literature utilize item sequences to predict the next item, they\noften fail to leverage semantic information from item titles or descriptions\nimpeding session intent identification and interpretability. Recent research\nhas explored Large Language Models (LLMs) as promising approaches to enhance\nsession-based recommendations, with both prompt-based and fine-tuning based\nmethods being widely investigated. However, prompt-based methods struggle to\nidentify optimal prompts that elicit correct reasoning and lack task-specific\nfeedback at test time, resulting in sub-optimal recommendations. Fine-tuning\nmethods incorporate domain-specific knowledge but incur significant\ncomputational costs for implementation and maintenance. In this paper, we\npresent multiple approaches to utilize LLMs for session-based recommendation:\n(i) in-context LLMs as recommendation agents, (ii) LLM-generated\nrepresentations for semantic initialization of deep learning SR models, and\n(iii) integration of LLMs with data-driven SR models. Through comprehensive\nexperiments on two real-world publicly available datasets, we demonstrate that\nLLM-based methods excel at coarse-level retrieval (high recall values), while\ntraditional data-driven techniques perform well at fine-grained ranking (high\nMean Reciprocal Rank values). Furthermore, the integration of LLMs with\ndata-driven SR models significantly out performs both standalone LLM approaches\nand data-driven deep learning models, as well as baseline SR models, in terms\nof both Recall and MRR metrics."
    },
    {
        "date": "2025-08",
        "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
        "author": "Lorenz Hufe, Constantin Venhoff, Maximilian Dreyer, Sebastian Lapuschkin, and Wojciech Samek",
        "link": "http://arxiv.org/abs/2508.20570v1",
        "abstract": "Typographic attacks exploit multi-modal systems by injecting text into\nimages, leading to targeted misclassifications, malicious content generation\nand even Vision-Language Model jailbreaks. In this work, we analyze how CLIP\nvision encoders behave under typographic attacks, locating specialized\nattention heads in the latter half of the model's layers that causally extract\nand transmit typographic information to the cls token. Building on these\ninsights, we introduce a method to defend CLIP models against typographic\nattacks by selectively ablating a typographic circuit, consisting of attention\nheads. Without requiring finetuning, our method improves performance by up to\n19.6% on a typographic variant of ImageNet-100, while reducing standard\nImageNet-100 accuracy by less than 1%. Notably, our training-free approach\nremains competitive with current state-of-the-art typographic defenses that\nrely on finetuning. To this end, we release a family of dyslexic CLIP models\nwhich are significantly more robust against typographic attacks. These models\nserve as suitable drop-in replacements for a broad range of safety-critical\napplications, where the risks of text-based manipulation outweigh the utility\nof text recognition."
    },
    {
        "date": "2025-08",
        "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining",
        "author": "Dan Lin, Shunfeng Lu, Ziyan Liu, Jiajing Wu, Junyuan Fang, Kaixin Lin, Bowen Song, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2508.20517v1",
        "abstract": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems."
    }
]