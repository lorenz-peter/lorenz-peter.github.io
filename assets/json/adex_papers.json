[
    {
        "date": "2025-06",
        "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness",
        "author": "Lucas Piper, Arlindo L. Oliveira, and Tiago Marques",
        "link": "http://arxiv.org/abs/2506.03089v1",
        "abstract": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."
    },
    {
        "date": "2025-06",
        "title": "On the Benefits of Accelerated Optimization in Robust and Private Estimation",
        "author": "Laurentiu Andrei Marchis, and Po-Ling Loh",
        "link": "http://arxiv.org/abs/2506.03044v1",
        "abstract": "We study the advantages of accelerated gradient methods, specifically based\non the Frank-Wolfe method and projected gradient descent, for privacy and\nheavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe\nmethod, our technique is based on a tailored learning rate and a uniform lower\nbound on the gradient of the $\\ell_2$-norm over the constraint set. For\naccelerating projected gradient descent, we use the popular variant based on\nNesterov's momentum, and we optimize our objective over $\\mathbb{R}^p$. These\naccelerations reduce iteration complexity, translating into stronger\nstatistical guarantees for empirical and population risk minimization. Our\nanalysis covers three settings: non-random data, random model-free data, and\nparametric models (linear regression and generalized linear models).\nMethodologically, we approach both privacy and robustness based on noisy\ngradients. We ensure differential privacy via the Gaussian mechanism and\nadvanced composition, and we achieve heavy-tailed robustness using a geometric\nmedian-of-means estimator, which also sharpens the dependency on the dimension\nof the covariates. Finally, we compare our rates to existing bounds and\nidentify scenarios where our methods attain optimal convergence."
    },
    {
        "date": "2025-06",
        "title": "On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses",
        "author": "Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul R\u00e9camier, Salah Ghamizi, Maxime Cordy, and Mike Papadakis",
        "link": "http://arxiv.org/abs/2506.02978v1",
        "abstract": "Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage\nin-context learning to achieve strong performance without gradient updates or\nfine-tuning. However, their robustness to adversarial manipulation remains\nlargely unexplored. In this work, we present a comprehensive study of the\nadversarial vulnerabilities of tabular FM, focusing on both their fragility to\ntargeted test-time attacks and their potential misuse as adversarial tools. We\nshow on three benchmarks in finance, cybersecurity and healthcare, that small,\nstructured perturbations to test inputs can significantly degrade prediction\naccuracy, even when training context remain fixed. Additionally, we demonstrate\nthat tabular FM can be repurposed to generate transferable evasion to\nconventional models such as random forests and XGBoost, and on a lesser extent\nto deep tabular models. To improve tabular FM, we formulate the robustification\nproblem as an optimization of the weights (adversarial fine-tuning), or the\ncontext (adversarial in-context learning). We introduce an in-context\nadversarial training strategy that incrementally replaces the context with\nadversarial perturbed instances, without updating model weights. Our approach\nimproves robustness across multiple tabular benchmarks. Together, these\nfindings position tabular FM as both a target and a source of adversarial\nthreats, highlighting the urgent need for robust training and evaluation\npractices in this emerging paradigm."
    },
    {
        "date": "2025-06",
        "title": "ATAG: AI-Agent Application Threat Assessment with Attack Graphs",
        "author": "Parth Atulbhai Gandhi, Akansha Shukla, David Tayouri, Beni Ifland, Yuval Elovici, Rami Puzis, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2506.02859v1",
        "abstract": "Evaluating the security of multi-agent systems (MASs) powered by large\nlanguage models (LLMs) is challenging, primarily because of the systems'\ncomplex internal dynamics and the evolving nature of LLM vulnerabilities.\nTraditional attack graph (AG) methods often lack the specific capabilities to\nmodel attacks on LLMs. This paper introduces AI-agent application Threat\nassessment with Attack Graphs (ATAG), a novel framework designed to\nsystematically analyze the security risks associated with AI-agent\napplications. ATAG extends the MulVAL logic-based AG generation tool with\ncustom facts and interaction rules to accurately represent AI-agent topologies,\nvulnerabilities, and attack scenarios. As part of this research, we also\ncreated the LLM vulnerability database (LVD) to initiate the process of\nstandardizing LLM vulnerabilities documentation. To demonstrate ATAG's\nefficacy, we applied it to two multi-agent applications. Our case studies\ndemonstrated the framework's ability to model and generate AGs for\nsophisticated, multi-step attack scenarios exploiting vulnerabilities such as\nprompt injection, excessive agency, sensitive information disclosure, and\ninsecure output handling across interconnected agents. ATAG is an important\nstep toward a robust methodology and toolset to help understand, visualize, and\nprioritize complex attack paths in multi-agent AI systems (MAASs). It\nfacilitates proactive identification and mitigation of AI-agent threats in\nmulti-agent applications."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection",
        "author": "Luca Maiano, Fabrizio Casadei, and Irene Amerini",
        "link": "http://arxiv.org/abs/2506.02857v1",
        "abstract": "Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications."
    },
    {
        "date": "2025-06",
        "title": "Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings",
        "author": "Houssam Zenati, Bariscan Bozkurt, and Arthur Gretton",
        "link": "http://arxiv.org/abs/2506.02793v1",
        "abstract": "Estimating the distribution of outcomes under counterfactual policies is\ncritical for decision-making in domains such as recommendation, advertising,\nand healthcare. We analyze a novel framework-Counterfactual Policy Mean\nEmbedding (CPME)-that represents the entire counterfactual outcome distribution\nin a reproducing kernel Hilbert space (RKHS), enabling flexible and\nnonparametric distributional off-policy evaluation. We introduce both a plug-in\nestimator and a doubly robust estimator; the latter enjoys improved uniform\nconvergence rates by correcting for bias in both the outcome embedding and\npropensity models. Building on this, we develop a doubly robust kernel test\nstatistic for hypothesis testing, which achieves asymptotic normality and thus\nenables computationally efficient testing and straightforward construction of\nconfidence intervals. Our framework also supports sampling from the\ncounterfactual distribution. Numerical simulations illustrate the practical\nbenefits of CPME over existing methods."
    },
    {
        "date": "2025-06",
        "title": "Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack",
        "author": "Jing Xue, Zhishen Sun, Haishan Ye, Luo Luo, Xiangyu Chang, Ivor Tsang, and Guang Dai",
        "link": "http://arxiv.org/abs/2506.02711v1",
        "abstract": "Membership inference attack (MIA) has become one of the most widely used and\neffective methods for evaluating the privacy risks of machine learning models.\nThese attacks aim to determine whether a specific sample is part of the model's\ntraining set by analyzing the model's output. While traditional membership\ninference attacks focus on leveraging the model's posterior output, such as\nconfidence on the target sample, we propose IMIA, a novel attack strategy that\nutilizes the process of generating adversarial samples to infer membership. We\npropose to infer the member properties of the target sample using the number of\niterations required to generate its adversarial sample. We conduct experiments\nacross multiple models and datasets, and our results demonstrate that the\nnumber of iterations for generating an adversarial sample is a reliable feature\nfor membership inference, achieving strong performance both in black-box and\nwhite-box attack scenarios. This work provides a new perspective for evaluating\nmodel privacy and highlights the potential of adversarial example-based\nfeatures for privacy leakage assessment."
    },
    {
        "date": "2025-06",
        "title": "Poster: FedBlockParadox -- A Framework for Simulating and Securing Decentralized Federated Learning",
        "author": "Gabriele Digregorio, Francesco Bleggi, Federico Caroli, Michele Carminati, Stefano Zanero, and Stefano Longari",
        "link": "http://arxiv.org/abs/2506.02679v1",
        "abstract": "A significant body of research in decentralized federated learning focuses on\ncombining the privacy-preserving properties of federated learning with the\nresilience and transparency offered by blockchain-based systems. While these\napproaches are promising, they often lack flexible tools to evaluate system\nrobustness under adversarial conditions. To fill this gap, we present\nFedBlockParadox, a modular framework for modeling and evaluating decentralized\nfederated learning systems built on blockchain technologies, with a focus on\nresilience against a broad spectrum of adversarial attack scenarios. It\nsupports multiple consensus protocols, validation methods, aggregation\nstrategies, and configurable attack models. By enabling controlled experiments,\nFedBlockParadox provides a valuable resource for researchers developing secure,\ndecentralized learning solutions. The framework is open-source and built to be\nextensible by the community."
    },
    {
        "date": "2025-06",
        "title": "Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection",
        "author": "Tianci Liu, Tong Yang, Quan Zhang, and Qi Lei",
        "link": "http://arxiv.org/abs/2506.02665v1",
        "abstract": "As AI advances, copyrighted content faces growing risk of unauthorized use,\nwhether through model training or direct misuse. Building upon invisible\nadversarial perturbation, recent works developed copyright protections against\nspecific AI techniques such as unauthorized personalization through DreamBooth\nthat are misused. However, these methods offer only short-term security, as\nthey require retraining whenever the underlying model architectures change. To\nestablish long-term protection aiming at better robustness, we go beyond\ninvisible perturbation, and propose a universal approach that embeds\n\\textit{visible} watermarks that are \\textit{hard-to-remove} into images.\nGrounded in a new probabilistic and inverse problem-based formulation, our\nframework maximizes the discrepancy between the \\textit{optimal} reconstruction\nand the original content. We develop an effective and efficient approximation\nalgorithm to circumvent a intractable bi-level optimization. Experimental\nresults demonstrate superiority of our approach across diverse scenarios."
    },
    {
        "date": "2025-06",
        "title": "EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization",
        "author": "Ruibo Duan, Yuxin Liu, Xinyao Dong, and Chenglin Fan",
        "link": "http://arxiv.org/abs/2506.02594v1",
        "abstract": "Generating challenging instances is crucial for the evaluation and\nadvancement of combinatorial optimization solvers. In this work, we introduce\nEALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),\na novel framework that automates the co-evolution of optimization problem\ninstances and their corresponding heuristic solvers using large language models\n(LLMs). EALG leverages a mutation-based adversarial approach that dynamically\nevolves instance generation procedures to create increasingly difficult\nproblems, while simultaneously synthesizing adaptive heuristic algorithms\nthrough interactions with LLMs guided by algorithmic structure. Unlike existing\napproaches that focus solely on static benchmark creation or manual solver\ndesign, EALG provides a seamless pipeline from instance generation to solver\nsynthesis. Experimental results demonstrate that EALG generates significantly\nharder instances than current benchmarks, and its synthesized solvers\ngeneralize effectively across a broad spectrum of combinatorial tasks. This\nwork explores a new paradigm for combinatorial optimization that integrates\ninstance generation with solver design, resulting in state-of-the-art\nperformance."
    },
    {
        "date": "2025-06",
        "title": "VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning",
        "author": "Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, and Vishal Chowdhary",
        "link": "http://arxiv.org/abs/2506.02539v1",
        "abstract": "Continual memory augmentation allows computer-use agents (CUAs) to learn from\npast interactions and refine their task-solving strategies over time. However,\nunchecked memory accumulation can introduce spurious or hallucinated\n\"learnings\" that degrade agent performance, particularly in domain-specific\nworkflows such as productivity software. We present a novel framework,\nVerificAgent, that effectively manages memory for CUAs through (1) an\nexpert-curated seed of domain knowledge, (2) iterative, trajectory-based memory\nrefinement during training, and (3) a post-hoc fact-checking pass by human\nexperts to sanitize accumulated memory before deployment. On OSWorld\nproductivity tasks, VerificAgent achieves a 111.1% relative improvement in\nsuccess rate over baseline CUA without any additional fine-tuning."
    },
    {
        "date": "2025-06",
        "title": "VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents",
        "author": "Tri Cao, Bennett Lim, Yue Liu, Yuan Sui, Yuexin Li, Shumin Deng, Lin Lu, Nay Oo, Shuicheng Yan, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2506.02456v1",
        "abstract": "Computer-Use Agents (CUAs) with full system access enable powerful task\nautomation but pose significant security and privacy risks due to their ability\nto manipulate files, access user data, and execute arbitrary commands. While\nprior work has focused on browser-based agents and HTML-level attacks, the\nvulnerabilities of CUAs remain underexplored. In this paper, we investigate\nVisual Prompt Injection (VPI) attacks, where malicious instructions are\nvisually embedded within rendered user interfaces, and examine their impact on\nboth CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of\n306 test cases across five widely used platforms, to evaluate agent robustness\nunder VPI threats. Each test case is a variant of a web platform, designed to\nbe interactive, deployed in a realistic environment, and containing a visually\nembedded malicious prompt. Our empirical study shows that current CUAs and BUAs\ncan be deceived at rates of up to 51% and 100%, respectively, on certain\nplatforms. The experimental results also indicate that system prompt defenses\noffer only limited improvements. These findings highlight the need for robust,\ncontext-aware defenses to ensure the safe deployment of multimodal AI agents in\nreal-world environments. The code and dataset are available at:\nhttps://github.com/cua-framework/agents"
    },
    {
        "date": "2025-06",
        "title": "AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting",
        "author": "Karthikeyan Vaiapury",
        "link": "http://arxiv.org/abs/2506.02415v1",
        "abstract": "Optimization remains a fundamental pillar of machine learning, yet existing\nmethods often struggle to maintain stability and adaptability in dynamic, non\nlinear systems, especially under uncertainty. We introduce AERO (Adversarial\nEnergy-based Redirection Optimization), a novel framework inspired by the\nredirection principle in Judo, where external disturbances are leveraged rather\nthan resisted. AERO reimagines optimization as a redirection process guided by\n15 interrelated axioms encompassing adversarial correction, energy\nconservation, and disturbance-aware learning. By projecting gradients,\nintegrating uncertainty driven dynamics, and managing learning energy, AERO\noffers a principled approach to stable and robust model updates. Applied to\nprobabilistic solar energy forecasting, AERO demonstrates substantial gains in\npredictive accuracy, reliability, and adaptability, especially in noisy and\nuncertain environments. Our findings highlight AERO as a compelling new\ndirection in the theoretical and practical landscape of optimization."
    },
    {
        "date": "2025-06",
        "title": "GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure",
        "author": "Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, and Guoyin Wang",
        "link": "http://arxiv.org/abs/2506.02390v1",
        "abstract": "Adaptive Boosting (AdaBoost) faces significant challenges posed by label\nnoise, especially in multiclass classification tasks. Existing methods either\nlack mechanisms to handle label noise effectively or suffer from high\ncomputational costs due to redundant data usage. Inspired by granular\ncomputing, this paper proposes granular adaptive boosting (GAdaBoost), a novel\ntwo-stage framework comprising a data granulation stage and an adaptive\nboosting stage, to enhance efficiency and robustness under noisy conditions. To\nvalidate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is\nproposed. Specifically, first, a granular-ball generation method is designed to\ncompress data while preserving diversity and mitigating label noise. Second,\nthe granular ball-based SAMME algorithm focuses on granular balls rather than\nindividual samples, improving efficiency and reducing sensitivity to noise.\nExperimental results on some noisy datasets show that the proposed approach\nachieves superior robustness and efficiency compared with existing methods,\ndemonstrating that this work effectively extends AdaBoost and SAMME."
    },
    {
        "date": "2025-06",
        "title": "Exploring Explanations Improves the Robustness of In-Context Learning",
        "author": "Ukyo Honda, and Tatsushi Oka",
        "link": "http://arxiv.org/abs/2506.02378v1",
        "abstract": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches."
    },
    {
        "date": "2025-06",
        "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos",
        "author": "Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, and Klara Nahrstedt",
        "link": "http://arxiv.org/abs/2506.02167v1",
        "abstract": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset."
    },
    {
        "date": "2025-06",
        "title": "Mitigating Data Poisoning Attacks to Local Differential Privacy",
        "author": "Xiaolin Li, Ninghui Li, Boyang Wang, and Wenhai Sun",
        "link": "http://arxiv.org/abs/2506.02156v1",
        "abstract": "The distributed nature of local differential privacy (LDP) invites data\npoisoning attacks and poses unforeseen threats to the underlying LDP-supported\napplications. In this paper, we propose a comprehensive mitigation framework\nfor popular frequency estimation, which contains a suite of novel defenses,\nincluding malicious user detection, attack pattern recognition, and damaged\nutility recovery. In addition to existing attacks, we explore new adaptive\nadversarial activities for our mitigation design. For detection, we present a\nnew method to precisely identify bogus reports and thus LDP aggregation can be\nperformed over the ``clean'' data. When the attack behavior becomes stealthy\nand direct filtering out malicious users is difficult, we further propose a\ndetection that can effectively recognize hidden adversarial patterns, thus\nfacilitating the decision-making of service providers. These detection methods\nrequire no additional data and attack information and incur minimal\ncomputational cost. Our experiment demonstrates their excellent performance and\nsubstantial improvement over previous work in various settings. In addition, we\nconduct an empirical analysis of LDP post-processing for corrupted data\nrecovery and propose a new post-processing method, through which we reveal new\ninsights into protocol recommendations in practice and key design principles\nfor future research."
    },
    {
        "date": "2025-06",
        "title": "ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels",
        "author": "Rishi Raj Sahoo, Rucha Bhalchandra Joshi, and Subhankar Mishra",
        "link": "http://arxiv.org/abs/2506.02134v1",
        "abstract": "Graph Neural Networks (GNNs) achieve high performance across many\napplications but function as black-box models, limiting their use in critical\ndomains like healthcare and criminal justice. Explainability methods address\nthis by providing feature-level explanations that identify important node\nattributes for predictions. These explanations create privacy risks. Combined\nwith auxiliary information, feature explanations can enable adversaries to\nreconstruct graph structure, exposing sensitive relationships. Existing graph\nreconstruction attacks assume access to original auxiliary data, but practical\nsystems use differential privacy to protect node features and labels while\nproviding explanations for transparency. We study a threat model where\nadversaries access public feature explanations along with privatized node\nfeatures and labels. We show that existing explanation-based attacks like GSEF\nperform poorly with privatized data due to noise from differential privacy\nmechanisms. We propose ReconXF, a graph reconstruction attack for scenarios\nwith public explanations and privatized auxiliary data. Our method adapts\nexplanation-based frameworks by incorporating denoising mechanisms that handle\ndifferential privacy noise while exploiting structural signals in explanations.\nExperiments across multiple datasets show ReconXF outperforms SoTA methods in\nprivatized settings, with improvements in AUC and average precision. Results\nindicate that public explanations combined with denoising enable graph\nstructure recovery even under the privacy protection of auxiliary data. Code is\navailable at (link to be made public after acceptance)."
    },
    {
        "date": "2025-06",
        "title": "Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent",
        "author": "Yaroslava Lochman, Carl Olsson, and Christopher Zach",
        "link": "http://arxiv.org/abs/2506.01940v1",
        "abstract": "Anisotropic rotation averaging has recently been explored as a natural\nextension of respective isotropic methods. In the anisotropic formulation,\nuncertainties of the estimated relative rotations -- obtained via standard\ntwo-view optimization -- are propagated to the optimization of absolute\nrotations. The resulting semidefinite relaxations are able to recover global\nminima but scale poorly with the problem size. Local methods are fast and also\nadmit robust estimation but are sensitive to initialization. They usually\nemploy minimum spanning trees and therefore suffer from drift accumulation and\ncan get trapped in poor local minima. In this paper, we attempt to bridge the\ngap between optimality, robustness and efficiency of anisotropic rotation\naveraging. We analyze a family of block coordinate descent methods initially\nproposed to optimize the standard chordal distances, and derive a much simpler\nformulation and an anisotropic extension obtaining a fast general solver. We\nintegrate this solver into the extended anisotropic large-scale robust rotation\naveraging pipeline. The resulting algorithm achieves state-of-the-art\nperformance on public structure-from-motion datasets. Project page:\nhttps://ylochman.github.io/acd"
    },
    {
        "date": "2025-06",
        "title": "COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents",
        "author": "Manish Bhatt, Ronald F. Del Rosario, Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2506.01900v1",
        "abstract": "The meteoric rise and proliferation of autonomous Large Language Model (LLM)\nagents promise significant capabilities across various domains. However, their\ndeployment is increasingly constrained by substantial computational demands,\nspecifically for Graphics Processing Unit (GPU) resources. This paper addresses\nthe critical problem of optimizing resource utilization in LLM agent systems.\nWe introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via\nSkill-based Competence Estimation), a novel framework designed to enable\nautonomous LLM agents to dynamically outsource specific subtasks to\nspecialized, cost-effective third-party LLM agents. The framework integrates\nmechanisms for hybrid skill representation, dynamic skill discovery, automated\ntask decomposition, a unified cost model comparing internal execution costs\nagainst external outsourcing prices, simplified market-based decision-making\nalgorithms, and a standardized communication protocol between LLM agents.\nComprehensive validation through 239 theoretical simulations demonstrates\n41.8\\% cost reduction potential, while large-scale empirical validation across\n240 real LLM tasks confirms 20.3\\% cost reduction with proper epsilon-greedy\nexploration, establishing both theoretical viability and practical\neffectiveness. The emergence of proposed open standards like Google's\nAgent2Agent (A2A) protocol further underscores the need for frameworks like\nCOALESCE that can leverage such standards for efficient agent interaction. By\nfacilitating a dynamic market for agent capabilities, potentially utilizing\nprotocols like A2A for communication, COALESCE aims to significantly reduce\noperational costs, enhance system scalability, and foster the emergence of\nspecialized agent economies, making complex LLM agent functionalities more\naccessible and economically viable."
    },
    {
        "date": "2025-06",
        "title": "Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study",
        "author": "Chenyu Wang, Zhou Yang, Yaniv Harel, and David Lo",
        "link": "http://arxiv.org/abs/2506.01825v1",
        "abstract": "Code LLMs are increasingly employed in software development. However, studies\nhave shown that they are vulnerable to backdoor attacks: when a trigger (a\nspecific input pattern) appears in the input, the backdoor will be activated\nand cause the model to generate malicious outputs. Researchers have designed\nvarious triggers and demonstrated the feasibility of implanting backdoors by\npoisoning a fraction of the training data. Some basic conclusions have been\nmade, such as backdoors becoming easier to implant when more training data are\nmodified. However, existing research has not explored other factors influencing\nbackdoor attacks on Code LLMs, such as training batch size, epoch number, and\nthe broader design space for triggers, e.g., trigger length.\n  To bridge this gap, we use code summarization as an example to perform an\nempirical study that systematically investigates the factors affecting backdoor\neffectiveness and understands the extent of the threat posed. Three categories\nof factors are considered: data, model, and inference, revealing previously\noverlooked findings. We find that the prevailing consensus -- that attacks are\nineffective at extremely low poisoning rates -- is incorrect. The absolute\nnumber of poisoned samples matters as well. Specifically, poisoning just 20 out\nof 454K samples (0.004\\% poisoning rate -- far below the minimum setting of\n0.1\\% in prior studies) successfully implants backdoors! Moreover, the common\ndefense is incapable of removing even a single poisoned sample from it.\nAdditionally, small batch sizes increase the risk of backdoor attacks. We also\nuncover other critical factors such as trigger types, trigger length, and the\nrarity of tokens in the triggers, leading to valuable insights for assessing\nCode LLMs' vulnerability to backdoor attacks. Our study highlights the urgent\nneed for defense mechanisms against extremely low poisoning rate settings."
    },
    {
        "date": "2025-06",
        "title": "DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems",
        "author": "Hithem Lamri, Manaar Alam, Haiyan Jiang, and Michail Maniatakos",
        "link": "http://arxiv.org/abs/2506.01777v1",
        "abstract": "Federated Unlearning (FU) enables clients to remove the influence of specific\ndata from a collaboratively trained shared global model, addressing regulatory\nrequirements such as GDPR and CCPA. However, this unlearning process introduces\na new privacy risk: A malicious server may exploit unlearning updates to\nreconstruct the data requested for removal, a form of Data Reconstruction\nAttack (DRA). While DRAs for machine unlearning have been studied extensively\nin centralized Machine Learning-as-a-Service (MLaaS) settings, their\napplicability to FU remains unclear due to the decentralized, client-driven\nnature of FU. This work presents DRAUN, the first attack framework to\nreconstruct unlearned data in FU systems. DRAUN targets optimization-based\nunlearning methods, which are widely adopted for their efficiency. We\ntheoretically demonstrate why existing DRAs targeting machine unlearning in\nMLaaS fail in FU and show how DRAUN overcomes these limitations. We validate\nour approach through extensive experiments on four datasets and four model\narchitectures, evaluating its performance against five popular unlearning\nmethods, effectively demonstrating that state-of-the-art FU methods remain\nvulnerable to DRAs."
    },
    {
        "date": "2025-06",
        "title": "Predictive-CSM: Lightweight Fragment Security for 6LoWPAN IoT Networks",
        "author": "Somayeh Sobati-M",
        "link": "http://arxiv.org/abs/2506.01767v1",
        "abstract": "Fragmentation is a routine part of communication in 6LoWPAN-based IoT\nnetworks,\n  designed to accommodate small frame sizes on constrained wireless links.\nHowever, this process\n  introduces a critical vulnerability fragments are typically stored and\nprocessed before their\n  legitimacy is confirmed, allowing attackers to exploit this gap with minimal\neffort.\n  In this work, we explore a defense strategy that takes a more adaptive,\nbehavior-aware approach to this problem. Our system, called Predictive-CSM,\nintroduces a combination of two\n  lightweight mechanisms. The first tracks how each node behaves over time,\nrewarding consistent\n  and successful interactions while quickly penalizing suspicious or failing\npatterns. The second\n  checks the integrity of packet fragments using a chained hash, allowing\nincomplete or manipulated sequences to be caught early, before they can occupy\nmemory or waste processing time.\n  We put this system to the test using a set of targeted attack simulations,\nincluding early fragment injection, replayed headers, and flooding with fake\ndata. Across all scenarios, Predictive CSM preserved network delivery and\nmaintained energy efficiency, even under pressure. Rather\n  than relying on heavyweight cryptography or rigid filters, this approach\nallows constrained de vices to adapt their defenses in real time based on what\nthey observe, not just what they're\n  told. In that way, it offers a step forward for securing fragmented\ncommunication in real world\n  IoT systems"
    },
    {
        "date": "2025-06",
        "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks",
        "author": "Artun Saday, Ya\u015far Cahit Y\u0131ld\u0131r\u0131m, and Cem Tekin",
        "link": "http://arxiv.org/abs/2506.01625v1",
        "abstract": "We address the problem of Gaussian Process (GP) optimization in the presence\nof unknown and potentially varying adversarial perturbations. Unlike\ntraditional robust optimization approaches that focus on maximizing performance\nunder worst-case scenarios, we consider a robust satisficing objective, where\nthe goal is to consistently achieve a predefined performance threshold $\\tau$,\neven under adversarial conditions. We propose two novel algorithms based on\ndistinct formulations of robust satisficing, and show that they are instances\nof a general robust satisficing framework. Further, each algorithm offers\ndifferent guarantees depending on the nature of the adversary. Specifically, we\nderive two regret bounds: one that is sublinear over time, assuming certain\nconditions on the adversary and the satisficing threshold $\\tau$, and another\nthat scales with the perturbation magnitude but requires no assumptions on the\nadversary. Through extensive experiments, we demonstrate that our approach\noutperforms the established robust optimization methods in achieving the\nsatisficing objective, particularly when the ambiguity set of the robust\noptimization framework is inaccurately specified."
    },
    {
        "date": "2025-06",
        "title": "Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation",
        "author": "Yuan Gan, Jiaxu Miao, Yunze Wang, and Yi Yang",
        "link": "http://arxiv.org/abs/2506.01591v1",
        "abstract": "Advances in talking-head animation based on Latent Diffusion Models (LDM)\nenable the creation of highly realistic, synchronized videos. These fabricated\nvideos are indistinguishable from real ones, increasing the risk of potential\nmisuse for scams, political manipulation, and misinformation. Hence, addressing\nthese ethical concerns has become a pressing issue in AI security. Recent\nproactive defense studies focused on countering LDM-based models by adding\nperturbations to portraits. However, these methods are ineffective at\nprotecting reference portraits from advanced image-to-video animation. The\nlimitations are twofold: 1) they fail to prevent images from being manipulated\nby audio signals, and 2) diffusion-based purification techniques can\neffectively eliminate protective perturbations. To address these challenges, we\npropose Silencer, a two-stage method designed to proactively protect the\nprivacy of portraits. First, a nullifying loss is proposed to ignore audio\ncontrol in talking-head generation. Second, we apply anti-purification loss in\nLDM to optimize the inverted latent feature to generate robust perturbations.\nExtensive experiments demonstrate the effectiveness of Silencer in proactively\nprotecting portrait privacy. We hope this work will raise awareness among the\nAI security community regarding critical ethical issues related to talking-head\ngeneration techniques. Code: https://github.com/yuangan/Silencer."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment",
        "author": "Kaixun Jiang, Zhaoyu Chen, Haijing Guo, Jinglun Li, Jiyuan Fu, Pinxue Guo, Hao Tang, Bo Li, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2506.01511v1",
        "abstract": "Preference alignment in diffusion models has primarily focused on benign\nhuman preferences (e.g., aesthetic). In this paper, we propose a novel\nperspective: framing unrestricted adversarial example generation as a problem\nof aligning with adversary preferences. Unlike benign alignment, adversarial\nalignment involves two inherently conflicting preferences: visual consistency\nand attack effectiveness, which often lead to unstable optimization and reward\nhacking (e.g., reducing visual quality to improve attack success). To address\nthis, we propose APA (Adversary Preferences Alignment), a two-stage framework\nthat decouples conflicting preferences and optimizes each with differentiable\nrewards. In the first stage, APA fine-tunes LoRA to improve visual consistency\nusing rule-based similarity reward. In the second stage, APA updates either the\nimage latent or prompt embedding based on feedback from a substitute\nclassifier, guided by trajectory-level and step-wise rewards. To enhance\nblack-box transferability, we further incorporate a diffusion augmentation\nstrategy. Experiments demonstrate that APA achieves significantly better attack\ntransferability while maintaining high visual consistency, inspiring further\nresearch to approach adversarial attacks from an alignment perspective. Code\nwill be available at https://github.com/deep-kaixun/APA."
    },
    {
        "date": "2025-06",
        "title": "Robust Federated Learning against Noisy Clients via Masked Optimization",
        "author": "Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, and Min Liu",
        "link": "http://arxiv.org/abs/2506.02079v1",
        "abstract": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim ."
    },
    {
        "date": "2025-06",
        "title": "Variance-Based Defense Against Blended Backdoor Attacks",
        "author": "Sujeevan Aseervatham, Achraf Kerzazi, and Youn\u00e8s Bennani",
        "link": "http://arxiv.org/abs/2506.01444v1",
        "abstract": "Backdoor attacks represent a subtle yet effective class of cyberattacks\ntargeting AI models, primarily due to their stealthy nature. The model behaves\nnormally on clean data but exhibits malicious behavior only when the attacker\nembeds a specific trigger into the input. This attack is performed during the\ntraining phase, where the adversary corrupts a small subset of the training\ndata by embedding a pattern and modifying the labels to a chosen target. The\nobjective is to make the model associate the pattern with the target label\nwhile maintaining normal performance on unaltered data. Several defense\nmechanisms have been proposed to sanitize training data-sets. However, these\nmethods often rely on the availability of a clean dataset to compute\nstatistical anomalies, which may not always be feasible in real-world scenarios\nwhere datasets can be unavailable or compromised. To address this limitation,\nwe propose a novel defense method that trains a model on the given dataset,\ndetects poisoned classes, and extracts the critical part of the attack trigger\nbefore identifying the poisoned instances. This approach enhances\nexplainability by explicitly revealing the harmful part of the trigger. The\neffectiveness of our method is demonstrated through experimental evaluations on\nwell-known image datasets and comparative analysis against three\nstate-of-the-art algorithms: SCAn, ABL, and AGPD."
    },
    {
        "date": "2025-06",
        "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation",
        "author": "Kyuyoung Kim, Hyunjun Jeon, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2506.01420v1",
        "abstract": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments."
    },
    {
        "date": "2025-06",
        "title": "Formal Security Analysis of SPV Clients Versus Home-Based Full Nodes in Bitcoin-Derived Systems",
        "author": "Craig Steven Wright",
        "link": "http://arxiv.org/abs/2506.01384v1",
        "abstract": "This paper presents a mathematically rigorous formal analysis of Simplified\nPayment Verification (SPV) clients, as specified in Section 8 of the original\nBitcoin white paper, versus non-mining full nodes operated by home users. It\ndefines security as resistance to divergence from global consensus and models\ntransaction acceptance, enforcement capability, and divergence probability\nunder adversarial conditions. The results demonstrate that SPV clients, despite\nomitting script verification, are cryptographically sufficient under\nhonest-majority assumptions and topologically less vulnerable to attack than\nstructurally passive, non-enforcing full nodes. The paper introduces new axioms\non behavioral divergence and communication topology, proving that home-based\nfull nodes increase systemic entropy without contributing to consensus\nintegrity. Using a series of formally defined lemmas, propositions, and Monte\nCarlo simulation results, it is shown that SPV clients represent the rational\nequilibrium strategy for non-mining participants. This challenges the\nprevailing narrative that home validators enhance network security, providing\nformal and operational justifications for the sufficiency of SPV models."
    },
    {
        "date": "2025-06",
        "title": "TimeGraph: Synthetic Benchmark Datasets for Robust Time-Series Causal Discovery",
        "author": "Muhammad Hasan Ferdous, Emam Hossain, and Md Osman Gani",
        "link": "http://arxiv.org/abs/2506.01361v1",
        "abstract": "Robust causal discovery in time series datasets depends on reliable benchmark\ndatasets with known ground-truth causal relationships. However, such datasets\nremain scarce, and existing synthetic alternatives often overlook critical\ntemporal properties inherent in real-world data, including nonstationarity\ndriven by trends and seasonality, irregular sampling intervals, and the\npresence of unobserved confounders. To address these challenges, we introduce\nTimeGraph, a comprehensive suite of synthetic time-series benchmark datasets\nthat systematically incorporates both linear and nonlinear dependencies while\nmodeling key temporal characteristics such as trends, seasonal effects, and\nheterogeneous noise patterns. Each dataset is accompanied by a fully specified\ncausal graph featuring varying densities and diverse noise distributions and is\nprovided in two versions: one including unobserved confounders and one without,\nthereby offering extensive coverage of real-world complexity while preserving\nmethodological neutrality. We further demonstrate the utility of TimeGraph\nthrough systematic evaluations of state-of-the-art causal discovery algorithms\nincluding PCMCI+, LPCMCI, and FGES across a diverse array of configurations and\nmetrics. Our experiments reveal significant variations in algorithmic\nperformance under realistic temporal conditions, underscoring the need for\nrobust synthetic benchmarks in the fair and transparent assessment of causal\ndiscovery methods. The complete TimeGraph suite, including dataset generation\nscripts, evaluation metrics, and recommended experimental protocols, is freely\navailable to facilitate reproducible research and foster community-driven\nadvancements in time-series causal discovery."
    },
    {
        "date": "2025-06",
        "title": "Distributionally Robust Learning in Survival Analysis",
        "author": "Yeping Jin, Lauren Wise, and Ioannis Paschalidis",
        "link": "http://arxiv.org/abs/2506.01348v1",
        "abstract": "We introduce an innovative approach that incorporates a Distributionally\nRobust Learning (DRL) approach into Cox regression to enhance the robustness\nand accuracy of survival predictions. By formulating a DRL framework with a\nWasserstein distance-based ambiguity set, we develop a variant Cox model that\nis less sensitive to assumptions about the underlying data distribution and\nmore resilient to model misspecification and data perturbations. By leveraging\nWasserstein duality, we reformulate the original min-max DRL problem into a\ntractable regularized empirical risk minimization problem, which can be\ncomputed by exponential conic programming. We provide guarantees on the finite\nsample behavior of our DRL-Cox model. Moreover, through extensive simulations\nand real world case studies, we demonstrate that our regression model achieves\nsuperior performance in terms of prediction accuracy and robustness compared\nwith traditional methods."
    },
    {
        "date": "2025-06",
        "title": "ETDI: Mitigating Tool Squatting and Rug Pull Attacks in Model Context Protocol (MCP) by using OAuth-Enhanced Tool Definitions and Policy-Based Access Control",
        "author": "Manish Bhatt, Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2506.01333v1",
        "abstract": "The Model Context Protocol (MCP) plays a crucial role in extending the\ncapabilities of Large Language Models (LLMs) by enabling integration with\nexternal tools and data sources. However, the standard MCP specification\npresents significant security vulnerabilities, notably Tool Poisoning and Rug\nPull attacks. This paper introduces the Enhanced Tool Definition Interface\n(ETDI), a security extension designed to fortify MCP. ETDI incorporates\ncryptographic identity verification, immutable versioned tool definitions, and\nexplicit permission management, often leveraging OAuth 2.0. We further propose\nextending MCP with fine-grained, policy-based access control, where tool\ncapabilities are dynamically evaluated against explicit policies using a\ndedicated policy engine, considering runtime context beyond static OAuth\nscopes. This layered approach aims to establish a more secure, trustworthy, and\ncontrollable ecosystem for AI applications interacting with LLMs and external\ntools."
    },
    {
        "date": "2025-06",
        "title": "Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack",
        "author": "SeungBum Ha, Saerom Park, and Sung Whan Yoon",
        "link": "http://arxiv.org/abs/2506.01318v2",
        "abstract": "Machine unlearning (MU) aims to expunge a designated forget set from a\ntrained model without costly retraining, yet the existing techniques overlook\ntwo critical blind spots: \"over-unlearning\" that deteriorates retained data\nnear the forget set, and post-hoc \"relearning\" attacks that aim to resurrect\nthe forgotten knowledge. We first derive the over-unlearning metric\nOU@{\\epsilon}, which represents the collateral damage to the nearby region of\nthe forget set, where the over-unlearning mainly appears. Next, we expose an\nunforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,\nwhich exploits the per-class prototype of the forget class with just a few\nsamples, and easily restores the pre-unlearning performance. To counter both\nblind spots, we introduce Spotter, a plug-and-play objective that combines (i)\na masked knowledge-distillation penalty on the nearby region of forget set to\nsuppress OU@{\\epsilon}, and (ii) an intra-class dispersion loss that scatters\nforget-class embeddings, neutralizing prototypical relearning attacks. On\nCIFAR-10, as one of validations, Spotter reduces OU@{\\epsilon}by below the\n0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the\nretain set within 1% of difference with the original, and denies the\nprototype-attack by keeping the forget set accuracy within <1%, without\naccessing retained data. It confirms that Spotter is a practical remedy of the\nunlearning's blind spots."
    },
    {
        "date": "2025-06",
        "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models",
        "author": "Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, and Richang Hong",
        "link": "http://arxiv.org/abs/2506.01307v1",
        "abstract": "Large Language Models (LLMs) have evolved into Multimodal Large Language\nModels (MLLMs), significantly enhancing their capabilities by integrating\nvisual information and other types, thus aligning more closely with the nature\nof human intelligence, which processes a variety of data forms beyond just\ntext. Despite advancements, the undesirable generation of these models remains\na critical concern, particularly due to vulnerabilities exposed by text-based\njailbreak attacks, which have represented a significant threat by challenging\nexisting safety protocols. Motivated by the unique security risks posed by the\nintegration of new and old modalities for MLLMs, we propose a unified\nmultimodal universal jailbreak attack framework that leverages iterative\nimage-text interactions and transfer-based strategy to generate a universal\nadversarial suffix and image. Our work not only highlights the interaction of\nimage-text modalities can be used as a critical vulnerability but also\nvalidates that multimodal universal jailbreak attacks can bring higher-quality\nundesirable generations across different MLLMs. We evaluate the undesirable\ncontext generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and\nInstructBLIP, and reveal significant multimodal safety alignment issues,\nhighlighting the inadequacy of current safety mechanisms against sophisticated\nmultimodal attacks. This study underscores the urgent need for robust safety\nmeasures in MLLMs, advocating for a comprehensive review and enhancement of\nsecurity protocols to mitigate potential risks associated with multimodal\ncapabilities."
    },
    {
        "date": "2025-06",
        "title": "Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation",
        "author": "Jingfu Peng, and Yuhong Yang",
        "link": "http://arxiv.org/abs/2506.01267v1",
        "abstract": "Despite tremendous advancements of machine learning models and algorithms in\nvarious application domains, they are known to be vulnerable to subtle, natural\nor intentionally crafted perturbations in future input data, known as\nadversarial attacks. While numerous adversarial learning methods have been\nproposed, fundamental questions about their statistical optimality in robust\nloss remain largely unanswered. In particular, the minimax rate of convergence\nand the construction of rate-optimal estimators under future $X$-attacks are\nyet to be worked out.\n  In this paper, we address this issue in the context of nonparametric\nregression, under suitable assumptions on the smoothness of the regression\nfunction and the geometric structure of the input perturbation set. We first\nestablish the minimax rate of convergence under adversarial $L_q$-risks with $1\n\\leq q \\leq \\infty$ and propose a piecewise local polynomial estimator that\nachieves the minimax optimality. The established minimax rate elucidates how\nthe smoothness level and perturbation magnitude affect the fundamental limit of\nadversarial learning under future $X$-attacks. Furthermore, we construct a\ndata-driven adaptive estimator that is shown to achieve, within a logarithmic\nfactor, the optimal rate across a broad scale of nonparametric and adversarial\nclasses."
    },
    {
        "date": "2025-06",
        "title": "Stress-Testing ML Pipelines with Adversarial Data Corruption",
        "author": "Jiongli Zhu, Geyang Xu, Felipe Lorenzi, Boris Glavic, and Babak Salimi",
        "link": "http://arxiv.org/abs/2506.01230v1",
        "abstract": "Structured data-quality issues, such as missing values correlated with\ndemographics, culturally biased labels, or systemic selection biases, routinely\ndegrade the reliability of machine-learning pipelines. Regulators now\nincreasingly demand evidence that high-stakes systems can withstand these\nrealistic, interdependent errors, yet current robustness evaluations typically\nuse random or overly simplistic corruptions, leaving worst-case scenarios\nunexplored. We introduce SAVAGE, a causally inspired framework that (i)\nformally models realistic data-quality issues through dependency graphs and\nflexible corruption templates, and (ii) systematically discovers corruption\npatterns that maximally degrade a target performance metric. SAVAGE employs a\nbi-level optimization approach to efficiently identify vulnerable data\nsubpopulations and fine-tune corruption severity, treating the full ML\npipeline, including preprocessing and potentially non-differentiable models, as\na black box. Extensive experiments across multiple datasets and ML tasks (data\ncleaning, fairness-aware learning, uncertainty quantification) demonstrate that\neven a small fraction (around 5 %) of structured corruptions identified by\nSAVAGE severely impacts model performance, far exceeding random or manually\ncrafted errors, and invalidating core assumptions of existing techniques. Thus,\nSAVAGE provides a practical tool for rigorous pipeline stress-testing, a\nbenchmark for evaluating robustness methods, and actionable guidance for\ndesigning more resilient data workflows."
    },
    {
        "date": "2025-06",
        "title": "SPEAR: Security Posture Evaluation using AI Planner-Reasoning on Attack-Connectivity Hypergraphs",
        "author": "Rakesh Podder, Turgay Caglar, Shadaab Kawnain Bashir, Sarath Sreedharan, Indrajit Ray, and Indrakshi Ray",
        "link": "http://arxiv.org/abs/2506.01227v1",
        "abstract": "Graph-based frameworks are often used in network hardening to help a cyber\ndefender understand how a network can be attacked and how the best defenses can\nbe deployed. However, incorporating network connectivity parameters in the\nattack graph, reasoning about the attack graph when we do not have access to\ncomplete information, providing system administrator suggestions in an\nunderstandable format, and allowing them to do what-if analysis on various\nscenarios and attacker motives is still missing. We fill this gap by presenting\nSPEAR, a formal framework with tool support for security posture evaluation and\nanalysis that keeps human-in-the-loop. SPEAR uses the causal formalism of AI\nplanning to model vulnerabilities and configurations in a networked system. It\nautomatically converts network configurations and vulnerability descriptions\ninto planning models expressed in the Planning Domain Definition Language\n(PDDL). SPEAR identifies a set of diverse security hardening strategies that\ncan be presented in a manner understandable to the domain expert. These allow\nthe administrator to explore the network hardening solution space in a\nsystematic fashion and help evaluate the impact and compare the different\nsolutions."
    },
    {
        "date": "2025-06",
        "title": "Dirty and Clean-Label attack detection using GAN discriminators",
        "author": "John W. Smutny",
        "link": "http://arxiv.org/abs/2506.01224v2",
        "abstract": "Gathering enough images to train a deep computer vision model is a constant\nchallenge. Unfortunately, collecting images from unknown sources can leave your\nmodel s behavior at risk of being manipulated by a dirty-label or clean-label\nattack unless the images are properly inspected. Manually inspecting each\nimage-label pair is impractical and common poison-detection methods that\ninvolve re-training your model can be time consuming. This research uses GAN\ndiscriminators to protect a single class against mislabeled and different\nlevels of modified images. The effect of said perturbation on a basic\nconvolutional neural network classifier is also included for reference. The\nresults suggest that after training on a single class, GAN discriminator s\nconfidence scores can provide a threshold to identify mislabeled images and\nidentify 100% of the tested poison starting at a perturbation epsilon magnitude\nof 0.20, after decision threshold calibration using in-class samples.\nDevelopers can use this report as a basis to train their own discriminators to\nprotect high valued classes in their CV models."
    },
    {
        "date": "2025-06",
        "title": "FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA",
        "author": "Divyansh Jhunjhunwala, Arian Raje, Madan Ravi Ganesh, Chaithanya Kumar Mummadi, Chaoqun Dong, Jiawei Zhou, Wan-Yi Lin, Gauri Joshi, and Zhenzhen Li",
        "link": "http://arxiv.org/abs/2506.01194v1",
        "abstract": "LoRA has emerged as one of the most promising fine-tuning techniques,\nespecially for federated learning (FL), since it significantly reduces\ncommunication and computation costs at resource-constrained clients. However,\ndata heterogeneity remains a significant challenge for LoRA-based FL, and the\nconventional aggregation strategy based on FedAvg suffers from slow convergence\nand suboptimal accuracy. Motivated by recent advances in model merging,\nparticularly Task Arithmetic, we explore the idea of aggregating client LoRA\nparameters using scaled averaging. We first observe that a naive application of\nTask Arithmetic is ineffective due to the high cosine similarity between client\nupdates, indicating significant common knowledge in the updates across clients.\nTo address this issue, we propose decomposing client LoRA updates via Robust\nPrincipal Component Analysis (Robust-PCA) into a common low-rank component and\nclient-specific sparse components. Our proposed algorithm FedRPCA aggregates\nthe low-rank components through averaging, consolidating common knowledge, and\napplies scaled averaging to the sparse components to amplify client-specific\nknowledge. We evaluate our approach across a variety of vision and language\ntasks and demonstrate that it achieves higher final accuracy and faster\nconvergence compared to competing baselines."
    },
    {
        "date": "2025-06",
        "title": "Doubly Robust Alignment for Large Language Models",
        "author": "Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, and Chengchun Shi",
        "link": "http://arxiv.org/abs/2506.01183v1",
        "abstract": "This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM"
    },
    {
        "date": "2025-06",
        "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation",
        "author": "Jacob K. Christopher, Michael Cardei, Jinhao Liang, and Ferdinando Fioretto",
        "link": "http://arxiv.org/abs/2506.01121v1",
        "abstract": "Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution."
    },
    {
        "date": "2025-06",
        "title": "IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning",
        "author": "Shuyu Chen, Guopeng Lin, Haoyu Niu, Lushan Song, Chengxun Hong, and Weili Han",
        "link": "http://arxiv.org/abs/2506.01072v1",
        "abstract": "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes."
    },
    {
        "date": "2025-06",
        "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs",
        "author": "Yudong Zhang, Ruobing Xie, Yiqing Huang, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Di Wang, and Yu Wang",
        "link": "http://arxiv.org/abs/2506.01064v1",
        "abstract": "Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable."
    },
    {
        "date": "2025-06",
        "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution",
        "author": "Meysam Alizadeh, Zeynab Samei, Daria Stetsenko, and Fabrizio Gilardi",
        "link": "http://arxiv.org/abs/2506.01055v1",
        "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy."
    },
    {
        "date": "2025-06",
        "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack",
        "author": "Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, and Jinjun Wang",
        "link": "http://arxiv.org/abs/2506.01011v1",
        "abstract": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks."
    },
    {
        "date": "2025-06",
        "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
        "author": "Zhan Li, Mingyu Zhao, Xin Dong, Haibin Ling, and Bingyao Huang",
        "link": "http://arxiv.org/abs/2506.00978v1",
        "abstract": "Projector-based adversarial attack aims to project carefully designed light\npatterns (i.e., adversarial projections) onto scenes to deceive deep image\nclassifiers. It has potential applications in privacy protection and the\ndevelopment of more robust classifiers. However, existing approaches primarily\nfocus on individual classifiers and fixed camera poses, often neglecting the\ncomplexities of multi-classifier systems and scenarios with varying camera\nposes. This limitation reduces their effectiveness when introducing new\nclassifiers or camera poses. In this paper, we introduce Classifier-Agnostic\nProjector-Based Adversarial Attack (CAPAA) to address these issues. First, we\ndevelop a novel classifier-agnostic adversarial loss and optimization framework\nthat aggregates adversarial and stealthiness loss gradients from multiple\nclassifiers. Then, we propose an attention-based gradient weighting mechanism\nthat concentrates perturbations on regions of high classification activation,\nthereby improving the robustness of adversarial projections when applied to\nscenes with varying camera poses. Our extensive experimental evaluations\ndemonstrate that CAPAA achieves both a higher attack success rate and greater\nstealthiness compared to existing baselines. Codes are available at:\nhttps://github.com/ZhanLiQxQ/CAPAA."
    },
    {
        "date": "2025-06",
        "title": "Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation",
        "author": "Xiaohan Wang, Yu Zhang, Guibin Jiang, Bing Cheng, and Wei Lin",
        "link": "http://arxiv.org/abs/2506.00959v1",
        "abstract": "Marketing optimization, commonly formulated as an online budget allocation\nproblem, has emerged as a pivotal factor in driving user growth. Most existing\nresearch addresses this problem by following the principle of 'first predict\nthen optimize' for each individual, which presents challenges related to\nlarge-scale counterfactual prediction and solving complexity trade-offs. Note\nthat the practical data quality is uncontrollable, and the solving scale tends\nto be tens of millions. Therefore, the existing approaches make the robust\nbudget allocation non-trivial, especially in industrial scenarios with\nconsiderable data noise. To this end, this paper proposes a novel approach that\nsolves the problem from the cluster perspective. Specifically, we propose a\nmulti-task representation network to learn the inherent attributes of\nindividuals and project the original features into high-dimension hidden\nrepresentations through the first two layers of the trained network. Then, we\ndivide these hidden representations into $K$ groups through partitioning-based\nclustering, thus reformulating the problem as an integer stochastic programming\nproblem under different total budgets. Finally, we distill the representation\nmodule and clustering model into a multi-category model to facilitate online\ndeployment. Offline experiments validate the effectiveness and superiority of\nour approach compared to six state-of-the-art marketing optimization\nalgorithms. Online A/B tests on the Meituan platform indicate that the approach\noutperforms the online algorithm by 0.53% and 0.65%, considering order volume\n(OV) and gross merchandise volume (GMV), respectively."
    },
    {
        "date": "2025-06",
        "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models",
        "author": "Huixin Zhan, and Jason H. Moore",
        "link": "http://arxiv.org/abs/2506.00821v1",
        "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated significant success in variant effect prediction. However,\ntheir adversarial robustness remains largely unexplored. To address this gap,\nwe propose SafeGenes: a framework for Secure analysis of genomic foundation\nmodels, leveraging adversarial attacks to evaluate robustness against both\nengineered near-identical adversarial Genes and embedding-space manipulations.\nIn this study, we assess the adversarial vulnerabilities of GFMs using two\napproaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM\nintroduces minimal perturbations to input sequences, while the soft prompt\nattack optimizes continuous embeddings to manipulate model predictions without\nmodifying the input tokens. By combining these techniques, SafeGenes provides a\ncomprehensive assessment of GFM susceptibility to adversarial manipulation.\nTargeted soft prompt attacks led to substantial performance degradation, even\nin large models such as ESM1b and ESM1v. These findings expose critical\nvulnerabilities in current foundation models, opening new research directions\ntoward improving their security and robustness in high-stakes genomic\napplications such as variant effect prediction."
    },
    {
        "date": "2025-06",
        "title": "TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning",
        "author": "Jiaqi Luo, Yuan Yuan, and Shixin Xu",
        "link": "http://arxiv.org/abs/2506.00813v1",
        "abstract": "Tabular-image multimodal learning, which integrates structured tabular data\nwith imaging data, holds great promise for a variety of tasks, especially in\nmedical applications. Yet, two key challenges remain: (1) the lack of a\nstandardized, pretrained representation for tabular data, as is commonly\navailable in vision and language domains; and (2) the difficulty of handling\nmissing values in the tabular modality, which are common in real-world medical\ndatasets. To address these issues, we propose the TabPFN-Integrated Multimodal\nEngine (TIME), a novel multimodal framework that builds on the recently\nintroduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen\ntabular encoder to generate robust, strong embeddings that are naturally\nresilient to missing data, and combines them with image features from\npretrained vision backbones. We explore a range of fusion strategies and\ntabular encoders, and evaluate our approach on both natural and medical\ndatasets. Extensive experiments demonstrate that TIME consistently outperforms\ncompetitive baselines across both complete and incomplete tabular inputs,\nunderscoring its practical value in real-world multimodal learning scenarios."
    },
    {
        "date": "2025-06",
        "title": "Unlearning Inversion Attacks for Graph Neural Networks",
        "author": "Jiahao Zhang, Yilong Wang, Zhiwei Zhang, Xiaorui Liu, and Suhang Wang",
        "link": "http://arxiv.org/abs/2506.00808v1",
        "abstract": "Graph unlearning methods aim to efficiently remove the impact of sensitive\ndata from trained GNNs without full retraining, assuming that deleted\ninformation cannot be recovered. In this work, we challenge this assumption by\nintroducing the graph unlearning inversion attack: given only black-box access\nto an unlearned GNN and partial graph knowledge, can an adversary reconstruct\nthe removed edges? We identify two key challenges: varying\nprobability-similarity thresholds for unlearned versus retained edges, and the\ndifficulty of locating unlearned edge endpoints, and address them with\nTrendAttack. First, we derive and exploit the confidence pitfall, a theoretical\nand empirical pattern showing that nodes adjacent to unlearned edges exhibit a\nlarge drop in model confidence. Second, we design an adaptive prediction\nmechanism that applies different similarity thresholds to unlearned and other\nmembership edges. Our framework flexibly integrates existing membership\ninference techniques and extends them with trend features. Experiments on four\nreal-world datasets demonstrate that TrendAttack significantly outperforms\nstate-of-the-art GNN membership inference baselines, exposing a critical\nprivacy vulnerability in current graph unlearning methods."
    },
    {
        "date": "2025-05",
        "title": "Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors",
        "author": "Richard E. Neddo, Sean Willis, Zander Blasingame, and Chen Liu",
        "link": "http://arxiv.org/abs/2506.00661v1",
        "abstract": "Image classifiers, such as those used for autonomous vehicle navigation, are\nlargely known to be susceptible to adversarial attacks that target the input\nimage set. There is extensive discussion on adversarial attacks including\nperturbations that alter the input images to cause malicious misclassifications\nwithout perceivable modification. This work proposes a countermeasure for such\nattacks by adjusting the weights and classes of pretrained vision transformers\nwith a low-rank adaptation to become more robust against adversarial attacks\nand allow for scalable fine-tuning without retraining."
    },
    {
        "date": "2025-05",
        "title": "Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection",
        "author": "Marco Di Gennaro, Francesco Panebianco, Marco Pianta, Stefano Zanero, and Michele Carminati",
        "link": "http://arxiv.org/abs/2506.00654v1",
        "abstract": "Money laundering is a financial crime that poses a serious threat to\nfinancial integrity and social security. The growing number of transactions\nmakes it necessary to use automatic tools that help law enforcement agencies\ndetect such criminal activity. In this work, we present Amatriciana, a novel\napproach based on Graph Neural Networks to detect money launderers inside a\ngraph of transactions by considering temporal information. Amatriciana uses the\nwhole graph of transactions without splitting it into several time-based\nsubgraphs, exploiting all relational information in the dataset. Our\nexperiments on a public dataset reveal that the model can learn from a limited\namount of data. Furthermore, when more data is available, the model outperforms\nother State-of-the-art approaches; in particular, Amatriciana decreases the\nnumber of False Positives (FPs) while detecting many launderers. In summary,\nAmatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%\nwith respect to other State-of-the-art models."
    },
    {
        "date": "2025-05",
        "title": "AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents",
        "author": "Hanjun Luo, Shenyu Dai, Chiming Ni, Xinfeng Li, Guibin Zhang, Kun Wang, Tongliang Liu, and Hanan Salam",
        "link": "http://arxiv.org/abs/2506.00641v1",
        "abstract": "Despite the rapid advancement of LLM-based agents, the reliable evaluation of\ntheir safety and security remains a significant challenge. Existing rule-based\nor LLM-based evaluators often miss dangers in agents' step-by-step actions,\noverlook subtle meanings, fail to see how small issues compound, and get\nconfused by unclear safety or security rules. To overcome this evaluation\ncrisis, we introduce \\sys, a universal, training-free, memory-augmented\nreasoning framework that empowers LLM evaluators to emulate human expert\nevaluators. \\sys constructs an experiential memory by having an LLM adaptively\nextract structured semantic features (e.g., scenario, risk, behavior) and\ngenerate associated chain-of-thought reasoning traces for past interactions. A\nmulti-stage, context-aware retrieval-augmented generation process then\ndynamically retrieves the most relevant reasoning experiences to guide the LLM\nevaluator's assessment of new cases. Moreover, we developed \\data, the first\nbenchmark designed to check how well LLM-based evaluators can spot both safety\nrisks and security threats. \\data comprises \\textbf{2293} meticulously\nannotated interaction records, covering \\textbf{15} risk types across\n\\textbf{29} application scenarios. A key feature of \\data is its nuanced\napproach to ambiguous risk situations, employing ``Strict'' and ``Lenient''\njudgment standards. Experiments demonstrate that \\sys not only consistently\nimproves the evaluation performance of LLMs across all benchmarks but also sets\na new state-of-the-art in LLM-as-a-judge for agent safety and security,\nachieving human-level accuracy. Our work is openly openly accessible."
    },
    {
        "date": "2025-05",
        "title": "A \"Wenlu\" Brain System for Multimodal Cognition and Embodied Decision-Making: A Secure New Architecture for Deep Integration of Foundation Models and Domain Knowledge",
        "author": "Liang Geng",
        "link": "http://arxiv.org/abs/2506.00570v1",
        "abstract": "With the rapid penetration of artificial intelligence across industries and\nscenarios, a key challenge in building the next-generation intelligent core\nlies in effectively integrating the language understanding capabilities of\nfoundation models with domain-specific knowledge bases in complex real-world\napplications. This paper proposes a multimodal cognition and embodied\ndecision-making brain system, ``Wenlu\", designed to enable secure fusion of\nprivate knowledge and public models, unified processing of multimodal data such\nas images and speech, and closed-loop decision-making from cognition to\nautomatic generation of hardware-level code. The system introduces a\nbrain-inspired memory tagging and replay mechanism, seamlessly integrating\nuser-private data, industry-specific knowledge, and general-purpose language\nmodels. It provides precise and efficient multimodal services for enterprise\ndecision support, medical analysis, autonomous driving, robotic control, and\nmore. Compared with existing solutions, ``Wenlu\" demonstrates significant\nadvantages in multimodal processing, privacy security, end-to-end hardware\ncontrol code generation, self-learning, and sustainable updates, thus laying a\nsolid foundation for constructing the next-generation intelligent core."
    },
    {
        "date": "2025-05",
        "title": "Docker under Siege: Securing Containers in the Modern Era",
        "author": "Gogulakrishnan Thiyagarajan, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2506.02043v1",
        "abstract": "Containerization, driven by Docker, has transformed application development\nand deployment by enhancing efficiency and scalability. However, the rapid\nadoption of container technologies introduces significant security challenges\nthat require careful management. This paper investigates key areas of container\nsecurity, including runtime protection, network safeguards, configuration best\npractices, supply chain security, and comprehensive monitoring and logging\nsolutions. We identify common vulnerabilities within these domains and provide\nactionable recommendations to address and mitigate these risks. By integrating\nsecurity throughout the Software Development Lifecycle (SDLC), organizations\ncan reinforce their security posture, creating a resilient and reliable\ncontainerized application infrastructure that withstands evolving threats."
    },
    {
        "date": "2025-05",
        "title": "The Security Threat of Compressed Projectors in Large Vision-Language Models",
        "author": "Yudong Zhang, Ruobing Xie, Xingwu Sun, Jiansheng Chen, Zhanhui Kang, Di Wang, and Yu Wang",
        "link": "http://arxiv.org/abs/2506.00534v1",
        "abstract": "The choice of a suitable visual language projector (VLP) is critical to the\nsuccessful training of large visual language models (LVLMs). Mainstream VLPs\ncan be broadly categorized into compressed and uncompressed projectors, and\neach offering distinct advantages in performance and computational efficiency.\nHowever, their security implications have not been thoroughly examined. Our\ncomprehensive evaluation reveals significant differences in their security\nprofiles: compressed projectors exhibit substantial vulnerabilities, allowing\nadversaries to successfully compromise LVLMs even with minimal knowledge of\nstructural information. In stark contrast, uncompressed projectors demonstrate\nrobust security properties and do not introduce additional vulnerabilities.\nThese findings provide critical guidance for researchers in selecting optimal\nVLPs that enhance the security and reliability of visual language models. The\ncode will be released."
    },
    {
        "date": "2025-05",
        "title": "Robust and Verifiable MPC with Applications to Linear Machine Learning Inference",
        "author": "Tzu-Shen Wang, Jimmy Dani, Juan Garay, Soamar Homsi, and Nitesh Saxena",
        "link": "http://arxiv.org/abs/2506.00518v1",
        "abstract": "In this work, we present an efficient secure multi-party computation MPC\nprotocol that provides strong security guarantees in settings with dishonest\nmajority of participants who may behave arbitrarily. Unlike the popular MPC\nimplementation known as SPDZ [Crypto '12], which only ensures security with\nabort, our protocol achieves both complete identifiability and robustness. With\ncomplete identifiability, honest parties can detect and unanimously agree on\nthe identity of any malicious party. Robustness allows the protocol to continue\nwith the computation without requiring a restart, even when malicious behavior\nis detected. Additionally, our approach addresses the performance limitations\nobserved in the protocol by Cunningham et al. [ICITS '17], which, while\nachieving complete identifiability, is hindered by the costly exponentiation\noperations required by the choice of commitment scheme.\n  Our protocol is based on the approach by Rivinius et al. [S&P '22], utilizing\nlattice-based commitment for better efficiency. We achieved robustness with the\nhelp of a semi-honest trusted third party. We benchmark our robust protocol,\nshowing the efficient recovery from parties' malicious behavior.\n  Finally, we benchmark our protocol on a ML-as-a-service scenario, wherein\nclients off-load the desired computation to the servers, and verify the\ncomputation result. We benchmark on linear ML inference, running on various\ndatasets. While our efficiency is slightly lower compared to SPDZ's, we offer\nstronger security properties that provide distinct advantages."
    },
    {
        "date": "2025-05",
        "title": "Monitoring Robustness and Individual Fairness",
        "author": "Ashutosh Gupta, Thomas A. Henzinger, Konstantin Kueffner, Kaushik Mallik, and David Pape",
        "link": "http://arxiv.org/abs/2506.00496v1",
        "abstract": "Input-output robustness appears in various different forms in the literature,\nsuch as robustness of AI models to adversarial or semantic perturbations and\nindividual fairness of AI models that make decisions about humans.\n  We propose runtime monitoring of input-output robustness of deployed,\nblack-box AI models, where the goal is to design monitors that would observe\none long execution sequence of the model, and would raise an alarm whenever it\nis detected that two similar inputs from the past led to dissimilar outputs.\n  This way, monitoring will complement existing offline ``robustification''\napproaches to increase the trustworthiness of AI decision-makers.\n  We show that the monitoring problem can be cast as the fixed-radius nearest\nneighbor (FRNN) search problem, which, despite being well-studied, lacks\nsuitable online solutions.\n  We present our tool Clemont, which offers a number of lightweight monitors,\nsome of which use upgraded online variants of existing FRNN algorithms, and one\nuses a novel algorithm based on binary decision diagrams -- a data-structure\ncommonly used in software and hardware verification.\n  We have also developed an efficient parallelization technique that can\nsubstantially cut down the computation time of monitors for which the distance\nbetween input-output pairs is measured using the $L_\\infty$ norm.\n  Using standard benchmarks from the literature of adversarial and semantic\nrobustness and individual fairness, we perform a comparative study of different\nmonitors in \\tool, and demonstrate their effectiveness in correctly detecting\nrobustness violations at runtime."
    },
    {
        "date": "2025-05",
        "title": "Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem",
        "author": "Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, and Jiachi Chen",
        "link": "http://arxiv.org/abs/2506.02040v1",
        "abstract": "The Model Context Protocol (MCP) is an emerging standard designed to enable\nseamless interaction between Large Language Model (LLM) applications and\nexternal tools or resources. Within a short period, thousands of MCP services\nhave already been developed and deployed. However, the client-server\nintegration architecture inherent in MCP may expand the attack surface against\nLLM Agent systems, introducing new vulnerabilities that allow attackers to\nexploit by designing malicious MCP servers. In this paper, we present the first\nsystematic study of attack vectors targeting the MCP ecosystem. Our analysis\nidentifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet\nAttacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.\nTo evaluate the feasibility of these attacks, we conduct experiments following\nthe typical steps of launching an attack through malicious MCP servers:\nupload-download-attack. Specifically, we first construct malicious MCP servers\nand successfully upload them to three widely used MCP aggregation platforms.\nThe results indicate that current audit mechanisms are insufficient to identify\nand prevent the proposed attack methods. Next, through a user study and\ninterview with 20 participants, we demonstrate that users struggle to identify\nmalicious MCP servers and often unknowingly install them from aggregator\nplatforms. Finally, we demonstrate that these attacks can trigger harmful\nbehaviors within the user's local environment-such as accessing private files\nor controlling devices to transfer digital assets-by deploying a\nproof-of-concept (PoC) framework against five leading LLMs. Additionally, based\non interview results, we discuss four key challenges faced by the current\nsecurity ecosystem surrounding MCP servers. These findings underscore the\nurgent need for robust security mechanisms to defend against malicious MCP\nservers."
    },
    {
        "date": "2025-05",
        "title": "Hybrid Cloud Security: Balancing Performance, Cost, and Compliance in Multi-Cloud Deployments",
        "author": "Anjani kumar Polinati",
        "link": "http://arxiv.org/abs/2506.00426v1",
        "abstract": "The pervasive use of hybrid cloud computing models has changed enterprise as\nwell as Information Technology services infrastructure by giving businesses\nsimple and cost-effective options of combining on-premise IT equipment with\npublic cloud services. hybrid cloud solutions deploy multifaceted models of\nsecurity, performance optimization, and cost efficiency, conventionally\nfragmented in the cloud computing milieu. This paper examines how organizations\nmanage these parameters in hybrid cloud ecosystems while providing solutions to\nthe challenges they face in operationalizing hybrid cloud adoptions. The study\ncaptures the challenges of achieving a balance in resource distribution between\non-premise and cloud resources (herein referred to as the \"resource allocation\nchallenge\"), the complexity of pricing models from cloud providers like AWS,\nMicrosoft Azure, Google Cloud (herein called the 'pricing complexity problem'),\nand the urgency for strong security infrastructure to safeguard sensitive\ninformation (known as 'the information security problem'). This study\ndemonstrates the security and performance management solutions proposed were\nvalidated in a detailed case study of adoption of AWS and Azure based hybrid\ncloud and provides useful guidance. Also, a hybrid cloud security and cost\noptimization framework based on zero trust architecture, encryption, hybrid\ncloud policies, and others, is proposed.\n  The conclusion includes recommendations for research on automation of hybrid\ncloud service management, integration of multi-clouds, and the ever-present\nquestion of data privacy, stressing how those matters affect contemporary\nenterprises."
    },
    {
        "date": "2025-05",
        "title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on Distilled Preferences",
        "author": "Mohammad Saqib, Saikat Chakraborty, Santu Karmaker, and Niranjan Balasubramanian",
        "link": "http://arxiv.org/abs/2506.00419v1",
        "abstract": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo."
    },
    {
        "date": "2025-05",
        "title": "Label-shift robust federated feature screening for high-dimensional classification",
        "author": "Qi Qin, Erbo Li, Xingxiang Li, Yifan Sun, Wu Wang, and Chen Xu",
        "link": "http://arxiv.org/abs/2506.00379v1",
        "abstract": "Distributed and federated learning are important tools for high-dimensional\nclassification of large datasets. To reduce computational costs and overcome\nthe curse of dimensionality, feature screening plays a pivotal role in\neliminating irrelevant features during data preprocessing. However, data\nheterogeneity, particularly label shifting across different clients, presents\nsignificant challenges for feature screening. This paper introduces a general\nframework that unifies existing screening methods and proposes a novel utility,\nlabel-shift robust federated feature screening (LR-FFS), along with its\nfederated estimation procedure. The framework facilitates a uniform analysis of\nmethods and systematically characterizes their behaviors under label shift\nconditions. Building upon this framework, LR-FFS leverages conditional\ndistribution functions and expectations to address label shift without adding\ncomputational burdens and remains robust against model misspecification and\noutliers. Additionally, the federated procedure ensures computational\nefficiency and privacy protection while maintaining screening effectiveness\ncomparable to centralized processing. We also provide a false discovery rate\n(FDR) control method for federated feature screening. Experimental results and\ntheoretical analyses demonstrate LR-FFS's superior performance across diverse\nclient environments, including those with varying class distributions, sample\nsizes, and missing categorical data."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Machine Learning for Robust Password Strength Estimation",
        "author": "Pappu Jha, Hanzla Hamid, Oluseyi Olukola, Ashim Dahal, and Nick Rahimi",
        "link": "http://arxiv.org/abs/2506.00373v1",
        "abstract": "Passwords remain one of the most common methods for securing sensitive data\nin the digital age. However, weak password choices continue to pose significant\nrisks to data security and privacy. This study aims to solve the problem by\nfocusing on developing robust password strength estimation models using\nadversarial machine learning, a technique that trains models on intentionally\ncrafted deceptive passwords to expose and address vulnerabilities posed by such\npasswords. We apply five classification algorithms and use a dataset with more\nthan 670,000 samples of adversarial passwords to train the models. Results\ndemonstrate that adversarial training improves password strength classification\naccuracy by up to 20% compared to traditional machine learning models. It\nhighlights the importance of integrating adversarial machine learning into\nsecurity systems to enhance their robustness against modern adaptive threats.\n  Keywords: adversarial attack, password strength, classification, machine\nlearning"
    },
    {
        "date": "2025-05",
        "title": "$\\texttt{AVROBUSTBENCH}$: Benchmarking the Robustness of Audio-Visual Recognition Models at Test-Time",
        "author": "Sarthak Kumar Maharana, Saksham Singh Kushwaha, Baoming Zhang, Adrian Rodriguez, Songtao Wei, Yapeng Tian, and Yunhui Guo",
        "link": "http://arxiv.org/abs/2506.00358v1",
        "abstract": "While recent audio-visual models have demonstrated impressive performance,\ntheir robustness to distributional shifts at test-time remains not fully\nunderstood. Existing robustness benchmarks mainly focus on single modalities,\nmaking them insufficient for thoroughly assessing the robustness of\naudio-visual models. Motivated by real-world scenarios where shifts can occur\n$\\textit{simultaneously}$ in both audio and visual modalities, we introduce\n$\\texttt{AVROBUSTBENCH}$, a comprehensive benchmark designed to evaluate the\ntest-time robustness of audio-visual recognition models.\n$\\texttt{AVROBUSTBENCH}$ comprises four audio-visual benchmark datasets,\n$\\texttt{AUDIOSET-2C}$, $\\texttt{VGGSOUND-2C}$, $\\texttt{KINETICS-2C}$, and\n$\\texttt{EPICKITCHENS-2C}$, each incorporating 75 bimodal audio-visual\ncorruptions that are $\\textit{co-occurring}$ and $\\textit{correlated}$. Through\nextensive evaluations, we observe that state-of-the-art supervised and\nself-supervised audio-visual models exhibit declining robustness as corruption\nseverity increases. Furthermore, online test-time adaptation (TTA) methods, on\n$\\texttt{VGGSOUND-2C}$ and $\\texttt{KINETICS-2C}$, offer minimal improvements\nin performance under bimodal corruptions. We further propose $\\texttt{AV2C}$, a\nsimple TTA approach enabling on-the-fly cross-modal fusion by penalizing\nhigh-entropy samples, which achieves improvements on $\\texttt{VGGSOUND-2C}$. We\nhope that $\\texttt{AVROBUSTBENCH}$ will steer the development of more effective\nand robust audio-visual TTA approaches. Our code is available\n$\\href{https://github.com/sarthaxxxxx/AV-C-Robustness-Benchmark}{here}$."
    },
    {
        "date": "2025-05",
        "title": "Enabling Secure and Ephemeral AI Workloads in Data Mesh Environments",
        "author": "Chinkit Patel, and Kee Siong Ng",
        "link": "http://arxiv.org/abs/2506.00352v1",
        "abstract": "Many large enterprises that operate highly governed and complex ICT\nenvironments have no efficient and effective way to support their Data and AI\nteams in rapidly spinning up and tearing down self-service data and compute\ninfrastructure, to experiment with new data analytic tools, and deploy data\nproducts into operational use. This paper proposes a key piece of the solution\nto the overall problem, in the form of an on-demand self-service data-platform\ninfrastructure to empower de-centralised data teams to build data products on\ntop of centralised templates, policies and governance. The core innovation is\nan efficient method to leverage immutable container operating systems and\ninfrastructure-as-code methodologies for creating, from scratch, vendor-neutral\nand short-lived Kubernetes clusters on-premises and in any cloud environment.\nOur proposed approach can serve as a repeatable, portable and cost-efficient\nalternative or complement to commercial Platform-as-a-Service (PaaS) offerings,\nand this is particularly important in supporting interoperability in complex\ndata mesh environments with a mix of modern and legacy compute infrastructure."
    },
    {
        "date": "2025-05",
        "title": "Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking",
        "author": "Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, and Ru-Yue Yuan",
        "link": "http://arxiv.org/abs/2506.00325v1",
        "abstract": "Although deep learning-based visual tracking methods have made significant\nprogress, they exhibit vulnerabilities when facing carefully designed\nadversarial attacks, which can lead to a sharp decline in tracking performance.\nTo address this issue, this paper proposes for the first time a novel\nadversarial defense method based on denoise diffusion probabilistic models,\ntermed DiffDf, aimed at effectively improving the robustness of existing visual\ntracking methods against adversarial attacks. DiffDf establishes a multi-scale\ndefense mechanism by combining pixel-level reconstruction loss, semantic\nconsistency loss, and structural similarity loss, effectively suppressing\nadversarial perturbations through a gradual denoising process. Extensive\nexperimental results on several mainstream datasets show that the DiffDf method\ndemonstrates excellent generalization performance for trackers with different\narchitectures, significantly improving various evaluation metrics while\nachieving real-time inference speeds of over 30 FPS, showcasing outstanding\ndefense performance and efficiency. Codes are available at\nhttps://github.com/pgao-lab/DiffDf."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Threat Vectors and Risk Mitigation for Retrieval-Augmented Generation Systems",
        "author": "Chris M. Ward, and Josh Harguess",
        "link": "http://arxiv.org/abs/2506.00281v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems, which integrate Large Language\nModels (LLMs) with external knowledge sources, are vulnerable to a range of\nadversarial attack vectors. This paper examines the importance of RAG systems\nthrough recent industry adoption trends and identifies the prominent attack\nvectors for RAG: prompt injection, data poisoning, and adversarial query\nmanipulation. We analyze these threats under risk management lens, and propose\nrobust prioritized control list that includes risk-mitigating actions like\ninput validation, adversarial training, and real-time monitoring."
    },
    {
        "date": "2025-05",
        "title": "DeGLIF for Label Noise Robust Node Classification using GNNs",
        "author": "Pintu Kumar, and Nandyala Hemachandra",
        "link": "http://arxiv.org/abs/2506.00244v1",
        "abstract": "Noisy labelled datasets are generally inexpensive compared to clean labelled\ndatasets, and the same is true for graph data. In this paper, we propose a\ndenoising technique DeGLIF: Denoising Graph Data using Leave-One-Out Influence\nFunction. DeGLIF uses a small set of clean data and the leave-one-out influence\nfunction to make label noise robust node-level prediction on graph data.\nLeave-one-out influence function approximates the change in the model\nparameters if a training point is removed from the training dataset. Recent\nadvances propose a way to calculate the leave-one-out influence function for\nGraph Neural Networks (GNNs). We extend that recent work to estimate the change\nin validation loss, if a training node is removed from the training dataset. We\nuse this estimate and a new theoretically motivated relabelling function to\ndenoise the training dataset. We propose two DeGLIF variants to identify noisy\nnodes. Both these variants do not require any information about the noise model\nor the noise level in the dataset; DeGLIF also does not estimate these\nquantities. For one of these variants, we prove that the noisy points detected\ncan indeed increase risk. We carry out detailed computational experiments on\ndifferent datasets to show the effectiveness of DeGLIF. It achieves better\naccuracy than other baseline algorithms"
    },
    {
        "date": "2025-05",
        "title": "Heterogeneous Graph Backdoor Attack",
        "author": "Jiawei Chen, Lusi Li, Daniel Takabi, Masha Sosonkina, and Rui Ning",
        "link": "http://arxiv.org/abs/2506.00191v1",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) excel in modeling complex,\nmulti-typed relationships across diverse domains, yet their vulnerability to\nbackdoor attacks remains unexplored. To address this gap, we conduct the first\ninvestigation into the susceptibility of HGNNs to existing graph backdoor\nattacks, revealing three critical issues: (1) high attack budget required for\neffective backdoor injection, (2) inefficient and unreliable backdoor\nactivation, and (3) inaccurate attack effectiveness evaluation. To tackle these\nissues, we propose the Heterogeneous Graph Backdoor Attack (HGBA), the first\nbackdoor attack specifically designed for HGNNs, introducing a novel\nrelation-based trigger mechanism that establishes specific connections between\na strategically selected trigger node and poisoned nodes via the backdoor\nmetapath. HGBA achieves efficient and stealthy backdoor injection with minimal\nstructural modifications and supports easy backdoor activation through two\nflexible strategies: Self-Node Attack and Indiscriminate Attack. Additionally,\nwe improve the ASR measurement protocol, enabling a more accurate assessment of\nattack effectiveness. Extensive experiments demonstrate that HGBA far surpasses\nmultiple state-of-the-art graph backdoor attacks in black-box settings,\nefficiently attacking HGNNs with low attack budgets. Ablation studies show that\nthe strength of HBGA benefits from our trigger node selection method and\nbackdoor metapath selection strategy. In addition, HGBA shows superior\nrobustness against node feature perturbations and multiple types of existing\ngraph backdoor defense mechanisms. Finally, extension experiments demonstrate\nthat the relation-based trigger mechanism can effectively extend to tasks in\nhomogeneous graph scenarios, thereby posing severe threats to broader\nsecurity-critical domains."
    },
    {
        "date": "2025-05",
        "title": "Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges",
        "author": "Raj Patel, Himanshu Tripathi, Jasper Stone, Noorbakhsh Amiri Golilarz, Sudip Mittal, Shahram Rahimi, and Vini Chaudhary",
        "link": "http://arxiv.org/abs/2506.02032v1",
        "abstract": "The rapid adoption of machine learning (ML) technologies has driven\norganizations across diverse sectors to seek efficient and reliable methods to\naccelerate model development-to-deployment. Machine Learning Operations (MLOps)\nhas emerged as an integrative approach addressing these requirements by\nunifying relevant roles and streamlining ML workflows. As the MLOps market\ncontinues to grow, securing these pipelines has become increasingly critical.\nHowever, the unified nature of MLOps ecosystem introduces vulnerabilities,\nmaking them susceptible to adversarial attacks where a single misconfiguration\ncan lead to compromised credentials, severe financial losses, damaged public\ntrust, and the poisoning of training data. Our paper presents a systematic\napplication of the MITRE ATLAS (Adversarial Threat Landscape for\nArtificial-Intelligence Systems) framework, a comprehensive and continuously\nupdated catalog of AI-focused attacks, to systematically assess attacks across\ndifferent phases of the MLOps ecosystem. We begin by examining the preparatory\nphases during which adversaries acquire the essential intelligence required to\ninitiate their attacks. We then present a structured taxonomy of attack\ntechniques explicitly mapped to corresponding phases of the MLOps ecosystem,\nsupported by examples drawn from red-teaming exercises and real-world\nincidents. This is followed by a taxonomy of mitigation strategies aligned with\nthese attack categories, offering actionable early-stage defenses to strengthen\nthe security of MLOps ecosystem. Given the rapid evolution and adoption of\nMLOps, we further highlight key research gaps that require immediate attention.\nOur work emphasizes the importance of implementing robust security protocols\nfrom the outset, empowering practitioners to safeguard MLOps ecosystem against\nevolving cyber attacks."
    },
    {
        "date": "2025-05",
        "title": "From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching",
        "author": "Ruqi Bai, Yao Ji, Zeyu Zhou, and David I. Inouye",
        "link": "http://arxiv.org/abs/2505.24843v1",
        "abstract": "Spurious correlations can cause model performance to degrade in new\nenvironments. Prior causality-inspired works aim to learn invariant\nrepresentations (e.g., IRM) but typically underperform empirical risk\nminimization (ERM). Recent alternatives improve robustness by leveraging\ntest-time data, but such data may be unavailable in practice. To address these\nissues, we take a data-centric approach by leveraging invariant data pairs,\npairs of samples that would have the same prediction with the optimally robust\nclassifier. We prove that certain counterfactual pairs will naturally satisfy\nthis invariance property and introduce noisy counterfactual matching (NCM), a\nsimple constraint-based method for leveraging invariant pairs for enhanced\nrobustness, even with a small set of noisy pairs-in the ideal case, each pair\ncan eliminate one spurious feature. For linear causal models, we prove that the\ntest domain error can be upper bounded by the in-domain error and a term that\ndepends on the counterfactuals' diversity and quality. We validate on a\nsynthetic dataset and demonstrate on real-world benchmarks that linear probing\non a pretrained backbone improves robustness."
    },
    {
        "date": "2025-05",
        "title": "Cascading Adversarial Bias from Injection to Distillation in Language Models",
        "author": "Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, and Alina Oprea",
        "link": "http://arxiv.org/abs/2505.24842v1",
        "abstract": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."
    },
    {
        "date": "2025-05",
        "title": "ByzFL: Research Framework for Robust Federated Learning",
        "author": "Marc Gonz\u00e1lez, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, and Fran\u00e7ois Ta\u00efani",
        "link": "http://arxiv.org/abs/2505.24802v1",
        "abstract": "We present ByzFL, an open-source Python library for developing and\nbenchmarking robust federated learning (FL) algorithms. ByzFL provides a\nunified and extensible framework that includes implementations of\nstate-of-the-art robust aggregators, a suite of configurable attacks, and tools\nfor simulating a variety of FL scenarios, including heterogeneous data\ndistributions, multiple training algorithms, and adversarial threat models. The\nlibrary enables systematic experimentation via a single JSON-based\nconfiguration file and includes built-in utilities for result visualization.\nCompatible with PyTorch tensors and NumPy arrays, ByzFL is designed to\nfacilitate reproducible research and rapid prototyping of robust FL solutions.\nByzFL is available at https://byzfl.epfl.ch/, with source code hosted on\nGitHub: https://github.com/LPD-EPFL/byzfl."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning against Model Perturbation in Edge Networks",
        "author": "Dongzi Jin, Yong Xiao, and Yingyu Li",
        "link": "http://arxiv.org/abs/2505.24728v1",
        "abstract": "Federated Learning (FL) is a promising paradigm for realizing edge\nintelligence, allowing collaborative learning among distributed edge devices by\nsharing models instead of raw data. However, the shared models are often\nassumed to be ideal, which would be inevitably violated in practice due to\nvarious perturbations, leading to significant performance degradation. To\novercome this challenge, we propose a novel method, termed Sharpness-Aware\nMinimization-based Robust Federated Learning (SMRFL), which aims to improve\nmodel robustness against perturbations by exploring the geometrical property of\nthe model landscape. Specifically, SMRFL solves a min-max optimization problem\nthat promotes model convergence towards a flat minimum by minimizing the\nmaximum loss within a neighborhood of the model parameters. In this way, model\nsensitivity to perturbations is reduced, and robustness is enhanced since\nmodels in the neighborhood of the flat minimum also enjoy low loss values. The\ntheoretical result proves that SMRFL can converge at the same rate as FL\nwithout perturbations. Extensive experimental results show that SMRFL\nsignificantly enhances robustness against perturbations compared to three\nbaseline methods on two real-world datasets under three perturbation scenarios."
    },
    {
        "date": "2025-05",
        "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences",
        "author": "Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2505.24709v1",
        "abstract": "Optimizing policies based on human preferences is key to aligning language\nmodels with human intent. This work focuses on reward modeling, a core\ncomponent in reinforcement learning from human feedback (RLHF), and offline\npreference optimization, such as direct preference optimization. Conventional\napproaches typically assume accurate annotations. However, real-world\npreference data often contains noise due to human errors or biases. We propose\na principled framework for robust policy optimization under noisy preferences,\nviewing reward modeling as a classification problem. This allows us to leverage\nsymmetric losses, known for their robustness to label noise in classification,\nleading to our Symmetric Preference Optimization (SymPO) method. We prove that\nsymmetric losses enable successful policy optimization even under noisy labels,\nas the resulting reward remains rank-preserving -- a property sufficient for\npolicy improvement. Experiments on synthetic and real-world tasks demonstrate\nthe effectiveness of SymPO."
    },
    {
        "date": "2025-05",
        "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches",
        "author": "Dennis Jacob, Chong Xiang, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2505.24703v1",
        "abstract": "Deep learning techniques have enabled vast improvements in computer vision\ntechnologies. Nevertheless, these models are vulnerable to adversarial patch\nattacks which catastrophically impair performance. The physically realizable\nnature of these attacks calls for certifiable defenses, which feature provable\nguarantees on robustness. While certifiable defenses have been successfully\napplied to single-label classification, limited work has been done for\nmulti-label classification. In this work, we present PatchDEMUX, a certifiably\nrobust framework for multi-label classifiers against adversarial patches. Our\napproach is a generalizable method which can extend any existing certifiable\ndefense for single-label classification; this is done by considering the\nmulti-label classification task as a series of isolated binary classification\nproblems to provably guarantee robustness. Furthermore, in the scenario where\nan attacker is limited to a single patch we propose an additional certification\nprocedure that can provide tighter robustness bounds. Using the current\nstate-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a\nbackbone, we find that PatchDEMUX can achieve non-trivial robustness on the\nMS-COCO and PASCAL VOC datasets while maintaining high clean performance"
    },
    {
        "date": "2025-05",
        "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms",
        "author": "Maria Rafaela Gkeka, Bowen Sun, Evgenia Smirni, Christos D. Antonopoulos, Spyros Lalis, and Nikolaos Bellas",
        "link": "http://arxiv.org/abs/2505.24654v1",
        "abstract": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system."
    },
    {
        "date": "2025-05",
        "title": "A Flat Minima Perspective on Understanding Augmentations and Model Robustness",
        "author": "Weebum Yoo, and Sung Whan Yoon",
        "link": "http://arxiv.org/abs/2505.24592v1",
        "abstract": "Model robustness indicates a model's capability to generalize well on\nunforeseen distributional shifts, including data corruption, adversarial\nattacks, and domain shifts. Data augmentation is one of the prevalent and\neffective ways to enhance robustness. Despite the great success of\naugmentations in different fields, a general theoretical understanding of their\nefficacy in improving model robustness is lacking. We offer a unified\ntheoretical framework to clarify how augmentations can enhance model robustness\nthrough the lens of loss surface flatness and PAC generalization bound. Our\nwork diverges from prior studies in that our analysis i) broadly encompasses\nmuch of the existing augmentation methods, and ii) is not limited to specific\ntypes of distribution shifts like adversarial attacks. We confirm our theories\nthrough simulations on the existing common corruption and adversarial\nrobustness benchmarks based on the CIFAR and ImageNet datasets, as well as\ndomain generalization benchmarks including PACS and OfficeHome."
    },
    {
        "date": "2025-05",
        "title": "CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control",
        "author": "Chaohui Xu, Qi Cui, and Chip-Hong Chang",
        "link": "http://arxiv.org/abs/2505.24536v1",
        "abstract": "The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous\ntraining costs make their intellectual property (IP) protection of paramount\nimportance. Recently introduced passport-based methods attempt to steer DNN\nwatermarking towards strengthening ownership verification against ambiguity\nattacks by modulating the affine parameters of normalization layers.\nUnfortunately, neither watermarking nor passport-based methods provide a\nholistic protection with robust ownership proof, high fidelity, active usage\nauthorization and user traceability for offline access distributed models and\nmulti-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we\npropose a Chameleon Hash-based Irreversible Passport (CHIP) protection\nframework that utilizes the cryptographic chameleon hash function to achieve\nall these goals. The collision-resistant property of chameleon hash allows for\nstrong model ownership claim upon IP infringement and liable user traceability,\nwhile the trapdoor-collision property enables hashing of multiple user\npassports and licensee certificates to the same immutable signature to realize\nactive usage control. Using the owner passport as an oracle, multiple\nuser-specific triplets, each contains a passport-aware user model, a user\npassport, and a licensee certificate can be created for secure offline\ndistribution. The watermarked master model can also be deployed for MLaaS with\nusage permission verifiable by the provision of any trapdoor-colliding user\npassports. CHIP is extensively evaluated on four datasets and two architectures\nto demonstrate its protection versatility and robustness. Our code is released\nat https://github.com/Dshm212/CHIP."
    },
    {
        "date": "2025-05",
        "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders",
        "author": "Yuqi Zhang, Yuchun Miao, Zuchao Li, and Liang Ding",
        "link": "http://arxiv.org/abs/2505.24519v1",
        "abstract": "We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off."
    },
    {
        "date": "2025-05",
        "title": "Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers",
        "author": "Heejo Kong, Sung-Jin Kim, Gunho Jung, and Seong-Whan Lee",
        "link": "http://arxiv.org/abs/2505.24443v1",
        "abstract": "Conventional semi-supervised learning (SSL) ideally assumes that labeled and\nunlabeled data share an identical class distribution, however in practice, this\nassumption is easily violated, as unlabeled data often includes unknown class\ndata, i.e., outliers. The outliers are treated as noise, considerably degrading\nthe performance of SSL models. To address this drawback, we propose a novel\nframework, Diversify and Conquer (DAC), to enhance SSL robustness in the\ncontext of open-set semi-supervised learning. In particular, we note that\nexisting open-set SSL methods rely on prediction discrepancies between inliers\nand outliers from a single model trained on labeled data. This approach can be\neasily failed when the labeled data is insufficient, leading to performance\ndegradation that is worse than naive SSL that do not account for outliers. In\ncontrast, our approach exploits prediction disagreements among multiple models\nthat are differently biased towards the unlabeled distribution. By leveraging\nthe discrepancies arising from training on unlabeled data, our method enables\nrobust outlier detection even when the labeled data is underspecified. Our key\ncontribution is constructing a collection of differently biased models through\na single training process. By encouraging divergent heads to be differently\nbiased towards outliers while making consistent predictions for inliers, we\nexploit the disagreement among these heads as a measure to identify unknown\nconcepts. Our code is available at https://github.com/heejokong/DivCon."
    },
    {
        "date": "2025-05",
        "title": "pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation",
        "author": "Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, and Kannie W. Y. Chan",
        "link": "http://arxiv.org/abs/2505.24421v1",
        "abstract": "Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Preference Learning for Robust LLM Alignment",
        "author": "Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, and Mingchuan Yang",
        "link": "http://arxiv.org/abs/2505.24369v1",
        "abstract": "Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model."
    },
    {
        "date": "2025-05",
        "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations",
        "author": "Xin Quan, Marco Valentino, Louise A. Dennis, and Andr\u00e9 Freitas",
        "link": "http://arxiv.org/abs/2505.24264v1",
        "abstract": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification."
    },
    {
        "date": "2025-05",
        "title": "Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization",
        "author": "Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, and Hongbin Liu",
        "link": "http://arxiv.org/abs/2505.24249v1",
        "abstract": "Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage."
    },
    {
        "date": "2025-05",
        "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring",
        "author": "Sana Ebrahimi, Mohsen Dehghankar, and Abolfazl Asudeh",
        "link": "http://arxiv.org/abs/2505.24239v1",
        "abstract": "While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings."
    },
    {
        "date": "2025-05",
        "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap",
        "author": "Wenhan Yang, Spencer Stice, Ali Payani, and Baharan Mirzasoleiman",
        "link": "http://arxiv.org/abs/2505.24208v1",
        "abstract": "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for\ntheir reliable deployment. However, LVLMs suffer from drastic safety\ndegradation compared to their LLM backbone. Even blank or irrelevant images can\ntrigger LVLMs to generate harmful responses to prompts that would otherwise be\nrefused in text-only contexts. The modality gap between image and text\nrepresentations has been recently hypothesized to contribute to safety\ndegradation of LVLMs. However, if and how the amount of modality gap affects\nLVLMs' safety is not studied. In this work, we show that the amount of modality\ngap is highly inversely correlated with VLMs' safety. Then, we show that this\nmodality gap is introduced during pretraining LVLMs and persists through\nfine-tuning. Inspired by this observation, we propose a regularization to\nreduce the modality gap during pretraining. Our extensive experiments on LLaVA\nv1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves\nsafety alignment of LVLMs, reducing unsafe rate by up to 16.3% without\ncompromising performance, and can further boost existing defenses by up to\n18.2%."
    },
    {
        "date": "2025-05",
        "title": "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents",
        "author": "Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, and Jungseul Ok",
        "link": "http://arxiv.org/abs/2505.24157v1",
        "abstract": "Developing autonomous agents capable of mastering complex, multi-step tasks\nin unpredictable, interactive environments presents a significant challenge.\nWhile Large Language Models (LLMs) offer promise for planning, existing\napproaches often rely on problematic internal knowledge or make unrealistic\nenvironmental assumptions. Although recent work explores learning planning\nknowledge, they still retain limitations due to partial reliance on external\nknowledge or impractical setups. Indeed, prior research has largely overlooked\ndeveloping agents capable of acquiring planning knowledge from scratch,\ndirectly in realistic settings. While realizing this capability is necessary,\nit presents significant challenges, primarily achieving robustness given the\nsubstantial risk of incorporating LLMs' inaccurate knowledge. Moreover,\nefficiency is crucial for practicality as learning can demand prohibitive\nexploration. In response, we introduce Robust and Efficient Planning for\nOpen-world Agents (REPOA), a novel framework designed to tackle these issues.\nREPOA features three key components: adaptive dependency learning and\nfine-grained failure-aware operation memory to enhance robustness to knowledge\ninaccuracies, and difficulty-based exploration to improve learning efficiency.\nOur evaluation in two established open-world testbeds demonstrates REPOA's\nrobust and efficient planning, showcasing its capability to successfully obtain\nchallenging late-game items that were beyond the reach of prior approaches."
    },
    {
        "date": "2025-05",
        "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models",
        "author": "Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, and Yefeng Zheng",
        "link": "http://arxiv.org/abs/2505.24141v1",
        "abstract": "With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models."
    },
    {
        "date": "2025-05",
        "title": "Practical Bayes-Optimal Membership Inference Attacks",
        "author": "Marcus Lassila, Johan \u00d6stman, Khac-Hoang Ngo, and Alexandre Graell i Amat",
        "link": "http://arxiv.org/abs/2505.24089v1",
        "abstract": "We develop practical and theoretically grounded membership inference attacks\n(MIAs) against both independent and identically distributed (i.i.d.) data and\ngraph-structured data. Building on the Bayesian decision-theoretic framework of\nSablayrolles et al., we derive the Bayes-optimal membership inference rule for\nnode-level MIAs against graph neural networks, addressing key open questions\nabout optimal query strategies in the graph setting. We introduce BASE and\nG-BASE, computationally efficient approximations of the Bayes-optimal attack.\nG-BASE achieves superior performance compared to previously proposed\nclassifier-based node-level MIA attacks. BASE, which is also applicable to\nnon-graph data, matches or exceeds the performance of prior state-of-the-art\nMIAs, such as LiRA and RMIA, at a significantly lower computational cost.\nFinally, we show that BASE and RMIA are equivalent under a specific\nhyperparameter setting, providing a principled, Bayes-optimal justification for\nthe RMIA attack."
    },
    {
        "date": "2025-05",
        "title": "DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals",
        "author": "Alireza Jafari, Fereshteh Yousefirizi, and Vahid Seydi",
        "link": "http://arxiv.org/abs/2505.24085v1",
        "abstract": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with\nelevated health risks, where timely detection is pivotal for mitigating\nstroke-related morbidity. This study introduces an innovative hybrid\nmethodology integrating unsupervised deep learning and gradient boosting models\nto improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is\ncoupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM\n(LGBM)-to harness their complementary advantages while addressing individual\nlimitations. The proposed framework uniquely combines DCAE with gradient\nboosting, enabling end-to-end AF identification devoid of manual feature\nextraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of\n99.99%, and inference latency of four seconds, outperforming existing methods\nand aligning with clinical deployment requirements. The DCAE integration\nsignificantly enhances boosting models, positioning this hybrid system as a\nreliable tool for automated AF detection in clinical settings."
    },
    {
        "date": "2025-05",
        "title": "An Advanced Cyber-Physical System Security Testbed for Substation Automation",
        "author": "Akila Herath, Chen-Ching Liu, Junho Hong, and Mansi Girdhar",
        "link": "http://arxiv.org/abs/2505.24021v1",
        "abstract": "A Cyber-Physical System (CPS) testbed serves as a powerful platform for\ntesting and validating cyber intrusion detection and mitigation strategies in\nsubstations. This study presents the design and development of a CPS testbed\nthat can effectively assess the real-time dynamics of a substation. Cyber\nattacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated\nusing the testbed, along with an analysis on attack detection. Realistic timing\nmeasurements are obtained, and the time frames for deploying detection and\nmitigation strategies are evaluated."
    },
    {
        "date": "2025-05",
        "title": "LLM Agents Should Employ Security Principles",
        "author": "Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2505.24019v1",
        "abstract": "Large Language Model (LLM) agents show considerable promise for automating\ncomplex tasks using contextual reasoning; however, interactions involving\nmultiple agents and the system's susceptibility to prompt injection and other\nforms of context manipulation introduce new vulnerabilities related to privacy\nleakage and system exploitation. This position paper argues that the\nwell-established design principles in information security, which are commonly\nreferred to as security principles, should be employed when deploying LLM\nagents at scale. Design principles such as defense-in-depth, least privilege,\ncomplete mediation, and psychological acceptability have helped guide the\ndesign of mechanisms for securing information systems over the last five\ndecades, and we argue that their explicit and conscientious adoption will help\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\na conceptual framework embedding these security principles to provide\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\nLLMs along three dimensions: benign utility, attack utility, and attack success\nrate. AgentSandbox maintains high utility for its intended functions under both\nbenign and adversarial evaluations while substantially mitigating privacy\nrisks. By embedding secure design principles as foundational elements within\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\naligned with user privacy expectations and evolving regulatory requirements."
    },
    {
        "date": "2025-05",
        "title": "SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations",
        "author": "Gaurav Sarkar, Jay Gala, and Subarna Tripathi",
        "link": "http://arxiv.org/abs/2505.23942v1",
        "abstract": "The design of activation functions remains a pivotal component in optimizing\ndeep neural networks. While prevailing choices like Swish and GELU demonstrate\nconsiderable efficacy, they often exhibit domain-specific optima. This work\nintroduces SG-Blend, a novel activation function that blends our proposed\nSSwish, a first-order symmetric variant of Swish and the established GELU\nthrough dynamic interpolation. By adaptively blending these constituent\nfunctions via learnable parameters, SG-Blend aims to harness their\ncomplementary strengths: SSwish's controlled non-monotonicity and symmetry, and\nGELU's smooth, probabilistic profile, to achieve a more universally robust\nbalance between model expressivity and gradient stability. We conduct\ncomprehensive empirical evaluations across diverse modalities and\narchitectures, showing performance improvements across all considered natural\nlanguage and computer vision tasks and models. These results, achieved with\nnegligible computational overhead, underscore SG-Blend's potential as a\nversatile, drop-in replacement that consistently outperforms strong\ncontemporary baselines. The code is available at\nhttps://anonymous.4open.science/r/SGBlend-6CBC."
    },
    {
        "date": "2025-05",
        "title": "FMG-Det: Foundation Model Guided Robust Object Detection",
        "author": "Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, and Yijing Watkins",
        "link": "http://arxiv.org/abs/2505.23726v1",
        "abstract": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches."
    },
    {
        "date": "2025-05",
        "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
        "author": "Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, and Mengran Xue",
        "link": "http://arxiv.org/abs/2505.23706v1",
        "abstract": "In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks."
    },
    {
        "date": "2025-05",
        "title": "Synopsis: Secure and private trend inference from encrypted semantic embeddings",
        "author": "Madelyne Xiao, Palak Jain, Micha Gorelick, and Sarah Scheffler",
        "link": "http://arxiv.org/abs/2505.23880v1",
        "abstract": "WhatsApp and many other commonly used communication platforms guarantee\nend-to-end encryption (E2EE), which requires that service providers lack the\ncryptographic keys to read communications on their own platforms. WhatsApp's\nprivacy-preserving design makes it difficult to study important phenomena like\nthe spread of misinformation or political messaging, as users have a clear\nexpectation and desire for privacy and little incentive to forfeit that privacy\nin the process of handing over raw data to researchers, journalists, or other\nparties.\n  We introduce Synopsis, a secure architecture for analyzing messaging trends\nin consensually-donated E2EE messages using message embeddings. Since the goal\nof this system is investigative journalism workflows, Synopsis must facilitate\nboth exploratory and targeted analyses -- a challenge for systems using\ndifferential privacy (DP), and, for different reasons, a challenge for private\ncomputation approaches based on cryptography. To meet these challenges, we\ncombine techniques from the local and central DP models and wrap the system in\nmalicious-secure multi-party computation to ensure the DP query architecture is\nthe only way to access messages, preventing any party from directly viewing\nstored message embeddings.\n  Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages\nrepresented as 500-dimensional embeddings) demonstrate the efficiency and\naccuracy of our approach. Queries on this data run in about 30 seconds, and the\naccuracy of the fine-grained interface exceeds 94% on benchmark tasks."
    },
    {
        "date": "2025-05",
        "title": "Securing AI Agents with Information-Flow Control",
        "author": "Manuel Costa, Boris K\u00f6pf, Aashish Kolluri, Andrew Paverd, Mark Russinovich, Ahmed Salem, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B\u00e9guelin",
        "link": "http://arxiv.org/abs/2505.23643v1",
        "abstract": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides"
    },
    {
        "date": "2025-05",
        "title": "DRO: A Python Library for Distributionally Robust Optimization in Machine Learning",
        "author": "Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, and Jose Blanchet",
        "link": "http://arxiv.org/abs/2505.23565v1",
        "abstract": "We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org."
    },
    {
        "date": "2025-05",
        "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
        "author": "Zenghui Yuan, Yangming Xu, Jiawen Shi, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.23561v1",
        "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."
    },
    {
        "date": "2025-05",
        "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
        "author": "Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, and Seyit Camtepe",
        "link": "http://arxiv.org/abs/2505.23397v2",
        "abstract": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making."
    },
    {
        "date": "2025-05",
        "title": "Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment",
        "author": "Yun-Cheng Tsai",
        "link": "http://arxiv.org/abs/2505.23392v1",
        "abstract": "Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment."
    },
    {
        "date": "2025-05",
        "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning",
        "author": "Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, and Xiu Li",
        "link": "http://arxiv.org/abs/2505.23871v1",
        "abstract": "Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results."
    },
    {
        "date": "2025-05",
        "title": "Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert",
        "author": "Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou, Libo Qin, and Wenhong Tian",
        "link": "http://arxiv.org/abs/2505.23868v2",
        "abstract": "Current parameter-efficient fine-tuning methods for adapting pre-trained\nlanguage models to downstream tasks are susceptible to interference from noisy\ndata. Conventional noise-handling approaches either rely on laborious data\npre-processing or employ model architecture modifications prone to error\naccumulation. In contrast to existing noise-process paradigms, we propose a\nnoise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a\nnovel framework that enhances model robustness to noise only with generated\nnoisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE\nstrategically integrates a dedicated poisoning expert in an asymmetric LoRA\nconfiguration. Through a two-stage paradigm, LoPE performs noise injection on\nthe poisoning expert during fine-tuning to enhance its noise discrimination and\nprocessing ability. During inference, we selectively mask the dedicated\npoisoning expert to leverage purified knowledge acquired by normal experts for\nnoise-robust output. Extensive experiments demonstrate that LoPE achieves\nstrong performance and robustness purely through the low-cost noise injection,\nwhich completely eliminates the requirement of data cleaning."
    },
    {
        "date": "2025-05",
        "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis",
        "author": "Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang",
        "link": "http://arxiv.org/abs/2505.23325v1",
        "abstract": "Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/."
    },
    {
        "date": "2025-05",
        "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation",
        "author": "Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, and Hongxia Yang",
        "link": "http://arxiv.org/abs/2505.23867v1",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition",
        "author": "Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, and Jin Tang",
        "link": "http://arxiv.org/abs/2505.23313v1",
        "abstract": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR."
    },
    {
        "date": "2025-05",
        "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
        "author": "Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, and Tao Xiang",
        "link": "http://arxiv.org/abs/2505.23266v1",
        "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
        "author": "Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, and Lei Xie",
        "link": "http://arxiv.org/abs/2505.23207v1",
        "abstract": "Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD."
    },
    {
        "date": "2025-05",
        "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
        "author": "Run Hao, and Peng Ying",
        "link": "http://arxiv.org/abs/2505.23192v1",
        "abstract": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems."
    },
    {
        "date": "2025-05",
        "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
        "author": "Junyan Liu, Arnab Maiti, Artin Tajdini, Kevin Jamieson, and Lillian J. Ratliff",
        "link": "http://arxiv.org/abs/2505.23124v1",
        "abstract": "We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round."
    },
    {
        "date": "2025-05",
        "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
        "author": "Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, and Yanmin Qian",
        "link": "http://arxiv.org/abs/2505.23049v1",
        "abstract": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator."
    },
    {
        "date": "2025-05",
        "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
        "author": "Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, and Rahul G. Krishnan",
        "link": "http://arxiv.org/abs/2505.23027v1",
        "abstract": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop"
    },
    {
        "date": "2025-05",
        "title": "Context-Robust Knowledge Editing for Language Models",
        "author": "Haewon Park, Gyubin Choi, Minjun Kim, and Yohan Jo",
        "link": "http://arxiv.org/abs/2505.23026v2",
        "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success."
    },
    {
        "date": "2025-05",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning",
        "author": "Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, and Sunil Gupta",
        "link": "http://arxiv.org/abs/2505.23003v1",
        "abstract": "Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL."
    },
    {
        "date": "2025-05",
        "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates",
        "author": "Jaewoo Ahn, Heeseung Yun, Dayoon Ko, and Gunhee Kim",
        "link": "http://arxiv.org/abs/2505.22943v1",
        "abstract": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios."
    },
    {
        "date": "2025-05",
        "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging",
        "author": "Haobo Zhang, and Jiayu Zhou",
        "link": "http://arxiv.org/abs/2505.22934v1",
        "abstract": "Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models."
    },
    {
        "date": "2025-05",
        "title": "Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints",
        "author": "Hyungki Im, Wyame Benslimane, and Paul Grigas",
        "link": "http://arxiv.org/abs/2505.22881v1",
        "abstract": "We study an extension of contextual stochastic linear optimization (CSLO)\nthat, in contrast to most of the existing literature, involves inequality\nconstraints that depend on uncertain parameters predicted by a machine learning\nmodel. To handle the constraint uncertainty, we use contextual uncertainty sets\nconstructed via methods like conformal prediction. Given a contextual\nuncertainty set method, we introduce the \"Smart Predict-then-Optimize with\nRobust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the\nSPO loss that measures decision error of predicted objective parameters. We\nalso introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with\nSPO-RC. To enhance performance, we train on truncated datasets where true\nconstraint parameters lie within the uncertainty sets, and we correct the\ninduced sample selection bias using importance reweighting techniques. Through\nexperiments on fractional knapsack and alloy production problem instances, we\ndemonstrate that SPO-RC+ effectively handles uncertainty in constraints and\nthat combining truncation with importance reweighting can further improve\nperformance."
    },
    {
        "date": "2025-05",
        "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
        "author": "Krti Tallam, and Emma Miller",
        "link": "http://arxiv.org/abs/2505.22852v1",
        "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based\nsandbox to mitigate prompt injection attacks in large language model (LLM)\nagents. While effective, CaMeL assumes a trusted user prompt, omits\nside-channel concerns, and incurs performance tradeoffs due to its dual-LLM\ndesign. This response identifies these issues and proposes engineering\nimprovements to expand CaMeL's threat coverage and operational usability. We\nintroduce: (1) prompt screening for initial inputs, (2) output auditing to\ndetect instruction leakage, (3) a tiered-risk access model to balance usability\nand control, and (4) a verified intermediate language for formal guarantees.\nTogether, these upgrades align CaMeL with best practices in enterprise security\nand support scalable deployment."
    },
    {
        "date": "2025-05",
        "title": "Security Benefits and Side Effects of Labeling AI-Generated Images",
        "author": "Sandra H\u00f6ltervennhoff, Jonas Ricker, Maike M. Raphael, Charlotte Schwedes, Rebecca Weil, Asja Fischer, Thorsten Holz, Lea Sch\u00f6nherr, and Sascha Fahl",
        "link": "http://arxiv.org/abs/2505.22845v1",
        "abstract": "Generative artificial intelligence is developing rapidly, impacting humans'\ninteraction with information and digital media. It is increasingly used to\ncreate deceptively realistic misinformation, so lawmakers have imposed\nregulations requiring the disclosure of AI-generated content. However, only\nlittle is known about whether these labels reduce the risks of AI-generated\nmisinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we\nstudy the implications of labels, including the possibility of mislabeling.\nAssuming that simplicity, transparency, and trust are likely to impact the\nsuccessful adoption of such labels, we first qualitatively explore users'\nopinions and expectations of AI labeling using five focus groups. Second, we\nconduct a pre-registered online survey with over 1300 U.S. and EU participants\nto quantitatively assess the effect of AI labels on users' ability to recognize\nmisinformation containing either human-made or AI-generated images. Our focus\ngroups illustrate that, while participants have concerns about the practical\nimplementation of labeling, they consider it helpful in identifying\nAI-generated images and avoiding deception. However, considering security\nbenefits, our survey revealed an ambiguous picture, suggesting that users might\nover-rely on labels. While inaccurate claims supported by labeled AI-generated\nimages were rated less credible than those with unlabeled AI-images, the belief\nin accurate claims also decreased when accompanied by a labeled AI-generated\nimage. Moreover, we find the undesired side effect that human-made images\nconveying inaccurate claims were perceived as more credible in the presence of\nlabels."
    },
    {
        "date": "2025-05",
        "title": "How Do Diffusion Models Improve Adversarial Robustness?",
        "author": "Liu Yuezhang, and Xue-Xin Wei",
        "link": "http://arxiv.org/abs/2505.22839v1",
        "abstract": "Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems."
    },
    {
        "date": "2025-05",
        "title": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems",
        "author": "Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, and Wonyong Shin",
        "link": "http://arxiv.org/abs/2505.23847v1",
        "abstract": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."
    },
    {
        "date": "2025-05",
        "title": "Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook",
        "author": "Banafsheh Saber Latibari, Najmeh Nazari, Avesta Sasan, Houman Homayoun, Pratik Satam, Soheil Salehi, and Hossein Sayadi",
        "link": "http://arxiv.org/abs/2505.22605v1",
        "abstract": "The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective",
        "author": "Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, and Meng Wang",
        "link": "http://arxiv.org/abs/2505.22604v2",
        "abstract": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."
    },
    {
        "date": "2025-05",
        "title": "Training RL Agents for Multi-Objective Network Defense Tasks",
        "author": "Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, and Ahmed Ridley",
        "link": "http://arxiv.org/abs/2505.22531v1",
        "abstract": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."
    },
    {
        "date": "2025-05",
        "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas",
        "author": "Agnideep Aich, Ashit Baran Aich, and Bruce Wade",
        "link": "http://arxiv.org/abs/2505.22518v1",
        "abstract": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."
    },
    {
        "date": "2025-05",
        "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector",
        "author": "Aixuan Li, Mochu Xiang, Jing Zhang, and Yuchao Dai",
        "link": "http://arxiv.org/abs/2505.22499v2",
        "abstract": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."
    },
    {
        "date": "2025-05",
        "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods",
        "author": "Michal Kmicikiewicz, Vincent Fortuin, and Ewa Szczurek",
        "link": "http://arxiv.org/abs/2505.22494v1",
        "abstract": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty."
    },
    {
        "date": "2025-05",
        "title": "Understanding Adversarial Training with Energy-based Models",
        "author": "Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.22486v1",
        "abstract": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."
    },
    {
        "date": "2025-05",
        "title": "GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance",
        "author": "Zaixi Zhang, Zhenghong Zhou, Ruofan Jin, Le Cong, and Mengdi Wang",
        "link": "http://arxiv.org/abs/2505.23839v1",
        "abstract": "DNA, encoding genetic instructions for almost all living organisms, fuels\ngroundbreaking advances in genomics and synthetic biology. Recently, DNA\nFoundation Models have achieved success in designing synthetic functional DNA\nsequences, even whole genomes, but their susceptibility to jailbreaking remains\nunderexplored, leading to potential concern of generating harmful sequences\nsuch as pathogens or toxin-producing genes. In this paper, we introduce\nGeneBreaker, the first framework to systematically evaluate jailbreak\nvulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent\nwith customized bioinformatic tools to design high-homology, non-pathogenic\njailbreaking prompts, (2) beam search guided by PathoLM and log-probability\nheuristics to steer generation toward pathogen-like sequences, and (3) a\nBLAST-based evaluation pipeline against a curated Human Pathogen Database\n(JailbreakDNABench) to detect successful jailbreaks. Evaluated on our\nJailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series\nmodels across 6 viral categories consistently (up to 60\\% Attack Success Rate\nfor Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1\nenvelope protein demonstrate the sequence and structural fidelity of jailbreak\noutput, while evolutionary modeling of SARS-CoV-2 underscores biosecurity\nrisks. Our findings also reveal that scaling DNA foundation models amplifies\ndual-use risks, motivating enhanced safety alignment and tracing mechanisms.\nOur code is at https://github.com/zaixizhang/GeneBreaker."
    },
    {
        "date": "2025-05",
        "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models",
        "author": "Yongcan Yu, Yanbo Wang, Ran He, and Jian Liang",
        "link": "http://arxiv.org/abs/2505.22271v1",
        "abstract": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."
    },
    {
        "date": "2025-05",
        "title": "Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses",
        "author": "Seung Gyu Jeong, and Seong Eun Kim",
        "link": "http://arxiv.org/abs/2505.23834v1",
        "abstract": "Lung sound classification is vital for early diagnosis of respiratory\ndiseases. However, biomedical signals often exhibit inter-patient variability\neven among patients with the same symptoms, requiring a learning approach that\nconsiders individual differences. We propose a Patient-Aware Feature Alignment\n(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)\nand Global Patient Alignment Loss (GPAL). PCSL clusters features of the same\npatient while separating those from other patients to capture patient\nvariability, whereas GPAL draws each patient's centroid toward a global center,\npreventing feature space fragmentation. Our method achieves outstanding results\non the ICBHI dataset with a score of 64.84\\% for four-class and 72.08\\% for\ntwo-class classification. These findings highlight PAFA's ability to capture\nindividualized patterns and demonstrate performance gains in distinct patient\nclusters, offering broader applications for patient-centered healthcare."
    },
    {
        "date": "2025-05",
        "title": "Accountable, Scalable and DoS-resilient Secure Vehicular Communication",
        "author": "Hongyu Jin, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.22162v1",
        "abstract": "Paramount to vehicle safety, broadcasted Cooperative Awareness Messages\n(CAMs) and Decentralized Environmental Notification Messages (DENMs) are\npseudonymously authenticated for security and privacy protection, with each\nnode needing to have all incoming messages validated within an expiration\ndeadline. This creates an asymmetry that can be easily exploited by external\nadversaries to launch a clogging Denial of Service (DoS) attack: each forged VC\nmessage forces all neighboring nodes to cryptographically validate it; at\nincreasing rates, easy to generate forged messages gradually exhaust processing\nresources and severely degrade or deny timely validation of benign CAMs/DENMs.\nThe result can be catastrophic when awareness of neighbor vehicle positions or\ncritical reports are missed. We address this problem making the standardized VC\npseudonymous authentication DoS-resilient. We propose efficient cryptographic\nconstructs, which we term message verification facilitators, to prioritize\nprocessing resources for verification of potentially valid messages among bogus\nmessages and verify multiple messages based on one signature verification. Any\nmessage acceptance is strictly based on public-key based message\nauthentication/verification for accountability, i.e., non-repudiation is not\nsacrificed, unlike symmetric key based approaches. This further enables drastic\nmisbehavior detection, also exploiting the newly introduced facilitators, based\non probabilistic signature verification and cross-checking over multiple\nfacilitators verifying the same message; while maintaining verification latency\nlow even when under attack, trading off modest communication overhead. Our\nfacilitators can also be used for efficient discovery and verification of DENM\nor any event-driven message, including misbehavior evidence used for our\nscheme."
    },
    {
        "date": "2025-05",
        "title": "Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance",
        "author": "Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, and Zhenyu He",
        "link": "http://arxiv.org/abs/2505.22154v1",
        "abstract": "RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."
    },
    {
        "date": "2025-05",
        "title": "Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM",
        "author": "Lei Yu, Yechao Zhang, Ziqi Zhou, Yang Wu, Wei Wan, Minghui Li, Shengshan Hu, Pei Xiaobing, and Jing Wang",
        "link": "http://arxiv.org/abs/2505.23828v1",
        "abstract": "With the rapid development of the Vision-Language Model (VLM), significant\nprogress has been made in Visual Question Answering (VQA) tasks. However,\nexisting VLM often generate inaccurate answers due to a lack of up-to-date\nknowledge. To address this issue, recent research has introduced\nRetrieval-Augmented Generation (RAG) techniques, commonly used in Large\nLanguage Models (LLM), into VLM, incorporating external multi-modal knowledge\nto enhance the accuracy and practicality of VLM systems. Nevertheless, the RAG\nin LLM may be susceptible to data poisoning attacks. RAG-based VLM may also\nface the threat of this attack. This paper first reveals the vulnerabilities of\nthe RAG-based large model under poisoning attack, showing that existing\nsingle-modal RAG poisoning attacks have a 100\\% failure rate in multi-modal RAG\nscenarios. To address this gap, we propose Spa-VLM (Stealthy Poisoning Attack\non RAG-based VLM), a new paradigm for poisoning attacks on large models. We\ncarefully craft malicious multi-modal knowledge entries, including adversarial\nimages and misleading text, which are then injected into the RAG's knowledge\nbase. When users access the VLM service, the system may generate misleading\noutputs. We evaluate Spa-VLM on two Wikipedia datasets and across two different\nRAGs. Results demonstrate that our method achieves highly stealthy poisoning,\nwith the attack success rate exceeding 0.8 after injecting just 5 malicious\nentries into knowledge bases with 100K and 2M entries, outperforming\nstate-of-the-art poisoning attacks designed for RAG-based LLMs. Additionally,\nwe evaluated several defense mechanisms, all of which ultimately proved\nineffective against Spa-VLM, underscoring the effectiveness and robustness of\nour attack."
    },
    {
        "date": "2025-05",
        "title": "Are classical deep neural networks weakly adversarially robust?",
        "author": "Nuolin Sun, Linyuan Wang, Dongyang Li, Bin Yan, and Lei Li",
        "link": "http://arxiv.org/abs/2506.02016v1",
        "abstract": "Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs."
    },
    {
        "date": "2025-05",
        "title": "Securing the Software Package Supply Chain for Critical Systems",
        "author": "Ritwik Murali, and Akash Ravi",
        "link": "http://arxiv.org/abs/2505.22023v1",
        "abstract": "Software systems have grown as an indispensable commodity used across various\nindustries, and almost all essential services depend on them for effective\noperation. The software is no longer an independent or stand-alone piece of\ncode written by a developer but rather a collection of packages designed by\nmultiple developers across the globe. Ensuring the reliability and resilience\nof these systems is crucial since emerging threats target software supply\nchains, as demonstrated by the widespread SolarWinds hack in late 2020. These\nsupply chains extend beyond patches and updates, involving distribution\nnetworks throughout the software lifecycle. Industries like smart grids,\nmanufacturing, healthcare, and finance rely on interconnected software systems\nand their dependencies for effective functioning. To secure software modules\nand add-ons, robust distribution architectures are essential. The proposed\nchapter enhances the existing delivery frameworks by including a permissioned\nledger with Proof of Authority consensus and multi-party signatures. The\nproposed system aims to prevent attacks while permitting every stakeholder to\nverify the same. Critical systems can interface with the secure pipeline\nwithout disrupting existing functionalities, thus preventing the cascading\neffect of an attack at any point in the supply chain."
    },
    {
        "date": "2025-05",
        "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement",
        "author": "Zhihong Tang, and Yang Li",
        "link": "http://arxiv.org/abs/2505.22021v1",
        "abstract": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection",
        "author": "Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, and Jinhang Zuo",
        "link": "http://arxiv.org/abs/2505.21938v2",
        "abstract": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."
    },
    {
        "date": "2025-05",
        "title": "SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking",
        "author": "Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Xun Chen, and Miao Pan",
        "link": "http://arxiv.org/abs/2505.23821v2",
        "abstract": "With the surge of social media, maliciously tampered public speeches,\nespecially those from influential figures, have seriously affected social\nstability and public trust. Existing speech tampering detection methods remain\ninsufficient: they either rely on external reference data or fail to be both\nsensitive to attacks and robust to benign operations, such as compression and\nresampling. To tackle these challenges, we introduce SpeechVerifer to\nproactively verify speech integrity using only the published speech itself,\ni.e., without requiring any external references. Inspired by audio\nfingerprinting and watermarking, SpeechVerifier can (i) effectively detect\ntampering attacks, (ii) be robust to benign operations and (iii) verify the\nintegrity only based on published speeches. Briefly, SpeechVerifier utilizes\nmultiscale feature extraction to capture speech features across different\ntemporal resolutions. Then, it employs contrastive learning to generate\nfingerprints that can detect modifications at varying granularities. These\nfingerprints are designed to be robust to benign operations, but exhibit\nsignificant changes when malicious tampering occurs. To enable speech\nverification in a self-contained manner, the generated fingerprints are then\nembedded into the speech signal by segment-wise watermarking. Without external\nreferences, SpeechVerifier can retrieve the fingerprint from the published\naudio and check it with the embedded watermark to verify the integrity of the\nspeech. Extensive experimental results demonstrate that the proposed\nSpeechVerifier is effective in detecting tampering attacks and robust to benign\noperations."
    },
    {
        "date": "2025-05",
        "title": "Evaluating the Retrieval Robustness of Large Language Models",
        "author": "Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, and Shiyue Zhang",
        "link": "http://arxiv.org/abs/2505.21870v1",
        "abstract": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."
    },
    {
        "date": "2025-05",
        "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification",
        "author": "Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, and Chongshou Li",
        "link": "http://arxiv.org/abs/2505.21854v1",
        "abstract": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."
    },
    {
        "date": "2025-05",
        "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
        "author": "Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, and Honghao Wei",
        "link": "http://arxiv.org/abs/2505.21841v1",
        "abstract": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."
    },
    {
        "date": "2025-05",
        "title": "Faster Rates for Private Adversarial Bandits",
        "author": "Hilal Asi, Vinod Raman, and Kunal Talwar",
        "link": "http://arxiv.org/abs/2505.21790v1",
        "abstract": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"
    },
    {
        "date": "2025-05",
        "title": "System Prompt Extraction Attacks and Defenses in Large Language Models",
        "author": "Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu",
        "link": "http://arxiv.org/abs/2505.23817v1",
        "abstract": "The system prompt in Large Language Models (LLMs) plays a pivotal role in\nguiding model behavior and response generation. Often containing private\nconfiguration details, user roles, and operational instructions, the system\nprompt has become an emerging attack target. Recent studies have shown that LLM\nsystem prompts are highly susceptible to extraction attacks through\nmeticulously designed queries, raising significant privacy and security\nconcerns. Despite the growing threat, there is a lack of systematic studies of\nsystem prompt extraction attacks and defenses. In this paper, we present a\ncomprehensive framework, SPE-LLM, to systematically evaluate System Prompt\nExtraction attacks and defenses in LLMs. First, we design a set of novel\nadversarial queries that effectively extract system prompts in state-of-the-art\n(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction\nattacks. Second, we propose three defense techniques to mitigate system prompt\nextraction attacks in LLMs, providing practical solutions for secure LLM\ndeployments. Third, we introduce a set of rigorous evaluation metrics to\naccurately quantify the severity of system prompt extraction attacks in LLMs\nand conduct comprehensive experiments across multiple benchmark datasets, which\nvalidates the efficacy of our proposed SPE-LLM framework."
    },
    {
        "date": "2025-05",
        "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
        "author": "Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira",
        "link": "http://arxiv.org/abs/2505.21755v1",
        "abstract": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."
    },
    {
        "date": "2025-05",
        "title": "What is Adversarial Training for Diffusion Models?",
        "author": "Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.21742v1",
        "abstract": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks",
        "author": "Julia Boone, Tolunay Seyfi, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2505.21703v1",
        "abstract": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another."
    },
    {
        "date": "2025-05",
        "title": "Expert Survey: AI Reliability & Security Research Priorities",
        "author": "Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, and Cara Labrador",
        "link": "http://arxiv.org/abs/2505.21664v1",
        "abstract": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research."
    },
    {
        "date": "2025-05",
        "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking",
        "author": "Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.21620v1",
        "abstract": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."
    },
    {
        "date": "2025-05",
        "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
        "author": "Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, and Qing Wang",
        "link": "http://arxiv.org/abs/2505.21499v1",
        "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."
    },
    {
        "date": "2025-05",
        "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
        "author": "Mathew J. Walter, Aaron Barrett, and Kimberly Tam",
        "link": "http://arxiv.org/abs/2505.21609v1",
        "abstract": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.21494v1",
        "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."
    },
    {
        "date": "2025-05",
        "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming",
        "author": "Yang Yang, Jiemin Wu, and Yutao Yue",
        "link": "http://arxiv.org/abs/2505.21486v1",
        "abstract": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation."
    },
    {
        "date": "2025-05",
        "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
        "author": "Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, and Joseph Weissman",
        "link": "http://arxiv.org/abs/2505.21414v1",
        "abstract": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments."
    },
    {
        "date": "2025-05",
        "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling",
        "author": "Hovhannes Tamoyan, Subhabrata Dutta, and Iryna Gurevych",
        "link": "http://arxiv.org/abs/2505.21399v1",
        "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."
    },
    {
        "date": "2025-05",
        "title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment",
        "author": "Xingyu Zhou, Yulian Wu, Wenqian Weng, and Francesco Orabona",
        "link": "http://arxiv.org/abs/2505.21395v1",
        "abstract": "In this paper, we theoretically study the offline alignment of language\nmodels with human preference feedback, under both preference label corruption\nand privacy protections. To this end, we propose Square$\\chi$PO, a simple\none-line change to $\\chi$PO where the standard log-loss is replaced by a new\nsquare loss over probability. Thanks to the inherent properties of this new\nloss, we have advanced the state-of-the-art of differentially private and\nrobust offline direct alignment. Specifically, for the local model of label\nprivacy, Square$\\chi$PO is the first algorithm that attains an optimal rate\nbased on single-policy concentrability even with general function\napproximations. It also gives the first result under the central model of\nprivacy protection over both prompts (responses) and labels. On the robustness\nside against Huber label corruption, Square$\\chi$PO is the first alignment\nmethod that has a meaningful theoretical guarantee under general function\napproximations. More importantly, Square$\\chi$PO can address privacy protection\nand corruption simultaneously, where an interesting separation is observed,\nimplying that the order of privacy and corruption matters. Furthermore, we show\nthat Square$\\chi$PO can also be easily extended to handle the scenario of the\ngeneral preference model with state-of-the-art guarantees under corruption and\nprivacy. Last but not least, all of our theoretical guarantees enjoy a unified\nanalysis, building upon a new result on the generalization error bounds of\nleast-square regression under corruption and privacy constraints, which we\nbelieve is of independent interest to the community."
    },
    {
        "date": "2025-05",
        "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios",
        "author": "Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, and Yueming Jin",
        "link": "http://arxiv.org/abs/2505.21387v1",
        "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github."
    },
    {
        "date": "2025-05",
        "title": "Subgroups Matter for Robust Bias Mitigation",
        "author": "Anissa Alloula, Charles Jones, Ben Glocker, and Bart\u0142omiej W. Papie\u017c",
        "link": "http://arxiv.org/abs/2505.21363v2",
        "abstract": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and presents it as an alternative lever for improving the\nrobustness and fairness of machine learning models."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations",
        "author": "Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, and Yu Tsao",
        "link": "http://arxiv.org/abs/2505.21356v3",
        "abstract": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders by providing standardized evaluations of vocal\nfunction. Traditionally, expert raters use standard scales such as the\nConsensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade,\nRoughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics\nare subjective and prone to inter-rater variability, motivating the need for\nautomated, objective assessment methods. This study proposes Voice Quality\nAssessment Network (VOQANet), a deep learning-based framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To enhance robustness and\ninterpretability, we also introduce VOQANet+, which integrates low-level speech\ndescriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with\nSFM embeddings into a hybrid representation. Unlike prior studies focused only\non vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality\nDataset (PVQD), we evaluate our models on both vowel-based and sentence-level\nspeech (PVQD-S subset) to improve generalizability. Results show that\nsentence-based input outperforms vowel-based input, especially at the patient\nlevel, underscoring the value of longer utterances for capturing perceptual\nvoice attributes. VOQANet consistently surpasses baseline methods in root mean\nsquared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V\nand GRBAS dimensions, with VOQANet+ achieving even better performance.\nAdditional experiments under noisy conditions show that VOQANet+ maintains high\nprediction accuracy and robustness, supporting its potential for real-world and\ntelehealth deployment."
    },
    {
        "date": "2025-05",
        "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
        "author": "Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.21277v2",
        "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
    },
    {
        "date": "2025-05",
        "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion",
        "author": "Yayin Zheng, Chen Wan, Zihong Guo, Hailing Kuang, and Xiaohai Lu",
        "link": "http://arxiv.org/abs/2505.21181v1",
        "abstract": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels."
    },
    {
        "date": "2025-05",
        "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images",
        "author": "Xurui Li, Zhonesheng Jiang, Tingxuan Ai, and Yu Zhou",
        "link": "http://arxiv.org/abs/2505.21152v1",
        "abstract": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS."
    },
    {
        "date": "2025-05",
        "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
        "author": "Honglin Gao, Xiang Li, Lan Zhao, and Gaoxi Xiao",
        "link": "http://arxiv.org/abs/2505.21140v1",
        "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios."
    },
    {
        "date": "2025-05",
        "title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach",
        "author": "Subhagata Chattopadhyay, and Amit K Chattopadhyay",
        "link": "http://arxiv.org/abs/2505.21139v1",
        "abstract": "The COVID-19 pandemic has significantly increased the incidence of\npost-infection cardiovascular events, particularly myocardial infarction, in\nindividuals over 40. While the underlying mechanisms remain elusive, this study\nemploys a hybrid machine learning approach to analyze epidemiological data in\nassessing 13 key heart attack risk factors and their susceptibility. Based on a\nunique dataset that combines demographic, biochemical, ECG, and thallium\nstress-tests, this study categorizes distinct subpopulations against varying\nrisk profiles and then divides the population into 'at-risk' (AR) and\n'not-at-risk' (NAR) groups using clustering algorithms. The study reveals\nstrong association between the likelihood of experiencing a heart attack on the\n13 risk factors studied. The aggravated risk for postmenopausal patients\nindicates compromised individual risk factors due to estrogen depletion that\nmay be, further compromised by extraneous stress impacts, like anxiety and\nfear, aspects that have traditionally eluded data modeling predictions."
    },
    {
        "date": "2025-05",
        "title": "Robust and Computation-Aware Gaussian Processes",
        "author": "Marshal Arijona Sinaga, Julien Martinelli, and Samuel Kaski",
        "link": "http://arxiv.org/abs/2505.21133v1",
        "abstract": "Gaussian processes (GPs) are widely used for regression and optimization\ntasks such as Bayesian optimization (BO) due to their expressiveness and\nprincipled uncertainty estimates. However, in settings with large datasets\ncorrupted by outliers, standard GPs and their sparse approximations struggle\nwith computational tractability and robustness. We introduce Robust\nComputation-aware Gaussian Process (RCaGP), a novel GP model that jointly\naddresses these challenges by combining a principled treatment of\napproximation-induced uncertainty with robust generalized Bayesian updating.\nThe key insight is that robustness and approximation-awareness are not\northogonal but intertwined: approximations can exacerbate the impact of\noutliers, and mitigating one without the other is insufficient. Unlike previous\nwork that focuses narrowly on either robustness or approximation quality, RCaGP\ncombines both in a principled and scalable framework, thus effectively managing\nboth outliers and computational uncertainties introduced by approximations such\nas low-rank matrix multiplications. Our model ensures more conservative and\nreliable uncertainty estimates, a property we rigorously demonstrate.\nAdditionally, we establish a robustness property and show that the mean\nfunction is key to preserving it, motivating a tailored model selection scheme\nfor robust mean functions. Empirical results confirm that solving these\nchallenges jointly leads to superior performance across both clean and\noutlier-contaminated settings, both on regression and high-throughput Bayesian\noptimization benchmarks."
    },
    {
        "date": "2025-05",
        "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing",
        "author": "Dehao Wang, Haohang Zhu, Yiwen Xu, and Kaiqi Liu",
        "link": "http://arxiv.org/abs/2505.21049v1",
        "abstract": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability."
    },
    {
        "date": "2025-05",
        "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
        "author": "Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, and Catarina Moreira",
        "link": "http://arxiv.org/abs/2505.21027v1",
        "abstract": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data."
    },
    {
        "date": "2025-05",
        "title": "Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models",
        "author": "Puwei Lian, Yujun Cai, and Songze Li",
        "link": "http://arxiv.org/abs/2505.20955v1",
        "abstract": "Diffusion models have achieved tremendous success in image generation, but\nthey also raise significant concerns regarding privacy and copyright issues.\nMembership Inference Attacks (MIAs) are designed to ascertain whether specific\ndata were utilized during a model's training phase. As current MIAs for\ndiffusion models typically exploit the model's image prediction ability, we\nformalize them into a unified general paradigm which computes the membership\nscore for membership identification. Under this paradigm, we empirically find\nthat existing attacks overlook the inherent deficiency in how diffusion models\nprocess high-frequency information. Consequently, this deficiency leads to\nmember data with more high-frequency content being misclassified as hold-out\ndata, and hold-out data with less high-frequency content tend to be\nmisclassified as member data. Moreover, we theoretically demonstrate that this\ndeficiency reduces the membership advantage of attacks, thereby interfering\nwith the effective discrimination of member data and hold-out data. Based on\nthis insight, we propose a plug-and-play high-frequency filter module to\nmitigate the adverse effects of the deficiency, which can be seamlessly\nintegrated into any attacks within this general paradigm without additional\ntime costs. Extensive experiments corroborate that this module significantly\nimproves the performance of baseline attacks across different datasets and\nmodels."
    },
    {
        "date": "2025-05",
        "title": "NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion",
        "author": "Max Collins, Jordan Vice, Tim French, and Ajmal Mian",
        "link": "http://arxiv.org/abs/2505.20934v1",
        "abstract": "Adversarial samples exploit irregularities in the manifold ``learned'' by\ndeep learning models to cause misclassifications. The study of these\nadversarial samples provides insight into the features a model uses to classify\ninputs, which can be leveraged to improve robustness against future attacks.\nHowever, much of the existing literature focuses on constrained adversarial\nsamples, which do not accurately reflect test-time errors encountered in\nreal-world settings. To address this, we propose `NatADiff', an adversarial\nsampling scheme that leverages denoising diffusion to generate natural\nadversarial samples. Our approach is based on the observation that natural\nadversarial samples frequently contain structural elements from the adversarial\nclass. Deep learning models can exploit these structural elements to shortcut\nthe classification process, rather than learning to genuinely distinguish\nbetween classes. To leverage this behavior, we guide the diffusion trajectory\ntowards the intersection of the true and adversarial classes, combining\ntime-travel sampling with augmented classifier guidance to enhance attack\ntransferability while preserving image fidelity. Our method achieves comparable\nattack success rates to current state-of-the-art techniques, while exhibiting\nsignificantly higher transferability across model architectures and better\nalignment with natural test-time errors as measured by FID. These results\ndemonstrate that NatADiff produces adversarial samples that not only transfer\nmore effectively across models, but more faithfully resemble naturally\noccurring test-time errors."
    },
    {
        "date": "2025-05",
        "title": "Towards a DSL for hybrid secure computation",
        "author": "Romain de Laage",
        "link": "http://arxiv.org/abs/2505.20912v1",
        "abstract": "Fully homomorphic encryption (FHE) and trusted execution environments (TEE)\nare two approaches to provide confidentiality during data processing. Each\napproach has its own strengths and weaknesses. In certain scenarios,\ncomputations can be carried out in a hybrid environment, using both FHE and\nTEE. However, processing data in such hybrid settings presents challenges, as\nit requires to adapt and rewrite the algorithms for the chosen technique. We\npropose a domain-specific language (DSL) for secure computation that allows to\nexpress the computations to perform and execute them using a backend that\nleverages either FHE or TEE, depending on what is available."
    },
    {
        "date": "2025-05",
        "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties",
        "author": "Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, and Edward Choi",
        "link": "http://arxiv.org/abs/2505.20875v2",
        "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our code and datasets\nare publicly available at https://github.com/jiyounglee-0523/TransEnV and\nhttps://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1."
    },
    {
        "date": "2025-05",
        "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks",
        "author": "Ta\u00efga Gon\u00e7alves, Tomo Miyazaki, and Shinichiro Omachi",
        "link": "http://arxiv.org/abs/2505.20782v1",
        "abstract": "We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for\ngenerating adversarial examples that mislead image classifiers toward any\ntarget class, including those not seen during training. Traditional targeted\nattacks are limited to one class per model, requiring expensive retraining for\neach target. Multi-targeted attacks address this by introducing a perturbation\ngenerator with a conditional input to specify the target class. However,\nexisting methods are constrained to classes observed during training and\nrequire access to the black-box model's training data--introducing a form of\ndata leakage that undermines realistic evaluation in practical black-box\nscenarios. We identify overreliance on class embeddings as a key limitation,\nleading to overfitting and poor generalization to unseen classes. To address\nthis, CD-MTA replaces class-level supervision with an image-based conditional\ninput and introduces class-agnostic losses that align the perturbed and target\nimages in the feature space. This design removes dependence on class semantics,\nthereby enabling generalization to unseen classes across datasets. Experiments\non ImageNet and seven other datasets show that CD-MTA outperforms prior\nmulti-targeted attacks in both standard and cross-domain settings--without\naccessing the black-box model's training data."
    },
    {
        "date": "2025-05",
        "title": "Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies",
        "author": "Kohei Obata, Yasuko Matsubara, and Yasushi Sakurai",
        "link": "http://arxiv.org/abs/2505.20765v1",
        "abstract": "Unsupervised anomaly detection in time series has been a pivotal research\narea for decades. Current mainstream approaches focus on learning normality, on\nthe assumption that all or most of the samples in the training set are normal.\nHowever, anomalies in the training set (i.e., anomaly contamination) can be\nmisleading. Recent studies employ data augmentation to generate\npseudo-anomalies and learn the boundary separating the training samples from\nthe augmented samples. Although this approach mitigates anomaly contamination\nif augmented samples mimic unseen real anomalies, it suffers from several\nlimitations. (1) Covering a wide range of time series anomalies is challenging.\n(2) It disregards augmented samples that resemble normal samples (i.e., false\nanomalies). (3) It places too much trust in the labels of training and\naugmented samples. In response, we propose RedLamp, which employs diverse data\naugmentations to generate multiclass pseudo-anomalies and learns the multiclass\nboundary. Such multiclass pseudo-anomalies cover a wide variety of time series\nanomalies. We conduct multiclass classification using soft labels, which\nprevents the model from being overconfident and ensures its robustness against\ncontaminated/false anomalies. The learned latent space is inherently\nexplainable as it is trained to separate pseudo-anomalies into multiclasses.\nExtensive experiments demonstrate the effectiveness of RedLamp in anomaly\ndetection and its robustness against anomaly contamination."
    },
    {
        "date": "2025-05",
        "title": "Adversarial bandit optimization for approximately linear functions",
        "author": "Zhuoyu Cheng, Kohei Hatano, and Eiji Takimoto",
        "link": "http://arxiv.org/abs/2505.20734v3",
        "abstract": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret."
    },
    {
        "date": "2025-05",
        "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment",
        "author": "Lingyu Qiu, Ke Jiang, and Xiaoyang Tan",
        "link": "http://arxiv.org/abs/2505.20653v1",
        "abstract": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA."
    },
    {
        "date": "2025-05",
        "title": "ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes",
        "author": "Akram Sheriff, Ken Huang, Zsolt Nemeth, and Madjid Nakhjiri",
        "link": "http://arxiv.org/abs/2505.23805v1",
        "abstract": "This paper introduces the Adaptive Defense Agent (ADA), an innovative\nAutomated Moving Target Defense (AMTD) system designed to fundamentally enhance\nthe security posture of AI workloads. ADA operates by continuously and\nautomatically rotating these workloads at the infrastructure level, leveraging\nthe inherent ephemerality of Kubernetes pods. This constant managed churn\nsystematically invalidates attacker assumptions and disrupts potential kill\nchains by regularly destroying and respawning AI service instances. This\nmethodology, applying principles of chaos engineering as a continuous,\nproactive defense, offers a paradigm shift from traditional static defenses\nthat rely on complex and expensive confidential or trusted computing solutions\nto secure the underlying compute platforms, while at the same time agnostically\nsupporting the latest advancements in agentic and nonagentic AI ecosystems and\nsolutions such as agent-to-agent (A2A) communication frameworks or model\ncontext protocols (MCP). This AI-native infrastructure design, relying on the\nwidely proliferated cloud-native Kubernetes technologies, facilitates easier\ndeployment, simplifies maintenance through an inherent zero trust posture\nachieved by rotation, and promotes faster adoption. We posit that ADA's novel\napproach to AMTD provides a more robust, agile, and operationally efficient\nzero-trust model for AI services, achieving security through proactive\nenvironmental manipulation rather than reactive patching."
    },
    {
        "date": "2025-05",
        "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
        "author": "Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, and Bin Ma",
        "link": "http://arxiv.org/abs/2505.20635v1",
        "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method."
    },
    {
        "date": "2025-05",
        "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
        "author": "Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2505.20621v1",
        "abstract": "Similar to other machine learning frameworks, Offline Reinforcement Learning\n(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on\nexternally sourced datasets, a vulnerability that is exacerbated by its\nsequential nature. To mitigate the risks posed by RL poisoning, we extend\ncertified defenses to provide larger guarantees against adversarial\nmanipulation, ensuring robustness for both per-state actions, and the overall\nexpected cumulative reward. Our approach leverages properties of Differential\nPrivacy, in a manner that allows this work to span both continuous and discrete\nspaces, as well as stochastic and deterministic environments -- significantly\nexpanding the scope and applicability of achievable guarantees. Empirical\nevaluations demonstrate that our approach ensures the performance drops to no\nmore than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly\nimproving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while\nproducing certified radii that is $5$ times larger as well. This highlights the\npotential of our framework to enhance safety and reliability in offline RL."
    },
    {
        "date": "2025-05",
        "title": "EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms",
        "author": "Jiaxiong He",
        "link": "http://arxiv.org/abs/2505.20614v1",
        "abstract": "This paper introduces EarthOL, a novel consensus protocol that attempts to\nreplace computational waste in blockchain systems with verifiable human\ncontributions within bounded domains. While recognizing the fundamental\nimpossibility of universal value assessment, we propose a domain-restricted\napproach that acknowledges cultural diversity and subjective preferences while\nmaintaining cryptographic security. Our enhanced Proof-of-Human-Contribution\n(PoHC) protocol uses a multi-layered verification system with domain-specific\nevaluation criteria, time-dependent validation mechanisms, and comprehensive\nsecurity frameworks. We present theoretical analysis demonstrating meaningful\nprogress toward incentive-compatible human contribution verification in\nhigh-consensus domains, achieving Byzantine fault tolerance in controlled\nscenarios while addressing significant scalability and cultural bias\nchallenges. Through game-theoretic analysis, probabilistic modeling, and\nenhanced security protocols, we identify specific conditions under which the\nprotocol remains stable and examine failure modes with comprehensive mitigation\nstrategies. This work contributes to understanding the boundaries of\ndecentralized value assessment and provides a framework for future research in\nhuman-centered consensus mechanisms for specific application domains, with\nparticular emphasis on validator and security specialist incentive systems."
    },
    {
        "date": "2025-05",
        "title": "One-shot Robust Federated Learning of Independent Component Analysis",
        "author": "Dian Jin, Xin Bing, and Yuqian Zhang",
        "link": "http://arxiv.org/abs/2505.20532v1",
        "abstract": "This paper investigates a general robust one-shot aggregation framework for\ndistributed and federated Independent Component Analysis (ICA) problem. We\npropose a geometric median-based aggregation algorithm that leverages $k$-means\nclustering to resolve the permutation ambiguity in local client estimations.\nOur method first performs k-means to partition client-provided estimators into\nclusters and then aggregates estimators within each cluster using the geometric\nmedian. This approach provably remains effective even in highly heterogeneous\nscenarios where at most half of the clients can observe only a minimal number\nof samples. The key theoretical contribution lies in the combined analysis of\nthe geometric median's error bound-aided by sample quantiles-and the maximum\nmisclustering rates of the aforementioned solution of $k$-means. The\neffectiveness of the proposed approach is further supported by simulation\nstudies conducted under various heterogeneous settings."
    },
    {
        "date": "2025-05",
        "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
        "author": "Aideen Fay, In\u00e9s Garc\u00eda-Redondo, Qiquan Wang, Haim Dubossarsky, and Anthea Monod",
        "link": "http://arxiv.org/abs/2505.20435v1",
        "abstract": "Understanding how adversarial conditions affect language models requires\ntechniques that capture both global structure and local detail within\nhigh-dimensional activation spaces. We propose persistent homology (PH), a tool\nfrom topological data analysis, to systematically characterize multiscale\nlatent space dynamics in LLMs under two distinct attack modes -- backdoor\nfine-tuning and indirect prompt injection. By analyzing six state-of-the-art\nLLMs, we show that adversarial conditions consistently compress latent\ntopologies, reducing structural diversity at smaller scales while amplifying\ndominant features at coarser ones. These topological signatures are\nstatistically robust across layers, architectures, model sizes, and align with\nthe emergence of adversarial effects deeper in the network. To capture\nfiner-grained mechanisms underlying these shifts, we introduce a neuron-level\nPH framework that quantifies how information flows and transforms within and\nacross layers. Together, our findings demonstrate that PH offers a principled\nand unifying approach to interpreting representational dynamics in LLMs,\nparticularly under distributional shift."
    },
    {
        "date": "2025-05",
        "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
        "author": "Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2505.20426v1",
        "abstract": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/"
    },
    {
        "date": "2025-05",
        "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
        "author": "Simin Fan, Maria Ios Glarou, and Martin Jaggi",
        "link": "http://arxiv.org/abs/2505.20380v1",
        "abstract": "The performance of large language models (LLMs) across diverse downstream\napplications is fundamentally governed by the quality and composition of their\npretraining corpora. Existing domain reweighting algorithms primarily optimize\ndata mixtures for a single target task, thereby resulting in models that\noverfit to specialized objectives while exhibiting substantial performance\ndegradation on other benchmarks. This paper introduces Group Robust\nMulti-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target\ndomain reweighting framework designed to calibrate pretraining data mixtures\nfor robust performance across multiple target tasks simultaneously. GRAPE\ndynamically adjusts sampling weights across source domains (domain weights)\nwhile concurrently modulating task weights that quantify the relative\nimportance of each individual target task. This adaptive process prioritizes\ntasks based on their learning difficulty throughout training. We formulate this\ninterleaved reweighting mechanism as a minimax optimization problem: The inner\nmaximization adjusts task weights leveraging group\ndistributed-robust-optimization (DRO), where those tasks demonstrating the\nleast improvement under the current data mixture are prioritized with higher\nweights; The outer minimization then optimizes domain weights to maximize loss\nreduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama\ndatasets demonstrate that GRAPE consistently outperforms baseline methods in\nterms of reasoning performance across 6 benchmarks. Furthermore, when applied\nto multilingual targets, GRAPE effectively identifies optimal training mixtures\nfrom mainstream languages, achieving superior language modeling capabilities\nacross 8 low-resource target languages."
    },
    {
        "date": "2025-05",
        "title": "An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks",
        "author": "Jinyan Wang, Liu Yang, Yuecen Wei, Jiaxuan Si, Chenhao Guo, Qingyun Sun, Xianxian Li, and Xingcheng Fu",
        "link": "http://arxiv.org/abs/2505.20074v1",
        "abstract": "Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains."
    },
    {
        "date": "2025-05",
        "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
        "author": "Xinping Chen, and Chen Liu",
        "link": "http://arxiv.org/abs/2505.20026v1",
        "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters."
    },
    {
        "date": "2025-05",
        "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
        "author": "Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, and Oleg Y. Rogov",
        "link": "http://arxiv.org/abs/2505.19951v1",
        "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths."
    },
    {
        "date": "2025-05",
        "title": "Cellwise and Casewise Robust Covariance in High Dimensions",
        "author": "Fabio Centofanti, Mia Hubert, and Peter J. Rousseeuw",
        "link": "http://arxiv.org/abs/2505.19925v1",
        "abstract": "The sample covariance matrix is a cornerstone of multivariate statistics, but\nit is highly sensitive to outliers. These can be casewise outliers, such as\ncases belonging to a different population, or cellwise outliers, which are\ndeviating cells (entries) of the data matrix. Recently some robust covariance\nestimators have been developed that can handle both types of outliers, but\ntheir computation is only feasible up to at most 20 dimensions. To remedy this\nwe propose the cellRCov method, a robust covariance estimator that\nsimultaneously handles casewise outliers, cellwise outliers, and missing data.\nIt relies on a decomposition of the covariance on principal and orthogonal\nsubspaces, leveraging recent work on robust PCA. It also employs a ridge-type\nregularization to stabilize the estimated covariance matrix. We establish some\ntheoretical properties of cellRCov, including its casewise and cellwise\ninfluence functions as well as consistency and asymptotic normality. A\nsimulation study demonstrates the superior performance of cellRCov in\ncontaminated and missing data scenarios. Furthermore, its practical utility is\nillustrated in a real-world application to anomaly detection. We also construct\nand illustrate the cellRCCA method for robust and regularized canonical\ncorrelation analysis."
    },
    {
        "date": "2025-05",
        "title": "CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models",
        "author": "Chunyang Li, Junwei Zhang, Anda Cheng, Zhuo Ma, Xinghua Li, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2505.19864v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge, but its openness introduces vulnerabilities\nthat can be exploited by poisoning attacks. Existing poisoning methods for RAG\nsystems have limitations, such as poor generalization and lack of fluency in\nadversarial texts. In this paper, we propose CPA-RAG, a black-box adversarial\nframework that generates query-relevant texts capable of manipulating the\nretrieval process to induce target answers. The proposed method integrates\nprompt-based text generation, cross-guided optimization through multiple LLMs,\nand retriever-based scoring to construct high-quality adversarial samples. We\nconduct extensive experiments across multiple datasets and LLMs to evaluate its\neffectiveness. Results show that the framework achieves over 90\\% attack\nsuccess when the top-k retrieval setting is 5, matching white-box performance,\nand maintains a consistent advantage of approximately 5 percentage points\nacross different top-k values. It also outperforms existing black-box baselines\nby 14.5 percentage points under various defense strategies. Furthermore, our\nmethod successfully compromises a commercial RAG system deployed on Alibaba's\nBaiLian platform, demonstrating its practical threat in real-world\napplications. These findings underscore the need for more robust and secure RAG\nframeworks to defend against poisoning attacks."
    },
    {
        "date": "2025-05",
        "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
        "author": "Binyan Xu, Xilin Dai, Di Tang, and Kehuan Zhang",
        "link": "http://arxiv.org/abs/2505.19840v1",
        "abstract": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone\nto adversarial attacks. Typically, such attacks either involve frequent queries\nto the target model or rely on surrogate models closely mirroring the target\nmodel -- often trained with subsets of the target model's training data -- to\nachieve high attack success rates through transferability. However, in\nrealistic scenarios where training data is inaccessible and excessive queries\ncan raise alarms, crafting adversarial examples becomes more challenging. In\nthis paper, we present UnivIntruder, a novel attack framework that relies\nsolely on a single, publicly available CLIP model and publicly available\ndatasets. By using textual concepts, UnivIntruder generates universal,\ntransferable, and targeted adversarial perturbations that mislead DNNs into\nmisclassifying inputs into adversary-specified classes defined by textual\nconcepts.\n  Our extensive experiments show that our approach achieves an Attack Success\nRate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly\noutperforming existing transfer-based methods. Additionally, we reveal\nreal-world vulnerabilities, showing that even without querying target models,\nUnivIntruder compromises image search engines like Google and Baidu with ASR\nrates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR\nrates up to 80%. These findings underscore the practicality of our attack in\nscenarios where traditional avenues are blocked, highlighting the need to\nreevaluate security paradigms in AI applications."
    },
    {
        "date": "2025-05",
        "title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks",
        "author": "Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, and Shouling Ji",
        "link": "http://arxiv.org/abs/2505.19821v1",
        "abstract": "Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations."
    },
    {
        "date": "2025-05",
        "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning",
        "author": "Bariscan Bozkurt, Houssam Zenati, Dimitri Meunier, Liyuan Xu, and Arthur Gretton",
        "link": "http://arxiv.org/abs/2505.19807v1",
        "abstract": "We study the problem of causal function estimation in the Proxy Causal\nLearning (PCL) framework, where confounders are not observed but proxies for\nthe confounders are available. Two main approaches have been proposed: outcome\nbridge-based and treatment bridge-based methods. In this work, we propose two\nkernel-based doubly robust estimators that combine the strengths of both\napproaches, and naturally handle continuous and high-dimensional variables. Our\nidentification strategy builds on a recent density ratio-free method for\ntreatment bridge-based PCL; furthermore, in contrast to previous approaches, it\ndoes not require indicator functions or kernel smoothing over the treatment\nvariable. These properties make it especially well-suited for continuous or\nhigh-dimensional treatments. By using kernel mean embeddings, we have\nclosed-form solutions and strong consistency guarantees. Our estimators\noutperform existing methods on PCL benchmarks, including a prior doubly robust\nmethod that requires both kernel smoothing and density ratio estimation."
    },
    {
        "date": "2025-05",
        "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
        "author": "Sangyeop Kim, Yohan Lee, Yongwoo Song, and Kimin Lee",
        "link": "http://arxiv.org/abs/2505.19773v1",
        "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."
    },
    {
        "date": "2025-05",
        "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
        "author": "Bingrui Sima, Linhua Cong, Wenxuan Wang, and Kun He",
        "link": "http://arxiv.org/abs/2505.19684v2",
        "abstract": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks."
    },
    {
        "date": "2025-05",
        "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2505.19613v1",
        "abstract": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization."
    },
    {
        "date": "2025-05",
        "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts",
        "author": "Shadi Alijani, and Homayoun Najjaran",
        "link": "http://arxiv.org/abs/2505.19587v1",
        "abstract": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."
    },
    {
        "date": "2025-05",
        "title": "Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things",
        "author": "Kai Li, Conggai Li, Xin Yuan, Shenghong Li, Sai Zou, Syed Sohail Ahmed, Wei Ni, Dusit Niyato, Abbas Jamalipour, Falko Dressler, and Ozgur B. Akan",
        "link": "http://arxiv.org/abs/2505.23792v1",
        "abstract": "This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm\nthat embeds zero-trust security principles into the lifecycle of foundation\nmodels (FMs) for Internet of Things (IoT) systems. By integrating core tenets,\nsuch as continuous verification, least privilege access (LPA), data\nconfidentiality, and behavioral analytics into the design, training, and\ndeployment of FMs, ZTFMs can enable secure, privacy-preserving AI across\ndistributed, heterogeneous, and potentially adversarial IoT environments. We\npresent the first structured synthesis of ZTFMs, identifying their potential to\ntransform conventional trust-based IoT architectures into resilient,\nself-defending ecosystems. Moreover, we propose a comprehensive technical\nframework, incorporating federated learning (FL), blockchain-based identity\nmanagement, micro-segmentation, and trusted execution environments (TEEs) to\nsupport decentralized, verifiable intelligence at the network edge. In\naddition, we investigate emerging security threats unique to ZTFM-enabled\nsystems and evaluate countermeasures, such as anomaly detection, adversarial\ntraining, and secure aggregation. Through this analysis, we highlight key open\nresearch challenges in terms of scalability, secure orchestration,\ninterpretable threat attribution, and dynamic trust calibration. This survey\nlays a foundational roadmap for secure, intelligent, and trustworthy IoT\ninfrastructures powered by FMs."
    },
    {
        "date": "2025-05",
        "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare",
        "author": "Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, and Jie M. Zhang",
        "link": "http://arxiv.org/abs/2505.19562v1",
        "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI."
    },
    {
        "date": "2025-05",
        "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds",
        "author": "Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Mehrdad Noori, Gustavo Adolfo Vargas Hakim, David Osowiechi, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers",
        "link": "http://arxiv.org/abs/2505.19546v1",
        "abstract": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC."
    }
]