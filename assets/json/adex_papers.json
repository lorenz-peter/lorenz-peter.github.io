[
    {
        "date": "2025-02",
        "title": "The Round Complexity of Black-Box Post-Quantum Secure Computation",
        "author": "Rohit Chatterjee, Xiao Liang, Omkant Pandey, and Takashi Yamakawa",
        "link": "http://arxiv.org/abs/2502.13830v1",
        "abstract": "We study the round complexity of secure multi-party computation (MPC) in the\npost-quantum regime. Our focus is on the fully black-box setting, where both\nthe construction and security reduction are black-box. Chia, Chung, Liu, and\nYamakawa [FOCS'22] demonstrated the infeasibility of achieving standard\nsimulation-based security within constant rounds unless $\\mathbf{NP} \\subseteq\n\\mathbf{BQP}$. This leaves crucial feasibility questions unresolved.\nSpecifically, it remains unknown whether black-box constructions are achievable\nwithin polynomial rounds; also, the existence of constant-round constructions\nwith respect to $\\epsilon$-simulation, a relaxed yet useful alternative to\nstandard simulation, remains unestablished.\n  This work provides positive answers. We introduce the first black-box\nconstruction for PQ-MPC in polynomial rounds, from the minimal assumption of\npost-quantum semi-honest oblivious transfers. In the two-party scenario, our\nconstruction requires only $\\omega(1)$ rounds. These results have already been\napplied in the oracle separation between classical-communication quantum MPC\nand $\\mathbf{P} = \\mathbf{NP}$ in Kretschmer, Qian, and Tal [STOC'25].\n  As for $\\epsilon$-simulation, Chia, Chung, Liang, and Yamakawa [CRYPTO'22]\nresolved the issue for the two-party setting, leaving the multi-party case\nopen. We complete the picture by presenting the first black-box, constant-round\nconstruction in the multi-party setting, instantiable using various standard\npost-quantum primitives.\n  En route, we obtain a black-box, constant-round post-quantum commitment\nachieving a weaker version of 1-many non-malleability, from post-quantum\none-way functions. Besides its role in our MPC construction, this commitment\nalso reduces the assumption used in the quantum parallel repetition lower bound\nby Bostanci, Qian, Spooner, and Yuen [STOC'24]. We anticipate further\napplications in the future."
    },
    {
        "date": "2025-02",
        "title": "RobustX: Robust Counterfactual Explanations Made Easy",
        "author": "Junqi Jiang, Luca Marzari, Aaryan Purohit, and Francesco Leofante",
        "link": "http://arxiv.org/abs/2502.13751v1",
        "abstract": "The increasing use of Machine Learning (ML) models to aid decision-making in\nhigh-stakes industries demands explainability to facilitate trust.\nCounterfactual Explanations (CEs) are ideally suited for this, as they can\noffer insights into the predictions of an ML model by illustrating how changes\nin its input data may lead to different outcomes. However, for CEs to realise\ntheir explanatory potential, significant challenges remain in ensuring their\nrobustness under slight changes in the scenario being explained. Despite the\nwidespread recognition of CEs' robustness as a fundamental requirement, a lack\nof standardised tools and benchmarks hinders a comprehensive and effective\ncomparison of robust CE generation methods. In this paper, we introduce\nRobustX, an open-source Python library implementing a collection of CE\ngeneration and evaluation methods, with a focus on the robustness property.\nRobustX provides interfaces to several existing methods from the literature,\nenabling streamlined access to state-of-the-art techniques. The library is also\neasily extensible, allowing fast prototyping of novel robust CE generation and\nevaluation methods."
    },
    {
        "date": "2025-02",
        "title": "Robust Counterfactual Inference in Markov Decision Processes",
        "author": "Jessica Lally, Milad Kazemi, and Nicola Paoletti",
        "link": "http://arxiv.org/abs/2502.13731v1",
        "abstract": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
    },
    {
        "date": "2025-02",
        "title": "Secure Federated Data Distillation",
        "author": "Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, and Antonino Nocera",
        "link": "http://arxiv.org/abs/2502.13728v1",
        "abstract": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications."
    },
    {
        "date": "2025-02",
        "title": "What Skills Do Cyber Security Professionals Need?",
        "author": "Faheem Ullah, Xiaohan Ye, Uswa Fatima, Zahid Akhtar, Yuxi Wu, and Hussain Ahmad",
        "link": "http://arxiv.org/abs/2502.13658v1",
        "abstract": "Purpose: The increasing number of cyber-attacks has elevated the importance\nof cybersecurity for organizations. This has also increased the demand for\nprofessionals with the necessary skills to protect these organizations. As a\nresult, many individuals are looking to enter the field of cybersecurity.\nHowever, there is a lack of clear understanding of the skills required for a\nsuccessful career in this field. In this paper, we identify the skills required\nfor cybersecurity professionals. We also determine how the demand for cyber\nskills relates to various cyber roles such as security analyst and security\narchitect. Furthermore, we identify the programming languages that are\nimportant for cybersecurity professionals. Design/Methodology: For this study,\nwe have collected and analyzed data from 12,161 job ads and 49,002 Stack\nOverflow posts. By examining this, we identified patterns and trends related to\nskill requirements, role-specific demands, and programming languages in\ncybersecurity. Findings: Our results reveal that (i) communication skills and\nproject management skills are the most important soft skills, (ii) as compared\nto soft skills, the demand for technical skills varies more across various\ncyber roles, and (iii) Java is the most commonly used programming language.\nOriginality: Our findings serve as a guideline for individuals aiming to get\ninto the field of cybersecurity. Moreover, our findings are useful in terms of\ninforming educational institutes to teach the correct set of skills to students\ndoing degrees in cybersecurity."
    },
    {
        "date": "2025-02",
        "title": "Toward Robust Non-Transferable Learning: A Survey and Benchmark",
        "author": "Ziming Hong, Yongli Xiang, and Tongliang Liu",
        "link": "http://arxiv.org/abs/2502.13593v1",
        "abstract": "Over the past decades, researchers have primarily focused on improving the\ngeneralization abilities of models, with limited attention given to regulating\nsuch generalization. However, the ability of models to generalize to unintended\ndata (e.g., harmful or unauthorized data) can be exploited by malicious\nadversaries in unforeseen ways, potentially resulting in violations of model\nethics. Non-transferable learning (NTL), a task aimed at reshaping the\ngeneralization abilities of deep learning models, was proposed to address these\nchallenges. While numerous methods have been proposed in this field, a\ncomprehensive review of existing progress and a thorough analysis of current\nlimitations remain lacking. In this paper, we bridge this gap by presenting the\nfirst comprehensive survey on NTL and introducing NTLBench, the first benchmark\nto evaluate NTL performance and robustness within a unified framework.\nSpecifically, we first introduce the task settings, general framework, and\ncriteria of NTL, followed by a summary of NTL approaches. Furthermore, we\nemphasize the often-overlooked issue of robustness against various attacks that\ncan destroy the non-transferable mechanism established by NTL. Experiments\nconducted via NTLBench verify the limitations of existing NTL methods in\nrobustness. Finally, we discuss the practical applications of NTL, along with\nits future directions and associated challenges."
    },
    {
        "date": "2025-02",
        "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
        "author": "Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, and Lei Zou",
        "link": "http://arxiv.org/abs/2502.13527v1",
        "abstract": "The rise of Large Language Models (LLMs) has led to significant applications\nbut also introduced serious security threats, particularly from jailbreak\nattacks that manipulate output generation. These attacks utilize prompt\nengineering and logit manipulation to steer models toward harmful content,\nprompting LLM providers to implement filtering and safety alignment strategies.\nWe investigate LLMs' safety mechanisms and their recent applications, revealing\na new threat model targeting structured output interfaces, which enable\nattackers to manipulate the inner logit during LLM generation, requiring only\nAPI access permissions. To demonstrate this threat model, we introduce a\nblack-box attack framework called AttackPrefixTree (APT). APT exploits\nstructured output interfaces to dynamically construct attack patterns. By\nleveraging prefixes of models' safety refusal response and latent harmful\noutputs, APT effectively bypasses safety measures. Experiments on benchmark\ndatasets indicate that this approach achieves higher attack success rate than\nexisting methods. This work highlights the urgent need for LLM providers to\nenhance security protocols to address vulnerabilities arising from the\ninteraction between safety patterns and structured outputs."
    },
    {
        "date": "2025-02",
        "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
        "author": "Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu",
        "link": "http://arxiv.org/abs/2502.13407v1",
        "abstract": "Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, all-inclusive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5\nto 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation\n(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD\ndatasets demonstrate that the MTKD framework significantly improves the\nperformance of CD models with various network architectures and parameter\nsizes, achieving new state-of-the-art results. The code is available at\nhttps://github.com/circleLZY/MTKD-CD."
    },
    {
        "date": "2025-02",
        "title": "CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks",
        "author": "Ke Jiang, Sen Deng, Yinshuai Li, Shuai Wang, Tianwei Zhang, and Yinqian Zhang",
        "link": "http://arxiv.org/abs/2502.13401v1",
        "abstract": "Cryptographic implementations bolster security against timing side-channel\nattacks by integrating constant-time components. However, the new ciphertext\nside channels resulting from the deterministic memory encryption in Trusted\nExecution Environments (TEEs), enable ciphertexts to manifest identifiable\npatterns when being sequentially written to the same memory address. Attackers\nwith read access to encrypted memory in TEEs can potentially deduce plaintexts\nby analyzing these changing ciphertext patterns.\n  In this paper, we design CipherGuard, a compiler-aided mitigation methodology\nto counteract ciphertext side channels with high efficiency and security.\nCipherGuard is based on the LLVM ecosystem, and encompasses multiple mitigation\nstrategies, including software-based probabilistic encryption and secret-aware\nregister allocation. Through a comprehensive evaluation, we demonstrate that\nCipherGuard can strengthen the security of various cryptographic\nimplementations more efficiently than existing state-of-the-art defense\nmechanism, i.e., CipherFix."
    },
    {
        "date": "2025-02",
        "title": "Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios",
        "author": "Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, and Qi Wu",
        "link": "http://arxiv.org/abs/2502.13345v1",
        "abstract": "Latent diffusion models have exhibited considerable potential in generative\ntasks. Watermarking is considered to be an alternative to safeguard the\ncopyright of generative models and prevent their misuse. However, in the\ncontext of model distribution scenarios, the accessibility of models to large\nscale of model users brings new challenges to the security, efficiency and\nrobustness of existing watermark solutions. To address these issues, we propose\na secure and efficient watermarking solution. A new security mechanism is\ndesigned to prevent watermark leakage and watermark escape, which considers\nwatermark randomness and watermark-model association as two constraints for\nmandatory watermark injection. To reduce the time cost of training the security\nmodule, watermark injection and the security mechanism are decoupled, ensuring\nthat fine-tuning VAE only accomplishes the security mechanism without the\nburden of learning watermark patterns. A watermark distribution-based\nverification strategy is proposed to enhance the robustness against diverse\nattacks in the model distribution scenarios. Experimental results prove that\nour watermarking consistently outperforms existing six baselines on\neffectiveness and robustness against ten image processing attacks and\nadversarial attacks, while enhancing security in the distribution scenarios."
    },
    {
        "date": "2025-02",
        "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
        "author": "Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, and Weijie Zhao",
        "link": "http://arxiv.org/abs/2502.13141v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs."
    },
    {
        "date": "2025-02",
        "title": "The Role of GitHub Copilot on Software Development: A Perspec-tive on Productivity, Security, Best Practices and Future Directions",
        "author": "Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, and Akhil Dusi",
        "link": "http://arxiv.org/abs/2502.13199v1",
        "abstract": "GitHub Copilot is transforming software development by automating tasks and\nboosting productivity through AI-driven code generation. In this paper, we\ncon-duct a literature survey to synthesize insights on Copilot's impact on\nproductivity and security. We review academic journal databases, industry\nreports, and official docu-mentation to highlight key findings and challenges.\nWhile Copilot accelerates coding and prototyping, concerns over security\nvulnerabilities and intellectual property risks persist. Drawing from the\nliterature, we provide a perspective on best practices and future directions\nfor responsible AI adoption in software engineering, offering action-able\ninsights for developers and organizations to integrate Copilot effectively\nwhile maintaining high standards of quality and security."
    },
    {
        "date": "2025-02",
        "title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection",
        "author": "Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2502.13071v1",
        "abstract": "While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from\nenvironmental and intrinsic disturbances. Poor lighting or adverse weather\nconditions degrade camera performance, while radar suffers from noise and\npositional ambiguity. Achieving robust radar-camera 3D object detection\nrequires consistent performance across varying conditions, a topic that has not\nyet been fully explored. In this work, we first conduct a systematic analysis\nof robustness in radar-camera detection on five kinds of noises and propose\nRobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D\nGaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,\nincluding position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS\nand velocity priors to generate a deformable kernel map and variance for kernel\nsize adjustment and value distribution. Additionally, we introduce a\nweather-adaptive fusion module, which adaptively fuses radar and camera\nfeatures based on camera signal confidence. Extensive experiments on the\npopular benchmark, nuScenes, show that our model achieves competitive results\nin regular and noisy conditions."
    },
    {
        "date": "2025-02",
        "title": "Sublinear-Overhead Secure Linear Algebra on a Dishonest Server",
        "author": "Mark Braverman, and Stephen Newman",
        "link": "http://arxiv.org/abs/2502.13060v1",
        "abstract": "Most heavy computation occurs on servers owned by a second party. This\nreduces data privacy, resulting in interest in data-oblivious computation,\nwhich typically severely degrades performance. Secure and fast remote\ncomputation is particularly important for linear algebra, which comprises a\nlarge fraction of total computation and is best run on highly specialized\nhardware often only accessible through the cloud. We state the natural\nefficiency and security desiderata for fast, remote, and data-oblivious linear\nalgebra, conjecture the existence of matrix and vector families implying\nsatisfactory algorithms, and provide such an algorithm contingent on common\ncryptographic assumptions. We achieve sublinear overhead for the server,\ndramatically reduced computation cost for the client, and various other\npractical advantages over previous algorithms.\n  Keywords: Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic\nEncryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN,\nMatrix Multiplication."
    },
    {
        "date": "2025-02",
        "title": "Preventing the Popular Item Embedding Based Attack in Federated Recommendations",
        "author": "Jun Zhang, Huan Li, Dazhong Rong, Yan Zhao, Ke Chen, and Lidan Shou",
        "link": "http://arxiv.org/abs/2502.12958v1",
        "abstract": "Privacy concerns have led to the rise of federated recommender systems (FRS),\nwhich can create personalized models across distributed clients. However, FRS\nis vulnerable to poisoning attacks, where malicious users manipulate gradients\nto promote their target items intentionally. Existing attacks against FRS have\nlimitations, as they depend on specific models and prior knowledge, restricting\ntheir real-world applicability. In our exploration of practical FRS\nvulnerabilities, we devise a model-agnostic and prior-knowledge-free attack,\nnamed PIECK (Popular Item Embedding based Attack). The core module of PIECK is\npopular item mining, which leverages embedding changes during FRS training to\neffectively identify the popular items. Built upon the core module, PIECK\nbranches into two diverse solutions: The PIECKIPE solution employs an item\npopularity enhancement module, which aligns the embeddings of targeted items\nwith the mined popular items to increase item exposure. The PIECKUEA further\nenhances the robustness of the attack by using a user embedding approximation\nmodule, which approximates private user embeddings using mined popular items.\nUpon identifying PIECK, we evaluate existing federated defense methods and find\nthem ineffective against PIECK, as poisonous gradients inevitably overwhelm the\ncold target items. We then propose a novel defense method by introducing two\nregularization terms during user training, which constrain item popularity\nenhancement and user embedding approximation while preserving FRS performance.\nWe evaluate PIECK and its defense across two base models, three real datasets,\nfour top-tier attacks, and six general defense methods, affirming the efficacy\nof both PIECK and its defense."
    },
    {
        "date": "2025-02",
        "title": "Strands Rocq: Why is a Security Protocol Correct, Mechanically?",
        "author": "Matteo Busi, Riccardo Focardi, and Flaminia L. Luccio",
        "link": "http://arxiv.org/abs/2502.12848v1",
        "abstract": "Strand spaces are a formal framework for symbolic protocol verification that\nallows for pen-and-paper proofs of security. While extremely insightful,\npen-and-paper proofs are error-prone, and it is hard to gain confidence on\ntheir correctness. To overcome this problem, we developed StrandsRocq, a full\nmechanization of the strand spaces in Coq (soon to be renamed Rocq). The\nmechanization was designed to be faithful to the original pen-and-paper\ndevelopment, and it was engineered to be modular and extensible. StrandsRocq\nincorporates new original proof techniques, a novel notion of maximal\npenetrator that enables protocol compositionality, and a set of Coq tactics\ntailored to the domain, facilitating proof automation and reuse, and\nsimplifying the work of protocol analysts. To demonstrate the versatility of\nour approach, we modelled and analyzed a family of authentication protocols,\ndrawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical\nNeedham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis\nfor a key management API. The analyses in StrandsRocq confirmed the high degree\nof proof reuse, and enabled us to distill the minimal requirements for protocol\nsecurity. Through mechanization, we identified and addressed several issues in\nthe original proofs and we were able to significantly improve the precision of\nthe static analysis for the key management API. Moreover, we were able to\nleverage the novel notion of maximal penetrator to provide a compositional\nproof of security for two simple authentication protocols."
    },
    {
        "date": "2025-02",
        "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training",
        "author": "Yuanfan Li, Zhaohan Zhang, Chengzhengxu Li, Chao Shen, and Xiaoming Liu",
        "link": "http://arxiv.org/abs/2502.12734v1",
        "abstract": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 9 text perturbation strategies and 5\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches."
    },
    {
        "date": "2025-02",
        "title": "Chronus: Understanding and Securing the Cutting-Edge Industry Solutions to DRAM Read Disturbance",
        "author": "O\u011fuzhan Canpolat, A. Giray Ya\u011fl\u0131k\u00e7\u0131, Geraldo F. Oliveira, Ataberk Olgun, Nisa Bostanc\u0131, \u0130smail Emir Y\u00fcksel, Haocong Luo, O\u011fuz Ergin, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2502.12650v1",
        "abstract": "We 1) present the first rigorous security, performance, energy, and cost\nanalyses of the state-of-the-art on-DRAM-die read disturbance mitigation\nmethod, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new\nmechanism that addresses PRAC's two major weaknesses. Our analysis shows that\nPRAC's system performance overhead on benign applications is non-negligible for\nmodern DRAM chips and prohibitively large for future DRAM chips that are more\nvulnerable to read disturbance. We identify two weaknesses of PRAC that cause\nthese overheads. First, PRAC increases critical DRAM access latency parameters\ndue to the additional time required to increment activation counters. Second,\nPRAC performs a constant number of preventive refreshes at a time, making it\nvulnerable to an adversarial access pattern, known as the wave attack, and\nconsequently requiring it to be configured for significantly smaller activation\nthresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die\nRowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation\ncounters concurrently while serving accesses by separating counters from the\ndata and 2) prevents the wave attack by dynamically controlling the number of\npreventive refreshes performed. Our performance analysis shows that Chronus's\nsystem performance overhead is near-zero for modern DRAM chips and very low for\nfuture DRAM chips. Chronus outperforms three variants of PRAC and three other\nstate-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's\nimplications for future systems and foreshadow future research directions. To\naid future research, we open-source our Chronus implementation at\nhttps://github.com/CMU-SAFARI/Chronus."
    },
    {
        "date": "2025-02",
        "title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach",
        "author": "Tvrtko Sternak, Davor Runje, Dorian Grano\u0161a, and Chi Wang",
        "link": "http://arxiv.org/abs/2502.12630v1",
        "abstract": "This paper presents a novel approach to evaluating the security of large\nlanguage models (LLMs) against prompt leakage-the exposure of system-level\nprompts or proprietary configurations. We define prompt leakage as a critical\nthreat to secure LLM deployment and introduce a framework for testing the\nrobustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we\nimplement a multi-agent system where cooperative agents are tasked with probing\nand exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further\ndefine a prompt leakage-safe system as one in which an attacker cannot\ndistinguish between two agents: one initialized with an original prompt and the\nother with a prompt stripped of all sensitive information. In a safe system,\nthe agents' outputs will be indistinguishable to the attacker, ensuring that\nsensitive information remains secure. This cryptographically inspired framework\nprovides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of\nprompt leakage, bridging the gap between automated threat modeling and\npractical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub."
    },
    {
        "date": "2025-02",
        "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
        "author": "Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, and Sen Su",
        "link": "http://arxiv.org/abs/2502.12575v1",
        "abstract": "As LLM-based agents become increasingly prevalent, backdoors can be implanted\ninto agents through user queries or environment feedback, raising critical\nconcerns regarding safety vulnerabilities. However, backdoor attacks are\ntypically detectable by safety audits that analyze the reasoning process of\nagents. To this end, we propose a novel backdoor implantation strategy called\n\\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}.\nSpecifically, we introduce dynamic encryption, which maps the backdoor into\nbenign content, effectively circumventing safety audits. To enhance\nstealthiness, we further decompose the backdoor into multiple sub-backdoor\nfragments. Based on these advancements, backdoors are allowed to bypass safety\naudits significantly. Additionally, we present AgentBackdoorEval, a dataset\ndesigned for the comprehensive evaluation of agent backdoor attacks.\nExperimental results across multiple datasets demonstrate that our method\nachieves an attack success rate nearing 100\\% while maintaining a detection\nrate of 0\\%, illustrating its effectiveness in evading safety audits. Our\nfindings highlight the limitations of existing safety mechanisms in detecting\nadvanced attacks, underscoring the urgent need for more robust defenses against\nbackdoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
        "author": "Wenpeng Xing, Minghao Li, Mohan Li, and Meng Han",
        "link": "http://arxiv.org/abs/2502.13175v1",
        "abstract": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI."
    },
    {
        "date": "2025-02",
        "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
        "author": "Kounianhua Du, Hanjing Wang, Jianxing Liu, Jizheng Chen, Xinyi Dai, Yasheng Wang, Ruiming Tang, Yong Yu, Jun Wang, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2502.12492v1",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution."
    },
    {
        "date": "2025-02",
        "title": "PKE and ABE with Collusion-Resistant Secure Key Leasing",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2502.12491v1",
        "abstract": "Secure key leasing (SKL) is an advanced encryption functionality that allows\na secret key holder to generate a quantum decryption key and securely lease it\nto a user. Once the user returns the quantum decryption key (or provides a\nclassical certificate confirming its deletion), they lose their decryption\ncapability. Previous works on public key encryption with SKL (PKE-SKL) have\nonly considered the single-key security model, where the adversary receives at\nmost one quantum decryption key. However, this model does not accurately\nreflect real-world applications of PKE-SKL. To address this limitation, we\nintroduce collusion-resistant security for PKE-SKL (denoted as PKE-CR-SKL). In\nthis model, the adversary can adaptively obtain multiple quantum decryption\nkeys and access a verification oracle which validates the correctness of\nqueried quantum decryption keys. Importantly, the size of the public key and\nciphertexts must remain independent of the total number of generated quantum\ndecryption keys. We present the following constructions:\n  - A PKE-CR-SKL scheme based on the learning with errors (LWE) assumption.\n  - An attribute-based encryption scheme with collusion-resistant SKL\n(ABE-CR-SKL), also based on the LWE assumption.\n  - An ABE-CR-SKL scheme with classical certificates, relying on multi-input\nABE with polynomial arity."
    },
    {
        "date": "2025-02",
        "title": "Software Security in Software-Defined Networking: A Systematic Literature Review",
        "author": "Moustapha Awwalou Diouf, Samuel Ouya, Jacques Klein, and Tegawend\u00e9 F. Bissyand\u00e9",
        "link": "http://arxiv.org/abs/2502.13828v1",
        "abstract": "Software-defined networking (SDN) has shifted network management by\ndecoupling the data and control planes. This enables programmatic control via\nsoftware applications using open APIs. SDN's programmability has fueled its\npopularity but may have opened issues extending the attack surface by\nintroducing vulnerable software. Therefore, the research community needs to\nhave a deep and broad understanding of the risks posed by SDN to propose\nmitigating measures. The literature, however, lacks a comprehensive review of\nthe current state of research in this direction. This paper addresses this gap\nby providing a comprehensive overview of the state-of-the-art research in SDN\nsecurity focusing on the software (i.e., the controller, APIs, applications)\npart. We systematically reviewed 58 relevant publications to analyze trends,\nidentify key testing and analysis methodologies, and categorize studied\nvulnerabilities. We further explore areas where the research community can make\nsignificant contributions. This work offers the most extensive and in-depth\nanalysis of SDN software security to date."
    },
    {
        "date": "2025-02",
        "title": "Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning",
        "author": "Mengshi Qi, Changsheng Lv, and Huadong Ma",
        "link": "http://arxiv.org/abs/2502.12425v1",
        "abstract": "In this paper, we propose a new Robust Disentangled Counterfactual Learning\n(RDCL) approach for physical audiovisual commonsense reasoning. The task aims\nto infer objects' physics commonsense based on both video and audio input, with\nthe main challenge being how to imitate the reasoning ability of humans, even\nunder the scenario of missing modalities. Most of the current methods fail to\ntake full advantage of different characteristics in multi-modal data, and\nlacking causal reasoning ability in models impedes the progress of implicit\nphysical knowledge inferring. To address these issues, our proposed RDCL method\ndecouples videos into static (time-invariant) and dynamic (time-varying)\nfactors in the latent space by the disentangled sequential encoder, which\nadopts a variational autoencoder (VAE) to maximize the mutual information with\na contrastive loss function. Furthermore, we introduce a counterfactual\nlearning module to augment the model's reasoning ability by modeling physical\nknowledge relationships among different objects under counterfactual\nintervention. To alleviate the incomplete modality data issue, we introduce a\nrobust multimodal learning method to recover the missing data by decomposing\nthe shared features and model-specific features. Our proposed method is a\nplug-and-play module that can be incorporated into any baseline including VLMs.\nIn experiments, we show that our proposed method improves the reasoning\naccuracy and robustness of baseline methods and achieves the state-of-the-art\nperformance."
    },
    {
        "date": "2025-02",
        "title": "Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness",
        "author": "Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, and Meie Fang",
        "link": "http://arxiv.org/abs/2502.12418v1",
        "abstract": "Color constancy estimates illuminant chromaticity to correct color-biased\nimages. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models\nhave made substantial advancements. Nevertheless, the potential risks in DNNCC\ndue to the vulnerability of deep neural networks have not yet been explored. In\nthis paper, we conduct the first investigation into the impact of a key factor\nin color constancy-brightness-on DNNCC from a robustness perspective. Our\nevaluation reveals that several mainstream DNNCC models exhibit high\nsensitivity to brightness despite their focus on chromaticity estimation. This\nsheds light on a potential limitation of existing DNNCC models: their\nsensitivity to brightness may hinder performance given the widespread\nbrightness variations in real-world datasets. From the insights of our\nanalysis, we propose a simple yet effective brightness robustness enhancement\nstrategy for DNNCC models, termed BRE. The core of BRE is built upon the\nadaptive step-size adversarial brightness augmentation technique, which\nidentifies high-risk brightness variation and generates augmented images via\nexplicit brightness adjustment. Subsequently, BRE develops a\nbrightness-robustness-aware model optimization strategy that integrates\nadversarial brightness training and brightness contrastive loss, significantly\nbolstering the brightness robustness of DNNCC models. BRE is\nhyperparameter-free and can be integrated into existing DNNCC models, without\nincurring additional overhead during the testing phase. Experiments on two\npublic color constancy datasets-ColorChecker and Cube+-demonstrate that the\nproposed BRE consistently enhances the illuminant estimation performance of\nexisting DNNCC models, reducing the estimation error by an average of 5.04%\nacross six mainstream DNNCC models, underscoring the critical role of enhancing\nbrightness robustness in these models."
    },
    {
        "date": "2025-02",
        "title": "Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?",
        "author": "Blaine Hoak, Kunyang Li, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2502.12377v1",
        "abstract": "Representational alignment refers to the extent to which a model's internal\nrepresentations mirror biological vision, offering insights into both neural\nsimilarity and functional correspondence. Recently, some more aligned models\nhave demonstrated higher resiliency to adversarial examples, raising the\nquestion of whether more human-aligned models are inherently more secure. In\nthis work, we conduct a large-scale empirical analysis to systematically\ninvestigate the relationship between representational alignment and adversarial\nrobustness. We evaluate 118 models spanning diverse architectures and training\nparadigms, measuring their neural and behavioral alignment and engineering task\nperformance across 106 benchmarks as well as their adversarial robustness via\nAutoAttack. Our findings reveal that while average alignment and robustness\nexhibit a weak overall correlation, specific alignment benchmarks serve as\nstrong predictors of adversarial robustness, particularly those that measure\nselectivity towards texture or shape. These results suggest that different\nforms of alignment play distinct roles in model robustness, motivating further\ninvestigation into how alignment-driven approaches can be leveraged to build\nmore secure and perceptually-grounded vision models."
    },
    {
        "date": "2025-02",
        "title": "Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV",
        "author": "Allen M. Wang, Alessandro Pau, Cristina Rea, Oswin So, Charles Dawson, Olivier Sauter, Mark D. Boyer, Anna Vu, Cristian Galperti, Chuchu Fan, Antoine Merle, Yoeri Poels, Cristina Venturini, Stefano Marchioni, and the TCV Team",
        "link": "http://arxiv.org/abs/2502.12327v1",
        "abstract": "The rampdown in tokamak operations is a difficult to simulate phase during\nwhich the plasma is often pushed towards multiple instability limits. To\naddress this challenge, and reduce the risk of disrupting operations, we\nleverage recent advances in Scientific Machine Learning (SciML) to develop a\nneural state-space model (NSSM) that predicts plasma dynamics during Tokamak\n\\`a Configuration Variable (TCV) rampdowns. By integrating simple physics\nstructure and data-driven models, the NSSM efficiently learns plasma dynamics\nduring the rampdown from a modest dataset of 311 pulses with only five pulses\nin the reactor relevant high performance regime. The NSSM is parallelized\nacross uncertainties, and reinforcement learning (RL) is applied to design\ntrajectories that avoid multiple instability limits with high probability.\nExperiments at TCV ramping down high performance plasmas show statistically\nsignificant improvements in current and energy at plasma termination, with\nimprovements in speed through continuous re-training. A predict-first\nexperiment, increasing plasma current by 20\\% from baseline, demonstrates the\nNSSM's ability to make small extrapolations with sufficient accuracy to design\ntrajectories that successfully terminate the pulse. The developed approach\npaves the way for designing tokamak controls with robustness to considerable\nuncertainty, and demonstrates the relevance of the SciML approach to learning\nplasma dynamics for rapidly developing robust trajectories and controls during\nthe incremental campaigns of upcoming burning plasma tokamaks."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Debiasing for Unbiased Parameter Recovery",
        "author": "Luke C Sanford, Megan Ayers, Matthew Gordon, and Eliana Stone",
        "link": "http://arxiv.org/abs/2502.12323v1",
        "abstract": "Advances in machine learning and the increasing availability of\nhigh-dimensional data have led to the proliferation of social science research\nthat uses the predictions of machine learning models as proxies for measures of\nhuman activity or environmental outcomes. However, prediction errors from\nmachine learning models can lead to bias in the estimates of regression\ncoefficients. In this paper, we show how this bias can arise, propose a test\nfor detecting bias, and demonstrate the use of an adversarial machine learning\nalgorithm in order to de-bias predictions. These methods are applicable to any\nsetting where machine-learned predictions are the dependent variable in a\nregression. We conduct simulations and empirical exercises using ground truth\nand satellite data on forest cover in Africa. Using the predictions from a\nnaive machine learning model leads to biased parameter estimates, while the\npredictions from the adversarial model recover the true coefficients."
    },
    {
        "date": "2025-02",
        "title": "Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance",
        "author": "Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, and Jian Yang",
        "link": "http://arxiv.org/abs/2502.11971v1",
        "abstract": "Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU."
    },
    {
        "date": "2025-02",
        "title": "A limited technical background is sufficient for attack-defense tree acceptability",
        "author": "Nathan Daniel Schiele, and Olga Gadyatskaya",
        "link": "http://arxiv.org/abs/2502.11920v1",
        "abstract": "Attack-defense trees (ADTs) are a prominent graphical threat modeling method\nthat is highly recommended for analyzing and communicating security-related\ninformation. Despite this, existing empirical studies of attack trees have\nestablished their acceptability only for users with highly technical (computer\nscience) backgrounds while raising questions about their suitability for threat\nmodeling stakeholders with a limited technical background. Our research\naddresses this gap by investigating the impact of the users' technical\nbackground on ADT acceptability in an empirical study.\n  Our Method Evaluation Model-based study consisted of n = 102 participants (53\nwith a strong computer science background and 49 with a limited computer\nscience background) who were asked to complete a series of ADT-related tasks.\nBy analyzing their responses and comparing the results, we reveal that a very\nlimited technical background is sufficient for ADT acceptability. This finding\nunderscores attack trees' viability as a threat modeling method."
    },
    {
        "date": "2025-02",
        "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
        "author": "Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, and Xuan-Lam Pham",
        "link": "http://arxiv.org/abs/2502.11915v1",
        "abstract": "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives",
        "author": "Leo Schwinn, Yan Scholten, Tom Wollschl\u00e4ger, Sophie Xhonneux, Stephen Casper, Stephan G\u00fcnnemann, and Gauthier Gidel",
        "link": "http://arxiv.org/abs/2502.11910v1",
        "abstract": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes."
    },
    {
        "date": "2025-02",
        "title": "On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series",
        "author": "Kausik Lakkaraju, Rachneet Kaur, Parisa Zehtabi, Sunandita Patra, Siva Likitha Valluru, Zhen Zeng, Biplav Srivastava, and Marco Valtorta",
        "link": "http://arxiv.org/abs/2502.12226v1",
        "abstract": "Foundation Models (FMs) have improved time series forecasting in various\nsectors, such as finance, but their vulnerability to input disturbances can\nhinder their adoption by stakeholders, such as investors and analysts. To\naddress this, we propose a causally grounded rating framework to study the\nrobustness of Foundational Models for Time Series (FMTS) with respect to input\nperturbations. We evaluate our approach to the stock price prediction problem,\na well-studied problem with easily accessible public data, evaluating six\nstate-of-the-art (some multi-modal) FMTS across six prominent stocks spanning\nthree industries. The ratings proposed by our framework effectively assess the\nrobustness of FMTS and also offer actionable insights for model selection and\ndeployment. Within the scope of our study, we find that (1) multi-modal FMTS\nexhibit better robustness and accuracy compared to their uni-modal versions\nand, (2) FMTS pre-trained on time series forecasting task exhibit better\nrobustness and forecasting accuracy compared to general-purpose FMTS\npre-trained across diverse settings. Further, to validate our framework's\nusability, we conduct a user study showcasing FMTS prediction errors along with\nour computed ratings. The study confirmed that our ratings reduced the\ndifficulty for users in comparing the robustness of different systems."
    },
    {
        "date": "2025-02",
        "title": "FedEAT: A Robustness Optimization Framework for Federated LLMs",
        "author": "Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, and Hai Jin",
        "link": "http://arxiv.org/abs/2502.11863v1",
        "abstract": "Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss."
    },
    {
        "date": "2025-02",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "author": "Zeliang Zhang, Susan Liang, Daiki Shimada, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2502.11858v2",
        "abstract": "While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency."
    },
    {
        "date": "2025-02",
        "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models",
        "author": "Shehel Yoosuf, Temoor Ali, Ahmed Lekssays, Mashael AlSabah, and Issa Khalil",
        "link": "http://arxiv.org/abs/2502.11853v1",
        "abstract": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g. SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing\n\\textit{content transformations}, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware, and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection."
    },
    {
        "date": "2025-02",
        "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "author": "Mark Vero, Niels M\u00fcndler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanovi\u0107, Jingxuan He, and Martin Vechev",
        "link": "http://arxiv.org/abs/2502.11844v1",
        "abstract": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs."
    },
    {
        "date": "2025-02",
        "title": "Robust Partial-Label Learning by Leveraging Class Activation Values",
        "author": "Tobias Fuchs, and Florian Kalinke",
        "link": "http://arxiv.org/abs/2502.11743v1",
        "abstract": "Real-world training data is often noisy; for example, human annotators assign\nconflicting class labels to the same instances. Partial-label learning (PLL) is\na weakly supervised learning paradigm that allows training classifiers in this\ncontext without manual data cleaning. While state-of-the-art methods have good\npredictive performance, their predictions are sensitive to high noise levels,\nout-of-distribution data, and adversarial perturbations. We propose a novel PLL\nmethod based on subjective logic, which explicitly represents uncertainty by\nleveraging the magnitudes of the underlying neural network's class activation\nvalues. Thereby, we effectively incorporate prior knowledge about the class\nlabels by using a novel label weight re-distribution strategy that we prove to\nbe optimal. We empirically show that our method yields more robust predictions\nin terms of predictive performance under high PLL noise levels, handling\nout-of-distribution examples, and handling adversarial perturbations on the\ntest instances."
    },
    {
        "date": "2025-02",
        "title": "Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics",
        "author": "Francesco Croce, Christian Schlarmann, Naman Deep Singh, and Matthias Hein",
        "link": "http://arxiv.org/abs/2502.11725v1",
        "abstract": "Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries."
    },
    {
        "date": "2025-02",
        "title": "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning",
        "author": "Manaar Alam, Hithem Lamri, and Michail Maniatakos",
        "link": "http://arxiv.org/abs/2502.11687v1",
        "abstract": "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning."
    },
    {
        "date": "2025-02",
        "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
        "author": "Yi Wang, Fenghua Weng, Sibei Yang, Zhan Qin, Minlie Huang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2502.11647v1",
        "abstract": "Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection."
    },
    {
        "date": "2025-02",
        "title": "Membership Inference Attacks for Face Images Against Fine-Tuned Latent Diffusion Models",
        "author": "Lauritz Christian Holme, Anton Mosquera Storgaard, and Siavash Arjomand Bigdeli",
        "link": "http://arxiv.org/abs/2502.11619v1",
        "abstract": "The rise of generative image models leads to privacy concerns when it comes\nto the huge datasets used to train such models. This paper investigates the\npossibility of inferring if a set of face images was used for fine-tuning a\nLatent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is\npresented for this task. Using generated auxiliary data for the training of the\nattack model leads to significantly better performance, and so does the use of\nwatermarks. The guidance scale used for inference was found to have a\nsignificant influence. If a LDM is fine-tuned for long enough, the text prompt\nused for inference has no significant influence. The proposed MIA is found to\nbe viable in a realistic black-box setup against LDMs fine-tuned on\nface-images."
    },
    {
        "date": "2025-02",
        "title": "Trinity: A Scalable and Forward-Secure DSSE for Spatio-Temporal Range Query",
        "author": "Zhijun Li, Kuizhi Liu, Minghui Xu, Xiangyu Wang, Yinbin Miao, Jianfeng Ma, and Xiuzhen Cheng",
        "link": "http://arxiv.org/abs/2502.11550v1",
        "abstract": "Cloud-based outsourced Location-based services have profound impacts on\nvarious aspects of people's lives but bring security concerns. Existing\nspatio-temporal data secure retrieval schemes have significant shortcomings\nregarding dynamic updates, either compromising privacy through leakage during\nupdates (forward insecurity) or incurring excessively high update costs that\nhinder practical application. Under these circumstances, we first propose a\nbasic filter-based spatio-temporal range query scheme \\TrinityI that supports\nlow-cost dynamic updates and automatic expansion. Furthermore, to improve\nsecurity, reduce storage cost, and false positives, we propose a forward secure\nand verifiable scheme \\TrinityII that simultaneously minimizes storage\noverhead. A formal security analysis proves that \\TrinityI and \\TrinityII are\nIndistinguishable under Selective Chosen-Plaintext Attack (IND-SCPA). Finally,\nextensive experiments demonstrate that our design \\TrinityII significantly\nreduces storage requirements by 80\\%, enables data retrieval at the 1\nmillion-record level in just 0.01 seconds, and achieves 10 $\\times$ update\nefficiency than state-of-art."
    },
    {
        "date": "2025-02",
        "title": "Optimized detection of cyber-attacks on IoT networks via hybrid deep learning models",
        "author": "Ahmed Bensaoud, and Jugal Kalita",
        "link": "http://arxiv.org/abs/2502.11470v1",
        "abstract": "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies."
    },
    {
        "date": "2025-02",
        "title": "Semantically Robust Unsupervised Image Translation for Paired Remote Sensing Images",
        "author": "Sheng Fang, Kaiyu Li, Zhe Li, Jianli Zhao, and Xingli Zhang",
        "link": "http://arxiv.org/abs/2502.11468v1",
        "abstract": "Image translation for change detection or classification in bi-temporal\nremote sensing images is unique. Although it can acquire paired images, it is\nstill unsupervised. Moreover, strict semantic preservation in translation is\nalways needed instead of multimodal outputs. In response to these problems,\nthis paper proposes a new method, SRUIT (Semantically Robust Unsupervised\nImage-to-image Translation), which ensures semantically robust translation and\nproduces deterministic output. Inspired by previous works, the method explores\nthe underlying characteristics of bi-temporal Remote Sensing images and designs\nthe corresponding networks. Firstly, we assume that bi-temporal Remote Sensing\nimages share the same latent space, for they are always acquired from the same\nland location. So SRUIT makes the generators share their high-level layers, and\nthis constraint will compel two domain mapping to fall into the same latent\nspace. Secondly, considering land covers of bi-temporal images could evolve\ninto each other, SRUIT exploits the cross-cycle-consistent adversarial networks\nto translate from one to the other and recover them. Experimental results show\nthat constraints of sharing weights and cross-cycle consistency enable\ntranslated images with both good perceptual image quality and semantic\npreservation for significant differences."
    },
    {
        "date": "2025-02",
        "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
        "author": "Fenghua Weng, Jian Lou, Jun Feng, Minlie Huang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2502.11455v1",
        "abstract": "Safety alignment is critical in pre-training large language models (LLMs) to\ngenerate responses aligned with human values and refuse harmful queries. Unlike\nLLM, the current safety alignment of VLMs is often achieved with post-hoc\nsafety fine-tuning. However, these methods are less effective to white-box\nattacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a\nnovel training framework that explicitly considers adversarial.\n$\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO\nto enhance the safety alignment of VLMs under worst-case adversarial\nperturbations. $\\textit{ADPO}$ introduces two key components: (1) an\nadversarial-trained reference model that generates human-preferred responses\nunder worst-case perturbations, and (2) an adversarial-aware DPO loss that\ngenerates winner-loser pairs accounting for adversarial distortions. By\ncombining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust\nand reliable even in the presence of sophisticated jailbreak attacks. Extensive\nexperiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the\nsafety alignment and general utility of VLMs."
    },
    {
        "date": "2025-02",
        "title": "Learning Dexterous Bimanual Catch Skills through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning",
        "author": "Taewoo Kim, Youngwoo Yoon, and Jaehong Kim",
        "link": "http://arxiv.org/abs/2502.11437v1",
        "abstract": "Robotic catching has traditionally focused on single-handed systems, which\nare limited in their ability to handle larger or more complex objects. In\ncontrast, bimanual catching offers significant potential for improved dexterity\nand object handling but introduces new challenges in coordination and control.\nIn this paper, we propose a novel framework for learning dexterous bimanual\ncatching skills using Heterogeneous-Agent Reinforcement Learning (HARL). Our\napproach introduces an adversarial reward scheme, where a throw agent increases\nthe difficulty of throws-adjusting speed-while a catch agent learns to\ncoordinate both hands to catch objects under these evolving conditions. We\nevaluate the framework in simulated environments using 15 different objects,\ndemonstrating robustness and versatility in handling diverse objects. Our\nmethod achieved approximately a 2x increase in catching reward compared to\nsingle-agent baselines across 15 diverse objects."
    },
    {
        "date": "2025-02",
        "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
        "author": "Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, and Jun Zhou",
        "link": "http://arxiv.org/abs/2502.11379v1",
        "abstract": "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted."
    },
    {
        "date": "2025-02",
        "title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System",
        "author": "Ziyou Jiang, Mingyang Li, Guowei Yang, Junjie Wang, Yuekai Huang, Zhiyuan Chang, and Qing Wang",
        "link": "http://arxiv.org/abs/2502.11358v1",
        "abstract": "Information theft attacks pose a significant risk to Large Language Model\n(LLM) tool-learning systems. Adversaries can inject malicious commands through\ncompromised tools, manipulating LLMs to send sensitive information to these\ntools, which leads to potential privacy breaches. However, existing attack\napproaches are black-box oriented and rely on static commands that cannot adapt\nflexibly to the changes in user queries and the invocation chain of tools. It\nmakes malicious commands more likely to be detected by LLM and leads to attack\nfailure. In this paper, we propose AutoCMD, a dynamic attack comment generation\napproach for information theft attacks in LLM tool-learning systems. Inspired\nby the concept of mimicking the familiar, AutoCMD is capable of inferring the\ninformation utilized by upstream tools in the toolchain through learning on\nopen-source systems and reinforcement with target system examples, thereby\ngenerating more targeted commands for information theft. The evaluation results\nshow that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can\nbe generalized to new tool-learning systems to expose their information leakage\nrisks. We also design four defense methods to effectively protect tool-learning\nsystems from the attack."
    },
    {
        "date": "2025-02",
        "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
        "author": "Cullen Anderson, and Jeff M. Phillips",
        "link": "http://arxiv.org/abs/2502.11324v1",
        "abstract": "Robust statistics aims to compute quantities to represent data where a\nfraction of it may be arbitrarily corrupted. The most essential statistic is\nthe mean, and in recent years, there has been a flurry of theoretical\nadvancement for efficiently estimating the mean in high dimensions on corrupted\ndata. While several algorithms have been proposed that achieve near-optimal\nerror, they all rely on large data size requirements as a function of\ndimension. In this paper, we perform an extensive experimentation over various\nmean estimation techniques where data size might not meet this requirement due\nto the high-dimensional setting."
    },
    {
        "date": "2025-02",
        "title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation",
        "author": "Yiyi Chen, Qiongkai Xu, and Johannes Bjerva",
        "link": "http://arxiv.org/abs/2502.11308v2",
        "abstract": "With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP."
    },
    {
        "date": "2025-02",
        "title": "Game-Of-Goals: Using adversarial games to achieve strategic resilience",
        "author": "Aditya Ghose, and Asjad Khan",
        "link": "http://arxiv.org/abs/2502.11295v1",
        "abstract": "Our objective in this paper is to develop a machinery that makes a given\norganizational strategic plan resilient to the actions of competitor agents\n(adverse environmental actions). We assume that we are given a goal tree\nrepresenting strategic goals (can also be seen business requirements for a\nsoftware systems) with the assumption that competitor agents are behaving in a\nmaximally adversarial fashion(opposing actions against our sub goals or goals\nin general). We use game tree search methods (such as minimax) to select an\noptimal execution strategy(at a given point in time), such that it can maximize\nour chances of achieving our (high level) strategic goals. Our machinery helps\nus determine which path to follow(strategy selection) to achieve the best end\noutcome. This is done by comparing alternative execution strategies available\nto us via an evaluation function. Our evaluation function is based on the idea\nthat we want to make our execution plans defensible(future-proof) by selecting\nexecution strategies that make us least vulnerable to adversarial actions by\nthe competitor agents. i.e we want to select an execution strategy such that\nits leaves minimum room(or options) for the adversary to cause\nimpediment/damage to our business goals/plans."
    },
    {
        "date": "2025-02",
        "title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN",
        "author": "Jiayu Zhang, Zhiyu Zhu, Xinyi Wang, Silin Liao, Zhibo Jin, Flora D. Salim, and Huaming Chen",
        "link": "http://arxiv.org/abs/2502.12207v1",
        "abstract": "Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead\nto erroneous predictions. Generative Adversarial Networks (GANs) can leverage\nthe generators and discriminators model to quickly produce high-quality\nadversarial examples. Since both modules train in a competitive and\nsimultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial\nexamples with better transferability compared to traditional methods. However,\nthe generation of perturbations is usually limited to a single iteration,\npreventing these examples from fully exploiting the potential of the methods.\nTo tackle this issue, we introduce a novel approach named Progressive\nAuto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive\niteration mechanism within a progressive generation network to craft\nadversarial examples with enhanced attack capability. We thoroughly evaluate\nour PAR-AdvGAN method with a large-scale experiment, demonstrating its superior\nperformance over various state-of-the-art black-box adversarial attacks, as\nwell as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the\nadversarial example generation, i.e., achieving the speeds of up to 335.5\nframes per second on Inception-v3 model, outperforming the gradient-based\ntransferable attack algorithms. Our code is available at:\nhttps://anonymous.4open.science/r/PAR-01BF/"
    },
    {
        "date": "2025-02",
        "title": "ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs",
        "author": "Ziyi Ni, Hao Wang, and Huacan Wang",
        "link": "http://arxiv.org/abs/2502.13162v1",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains but remain vulnerable to adversarial jailbreak attacks. Existing\nprompt-defense strategies, including parameter-modifying and parameter-free\napproaches, face limitations in adaptability, interpretability, and\ncustomization, constraining their effectiveness against evolving threats. To\naddress these challenges, we propose ShieldLearner, a novel paradigm that\nmimics human learning in defense. Through trial and error, it autonomously\ndistills attack signatures into a Pattern Atlas and synthesizes defense\nheuristics into a Meta-analysis Framework, enabling systematic and\ninterpretable threat detection. Furthermore, we introduce Adaptive Adversarial\nAugmentation to generate adversarial variations of successfully defended\nprompts, enabling continuous self-improvement without model retraining. In\naddition to standard benchmarks, we create a hard test set by curating\nadversarial prompts from the Wildjailbreak dataset, emphasizing more concealed\nmalicious intent. Experimental results show that ShieldLearner achieves a\nsignificantly higher defense success rate than existing baselines on both\nconventional and hard test sets, while also operating with lower computational\noverhead, making it a practical and efficient solution for real-world\nadversarial defense."
    },
    {
        "date": "2025-02",
        "title": "Logarithmic Width Suffices for Robust Memorization",
        "author": "Amitsour Egosi, Gilad Yehudai, and Ohad Shamir",
        "link": "http://arxiv.org/abs/2502.11162v1",
        "abstract": "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$)."
    },
    {
        "date": "2025-02",
        "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
        "author": "Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang",
        "link": "http://arxiv.org/abs/2502.11127v1",
        "abstract": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated\nremarkable capabilities in various complex tasks, ranging from collaborative\nproblem-solving to autonomous decision-making. However, as these systems become\nincreasingly integrated into critical applications, their vulnerability to\nadversarial attacks, misinformation propagation, and unintended behaviors have\nraised significant concerns. To address this challenge, we introduce\nG-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS,\nwhich leverages graph neural networks to detect anomalies on the multi-agent\nutterance graph and employ topological intervention for attack remediation.\nExtensive experiments demonstrate that G-Safeguard: (I) exhibits significant\neffectiveness under various attack strategies, recovering over 40% of the\nperformance for prompt injection; (II) is highly adaptable to diverse LLM\nbackbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS\nwith security guarantees. The code is available at\nhttps://github.com/wslong20/G-safeguard."
    },
    {
        "date": "2025-02",
        "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
        "author": "Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, and Junlan Feng",
        "link": "http://arxiv.org/abs/2502.11090v2",
        "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack",
        "author": "Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2502.12202v1",
        "abstract": "Longer thought, better performance: large language models with deep reasoning\ncapabilities, particularly o1-like models, have demonstrated remarkable\nperformance by generating extensive thought processes during inference. This\ntrade-off reveals a potential vulnerability: adversaries could compromise model\nperformance by forcing immediate responses without thought processes. To this\nend, in this paper, we introduce a novel attack scenario targeting the long\nthought processes of o1-like models and propose BoT (Break CoT), which can\nselectively break intrinsic reasoning mechanisms through backdoor attacks. BoT\nconstructs poisoned datasets with designed triggers and injects backdoor by\neither supervised fine-tuning or direct preference optimization. When\ntriggered, the model directly generates answers without thought processes,\nwhile maintaining normal reasoning capabilities for clean inputs. Extensive\nexperiments on open-source o1-like models, including recent DeepSeek-R1,\ndemonstrate that BoT nearly achieves high attack success rates while\nmaintaining clean accuracy, highlighting the critical safety risk in current\nmodels. Furthermore, the relationship between task difficulty and helpfulness\nreveals a potential application for good, enabling users to customize model\nbehavior based on task complexity. Code is available at\n\\href{https://github.com/zihao-ai/BoT}{https://github.com/zihao-ai/BoT}."
    },
    {
        "date": "2025-02",
        "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
        "author": "Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2502.11054v3",
        "abstract": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain."
    },
    {
        "date": "2025-02",
        "title": "Leveraging Large Language Models for Cybersecurity: Enhancing SMS Spam Detection with Robust and Context-Aware Text Classification",
        "author": "Mohsen Ahmadi, Matin Khajavi, Abbas Varmaghani, Ali Ala, Kasra Danesh, and Danial Javaheri",
        "link": "http://arxiv.org/abs/2502.11014v1",
        "abstract": "This study evaluates the effectiveness of different feature extraction\ntechniques and classification algorithms in detecting spam messages within SMS\ndata. We analyzed six classifiers Naive Bayes, K-Nearest Neighbors, Support\nVector Machines, Linear Discriminant Analysis, Decision Trees, and Deep Neural\nNetworks using two feature extraction methods: bag-of-words and TF-IDF. The\nprimary objective was to determine the most effective classifier-feature\ncombination for SMS spam detection. Our research offers two main contributions:\nfirst, by systematically examining various classifier and feature extraction\npairings, and second, by empirically evaluating their ability to distinguish\nspam messages. Our results demonstrate that the TF-IDF method consistently\noutperforms the bag-of-words approach across all six classifiers. Specifically,\nNaive Bayes with TF-IDF achieved the highest accuracy of 96.2%, with a\nprecision of 0.976 for non-spam and 0.754 for spam messages. Similarly, Support\nVector Machines with TF-IDF exhibited an accuracy of 94.5%, with a precision of\n0.926 for non-spam and 0.891 for spam. Deep Neural Networks using TF-IDF\nyielded an accuracy of 91.0%, with a recall of 0.991 for non-spam and 0.415 for\nspam messages. In contrast, classifiers such as K-Nearest Neighbors, Linear\nDiscriminant Analysis, and Decision Trees showed weaker performance, regardless\nof the feature extraction method employed. Furthermore, we observed substantial\nvariability in classifier effectiveness depending on the chosen feature\nextraction technique. Our findings emphasize the significance of feature\nselection in SMS spam detection and suggest that TF-IDF, when paired with Naive\nBayes, Support Vector Machines, or Deep Neural Networks, provides the most\nreliable performance. These insights provide a foundation for improving SMS\nspam detection through optimized feature extraction and classification methods."
    },
    {
        "date": "2025-02",
        "title": "FeaKM: Robust Collaborative Perception under Noisy Pose Conditions",
        "author": "Jiuwu Hao, Liguo Sun, Ti Xiang, Yuting Wan, Haolin Song, and Pin Lv",
        "link": "http://arxiv.org/abs/2502.11003v1",
        "abstract": "Collaborative perception is essential for networks of agents with limited\nsensing capabilities, enabling them to work together by exchanging information\nto achieve a robust and comprehensive understanding of their environment.\nHowever, localization inaccuracies often lead to significant spatial message\ndisplacement, which undermines the effectiveness of these collaborative\nefforts. To tackle this challenge, we introduce FeaKM, a novel method that\nemploys Feature-level Keypoints Matching to effectively correct pose\ndiscrepancies among collaborating agents. Our approach begins by utilizing a\nconfidence map to identify and extract salient points from intermediate feature\nrepresentations, allowing for the computation of their descriptors. This step\nensures that the system can focus on the most relevant information, enhancing\nthe matching process. We then implement a target-matching strategy that\ngenerates an assignment matrix, correlating the keypoints identified by\ndifferent agents. This is critical for establishing accurate correspondences,\nwhich are essential for effective collaboration. Finally, we employ a\nfine-grained transformation matrix to synchronize the features of all agents\nand ascertain their relative statuses, ensuring coherent communication among\nthem. Our experimental results demonstrate that FeaKM significantly outperforms\nexisting methods on the DAIR-V2X dataset, confirming its robustness even under\nsevere noise conditions. The code and implementation details are available at\nhttps://github.com/uestchjw/FeaKM."
    },
    {
        "date": "2025-02",
        "title": "SSVEP-BiMA: Bifocal Masking Attention Leveraging Native and Symmetric-Antisymmetric Components for Robust SSVEP Decoding",
        "author": "Yuxin Liu, Zhenxi Song, Guoyang Xu, Zirui Wang, Feng Wan, Yong Hu, Min Zhang, and Zhiguo Zhang",
        "link": "http://arxiv.org/abs/2502.10994v1",
        "abstract": "Brain-computer interface (BCI) based on steady-state visual evoked potentials\n(SSVEP) is a popular paradigm for its simplicity and high information transfer\nrate (ITR). Accurate and fast SSVEP decoding is crucial for reliable BCI\nperformance. However, conventional decoding methods demand longer time windows,\nand deep learning models typically require subject-specific fine-tuning,\nleaving challenges in achieving optimal performance in cross-subject settings.\nThis paper proposed a biofocal masking attention-based method (SSVEP-BiMA) that\nsynergistically leverages the native and symmetric-antisymmetric components for\ndecoding SSVEP. By utilizing multiple signal representations, the network is\nable to integrate features from a wider range of sample perspectives, leading\nto more generalized and comprehensive feature learning, which enhances both\nprediction accuracy and robustness. We performed experiments on two public\ndatasets, and the results demonstrate that our proposed method surpasses\nbaseline approaches in both accuracy and ITR. We believe that this work will\ncontribute to the development of more efficient SSVEP-based BCI systems."
    },
    {
        "date": "2025-02",
        "title": "RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization",
        "author": "Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2502.10993v1",
        "abstract": "Large language models (LLMs) have achieved impressive performance but face\nhigh computational costs and latency, limiting their deployment in\nresource-constrained settings. In contrast, small-scale LLMs (SLMs) are more\nefficient yet struggle to capture evolving real-world knowledge.\nRetrieval-augmented generation (RAG) helps by integrating external knowledge,\nbut imperfect retrieval can introduce distracting noise that misleads SLMs. We\npropose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference\nOptimization. RoseRAG employs multi-turn prompting for detailed reasoning,\nrejection sampling for high-quality explanations, and contrastive preference\nselection to refine responses by maximizing the likelihood gap between\npreferred and non-preferred outputs. By integrating these components into a\nmargin-aware optimization process, RoseRAG robustly enhances the accuracy and\nreliability of SLMs for RAG applications. Extensive experiments on three\nopen-domain question answering benchmarks indicate that our innovative RoseRAG\nsurpasses state-of-the-art baselines significantly."
    },
    {
        "date": "2025-02",
        "title": "D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and Heterogeneous Execution for Enhanced Reasoning in Offensive Security",
        "author": "Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2502.10931v1",
        "abstract": "Large Language Models (LLMs) have been used in cybersecurity in many ways,\nincluding their recent use as intelligent agent systems for autonomous security\nanalysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing\nthe automated task-planning abilities of LLM agents across various\ncybersecurity skill sets. Early attempts to apply LLMs for solving CTF\nchallenges relied on single-agent systems, where feedback was restricted to a\nsingle reasoning-action loop. This approach proved inadequate for handling\ncomplex CTF tasks. Drawing inspiration from real-world CTF competitions, where\nteams of experts collaborate, we introduce the D-CIPHER multi-agent LLM\nframework for collaborative CTF challenge solving. D-CIPHER integrates agents\nwith distinct roles, enabling dynamic feedback loops to enhance reasoning on\nCTF challenges. It introduces the Planner-Executor agent system, consisting of\na Planner agent for overall problem-solving along with multiple heterogeneous\nExecutor agents for individual tasks, facilitating efficient allocation of\nresponsibilities among the LLMs. Additionally, D-CIPHER incorporates an\nAuto-prompter agent, which improves problem-solving by exploring the challenge\nenvironment and generating a highly relevant initial prompt. We evaluate\nD-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive\nstudies to highlight the impact of our enhancements. Our results demonstrate\nthat the multi-agent D-CIPHER system achieves a significant improvement in\nchallenges solved, setting a state-of-the-art performance on three benchmarks:\n22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is\navailable at https://github.com/NYU-LLM-CTF/nyuctf_agents as the\nnyuctf_multiagent package."
    },
    {
        "date": "2025-02",
        "title": "A Closer Look at System Prompt Robustness",
        "author": "Norman Mu, Jonathan Lu, Michael Lavery, and David Wagner",
        "link": "http://arxiv.org/abs/2502.12197v1",
        "abstract": "System prompts have emerged as a critical control surface for specifying the\nbehavior of LLMs in chat and agent settings. Developers depend on system\nprompts to specify important context, output format, personalities, guardrails,\ncontent policies, and safety countermeasures, all of which require models to\nrobustly adhere to the system prompt, especially when facing conflicting or\nadversarial user inputs. In practice, models often forget to consider relevant\nguardrails or fail to resolve conflicting demands between the system and the\nuser. In this work, we study various methods for improving system prompt\nrobustness by creating realistic new evaluation and fine-tuning datasets based\non prompts collected from from OpenAI's GPT Store and HuggingFace's\nHuggingChat. Our experiments assessing models with a panel of new and existing\nbenchmarks show that performance can be considerably improved with realistic\nfine-tuning data, as well as inference-time interventions such as\nclassifier-free guidance. Finally, we analyze the results of recently released\nreasoning models from OpenAI and DeepSeek, which show exciting but uneven\nimprovements on the benchmarks we study. Overall, current techniques fall short\nof ensuring system prompt robustness and further study is warranted."
    },
    {
        "date": "2025-02",
        "title": "Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm",
        "author": "Nanyu Luo, and Feng Ji",
        "link": "http://arxiv.org/abs/2502.10650v1",
        "abstract": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. This study introduces Adversarial\nVariational Bayes (AVB) algorithms as an improvement to VAEs for IFA with\nimproved flexibility and accuracy. By bridging the strengths of VAEs and\nGenerative Adversarial Networks (GANs), AVB incorporates an auxiliary\ndiscriminator network to reframe the estimation process as a two-player\nadversarial game and removes the restrictive assumption of standard normal\ndistributions in the inference model. Theoretically, AVB can achieve similar or\nhigher likelihood compared to VAEs. A further enhanced algorithm,\nImportance-weighted Adversarial Variational Bayes (IWAVB) is proposed and\ncompared with Importance-weighted Autoencoders (IWAE). In an exploratory\nanalysis of real empirical data, IWAVB demonstrated superior expressiveness by\nachieving a higher likelihood compared to IWAE. In confirmatory studies with\nsimulated data, IWAVB achieved similar mean-square error results to IWAE while\nconsistently achieving higher likelihoods. Moreover, in simulations where\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE by\nproviding more accurate parameter estimates. With its innovative use of GANs,\nIWAVB is shown to have the potential to extend IFA to handle large-scale data,\nfacilitating the potential integration of psychometrics and multimodal data\nanalysis."
    },
    {
        "date": "2025-02",
        "title": "LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization",
        "author": "Erica Zhang, Ryunosuke Goto, Naomi Sagan, Jurik Mutter, Nick Phillips, Ash Alizadeh, Kangwook Lee, Jose Blanchet, Mert Pilanci, and Robert Tibshirani",
        "link": "http://arxiv.org/abs/2502.10648v1",
        "abstract": "We introduce LLM-Lasso, a novel framework that leverages large language\nmodels (LLMs) to guide feature selection in Lasso $\\ell_1$ regression. Unlike\ntraditional methods that rely solely on numerical data, LLM-Lasso incorporates\ndomain-specific knowledge extracted from natural language, enhanced through a\nretrieval-augmented generation (RAG) pipeline, to seamlessly integrate\ndata-driven modeling with contextual insights. Specifically, the LLM generates\npenalty factors for each feature, which are converted into weights for the\nLasso penalty using a simple, tunable model. Features identified as more\nrelevant by the LLM receive lower penalties, increasing their likelihood of\nbeing retained in the final model, while less relevant features are assigned\nhigher penalties, reducing their influence. Importantly, LLM-Lasso has an\ninternal validation step that determines how much to trust the contextual\nknowledge in our prediction pipeline. Hence it addresses key challenges in\nrobustness, making it suitable for mitigating potential inaccuracies or\nhallucinations from the LLM. In various biomedical case studies, LLM-Lasso\noutperforms standard Lasso and existing feature selection baselines, all while\nensuring the LLM operates without prior access to the datasets. To our\nknowledge, this is the first approach to effectively integrate conventional\nfeature selection techniques directly with LLM-based domain-specific reasoning."
    },
    {
        "date": "2025-02",
        "title": "Dark Deceptions in DHCP: Dismantling Network Defenses",
        "author": "Robert Dilworth",
        "link": "http://arxiv.org/abs/2502.10646v1",
        "abstract": "This paper explores vulnerabilities in the Dynamic Host Configuration\nProtocol (DHCP) and their implications on the Confidentiality, Integrity, and\nAvailability (CIA) triad. Through an analysis of various attacks, including\nDHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits,\nthe paper provides a taxonomic classification of threats, assesses risks, and\nproposes appropriate controls. The discussion also highlights the dangers of\nVPN decloaking through DHCP exploits and underscores the importance of\nsafeguarding network infrastructures. By bringing awareness to the TunnelVision\nexploit, this paper aims to mitigate risks associated with these prevalent\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network",
        "author": "Arun Kunwar, Dibakar Raj Pant, Jukka Heikkonen, and Rajeev Kanth",
        "link": "http://arxiv.org/abs/2502.10334v1",
        "abstract": "The Convolutional Neural Network (CNN) has shown impressive performance in\nimage classification because of its strong learning capabilities. However, it\ndemands a substantial and balanced dataset for effective training. Otherwise,\nnetworks frequently exhibit over fitting and struggle to generalize to new\nexamples. Publicly available dataset of fundus images of ocular disease is\ninsufficient to train any classification model to achieve satisfactory\naccuracy. So, we propose Generative Adversarial Network(GAN) based data\ngeneration technique to synthesize dataset for training CNN based\nclassification model and later use original disease containing ocular images to\ntest the model. During testing the model classification accuracy with the\noriginal ocular image, the model achieves an accuracy rate of 78.6% for myopia,\n88.6% for glaucoma, and 84.6% for cataract, with an overall classification\naccuracy of 84.6%."
    },
    {
        "date": "2025-02",
        "title": "VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect",
        "author": "Qingyuan Fei, Wenjie Hou, Xuan Hai, and Xin Liu",
        "link": "http://arxiv.org/abs/2502.10329v1",
        "abstract": "The rapid advancements in AI voice cloning, fueled by machine learning, have\nsignificantly impacted text-to-speech (TTS) and voice conversion (VC) fields.\nWhile these developments have led to notable progress, they have also raised\nconcerns about the misuse of AI VC technology, causing economic losses and\nnegative public perceptions. To address this challenge, this study focuses on\ncreating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds\npseudo-timbre (jamming information) based on SFS into audio segments that are\nimperceptible to the human ear, thereby forming systematic fragments to prevent\nvoice cloning. This approach protects the voice without compromising its\nquality. In comparison to existing methods, such as adversarial noise\nincorporation, VocalCrypt significantly enhances robustness and real-time\nperformance, achieving a 500\\% increase in generation speed while maintaining\ninterference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our\nmethod offers preemptive defense, reducing implementation costs and enhancing\nfeasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets\nshow that our AI-cloned speech defense system performs excellently in automatic\nspeaker verification (ASV) tests while preserving the integrity of the\nprotected audio."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Mixup Unlearning",
        "author": "Zhuoyi Peng, Yixuan Tang, and Yi Yang",
        "link": "http://arxiv.org/abs/2502.10288v1",
        "abstract": "Machine unlearning is a critical area of research aimed at safeguarding data\nprivacy by enabling the removal of sensitive information from machine learning\nmodels. One unique challenge in this field is catastrophic unlearning, where\nerasing specific data from a well-trained model unintentionally removes\nessential knowledge, causing the model to deviate significantly from a\nretrained one. To address this, we introduce a novel approach that regularizes\nthe unlearning process by utilizing synthesized mixup samples, which simulate\nthe data susceptible to catastrophic effects. At the core of our approach is a\ngenerator-unlearner framework, MixUnlearn, where a generator adversarially\nproduces challenging mixup examples, and the unlearner effectively forgets\ntarget information based on these synthesized data. Specifically, we first\nintroduce a novel contrastive objective to train the generator in an\nadversarial direction: generating examples that prompt the unlearner to reveal\ninformation that should be forgotten, while losing essential knowledge. Then\nthe unlearner, guided by two other contrastive loss terms, processes the\nsynthesized and real data jointly to ensure accurate unlearning without losing\ncritical knowledge, overcoming catastrophic effects. Extensive evaluations\nacross benchmark datasets demonstrate that our method significantly outperforms\nstate-of-the-art approaches, offering a robust solution to machine unlearning.\nThis work not only deepens understanding of unlearning mechanisms but also lays\nthe foundation for effective machine unlearning with mixup augmentation."
    },
    {
        "date": "2025-02",
        "title": "Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study",
        "author": "Sharjeel Imtiaz, Uljana Reinsalu, and Tara Ghasempouri",
        "link": "http://arxiv.org/abs/2502.10194v1",
        "abstract": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches."
    },
    {
        "date": "2025-02",
        "title": "A Robust Attack: Displacement Backdoor Attack",
        "author": "Yong Li, and Han Gao",
        "link": "http://arxiv.org/abs/2502.10490v1",
        "abstract": "As artificial intelligence becomes more prevalent in our lives, people are\nenjoying the convenience it brings, but they are also facing hidden threats,\nsuch as data poisoning and ad- versarial attacks. These threats can have\ndisastrous consequences for the application of artificial intelligence,\nespecially for some applications that take effect immediately, such as\nautonomous driving and medical fields. Among these threats, backdoor attacks\nhave left a deep impression on people with their concealment and simple\ndeployment, making them a threat that cannot be ignored, however, in the\nprocess of deploying the backdoor model, the backdoor attack often has some\nreasons that make it unsatisfactory in real-world applications, such as jitter\nand brightness changes. Based on this, we propose a highly robust backdoor\nattack that shifts the target sample and combines it with itself to form a\nbackdoor sample, the Displacement Backdoor Attack(DBA). Experimental results\nshow that the DBA attack can resist data augmentation that simulates real-world\ndifferences, such as rotation and cropping."
    },
    {
        "date": "2025-02",
        "title": "Fast Proxies for LLM Robustness Evaluation",
        "author": "Tim Beyer, Jan Schuchardt, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2502.10487v1",
        "abstract": "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe\ndeployment, yet current red-teaming methods are often prohibitively expensive.\nWe compare the ability of fast proxy metrics to predict the real-world\nrobustness of an LLM against a simulated attacker ensemble. This allows us to\nestimate a model's robustness to computationally expensive attacks without\nrequiring runs of the attacks themselves. Specifically, we consider\ngradient-descent-based embedding-space attacks, prefilling attacks, and direct\nprompting. Even though direct prompting in particular does not achieve high\nASR, we find that it and embedding-space attacks can predict attack success\nrates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank)\ncorrelations with the full attack ensemble while reducing computational cost by\nthree orders of magnitude."
    },
    {
        "date": "2025-02",
        "title": "Robust Anomaly Detection via Tensor Chidori Pseudoskeleton Decomposition",
        "author": "Bowen Su",
        "link": "http://arxiv.org/abs/2502.09926v1",
        "abstract": "Anomaly detection plays a critical role in modern data-driven applications,\nfrom identifying fraudulent transactions and safeguarding network\ninfrastructure to monitoring sensor systems for irregular patterns. Traditional\napproaches, such as distance, density, or cluster-based methods, face\nsignificant challenges when applied to high dimensional tensor data, where\ncomplex interdependencies across dimensions amplify noise and computational\ncomplexity. To address these limitations, this paper leverages Tensor Chidori\npseudoskeleton decomposition within a tensor-robust principal component\nanalysis framework to extract low Tucker rank structure while isolating sparse\nanomalies, ensuring robustness to anomaly detection. We establish theoretical\nresults regarding convergence, and estimation error, demonstrating the\nstability and accuracy of the proposed approach. Numerical experiments on\nreal-world spatiotemporal data from New York City taxi trip records validate\nthe superiority of the proposed method in detecting anomalous urban events\ncompared to existing benchmark methods. The results underscore the potential of\nTensor Chidori pseudoskeleton decomposition to enhance anomaly detection for\nlarge-scale, high-dimensional data."
    },
    {
        "date": "2025-02",
        "title": "ChatIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation",
        "author": "Ye Dong, Yan Lin Aung, Sudipta Chattopadhyay, and Jianying Zhou",
        "link": "http://arxiv.org/abs/2502.09896v1",
        "abstract": "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone."
    },
    {
        "date": "2025-02",
        "title": "U Can Touch This! Microarchitectural Timing Attacks via Machine Clears",
        "author": "Billy Bob Brumley",
        "link": "http://arxiv.org/abs/2502.09864v1",
        "abstract": "Microarchitectural timing attacks exploit subtle timing variations caused by\nhardware behaviors to leak sensitive information. In this paper, we introduce\nMCHammer, a novel side-channel technique that leverages machine clears induced\nby self-modifying code detection mechanisms. Unlike most traditional\ntechniques, MCHammer does not require memory access or waiting periods, making\nit highly efficient. We compare MCHammer to the classical Flush+Reload\ntechnique, improving in terms of trace granularity, providing a powerful\nside-channel attack vector. Using MCHammer, we successfully recover keys from a\ndeployed implementation of a cryptographic tool. Our findings highlight the\npractical implications of MCHammer and its potential impact on real-world\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Elastic Representation: Mitigating Spurious Correlations for Group Robustness",
        "author": "Tao Wen, Zihan Wang, Quan Zhang, and Qi Lei",
        "link": "http://arxiv.org/abs/2502.09850v1",
        "abstract": "Deep learning models can suffer from severe performance degradation when\nrelying on spurious correlations between input features and labels, making the\nmodels perform well on training data but have poor prediction accuracy for\nminority groups. This problem arises especially when training data are limited\nor imbalanced. While most prior work focuses on learning invariant features\n(with consistent correlations to y), it overlooks the potential harm of\nspurious correlations between features. We hereby propose Elastic\nRepresentation (ElRep) to learn features by imposing Nuclear- and\nFrobenius-norm penalties on the representation from the last layer of a neural\nnetwork. Similar to the elastic net, ElRep enjoys the benefits of learning\nimportant features without losing feature diversity. The proposed method is\nsimple yet effective. It can be integrated into many deep learning approaches\nto mitigate spurious correlations and improve group robustness. Moreover, we\ntheoretically show that ElRep has minimum negative impacts on in-distribution\npredictions. This is a remarkable advantage over approaches that prioritize\nminority groups at the cost of overall performance."
    },
    {
        "date": "2025-02",
        "title": "On the robustness of multimodal language model towards distractions",
        "author": "Ming Liu, Hao Chen, Jindong Wang, and Wensheng Zhang",
        "link": "http://arxiv.org/abs/2502.09818v1",
        "abstract": "Although vision-language models (VLMs) have achieved significant success in\nvarious applications such as visual question answering, their resilience to\nprompt variations remains an under-explored area. Understanding how\ndistractions affect VLMs is crucial for improving their real-world\napplicability, as inputs could have noisy and irrelevant information in many\npractical scenarios. This paper aims to assess the robustness of VLMs against\nboth visual and textual distractions in the context of science question\nanswering. Built on the ScienceQA dataset, we developed a new benchmark that\nintroduces distractions in both the visual and textual contexts to evaluate the\nreasoning capacity of VLMs amid these distractions. Our findings reveal that\nmost-of-the-art VLMs, including GPT-4, are vulnerable to various types of\ndistractions, experiencing noticeable degradation in reasoning capabilities\nwhen confronted with distractions. Notably, models such as InternVL2\ndemonstrate a higher degree of robustness to these distractions. We also found\nthat models exhibit greater sensitivity to textual distractions than visual\nones. Additionally, we explored various mitigation strategies, such as prompt\nengineering, to counteract the impact of distractions. While these strategies\nimproved solution accuracy, our analysis shows that there remain significant\nopportunities for improvement."
    },
    {
        "date": "2025-02",
        "title": "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from Sparse Matrix Decomposition",
        "author": "Yu Zheng, Qizhi Zhang, Lichun Li, Kai Zhou, and Shan Yin",
        "link": "http://arxiv.org/abs/2502.09808v1",
        "abstract": "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions."
    },
    {
        "date": "2025-02",
        "title": "Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models",
        "author": "Jin Hyun Park, Seyyed Ali Ayati, and Yichen Cai",
        "link": "http://arxiv.org/abs/2502.09782v3",
        "abstract": "The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios."
    },
    {
        "date": "2025-02",
        "title": "SoK: Come Together -- Unifying Security, Information Theory, and Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework",
        "author": "Ali Teymourian, Andrew M. Webb, Taha Gharaibeh, Arushi Ghildiyal, and Ibrahim Baggili",
        "link": "http://arxiv.org/abs/2502.09763v1",
        "abstract": "We present a primary attack ontology and analysis framework for deception\nattacks in Mixed Reality (MR). This is achieved through multidisciplinary\nSystematization of Knowledge (SoK), integrating concepts from MR security,\ninformation theory, and cognition. While MR grows in popularity, it presents\nmany cybersecurity challenges, particularly concerning deception attacks and\ntheir effects on humans. In this paper, we use the Borden-Kopp model of\ndeception to develop a comprehensive ontology of MR deception attacks. Further,\nwe derive two models to assess impact of MR deception attacks on information\ncommunication and decision-making. The first, an information-theoretic model,\nmathematically formalizes the effects of attacks on information communication.\nThe second, a decision-making model, details the effects of attacks on\ninterlaced cognitive processes. Using our ontology and models, we establish the\nMR Deception Analysis Framework (DAF) to assess the effects of MR deception\nattacks on information channels, perception, and attention. Our SoK uncovers\nfive key findings for research and practice and identifies five research gaps\nto guide future work."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization",
        "author": "Amit Levi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, and Chaim Baskin",
        "link": "http://arxiv.org/abs/2502.09755v1",
        "abstract": "Jailbreak attacks aim to exploit large language models (LLMs) and pose a\nsignificant threat to their proper conduct; they seek to bypass models'\nsafeguards and often provoke transgressive behaviors. However, existing\nautomatic jailbreak attacks require extensive computational resources and are\nprone to converge on suboptimal solutions. In this work, we propose\n\\textbf{C}ompliance \\textbf{R}efusal \\textbf{I}nitialization (CRI), a novel,\nattack-agnostic framework that efficiently initializes the optimization in the\nproximity of the compliance subspace of harmful prompts. By narrowing the\ninitial gap to the adversarial objective, CRI substantially improves\nadversarial success rates (ASR) and drastically reduces computational overhead\n-- often requiring just a single optimization step. We evaluate CRI on the\nwidely-used AdvBench dataset over the standard jailbreak attacks of GCG and\nAutoDAN. Results show that CRI boosts ASR and decreases the median steps to\nsuccess by up to \\textbf{\\(\\times 60\\)}. The project page, along with the\nreference implementation, is publicly available at\n\\texttt{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}."
    },
    {
        "date": "2025-02",
        "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
        "author": "Abdullah Aydeger, Sanzida Hoque, Engin Zeydan, and Kapal Dev",
        "link": "http://arxiv.org/abs/2502.09726v1",
        "abstract": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
    },
    {
        "date": "2025-02",
        "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
        "author": "Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li",
        "link": "http://arxiv.org/abs/2502.09621v1",
        "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/"
    },
    {
        "date": "2025-02",
        "title": "SyntheticPop: Attacking Speaker Verification Systems With Synthetic VoicePops",
        "author": "Eshaq Jamdar, and Amith Kamath Belman",
        "link": "http://arxiv.org/abs/2502.09553v1",
        "abstract": "Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method."
    },
    {
        "date": "2025-02",
        "title": "Registration, Detection, and Deregistration: Analyzing DNS Abuse for Phishing Attacks",
        "author": "Kyungchan Lim, Kiho Lee, Raffaele Sommese, Mattis Jonker, Ricky Mok, kc claffy, and Doowon Kim",
        "link": "http://arxiv.org/abs/2502.09549v1",
        "abstract": "Phishing continues to pose a significant cybersecurity threat. While\nblocklists currently serve as a primary defense, due to their reactive, passive\nnature, these delayed responses leave phishing websites operational long enough\nto harm potential victims. It is essential to address this fundamental\nchallenge at the root, particularly in phishing domains. Domain registration\npresents a crucial intervention point, as domains serve as the primary gateway\nbetween users and websites. We conduct a comprehensive longitudinal analysis of\n690,502 unique phishing domains, spanning a 39 month period, to examine their\ncharacteristics and behavioral patterns throughout their lifecycle-from initial\nregistration to detection and eventual deregistration. We find that 66.1% of\nthe domains in our dataset are maliciously registered, leveraging\ncost-effective TLDs and targeting brands by mimicking their domain names under\nalternative TLDs (e.g., .top and .tk) instead of the TLDs under which the brand\ndomains are registered (e.g., .com and .ru). We also observe minimal\nimprovements in detection speed for maliciously registered domains compared to\ncompromised domains. Detection times vary widely across blocklists, and\nphishing domains remain accessible for an average of 11.5 days after detection,\nprolonging their potential impact. Our systematic investigation uncovers key\npatterns from registration through detection to deregistration, which could be\nleveraged to enhance anti-phishing active defenses at the DNS level."
    },
    {
        "date": "2025-02",
        "title": "Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based Security",
        "author": "Carlton Shepherd, and Elliot Hurley",
        "link": "http://arxiv.org/abs/2502.09535v3",
        "abstract": "Mobile sensor data has been proposed for security-critical applications such\nas device pairing, proximity detection, and continuous authentication. However,\nthe foundational assumption that these signals provide sufficient entropy\nremains under-explored. In this work, we systematically analyse the entropy of\nmobile sensor data across four diverse datasets spanning multiple application\ncontexts. Our findings reveal pervasive biases, with single-sensor mean\nmin-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite\nShannon entropy being several multiples higher. We further demonstrate that\ncorrelations between sensor modalities reduce the worst-case entropy of using\nmultiple sensors by up to approx. 75% compared to average-case Shannon entropy.\nThis brings joint min-entropy well below 10 bits in many cases and, in the best\ncase, yielding only approx. 24 bits of min-entropy when combining 20 sensor\nmodalities. These results call into question the widely held assumption that\nadding more sensors inherently yields higher security. We ultimately caution\nagainst relying on raw sensor data as a primary source of randomness."
    },
    {
        "date": "2025-02",
        "title": "Robust Learning of Multi-index Models via Iterative Subspace Approximation",
        "author": "Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, and Nikos Zarifis",
        "link": "http://arxiv.org/abs/2502.09525v1",
        "abstract": "We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$."
    },
    {
        "date": "2025-02",
        "title": "Variable Stiffness for Robust Locomotion through Reinforcement Learning",
        "author": "Dario Spoljaric, Yashuai Yan, and Dongheui Lee",
        "link": "http://arxiv.org/abs/2502.09436v1",
        "abstract": "Reinforcement-learned locomotion enables legged robots to perform highly\ndynamic motions but often accompanies time-consuming manual tuning of joint\nstiffness. This paper introduces a novel control paradigm that integrates\nvariable stiffness into the action space alongside joint positions, enabling\ngrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness\n(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness\npolicies, with grouping in per-leg stiffness (PLS), outperform position-based\ncontrol in velocity tracking and push recovery. In contrast, HJLS excels in\nenergy efficiency. Furthermore, our method showcases robust walking behaviour\non diverse outdoor terrains by sim-to-real transfer, although the policy is\nsorely trained on a flat floor. Our approach simplifies design by eliminating\nper-joint stiffness tuning while keeping competitive results with various\nmetrics."
    },
    {
        "date": "2025-02",
        "title": "Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes",
        "author": "Navdeep Kumar, Adarsh Gupta, Maxence Mohamed Elfatihi, Giorgia Ramponi, Kfir Yehuda Levy, and Shie Mannor",
        "link": "http://arxiv.org/abs/2502.09432v1",
        "abstract": "We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs."
    },
    {
        "date": "2025-02",
        "title": "A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack",
        "author": "Richard J. Preen, and Jim Smith",
        "link": "http://arxiv.org/abs/2502.09396v1",
        "abstract": "Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy."
    },
    {
        "date": "2025-02",
        "title": "Wasserstein distributional adversarial training for deep neural networks",
        "author": "Xingjian Bai, Guangyi He, Yifan Jiang, and Jan Obloj",
        "link": "http://arxiv.org/abs/2502.09352v1",
        "abstract": "Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images)."
    },
    {
        "date": "2025-02",
        "title": "LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection",
        "author": "Wenlun Zhang, Enyan Dai, and Kentaro Yoshioka",
        "link": "http://arxiv.org/abs/2502.09271v2",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method."
    },
    {
        "date": "2025-02",
        "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation",
        "author": "Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang",
        "link": "http://arxiv.org/abs/2502.09268v2",
        "abstract": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks."
    },
    {
        "date": "2025-02",
        "title": "DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in Segmenting Hemorrhagic Lesions from Fundus Images",
        "author": "Zesheng Li, Minwen Liao, Haoran Chen, Yan Su, Chengchang Pan, and Honggang Qi",
        "link": "http://arxiv.org/abs/2502.09256v1",
        "abstract": "The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]"
    },
    {
        "date": "2025-02",
        "title": "In Specs we Trust? Conformance-Analysis of Implementation to Specifications in Node-RED and Associated Security Risks",
        "author": "Simon Schneider, Komal Kashish, Katja Tuma, and Riccardo Scandariato",
        "link": "http://arxiv.org/abs/2502.09117v1",
        "abstract": "Low-code development frameworks for IoT platforms offer a simple\ndrag-and-drop mechanism to create applications for the billions of existing IoT\ndevices without the need for extensive programming knowledge. The security of\nsuch software is crucial given the close integration of IoT devices in many\nhighly sensitive areas such as healthcare or home automation. Node-RED is such\na framework, where applications are built from nodes that are contributed by\nopen-source developers. Its reliance on unvetted open-source contributions and\nlack of security checks raises the concern that the applications could be\nvulnerable to attacks, thereby imposing a security risk to end users. The\nlow-code approach suggests, that many users could lack the technical knowledge\nto mitigate, understand, or even realize such security concerns. This paper\nfocuses on \"hidden\" information flows in Node-RED nodes, meaning flows that are\nnot captured by the specifications. They could (unknowingly or with malicious\nintent) cause leaks of sensitive information to unauthorized entities. We\nreport the results of a conformance analysis of all nodes in the Node-RED\nframework, for which we compared the numbers of specified inputs and outputs of\neach node against the number of sources and sinks detected with CodeQL. The\nresults show, that 55% of all nodes exhibit more possible flows than are\nspecified. A risk assessment of a subset of the nodes showed, that 28% of them\nare associated with a high severity and 36% with a medium severity rating."
    },
    {
        "date": "2025-02",
        "title": "Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks",
        "author": "Eylon Mizrahi, Raz Lapid, and Moshe Sipper",
        "link": "http://arxiv.org/abs/2502.09110v1",
        "abstract": "Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems."
    },
    {
        "date": "2025-02",
        "title": "PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration",
        "author": "Jinhui Guo, Lubin Fan, Bojian Wu, Jiaqi Gu, Shen Cao, and Jieping Ye",
        "link": "http://arxiv.org/abs/2502.09075v1",
        "abstract": "In this paper, we present PTZ-Calib, a robust two-stage PTZ camera\ncalibration method, that efficiently and accurately estimates camera parameters\nfor arbitrary viewpoints. Our method includes an offline and an online stage.\nIn the offline stage, we first uniformly select a set of reference images that\nsufficiently overlap to encompass a complete 360{\\deg} view. We then utilize\nthe novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to\nautomatically calibrate the cameras within a local coordinate system.\nAdditionally, for practical application, we can further optimize camera\nparameters and align them with the geographic coordinate system using extra\nglobal reference 3D information. In the online stage, we formulate the\ncalibration of any new viewpoints as a relocalization problem. Our approach\nbalances the accuracy and computational efficiency to meet real-world demands.\nExtensive evaluations demonstrate our robustness and superior performance over\nstate-of-the-art methods on various real and synthetic datasets. Datasets and\nsource code can be accessed online at https://github.com/gjgjh/PTZ-Calib"
    },
    {
        "date": "2025-02",
        "title": "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection: Balancing Security and Data Protection",
        "author": "Shaobo Liu, Zihao Zhao, Weijie He, Jiren Wang, Jing Peng, and Haoyuan Ma",
        "link": "http://arxiv.org/abs/2502.09001v1",
        "abstract": "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards."
    },
    {
        "date": "2025-02",
        "title": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning",
        "author": "Nazatul H. Sultan, Yan Bo, Yansong Gao, Seyit Camtepe, Arash Mahboubi, Hang Thanh Bui, Aufeef Chauhan, Hamed Aboutorab, Michael Bewong, Praveen Gauravaram, Rafiqul Islam, and Sharif Abuadbba",
        "link": "http://arxiv.org/abs/2502.08989v1",
        "abstract": "Federated Learning (FL) allows users to collaboratively train a global\nmachine learning model by sharing local model only, without exposing their\nprivate data to a central server. This distributed learning is particularly\nappealing in scenarios where data privacy is crucial, and it has garnered\nsubstantial attention from both industry and academia. However, studies have\nrevealed privacy vulnerabilities in FL, where adversaries can potentially infer\nsensitive information from the shared model parameters. In this paper, we\npresent an efficient masking-based secure aggregation scheme utilizing\nlightweight cryptographic primitives to mitigate privacy risks. Our scheme\noffers several advantages over existing methods. First, it requires only a\nsingle setup phase for the entire FL training session, significantly reducing\ncommunication overhead. Second, it minimizes user-side overhead by eliminating\nthe need for user-to-user interactions, utilizing an intermediate server layer\nand a lightweight key negotiation method. Third, the scheme is highly resilient\nto user dropouts, and the users can join at any FL round. Fourth, it can detect\nand defend against malicious server activities, including recently discovered\nmodel inconsistency attacks. Finally, our scheme ensures security in both\nsemi-honest and malicious settings. We provide security analysis to formally\nprove the robustness of our approach. Furthermore, we implemented an end-to-end\nprototype of our scheme. We conducted comprehensive experiments and\ncomparisons, which show that it outperforms existing solutions in terms of\ncommunication and computation overhead, functionality, and security."
    },
    {
        "date": "2025-02",
        "title": "Linear-Time User-Level DP-SCO via Robust Statistics",
        "author": "Badih Ghazi, Ravi Kumar, Daogao Liu, and Pasin Manurangsi",
        "link": "http://arxiv.org/abs/2502.08889v1",
        "abstract": "User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field."
    },
    {
        "date": "2025-02",
        "title": "Generative AI for Internet of Things Security: Challenges and Opportunities",
        "author": "Yan Lin Aung, Ivan Christian, Ye Dong, Xiaodong Ye, Sudipta Chattopadhyay, and Jianying Zhou",
        "link": "http://arxiv.org/abs/2502.08886v1",
        "abstract": "As Generative AI (GenAI) continues to gain prominence and utility across\nvarious sectors, their integration into the realm of Internet of Things (IoT)\nsecurity evolves rapidly. This work delves into an examination of the\nstate-of-the-art literature and practical applications on how GenAI could\nimprove and be applied in the security landscape of IoT. Our investigation aims\nto map the current state of GenAI implementation within IoT security, exploring\ntheir potential to fortify security measures further. Through the compilation,\nsynthesis, and analysis of the latest advancements in GenAI technologies\napplied to IoT, this paper not only introduces fresh insights into the field,\nbut also lays the groundwork for future research directions. It explains the\nprevailing challenges within IoT security, discusses the effectiveness of GenAI\nin addressing these issues, and identifies significant research gaps through\nMITRE Mitigations. Accompanied with three case studies, we provide a\ncomprehensive overview of the progress and future prospects of GenAI\napplications in IoT security. This study serves as a foundational resource to\nimprove IoT security through the innovative application of GenAI, thus\ncontributing to the broader discourse on IoT security and technology\nintegration."
    },
    {
        "date": "2025-02",
        "title": "Robust Graph-Based Semi-Supervised Learning via $p$-Conductances",
        "author": "Sawyer Jack Robertson, Chester Holtz, Zhengchao Wan, Gal Mishne, and Alexander Cloninger",
        "link": "http://arxiv.org/abs/2502.08873v1",
        "abstract": "We study the problem of semi-supervised learning on graphs in the regime\nwhere data labels are scarce or possibly corrupted. We propose an approach\ncalled $p$-conductance learning that generalizes the $p$-Laplace and Poisson\nlearning methods by introducing an objective reminiscent of $p$-Laplacian\nregularization and an affine relaxation of the label constraints. This leads to\na family of probability measure mincut programs that balance sparse edge\nremoval with accurate distribution separation. Our theoretical analysis\nconnects these programs to well-known variational and probabilistic problems on\ngraphs (including randomized cuts, effective resistance, and Wasserstein\ndistance) and provides motivation for robustness when labels are diffused via\nthe heat kernel. Computationally, we develop a semismooth Newton-conjugate\ngradient algorithm and extend it to incorporate class-size estimates when\nconverting the continuous solutions into label assignments. Empirical results\non computer vision and citation datasets demonstrate that our approach achieves\nstate-of-the-art accuracy in low label-rate, corrupted-label, and partial-label\nregimes."
    },
    {
        "date": "2025-02",
        "title": "Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic Attacks",
        "author": "Zijian Huang, Yicheng Zhang, Sophie Chen, Nael Abu-Ghazaleh, and Jiasi Chen",
        "link": "http://arxiv.org/abs/2502.08865v2",
        "abstract": "Extended Reality (XR) experiences involve interactions between users, the\nreal world, and virtual content. A key step to enable these experiences is the\nXR headset sensing and estimating the user's pose in order to accurately place\nand render virtual content in the real world. XR headsets use multiple sensors\n(e.g., cameras, inertial measurement unit) to perform pose estimation and\nimprove its robustness, but this provides an attack surface for adversaries to\ninterfere with the pose estimation process. In this paper, we create and study\nthe effects of acoustic attacks that create false signals in the inertial\nmeasurement unit (IMU) on XR headsets, leading to adverse downstream effects on\nXR applications. We generate resonant acoustic signals on a HoloLens 2 and\nmeasure the resulting perturbations in the IMU readings, and also demonstrate\nboth fine-grained and coarse attacks on the popular ORB-SLAM3 and an\nopen-source XR system (ILLIXR). With the knowledge gleaned from attacking these\nopen-source frameworks, we demonstrate four end-to-end proof-of-concept attacks\non a HoloLens 2: manipulating user input, clickjacking, zone invasion, and\ndenial of user interaction. Our experiments show that current commercial XR\nheadsets are susceptible to acoustic attacks, raising concerns for their\nsecurity."
    },
    {
        "date": "2025-02",
        "title": "LSM Trees in Adversarial Environments",
        "author": "Hayder Tirmazi",
        "link": "http://arxiv.org/abs/2502.08832v2",
        "abstract": "The Log Structured Merge (LSM) Tree is a popular choice for key-value stores\nthat focus on optimized write throughput while maintaining performant,\nproduction-ready read latencies. To optimize read performance, LSM stores rely\non a probabilistic data structure called the Bloom Filter (BF). In this paper,\nwe focus on adversarial workloads that lead to a sharp degradation in read\nperformance by impacting the accuracy of BFs used within the LSM store. Our\nevaluation shows up to $800\\%$ increase in the read latency of lookups for\npopular LSM stores. We define adversarial models and security definitions for\nLSM stores. We implement adversary resilience into two popular LSM stores,\nLevelDB and RocksDB. We use our implementations to demonstrate how performance\ndegradation under adversarial workloads can be mitigated."
    },
    {
        "date": "2025-02",
        "title": "Deep EEG Super-Resolution: Upsampling EEG Spatial Resolution with Generative Adversarial Networks",
        "author": "Isaac Corley, and Yufei Huang",
        "link": "http://arxiv.org/abs/2502.08803v1",
        "abstract": "Electroencephalography (EEG) activity contains a wealth of information about\nwhat is happening within the human brain. Recording more of this data has the\npotential to unlock endless future applications. However, the cost of EEG\nhardware is increasingly expensive based upon the number of EEG channels being\nrecorded simultaneously. We combat this problem in this paper by proposing a\nnovel deep EEG super-resolution (SR) approach based on Generative Adversarial\nNetworks (GANs). This approach can produce high spatial resolution EEG data\nfrom low resolution samples, by generating channel-wise upsampled data to\neffectively interpolate numerous missing channels, thus reducing the need for\nexpensive EEG equipment. We tested the performance using an EEG dataset from a\nmental imagery task. Our proposed GAN model provided 10^4 fold and 10^2 fold\nreduction in mean-squared error (MSE) and mean-absolute error (MAE),\nrespectively, over the baseline bicubic interpolation method. We further\nvalidate our method by training a classifier on the original classification\ntask, which displayed minimal loss in accuracy while using the super-resolved\ndata. The proposed SR EEG by GAN is a promising approach to improve the spatial\nresolution of low density EEG headsets."
    },
    {
        "date": "2025-02",
        "title": "Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards",
        "author": "Keerthana Madhavan, Abbas Yazdinejad, Fattane Zarrinkalam, and Ali Dehghantanha",
        "link": "http://arxiv.org/abs/2502.08610v1",
        "abstract": "As AI systems integrate into critical infrastructure, security gaps in AI\ncompliance frameworks demand urgent attention. This paper audits and quantifies\nsecurity risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI\nand Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk\nassessment methodology, we develop four key metrics: Risk Severity Index (RSI),\nAttack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and\nRoot Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns\nacross the frameworks, exposing significant gaps. NIST fails to address 69.23\npercent of identified risks, ALTAI has the highest attack vector vulnerability\n(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with\n80.00 percent of high-risk concerns remaining unresolved. Root cause analysis\nhighlights under-defined processes (ALTAI RCVS = 033) and weak implementation\nguidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings\nemphasize the need for stronger, enforceable security controls in AI\ncompliance. We offer targeted recommendations to enhance security posture and\nbridge the gap between compliance and real-world AI risks."
    },
    {
        "date": "2025-02",
        "title": "Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks",
        "author": "Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, and Micah Goldblum",
        "link": "http://arxiv.org/abs/2502.08586v1",
        "abstract": "A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning."
    },
    {
        "date": "2025-02",
        "title": "Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware Minimization Based on Loss Geometry",
        "author": "Albert Kj\u00f8ller Jacobsen, and Georgios Arvanitidis",
        "link": "http://arxiv.org/abs/2502.08448v1",
        "abstract": "Recent studies on deep neural networks show that flat minima of the loss\nlandscape correlate with improved generalization. Sharpness-aware minimization\n(SAM) efficiently finds flat regions by updating the parameters according to\nthe gradient at an adversarial perturbation. The perturbation depends on the\nEuclidean metric, making SAM non-invariant under reparametrizations, which\nblurs sharpness and generalization. We propose Monge SAM (M-SAM), a\nreparametrization invariant version of SAM by considering a Riemannian metric\nin the parameter space induced naturally by the loss surface. Compared to\nprevious approaches, M-SAM works under any modeling choice, relies only on mild\nassumptions while being as computationally efficient as SAM. We theoretically\nargue that M-SAM varies between SAM and gradient descent (GD), which increases\nrobustness to hyperparameter selection and reduces attraction to suboptimal\nequilibria like saddle points. We demonstrate this behavior both theoretically\nand empirically on a multi-modal representation alignment task."
    },
    {
        "date": "2025-02",
        "title": "AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception",
        "author": "Yuanhao Huang, Qinfan Zhang, Jiandong Xing, Mengyue Cheng, Haiyang Yu, Yilong Ren, and Xiao Xiong",
        "link": "http://arxiv.org/abs/2502.08374v1",
        "abstract": "Perception module of Autonomous vehicles (AVs) are increasingly susceptible\nto be attacked, which exploit vulnerabilities in neural networks through\nadversarial inputs, thereby compromising the AI safety. Some researches focus\non creating covert adversarial samples, but existing global noise techniques\nare detectable and difficult to deceive the human visual system. This paper\nintroduces a novel adversarial attack method, AdvSwap, which creatively\nutilizes wavelet-based high-frequency information swapping to generate covert\nadversarial samples and fool the camera. AdvSwap employs invertible neural\nnetwork for selective high-frequency information swapping, preserving both\nforward propagation and data integrity. The scheme effectively removes the\noriginal label data and incorporates the guidance image data, producing\nconcealed and robust adversarial samples. Experimental evaluations and\ncomparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can\nmake concealed attacks on common traffic targets. The generates adversarial\nsamples are also difficult to perceive by humans and algorithms. Meanwhile, the\nmethod has strong attacking robustness and attacking transferability."
    },
    {
        "date": "2025-02",
        "title": "Compromising Honesty and Harmlessness in Language Models via Deception Attacks",
        "author": "Laur\u00e8ne Vaugrante, Francesca Carlon, Maluna Menke, and Thilo Hagendorff",
        "link": "http://arxiv.org/abs/2502.08301v1",
        "abstract": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical."
    },
    {
        "date": "2025-02",
        "title": "Investigating Vulnerabilities of GPS Trip Data to Trajectory-User Linking Attacks",
        "author": "Benedikt Str\u00f6bl, and Alexandra Kapp",
        "link": "http://arxiv.org/abs/2502.08217v1",
        "abstract": "Open human mobility data is considered an essential basis for the profound\nresearch and analysis required for the transition to sustainable mobility and\nsustainable urban planning. Cycling data has especially been the focus of data\ncollection endeavors in recent years. Although privacy risks regarding location\ndata are widely known, practitioners often refrain from advanced privacy\nmechanisms to prevent utility losses. Removing user identifiers from trips is\nthereby deemed a major privacy gain, as it supposedly prevents linking single\ntrips to obtain entire movement patterns. In this paper, we propose a novel\nattack to reconstruct user identifiers in GPS trip datasets consisting of\nsingle trips, unlike previous ones that are dedicated to evaluating\ntrajectory-user linking in the context of check-in data. We evaluate the\nremaining privacy risk for users in such datasets and our empirical findings\nfrom two real-world datasets show that the risk of re-identification is\nsignificant even when personal identifiers have been removed, and that\ntruncation as a simple additional privacy mechanism may not be effective in\nprotecting user privacy. Further investigations indicate that users who\nfrequently visit locations that are only visited by a small number of others,\ntend to be more vulnerable to re-identification."
    },
    {
        "date": "2025-02",
        "title": "Typographic Attacks in a Multi-Image Setting",
        "author": "Xiaomeng Wang, Zhengyu Zhao, and Martha Larson",
        "link": "http://arxiv.org/abs/2502.08193v1",
        "abstract": "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP."
    },
    {
        "date": "2025-02",
        "title": "CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic Feature Trust Modulus",
        "author": "Yunjiang Xu, Lingzhi Li, Jin Wang, Benyuan Yang, Zhiwen Wu, Xinhong Chen, and Jianping Wang",
        "link": "http://arxiv.org/abs/2502.08169v1",
        "abstract": "Collaborative perception, fusing information from multiple agents, can extend\nperception range so as to improve perception performance. However, temporal\nasynchrony in real-world environments, caused by communication delays, clock\nmisalignment, or sampling configuration differences, can lead to information\nmismatches. If this is not well handled, then the collaborative performance is\npatchy, and what's worse safety accidents may occur. To tackle this challenge,\nwe propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception\nframework that is robust to the information mismatches caused by temporal\nasynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each\nregion of interest by modeling aleatoric and epistemic uncertainty as well as\nselectively suppressing or retaining single-vehicle features, thereby\nmitigating information mismatches. We then design a multi-scale fusion module\nto handle multi-scale feature maps processed by DFTM. Compared to existing\nworks that also consider asynchronous collaborative perception, CoDynTrust\ncombats various low-quality information in temporally asynchronous scenarios\nand allows uncertainty to be propagated to downstream tasks such as planning\nand control. Experimental results demonstrate that CoDynTrust significantly\nreduces performance degradation caused by temporal asynchrony across multiple\ndatasets, achieving state-of-the-art detection performance even with temporal\nasynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust."
    },
    {
        "date": "2025-02",
        "title": "Local Differential Privacy is Not Enough: A Sample Reconstruction Attack against Federated Learning with Local Differential Privacy",
        "author": "Zhichao You, Xuewen Dong, Shujun Li, Ximeng Liu, Siqi Ma, and Yulong Shen",
        "link": "http://arxiv.org/abs/2502.08151v1",
        "abstract": "Reconstruction attacks against federated learning (FL) aim to reconstruct\nusers' samples through users' uploaded gradients. Local differential privacy\n(LDP) is regarded as an effective defense against various attacks, including\nsample reconstruction in FL, where gradients are clipped and perturbed.\nExisting attacks are ineffective in FL with LDP since clipped and perturbed\ngradients obliterate most sample information for reconstruction. Besides,\nexisting attacks embed additional sample information into gradients to improve\nthe attack effect and cause gradient expansion, leading to a more severe\ngradient clipping in FL with LDP. In this paper, we propose a sample\nreconstruction attack against LDP-based FL with any target models to\nreconstruct victims' sensitive samples to illustrate that FL with LDP is not\nflawless. Considering gradient expansion in reconstruction attacks and noise in\nLDP, the core of the proposed attack is gradient compression and reconstructed\nsample denoising. For gradient compression, an inference structure based on\nsample characteristics is presented to reduce redundant gradients against LDP.\nFor reconstructed sample denoising, we artificially introduce zero gradients to\nobserve noise distribution and scale confidence interval to filter the noise.\nTheoretical proof guarantees the effectiveness of the proposed attack.\nEvaluations show that the proposed attack is the only attack that reconstructs\nvictims' training samples in LDP-based FL and has little impact on the target\nmodel's accuracy. We conclude that LDP-based FL needs further improvements to\ndefend against sample reconstruction attacks effectively."
    },
    {
        "date": "2025-02",
        "title": "Knowledge-Guided Wasserstein Distributionally Robust Optimization",
        "author": "Zitao Wang, Ziyuan Wang, Molei Liu, and Nian Si",
        "link": "http://arxiv.org/abs/2502.08146v1",
        "abstract": "Transfer learning is a popular strategy to leverage external knowledge and\nimprove statistical efficiency, particularly with a limited target sample. We\npropose a novel knowledge-guided Wasserstein Distributionally Robust\nOptimization (KG-WDRO) framework that adaptively incorporates multiple sources\nof external knowledge to overcome the conservativeness of vanilla WDRO, which\noften results in overly pessimistic shrinkage toward zero. Our method\nconstructs smaller Wasserstein ambiguity sets by controlling the transportation\nalong directions informed by the source knowledge. This strategy can alleviate\nperturbations on the predictive projection of the covariates and protect\nagainst information loss. Theoretically, we establish the equivalence between\nour WDRO formulation and the knowledge-guided shrinkage estimation based on\ncollinear similarity, ensuring tractability and geometrizing the feasible set.\nThis also reveals a novel and general interpretation for recent shrinkage-based\ntransfer learning approaches from the perspective of distributional robustness.\nIn addition, our framework can adjust for scaling differences in the regression\nmodels between the source and target and accommodates general types of\nregularization such as lasso and ridge. Extensive simulations demonstrate the\nsuperior performance and adaptivity of KG-WDRO in enhancing small-sample\ntransfer learning."
    },
    {
        "date": "2025-02",
        "title": "LIR-LIVO: A Lightweight,Robust LiDAR/Vision/Inertial Odometry with Illumination-Resilient Deep Features",
        "author": "Shujie Zhou, Zihao Wang, Xinye Dai, Weiwei Song, and Shengfeng Gu",
        "link": "http://arxiv.org/abs/2502.08676v1",
        "abstract": "In this paper, we propose LIR-LIVO, a lightweight and robust\nLiDAR-inertial-visual odometry system designed for challenging illumination and\ndegraded environments. The proposed method leverages deep learning-based\nillumination-resilient features and LiDAR-Inertial-Visual Odometry (LIVO). By\nincorporating advanced techniques such as uniform depth distribution of\nfeatures enabled by depth association with LiDAR point clouds and adaptive\nfeature matching utilizing Superpoint and LightGlue, LIR-LIVO achieves\nstate-of-the-art (SOTA) accuracy and robustness with low computational cost.\nExperiments are conducted on benchmark datasets, including NTU-VIRAL, Hilti'22,\nand R3LIVE-Dataset. The corresponding results demonstrate that our proposed\nmethod outperforms other SOTA methods on both standard and challenging\ndatasets. Particularly, the proposed method demonstrates robust pose estimation\nunder poor ambient lighting conditions in the Hilti'22 dataset. The code of\nthis work is publicly accessible on GitHub to facilitate advancements in the\nrobotics community."
    },
    {
        "date": "2025-02",
        "title": "Provably Robust Federated Reinforcement Learning",
        "author": "Minghong Fang, Xilong Wang, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2502.08123v1",
        "abstract": "Federated reinforcement learning (FRL) allows agents to jointly learn a\nglobal decision-making policy under the guidance of a central server. While FRL\nhas advantages, its decentralized design makes it prone to poisoning attacks.\nTo mitigate this, Byzantine-robust aggregation techniques tailored for FRL have\nbeen introduced. Yet, in our work, we reveal that these current\nByzantine-robust techniques are not immune to our newly introduced Normalized\nattack. Distinct from previous attacks that targeted enlarging the distance of\npolicy updates before and after an attack, our Normalized attack emphasizes on\nmaximizing the angle of deviation between these updates. To counter these\nthreats, we develop an ensemble FRL approach that is provably secure against\nboth known and our newly proposed attacks. Our ensemble method involves\ntraining multiple global policies, where each is learnt by a group of agents\nusing any foundational aggregation rule. These well-trained global policies\nthen individually predict the action for a specific test state. The ultimate\naction is chosen based on a majority vote for discrete action systems or the\ngeometric median for continuous ones. Our experimental results across different\nsettings show that the Normalized attack can greatly disrupt non-ensemble\nByzantine-robust methods, and our ensemble approach offers substantial\nresistance against poisoning attacks."
    },
    {
        "date": "2025-02",
        "title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models",
        "author": "Peng-Fei Zhang, Guangdong Bai, and Zi Huang",
        "link": "http://arxiv.org/abs/2502.08079v1",
        "abstract": "Current adversarial attacks for evaluating the robustness of vision-language\npre-trained (VLP) models in multi-modal tasks suffer from limited\ntransferability, where attacks crafted for a specific model often struggle to\ngeneralize effectively across different models, limiting their utility in\nassessing robustness more broadly. This is mainly attributed to the\nover-reliance on model-specific features and regions, particularly in the image\nmodality. In this paper, we propose an elegant yet highly effective method\ntermed Meticulous Adversarial Attack (MAA) to fully exploit model-independent\ncharacteristics and vulnerabilities of individual samples, achieving enhanced\ngeneralizability and reduced model dependence. MAA emphasizes fine-grained\noptimization of adversarial images by developing a novel resizing and sliding\ncrop (RScrop) technique, incorporating a multi-granularity similarity\ndisruption (MGSD) strategy. Extensive experiments across diverse VLP models,\nmultiple benchmark datasets, and a variety of downstream tasks demonstrate that\nMAA significantly enhances the effectiveness and transferability of adversarial\nattacks. A large cohort of performance studies is conducted to generate\ninsights into the effectiveness of various model configurations, guiding future\nadvancements in this domain."
    },
    {
        "date": "2025-02",
        "title": "Cascading Bandits Robust to Adversarial Corruptions",
        "author": "Jize Xie, Cheng Chen, Zhiyong Wang, and Shuai Li",
        "link": "http://arxiv.org/abs/2502.08077v1",
        "abstract": "Online learning to rank sequentially recommends a small list of items to\nusers from a large candidate set and receives the users' click feedback. In\nmany real-world scenarios, users browse the recommended list in order and click\nthe first attractive item without checking the rest. Such behaviors are usually\nformulated as the cascade model. Many recent works study algorithms for\ncascading bandits, an online learning to rank framework in the cascade model.\nHowever, the performance of existing methods may drop significantly if part of\nthe user feedback is adversarially corrupted (e.g., click fraud). In this work,\nwe study how to resist adversarial corruptions in cascading bandits. We first\nformulate the ``\\textit{Cascading Bandits with Adversarial Corruptions}\" (CBAC)\nproblem, which assumes that there is an adaptive adversary that may manipulate\nthe user feedback. Then we propose two robust algorithms for this problem,\nwhich assume the corruption level is known and agnostic, respectively. We show\nthat both algorithms can achieve logarithmic regret when the algorithm is not\nunder attack, and the regret increases linearly with the corruption level. The\nexperimental results also verify the robustness of our methods."
    },
    {
        "date": "2025-02",
        "title": "General Coded Computing: Adversarial Settings",
        "author": "Parsa Moradi, Hanzaleh Akbarinodehi, and Mohammad Ali Maddah-Ali",
        "link": "http://arxiv.org/abs/2502.08058v1",
        "abstract": "Conventional coded computing frameworks are predominantly tailored for\nstructured computations, such as matrix multiplication and polynomial\nevaluation. Such tasks allow the reuse of tools and techniques from algebraic\ncoding theory to improve the reliability of distributed systems in the presence\nof stragglers and adversarial servers.\n  This paper lays the foundation for general coded computing, which extends the\napplicability of coded computing to handle a wide class of computations. In\naddition, it particularly addresses the challenging problem of managing\nadversarial servers. We demonstrate that, in the proposed scheme, for a system\nwith $N$ servers, where $\\mathcal{O}(N^a)$, $a \\in [0,1)$, are adversarial, the\nsupremum of the average approximation error over all adversarial strategies\ndecays at a rate of $N^{\\frac{6}{5}(a-1)}$, under minimal assumptions on the\ncomputing tasks. Furthermore, we show that within a general framework, the\nproposed scheme achieves optimal adversarial robustness, in terms of maximum\nnumber of adversarial servers it can tolerate. This marks a significant step\ntoward practical and reliable general coded computing. Implementation results\nfurther validate the effectiveness of the proposed method in handling various\ncomputations, including inference in deep neural networks."
    },
    {
        "date": "2025-02",
        "title": "SLVR: Securely Leveraging Client Validation for Robust Federated Learning",
        "author": "Jihye Choi, Sai Rahul Rachuri, Ke Wang, Somesh Jha, and Yizhen Wang",
        "link": "http://arxiv.org/abs/2502.08055v1",
        "abstract": "Federated Learning (FL) enables collaborative model training while keeping\nclient data private. However, exposing individual client updates makes FL\nvulnerable to reconstruction attacks. Secure aggregation mitigates such privacy\nrisks but prevents the server from verifying the validity of each client\nupdate, creating a privacy-robustness tradeoff. Recent efforts attempt to\naddress this tradeoff by enforcing checks on client updates using\nzero-knowledge proofs, but they support limited predicates and often depend on\npublic validation data. We propose SLVR, a general framework that securely\nleverages clients' private data through secure multi-party computation. By\nutilizing clients' data, SLVR not only eliminates the need for public\nvalidation data, but also enables a wider range of checks for robustness,\nincluding cross-client accuracy validation. It also adapts naturally to\ndistribution shifts in client data as it can securely refresh its validation\ndata up-to-date. Our empirical evaluations show that SLVR improves robustness\nagainst model poisoning attacks, particularly outperforming existing methods by\nup to 50% under adaptive attacks. Additionally, SLVR demonstrates effective\nadaptability and stable convergence under various distribution shift scenarios."
    },
    {
        "date": "2025-02",
        "title": "Quaternion-Hadamard Network: A Novel Defense Against Adversarial Attacks with a New Dataset",
        "author": "Vladimir Frants, and Sos Agaian",
        "link": "http://arxiv.org/abs/2502.10452v1",
        "abstract": "This paper addresses the vulnerability of deep-learning models designed for\nrain, snow, and haze removal. Despite enhancing image quality in adverse\nweather, these models are susceptible to adversarial attacks that compromise\ntheir effectiveness. Traditional defenses such as adversarial training and\nmodel distillation often require extensive retraining, making them costly and\nimpractical for real-world deployment. While denoising and super-resolution\ntechniques can aid image classification models, they impose high computational\ndemands and introduce visual artifacts that hinder image processing tasks. We\npropose a model-agnostic defense against first-order white-box adversarial\nattacks using the Quaternion-Hadamard Network (QHNet) to tackle these\nchallenges. White-box attacks are particularly difficult to defend against\nsince attackers have full access to the model's architecture, weights, and\ntraining procedures. Our defense introduces the Quaternion Hadamard Denoising\nConvolutional Block (QHDCB) and the Quaternion Denoising Residual Block (QDRB),\nleveraging polynomial thresholding. QHNet incorporates these blocks within an\nencoder-decoder architecture, enhanced by feature refinement, to effectively\nneutralize adversarial noise. Additionally, we introduce the Adversarial\nWeather Conditions Vision Dataset (AWCVD), created by applying first-order\ngradient attacks on state-of-the-art weather removal techniques in scenarios\ninvolving haze, rain streaks, and snow. Using PSNR and SSIM metrics, we\ndemonstrate that QHNet significantly enhances the robustness of low-level\ncomputer vision models against adversarial attacks compared with\nstate-of-the-art denoising and super-resolution techniques. The source code and\ndataset will be released alongside the final version of this paper."
    },
    {
        "date": "2025-02",
        "title": "From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis",
        "author": "Kristofer Grover Roos, Atsushi Fukuda, and Quan Huu Cap",
        "link": "http://arxiv.org/abs/2502.08025v2",
        "abstract": "While functional magnetic resonance imaging (fMRI) offers rich spatial\nresolution, it is limited by high operational costs and significant\ninfrastructural demands. In contrast, electroencephalography (EEG) provides\nmillisecond-level precision in capturing electrical activity but lacks the\nspatial resolution necessary for precise neural localization. To bridge these\ngaps, we introduce E2fNet, a simple yet effective deep learning model for\nsynthesizing fMRI images from low-cost EEG data. E2fNet is specifically\ndesigned to capture and translate meaningful features from EEG across electrode\nchannels into accurate fMRI representations. Extensive evaluations across three\ndatasets demonstrate that E2fNet consistently outperforms existing methods,\nachieving state-of-the-art results in terms of the structural similarity index\nmeasure (SSIM). Our findings suggest that E2fNet is a promising, cost-effective\nsolution for enhancing neuroimaging capabilities. The code is available at\nhttps://github.com/kgr20/E2fNet."
    },
    {
        "date": "2025-02",
        "title": "Universal Adversarial Attack on Aligned Multimodal LLMs",
        "author": "Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, and Anton Razzhigaev",
        "link": "http://arxiv.org/abs/2502.07987v2",
        "abstract": "We propose a universal adversarial attack on multimodal Large Language Models\n(LLMs) that leverages a single optimized image to override alignment safeguards\nacross diverse queries and even multiple models. By backpropagating through the\nvision encoder and language head, we craft a synthetic image that forces the\nmodel to respond with a targeted phrase (e.g., ''Sure, here it is'') or\notherwise unsafe content-even for harmful prompts. In experiments on the\nSafeBench benchmark, our method achieves significantly higher attack success\nrates than existing baselines, including text-only universal prompts (e.g., up\nto 93% on certain models). We further demonstrate cross-model transferability\nby training on several multimodal LLMs simultaneously and testing on unseen\narchitectures. Additionally, a multi-answer variant of our approach produces\nmore natural-sounding (yet still malicious) responses. These findings\nunderscore critical vulnerabilities in current multimodal alignment and call\nfor more robust adversarial defenses. We will release code and datasets under\nthe Apache-2.0 license. Warning: some content generated by Multimodal LLMs in\nthis paper may be offensive to some readers."
    },
    {
        "date": "2025-02",
        "title": "PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization",
        "author": "Bing Fan, Yunhe Feng, Yapeng Tian, Yuewei Lin, Yan Huang, and Heng Fan",
        "link": "http://arxiv.org/abs/2502.07707v1",
        "abstract": "Egocentric visual query localization (EgoVQL) focuses on localizing the\ntarget of interest in space and time from first-person videos, given a visual\nquery. Despite recent progressive, existing methods often struggle to handle\nsevere object appearance changes and cluttering background in the video due to\nlacking sufficient target cues, leading to degradation. Addressing this, we\nintroduce PRVQL, a novel Progressive knowledge-guided Refinement framework for\nEgoVQL. The core is to continuously exploit target-relevant knowledge directly\nfrom videos and utilize it as guidance to refine both query and video features\nfor improving target localization. Our PRVQL contains multiple processing\nstages. The target knowledge from one stage, comprising appearance and spatial\nknowledge extracted via two specially designed knowledge learning modules, are\nutilized as guidance to refine the query and videos features for the next\nstage, which are used to generate more accurate knowledge for further feature\nrefinement. With such a progressive process, target knowledge in PRVQL can be\ngradually improved, which, in turn, leads to better refined query and video\nfeatures for localization in the final stage. Compared to previous methods, our\nPRVQL, besides the given object cues, enjoys additional crucial target\ninformation from a video as guidance to refine features, and hence enhances\nEgoVQL in complicated scenes. In our experiments on challenging Ego4D, PRVQL\nachieves state-of-the-art result and largely surpasses other methods, showing\nits efficacy. Our code, model and results will be released at\nhttps://github.com/fb-reps/PRVQL."
    },
    {
        "date": "2025-02",
        "title": "JBShield: Defending Large Language Models from Jailbreak Attacks through Activated Concept Analysis and Manipulation",
        "author": "Shenyi Zhang, Yuchen Zhai, Keyan Guo, Hongxin Hu, Shengnan Guo, Zheng Fang, Lingchen Zhao, Chao Shen, Cong Wang, and Qian Wang",
        "link": "http://arxiv.org/abs/2502.07557v1",
        "abstract": "Despite the implementation of safety alignment strategies, large language\nmodels (LLMs) remain vulnerable to jailbreak attacks, which undermine these\nsafety guardrails and pose significant security threats. Some defenses have\nbeen proposed to detect or mitigate jailbreaks, but they are unable to\nwithstand the test of time due to an insufficient understanding of jailbreak\nmechanisms. In this work, we investigate the mechanisms behind jailbreaks based\non the Linear Representation Hypothesis (LRH), which states that neural\nnetworks encode high-level concepts as subspaces in their hidden\nrepresentations. We define the toxic semantics in harmful and jailbreak prompts\nas toxic concepts and describe the semantics in jailbreak prompts that\nmanipulate LLMs to comply with unsafe requests as jailbreak concepts. Through\nconcept extraction and analysis, we reveal that LLMs can recognize the toxic\nconcepts in both harmful and jailbreak prompts. However, unlike harmful\nprompts, jailbreak prompts activate the jailbreak concepts and alter the LLM\noutput from rejection to compliance. Building on our analysis, we propose a\ncomprehensive jailbreak defense framework, JBShield, consisting of two key\ncomponents: jailbreak detection JBShield-D and mitigation JBShield-M.\nJBShield-D identifies jailbreak prompts by determining whether the input\nactivates both toxic and jailbreak concepts. When a jailbreak prompt is\ndetected, JBShield-M adjusts the hidden representations of the target LLM by\nenhancing the toxic concept and weakening the jailbreak concept, ensuring LLMs\nproduce safe content. Extensive experiments demonstrate the superior\nperformance of JBShield, achieving an average detection accuracy of 0.95 and\nreducing the average attack success rate of various jailbreak attacks to 2%\nfrom 61% across distinct LLMs."
    },
    {
        "date": "2025-02",
        "title": "Scalable and Robust Physics-Informed Graph Neural Networks for Water Distribution Systems",
        "author": "Inaam Ashraf, Andr\u00e9 Artelt, and Barbara Hammer",
        "link": "http://arxiv.org/abs/2502.12164v1",
        "abstract": "Water distribution systems (WDSs) are an important part of critical\ninfrastructure becoming increasingly significant in the face of climate change\nand urban population growth. We propose a robust and scalable surrogate deep\nlearning (DL) model to enable efficient planning, expansion, and rehabilitation\nof WDSs. Our approach incorporates an improved graph neural network\narchitecture, an adapted physics-informed algorithm, an innovative training\nscheme, and a physics-preserving data normalization method. Evaluation results\non a number of WDSs demonstrate that our model outperforms the current\nstate-of-the-art DL model. Moreover, our method allows us to scale the model to\nbigger and more realistic WDSs. Furthermore, our approach makes the model more\nrobust to out-of-distribution input features (demands, pipe diameters). Hence,\nour proposed method constitutes a significant step towards bridging the\nsimulation-to-real gap in the use of artificial intelligence for WDSs."
    },
    {
        "date": "2025-02",
        "title": "CodePhys: Robust Video-based Remote Physiological Measurement through Latent Codebook Querying",
        "author": "Shuyang Chu, Menghan Xia, Mengyao Yuan, Xin Liu, Tapio Seppanen, Guoying Zhao, and Jingang Shi",
        "link": "http://arxiv.org/abs/2502.07526v1",
        "abstract": "Remote photoplethysmography (rPPG) aims to measure non-contact physiological\nsignals from facial videos, which has shown great potential in many\napplications. Most existing methods directly extract video-based rPPG features\nby designing neural networks for heart rate estimation. Although they can\nachieve acceptable results, the recovery of rPPG signal faces intractable\nchallenges when interference from real-world scenarios takes place on facial\nvideo. Specifically, facial videos are inevitably affected by non-physiological\nfactors (e.g., camera device noise, defocus, and motion blur), leading to the\ndistortion of extracted rPPG signals. Recent rPPG extraction methods are easily\naffected by interference and degradation, resulting in noisy rPPG signals. In\nthis paper, we propose a novel method named CodePhys, which innovatively treats\nrPPG measurement as a code query task in a noise-free proxy space (i.e.,\ncodebook) constructed by ground-truth PPG signals. We consider noisy rPPG\nfeatures as queries and generate high-fidelity rPPG features by matching them\nwith noise-free PPG features from the codebook. Our approach also incorporates\na spatial-aware encoder network with a spatial attention mechanism to highlight\nphysiologically active areas and uses a distillation loss to reduce the\ninfluence of non-periodic visual interference. Experimental results on four\nbenchmark datasets demonstrate that CodePhys outperforms state-of-the-art\nmethods in both intra-dataset and cross-dataset settings."
    },
    {
        "date": "2025-02",
        "title": "RoMA: Robust Malware Attribution via Byte-level Adversarial Training with Global Perturbations and Adversarial Consistency Regularization",
        "author": "Yuxia Sun, Huihong Chen, Jingcai Guo, Aoxiang Sun, Zhetao Li, and Haolin Liu",
        "link": "http://arxiv.org/abs/2502.07492v2",
        "abstract": "Attributing APT (Advanced Persistent Threat) malware to their respective\ngroups is crucial for threat intelligence and cybersecurity. However, APT\nadversaries often conceal their identities, rendering attribution inherently\nadversarial. Existing machine learning-based attribution models, while\neffective, remain highly vulnerable to adversarial attacks. For example, the\nstate-of-the-art byte-level model MalConv sees its accuracy drop from over 90%\nto below 2% under PGD (projected gradient descent) attacks. Existing\ngradient-based adversarial training techniques for malware detection or image\nprocessing were applied to malware attribution in this study, revealing that\nboth robustness and training efficiency require significant improvement. To\naddress this, we propose RoMA, a novel single-step adversarial training\napproach that integrates global perturbations to generate enhanced adversarial\nsamples and employs adversarial consistency regularization to improve\nrepresentation quality and resilience. A novel APT malware dataset named AMG18,\nwith diverse samples and realistic class imbalances, is introduced for\nevaluation. Extensive experiments show that RoMA significantly outperforms\nseven competing methods in both adversarial robustness (e.g., achieving over\n80% robust accuracy-more than twice that of the next-best method under PGD\nattacks) and training efficiency (e.g., more than twice as fast as the\nsecond-best method in terms of accuracy), while maintaining superior standard\naccuracy in non-adversarial scenarios."
    },
    {
        "date": "2025-02",
        "title": "Supply Chain Network Security Investment Strategies Based on Nonlinear Budget Constraints: The Moderating Roles of Market Share and Attack Risk",
        "author": "Jiajie Cheng, Jiaxin Wang, Caijiao Li, Luxiang Zhang, Yusheng Fan, Yujie Bao, and Wen Zhou",
        "link": "http://arxiv.org/abs/2502.10448v1",
        "abstract": "In the context of the rapid development of digital supply chain networks,\ndealing with the increasing cybersecurity threats and formulating effective\nsecurity investment strategies to defend against cyberattack risks are the core\nissues in supply chain management. Cybersecurity investment decision-making is\na key strategic task in enterprise supply chain manage-ment. Traditional game\ntheory models and linear programming methods make it challenging to deal with\ncomplex problems such as multi-party par-ticipation in the supply chain,\nresource constraints, and risk uncertainty, re-sulting in enterprises facing\nhigh risks and uncertainties in the field of cy-bersecurity. To effectively\nmeet this challenge, this study proposes a nonlin-ear budget-constrained\ncybersecurity investment optimization model based on variational inequality and\nprojection shrinkage algorithm. This method simulates the impact of market\ncompetition on security investment by intro-ducing market share variables,\ncombining variational inequality and projec-tion shrinkage algorithm to solve\nthe model, and analyzing the effect of dif-ferent variables such as budget\nconstraints, cyberattack losses, and market share on supply chain network\nsecurity. In numerical analysis, the model achieved high cybersecurity levels\nof 0.96 and 0.95 in the experimental sce-narios of two retailers and two demand\nmarkets, respectively, and the budget constraint analysis revealed the profound\nimpact of budget constraints on cybersecurity investment. Through numerical\nexperiments and comparative analysis, the effectiveness and operability of this\nmethod in improving sup-ply chain network security are verified."
    },
    {
        "date": "2025-02",
        "title": "MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition",
        "author": "Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, and Se-Young Yun",
        "link": "http://arxiv.org/abs/2502.10447v1",
        "abstract": "Audio-visual speech recognition (AVSR) has become critical for enhancing\nspeech recognition in noisy environments by integrating both auditory and\nvisual modalities. However, existing AVSR systems struggle to scale up without\ncompromising computational efficiency. In this study, we introduce MoHAVE\n(Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework\ndesigned to address these scalability constraints. By leveraging a\nMixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific\nexpert groups, ensuring dynamic adaptation to various audio-visual inputs with\nminimal computational overhead. Key contributions of MoHAVE include: (1) a\nsparse MoE framework that efficiently scales AVSR model capacity, (2) a\nhierarchical gating mechanism that dynamically utilizes the expert groups based\non input context, enhancing adaptability and robustness, and (3) remarkable\nperformance across robust AVSR benchmarks, including LRS3 and MuAViC\ntranscription and translation tasks, setting a new standard for scalable speech\nrecognition systems."
    },
    {
        "date": "2025-02",
        "title": "MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks",
        "author": "Lotfi Abdelkrim Mecharbat, Alberto Marchisio, Muhammad Shafique, Mohammad M. Ghassemi, and Tuka Alhanai",
        "link": "http://arxiv.org/abs/2502.07422v1",
        "abstract": "There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs."
    },
    {
        "date": "2025-02",
        "title": "Mining Power Destruction Attacks in the Presence of Petty-Compliant Mining Pools",
        "author": "Roozbeh Sarenche, Svetla Nikova, and Bart Preneel",
        "link": "http://arxiv.org/abs/2502.07410v1",
        "abstract": "Bitcoin's security relies on its Proof-of-Work consensus, where miners solve\npuzzles to propose blocks. The puzzle's difficulty is set by the difficulty\nadjustment mechanism (DAM), based on the network's available mining power.\nAttacks that destroy some portion of mining power can exploit the DAM to lower\ndifficulty, making such attacks profitable. In this paper, we analyze three\ntypes of mining power destruction attacks in the presence of petty-compliant\nmining pools: selfish mining, bribery, and mining power distraction attacks. We\nanalyze selfish mining while accounting for the distribution of mining power\namong pools, a factor often overlooked in the literature. Our findings indicate\nthat selfish mining can be more destructive when the non-adversarial mining\nshare is well distributed among pools. We also introduce a novel bribery\nattack, where the adversarial pool bribes petty-compliant pools to orphan\nothers' blocks. For small pools, we demonstrate that the bribery attack can\ndominate strategies like selfish mining or undercutting. Lastly, we present the\nmining distraction attack, where the adversarial pool incentivizes\npetty-compliant pools to abandon Bitcoin's puzzle and mine for a simpler\npuzzle, thus wasting some part of their mining power. Similar to the previous\nattacks, this attack can lower the mining difficulty, but with the difference\nthat it does not generate any evidence of mining power destruction, such as\norphan blocks."
    },
    {
        "date": "2025-02",
        "title": "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "author": "Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, and Ivan Oseledets",
        "link": "http://arxiv.org/abs/2502.07845v1",
        "abstract": "Generative models that can produce realistic images have improved\nsignificantly in recent years. The quality of the generated content has\nincreased drastically, so sometimes it is very difficult to distinguish between\nthe real images and the generated ones. Such an improvement comes at a price of\nethical concerns about the usage of the generative models: the users of\ngenerative models can improperly claim ownership of the generated content\nprotected by a license. In this paper, we propose an approach to embed\nwatermarks into the generated content to allow future detection of the\ngenerated content and identification of the user who generated it. The\nwatermark is embedded during the inference of the model, so the proposed\napproach does not require the retraining of the latter. We prove that\nwatermarks embedded are guaranteed to be robust against additive perturbations\nof a bounded magnitude. We apply our method to watermark diffusion models and\nshow that it matches state-of-the-art watermarking schemes in terms of\nrobustness to different types of synthetic watermark removal attacks."
    },
    {
        "date": "2025-02",
        "title": "Robust Indoor Localization in Dynamic Environments: A Multi-source Unsupervised Domain Adaptation Framework",
        "author": "Jiyu Jiao, Xiaojun Wang, and Chengpei Han",
        "link": "http://arxiv.org/abs/2502.07246v1",
        "abstract": "Fingerprint localization has gained significant attention due to its\ncost-effective deployment, low complexity, and high efficacy. However,\ntraditional methods, while effective for static data, often struggle in dynamic\nenvironments where data distributions and feature spaces evolve-a common\noccurrence in real-world scenarios. To address the challenges of robustness and\nadaptability in fingerprint localization for dynamic indoor environments, this\npaper proposes DF-Loc, an end-to-end dynamic fingerprint localization system\nbased on multi-source unsupervised domain adaptation (MUDA). DF-Loc leverages\nhistorical data from multiple time scales to facilitate knowledge transfer in\nspecific feature spaces, thereby enhancing generalization capabilities in the\ntarget domain and reducing reliance on labeled data. Specifically, the system\nincorporates a Quality Control (QC) module for CSI data preprocessing and\nemploys image processing techniques for CSI fingerprint feature reconstruction.\nAdditionally, a multi-scale attention-based feature fusion backbone network is\ndesigned to extract multi-level transferable fingerprint features. Finally, a\ndual-stage alignment model aligns the distributions of multiple source-target\ndomain pairs, improving regression characteristics in the target domain.\nExtensive experiments conducted in office and classroom environments\ndemonstrate that DF-Loc outperforms comparative methods in terms of both\nlocalization accuracy and robustness. With 60% of reference points used for\ntraining, DF-Loc achieves average localization errors of 0.79m and 3.72m in\n\"same-test\" scenarios, and 0.94m and 4.39m in \"different-test\" scenarios,\nrespectively. This work pioneers an end-to-end multi-source transfer learning\napproach for fingerprint localization, providing valuable insights for future\nresearch in dynamic environments."
    },
    {
        "date": "2025-02",
        "title": "Simplifying Adversarially Robust PAC Learning with Tolerance",
        "author": "Hassan Ashtiani, Vinayak Pathak, and Ruth Urner",
        "link": "http://arxiv.org/abs/2502.07232v1",
        "abstract": "Adversarially robust PAC learning has proved to be challenging, with the\ncurrently best known learners [Montasser et al., 2021a] relying on improper\nmethods based on intricate compression schemes, resulting in sample complexity\nexponential in the VC-dimension. A series of follow up work considered a\nslightly relaxed version of the problem called adversarially robust learning\nwith tolerance [Ashtiani et al., 2023, Bhattacharjee et al., 2023, Raman et\nal., 2024] and achieved better sample complexity in terms of the VC-dimension.\nHowever, those algorithms were either improper and complex, or required\nadditional assumptions on the hypothesis class H. We prove, for the first time,\nthe existence of a simpler learner that achieves a sample complexity linear in\nthe VC-dimension without requiring additional assumptions on H. Even though our\nlearner is improper, it is \"almost proper\" in the sense that it outputs a\nhypothesis that is \"similar\" to a hypothesis in H.\n  We also use the ideas from our algorithm to construct a semi-supervised\nlearner in the tolerant setting. This simple algorithm achieves comparable\nbounds to the previous (non-tolerant) semi-supervised algorithm of Attias et\nal. [2022a], but avoids the use of intricate subroutines from previous works,\nand is \"almost proper.\""
    },
    {
        "date": "2025-02",
        "title": "CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models",
        "author": "Sen Peng, Mingyue Wang, Jianfei He, Jijia Yang, and Xiaohua Jia",
        "link": "http://arxiv.org/abs/2502.07225v1",
        "abstract": "Latent diffusion models have recently demonstrated superior capabilities in\nmany downstream image synthesis tasks. However, customization of latent\ndiffusion models using unauthorized data can severely compromise the privacy\nand intellectual property rights of data owners. Adversarial examples as\nprotective perturbations have been developed to defend against unauthorized\ndata usage by introducing imperceptible noise to customization samples,\npreventing diffusion models from effectively learning them. In this paper, we\nfirst reveal that the primary reason adversarial examples are effective as\nprotective perturbations in latent diffusion models is the distortion of their\nlatent representations, as demonstrated through qualitative and quantitative\nexperiments. We then propose the Contrastive Adversarial Training (CAT)\nutilizing adapters as an adaptive attack against these protection methods,\nhighlighting their lack of robustness. Extensive experiments demonstrate that\nour CAT method significantly reduces the effectiveness of protective\nperturbations in customization configurations, urging the community to\nreconsider and enhance the robustness of existing protective perturbation\nmethods. Code is available at \\hyperlink{here}{https://github.com/senp98/CAT}."
    },
    {
        "date": "2025-02",
        "title": "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning",
        "author": "Pengyu Wang, Jialu Li, and Ling Shi",
        "link": "http://arxiv.org/abs/2502.07839v1",
        "abstract": "With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod."
    },
    {
        "date": "2025-02",
        "title": "Color-Quality Invariance for Robust Medical Image Segmentation",
        "author": "Ravi Shah, Atsushi Fukuda, and Quan Huu Cap",
        "link": "http://arxiv.org/abs/2502.07200v1",
        "abstract": "Single-source domain generalization (SDG) in medical image segmentation\nremains a significant challenge, particularly for images with varying color\ndistributions and qualities. Previous approaches often struggle when models\ntrained on high-quality images fail to generalize to low-quality test images\ndue to these color and quality shifts. In this work, we propose two novel\ntechniques to enhance generalization: dynamic color image normalization (DCIN)\nmodule and color-quality generalization (CQG) loss. The DCIN dynamically\nnormalizes the color of test images using two reference image selection\nstrategies. Specifically, the DCIN utilizes a global reference image selection\n(GRIS), which finds a universal reference image, and a local reference image\nselection (LRIS), which selects a semantically similar reference image per test\nsample. Additionally, CQG loss enforces invariance to color and quality\nvariations by ensuring consistent segmentation predictions across transformed\nimage pairs. Experimental results show that our proposals significantly improve\nsegmentation performance over the baseline on two target domain datasets,\ndespite being trained solely on a single source domain. Notably, our model\nachieved up to a 32.3-point increase in Dice score compared to the baseline,\nconsistently producing robust and usable results even under substantial domain\nshifts. Our work contributes to the development of more robust medical image\nsegmentation models that generalize across unseen domains. The implementation\ncode is available at https://github.com/RaviShah1/DCIN-CQG."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Robustness Of Digital Shadow For CO2 Storage Monitoring With Augmented Rock Physics Modeling",
        "author": "Abhinav Prakash Gahlot, and Felix J. Herrmann",
        "link": "http://arxiv.org/abs/2502.07171v1",
        "abstract": "To meet climate targets, the IPCC underscores the necessity of technologies\ncapable of removing gigatonnes of CO2 annually, with Geological Carbon Storage\n(GCS) playing a central role. GCS involves capturing CO2 and injecting it into\ndeep geological formations for long-term storage, requiring precise monitoring\nto ensure containment and prevent leakage. Time-lapse seismic imaging is\nessential for tracking CO2 migration but often struggles to capture the\ncomplexities of multi-phase subsurface flow. Digital Shadows (DS), leveraging\nmachine learning-driven data assimilation techniques such as nonlinear Bayesian\nfiltering and generative AI, provide a more detailed, uncertainty-aware\nmonitoring approach. By incorporating uncertainties in reservoir properties, DS\nframeworks improve CO2 migration forecasts, reducing risks in GCS operations.\nHowever, data assimilation depends on assumptions regarding reservoir\nproperties, rock physics models, and initial conditions, which, if inaccurate,\ncan compromise prediction reliability. This study demonstrates that augmenting\nforecast ensembles with diverse rock physics models mitigates the impact of\nincorrect assumptions and improves predictive accuracy, particularly in\ndifferentiating uniform versus patchy saturation models."
    },
    {
        "date": "2025-02",
        "title": "Does Training on Synthetic Data Make Models Less Robust?",
        "author": "Lingze Zhang, and Ellie Pavlick",
        "link": "http://arxiv.org/abs/2502.07164v1",
        "abstract": "An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized."
    },
    {
        "date": "2025-02",
        "title": "Towards a Robust Framework for Multimodal Hate Detection: A Study on Video vs. Image-based Content",
        "author": "Girish A. Koushik, Diptesh Kanojia, and Helen Treharne",
        "link": "http://arxiv.org/abs/2502.07138v1",
        "abstract": "Social media platforms enable the propagation of hateful content across\ndifferent modalities such as textual, auditory, and visual, necessitating\neffective detection methods. While recent approaches have shown promise in\nhandling individual modalities, their effectiveness across different modality\ncombinations remains unexplored. This paper presents a systematic analysis of\nfusion-based approaches for multimodal hate detection, focusing on their\nperformance across video and image-based content. Our comprehensive evaluation\nreveals significant modality-specific limitations: while simple embedding\nfusion achieves state-of-the-art performance on video content (HateMM dataset)\nwith a 9.9% points F1-score improvement, it struggles with complex image-text\nrelationships in memes (Hateful Memes dataset). Through detailed ablation\nstudies and error analysis, we demonstrate how current fusion approaches fail\nto capture nuanced cross-modal interactions, particularly in cases involving\nbenign confounders. Our findings provide crucial insights for developing more\nrobust hate detection systems and highlight the need for modality-specific\narchitectural considerations. The code is available at\nhttps://github.com/gak97/Video-vs-Meme-Hate."
    },
    {
        "date": "2025-02",
        "title": "Game of Coding With an Unknown Adversary",
        "author": "Hanzaleh Akbarinodehi, Parsa Moradi, and Mohammad Ali Maddah-Ali",
        "link": "http://arxiv.org/abs/2502.07109v1",
        "abstract": "Motivated by emerging decentralized applications, the \\emph{game of coding}\nframework has been recently introduced to address scenarios where the\nadversary's control over coded symbols surpasses the fundamental limits of\ntraditional coding theory. Still, the reward mechanism available in\ndecentralized systems, motivates the adversary to act rationally. While the\ndecoder, as the data collector (DC), has an acceptance and rejection mechanism,\nfollowed by an estimation module, the adversary aims to maximize its utility,\nas an increasing function of (1) the chance of acceptance (to increase the\nreward), and (2) estimation error. On the other hand, the decoder also adjusts\nits acceptance rule to maximize its own utility, as (1) an increasing function\nof the chance of acceptance (to keep the system functional), (2) decreasing\nfunction of the estimation error. Prior works within this framework rely on the\nassumption that the game is complete, that is, both the DC and the adversary\nare fully aware of each other's utility functions. However, in practice, the\ndecoder is often unaware of the utility of the adversary. To address this\nlimitation, we develop an algorithm enabling the DC to commit to a strategy\nthat achieves within the vicinity of the equilibrium, without knowledge of the\nadversary's utility function. Our approach builds on an observation that at the\nequilibrium, the relationship between the probability of acceptance and the\nmean squared error (MSE) follows a predetermined curve independent of the\nspecific utility functions of the players. By exploiting this invariant\nrelationship, the DC can iteratively refine its strategy based on observable\nparameters, converging to a near-optimal solution. We provide theoretical\nguarantees on sample complexity and accuracy of the proposed scheme."
    },
    {
        "date": "2025-02",
        "title": "LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights",
        "author": "Ze Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, and Jeff Huang",
        "link": "http://arxiv.org/abs/2502.07049v2",
        "abstract": "Large Language Models (LLMs) are emerging as transformative tools for\nsoftware vulnerability detection, addressing critical challenges in the\nsecurity domain. Traditional methods, such as static and dynamic analysis,\noften falter due to inefficiencies, high false positive rates, and the growing\ncomplexity of modern software systems. By leveraging their ability to analyze\ncode structures, identify patterns, and generate repair suggestions, LLMs,\nexemplified by models like GPT, BERT, and CodeBERT, present a novel and\nscalable approach to mitigating vulnerabilities. This paper provides a detailed\nsurvey of LLMs in vulnerability detection. It examines key aspects, including\nmodel architectures, application methods, target languages, fine-tuning\nstrategies, datasets, and evaluation metrics. We also analyze the scope of\ncurrent research problems, highlighting the strengths and weaknesses of\nexisting approaches. Further, we address challenges such as cross-language\nvulnerability detection, multimodal data integration, and repository-level\nanalysis. Based on these findings, we propose solutions for issues like dataset\nscalability, model interpretability, and applications in low-resource\nscenarios. Our contributions are threefold: (1) a systematic review of how LLMs\nare applied in vulnerability detection; (2) an analysis of shared patterns and\ndifferences across studies, with a unified framework for understanding the\nfield; and (3) a summary of key challenges and future research directions. This\nwork provides valuable insights for advancing LLM-based vulnerability\ndetection. We also maintain and regularly update latest selected paper on\nhttps://github.com/OwenSanzas/LLM-For-Vulnerability-Detection"
    },
    {
        "date": "2025-02",
        "title": "AstroLoc: Robust Space to Ground Image Localizer",
        "author": "Gabriele Berton, Alex Stoken, and Carlo Masone",
        "link": "http://arxiv.org/abs/2502.07003v1",
        "abstract": "Astronauts take thousands of photos of Earth per day from the International\nSpace Station, which, once localized on Earth's surface, are used for a\nmultitude of tasks, ranging from climate change research to disaster\nmanagement. The localization process, which has been performed manually for\ndecades, has recently been approached through image retrieval solutions: given\nan astronaut photo, find its most similar match among a large database of\ngeo-tagged satellite images, in a task called Astronaut Photography\nLocalization (APL). Yet, existing APL approaches are trained only using\nsatellite images, without taking advantage of the millions open-source\nastronaut photos. In this work we present the first APL pipeline capable of\nleveraging astronaut photos for training. We first produce full localization\ninformation for 300,000 manually weakly labeled astronaut photos through an\nautomated pipeline, and then use these images to train a model, called\nAstroLoc. AstroLoc learns a robust representation of Earth's surface features\nthrough two losses: astronaut photos paired with their matching satellite\ncounterparts in a pairwise loss, and a second loss on clusters of satellite\nimagery weighted by their relevance to astronaut photography via unsupervised\nmining. We find that AstroLoc achieves a staggering 35% average improvement in\nrecall@1 over previous SOTA, pushing the limits of existing datasets with a\nrecall@100 consistently over 99%. Finally, we note that AstroLoc, without any\nfine-tuning, provides excellent results for related tasks like the\nlost-in-space satellite problem and historical space imagery localization."
    },
    {
        "date": "2025-02",
        "title": "Resurrecting saturated LLM benchmarks with adversarial encoding",
        "author": "Igor Ivanov, and Dmitrii Volkov",
        "link": "http://arxiv.org/abs/2502.06738v1",
        "abstract": "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks."
    },
    {
        "date": "2025-02",
        "title": "Pinning Is Futile: You Need More Than Local Dependency Versioning to Defend against Supply Chain Attacks",
        "author": "Hao He, Bogdan Vasilescu, and Christian K\u00e4stner",
        "link": "http://arxiv.org/abs/2502.06662v1",
        "abstract": "Recent high-profile incidents in open-source software have greatly raised\npractitioner attention on software supply chain attacks. To guard against\npotential malicious package updates, security practitioners advocate pinning\ndependency to specific versions rather than floating in version ranges.\nHowever, it remains controversial whether pinning carries a meaningful security\nbenefit that outweighs the cost of maintaining outdated and possibly vulnerable\ndependencies. In this paper, we quantify, through counterfactual analysis and\nsimulations, the security and maintenance impact of version constraints in the\nnpm ecosystem. By simulating dependency resolutions over historical time\npoints, we find that pinning direct dependencies not only (as expected)\nincreases the cost of maintaining vulnerable and outdated dependencies, but\nalso (surprisingly) even increases the risk of exposure to malicious package\nupdates in larger dependency graphs due to the specifics of npm's dependency\nresolution mechanism. Finally, we explore collective pinning strategies to\nsecure the ecosystem against supply chain attacks, suggesting specific changes\nto npm to enable such interventions. Our study provides guidance for\npractitioners and tool designers to manage their supply chains more securely."
    },
    {
        "date": "2025-02",
        "title": "Automatic ISA analysis for Secure Context Switching",
        "author": "Neelu S. Kalani, Thomas Bourgeat, Guerney D. H. Hunt, and Wojciech Ozga",
        "link": "http://arxiv.org/abs/2502.06609v1",
        "abstract": "Instruction set architectures are complex, with hundreds of registers and\ninstructions that can modify dozens of them during execution, variably on each\ninstance. Prose-style ISA specifications struggle to capture these intricacies\nof the ISAs, where often the important details about a single register are\nspread out across hundreds of pages of documentation. Ensuring that all\nISA-state is swapped in context switch implementations of privileged software\nrequires meticulous examination of these pages. This manual process is tedious\nand error-prone.\n  We propose a tool called Sailor that leverages machine-readable ISA\nspecifications written in Sail to automate this task. Sailor determines the\nISA-state necessary to swap during the context switch using the data collected\nfrom Sail and a novel algorithm to classify ISA-state as security-sensitive.\nUsing Sailor's output, we identify three different classes of mishandled\nISA-state across four open-source confidential computing systems. We further\nreveal five distinct security vulnerabilities that can be exploited using the\nmishandled ISA-state. This research exposes an often overlooked attack surface\nthat stems from mishandled ISA-state, enabling unprivileged adversaries to\nexploit system vulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Robust Scatter Matrix Estimation for Elliptical Distributions in Polynomial Time",
        "author": "Gleb Novikov",
        "link": "http://arxiv.org/abs/2502.06564v1",
        "abstract": "We study the problem of computationally efficient robust estimation of\nscatter matrices of elliptical distributions under the strong contamination\nmodel. We design polynomial time algorithms that achieve dimension-independent\nerror in Frobenius norm.\n  Our first result is a sequence of efficient algorithms that approaches nearly\noptimal error. Specifically, under a mild assumption on the eigenvalues of the\nscatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator\nthat, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$\nsuch that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\,\n\\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$,\nwhere $\\varepsilon$ is the fraction of corruption. We do not require any\nassumptions on the moments of the distribution, while all previously known\ncomputationally efficient algorithms for robust covariance/scatter estimation\nwith dimension-independent error rely on strong assumptions on the moments,\nsuch as sub-Gaussianity or (certifiable) hypercontractivity.\n  Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$\n(that, in particular, is satisfied by all matrices with constant condition\nnumber),\n  we provide a fast (sub-quadratic in the input size) algorithm that, given\nnearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time\n$\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that\n$\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot\n\\sqrt{\\varepsilon})$.\n  Our approach is based on robust covariance estimation of the spatial sign\n(the projection onto the sphere of radius $\\sqrt{d}$) of elliptical\ndistributions."
    },
    {
        "date": "2025-02",
        "title": "Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning",
        "author": "Mario Garc\u00eda-M\u00e1rquez, Nuria Rodr\u00edguez-Barroso, M. Victoria Luz\u00f3n, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2502.06917v1",
        "abstract": "Federated Learning presents a nascent approach to machine learning, enabling\ncollaborative model training across decentralized devices while safeguarding\ndata privacy. However, its distributed nature renders it susceptible to\nadversarial attacks. Integrating blockchain technology with Federated Learning\noffers a promising avenue to enhance security and integrity. In this paper, we\ntackle the potential of blockchain in defending Federated Learning against\nadversarial attacks. First, we test Proof of Federated Learning, a well known\nconsensus mechanism designed ad-hoc to federated contexts, as a defense\nmechanism demonstrating its efficacy against Byzantine and backdoor attacks\nwhen at least one miner remains uncompromised. Second, we propose Krum\nFederated Chain, a novel defense strategy combining Krum and Proof of Federated\nLearning, valid to defend against any configuration of Byzantine or backdoor\nattacks, even when all miners are compromised. Our experiments conducted on\nimage classification datasets validate the effectiveness of our proposed\napproaches."
    },
    {
        "date": "2025-02",
        "title": "An Efficient Security Model for Industrial Internet of Things (IIoT) System Based on Machine Learning Principles",
        "author": "Sahar L. Qaddoori, and Qutaiba I. Ali",
        "link": "http://arxiv.org/abs/2502.06502v1",
        "abstract": "This paper presents a security paradigm for edge devices to defend against\nvarious internal and external threats. The first section of the manuscript\nproposes employing machine learning models to identify MQTT-based (Message\nQueue Telemetry Transport) attacks using the Intrusion Detection and Prevention\nSystem (IDPS) for edge nodes. Because the Machine Learning (ML) model cannot be\ntrained directly on low-performance platforms (such as edge devices),a new\nmethodology for updating ML models is proposed to provide a tradeoff between\nthe model performance and the computational complexity. The proposed\nmethodology involves training the model on a high-performance computing\nplatform and then installing the trained model as a detection engine on\nlow-performance platforms (such as the edge node of the edge layer) to identify\nnew attacks. Multiple security techniques have been employed in the second half\nof the manuscript to verify that the exchanged trained model and the exchanged\ndata files are valid and undiscoverable (information authenticity and privacy)\nand that the source (such as a fog node or edge device) is indeed what it it\nclaimed to be (source authentication and message integrity). Finally, the\nproposed security paradigm is found to be effective against various internal\nand external threats and can be applied to a low-cost single-board computer\n(SBC)."
    },
    {
        "date": "2025-02",
        "title": "Robust Watermarks Leak: Channel-Aware Feature Extraction Enables Adversarial Watermark Manipulation",
        "author": "Zhongjie Ba, Yitao Zhang, Peng Cheng, Bin Gong, Xinyu Zhang, Qinglong Wang, and Kui Ren",
        "link": "http://arxiv.org/abs/2502.06418v1",
        "abstract": "Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design."
    },
    {
        "date": "2025-02",
        "title": "When Data Manipulation Meets Attack Goals: An In-depth Survey of Attacks for VLMs",
        "author": "Aobotao Dai, Xinyu Ma, Lei Chen, Songze Li, and Lin Wang",
        "link": "http://arxiv.org/abs/2502.06390v2",
        "abstract": "Vision-Language Models (VLMs) have gained considerable prominence in recent\nyears due to their remarkable capability to effectively integrate and process\nboth textual and visual information. This integration has significantly\nenhanced performance across a diverse spectrum of applications, such as scene\nperception and robotics. However, the deployment of VLMs has also given rise to\ncritical safety and security concerns, necessitating extensive research to\nassess the potential vulnerabilities these VLM systems may harbor. In this\nwork, we present an in-depth survey of the attack strategies tailored for VLMs.\nWe categorize these attacks based on their underlying objectives - namely\njailbreak, camouflage, and exploitation - while also detailing the various\nmethodologies employed for data manipulation of VLMs. Meanwhile, we outline\ncorresponding defense mechanisms that have been proposed to mitigate these\nvulnerabilities. By discerning key connections and distinctions among the\ndiverse types of attacks, we propose a compelling taxonomy for VLM attacks.\nMoreover, we summarize the evaluation metrics that comprehensively describe the\ncharacteristics and impact of different attacks on VLMs. Finally, we conclude\nwith a discussion of promising future research directions that could further\nenhance the robustness and safety of VLMs, emphasizing the importance of\nongoing exploration in this critical area of study. To facilitate community\nengagement, we maintain an up-to-date project page, accessible at:\nhttps://github.com/AobtDai/VLM_Attack_Paper_List."
    },
    {
        "date": "2025-02",
        "title": "Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection",
        "author": "Dongsu Song, Daehwa Ko, and Jay Hoon Jung",
        "link": "http://arxiv.org/abs/2502.07821v1",
        "abstract": "It is well known that query-based attacks tend to have relatively higher\nsuccess rates in adversarial black-box attacks. While research on black-box\nattacks is actively being conducted, relatively few studies have focused on\npixel attacks that target only a limited number of pixels. In image\nclassification, query-based pixel attacks often rely on patches, which heavily\ndepend on randomness and neglect the fact that scattered pixels are more\nsuitable for adversarial attacks. Moreover, to the best of our knowledge,\nquery-based pixel attacks have not been explored in the field of object\ndetection. To address these issues, we propose a novel pixel-based black-box\nattack called Remember and Forget Pixel Attack using Reinforcement\nLearning(RFPAR), consisting of two main components: the Remember and Forget\nprocesses. RFPAR mitigates randomness and avoids patch dependency by leveraging\nrewards generated through a one-step RL algorithm to perturb pixels. RFPAR\neffectively creates perturbed images that minimize the confidence scores while\nadhering to limited pixel constraints. Furthermore, we advance our proposed\nattack beyond image classification to object detection, where RFPAR reduces the\nconfidence scores of detected objects to avoid detection. Experiments on the\nImageNet-1K dataset for classification show that RFPAR outperformed\nstate-of-the-art query-based pixel attacks. For object detection, using the\nMSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction\nto state-of-the-art query-based attack while requiring fewer query. Further\nexperiments on the Argoverse dataset using YOLOv8 confirm that RFPAR\neffectively removed objects on a larger scale dataset. Our code is available at\nhttps://github.com/KAU-QuantumAILab/RFPAR."
    },
    {
        "date": "2025-02",
        "title": "Hyperparameters in Score-Based Membership Inference Attacks",
        "author": "Gauri Pradhan, Joonas J\u00e4lk\u00f6, Marlon Tobaben, and Antti Honkela",
        "link": "http://arxiv.org/abs/2502.06374v1",
        "abstract": "Membership Inference Attacks (MIAs) have emerged as a valuable framework for\nevaluating privacy leakage by machine learning models. Score-based MIAs are\ndistinguished, in particular, by their ability to exploit the confidence scores\nthat the model generates for particular inputs. Existing score-based MIAs\nimplicitly assume that the adversary has access to the target model's\nhyperparameters, which can be used to train the shadow models for the attack.\nIn this work, we demonstrate that the knowledge of target hyperparameters is\nnot a prerequisite for MIA in the transfer learning setting. Based on this, we\npropose a novel approach to select the hyperparameters for training the shadow\nmodels for MIA when the attacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We demonstrate that using\nthe new approach yields hyperparameters that lead to an attack near\nindistinguishable in performance from an attack that uses target\nhyperparameters to train the shadow models. Furthermore, we study the empirical\nprivacy risk of unaccounted use of training data for hyperparameter\noptimization (HPO) in differentially private (DP) transfer learning. We find no\nstatistically significant evidence that performing HPO using training data\nwould increase vulnerability to MIA."
    },
    {
        "date": "2025-02",
        "title": "Accelerating Outlier-robust Rotation Estimation by Stereographic Projection",
        "author": "Taosi Xu, Yinlong Liu, Xianbo Wang, and Zhi-Xin Yang",
        "link": "http://arxiv.org/abs/2502.06337v1",
        "abstract": "Rotation estimation plays a fundamental role in many computer vision and\nrobot tasks. However, efficiently estimating rotation in large inputs\ncontaining numerous outliers (i.e., mismatches) and noise is a recognized\nchallenge. Many robust rotation estimation methods have been designed to\naddress this challenge. Unfortunately, existing methods are often inapplicable\ndue to their long computation time and the risk of local optima. In this paper,\nwe propose an efficient and robust rotation estimation method. Specifically,\nour method first investigates geometric constraints involving only the rotation\naxis. Then, it uses stereographic projection and spatial voting techniques to\nidentify the rotation axis and angle. Furthermore, our method efficiently\nobtains the optimal rotation estimation and can estimate multiple rotations\nsimultaneously. To verify the feasibility of our method, we conduct comparative\nexperiments using both synthetic and real-world data. The results show that,\nwith GPU assistance, our method can solve large-scale ($10^6$ points) and\nseverely corrupted (90\\% outlier rate) rotation estimation problems within 0.07\nseconds, with an angular error of only 0.01 degrees, which is superior to\nexisting methods in terms of accuracy and efficiency."
    },
    {
        "date": "2025-02",
        "title": "Dynamic Pricing with Adversarially-Censored Demands",
        "author": "Jianyu Xu, Yining Wang, Xi Chen, and Yu-Xiang Wang",
        "link": "http://arxiv.org/abs/2502.06168v1",
        "abstract": "We study an online dynamic pricing problem where the potential demand at each\ntime period $t=1,2,\\ldots, T$ is stochastic and dependent on the price.\nHowever, a perishable inventory is imposed at the beginning of each time $t$,\ncensoring the potential demand if it exceeds the inventory level. To address\nthis problem, we introduce a pricing algorithm based on the optimistic\nestimates of derivatives. We show that our algorithm achieves\n$\\tilde{O}(\\sqrt{T})$ optimal regret even with adversarial inventory series.\nOur findings advance the state-of-the-art in online decision-making problems\nwith censored feedback, offering a theoretically optimal solution against\nadversarial observations."
    },
    {
        "date": "2025-02",
        "title": "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in IoT Environment",
        "author": "A. Karthick kumar, S. Rathnamala, T. Vijayashanthi, M. Prabhananthakumar, Alavikunhu Panthakkan, Shadi Atalla, and Wathiq Mansoor",
        "link": "http://arxiv.org/abs/2502.06138v1",
        "abstract": "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks."
    },
    {
        "date": "2025-02",
        "title": "Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models",
        "author": "Marc Bruni, Fabio Gabrielli, Mohammad Ghafari, and Martin Kropp",
        "link": "http://arxiv.org/abs/2502.06039v1",
        "abstract": "Prompt engineering reduces reasoning mistakes in Large Language Models\n(LLMs). However, its effectiveness in mitigating vulnerabilities in\nLLM-generated code remains underexplored. To address this gap, we implemented a\nbenchmark to automatically assess the impact of various prompt engineering\nstrategies on code security. Our benchmark leverages two peer-reviewed prompt\ndatasets and employs static scanners to evaluate code security at scale. We\ntested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and\nGPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a\nsecurity-focused prompt prefix can reduce the occurrence of security\nvulnerabilities by up to 56%. Additionally, all tested models demonstrated the\nability to detect and repair between 41.9% and 68.7% of vulnerabilities in\npreviously generated code when using iterative prompting techniques. Finally,\nwe introduce a \"prompt agent\" that demonstrates how the most effective\ntechniques can be applied in real-world development workflows."
    },
    {
        "date": "2025-02",
        "title": "A Conditional Tabular GAN-Enhanced Intrusion Detection System for Rare Attacks in IoT Networks",
        "author": "Safaa Menssouri, and El Mehdi Amhoud",
        "link": "http://arxiv.org/abs/2502.06031v1",
        "abstract": "Internet of things (IoT) networks, boosted by 6G technology, are transforming\nvarious industries. However, their widespread adoption introduces significant\nsecurity risks, particularly in detecting rare but potentially damaging\ncyber-attacks. This makes the development of robust IDS crucial for monitoring\nnetwork traffic and ensuring their safety. Traditional IDS often struggle with\ndetecting rare attacks due to severe class imbalances in IoT data. In this\npaper, we propose a novel two-stage system called conditional tabular\ngenerative synthetic minority data generation with deep neural network\n(CTGSM-DNN). In the first stage, a conditional tabular generative adversarial\nnetwork (CTGAN) is employed to generate synthetic data for rare attack classes.\nIn the second stage, the SMOTEENN method is applied to improve dataset quality.\nThe full study was conducted using the CSE-CIC-IDS2018 dataset, and we assessed\nthe performance of the proposed IDS using different evaluation metrics. The\nexperimental results demonstrated the effectiveness of the proposed multiclass\nclassifier, achieving an overall accuracy of 99.90% and 80% accuracy in\ndetecting rare attacks."
    },
    {
        "date": "2025-02",
        "title": "Crypto Miner Attack: GPU Remote Code Execution Attacks",
        "author": "Ariel Szabo, and Uzy Hadad",
        "link": "http://arxiv.org/abs/2502.10439v1",
        "abstract": "Remote Code Execution (RCE) exploits pose a significant threat to AI and ML\nsystems, particularly in GPU-accelerated environments where the computational\npower of GPUs can be misused for malicious purposes. This paper focuses on RCE\nattacks leveraging deserialization vulnerabilities and custom layers, such as\nTensorFlow Lambda layers, which are often overlooked due to the complexity of\nmonitoring GPU workloads. These vulnerabilities enable attackers to execute\narbitrary code, blending malicious activity seamlessly into expected model\nbehavior and exploiting GPUs for unauthorized tasks such as cryptocurrency\nmining. Unlike traditional CPU-based attacks, the parallel processing nature of\nGPUs and their high resource utilization make runtime detection exceptionally\nchallenging. In this work, we provide a comprehensive examination of RCE\nexploits targeting GPUs, demonstrating an attack that utilizes these\nvulnerabilities to deploy a crypto miner on a GPU. We highlight the technical\nintricacies of such attacks, emphasize their potential for significant\nfinancial and computational costs, and propose strategies for mitigation. By\nshedding light on this underexplored attack vector, we aim to raise awareness\nand encourage the adoption of robust security measures in GPU-driven AI and ML\nsystems, with an emphasis on static and model scanning as an easier way to\ndetect exploits."
    },
    {
        "date": "2025-02",
        "title": "The AI Security Zugzwang",
        "author": "Lampis Alevizos",
        "link": "http://arxiv.org/abs/2502.06000v1",
        "abstract": "In chess, zugzwang describes a scenario where any move worsens the player's\nposition. Organizations face a similar dilemma right now at the intersection of\nartificial intelligence (AI) and cybersecurity. AI adoption creates an\ninevitable paradox: delaying it poses strategic risks, rushing it introduces\npoorly understood vulnerabilities, and even incremental adoption leads to\ncascading complexities. In this work we formalize this challenge as the AI\nSecurity Zugzwang, a phenomenon where security leaders must make decisions\nunder conditions of inevitable risk. Grounded in game theory, security\neconomics, and organizational decision theory, we characterize AI security\nzugzwang through three key properties, the forced movement, predictable\nvulnerability creation, and temporal pressure. Additionally, we develop a\ntaxonomy to categorize forced-move scenarios across AI adoption,\nimplementation, operational and governance contexts and provide corresponding\nstrategic mitigations. Our framework is supported by a practical decision\nflowchart, demonstrated through a real-world example of Copilot adoption, thus,\nshowing how security lead"
    },
    {
        "date": "2025-02",
        "title": "Detection of Physiological Data Tampering Attacks with Quantum Machine Learning",
        "author": "Md. Saif Hassan Onim, and Himanshu Thapliyal",
        "link": "http://arxiv.org/abs/2502.05966v1",
        "abstract": "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases."
    },
    {
        "date": "2025-02",
        "title": "Cyri: A Conversational AI-based Assistant for Supporting the Human User in Detecting and Responding to Phishing Attacks",
        "author": "Antonio La Torre, and Marco Angelini",
        "link": "http://arxiv.org/abs/2502.05951v1",
        "abstract": "This work introduces Cyri, an AI-powered conversational assistant designed to\nsupport a human user in detecting and analyzing phishing emails by leveraging\nLarge Language Models. Cyri has been designed to scrutinize emails for semantic\nfeatures used in phishing attacks, such as urgency, and undesirable\nconsequences, using an approach that unifies features already established in\nthe literature with others by Cyri features extraction methodology. Cyri can be\ndirectly plugged into a client mail or webmail, ensuring seamless integration\nwith the user's email workflow while maintaining data privacy through local\nprocessing. By performing analyses on the user's machine, Cyri eliminates the\nneed to transmit sensitive email data over the internet, reducing associated\nsecurity risks. The Cyri user interface has been designed to reduce habituation\neffects and enhance user engagement. It employs dynamic visual cues and\ncontext-specific explanations to keep users alert and informed while using\nemails. Additionally, it allows users to explore identified malicious semantic\nfeatures both through conversation with the agent and visual exploration,\nobtaining the advantages of both modalities for expert or non-expert users. It\nalso allows users to keep track of the conversation, supports the user in\nsolving additional questions on both computed features or new parts of the\nmail, and applies its detection on demand. To evaluate Cyri, we crafted a\ncomprehensive dataset of 420 phishing emails and 420 legitimate emails. Results\ndemonstrate high effectiveness in identifying critical phishing semantic\nfeatures fundamental to phishing detection. A user study involving 10\nparticipants, both experts and non-experts, evaluated Cyri's effectiveness and\nusability. Results indicated that Cyri significantly aided users in identifying\nphishing emails and enhanced their understanding of phishing tactics."
    },
    {
        "date": "2025-02",
        "title": "Sign-Symmetry Learning Rules are Robust Fine-Tuners",
        "author": "Aymene Berriche, Mehdi Zakaria Adjal, and Riyadh Baghdadi",
        "link": "http://arxiv.org/abs/2502.05925v1",
        "abstract": "Backpropagation (BP) has long been the predominant method for training neural\nnetworks due to its effectiveness. However, numerous alternative approaches,\nbroadly categorized under feedback alignment, have been proposed, many of which\nare motivated by the search for biologically plausible learning mechanisms.\nDespite their theoretical appeal, these methods have consistently\nunderperformed compared to BP, leading to a decline in research interest. In\nthis work, we revisit the role of such methods and explore how they can be\nintegrated into standard neural network training pipelines. Specifically, we\npropose fine-tuning BP-pre-trained models using Sign-Symmetry learning rules\nand demonstrate that this approach not only maintains performance parity with\nBP but also enhances robustness. Through extensive experiments across multiple\ntasks and benchmarks, we establish the validity of our approach. Our findings\nintroduce a novel perspective on neural network training and open new research\ndirections for leveraging biologically inspired learning rules in deep\nlearning."
    },
    {
        "date": "2025-02",
        "title": "Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks",
        "author": "Bowei He, Lihao Yin, Hui-Ling Zhen, Jianping Zhang, Lanqing Hong, Mingxuan Yuan, and Chen Ma",
        "link": "http://arxiv.org/abs/2502.06892v1",
        "abstract": "The widespread deployment of pre-trained language models (PLMs) has exposed\nthem to textual backdoor attacks, particularly those planted during the\npre-training stage. These attacks pose significant risks to high-reliability\napplications, as they can stealthily affect multiple downstream tasks. While\ncertifying robustness against such threats is crucial, existing defenses\nstruggle with the high-dimensional, interdependent nature of textual data and\nthe lack of access to original poisoned pre-training data. To address these\nchallenges, we introduce \\textbf{F}uzzed \\textbf{R}andomized \\textbf{S}moothing\n(\\textbf{FRS}), a novel approach for efficiently certifying language model\nrobustness against backdoor attacks. FRS integrates software robustness\ncertification techniques with biphased model parameter smoothing, employing\nMonte Carlo tree search for proactive fuzzing to identify vulnerable textual\nsegments within the Damerau-Levenshtein space. This allows for targeted and\nefficient text randomization, while eliminating the need for access to poisoned\ntraining data during model smoothing. Our theoretical analysis demonstrates\nthat FRS achieves a broader certified robustness radius compared to existing\nmethods. Extensive experiments across various datasets, model configurations,\nand attack strategies validate FRS's superiority in terms of defense\nefficiency, accuracy, and robustness."
    },
    {
        "date": "2025-02",
        "title": "Secure Visual Data Processing via Federated Learning",
        "author": "Pedro Santos, T\u00e2nia Carvalho, Filipe Magalh\u00e3es, and Lu\u00eds Antunes",
        "link": "http://arxiv.org/abs/2502.06889v1",
        "abstract": "As the demand for privacy in visual data management grows, safeguarding\nsensitive information has become a critical challenge. This paper addresses the\nneed for privacy-preserving solutions in large-scale visual data processing by\nleveraging federated learning. Although there have been developments in this\nfield, previous research has mainly focused on integrating object detection\nwith either anonymization or federated learning. However, these pairs often\nfail to address complex privacy concerns. On the one hand, object detection\nwith anonymization alone can be vulnerable to reverse techniques. On the other\nhand, federated learning may not provide sufficient privacy guarantees.\nTherefore, we propose a new approach that combines object detection, federated\nlearning and anonymization. Combining these three components aims to offer a\nrobust privacy protection strategy by addressing different vulnerabilities in\nvisual data. Our solution is evaluated against traditional centralized models,\nshowing that while there is a slight trade-off in accuracy, the privacy\nbenefits are substantial, making it well-suited for privacy sensitive\napplications."
    },
    {
        "date": "2025-02",
        "title": "RAMer: Reconstruction-based Adversarial Model for Multi-party Multi-modal Multi-label Emotion Recognition",
        "author": "Xudong Yang, Yizhang Zhu, Nan Tang, and Yuyu Luo",
        "link": "http://arxiv.org/abs/2502.10435v1",
        "abstract": "Conventional multi-modal multi-label emotion recognition (MMER) from videos\ntypically assumes full availability of visual, textual, and acoustic\nmodalities. However, real-world multi-party settings often violate this\nassumption, as non-speakers frequently lack acoustic and textual inputs,\nleading to a significant degradation in model performance. Existing approaches\nalso tend to unify heterogeneous modalities into a single representation,\noverlooking each modality's unique characteristics. To address these\nchallenges, we propose RAMer (Reconstruction-based Adversarial Model for\nEmotion Recognition), which leverages adversarial learning to refine\nmulti-modal representations by exploring both modality commonality and\nspecificity through reconstructed features enhanced by contrastive learning.\nRAMer also introduces a personality auxiliary task to complement missing\nmodalities using modality-level attention, improving emotion reasoning. To\nfurther strengthen the model's ability to capture label and modality\ninterdependency, we propose a stack shuffle strategy to enrich correlations\nbetween labels and modality-specific features. Experiments on three benchmarks,\ni.e., MEmoR, CMU-MOSEI, and $M^3$ED, demonstrate that RAMer achieves\nstate-of-the-art performance in dyadic and multi-party MMER scenarios."
    },
    {
        "date": "2025-02",
        "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
        "author": "Danny Wang, Ruihong Qiu, Guangdong Bai, and Zi Huang",
        "link": "http://arxiv.org/abs/2502.05780v1",
        "abstract": "Despite graph neural networks' (GNNs) great success in modelling\ngraph-structured data, out-of-distribution (OOD) test instances still pose a\ngreat challenge for current GNNs. One of the most effective techniques to\ndetect OOD nodes is to expose the detector model with an additional OOD\nnode-set, yet the extra OOD instances are often difficult to obtain in\npractice. Recent methods for image data address this problem using OOD data\nsynthesis, typically relying on pre-trained generative models like Stable\nDiffusion. However, these approaches require vast amounts of additional data,\nas well as one-for-all pre-trained generative models, which are not available\nfor graph data. Therefore, we propose the GOLD framework for graph OOD\ndetection, an implicit adversarial learning pipeline with synthetic OOD\nexposure without pre-trained models. The implicit adversarial training process\nemploys a novel alternating optimisation framework by training: (1) a latent\ngenerative model to regularly imitate the in-distribution (ID) embeddings from\nan evolving GNN, and (2) a GNN encoder and an OOD detector to accurately\nclassify ID data while increasing the energy divergence between the ID\nembeddings and the generative model's synthetic embeddings. This novel approach\nimplicitly transforms the synthetic embeddings into pseudo-OOD instances\nrelative to the ID data, effectively simulating exposure to OOD scenarios\nwithout auxiliary data. Extensive OOD detection experiments are conducted on\nfive benchmark graph datasets, verifying the superior performance of GOLD\nwithout using real OOD data compared with the state-of-the-art OOD exposure and\nnon-exposure baselines."
    },
    {
        "date": "2025-02",
        "title": "Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails",
        "author": "Yijun Yang, Lichao Wang, Xiao Yang, Lanqing Hong, and Jun Zhu",
        "link": "http://arxiv.org/abs/2502.05772v1",
        "abstract": "Vision Large Language Models (VLLMs) integrate visual data processing,\nexpanding their real-world applications, but also increasing the risk of\ngenerating unsafe responses. In response, leading companies have implemented\nMulti-Layered safety defenses, including alignment training, safety system\nprompts, and content moderation. However, their effectiveness against\nsophisticated adversarial attacks remains largely unexplored. In this paper, we\npropose MultiFaceted Attack, a novel attack framework designed to\nsystematically bypass Multi-Layered Defenses in VLLMs. It comprises three\ncomplementary attack facets: Visual Attack that exploits the multimodal nature\nof VLLMs to inject toxic system prompts through images; Alignment Breaking\nAttack that manipulates the model's alignment mechanism to prioritize the\ngeneration of contrasting responses; and Adversarial Signature that deceives\ncontent moderators by strategically placing misleading information at the end\nof the response. Extensive evaluations on eight commercial VLLMs in a black-box\nsetting demonstrate that MultiFaceted Attack achieves a 61.56% attack success\nrate, surpassing state-of-the-art methods by at least 42.18%."
    },
    {
        "date": "2025-02",
        "title": "Filter, Obstruct and Dilute: Defending Against Backdoor Attacks on Semi-Supervised Learning",
        "author": "Xinrui Wang, Chuanxing Geng, Wenhai Wan, Shao-yuan Li, and Songcan Chen",
        "link": "http://arxiv.org/abs/2502.05755v1",
        "abstract": "Recent studies have verified that semi-supervised learning (SSL) is\nvulnerable to data poisoning backdoor attacks. Even a tiny fraction of\ncontaminated training data is sufficient for adversaries to manipulate up to\n90\\% of the test outputs in existing SSL methods. Given the emerging threat of\nbackdoor attacks designed for SSL, this work aims to protect SSL against such\nrisks, marking it as one of the few known efforts in this area. Specifically,\nwe begin by identifying that the spurious correlations between the backdoor\ntriggers and the target class implanted by adversaries are the primary cause of\nmanipulated model predictions during the test phase. To disrupt these\ncorrelations, we utilize three key techniques: Gaussian Filter, complementary\nlearning and trigger mix-up, which collectively filter, obstruct and dilute the\ninfluence of backdoor attacks in both data pre-processing and feature learning.\nExperimental results demonstrate that our proposed method, Backdoor Invalidator\n(BI), significantly reduces the average attack success rate from 84.7\\% to\n1.8\\% across different state-of-the-art backdoor attacks. It is also worth\nmentioning that BI does not sacrifice accuracy on clean data and is supported\nby a theoretical guarantee of its generalization capability."
    },
    {
        "date": "2025-02",
        "title": "Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power System Optimizers",
        "author": "Nora Agah, Meiyi Li, and Javad Mohammadi",
        "link": "http://arxiv.org/abs/2502.05727v1",
        "abstract": "The increased integration of clean yet stochastic energy resources and the\ngrowing number of extreme weather events are narrowing the decision-making\nwindow of power grid operators. This time constraint is fueling a plethora of\nresearch on Machine Learning-, or ML-, based optimization proxies. While\nfinding a fast solution is appealing, the inherent vulnerabilities of the\nlearning-based methods are hindering their adoption. One of these\nvulnerabilities is data poisoning attacks, which adds perturbations to ML\ntraining data, leading to incorrect decisions. The impact of poisoning attacks\non learning-based power system optimizers have not been thoroughly studied,\nwhich creates a critical vulnerability. In this paper, we examine the impact of\ndata poisoning attacks on ML-based optimization proxies that are used to solve\nthe DC Optimal Power Flow problem. Specifically, we compare the resilience of\nthree different methods-a penalty-based method, a post-repair approach, and a\ndirect mapping approach-against the adverse effects of poisoning attacks. We\nwill use the optimality and feasibility of these proxies as performance\nmetrics. The insights of this work will establish a foundation for enhancing\nthe resilience of neural power system optimizers."
    },
    {
        "date": "2025-02",
        "title": "Mobile Application Threats and Security",
        "author": "Timur Mirzoev, Mark Miller, Shamimara Lasker, and Michael Brannon",
        "link": "http://arxiv.org/abs/2502.05685v1",
        "abstract": "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats."
    },
    {
        "date": "2025-02",
        "title": "Rigid Body Adversarial Attacks",
        "author": "Aravind Ramakrishnan, David I. W. Levin, and Alec Jacobson",
        "link": "http://arxiv.org/abs/2502.05669v1",
        "abstract": "Due to their performance and simplicity, rigid body simulators are often used\nin applications where the objects of interest can considered very stiff.\nHowever, no material has infinite stiffness, which means there are potentially\ncases where the non-zero compliance of the seemingly rigid object can cause a\nsignificant difference between its trajectories when simulated in a rigid body\nor deformable simulator.\n  Similarly to how adversarial attacks are developed against image classifiers,\nwe propose an adversarial attack against rigid body simulators. In this\nadversarial attack, we solve an optimization problem to construct perceptually\nrigid adversarial objects that have the same collision geometry and moments of\nmass to a reference object, so that they behave identically in rigid body\nsimulations but maximally different in more accurate deformable simulations. We\ndemonstrate the validity of our method by comparing simulations of several\nexamples in commercially available simulators."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Machine Learning: Attacks, Defenses, and Open Challenges",
        "author": "Pranav K Jha",
        "link": "http://arxiv.org/abs/2502.05637v1",
        "abstract": "Adversarial Machine Learning (AML) addresses vulnerabilities in AI systems\nwhere adversaries manipulate inputs or training data to degrade performance.\nThis article provides a comprehensive analysis of evasion and poisoning\nattacks, formalizes defense mechanisms with mathematical rigor, and discusses\nthe challenges of implementing robust solutions in adaptive threat models.\nAdditionally, it highlights open challenges in certified robustness,\nscalability, and real-world deployment."
    },
    {
        "date": "2025-02",
        "title": "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning",
        "author": "Runhua Xu, Shiqi Gao, Chao Li, James Joshi, and Jianxin Li",
        "link": "http://arxiv.org/abs/2502.05547v1",
        "abstract": "Federated learning (FL) is inherently susceptible to privacy breaches and\npoisoning attacks. To tackle these challenges, researchers have separately\ndevised secure aggregation mechanisms to protect data privacy and robust\naggregation methods that withstand poisoning attacks. However, simultaneously\naddressing both concerns is challenging; secure aggregation facilitates\npoisoning attacks as most anomaly detection techniques require access to\nunencrypted local model updates, which are obscured by secure aggregation. Few\nrecent efforts to simultaneously tackle both challenges offen depend on\nimpractical assumption of non-colluding two-server setups that disrupt FL's\ntopology, or three-party computation which introduces scalability issues,\ncomplicating deployment and application. To overcome this dilemma, this paper\nintroduce a Dual Defense Federated learning (DDFed) framework. DDFed\nsimultaneously boosts privacy protection and mitigates poisoning attacks,\nwithout introducing new participant roles or disrupting the existing FL\ntopology. DDFed initially leverages cutting-edge fully homomorphic encryption\n(FHE) to securely aggregate model updates, without the impractical requirement\nfor non-colluding two-server setups and ensures strong privacy protection.\nAdditionally, we proposes a unique two-phase anomaly detection mechanism for\nencrypted model updates, featuring secure similarity computation and\nfeedback-driven collaborative selection, with additional measures to prevent\npotential privacy breaches from Byzantine clients incorporated into the\ndetection process. We conducted extensive experiments on various model\npoisoning attacks and FL scenarios, including both cross-device and cross-silo\nFL. Experiments on publicly available datasets demonstrate that DDFed\nsuccessfully protects model privacy and effectively defends against model\npoisoning threats."
    },
    {
        "date": "2025-02",
        "title": "Democratic Training Against Universal Adversarial Perturbations",
        "author": "Bing Sun, Jun Sun, and Wei Zhao",
        "link": "http://arxiv.org/abs/2502.05542v1",
        "abstract": "Despite their advances and success, real-world deep neural networks are known\nto be vulnerable to adversarial attacks. Universal adversarial perturbation, an\ninput-agnostic attack, poses a serious threat for them to be deployed in\nsecurity-sensitive systems. In this case, a single universal adversarial\nperturbation deceives the model on a range of clean inputs without requiring\ninput-specific optimization, which makes it particularly threatening. In this\nwork, we observe that universal adversarial perturbations usually lead to\nabnormal entropy spectrum in hidden layers, which suggests that the prediction\nis dominated by a small number of ``feature'' in such cases (rather than\ndemocratically by many features). Inspired by this, we propose an efficient yet\neffective defense method for mitigating UAPs called \\emph{Democratic Training}\nby performing entropy-based model enhancement to suppress the effect of the\nuniversal adversarial perturbations in a given model. \\emph{Democratic\nTraining} is evaluated with 7 neural networks trained on 5 benchmark datasets\nand 5 types of state-of-the-art universal adversarial attack methods. The\nresults show that it effectively reduces the attack success rate, improves\nmodel robustness and preserves the model accuracy on clean samples."
    },
    {
        "date": "2025-02",
        "title": "Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks",
        "author": "Hamed Poursiami, Ayana Moshruba, and Maryam Parsa",
        "link": "http://arxiv.org/abs/2502.05509v1",
        "abstract": "As machine learning models become integral to security-sensitive\napplications, concerns over data leakage from adversarial attacks continue to\nrise. Model Inversion (MI) attacks pose a significant privacy threat by\nenabling adversaries to reconstruct training data from model outputs. While MI\nattacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking\nNeural Networks (SNNs) remain largely unexplored in this context. Due to their\nevent-driven and discrete computations, SNNs introduce fundamental differences\nin information processing that may offer inherent resistance to such attacks. A\ncritical yet underexplored aspect of this threat lies in black-box settings,\nwhere attackers operate through queries without direct access to model\nparameters or gradients-representing a more realistic adversarial scenario in\ndeployed systems. This work presents the first study of black-box MI attacks on\nSNNs. We adapt a generative adversarial MI framework to the spiking domain by\nincorporating rate-based encoding for input transformation and decoding\nmechanisms for output interpretation. Our results show that SNNs exhibit\nsignificantly greater resistance to MI attacks than ANNs, as demonstrated by\ndegraded reconstructions, increased instability in attack convergence, and\noverall reduced attack effectiveness across multiple evaluation metrics.\nFurther analysis suggests that the discrete and temporally distributed nature\nof SNN decision boundaries disrupts surrogate modeling, limiting the attacker's\nability to approximate the target model."
    },
    {
        "date": "2025-02",
        "title": "Gen-DFL: Decision-Focused Generative Learning for Robust Decision Making",
        "author": "Prince Zizhuang Wang, Jinhao Liang, Shuyi Chen, Ferdinando Fioretto, and Shixiang Zhu",
        "link": "http://arxiv.org/abs/2502.05468v1",
        "abstract": "Decision-focused learning (DFL) integrates predictive models with downstream\noptimization, directly training machine learning models to minimize decision\nerrors. While DFL has been shown to provide substantial advantages when\ncompared to a counterpart that treats the predictive and prescriptive models\nseparately, it has also been shown to struggle in high-dimensional and\nrisk-sensitive settings, limiting its applicability in real-world settings. To\naddress this limitation, this paper introduces decision-focused generative\nlearning (Gen-DFL), a novel framework that leverages generative models to\nadaptively model uncertainty and improve decision quality. Instead of relying\non fixed uncertainty sets, Gen-DFL learns a structured representation of the\noptimization parameters and samples from the tail regions of the learned\ndistribution to enhance robustness against worst-case scenarios. This approach\nmitigates over-conservatism while capturing complex dependencies in the\nparameter space. The paper shows, theoretically, that Gen-DFL achieves improved\nworst-case performance bounds compared to traditional DFL. Empirically, it\nevaluates Gen-DFL on various scheduling and logistics problems, demonstrating\nits strong performance against existing DFL methods."
    },
    {
        "date": "2025-02",
        "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code Conflicts",
        "author": "Seonghun Son, Daniel Moghimi, and Berk Gulmezoglu",
        "link": "http://arxiv.org/abs/2502.05429v1",
        "abstract": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
    },
    {
        "date": "2025-02",
        "title": "BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks",
        "author": "Wen Zhou, Shuichiro Miwa, Yang Liu, and Koji Okamoto",
        "link": "http://arxiv.org/abs/2502.06863v1",
        "abstract": "A generative AI architecture called bubbly flow generative adversarial\nnetworks (BF-GAN) is developed, designed to generate realistic and high-quality\nbubbly flow images through physically conditioned inputs, jg and jf. Initially,\n52 sets of bubbly flow experiments under varying conditions are conducted to\ncollect 140,000 bubbly flow images with physical labels of jg and jf for\ntraining data. A multi-scale loss function is then developed, incorporating\nmismatch loss and pixel loss to enhance the generative performance of BF-GAN\nfurther. Regarding evaluative metrics of generative AI, the BF-GAN has\nsurpassed conventional GAN. Physically, key parameters of bubbly flow generated\nby BF-GAN are extracted and compared with measurement values and empirical\ncorrelations, validating BF-GAN's generative performance. The comparative\nanalysis demonstrate that the BF-GAN can generate realistic and high-quality\nbubbly flow images with any given jg and jf within the research scope.\n  BF-GAN offers a generative AI solution for two-phase flow research,\nsubstantially lowering the time and cost required to obtain high-quality data.\nIn addition, it can function as a benchmark dataset generator for bubbly flow\ndetection and segmentation algorithms, enhancing overall productivity in this\nresearch domain. The BF-GAN model is available online\n(https://github.com/zhouzhouwen/BF-GAN)."
    },
    {
        "date": "2025-02",
        "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
        "author": "Chongyu Fan, Jinghan Jia, Yihua Zhang, Anil Ramakrishna, Mingyi Hong, and Sijia Liu",
        "link": "http://arxiv.org/abs/2502.05374v1",
        "abstract": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth."
    },
    {
        "date": "2025-02",
        "title": "Removing Neural Signal Artifacts with Autoencoder-Targeted Adversarial Transformers (AT-AT)",
        "author": "Benjamin J. Choi",
        "link": "http://arxiv.org/abs/2502.05332v1",
        "abstract": "Electromyogenic (EMG) noise is a major contamination source in EEG data that\ncan impede accurate analysis of brain-specific neural activity. Recent\nliterature on EMG artifact removal has moved beyond traditional linear\nalgorithms in favor of machine learning-based systems. However, existing deep\nlearning-based filtration methods often have large compute footprints and\nprohibitively long training times. In this study, we present a new machine\nlearning-based system for filtering EMG interference from EEG data using an\nautoencoder-targeted adversarial transformer (AT-AT). By leveraging the\nlightweight expressivity of an autoencoder to determine optimal time-series\ntransformer application sites, our AT-AT architecture achieves a >90% model\nsize reduction compared to published artifact removal models. The addition of\nadversarial training ensures that filtered signals adhere to the fundamental\ncharacteristics of EEG data. We trained AT-AT using published neural data from\n67 subjects and found that the system was able to achieve comparable test\nperformance to larger models; AT-AT posted a mean reconstructive correlation\ncoefficient above 0.95 at an initial signal-to-noise ratio (SNR) of 2 dB and\n0.70 at -7 dB SNR. Further research generalizing these results to broader\nsample sizes beyond these isolated test cases will be crucial; while outside\nthe scope of this study, we also include results from a real-world deployment\nof AT-AT in the Appendix."
    },
    {
        "date": "2025-02",
        "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
        "author": "Awa Khouna, Julien Ferry, and Thibaut Vidal",
        "link": "http://arxiv.org/abs/2502.05325v1",
        "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the\ntrade-off between model explainability and security. In particular,\nexplainability techniques, such as counterfactual explanations, inadvertently\nincrease the risk of model extraction attacks, enabling unauthorized\nreplication of proprietary models. In this paper, we formalize and characterize\nthe risks and inherent complexity of model reconstruction, focusing on the\n\"oracle'' queries required for faithfully inferring the underlying prediction\nfunction. We present the first formal analysis of model extraction attacks\nthrough the lens of competitive analysis, establishing a foundational framework\nto evaluate their efficiency. Focusing on models based on additive decision\ntrees (e.g., decision trees, gradient boosting, and random forests), we\nintroduce novel reconstruction algorithms that achieve provably perfect\nfidelity while demonstrating strong anytime performance. Our framework provides\ntheoretical bounds on the query complexity for extracting tree-based model,\noffering new insights into the security vulnerabilities of their deployment."
    },
    {
        "date": "2025-02",
        "title": "MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison",
        "author": "Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, and William Yang Wang",
        "link": "http://arxiv.org/abs/2502.05174v1",
        "abstract": "Recent research has explored that LLM agents are vulnerable to indirect\nprompt injection (IPI) attacks, where malicious tasks embedded in\ntool-retrieved information can redirect the agent to take unauthorized actions.\nExisting defenses against IPI have significant limitations: either require\nessential model training resources, lack effectiveness against sophisticated\nattacks, or harm the normal utilities. We present MELON (Masked re-Execution\nand TooL comparisON), a novel IPI defense. Our approach builds on the\nobservation that under a successful attack, the agent's next action becomes\nless dependent on user tasks and more on malicious tasks. Following this, we\ndesign MELON to detect attacks by re-executing the agent's trajectory with a\nmasked user prompt modified through a masking function. We identify an attack\nif the actions generated in the original and masked executions are similar. We\nalso include three key designs to reduce the potential false positives and\nfalse negatives. Extensive evaluation on the IPI benchmark AgentDojo\ndemonstrates that MELON outperforms SOTA defenses in both attack prevention and\nutility preservation. Moreover, we show that combining MELON with a SOTA prompt\naugmentation defense (denoted as MELON-Aug) further improves its performance.\nWe also conduct a detailed ablation study to validate our key designs."
    },
    {
        "date": "2025-02",
        "title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization Framework",
        "author": "Xiaoyu Deng, Ye Zhang, Tianmin Guo, Yongzhe Zhang, Zhengjian Kang, and Hang Yang",
        "link": "http://arxiv.org/abs/2502.05084v1",
        "abstract": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs."
    },
    {
        "date": "2025-02",
        "title": "New Security Challenges Towards In-Sensor Computing Systems",
        "author": "Mashrafi Kajol, and Qiaoyan Yu",
        "link": "http://arxiv.org/abs/2502.05046v1",
        "abstract": "Data collection and processing in advanced health monitoring systems are\nexperiencing revolutionary change. In-Sensor Computing (ISC) systems emerge as\na promising alternative to save energy on massive data transmission,\nanalog-to-digital conversion, and ineffective processing. While the new\nparadigm shift of ISC systems gains increasing attention, the highly compacted\nsystems could incur new challenges from a hardware security perspective. This\nwork first conducts a literature review to highlight the research trend of this\ntopic and then performs comprehensive analyses on the root of security\nchallenges. This is the first work that compares the security challenges of\ntraditional sensor-involved computing systems and emerging ISC systems.\nFurthermore, new attack scenarios are predicted for board-, chip-, and\ndevice-level ISC systems. Two proof-of-concept demos are provided to inspire\nnew countermeasure designs against unique hardware security threats in ISC\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Federated Learning for Anomaly Detection in Energy Consumption Data: Assessing the Vulnerability to Adversarial Attacks",
        "author": "Yohannis Kifle Telila, Damitha Senevirathne, Dumindu Tissera, Apurva Narayan, Miriam A. M. Capretz, and Katarina Grolinger",
        "link": "http://arxiv.org/abs/2502.05041v1",
        "abstract": "Anomaly detection is crucial in the energy sector to identify irregular\npatterns indicating equipment failures, energy theft, or other issues. Machine\nlearning techniques for anomaly detection have achieved great success, but are\ntypically centralized, involving sharing local data with a central server which\nraises privacy and security concerns. Federated Learning (FL) has been gaining\npopularity as it enables distributed learning without sharing local data.\nHowever, FL depends on neural networks, which are vulnerable to adversarial\nattacks that manipulate data, leading models to make erroneous predictions.\nWhile adversarial attacks have been explored in the image domain, they remain\nlargely unexplored in time series problems, especially in the energy domain.\nMoreover, the effect of adversarial attacks in the FL setting is also mostly\nunknown. This paper assesses the vulnerability of FL-based anomaly detection in\nenergy data to adversarial attacks. Specifically, two state-of-the-art models,\nLong Short Term Memory (LSTM) and Transformers, are used to detect anomalies in\nan FL setting, and two white-box attack methods, Fast Gradient Sign Method\n(FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data.\nThe results show that FL is more sensitive to PGD attacks than to FGSM attacks,\nattributed to PGD's iterative nature, resulting in an accuracy drop of over 10%\neven with naive, weaker attacks. Moreover, FL is more affected by these attacks\nthan centralized learning, highlighting the need for defense mechanisms in FL."
    },
    {
        "date": "2025-02",
        "title": "Robust Graph Learning Against Adversarial Evasion Attacks via Prior-Free Diffusion-Based Structure Purification",
        "author": "Jiayi Luo, Qingyun Sun, Haonan Yuan, Xingcheng Fu, and Jianxin Li",
        "link": "http://arxiv.org/abs/2502.05000v1",
        "abstract": "Adversarial evasion attacks pose significant threats to graph learning, with\nlines of studies that have improved the robustness of Graph Neural Networks\n(GNNs). However, existing works rely on priors about clean graphs or attacking\nstrategies, which are often heuristic and inconsistent. To achieve robust graph\nlearning over different types of evasion attacks and diverse datasets, we\ninvestigate this problem from a prior-free structure purification perspective.\nSpecifically, we propose a novel Diffusion-based Structure Purification\nframework named DiffSP, which creatively incorporates the graph diffusion model\nto learn intrinsic distributions of clean graphs and purify the perturbed\nstructures by removing adversaries under the direction of the captured\npredictive patterns without relying on priors. DiffSP is divided into the\nforward diffusion process and the reverse denoising process, during which\nstructure purification is achieved. To avoid valuable information loss during\nthe forward process, we propose an LID-driven nonisotropic diffusion mechanism\nto selectively inject noise anisotropically. To promote semantic alignment\nbetween the clean graph and the purified graph generated during the reverse\nprocess, we reduce the generation uncertainty by the proposed graph transfer\nentropy guided denoising mechanism. Extensive experiments demonstrate the\nsuperior robustness of DiffSP against evasion attacks."
    },
    {
        "date": "2025-02",
        "title": "A Systematic Literature Review on Automated Exploit and Security Test Generation",
        "author": "Quang-Cuong Bui, Emanuele Iannone, Maria Camporese, Torge Hinrichs, Catherine Tony, L\u00e1szl\u00f3 T\u00f3th, Fabio Palomba, P\u00e9ter Heged\u0171s, Fabio Massacci, and Riccardo Scandariato",
        "link": "http://arxiv.org/abs/2502.04953v1",
        "abstract": "The exploit or the Proof of Concept of the vulnerability plays an important\nrole in developing superior vulnerability repair techniques, as it can be used\nas an oracle to verify the correctness of the patches generated by the tools.\nHowever, the vulnerability exploits are often unavailable and require time and\nexpert knowledge to craft. Obtaining them from the exploit generation\ntechniques is another potential solution. The goal of this survey is to aid the\nresearchers and practitioners in understanding the existing techniques for\nexploit generation through the analysis of their characteristics and their\nusability in practice. We identify a list of exploit generation techniques from\nliterature and group them into four categories: automated exploit generation,\nsecurity testing, fuzzing, and other techniques. Most of the techniques focus\non the memory-based vulnerabilities in C/C++ programs and web-based injection\nvulnerabilities in PHP and Java applications. We found only a few studies that\npublicly provided usable tools associated with their techniques."
    },
    {
        "date": "2025-02",
        "title": "Does Unsupervised Domain Adaptation Improve the Robustness of Amortized Bayesian Inference? A Systematic Evaluation",
        "author": "Lasse Elsem\u00fcller, Valentin Pratz, Mischa von Krause, Andreas Voss, Paul-Christian B\u00fcrkner, and Stefan T. Radev",
        "link": "http://arxiv.org/abs/2502.04949v1",
        "abstract": "Neural networks are fragile when confronted with data that significantly\ndeviates from their training distribution. This is true in particular for\nsimulation-based inference methods, such as neural amortized Bayesian inference\n(ABI), where models trained on simulated data are deployed on noisy real-world\nobservations. Recent robust approaches employ unsupervised domain adaptation\n(UDA) to match the embedding spaces of simulated and observed data. However,\nthe lack of comprehensive evaluations across different domain mismatches raises\nconcerns about the reliability in high-stakes applications. We address this gap\nby systematically testing UDA approaches across a wide range of\nmisspecification scenarios in both a controlled and a high-dimensional\nbenchmark. We demonstrate that aligning summary spaces between domains\neffectively mitigates the impact of unmodeled phenomena or noise. However, the\nsame alignment mechanism can lead to failures under prior misspecifications - a\ncritical finding with practical consequences. Our results underscore the need\nfor careful consideration of misspecification types when using UDA techniques\nto increase the robustness of ABI in practice."
    },
    {
        "date": "2025-02",
        "title": "Securing 5G Bootstrapping: A Two-Layer IBS Authentication Protocol",
        "author": "Yilu Dong, Rouzbeh Behnia, Attila A. Yavuz, and Syed Rafiul Hussain",
        "link": "http://arxiv.org/abs/2502.04915v1",
        "abstract": "The lack of authentication during the initial bootstrapping phase between\ncellular devices and base stations allows attackers to deploy fake base\nstations and send malicious messages to the devices. These attacks have been a\nlong-existing problem in cellular networks, enabling adversaries to launch\ndenial-of-service (DoS), information leakage, and location-tracking attacks.\nWhile some defense mechanisms are introduced in 5G, (e.g., encrypting user\nidentifiers to mitigate IMSI catchers), the initial communication between\ndevices and base stations remains unauthenticated, leaving a critical security\ngap. To address this, we propose E2IBS, a novel and efficient two-layer\nidentity-based signature scheme designed for seamless integration with existing\ncellular protocols. We implement E2IBS on an open-source 5G stack and conduct a\ncomprehensive performance evaluation against alternative solutions. Compared to\nthe state-of-the-art Schnorr-HIBS, E2IBS reduces attack surfaces, enables\nfine-grained lawful interception, and achieves 2x speed in verification, making\nit a practical solution for securing 5G base station authentication."
    },
    {
        "date": "2025-02",
        "title": "On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark",
        "author": "Jaiden Fairoze, Guillermo Ortiz-Jim\u00e9nez, Mel Vecerik, Somesh Jha, and Sven Gowal",
        "link": "http://arxiv.org/abs/2502.04901v1",
        "abstract": "This work investigates the theoretical boundaries of creating\npublicly-detectable schemes to enable the provenance of watermarked imagery.\nMetadata-based approaches like C2PA provide unforgeability and\npublic-detectability. ML techniques offer robust retrieval and watermarking.\nHowever, no existing scheme combines robustness, unforgeability, and\npublic-detectability. In this work, we formally define such a scheme and\nestablish its existence. Although theoretically possible, we find that at\npresent, it is intractable to build certain components of our scheme without a\nleap in deep learning capabilities. We analyze these limitations and propose\nresearch directions that need to be addressed before we can practically realize\nrobust and publicly-verifiable provenance."
    },
    {
        "date": "2025-02",
        "title": "CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception",
        "author": "Senkang Hu, Yihang Tao, Zihan Fang, Guowen Xu, Yiqin Deng, Sam Kwong, and Yuguang Fang",
        "link": "http://arxiv.org/abs/2502.07807v1",
        "abstract": "Collaborative perception (CP) is a promising method for safe connected and\nautonomous driving, which enables multiple vehicles to share sensing\ninformation to enhance perception performance. However, compared with\nsingle-vehicle perception, the openness of a CP system makes it more vulnerable\nto malicious attacks that can inject malicious information to mislead the\nperception of an ego vehicle, resulting in severe risks for safe driving. To\nmitigate such vulnerability, we first propose a new paradigm for malicious\nagent detection that effectively identifies malicious agents at the feature\nlevel without requiring verification of final perception results, significantly\nreducing computational overhead. Building on this paradigm, we introduce\nCP-GuardBench, the first comprehensive dataset provided to train and evaluate\nvarious malicious agent detection methods for CP systems. Furthermore, we\ndevelop a robust defense method called CP-Guard+, which enhances the margin\nbetween the representations of benign and malicious features through a\ncarefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we\nconduct extensive experiments on both CP-GuardBench and V2X-Sim, and\ndemonstrate the superiority of CP-Guard+."
    },
    {
        "date": "2025-02",
        "title": "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning",
        "author": "Yuchen Liu, Chen Chen, Lingjuan Lyu, Yaochu Jin, and Gang Chen",
        "link": "http://arxiv.org/abs/2502.04890v2",
        "abstract": "Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack"
    },
    {
        "date": "2025-02",
        "title": "Robust Conformal Outlier Detection under Contaminated Reference Data",
        "author": "Meshi Bashari, Matteo Sesia, and Yaniv Romano",
        "link": "http://arxiv.org/abs/2502.04807v1",
        "abstract": "Conformal prediction is a flexible framework for calibrating machine learning\npredictions, providing distribution-free statistical guarantees. In outlier\ndetection, this calibration relies on a reference set of labeled inlier data to\ncontrol the type-I error rate. However, obtaining a perfectly labeled inlier\nreference set is often unrealistic, and a more practical scenario involves\naccess to a contaminated reference set containing a small fraction of outliers.\nThis paper analyzes the impact of such contamination on the validity of\nconformal methods. We prove that under realistic, non-adversarial settings,\ncalibration on contaminated data yields conservative type-I error control,\nshedding light on the inherent robustness of conformal methods. This\nconservativeness, however, typically results in a loss of power. To alleviate\nthis limitation, we propose a novel, active data-cleaning framework that\nleverages a limited labeling budget and an outlier detection model to\nselectively annotate data points in the contaminated reference set that are\nsuspected as outliers. By removing only the annotated outliers in this\n``suspicious'' subset, we can effectively enhance power while mitigating the\nrisk of inflating the type-I error rate, as supported by our theoretical\nanalysis. Experiments on real datasets validate the conservative behavior of\nconformal methods under contamination and show that the proposed data-cleaning\nstrategy improves power without sacrificing validity."
    },
    {
        "date": "2025-02",
        "title": "DMPA: Model Poisoning Attacks on Decentralized Federated Learning for Model Differences",
        "author": "Chao Feng, Yunlong Li, Yuanzhe Gao, Alberto Huertas Celdr\u00e1n, Jan von der Assen, G\u00e9r\u00f4me Bovet, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2502.04771v1",
        "abstract": "Federated learning (FL) has garnered significant attention as a prominent\nprivacy-preserving Machine Learning (ML) paradigm. Decentralized FL (DFL)\neschews traditional FL's centralized server architecture, enhancing the\nsystem's robustness and scalability. However, these advantages of DFL also\ncreate new vulnerabilities for malicious participants to execute adversarial\nattacks, especially model poisoning attacks. In model poisoning attacks,\nmalicious participants aim to diminish the performance of benign models by\ncreating and disseminating the compromised model. Existing research on model\npoisoning attacks has predominantly concentrated on undermining global models\nwithin the Centralized FL (CFL) paradigm, while there needs to be more research\nin DFL. To fill the research gap, this paper proposes an innovative model\npoisoning attack called DMPA. This attack calculates the differential\ncharacteristics of multiple malicious client models and obtains the most\neffective poisoning strategy, thereby orchestrating a collusive attack by\nmultiple participants. The effectiveness of this attack is validated across\nmultiple datasets, with results indicating that the DMPA approach consistently\nsurpasses existing state-of-the-art FL model poisoning attack strategies."
    }
]