[
    {
        "date": "2025-06",
        "title": "Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval",
        "author": "Hani Alomari, Anushka Sivakumar, Andrew Zhang, and Chris Thomas",
        "link": "http://arxiv.org/abs/2506.21538v1",
        "abstract": "Cross-modal image-text retrieval is challenging because of the diverse\npossible associations between content from different modalities. Traditional\nmethods learn a single-vector embedding to represent semantics of each sample,\nbut struggle to capture nuanced and diverse relationships that can exist across\nmodalities. Set-based approaches, which represent each sample with multiple\nembeddings, offer a promising alternative, as they can capture richer and more\ndiverse relationships. In this paper, we show that, despite their promise,\nthese set-based representations continue to face issues including sparse\nsupervision and set collapse, which limits their effectiveness. To address\nthese challenges, we propose Maximal Pair Assignment Similarity to optimize\none-to-one matching between embedding sets which preserve semantic diversity\nwithin the set. We also introduce two loss functions to further enhance the\nrepresentations: Global Discriminative Loss to enhance distinction among\nembeddings, and Intra-Set Divergence Loss to prevent collapse within each set.\nOur method achieves state-of-the-art performance on MS-COCO and Flickr30k\nwithout relying on external data."
    },
    {
        "date": "2025-06",
        "title": "TITAN: Query-Token based Domain Adaptive Adversarial Learning",
        "author": "Tajamul Ashraf, and Janibul Bashir",
        "link": "http://arxiv.org/abs/2506.21484v1",
        "abstract": "We focus on the source-free domain adaptive object detection (SF-DAOD)\nproblem when source data is unavailable during adaptation and the model must\nadapt to an unlabeled target domain. The majority of approaches for the problem\nemploy a self-supervised approach using a student-teacher (ST) framework where\npseudo-labels are generated via a source-pretrained model for further\nfine-tuning. We observe that the performance of a student model often degrades\ndrastically, due to the collapse of the teacher model, primarily caused by high\nnoise in pseudo-labels, resulting from domain bias, discrepancies, and a\nsignificant domain shift across domains. To obtain reliable pseudo-labels, we\npropose a Target-based Iterative Query-Token Adversarial Network (TITAN), which\nseparates the target images into two subsets: those similar to the source\n(easy) and those dissimilar (hard). We propose a strategy to estimate variance\nto partition the target domain. This approach leverages the insight that higher\ndetection variances correspond to higher recall and greater similarity to the\nsource domain. Also, we incorporate query-token-based adversarial modules into\na student-teacher baseline framework to reduce the domain gaps between two\nfeature representations. Experiments conducted on four natural imaging datasets\nand two challenging medical datasets have substantiated the superior\nperformance of TITAN compared to existing state-of-the-art (SOTA)\nmethodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7\npercent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,\nrespectively."
    },
    {
        "date": "2025-06",
        "title": "HyperSORT: Self-Organising Robust Training with hyper-networks",
        "author": "Samuel Joutard, Marijn Stollenga, Marc Balle Sanchez, Mohammad Farid Azampour, and Raphael Prevost",
        "link": "http://arxiv.org/abs/2506.21430v1",
        "abstract": "Medical imaging datasets often contain heterogeneous biases ranging from\nerroneous labels to inconsistent labeling styles. Such biases can negatively\nimpact deep segmentation networks performance. Yet, the identification and\ncharacterization of such biases is a particularly tedious and challenging task.\nIn this paper, we introduce HyperSORT, a framework using a hyper-network\npredicting UNets' parameters from latent vectors representing both the image\nand annotation variability. The hyper-network parameters and the latent vector\ncollection corresponding to each data sample from the training set are jointly\nlearned. Hence, instead of optimizing a single neural network to fit a dataset,\nHyperSORT learns a complex distribution of UNet parameters where low density\nareas can capture noise-specific patterns while larger modes robustly segment\norgans in differentiated but meaningful manners. We validate our method on two\n3D abdominal CT public datasets: first a synthetically perturbed version of the\nAMOS dataset, and TotalSegmentator, a large scale dataset containing real\nunknown biases and errors. Our experiments show that HyperSORT creates a\nstructured mapping of the dataset allowing the identification of relevant\nsystematic biases and erroneous samples. Latent space clusters yield UNet\nparameters performing the segmentation task in accordance with the underlying\nlearned systematic bias. The code and our analysis of the TotalSegmentator\ndataset are made available: https://github.com/ImFusionGmbH/HyperSORT"
    },
    {
        "date": "2025-06",
        "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models",
        "author": "Qifei Cui, and Xinyu Lu",
        "link": "http://arxiv.org/abs/2506.21245v1",
        "abstract": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings."
    },
    {
        "date": "2025-06",
        "title": "Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels",
        "author": "Aida Moafi, Danial Moafi, Evgeny M. Mirkes, Gerry P. McCann, Abbas S. Alatrany, Jayanth R. Arnold, and Mostafa Mehdipour Ghazi",
        "link": "http://arxiv.org/abs/2506.21151v1",
        "abstract": "The accurate segmentation of myocardial scars from cardiac MRI is essential\nfor clinical assessment and treatment planning. In this study, we propose a\nrobust deep-learning pipeline for fully automated myocardial scar detection and\nsegmentation by fine-tuning state-of-the-art models. The method explicitly\naddresses challenges of label noise from semi-automatic annotations, data\nheterogeneity, and class imbalance through the use of Kullback-Leibler loss and\nextensive data augmentation. We evaluate the model's performance on both acute\nand chronic cases and demonstrate its ability to produce accurate and smooth\nsegmentations despite noisy labels. In particular, our approach outperforms\nstate-of-the-art models like nnU-Net and shows strong generalizability in an\nout-of-distribution test set, highlighting its robustness across various\nimaging conditions and clinical tasks. These results establish a reliable\nfoundation for automated myocardial scar quantification and support the broader\nclinical adoption of deep learning in cardiac imaging."
    },
    {
        "date": "2025-06",
        "title": "Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks",
        "author": "Deepak Kumar Panda, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21142v1",
        "abstract": "The growing integration of UAVs into civilian airspace underscores the need\nfor resilient and intelligent intrusion detection systems (IDS), as traditional\nanomaly detection methods often fail to identify novel threats. A common\napproach treats unfamiliar attacks as out-of-distribution (OOD) samples;\nhowever, this leaves systems vulnerable when mitigation is inadequate.\nMoreover, conventional OOD detectors struggle to distinguish stealthy\nadversarial attacks from genuine OOD events. This paper introduces a\nconditional generative adversarial network (cGAN)-based framework for crafting\nstealthy adversarial attacks that evade IDS mechanisms. We first design a\nrobust multi-class IDS classifier trained on benign UAV telemetry and known\ncyber-attacks, including Denial of Service (DoS), false data injection (FDI),\nman-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN\nperturbs known attacks to generate adversarial samples that misclassify as\nbenign while retaining statistical resemblance to OOD distributions. These\nadversarial samples are iteratively refined to achieve high stealth and success\nrates. To detect such perturbations, we implement a conditional variational\nautoencoder (CVAE), leveraging negative log-likelihood to separate adversarial\ninputs from authentic OOD samples. Comparative evaluation shows that CVAE-based\nregret scores significantly outperform traditional Mahalanobis distance-based\ndetectors in identifying stealthy adversarial threats. Our findings emphasize\nthe importance of advanced probabilistic modeling to strengthen IDS\ncapabilities against adaptive, generative-model-based cyber intrusions."
    },
    {
        "date": "2025-06",
        "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks",
        "author": "Deepak Kumar Panda, Adolfo Perrusquia, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21129v1",
        "abstract": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios."
    },
    {
        "date": "2025-06",
        "title": "Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments",
        "author": "Deepak Kumar Panda, and Weisi Guo",
        "link": "http://arxiv.org/abs/2506.21127v1",
        "abstract": "The increasing automation of navigation for unmanned aerial vehicles (UAVs)\nhas exposed them to adversarial attacks that exploit vulnerabilities in\nreinforcement learning (RL) through sensor manipulation. Although existing\nrobust RL methods aim to mitigate such threats, their effectiveness has limited\ngeneralization to out-of-distribution shifts from the optimal value\ndistribution, as they are primarily designed to handle fixed perturbation. To\naddress this limitation, this paper introduces an antifragile RL framework that\nenhances adaptability to broader distributional shifts by incorporating a\nswitching mechanism based on discounted Thompson sampling (DTS). This mechanism\ndynamically selects among multiple robust policies to minimize adversarially\ninduced state-action-value distribution shifts. The proposed approach first\nderives a diverse ensemble of action robust policies by accounting for a range\nof perturbations in the policy space. These policies are then modeled as a\nmultiarmed bandit (MAB) problem, where DTS optimally selects policies in\nresponse to nonstationary Bernoulli rewards, effectively adapting to evolving\nadversarial strategies. Theoretical framework has also been provided where by\noptimizing the DTS to minimize the overall regrets due to distributional shift,\nresults in effective adaptation against unseen adversarial attacks thus\ninducing antifragility. Extensive numerical simulations validate the\neffectiveness of the proposed framework in complex navigation environments with\nmultiple dynamic three-dimensional obstacles and with stronger projected\ngradient descent (PGD) and spoofing attacks. Compared to conventional robust,\nnon-adaptive RL methods, the antifragile approach achieves superior\nperformance, demonstrating shorter navigation path lengths and a higher rate of\nconflict-free navigation trajectories compared to existing robust RL techniques"
    },
    {
        "date": "2025-06",
        "title": "Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features",
        "author": "Shangbo Wu, Yu-an Tan, Ruinan Ma, Wencong Ma, Dehua Zhu, and Yuanzhang Li",
        "link": "http://arxiv.org/abs/2506.21046v1",
        "abstract": "The ability of deep neural networks (DNNs) come from extracting and\ninterpreting features from the data provided. By exploiting intermediate\nfeatures in DNNs instead of relying on hard labels, we craft adversarial\nperturbation that generalize more effectively, boosting black-box\ntransferability. These features ubiquitously come from supervised learning in\nprevious work. Inspired by the exceptional synergy between self-supervised\nlearning and the Transformer architecture, this paper explores whether\nexploiting self-supervised Vision Transformer (ViT) representations can improve\nadversarial transferability. We present dSVA -- a generative dual\nself-supervised ViT features attack, that exploits both global structural\nfeatures from contrastive learning (CL) and local textural features from masked\nimage modeling (MIM), the self-supervised learning paradigm duo for ViTs. We\ndesign a novel generative training framework that incorporates a generator to\ncreate black-box adversarial examples, and strategies to train the generator by\nexploiting joint features and the attention mechanism of self-supervised ViTs.\nOur findings show that CL and MIM enable ViTs to attend to distinct feature\ntendencies, which, when exploited in tandem, boast great adversarial\ngeneralizability. By disrupting dual deep features distilled by self-supervised\nViTs, we are rewarded with remarkable black-box transferability to models of\nvarious architectures that outperform state-of-the-arts. Code available at\nhttps://github.com/spencerwooo/dSVA."
    },
    {
        "date": "2025-06",
        "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling",
        "author": "Junwei You, Pei Li, Zhuoyu Jiang, Zilin Huang, Rui Gan, Haotian Shi, and Bin Ran",
        "link": "http://arxiv.org/abs/2506.21041v1",
        "abstract": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving."
    },
    {
        "date": "2025-06",
        "title": "HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation",
        "author": "Qingyue Jiao, Kangyu Zheng, Yiyu Shi, and Zhiding Liang",
        "link": "http://arxiv.org/abs/2506.21015v1",
        "abstract": "Machine learning-assisted diagnosis is gaining traction in skin disease\ndetection, but training effective models requires large amounts of high-quality\ndata. Skin disease datasets often suffer from class imbalance, privacy\nconcerns, and object bias, making data augmentation essential. While classical\ngenerative models are widely used, they demand extensive computational\nresources and lengthy training time. Quantum computing offers a promising\nalternative, but existing quantum-based image generation methods can only yield\ngrayscale low-quality images. Through a novel classical-quantum latent space\nfusion technique, our work overcomes this limitation and introduces the first\nclassical-quantum generative adversarial network (GAN) capable of generating\ncolor medical images. Our model outperforms classical deep convolutional GANs\nand existing hybrid classical-quantum GANs in both image generation quality and\nclassification performance boost when used as data augmentation. Moreover, the\nperformance boost is comparable with that achieved using state-of-the-art\nclassical generative models, yet with over 25 times fewer parameters and 10\ntimes fewer training epochs. Such results suggest a promising future for\nquantum image generation as quantum hardware advances. Finally, we demonstrate\nthe robust performance of our model on real IBM quantum machine with hardware\nnoise."
    },
    {
        "date": "2025-06",
        "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology",
        "author": "Qiuyi Qi, Xin Li, Ming Kong, Zikang Xu, Bingdi Chen, Qiang Zhu, and S Kevin Zhou",
        "link": "http://arxiv.org/abs/2506.21001v1",
        "abstract": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC."
    },
    {
        "date": "2025-06",
        "title": "SPA: Towards More Stealth and Persistent Backdoor Attacks in Federated Learning",
        "author": "Chengcheng Zhu, Ye Li, Bosen Rao, Jiale Zhang, Yunlong Mao, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2506.20931v1",
        "abstract": "Federated Learning (FL) has emerged as a leading paradigm for\nprivacy-preserving distributed machine learning, yet the distributed nature of\nFL introduces unique security challenges, notably the threat of backdoor\nattacks. Existing backdoor strategies predominantly rely on end-to-end label\nsupervision, which, despite their efficacy, often results in detectable feature\ndisentanglement and limited persistence. In this work, we propose a novel and\nstealthy backdoor attack framework, named SPA, which fundamentally departs from\ntraditional approaches by leveraging feature-space alignment rather than direct\ntrigger-label association. Specifically, SPA reduces representational distances\nbetween backdoor trigger features and target class features, enabling the\nglobal model to misclassify trigger-embedded inputs with high stealth and\npersistence. We further introduce an adaptive, adversarial trigger optimization\nmechanism, utilizing boundary-search in the feature space to enhance attack\nlongevity and effectiveness, even against defensive FL scenarios and non-IID\ndata distributions. Extensive experiments on various FL benchmarks demonstrate\nthat SPA consistently achieves high attack success rates with minimal impact on\nmodel utility, maintains robustness under challenging participation and data\nheterogeneity conditions, and exhibits persistent backdoor effects far\nexceeding those of conventional techniques. Our results call urgent attention\nto the evolving sophistication of backdoor threats in FL and emphasize the\npressing need for advanced, feature-level defense techniques."
    },
    {
        "date": "2025-06",
        "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity",
        "author": "Shuki Maruyama, and Hidenori Takeshima",
        "link": "http://arxiv.org/abs/2506.20897v1",
        "abstract": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra."
    },
    {
        "date": "2025-06",
        "title": "Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers",
        "author": "Furkan Mumcu, and Yasin Yilmaz",
        "link": "http://arxiv.org/abs/2506.20816v1",
        "abstract": "Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input\ndesigns with limited noise budgets. While numerous successful attacks with\nsubtle modifications to original input have been proposed, defense techniques\nagainst these attacks are relatively understudied. Existing defense approaches\neither focus on improving DNN robustness by negating the effects of\nperturbations or use a secondary model to detect adversarial data. Although\nequally important, the attack detection approach, which is studied in this\nwork, provides a more practical defense compared to the robustness approach. We\nshow that the existing detection methods are either ineffective against the\nstate-of-the-art attack techniques or computationally inefficient for real-time\nprocessing. We propose a novel universal and efficient method to detect\nadversarial examples by analyzing the varying degrees of impact of attacks on\ndifferent DNN layers. {Our method trains a lightweight regression model that\npredicts deeper-layer features from early-layer features, and uses the\nprediction error to detect adversarial samples.} Through theoretical arguments\nand extensive experiments, we demonstrate that our detection method is highly\neffective, computationally efficient for real-time processing, compatible with\nany DNN architecture, and applicable across different domains, such as image,\nvideo, and audio."
    },
    {
        "date": "2025-06",
        "title": "Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis",
        "author": "Zhonghao Zhan, Huichi Zhou, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2506.20806v1",
        "abstract": "Graph Neural Networks (GNNs) show great promise for Network Intrusion\nDetection Systems (NIDS), particularly in IoT environments, but suffer\nperformance degradation due to distribution drift and lack robustness against\nrealistic adversarial attacks. Current robustness evaluations often rely on\nunrealistic synthetic perturbations and lack demonstrations on systematic\nanalysis of different kinds of adversarial attack, which encompass both\nblack-box and white-box scenarios. This work proposes a novel approach to\nenhance GNN robustness and generalization by employing Large Language Models\n(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These\nagents scrutinize graph structures derived from network flow data, identifying\nand potentially mitigating suspicious or adversarially perturbed elements\nbefore GNN processing. Our experiments, using a framework designed for\nrealistic evaluation and testing with a variety of adversarial attacks\nincluding a dataset collected from physical testbed experiments, demonstrate\nthat integrating LLM analysis can significantly improve the resilience of\nGNN-based NIDS against challenges, showcasing the potential of LLM agent as a\ncomplementary layer in intrusion detection architectures."
    },
    {
        "date": "2025-06",
        "title": "On the Impact of Sybil-based Attacks on Mobile Crowdsensing for Transportation",
        "author": "Alexander S\u00f6derh\u00e4ll, Zahra Alimadadi, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2506.20585v1",
        "abstract": "Mobile Crowd-Sensing (MCS) enables users with personal mobile devices (PMDs)\nto gain information on their surroundings. Users collect and contribute data on\ndifferent phenomena using their PMD sensors, and the MCS system processes this\ndata to extract valuable information for end users. Navigation MCS-based\napplications (N-MCS) are prevalent and important for transportation: users\nshare their location and speed while driving and, in return, find efficient\nroutes to their destinations. However, N-MCS are currently vulnerable to\nmalicious contributors, often termed Sybils: submitting falsified data,\nseemingly from many devices that are not truly present on target roads, falsely\nreporting congestion when there is none, thus changing the road status the\nN-MCS infers. The attack effect is that the N-MCS returns suboptimal routes to\nusers, causing late arrival and, overall, deteriorating road traffic flow. We\ninvestigate exactly the impact of Sybil-based attacks on N-MCS: we design an\nN-MCS system that offers efficient routing on top of the vehicular simulator\nSUMO, using the InTAS road network as our scenario. We design experiments\nattacking an individual N-MCS user as well as a larger population of users,\nselecting the adversary targets based on graph-theoretical arguments. Our\nexperiments show that the resources required for a successful attack depend on\nthe location of the attack (i.e., the surrounding road network and traffic) and\nthe extent of Sybil contributed data for the targeted road(s). We demonstrate\nthat Sybil attacks can alter the route of N-MCS users, increasing average\ntravel time by 20% with Sybils 3% of the N-MCS user population."
    },
    {
        "date": "2025-06",
        "title": "Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS",
        "author": "Sabrine Ennaji, Elhadj Benkhelifa, and Luigi V. Mancini",
        "link": "http://arxiv.org/abs/2506.20576v1",
        "abstract": "Adversarial attacks, wherein slight inputs are carefully crafted to mislead\nintelligent models, have attracted increasing attention. However, a critical\ngap persists between theoretical advancements and practical application,\nparticularly in structured data like network traffic, where interdependent\nfeatures complicate effective adversarial manipulations. Moreover, ambiguity in\ncurrent approaches restricts reproducibility and limits progress in this field.\nHence, existing defenses often fail to handle evolving adversarial attacks.\nThis paper proposes a novel approach for black-box adversarial attacks, that\naddresses these limitations. Unlike prior work, which often assumes system\naccess or relies on repeated probing, our method strictly respect black-box\nconstraints, reducing interaction to avoid detection and better reflect\nreal-world scenarios. We present an adaptive feature selection strategy using\nchange-point detection and causality analysis to identify and target sensitive\nfeatures to perturbations. This lightweight design ensures low computational\ncost and high deployability. Our comprehensive experiments show the attack's\neffectiveness in evading detection with minimal interaction, enhancing its\nadaptability and applicability in real-world scenarios. By advancing the\nunderstanding of adversarial attacks in network traffic, this work lays a\nfoundation for developing robust defenses."
    },
    {
        "date": "2025-06",
        "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
        "author": "Kristian Minchev, Dimitar Iliev Dimitrov, and Nikola Konstantinov",
        "link": "http://arxiv.org/abs/2506.20573v1",
        "abstract": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets."
    },
    {
        "date": "2025-06",
        "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation",
        "author": "Lei Zhu, Jun Zhou, Rick Siow Mong Goh, and Yong Liu",
        "link": "http://arxiv.org/abs/2506.20563v1",
        "abstract": "Vision Transformer has recently gained tremendous popularity in medical image\nsegmentation task due to its superior capability in capturing long-range\ndependencies. However, transformer requires a large amount of labeled data to\nbe effective, which hinders its applicability in annotation scarce\nsemi-supervised learning scenario where only limited labeled data is available.\nState-of-the-art semi-supervised learning methods propose combinatorial\nCNN-Transformer learning to cross teach a transformer with a convolutional\nneural network, which achieves promising results. However, it remains a\nchallenging task to effectively train the transformer with limited labeled\ndata. In this paper, we propose an adversarial masked image modeling method to\nfully unleash the potential of transformer for semi-supervised medical image\nsegmentation. The key challenge in semi-supervised learning with transformer\nlies in the lack of sufficient supervision signal. To this end, we propose to\nconstruct an auxiliary masked domain from original domain with masked image\nmodeling and train the transformer to predict the entire segmentation mask with\nmasked inputs to increase supervision signal. We leverage the original labels\nfrom labeled data and pseudo-labels from unlabeled data to learn the masked\ndomain. To further benefit the original domain from masked domain, we provide a\ntheoretical analysis of our method from a multi-domain learning perspective and\ndevise a novel adversarial training loss to reduce the domain gap between the\noriginal and masked domain, which boosts semi-supervised learning performance.\nWe also extend adversarial masked image modeling to CNN network. Extensive\nexperiments on three public medical image segmentation datasets demonstrate the\neffectiveness of our method, where our method outperforms existing methods\nsignificantly. Our code is publicly available at\nhttps://github.com/zlheui/AdvMIM."
    },
    {
        "date": "2025-06",
        "title": "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos",
        "author": "Yitong Quan, Benjamin Kiefer, Martin Messmer, and Andreas Zell",
        "link": "http://arxiv.org/abs/2506.20550v1",
        "abstract": "Modern image-based object detection models, such as YOLOv7, primarily process\nindividual frames independently, thus ignoring valuable temporal context\nnaturally present in videos. Meanwhile, existing video-based detection methods\noften introduce complex temporal modules, significantly increasing model size\nand computational complexity. In practical applications such as surveillance\nand autonomous driving, transient challenges including motion blur, occlusions,\nand abrupt appearance changes can severely degrade single-frame detection\nperformance. To address these issues, we propose a straightforward yet highly\neffective strategy: stacking multiple consecutive frames as input to a\nYOLO-based detector while supervising only the output corresponding to a single\ntarget frame. This approach leverages temporal information with minimal\nmodifications to existing architectures, preserving simplicity, computational\nefficiency, and real-time inference capability. Extensive experiments on the\nchallenging MOT20Det and our BOAT360 datasets demonstrate that our method\nimproves detection robustness, especially for lightweight models, effectively\nnarrowing the gap between compact and heavy detection networks. Additionally,\nwe contribute the BOAT360 benchmark dataset, comprising annotated fisheye video\nsequences captured from a boat, to support future research in multi-frame video\nobject detection in challenging real-world scenarios."
    },
    {
        "date": "2025-06",
        "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks",
        "author": "Manyi Li, Renshuai Tao, Yufan Liu, Chuangchuang Tan, Haotong Qin, Bing Li, Yunchao Wei, and Yao Zhao",
        "link": "http://arxiv.org/abs/2506.20548v1",
        "abstract": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA."
    },
    {
        "date": "2025-06",
        "title": "Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery",
        "author": "Gilad Lerman, Kang Li, Tyler Maunu, and Teng Zhang",
        "link": "http://arxiv.org/abs/2506.20533v1",
        "abstract": "Robust subspace estimation is fundamental to many machine learning and data\nanalysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and\nempirically effective approach to this problem, yet its theoretical properties\nremain poorly understood. This paper establishes that, under deterministic\nconditions, a variant of IRLS with dynamic smoothing regularization converges\nlinearly to the underlying subspace from any initialization. We extend these\nguarantees to affine subspace estimation, a setting that lacks prior recovery\ntheory. Additionally, we illustrate the practical benefits of IRLS through an\napplication to low-dimensional neural network training. Our results provide the\nfirst global convergence guarantees for IRLS in robust subspace recovery and,\nmore broadly, for nonconvex IRLS on a Riemannian manifold."
    },
    {
        "date": "2025-06",
        "title": "SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models",
        "author": "Dipayan Saha, Shams Tarek, Hasan Al Shaikh, Khan Thamid Hasan, Pavan Sai Nalluri, Md. Ajoad Hasan, Nashmin Alam, Jingbo Zhou, Sujan Kumar Saha, Mark Tehranipoor, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2506.20415v1",
        "abstract": "Ensuring the security of complex system-on-chips (SoCs) designs is a critical\nimperative, yet traditional verification techniques struggle to keep pace due\nto significant challenges in automation, scalability, comprehensiveness, and\nadaptability. The advent of large language models (LLMs), with their remarkable\ncapabilities in natural language understanding, code generation, and advanced\nreasoning, presents a new paradigm for tackling these issues. Moving beyond\nmonolithic models, an agentic approach allows for the creation of multi-agent\nsystems where specialized LLMs collaborate to solve complex problems more\neffectively. Recognizing this opportunity, we introduce SV-LLM, a novel\nmulti-agent assistant system designed to automate and enhance SoC security\nverification. By integrating specialized agents for tasks like verification\nquestion answering, security asset identification, threat modeling, test plan\nand property generation, vulnerability detection, and simulation-based bug\nvalidation, SV-LLM streamlines the workflow. To optimize their performance in\nthese diverse tasks, agents leverage different learning paradigms, such as\nin-context learning, fine-tuning, and retrieval-augmented generation (RAG). The\nsystem aims to reduce manual intervention, improve accuracy, and accelerate\nsecurity analysis, supporting proactive identification and mitigation of risks\nearly in the design cycle. We demonstrate its potential to transform hardware\nsecurity practices through illustrative case studies and experiments that\nshowcase its applicability and efficacy."
    },
    {
        "date": "2025-06",
        "title": "Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning",
        "author": "Mohammad Mahdi Maheri, Denys Herasymuk, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2506.20413v1",
        "abstract": "The growing adoption of Artificial Intelligence (AI) in Internet of Things\n(IoT) ecosystems has intensified the need for personalized learning methods\nthat can operate efficiently and privately across heterogeneous,\nresource-constrained devices. However, enabling effective personalized learning\nin decentralized settings introduces several challenges, including efficient\nknowledge transfer between clients, protection of data privacy, and resilience\nagainst poisoning attacks. In this paper, we address these challenges by\ndeveloping P4 (Personalized, Private, Peer-to-Peer) -- a method designed to\ndeliver personalized models for resource-constrained IoT devices while ensuring\ndifferential privacy and robustness against poisoning attacks. Our solution\nemploys a lightweight, fully decentralized algorithm to privately detect client\nsimilarity and form collaborative groups. Within each group, clients leverage\ndifferentially private knowledge distillation to co-train their models,\nmaintaining high accuracy while ensuring robustness to the presence of\nmalicious clients. We evaluate P4 on popular benchmark datasets using both\nlinear and CNN-based architectures across various heterogeneity settings and\nattack scenarios. Experimental results show that P4 achieves 5% to 30% higher\naccuracy than leading differentially private peer-to-peer approaches and\nmaintains robustness with up to 30% malicious clients. Additionally, we\ndemonstrate its practicality by deploying it on resource-constrained devices,\nwhere collaborative training between two clients adds only ~7 seconds of\noverhead."
    },
    {
        "date": "2025-06",
        "title": "InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking",
        "author": "Abdullah All Tanvir, and Xin Zhong",
        "link": "http://arxiv.org/abs/2506.20370v1",
        "abstract": "This paper introduces a novel deep learning framework for robust image\nzero-watermarking based on distortion-invariant feature learning. As a\nzero-watermarking scheme, our method leaves the original image unaltered and\nlearns a reference signature through optimization in the feature space. The\nproposed framework consists of two key modules. In the first module, a feature\nextractor is trained via noise-adversarial learning to generate representations\nthat are both invariant to distortions and semantically expressive. This is\nachieved by combining adversarial supervision against a distortion\ndiscriminator and a reconstruction constraint to retain image content. In the\nsecond module, we design a learning-based multibit zero-watermarking scheme\nwhere the trained invariant features are projected onto a set of trainable\nreference codes optimized to match a target binary message. Extensive\nexperiments on diverse image datasets and a wide range of distortions show that\nour method achieves state-of-the-art robustness in both feature stability and\nwatermark recovery. Comparative evaluations against existing self-supervised\nand deep watermarking techniques further highlight the superiority of our\nframework in generalization and robustness."
    },
    {
        "date": "2025-06",
        "title": "Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design",
        "author": "Daniele Ravasio, Marcello Farina, Alessio La Bella, and Andrea Ballarino",
        "link": "http://arxiv.org/abs/2506.20334v1",
        "abstract": "This paper investigates the design of output-feedback schemes for systems\ndescribed by a class of recurrent neural networks. We propose a procedure based\non linear matrix inequalities for designing an observer and a static\nstate-feedback controller. The algorithm leverages global and regional\nincremental input-to-state stability (incremental ISS) and enables the tracking\nof constant setpoints, ensuring robustness to disturbances and state estimation\nuncertainty. To address the potential limitations of regional incremental ISS,\nwe introduce an alternative scheme in which the static law is replaced with a\ntube-based nonlinear model predictive controller (NMPC) that exploits regional\nincremental ISS properties. We show that these conditions enable the\nformulation of a robust NMPC law with guarantees of convergence and recursive\nfeasibility, leading to an enlarged region of attraction. Theoretical results\nare validated through numerical simulations on the pH-neutralisation process\nbenchmark, demonstrating the effectiveness of the proposed schemes."
    },
    {
        "date": "2025-06",
        "title": "Argumentative Ensembling for Robust Recourse under Model Multiplicity",
        "author": "Junqi Jiang, Antonio Rago, Francesco Leofante, and Francesca Toni",
        "link": "http://arxiv.org/abs/2506.20260v1",
        "abstract": "In machine learning, it is common to obtain multiple equally performing\nmodels for the same prediction task, e.g., when training neural networks with\ndifferent random seeds. Model multiplicity (MM) is the situation which arises\nwhen these competing models differ in their predictions for the same input, for\nwhich ensembling is often employed to determine an aggregation of the outputs.\nProviding recourse recommendations via counterfactual explanations (CEs) under\nMM thus becomes complex, since the CE may not be valid across all models, i.e.,\nthe CEs are not robust under MM. In this work, we formalise the problem of\nproviding recourse under MM, which we name recourse-aware ensembling (RAE). We\npropose the idea that under MM, CEs for each individual model should be\nconsidered alongside their predictions so that the aggregated prediction and\nrecourse are decided in tandem. Centred around this intuition, we introduce six\ndesirable properties for solutions to this problem. For solving RAE, we propose\na novel argumentative ensembling method which guarantees the robustness of CEs\nunder MM. Specifically, our method leverages computational argumentation to\nexplicitly represent the conflicts between models and counterfactuals regarding\nprediction results and CE validity. It then uses argumentation semantics to\nresolve the conflicts and obtain the final solution, in a manner which is\nparametric to the chosen semantics. Our method also allows for the\nspecification of preferences over the models under MM, allowing further\ncustomisation of the ensemble. In a comprehensive theoretical analysis, we\ncharacterise the behaviour of argumentative ensembling with four different\nargumentation semantics. We then empirically demonstrate the effectiveness of\nour approach in satisfying desirable properties with eight instantiations of\nour method. (Abstract is shortened for arXiv.)"
    },
    {
        "date": "2025-06",
        "title": "Secure Multi-Key Homomorphic Encryption with Application to Privacy-Preserving Federated Learning",
        "author": "Jiahui Wu, Tiecheng Sun, Fucai Luo, Haiyan Wang, and Weizhe Zhang",
        "link": "http://arxiv.org/abs/2506.20101v1",
        "abstract": "Multi-Key Homomorphic Encryption (MKHE), proposed by Lopez-Alt et al. (STOC\n2012), allows for performing arithmetic computations directly on ciphertexts\nencrypted under distinct keys. Subsequent works by Chen and Dai et al. (CCS\n2019) and Kim and Song et al. (CCS 2023) extended this concept by proposing\nmulti-key BFV/CKKS variants, referred to as the CDKS scheme. These variants\nincorporate asymptotically optimal techniques to facilitate secure computation\nacross multiple data providers. In this paper, we identify a critical security\nvulnerability in the CDKS scheme when applied to multiparty secure computation\ntasks, such as privacy-preserving federated learning (PPFL). In particular, we\nshow that CDKS may inadvertently leak plaintext information from one party to\nothers. To mitigate this issue, we propose a new scheme, SMHE (Secure Multi-Key\nHomomorphic Encryption), which incorporates a novel masking mechanism into the\nmulti-key BFV and CKKS frameworks to ensure that plaintexts remain confidential\nthroughout the computation. We implement a PPFL application using SMHE and\ndemonstrate that it provides significantly improved security with only a modest\noverhead in homomorphic evaluation. For instance, our PPFL model based on\nmulti-key CKKS incurs less than a 2\\times runtime and communication traffic\nincrease compared to the CDKS-based PPFL model. The code is publicly available\nat https://github.com/JiahuiWu2022/SMHE.git."
    },
    {
        "date": "2025-06",
        "title": "Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks",
        "author": "Yali Yuan, Weiyi Zou, and Guang Cheng",
        "link": "http://arxiv.org/abs/2506.20082v1",
        "abstract": "Website Fingerprinting (WF) attacks aim to infer which websites a user is\nvisiting by analyzing traffic patterns, thereby compromising user anonymity.\nAlthough this technique has been demonstrated to be effective in controlled\nexperimental environments, it remains largely limited to small-scale scenarios,\ntypically restricted to recognizing website homepages. In practical settings,\nhowever, users frequently access multiple subpages in rapid succession, often\nbefore previous content fully loads. WebPage Fingerprinting (WPF) generalizes\nthe WF framework to large-scale environments by modeling subpages of the same\nsite as distinct classes. These pages often share similar page elements,\nresulting in lower inter-class variance in traffic features. Furthermore, we\nconsider multi-tab browsing scenarios, in which a single trace encompasses\nmultiple categories of webpages. This leads to overlapping traffic segments,\nand similar features may appear in different positions within the traffic,\nthereby increasing the difficulty of classification. To address these\nchallenges, we propose an attention-driven fine-grained WPF attack, named\nADWPF. Specifically, during the training phase, we apply targeted augmentation\nto salient regions of the traffic based on attention maps, including attention\ncropping and attention masking. ADWPF then extracts low-dimensional features\nfrom both the original and augmented traffic and applies self-attention modules\nto capture the global contextual patterns of the trace. Finally, to handle the\nmulti-tab scenario, we employ the residual attention to generate class-specific\nrepresentations of webpages occurring at different temporal positions.\nExtensive experiments demonstrate that the proposed method consistently\nsurpasses state-of-the-art baselines across datasets of different scales."
    },
    {
        "date": "2025-06",
        "title": "Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis",
        "author": "Lorin Achey, Alec Reed, Brendan Crowe, Bradley Hayes, and Christoffer Heckman",
        "link": "http://arxiv.org/abs/2506.20049v1",
        "abstract": "We present a novel approach for enhancing robotic exploration by using\ngenerative occupancy mapping. We introduce SceneSense, a diffusion model\ndesigned and trained for predicting 3D occupancy maps given partial\nobservations. Our proposed approach probabilistically fuses these predictions\ninto a running occupancy map in real-time, resulting in significant\nimprovements in map quality and traversability. We implement SceneSense onboard\na quadruped robot and validate its performance with real-world experiments to\ndemonstrate the effectiveness of the model. In these experiments, we show that\noccupancy maps enhanced with SceneSense predictions better represent our fully\nobserved ground truth data (24.44% FID improvement around the robot and 75.59%\nimprovement at range). We additionally show that integrating\nSceneSense-enhanced maps into our robotic exploration stack as a \"drop-in\" map\nimprovement, utilizing an existing off-the-shelf planner, results in\nimprovements in robustness and traversability time. Finally we show results of\nfull exploration evaluations with our proposed system in two dissimilar\nenvironments and find that locally enhanced maps provide more consistent\nexploration results than maps constructed only from direct sensor measurements."
    },
    {
        "date": "2025-06",
        "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs",
        "author": "Xin Fan Guo, Albert Merono Penuela, Sergio Maffeis, and Fabio Pierazzi",
        "link": "http://arxiv.org/abs/2506.19802v1",
        "abstract": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %."
    },
    {
        "date": "2025-06",
        "title": "Geometric-Aware Variational Inference: Robust and Adaptive Regularization with Directional Weight Uncertainty",
        "author": "Carlos Stein Brito",
        "link": "http://arxiv.org/abs/2506.19726v1",
        "abstract": "Deep neural networks require principled uncertainty quantification, yet\nexisting variational inference methods often employ isotropic Gaussian\napproximations in weight space that poorly match the network's inherent\ngeometry. We address this mismatch by introducing Concentration-Adapted\nPerturbations (CAP), a variational framework that models weight uncertainties\ndirectly on the unit hypersphere using von Mises-Fisher distributions. Building\non recent work in radial-directional posterior decompositions and spherical\nweight constraints, CAP provides the first complete theoretical framework\nconnecting directional statistics to practical noise regularization in neural\nnetworks. Our key contribution is an analytical derivation linking vMF\nconcentration parameters to activation noise variance, enabling each layer to\nlearn its optimal uncertainty level through a novel closed-form KL divergence\nregularizer. In experiments on CIFAR-10, CAP significantly improves model\ncalibration - reducing Expected Calibration Error by 5.6x - while providing\ninterpretable layer-wise uncertainty profiles. CAP requires minimal\ncomputational overhead and integrates seamlessly into standard architectures,\noffering a theoretically grounded yet practical approach to uncertainty\nquantification in deep learning."
    },
    {
        "date": "2025-06",
        "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models",
        "author": "Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2506.19697v1",
        "abstract": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."
    },
    {
        "date": "2025-06",
        "title": "Model Guidance via Robust Feature Attribution",
        "author": "Mihnea Ghitu, Matthew Wicker, and Vihari Piratla",
        "link": "http://arxiv.org/abs/2506.19680v1",
        "abstract": "Controlling the patterns a model learns is essential to preventing reliance\non irrelevant or misleading features. Such reliance on irrelevant features,\noften called shortcut features, has been observed across domains, including\nmedical imaging and natural language processing, where it may lead to\nreal-world harms. A common mitigation strategy leverages annotations (provided\nby humans or machines) indicating which features are relevant or irrelevant.\nThese annotations are compared to model explanations, typically in the form of\nfeature salience, and used to guide the loss function during training.\nUnfortunately, recent works have demonstrated that feature salience methods are\nunreliable and therefore offer a poor signal to optimize. In this work, we\npropose a simplified objective that simultaneously optimizes for explanation\nrobustness and mitigation of shortcut learning. Unlike prior objectives with\nsimilar aims, we demonstrate theoretically why our approach ought to be more\neffective. Across a comprehensive series of experiments, we show that our\napproach consistently reduces test-time misclassifications by 20% compared to\nstate-of-the-art methods. We also extend prior experimental settings to include\nnatural language processing tasks. Additionally, we conduct novel ablations\nthat yield practical insights, including the relative importance of annotation\nquality over quantity. Code for our method and experiments is available at:\nhttps://github.com/Mihneaghitu/ModelGuidanceViaRobustFeatureAttribution."
    },
    {
        "date": "2025-06",
        "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures",
        "author": "Dezhang Kong, Shi Lin, Zhenhua Xu, Zhebo Wang, Minghao Li, Yufeng Li, Yilun Zhang, Zeyang Sha, Yuyuan Li, Changting Lin, Xun Wang, Xuan Liu, Muhammad Khurram Khan, Ningyu Zhang, Chaochao Chen, and Meng Han",
        "link": "http://arxiv.org/abs/2506.19676v1",
        "abstract": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field."
    },
    {
        "date": "2025-06",
        "title": "Angio-Diff: Learning a Self-Supervised Adversarial Diffusion Model for Angiographic Geometry Generation",
        "author": "Zhifeng Wang, Renjiao Yi, Xin Wen, Chenyang Zhu, Kai Xu, and Kunlun He",
        "link": "http://arxiv.org/abs/2506.19455v1",
        "abstract": "Vascular diseases pose a significant threat to human health, with X-ray\nangiography established as the gold standard for diagnosis, allowing for\ndetailed observation of blood vessels. However, angiographic X-rays expose\npersonnel and patients to higher radiation levels than non-angiographic X-rays,\nwhich are unwanted. Thus, modality translation from non-angiographic to\nangiographic X-rays is desirable. Data-driven deep approaches are hindered by\nthe lack of paired large-scale X-ray angiography datasets. While making\nhigh-quality vascular angiography synthesis crucial, it remains challenging. We\nfind that current medical image synthesis primarily operates at pixel level and\nstruggles to adapt to the complex geometric structure of blood vessels,\nresulting in unsatisfactory quality of blood vessel image synthesis, such as\ndisconnections or unnatural curvatures. To overcome this issue, we propose a\nself-supervised method via diffusion models to transform non-angiographic\nX-rays into angiographic X-rays, mitigating data shortages for data-driven\napproaches. Our model comprises a diffusion model that learns the distribution\nof vascular data from diffusion latent, a generator for vessel synthesis, and a\nmask-based adversarial module. To enhance geometric accuracy, we propose a\nparametric vascular model to fit the shape and distribution of blood vessels.\nThe proposed method contributes a pipeline and a synthetic dataset for X-ray\nangiography. We conducted extensive comparative and ablation experiments to\nevaluate the Angio-Diff. The results demonstrate that our method achieves\nstate-of-the-art performance in synthetic angiography image quality and more\naccurately synthesizes the geometric structure of blood vessels. The code is\navailable at https://github.com/zfw-cv/AngioDiff."
    },
    {
        "date": "2025-06",
        "title": "Unsupervised Dataset Dictionary Learning for domain shift robust clustering: application to sitting posture identification",
        "author": "Anas Hattay, Mayara Ayat, and Fred Ngole Mboula",
        "link": "http://arxiv.org/abs/2506.19410v1",
        "abstract": "This paper introduces a novel approach, Unsupervised Dataset Dictionary\nLearning (U-DaDiL), for totally unsupervised robust clustering applied to\nsitting posture identification. Traditional methods often lack adaptability to\ndiverse datasets and suffer from domain shift issues. U-DaDiL addresses these\nchallenges by aligning distributions from different datasets using Wasserstein\nbarycenter based representation. Experimental evaluations on the Office31\ndataset demonstrate significant improvements in cluster alignment accuracy.\nThis work also presents a promising step for addressing domain shift and robust\nclustering for unsupervised sitting posture identification"
    },
    {
        "date": "2025-06",
        "title": "Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models",
        "author": "Wanli Peng, Xin Chen, Hang Fu, XinYu He, Xue Yiming, and Juan Wen",
        "link": "http://arxiv.org/abs/2506.19889v1",
        "abstract": "Recent advances in large language models (LLMs) have made a profound impact\non our society and also raised new security concerns. Particularly, due to the\nremarkable inference ability of LLMs, the privacy violation attack (PVA),\nrevealed by Staab et al., introduces serious personal privacy issues. Existing\ndefense methods mainly leverage LLMs to anonymize the input query, which\nrequires costly inference time and cannot gain satisfactory defense\nperformance. Moreover, directly rejecting the PVA query seems like an effective\ndefense method, while the defense method is exposed, promoting the evolution of\nPVA. In this paper, we propose a novel defense paradigm based on\nretrieval-confused generation (RCG) of LLMs, which can efficiently and covertly\ndefend the PVA. We first design a paraphrasing prompt to induce the LLM to\nrewrite the \"user comments\" of the attack query to construct a disturbed\ndatabase. Then, we propose the most irrelevant retrieval strategy to retrieve\nthe desired user data from the disturbed database. Finally, the \"data comments\"\nare replaced with the retrieved user data to form a defended query, leading to\nresponding to the adversary with some wrong personal attributes, i.e., the\nattack fails. Extensive experiments are conducted on two datasets and eight\npopular LLMs to comprehensively evaluate the feasibility and the superiority of\nthe proposed defense method."
    },
    {
        "date": "2025-06",
        "title": "Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack",
        "author": "Xuesong Wang, Mo Li, Xingyan Shi, Zhaoqian Liu, and Shenghao Yang",
        "link": "http://arxiv.org/abs/2506.19886v1",
        "abstract": "Semantic communication has emerged as a promising neural network-based system\ndesign for 6G networks. Task-oriented semantic communication is a novel\nparadigm whose core goal is to efficiently complete specific tasks by\ntransmitting semantic information, optimizing communication efficiency and task\nperformance. The key challenge lies in preserving privacy while maintaining\ntask accuracy, as this scenario is susceptible to model inversion attacks. In\nsuch attacks, adversaries can restore or even reconstruct input data by\nanalyzing and processing model outputs, owing to the neural network-based\nnature of the systems. In addition, traditional systems use image quality\nindicators (such as PSNR or SSIM) to assess attack severity, which may be\ninadequate for task-oriented semantic communication, since visual differences\ndo not necessarily ensure semantic divergence. In this paper, we propose a\ndiffusion-based semantic communication framework, named DiffSem, that optimizes\nsemantic information reconstruction through a diffusion mechanism with\nself-referential label embedding to significantly improve task performance. Our\nmodel also compensates channel noise and adopt semantic information distortion\nto ensure the robustness of the system in various signal-to-noise ratio\nenvironments. To evaluate the attacker's effectiveness, we propose a new metric\nthat better quantifies the semantic fidelity of estimations from the adversary.\nExperimental results based on this criterion show that on the MNIST dataset,\nDiffSem improves the classification accuracy by 10.03%, and maintain stable\nperformance under dynamic channels. Our results further demonstrate that\nsignificant deviation exists between traditional image quality indicators and\nthe leakage of task-relevant semantic information."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Attacks on Deep Learning-Based False Data Injection Detection in Differential Relays",
        "author": "Ahmad Mohammad Saber, Aditi Maheshwari, Amr Youssef, and Deepa Kundur",
        "link": "http://arxiv.org/abs/2506.19302v1",
        "abstract": "The application of Deep Learning-based Schemes (DLSs) for detecting False\nData Injection Attacks (FDIAs) in smart grids has attracted significant\nattention. This paper demonstrates that adversarial attacks, carefully crafted\nFDIAs, can evade existing DLSs used for FDIA detection in Line Current\nDifferential Relays (LCDRs). We propose a novel adversarial attack framework,\nutilizing the Fast Gradient Sign Method, which exploits DLS vulnerabilities by\nintroducing small perturbations to LCDR remote measurements, leading to\nmisclassification of the FDIA as a legitimate fault while also triggering the\nLCDR to trip. We evaluate the robustness of multiple deep learning models,\nincluding multi-layer perceptrons, convolutional neural networks, long\nshort-term memory networks, and residual networks, under adversarial\nconditions. Our experimental results demonstrate that while these models\nperform well, they exhibit high degrees of vulnerability to adversarial\nattacks. For some models, the adversarial attack success rate exceeds 99.7%. To\naddress this threat, we introduce adversarial training as a proactive defense\nmechanism, significantly enhancing the models' ability to withstand adversarial\nFDIAs without compromising fault detection accuracy. Our results highlight the\nsignificant threat posed by adversarial attacks to DLS-based FDIA detection,\nunderscore the necessity for robust cybersecurity measures in smart grids, and\ndemonstrate the effectiveness of adversarial training in enhancing model\nrobustness against adversarial FDIAs."
    },
    {
        "date": "2025-06",
        "title": "Robust OOD Graph Learning via Mean Constraints and Noise Reduction",
        "author": "Yang Zhou, and Xiaoning Ren",
        "link": "http://arxiv.org/abs/2506.19281v1",
        "abstract": "Graph Out-of-Distribution (OOD) classification often suffers from sharp\nperformance drops, particularly under category imbalance and structural noise.\nThis work tackles two pressing challenges in this context: (1) the\nunderperformance of minority classes due to skewed label distributions, and (2)\ntheir heightened sensitivity to structural noise in graph data. To address\nthese problems, we propose two complementary solutions. First, Constrained Mean\nOptimization (CMO) improves minority class robustness by encouraging\nsimilarity-based instance aggregation under worst-case conditions. Second, the\nNeighbor-Aware Noise Reweighting (NNR) mechanism assigns dynamic weights to\ntraining samples based on local structural consistency, mitigating noise\ninfluence. We provide theoretical justification for our methods, and validate\ntheir effectiveness with extensive experiments on both synthetic and real-world\ndatasets, showing significant improvements in Graph OOD generalization and\nclassification accuracy. The code for our method is available at:\nhttps://anonymous.4open.science/r/CMO-NNR-2F30."
    },
    {
        "date": "2025-06",
        "title": "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation",
        "author": "Weichen Zhang, Dong Xu, Wanli Ouyang, and Wen Li",
        "link": "http://arxiv.org/abs/2506.19267v1",
        "abstract": "This paper proposes a new unsupervised domain adaptation approach called\nCollaborative and Adversarial Network (CAN), which uses the\ndomain-collaborative and domain-adversarial learning strategy for training the\nneural network. The domain-collaborative learning aims to learn domain-specific\nfeature representation to preserve the discriminability for the target domain,\nwhile the domain adversarial learning aims to learn domain-invariant feature\nrepresentation to reduce the domain distribution mismatch between the source\nand target domains. We show that these two learning strategies can be uniformly\nformulated as domain classifier learning with positive or negative weights on\nthe losses. We then design a collaborative and adversarial training scheme,\nwhich automatically learns domain-specific representations from lower blocks in\nCNNs through collaborative learning and domain-invariant representations from\nhigher blocks through adversarial learning. Moreover, to further enhance the\ndiscriminability in the target domain, we propose Self-Paced CAN (SPCAN), which\nprogressively selects pseudo-labeled target samples for re-training the\nclassifiers. We employ a self-paced learning strategy to select pseudo-labeled\ntarget samples in an easy-to-hard fashion. Comprehensive experiments on\ndifferent benchmark datasets, Office-31, ImageCLEF-DA, and VISDA-2017 for the\nobject recognition task, and UCF101-10 and HMDB51-10 for the video action\nrecognition task, show our newly proposed approaches achieve the\nstate-of-the-art performance, which clearly demonstrates the effectiveness of\nour proposed approaches for unsupervised domain adaptation."
    },
    {
        "date": "2025-06",
        "title": "Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning",
        "author": "Murtaza Rangwala, Richard O. Sinnott, and Rajkumar Buyya",
        "link": "http://arxiv.org/abs/2506.19260v1",
        "abstract": "Federated learning systems increasingly rely on diverse network topologies to\naddress scalability and organizational constraints. While existing privacy\nresearch focuses on gradient-based attacks, the privacy implications of network\ntopology knowledge remain critically understudied. We conduct the first\ncomprehensive analysis of topology-based privacy leakage across realistic\nadversarial knowledge scenarios, demonstrating that adversaries with varying\ndegrees of structural knowledge can infer sensitive data distribution patterns\neven under strong differential privacy guarantees. Through systematic\nevaluation of 4,720 attack instances, we analyze six distinct adversarial\nknowledge scenarios: complete topology knowledge and five partial knowledge\nconfigurations reflecting real-world deployment constraints. We propose three\ncomplementary attack vectors: communication pattern analysis, parameter\nmagnitude profiling, and structural position correlation, achieving success\nrates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.\nCritically, we find that 80% of realistic partial knowledge scenarios maintain\nattack effectiveness above security thresholds, with certain partial knowledge\nconfigurations achieving performance superior to the baseline complete\nknowledge scenario. To address these vulnerabilities, we propose and\nempirically validate structural noise injection as a complementary defense\nmechanism across 808 configurations, demonstrating up to 51.4% additional\nattack reduction when properly layered with existing privacy techniques. These\nresults establish that network topology represents a fundamental privacy\nvulnerability in federated learning systems while providing practical pathways\nfor mitigation through topology-aware defense mechanisms."
    },
    {
        "date": "2025-06",
        "title": "Robust Behavior Cloning Via Global Lipschitz Regularization",
        "author": "Shili Wu, Yizhao Jin, Puhua Niu, Aniruddha Datta, and Sean B. Andersson",
        "link": "http://arxiv.org/abs/2506.19250v1",
        "abstract": "Behavior Cloning (BC) is an effective imitation learning technique and has\neven been adopted in some safety-critical domains such as autonomous vehicles.\nBC trains a policy to mimic the behavior of an expert by using a dataset\ncomposed of only state-action pairs demonstrated by the expert, without any\nadditional interaction with the environment. However, During deployment, the\npolicy observations may contain measurement errors or adversarial disturbances.\nSince the observations may deviate from the true states, they can mislead the\nagent into making sub-optimal actions. In this work, we use a global Lipschitz\nregularization approach to enhance the robustness of the learned policy\nnetwork. We then show that the resulting global Lipschitz property provides a\nrobustness certificate to the policy with respect to different bounded norm\nperturbations. Then, we propose a way to construct a Lipschitz neural network\nthat ensures the policy robustness. We empirically validate our theory across\nvarious environments in Gymnasium. Keywords: Robust Reinforcement Learning;\nBehavior Cloning; Lipschitz Neural Network"
    },
    {
        "date": "2025-06",
        "title": "Enhancing Security in LLM Applications: A Performance Evaluation of Early Detection Systems",
        "author": "Valerii Gakh, and Hayretdin Bahsi",
        "link": "http://arxiv.org/abs/2506.19109v1",
        "abstract": "Prompt injection threatens novel applications that emerge from adapting LLMs\nfor various user tasks. The newly developed LLM-based software applications\nbecome more ubiquitous and diverse. However, the threat of prompt injection\nattacks undermines the security of these systems as the mitigation and defenses\nagainst them, proposed so far, are insufficient. We investigated the\ncapabilities of early prompt injection detection systems, focusing specifically\non the detection performance of techniques implemented in various open-source\nsolutions. These solutions are supposed to detect certain types of prompt\ninjection attacks, including the prompt leak. In prompt leakage attacks, an\nattacker maliciously manipulates the LLM into outputting its system\ninstructions, violating the system's confidentiality. Our study presents\nanalyzes of distinct prompt leakage detection techniques, and a comparative\nanalysis of several detection solutions, which implement those techniques. We\nidentify the strengths and weaknesses of these techniques and elaborate on\ntheir optimal configuration and usage in high-stake deployments. In one of the\nfirst studies on existing prompt leak detection solutions, we compared the\nperformances of LLM Guard, Vigil, and Rebuff. We concluded that the\nimplementations of canary word checks in Vigil and Rebuff were not effective at\ndetecting prompt leak attacks, and we proposed improvements for them. We also\nfound an evasion weakness in Rebuff's secondary model-based technique and\nproposed a mitigation. Then, the result of the comparison of LLM Guard, Vigil,\nand Rebuff at their peak performance revealed that Vigil is optimal for cases\nwhen minimal false positive rate is required, and Rebuff is the most optimal\nfor average needs."
    },
    {
        "date": "2025-06",
        "title": "NIC-RobustBench: A Comprehensive Open-Source Toolkit for Neural Image Compression and Robustness Analysis",
        "author": "Georgii Bychkov, Khaled Abud, Egor Kovalev, Alexander Gushchin, Dmitriy Vatolin, and Anastasia Antsiferova",
        "link": "http://arxiv.org/abs/2506.19051v1",
        "abstract": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI -- the first standard for\nend-to-end neural image compression (NIC) methods -- the question of evaluating\nNIC robustness has become critically significant. However, previous research\nhas been limited to a narrow range of codecs and attacks. To address this, we\npresent \\textbf{NIC-RobustBench}, the first open-source framework to evaluate\nNIC robustness and adversarial defenses' efficiency, in addition to comparing\nRate-Distortion (RD) performance. The framework includes the largest number of\ncodecs among all known NIC libraries and is easily scalable. The paper\ndemonstrates a comprehensive overview of the NIC-RobustBench framework and\nemploys it to analyze NIC robustness. Our code is available online at\nhttps://github.com/msu-video-group/NIC-RobustBench."
    },
    {
        "date": "2025-06",
        "title": "Amplifying Machine Learning Attacks Through Strategic Compositions",
        "author": "Yugeng Liu, Zheng Li, Hai Huang, Michael Backes, and Yang Zhang",
        "link": "http://arxiv.org/abs/2506.18870v1",
        "abstract": "Machine learning (ML) models are proving to be vulnerable to a variety of\nattacks that allow the adversary to learn sensitive information, cause\nmispredictions, and more. While these attacks have been extensively studied,\ncurrent research predominantly focuses on analyzing each attack type\nindividually. In practice, however, adversaries may employ multiple attack\nstrategies simultaneously rather than relying on a single approach. This\nprompts a crucial yet underexplored question: When the adversary has multiple\nattacks at their disposal, are they able to mount or amplify the effect of one\nattack with another? In this paper, we take the first step in studying the\nstrategic interactions among different attacks, which we define as attack\ncompositions. Specifically, we focus on four well-studied attacks during the\nmodel's inference phase: adversarial examples, attribute inference, membership\ninference, and property inference. To facilitate the study of their\ninteractions, we propose a taxonomy based on three stages of the attack\npipeline: preparation, execution, and evaluation. Using this taxonomy, we\nidentify four effective attack compositions, such as property inference\nassisting attribute inference at its preparation level and adversarial examples\nassisting property inference at its execution level. We conduct extensive\nexperiments on the attack compositions using three ML model architectures and\nthree benchmark image datasets. Empirical results demonstrate the effectiveness\nof these four attack compositions. We implement and release a modular reusable\ntoolkit, COAT. Arguably, our work serves as a call for researchers and\npractitioners to consider advanced adversarial settings involving multiple\nattack strategies, aiming to strengthen the security and robustness of AI\nsystems."
    },
    {
        "date": "2025-06",
        "title": "Multi-Agent Online Control with Adversarial Disturbances",
        "author": "Anas Barakat, John Lazarsfeld, Georgios Piliouras, and Antonios Varvitsiotis",
        "link": "http://arxiv.org/abs/2506.18814v1",
        "abstract": "Multi-agent control problems involving a large number of agents with\ncompeting and time-varying objectives are increasingly prevalent in\napplications across robotics, economics, and energy systems. In this paper, we\nstudy online control in multi-agent linear dynamical systems with disturbances.\nIn contrast to most prior work in multi-agent control, we consider an online\nsetting where disturbances are adversarial and where each agent seeks to\nminimize its own, adversarial sequence of convex losses. In this setting, we\ninvestigate the robustness of gradient-based controllers from single-agent\nonline control, with a particular focus on understanding how individual regret\nguarantees are influenced by the number of agents in the system. Under minimal\ncommunication assumptions, we prove near-optimal sublinear regret bounds that\nhold uniformly for all agents. Finally, when the objectives of the agents are\naligned, we show that the multi-agent control problem induces a time-varying\npotential game for which we derive equilibrium gap guarantees."
    },
    {
        "date": "2025-06",
        "title": "Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017",
        "author": "Zhaoyang Xu, and Yunbo Liu",
        "link": "http://arxiv.org/abs/2506.19877v1",
        "abstract": "Identifying suitable machine learning paradigms for intrusion detection\nremains critical for building effective and generalizable security solutions.\nIn this study, we present a controlled comparison of four representative models\n- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),\nOne-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on\nthe CICIDS2017 dataset under two scenarios: detecting known attack types and\ngeneralizing to previously unseen threats. Our results show that supervised MLP\nand CNN achieve near-perfect accuracy on familiar attacks but suffer drastic\nrecall drops on novel attacks. Unsupervised LOF attains moderate overall\naccuracy and high recall on unknown threats at the cost of elevated false\nalarms, while boundary-based OCSVM balances precision and recall best,\ndemonstrating robust detection across both scenarios. These findings offer\npractical guidance for selecting IDS models in dynamic network environments."
    },
    {
        "date": "2025-06",
        "title": "SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds",
        "author": "Mauricio Byrd Victorica, Gy\u00f6rgy D\u00e1n, and Henrik Sandberg",
        "link": "http://arxiv.org/abs/2506.18591v1",
        "abstract": "State-of-the-art convolutional neural network models for object detection and\nimage classification are vulnerable to physically realizable adversarial\nperturbations, such as patch attacks. Existing defenses have focused,\nimplicitly or explicitly, on single-patch attacks, leaving their sensitivity to\nthe number of patches as an open question or rendering them computationally\ninfeasible or inefficient against attacks consisting of multiple patches in the\nworst cases. In this work, we propose SpaNN, an attack detector whose\ncomputational complexity is independent of the expected number of adversarial\npatches. The key novelty of the proposed detector is that it builds an ensemble\nof binarized feature maps by applying a set of saliency thresholds to the\nneural activations of the first convolutional layer of the victim model. It\nthen performs clustering on the ensemble and uses the cluster features as the\ninput to a classifier for attack detection. Contrary to existing detectors,\nSpaNN does not rely on a fixed saliency threshold for identifying adversarial\nregions, which makes it robust against white box adversarial attacks. We\nevaluate SpaNN on four widely used data sets for object detection and\nclassification, and our results show that SpaNN outperforms state-of-the-art\ndefenses by up to 11 and 27 percentage points in the case of object detection\nand the case of image classification, respectively. Our code is available at\nhttps://github.com/gerkbyrd/SpaNN."
    },
    {
        "date": "2025-06",
        "title": "Towards Provable (In)Secure Model Weight Release Schemes",
        "author": "Xin Yang, Bintao Tang, Yuhao Wang, Zimo Ji, Terry Jingchen Zhang, and Wenyuan Jiang",
        "link": "http://arxiv.org/abs/2506.19874v2",
        "abstract": "Recent secure weight release schemes claim to enable open-source model\ndistribution while protecting model ownership and preventing misuse. However,\nthese approaches lack rigorous security foundations and provide only informal\nsecurity guarantees. Inspired by established works in cryptography, we\nformalize the security of weight release schemes by introducing several\nconcrete security definitions. We then demonstrate our definition's utility\nthrough a case study of TaylorMLP, a prominent secure weight release scheme.\nOur analysis reveals vulnerabilities that allow parameter extraction thus\nshowing that TaylorMLP fails to achieve its informal security goals. We hope\nthis work will advocate for rigorous research at the intersection of machine\nlearning and security communities and provide a blueprint for how future weight\nrelease schemes should be designed and evaluated."
    },
    {
        "date": "2025-06",
        "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks",
        "author": "Xiaodong Wu, Xiangman Li, and Jianbing Ni",
        "link": "http://arxiv.org/abs/2506.18543v1",
        "abstract": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs."
    },
    {
        "date": "2025-06",
        "title": "DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?",
        "author": "Francesco Marchiori, Marco Alecci, Luca Pajola, and Mauro Conti",
        "link": "http://arxiv.org/abs/2506.18516v1",
        "abstract": "Adversarial examples are small and often imperceptible perturbations crafted\nto fool machine learning models. These attacks seriously threaten the\nreliability of deep neural networks, especially in security-sensitive domains.\nEvasion attacks, a form of adversarial attack where input is modified at test\ntime to cause misclassification, are particularly insidious due to their\ntransferability: adversarial examples crafted against one model often fool\nother models as well. This property, known as adversarial transferability,\ncomplicates defense strategies since it enables black-box attacks to succeed\nwithout direct access to the victim model. While adversarial training is one of\nthe most widely adopted defense mechanisms, its effectiveness is typically\nevaluated on a narrow and homogeneous population of models. This limitation\nhinders the generalizability of empirical findings and restricts practical\nadoption.\n  In this work, we introduce DUMBer, an attack framework built on the\nfoundation of the DUMB (Dataset soUrces, Model architecture, and Balance)\nmethodology, to systematically evaluate the resilience of adversarially trained\nmodels. Our testbed spans multiple adversarial training techniques evaluated\nacross three diverse computer vision tasks, using a heterogeneous population of\nuniquely trained models to reflect real-world deployment variability. Our\nexperimental pipeline comprises over 130k evaluations spanning 13\nstate-of-the-art attack algorithms, allowing us to capture nuanced behaviors of\nadversarial training under varying threat models and dataset conditions. Our\nfindings offer practical, actionable insights for AI practitioners, identifying\nwhich defenses are most effective based on the model, dataset, and attacker\nsetup."
    },
    {
        "date": "2025-06",
        "title": "ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation",
        "author": "Trong-Vu Hoang, Quang-Binh Nguyen, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, and Trung-Nghia Le",
        "link": "http://arxiv.org/abs/2506.18493v1",
        "abstract": "Customizing image generation remains a core challenge in controllable image\nsynthesis. For single-concept generation, maintaining both identity\npreservation and prompt alignment is challenging. In multi-concept scenarios,\nrelying solely on a prompt without additional conditions like layout boxes or\nsemantic masks, often leads to identity loss and concept omission. In this\npaper, we introduce ShowFlow, a comprehensive framework designed to tackle\nthese challenges. We propose ShowFlow-S for single-concept image generation,\nand ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a\nKronA-WED adapter, which integrates a Kronecker adapter with weight and\nembedding decomposition, and employs a disentangled learning approach with a\nnovel attention regularization objective to enhance single-concept generation.\nBuilding on this foundation, ShowFlow-M directly reuses the learned models from\nShowFlow-S to support multi-concept generation without extra conditions,\nincorporating a Subject-Adaptive Matching Attention (SAMA) and a layout\nconsistency strategy as the plug-and-play module. Extensive experiments and\nuser studies validate ShowFlow's effectiveness, highlighting its potential in\nreal-world applications like advertising and virtual dressing."
    },
    {
        "date": "2025-06",
        "title": "Adaptive alert prioritisation in security operations centres via learning to defer with human feedback",
        "author": "Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, and C\u00e9cile Paris",
        "link": "http://arxiv.org/abs/2506.18462v1",
        "abstract": "Alert prioritisation (AP) is crucial for security operations centres (SOCs)\nto manage the overwhelming volume of alerts and ensure timely detection and\nresponse to genuine threats, while minimising alert fatigue. Although\npredictive AI can process large alert volumes and identify known patterns, it\nstruggles with novel and evolving scenarios that demand contextual\nunderstanding and nuanced judgement. A promising solution is Human-AI teaming\n(HAT), which combines human expertise with AI's computational capabilities.\nLearning to Defer (L2D) operationalises HAT by enabling AI to \"defer\" uncertain\nor unfamiliar cases to human experts. However, traditional L2D models rely on\nstatic deferral policies that do not evolve with experience, limiting their\nability to learn from human feedback and adapt over time. To overcome this, we\nintroduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral\nframework that leverages Deep Reinforcement Learning from Human Feedback\n(DRLHF) to optimise deferral decisions. By dynamically incorporating human\nfeedback, L2DHF continuously improves AP accuracy and reduces unnecessary\ndeferrals, enhancing SOC effectiveness and easing analyst workload. Experiments\non two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate\nthat L2DHF significantly outperforms baseline models. Notably, it achieves\n13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on\nCICIDS2017. It also reduces misprioritisations, for example, by 98% for\nhigh-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for\nexample, by 37% on UNSW-NB15, directly reducing analyst workload. These gains\nare achieved with efficient execution, underscoring L2DHF's practicality for\nreal-world SOC deployment."
    },
    {
        "date": "2025-06",
        "title": "How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models",
        "author": "Feng He, Zhenyang Liu, Marco Valentino, and Zhixue Zhao",
        "link": "http://arxiv.org/abs/2506.18428v1",
        "abstract": "Model editing offers a low-cost technique to inject or correct a particular\nbehavior in a pre-trained model without extensive retraining, supporting\napplications such as factual correction and bias mitigation. Despite this\ncommon practice, it remains unknown whether edits persist after fine-tuning or\nwhether they are inadvertently reversed. This question has fundamental\npractical implications. For example, if fine-tuning removes prior edits, it\ncould serve as a defence mechanism against hidden malicious edits. Vice versa,\nthe unintended removal of edits related to bias mitigation could pose serious\nsafety concerns. We systematically investigate the interaction between model\nediting and fine-tuning in the context of T2I diffusion models, which are known\nto exhibit biases and generate inappropriate content. Our study spans two T2I\nmodel families (Stable Diffusion and FLUX), two sota editing techniques, and\nthree fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive\nempirical analysis across diverse editing tasks and evaluation metrics, our\nfindings reveal a trend: edits generally fail to persist through fine-tuning,\neven when fine-tuning is tangential or unrelated to the edits. Notably, we\nobserve that DoRA exhibits the strongest edit reversal effect. At the same\ntime, among editing methods, UCE demonstrates greater robustness, retaining\nsignificantly higher efficacy post-fine-tuning compared to ReFACT. These\nfindings highlight a crucial limitation in current editing methodologies,\nemphasizing the need for more robust techniques to ensure reliable long-term\ncontrol and alignment of deployed AI systems. These findings have dual\nimplications for AI safety: they suggest that fine-tuning could serve as a\nremediation mechanism for malicious edits while simultaneously highlighting the\nneed for re-editing after fine-tuning to maintain beneficial safety and\nalignment properties."
    },
    {
        "date": "2025-06",
        "title": "Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies",
        "author": "Junchao Fan, Xuyang Lei, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2506.18304v1",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising paradigm for\nautonomous driving. However, despite their advanced capabilities, DRL-based\npolicies remain highly vulnerable to adversarial attacks, posing serious safety\nrisks in real-world deployments. Investigating such attacks is crucial for\nrevealing policy vulnerabilities and guiding the development of more robust\nautonomous systems. While prior attack methods have made notable progress, they\nstill face several challenges: 1) they often rely on high-frequency attacks,\nyet critical attack opportunities are typically context-dependent and\ntemporally sparse, resulting in inefficient attack patterns; 2) restricting\nattack frequency can improve efficiency but often results in unstable training\ndue to the adversary's limited exploration. To address these challenges, we\npropose an adaptive expert-guided adversarial attack method that enhances both\nthe stability and efficiency of attack policy training. Our method first\nderives an expert policy from successful attack demonstrations using imitation\nlearning, strengthened by an ensemble Mixture-of-Experts architecture for\nrobust generalization across scenarios. This expert policy then guides a\nDRL-based adversary through a KL-divergence regularization term. Due to the\ndiversity of scenarios, expert policies may be imperfect. To address this, we\nfurther introduce a performance-aware annealing strategy that gradually reduces\nreliance on the expert as the adversary improves. Extensive experiments\ndemonstrate that our method achieves outperforms existing approaches in terms\nof collision rate, attack efficiency, and training stability, especially in\ncases where the expert policy is sub-optimal."
    },
    {
        "date": "2025-06",
        "title": "ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments",
        "author": "Yu Liu, Yangtao Meng, Xianfei Pan, Jie Jiang, and Changhao Chen",
        "link": "http://arxiv.org/abs/2506.18268v1",
        "abstract": "Thermal cameras capture environmental data through heat emission, a\nfundamentally different mechanism compared to visible light cameras, which rely\non pinhole imaging. As a result, traditional visual relocalization methods\ndesigned for visible light images are not directly applicable to thermal\nimages. Despite significant advancements in deep learning for camera\nrelocalization, approaches specifically tailored for thermal camera-based\nrelocalization remain underexplored. To address this gap, we introduce\nThermalLoc, a novel end-to-end deep learning method for thermal image\nrelocalization. ThermalLoc effectively extracts both local and global features\nfrom thermal images by integrating EfficientNet with Transformers, and performs\nabsolute pose regression using two MLP networks. We evaluated ThermalLoc on\nboth the publicly available thermal-odometry dataset and our own dataset. The\nresults demonstrate that ThermalLoc outperforms existing representative methods\nemployed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,\nand RobustLoc, achieving superior accuracy and robustness."
    },
    {
        "date": "2025-06",
        "title": "Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability",
        "author": "Jongoh Jeong, Hunmin Yang, Jaeseok Jeong, and Kuk-Jin Yoon",
        "link": "http://arxiv.org/abs/2506.18248v1",
        "abstract": "Generative adversarial attacks train a perturbation generator on a white-box\nsurrogate model and subsequently apply the crafted perturbations to unseen\nblack-box victim models. In contrast to iterative attacks, these methods\ndeliver superior inference-time efficiency, scalability, and transferability;\nhowever, up until now, existing studies have not fully exploited the\nrepresentational capacity of generative models to preserve and harness semantic\ninformation. Specifically, the intermediate activations of the generator encode\nrich semantic features--object boundaries and coarse shapes--that remain\nunder-exploited, thereby limiting the alignment of perturbations with\nobject-salient regions which are critical for adversarial transferability. To\nremedy this, we introduce a semantic structure-aware attack framework based on\nthe Mean Teacher, which serves as a temporally smoothed feature reference. With\nthis smoothed reference, we further direct semantic consistency between the\nearly-layer activations in the student and those of the semantically rich\nteacher by feature distillation. By anchoring perturbation synthesis to the\nsemantically salient early intermediate blocks within the generator based on\nempirical findings, our method guides progressive adversarial perturbation on\nregions that substantially enhance adversarial transferability. We conduct\nextensive experiments over diverse models, domains and tasks to demonstrate\nconsistent improvements relative to state-of-the-art generative attacks,\ncomprehensively evaluated using conventional metrics and our newly proposed\nAccidental Correction Rate (ACR)."
    },
    {
        "date": "2025-06",
        "title": "Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection",
        "author": "Quan Zhou, Gan Luo, Qiang Hu, Qingyong Zhang, Jinhua Zhang, Yinjiao Tian, Qiang Li, and Zhiwei Wang",
        "link": "http://arxiv.org/abs/2506.18134v1",
        "abstract": "Polyp detection is crucial for colorectal cancer screening, yet existing\nmodels are limited by the scale and diversity of available data. While\ngenerative models show promise for data augmentation, current methods mainly\nfocus on enhancing polyp diversity, often overlooking the critical issue of\nfalse positives. In this paper, we address this gap by proposing an adversarial\ndiffusion framework to synthesize high-value false positives. The extensive\nvariability of negative backgrounds presents a significant challenge in false\npositive synthesis. To overcome this, we introduce two key innovations: First,\nwe design a regional noise matching strategy to construct a negative synthesis\nspace using polyp detection datasets. This strategy trains a negative-centric\ndiffusion model by masking polyp regions, ensuring the model focuses\nexclusively on learning diverse background patterns. Second, we introduce the\nDetector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs\nthe negative synthesis process to disrupt a pre-trained detector's decision,\nguiding the negative-centric diffusion model to generate high-value,\ndetector-confusing false positives instead of low-value, ordinary backgrounds.\nOur approach is the first to apply adversarial diffusion to lesion detection,\nestablishing a new paradigm for targeted false positive synthesis and paving\nthe way for more reliable clinical applications in colorectal cancer screening.\nExtensive results on public and in-house datasets verify the superiority of our\nmethod over the current state-of-the-arts, with our synthesized data improving\nthe detectors by at least 2.6% and 2.7% in F1-score, respectively, over the\nbaselines. Codes are at https://github.com/Huster-Hq/DADA."
    },
    {
        "date": "2025-06",
        "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
        "author": "Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, Weiliang Deng, Yubin Guo, Tian Nian, Xuanbing Xie, Qiangyu Chen, Kailun Su, Tianling Xu, Guodong Liu, Mengkang Hu, Huan-ang Gao, Kaixuan Wang, Zhixuan Liang, Yusen Qin, Xiaokang Yang, Ping Luo, and Yao Mu",
        "link": "http://arxiv.org/abs/2506.18088v1",
        "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation."
    },
    {
        "date": "2025-06",
        "title": "Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models",
        "author": "Huaiying Luo, and Cheng Ji",
        "link": "http://arxiv.org/abs/2506.18087v1",
        "abstract": "With the widespread application of edge computing and cloud systems in\nAI-driven applications, how to maintain efficient performance while ensuring\ndata privacy has become an urgent security issue. This paper proposes a\nfederated learning-based data collaboration method to improve the security of\nedge cloud AI systems, and use large-scale language models (LLMs) to enhance\ndata privacy protection and system robustness. Based on the existing federated\nlearning framework, this method introduces a secure multi-party computation\nprotocol, which optimizes the data aggregation and encryption process between\ndistributed nodes by using LLM to ensure data privacy and improve system\nefficiency. By combining advanced adversarial training techniques, the model\nenhances the resistance of edge cloud AI systems to security threats such as\ndata leakage and model poisoning. Experimental results show that the proposed\nmethod is 15% better than the traditional federated learning method in terms of\ndata protection and model robustness."
    },
    {
        "date": "2025-06",
        "title": "Distributionally robust minimization in meta-learning for system identification",
        "author": "Matteo Rufolo, Dario Piga, and Marco Forgione",
        "link": "http://arxiv.org/abs/2506.18074v1",
        "abstract": "Meta learning aims at learning how to solve tasks, and thus it allows to\nestimate models that can be quickly adapted to new scenarios. This work\nexplores distributionally robust minimization in meta learning for system\nidentification. Standard meta learning approaches optimize the expected loss,\noverlooking task variability. We use an alternative approach, adopting a\ndistributionally robust optimization paradigm that prioritizes high-loss tasks,\nenhancing performance in worst-case scenarios. Evaluated on a meta model\ntrained on a class of synthetic dynamical systems and tested in both\nin-distribution and out-of-distribution settings, the proposed approach allows\nto reduce failures in safety-critical applications."
    },
    {
        "date": "2025-06",
        "title": "On the Robustness of Human-Object Interaction Detection against Distribution Shift",
        "author": "Chi Xie, Shuang Liang, Jie Li, Feng Zhu, Rui Zhao, Yichen Wei, and Shengjie Zhao",
        "link": "http://arxiv.org/abs/2506.18021v1",
        "abstract": "Human-Object Interaction (HOI) detection has seen substantial advances in\nrecent years. However, existing works focus on the standard setting with ideal\nimages and natural distribution, far from practical scenarios with inevitable\ndistribution shifts. This hampers the practical applicability of HOI detection.\nIn this work, we investigate this issue by benchmarking, analyzing, and\nenhancing the robustness of HOI detection models under various distribution\nshifts. We start by proposing a novel automated approach to create the first\nrobustness evaluation benchmark for HOI detection. Subsequently, we evaluate\nmore than 40 existing HOI detection models on this benchmark, showing their\ninsufficiency, analyzing the features of different frameworks, and discussing\nhow the robustness in HOI is different from other tasks. With the insights from\nsuch analyses, we propose to improve the robustness of HOI detection methods\nthrough: (1) a cross-domain data augmentation integrated with mixup, and (2) a\nfeature fusion strategy with frozen vision foundation models. Both are simple,\nplug-and-play, and applicable to various methods. Our experimental results\ndemonstrate that the proposed approach significantly increases the robustness\nof various methods, with benefits on standard benchmarks, too. The dataset and\ncode will be released."
    },
    {
        "date": "2025-06",
        "title": "Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning",
        "author": "Thomas Boudou, Batiste Le Bars, Nirupam Gupta, and Aur\u00e9lien Bellet",
        "link": "http://arxiv.org/abs/2506.18020v1",
        "abstract": "Robust distributed learning algorithms aim to maintain good performance in\ndistributed and federated settings, even in the presence of misbehaving\nworkers. Two primary threat models have been studied: Byzantine attacks, where\nmisbehaving workers can send arbitrarily corrupted updates, and data poisoning\nattacks, where misbehavior is limited to manipulation of local training data.\nWhile prior work has shown comparable optimization error under both threat\nmodels, a fundamental question remains open: How do these threat models impact\ngeneralization? Empirical evidence suggests a gap between the two threat\nmodels, yet it remains unclear whether it is fundamental or merely an artifact\nof suboptimal attacks. In this work, we present the first theoretical\ninvestigation into this problem, formally showing that Byzantine attacks are\nintrinsically more harmful to generalization than data poisoning. Specifically,\nwe prove that: (i) under data poisoning, the uniform algorithmic stability of a\nrobust distributed learning algorithm, with optimal optimization error,\ndegrades by an additive factor of $\\varTheta ( \\frac{f}{n-f} )$, with $f$ the\nnumber of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine\nattacks, the degradation is in $\\mathcal{O} \\big( \\sqrt{ \\frac{f}{n-2f}}\n\\big)$.This difference in stability leads to a generalization error gap that is\nespecially significant as $f$ approaches its maximum value $\\frac{n}{2}$."
    },
    {
        "date": "2025-06",
        "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android & OP-TEE",
        "author": "Seongjin Kim, Sanguk Yun, and Jungho Jang",
        "link": "http://arxiv.org/abs/2506.17988v1",
        "abstract": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
    },
    {
        "date": "2025-06",
        "title": "Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models",
        "author": "Mischa Dombrowski, and Bernhard Kainz",
        "link": "http://arxiv.org/abs/2506.17975v1",
        "abstract": "Synthetic data has recently reached a level of visual fidelity that makes it\nnearly indistinguishable from real data, offering great promise for\nprivacy-preserving data sharing in medical imaging. However, fully synthetic\ndatasets still suffer from significant limitations: First and foremost, the\nlegal aspect of sharing synthetic data is often neglected and data regulations,\nsuch as the GDPR, are largley ignored. Secondly, synthetic models fall short of\nmatching the performance of real data, even for in-domain downstream\napplications. Recent methods for image generation have focused on maximising\nimage diversity instead of fidelity solely to improve the mode coverage and\ntherefore the downstream performance of synthetic data. In this work, we shift\nperspective and highlight how maximizing diversity can also be interpreted as\nprotecting natural persons from being singled out, which leads to predicate\nsingling-out (PSO) secure synthetic datasets. Specifically, we propose a\ngeneralisable framework for training diffusion models on personal data which\nleads to unpersonal synthetic datasets achieving performance within one\npercentage point of real-data models while significantly outperforming\nstate-of-the-art methods that do not ensure privacy. Our code is available at\nhttps://github.com/MischaD/Trichotomy."
    },
    {
        "date": "2025-06",
        "title": "LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN",
        "author": "Shefali Goel, Vinod Kumar Verma, and Abhishek Verma",
        "link": "http://arxiv.org/abs/2506.17911v1",
        "abstract": "Routing Protocol for Low-Power and Lossy Networks (RPL) is an\nenergy-efficient routing solution for IPv6 over Low-Power Wireless Personal\nArea Networks (6LoWPAN), recommended for resource-constrained devices. While\nRPL offers significant benefits, its security vulnerabilities pose challenges,\nparticularly due to unauthenticated control messages used to establish and\nmaintain routing information. These messages are susceptible to manipulation,\nenabling malicious nodes to inject false routing data. A notable security\nconcern is the Routing Table Falsification (RTF) attack, where attackers forge\nDestination Advertisement Object (DAO) messages to promote fake routes via a\nparent nodes routing table. Experimental results indicate that RTF attacks\nsignificantly reduce packet delivery ratio, increase end-to-end delay, and\nleverage power consumption. Currently, no effective countermeasures exist in\nthe literature, reinforcing the need for a security solution to prevent network\ndisruption and protect user applications. This paper introduces a Lightweight\nSecurity Solution against Routing Table Falsification Attack (LiSec-RTF),\nleveraging Physical Unclonable Functions (PUFs) to generate unique\nauthentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact\nwhile considering the resource limitations of 6LoWPAN devices in both static\nand mobile scenarios. Our testbed experiments indicate that LiSec-RTF\nsignificantly improves network performance compared to standard RPL under RTF\nattacks, thereby ensuring reliable and efficient operation."
    },
    {
        "date": "2025-06",
        "title": "Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases",
        "author": "Huanjia Zhu, Yishu Liu, Xiaozhao Fang, Guangming Lu, and Bingzhi Chen",
        "link": "http://arxiv.org/abs/2506.17903v1",
        "abstract": "Existing Medical Visual Question Answering (Med-VQA) models often suffer from\nlanguage biases, where spurious correlations between question types and answer\ncategories are inadvertently established. To address these issues, we propose a\nnovel Cause-Effect Driven Optimization framework called CEDO, that incorporates\nthree well-established mechanisms, i.e., Modality-driven Heterogeneous\nOptimization (MHO), Gradient-guided Modality Synergy (GMS), and\nDistribution-adapted Loss Rescaling (DLR), for comprehensively mitigating\nlanguage biases from both causal and effectual perspectives. Specifically, MHO\nemploys adaptive learning rates for specific modalities to achieve\nheterogeneous optimization, thus enhancing robust reasoning capabilities.\nAdditionally, GMS leverages the Pareto optimization method to foster\nsynergistic interactions between modalities and enforce gradient orthogonality\nto eliminate bias updates, thereby mitigating language biases from the effect\nside, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive\nweights to individual losses to ensure balanced learning across all answer\ncategories, effectively alleviating language biases from the cause side, i.e.,\nimbalance biases within datasets. Extensive experiments on multiple traditional\nand bias-sensitive benchmarks consistently demonstrate the robustness of CEDO\nover state-of-the-art competitors."
    },
    {
        "date": "2025-06",
        "title": "An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network",
        "author": "Yining Pang, and Chenghan Li",
        "link": "http://arxiv.org/abs/2506.19871v1",
        "abstract": "Insurance fraud detection represents a pivotal advancement in modern\ninsurance service, providing intelligent and digitalized monitoring to enhance\nmanagement and prevent fraud. It is crucial for ensuring the security and\nefficiency of insurance systems. Although AI and machine learning algorithms\nhave demonstrated strong performance in detecting fraudulent claims, the\nabsence of standardized defense mechanisms renders current systems vulnerable\nto emerging adversarial threats. In this paper, we propose a GAN-based approach\nto conduct adversarial attacks on fraud detection systems. Our results indicate\nthat an attacker, without knowledge of the training data or internal model\ndetails, can generate fraudulent cases that are classified as legitimate with a\n99\\% attack success rate (ASR). By subtly modifying real insurance records and\nclaims, adversaries can significantly increase the fraud risk, potentially\nbypassing compromised detection systems. These findings underscore the urgent\nneed to enhance the robustness of insurance fraud detection models against\nadversarial manipulation, thereby ensuring the stability and reliability of\ndifferent insurance systems."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval",
        "author": "Tam Trinh, Manh Nguyen, and Truong-Son Hy",
        "link": "http://arxiv.org/abs/2506.17878v1",
        "abstract": "The rapid spread of misinformation in the digital era poses significant\nchallenges to public discourse, necessitating robust and scalable fact-checking\nsolutions. Traditional human-led fact-checking methods, while credible,\nstruggle with the volume and velocity of online content, prompting the\nintegration of automated systems powered by Large Language Models (LLMs).\nHowever, existing automated approaches often face limitations, such as handling\ncomplex claims, ensuring source credibility, and maintaining transparency. This\npaper proposes a novel multi-agent system for automated fact-checking that\nenhances accuracy, efficiency, and explainability. The system comprises four\nspecialized agents: an Input Ingestion Agent for claim decomposition, a Query\nGeneration Agent for formulating targeted subqueries, an Evidence Retrieval\nAgent for sourcing credible evidence, and a Verdict Prediction Agent for\nsynthesizing veracity judgments with human-interpretable explanations.\nEvaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system\nachieves a 12.3% improvement in Macro F1-score over baseline methods. The\nsystem effectively decomposes complex claims, retrieves reliable evidence from\ntrusted sources, and generates transparent explanations for verification\ndecisions. Our approach contributes to the growing field of automated\nfact-checking by providing a more accurate, efficient, and transparent\nverification methodology that aligns with human fact-checking practices while\nmaintaining scalability for real-world applications. Our source code is\navailable at https://github.com/HySonLab/FactAgent"
    },
    {
        "date": "2025-06",
        "title": "DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation",
        "author": "Jiaming Hu, Debarghya Mukherjee, and Ioannis Ch. Paschalidis",
        "link": "http://arxiv.org/abs/2506.17874v2",
        "abstract": "In many real-world applications, ensuring the robustness and stability of\ndeep neural networks (DNNs) is crucial, particularly for image classification\ntasks that encounter various input perturbations. While data augmentation\ntechniques have been widely adopted to enhance the resilience of a trained\nmodel against such perturbations, there remains significant room for\nimprovement in robustness against corrupted data and adversarial attacks\nsimultaneously. To address this challenge, we introduce DRO-Augment, a novel\nframework that integrates Wasserstein Distributionally Robust Optimization\n(W-DRO) with various data augmentation strategies to improve the robustness of\nthe models significantly across a broad spectrum of corruptions. Our method\noutperforms existing augmentation methods under severe data perturbations and\nadversarial attack scenarios while maintaining the accuracy on the clean\ndatasets on a range of benchmark datasets, including but not limited to\nCIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we\nestablish novel generalization error bounds for neural networks trained using a\ncomputationally efficient, variation-regularized loss function closely related\nto the W-DRO problem."
    },
    {
        "date": "2025-06",
        "title": "LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation",
        "author": "Dinesh Reddy Ankireddy, Sudipta Paria, Aritra Dasgupta, Sandip Ray, and Swarup Bhunia",
        "link": "http://arxiv.org/abs/2506.17865v1",
        "abstract": "Ensuring the security of modern System-on-Chip (SoC) designs poses\nsignificant challenges due to increasing complexity and distributed assets\nacross the intellectual property (IP) blocks. Formal property verification\n(FPV) provides the capability to model and validate design behaviors through\nsecurity properties with model checkers; however, current practices require\nsignificant manual efforts to create such properties, making them\ntime-consuming, costly, and error-prone. The emergence of Large Language Models\n(LLMs) has showcased remarkable proficiency across diverse domains, including\nHDL code generation and verification tasks. Current LLM-based techniques often\nproduce vacuous assertions and lack efficient prompt generation, comprehensive\nverification, and bug detection. This paper presents LASA, a novel framework\nthat leverages LLMs and retrieval-augmented generation (RAG) to produce\nnon-vacuous security properties and SystemVerilog Assertions (SVA) from design\nspecifications and related documentation for bus-based SoC designs. LASA\nintegrates commercial EDA tool for FPV to generate coverage metrics and\niteratively refines prompts through a feedback loop to enhance coverage. The\neffectiveness of LASA is validated through various open-source SoC designs,\ndemonstrating high coverage values with an average of ~88\\%, denoting\ncomprehensive verification through efficient generation of security properties\nand SVAs. LASA also demonstrates bug detection capabilities, identifying five\nunique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition."
    },
    {
        "date": "2025-06",
        "title": "Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling",
        "author": "Kazuki Naganuma, and Shunsuke Ono",
        "link": "http://arxiv.org/abs/2506.17838v1",
        "abstract": "This paper proposes a foreground-background separation (FBS) method with a\nnovel foreground model based on convolutional sparse representation (CSR). In\norder to analyze the dynamic and static components of videos acquired under\nundesirable conditions, such as hardware, environmental, and power limitations,\nit is essential to establish an FBS method that can handle videos with low\nframe rates and various types of noise. Existing FBS methods have two\nlimitations that prevent us from accurately separating foreground and\nbackground components from such degraded videos. First, they only capture\neither data-specific or general features of the components. Second, they do not\ninclude explicit models for various types of noise to remove them in the FBS\nprocess. To this end, we propose a robust FBS method with a CSR-based\nforeground model. This model can adaptively capture specific spatial structures\nscattered in imaging data. Then, we formulate FBS as a constrained multiconvex\noptimization problem that incorporates CSR, functions that capture general\nfeatures, and explicit noise characterization functions for multiple types of\nnoise. Thanks to these functions, our method captures both data-specific and\ngeneral features to accurately separate the components from various types of\nnoise even under low frame rates. To obtain a solution of the optimization\nproblem, we develop an algorithm that alternately solves its two convex\nsubproblems by newly established algorithms. Experiments demonstrate the\nsuperiority of our method over existing methods using two types of degraded\nvideos: infrared and microscope videos."
    },
    {
        "date": "2025-06",
        "title": "Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability",
        "author": "Md Asif Ul Hoq Khan, MD Zahedul Islam, Istiaq Ahmed, Md Masud Karim Rabbi, Farhana Rahman Anonna, MD Abdul Fahim Zeeshan, Mehedi Hasan Ridoy, Bivash Ranjan Chowdhury, Md Nazmul Shakir Rabbi, and GM Alamin Sadnan",
        "link": "http://arxiv.org/abs/2506.19870v1",
        "abstract": "Peer-to-peer trading and the move to decentralized grids have reshaped the\nenergy markets in the United States. Notwithstanding, such developments lead to\nnew challenges, mainly regarding the safety and authenticity of energy trade.\nThis study aimed to develop and build a secure, intelligent, and efficient\nenergy transaction system for the decentralized US energy market. This research\ninterlinks the technological prowess of blockchain and artificial intelligence\n(AI) in a novel way to solve long-standing challenges in the distributed energy\nmarket, specifically those of security, fraudulent behavior detection, and\nmarket reliability. The dataset for this research is comprised of more than 1.2\nmillion anonymized energy transaction records from a simulated peer-to-peer\n(P2P) energy exchange network emulating real-life blockchain-based American\nmicrogrids, including those tested by LO3 Energy and Grid+ Labs. Each record\ncontains detailed fields of transaction identifier, timestamp, energy volume\n(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier\n(hashed for privacy), smart meter readings, geolocation regions, and settlement\nconfirmation status. The dataset also includes system-calculated behavior\nmetrics of transaction rate, variability of energy production, and historical\npricing patterns. The system architecture proposed involves the integration of\ntwo layers, namely a blockchain layer and artificial intelligence (AI) layer,\neach playing a unique but complementary function in energy transaction securing\nand market intelligence improvement. The machine learning models used in this\nresearch were specifically chosen for their established high performance in\nclassification tasks, specifically in the identification of energy transaction\nfraud in decentralized markets."
    },
    {
        "date": "2025-06",
        "title": "AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator",
        "author": "Md. Kamrul Hossain, Walid Aljoby, Anis Elgabli, Ahmed M. Abdelmoniem, and Khaled A. Harras",
        "link": "http://arxiv.org/abs/2506.17805v1",
        "abstract": "Federated Learning (FL) enables collaborative learning without exposing\nclients' data. While clients only share model updates with the aggregator,\nstudies reveal that aggregators can infer sensitive information from these\nupdates. Secure Aggregation (SA) protects individual updates during\ntransmission; however, recent work demonstrates a critical vulnerability where\nadversarial aggregators manipulate client selection to bypass SA protections,\nconstituting a Biased Selection Attack (BSA). Although verifiable random\nselection prevents BSA, it precludes informed client selection essential for FL\nperformance. We propose Adversarial Robust Federated Learning (AdRo-FL), which\nsimultaneously enables: informed client selection based on client utility, and\nrobust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL\nimplements two client selection frameworks tailored for distinct settings. The\nfirst framework assumes clients are grouped into clusters based on mutual\ntrust, such as different branches of an organization. The second framework\nhandles distributed clients where no trust relationships exist between them.\nFor the cluster-oriented setting, we propose a novel defense against BSA by (1)\nenforcing a minimum client selection quota from each cluster, supervised by a\ncluster-head in every round, and (2) introducing a client utility function to\nprioritize efficient clients. For the distributed setting, we design a\ntwo-phase selection protocol: first, the aggregator selects the top clients\nbased on our utility-driven ranking; then, a verifiable random function (VRF)\nensures a BSA-resistant final selection. AdRo-FL also applies quantization to\nreduce communication overhead and sets strict transmission deadlines to improve\nenergy efficiency. AdRo-FL achieves up to $1.85\\times$ faster time-to-accuracy\nand up to $1.06\\times$ higher final accuracy compared to insecure baselines."
    },
    {
        "date": "2025-06",
        "title": "AI Safety vs. AI Security: Demystifying the Distinction and Boundaries",
        "author": "Zhiqiang Lin, Huan Sun, and Ness Shroff",
        "link": "http://arxiv.org/abs/2506.18932v1",
        "abstract": "Artificial Intelligence (AI) is rapidly being integrated into critical\nsystems across various domains, from healthcare to autonomous vehicles. While\nits integration brings immense benefits, it also introduces significant risks,\nincluding those arising from AI misuse. Within the discourse on managing these\nrisks, the terms \"AI Safety\" and \"AI Security\" are often used, sometimes\ninterchangeably, resulting in conceptual confusion. This paper aims to\ndemystify the distinction and delineate the precise research boundaries between\nAI Safety and AI Security. We provide rigorous definitions, outline their\nrespective research focuses, and explore their interdependency, including how\nsecurity breaches can precipitate safety failures and vice versa. Using clear\nanalogies from message transmission and building construction, we illustrate\nthese distinctions. Clarifying these boundaries is crucial for guiding precise\nresearch directions, fostering effective cross-disciplinary collaboration,\nenhancing policy effectiveness, and ultimately, promoting the deployment of\ntrustworthy AI systems."
    },
    {
        "date": "2025-06",
        "title": "Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs",
        "author": "Shuang Ao, Yi Dong, Jinwei Hu, and Sarvapali Ramchurn",
        "link": "http://arxiv.org/abs/2506.18931v1",
        "abstract": "Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA)\nenhances adaptability while reducing computational costs. However, fine-tuning\ncan compromise safety alignment, even with benign data, increasing\nsusceptibility to harmful outputs. Existing safety alignment methods struggle\nto capture complex parameter shifts, leading to suboptimal safety-utility\ntrade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a\nnovel pruning-based approach that selectively removes LoRA layers that weaken\nsafety alignment, improving safety while preserving performance. At its core,\nwe introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric\nthat effectively detects safety misalignment in LoRA-adapted models. We conduct\nextensive experiments on LLMs fine-tuned with mixed of benign and malicious\ndata, and purely benign datasets, evaluating SPLoRA across utility, safety, and\nreliability metrics. Results demonstrate that SPLoRA outperforms\nstate-of-the-art safety alignment techniques, significantly reducing safety\nrisks while maintaining or improving model performance and reliability.\nAdditionally, SPLoRA reduces inference overhead, making it a scalable and\nefficient solution for deploying safer and more reliable LLMs. The code is\navailable at https://github.com/AoShuang92/SPLoRA."
    },
    {
        "date": "2025-06",
        "title": "Optimization-Free Patch Attack on Stereo Depth Estimation",
        "author": "Hangcheng Liu, Xu Kuang, Xingshuo Han, Xingwan Wu, Haoran Ou, Shangwei Guo, Xingyi Huang, Tao Xiang, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.17632v1",
        "abstract": "Stereo Depth Estimation (SDE) is essential for scene understanding in\nvision-based systems like autonomous driving. However, recent studies show that\nSDE models are vulnerable to adversarial attacks, which are often limited to\nunrealistic settings, e.g., digital perturbations on separate stereo views in\nstatic scenes, restricting their real-world applicability. This raises a\ncritical question: how can we design physically realizable, scene-adaptive, and\ntransferable attacks against SDE under realistic constraints?\n  To answer this, we make two key contributions. First, we propose a unified\nattack framework that extends optimization-based techniques to four core stages\nof stereo matching: feature extraction, cost-volume construction, cost\naggregation, and disparity regression. A comprehensive stage-wise evaluation\nacross 9 mainstream SDE models, under constraints like photometric consistency,\nreveals that optimization-based patches suffer from poor transferability.\nInterestingly, partially transferable patches suggest that patterns, rather\nthan pixel-level perturbations, may be key to generalizable attacks. Motivated\nby this, we present PatchHunter, the first optimization-free adversarial patch\nattack against SDE. PatchHunter formulates patch generation as a reinforcement\nlearning-driven search over a structured space of visual patterns crafted to\ndisrupt SDE assumptions.\n  We validate PatchHunter across three levels: the KITTI dataset, the CARLA\nsimulator, and real-world vehicle deployment. PatchHunter not only surpasses\noptimization-based methods in effectiveness but also achieves significantly\nbetter black-box transferability. Even under challenging physical conditions\nlike low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),\nwhereas optimization-based methods fail."
    },
    {
        "date": "2025-06",
        "title": "List-Decodable Byzantine Robust PIR: Lower Communication Complexity, Higher Byzantine Tolerance, Smaller List Size",
        "author": "Pengzhen Ke, Liang Feng Zhang, Huaxiong Wang, and Li-Ping Wang",
        "link": "http://arxiv.org/abs/2506.17625v1",
        "abstract": "Private Information Retrieval (PIR) is a privacy-preserving primitive in\ncryptography. Significant endeavors have been made to address the variant of\nPIR concerning the malicious servers. Among those endeavors, list-decodable\nByzantine robust PIR schemes may tolerate a majority of malicious responding\nservers that provide incorrect answers. In this paper, we propose two perfect\nlist-decodable BRPIR schemes. Our schemes are the first ones that can\nsimultaneously handle a majority of malicious responding servers, achieve a\ncommunication complexity of $o(n^{1/2})$ for a database of size n, and provide\na nontrivial estimation on the list sizes. Compared with the existing\nsolutions, our schemes attain lower communication complexity, higher byzantine\ntolerance, and smaller list size."
    },
    {
        "date": "2025-06",
        "title": "Semantic-Aware Parsing for Security Logs",
        "author": "Julien Piet, Vivian Fang, Rishi Khare, Vern Paxson, Raluca Ada Popa, and David Wagner",
        "link": "http://arxiv.org/abs/2506.17512v1",
        "abstract": "Security analysts struggle to quickly and efficiently query and correlate log\ndata due to the heterogeneity and lack of structure in real-world logs.\nExisting AI-based parsers focus on learning syntactic log templates but lack\nthe semantic interpretation needed for querying. Directly querying large\nlanguage models on raw logs is impractical at scale and vulnerable to prompt\ninjection attacks.\n  In this paper, we introduce Matryoshka, the first end-to-end system\nleveraging LLMs to automatically generate semantically-aware structured log\nparsers. Matryoshka combines a novel syntactic parser-employing precise regular\nexpressions rather than wildcards-with a completely new semantic parsing layer\nthat clusters variables and maps them into a queryable, contextually meaningful\nschema. This approach provides analysts with queryable and semantically rich\ndata representations, facilitating rapid and precise log querying without the\ntraditional burden of manual parser construction. Additionally, Matryoshka can\nmap the newly created fields to recognized attributes within the Open\nCybersecurity Schema Framework (OCSF), enabling interoperability.\n  We evaluate Matryoshka on a newly curated real-world log benchmark,\nintroducing novel metrics to assess how consistently fields are named and\nmapped across logs. Matryoshka's syntactic parser outperforms prior works, and\nthe semantic layer achieves an F1 score of 0.95 on realistic security queries.\nAlthough mapping fields to the extensive OCSF taxonomy remains challenging,\nMatryoshka significantly reduces manual effort by automatically extracting and\norganizing valuable fields, moving us closer to fully automated, AI-driven log\nanalytics."
    },
    {
        "date": "2025-06",
        "title": "Open Sky, Open Threats: Replay Attacks in Space Launch and Re-entry Phases",
        "author": "Nesrine Benchoubane, Eray Guven, and Gunes Karabulut Kurt",
        "link": "http://arxiv.org/abs/2506.17446v1",
        "abstract": "This paper examines the effects of replay attacks on the integrity of both\nuplink and downlink communications during critical phases of spacecraft\ncommunication. By combining software-defined radios (SDRs) with a real-time\nchannel emulator, we replicate realistic attack conditions on the Orion\nspacecraft's communication systems in both launch and reentry. Our evaluation\nshows that, under replay attacks, the attacker's signal can overpower\nlegitimate transmissions, leading to a Signal to Noise Ratio (SNR) difference\nof up to -7.8 dB during reentry and -6.5 dB during launch. To mitigate these\nthreats, we propose a more secure receiver design incorporating a\nphase-coherency-dependent decision-directed (DD) equalizer with a narrowed\nphase-locked loop (PLL) bandwidth. This configuration enhances resilience by\nmaking synchronization more sensitive to phase distortions caused by replay\ninterference."
    },
    {
        "date": "2025-06",
        "title": "Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network",
        "author": "Mahin Montasir Afif, Abdullah Al Noman, K. M. Tahsin Kabir, Md. Mortuza Ahmmed, Md. Mostafizur Rahman, Mufti Mahmud, and Md. Ashraful Babu",
        "link": "http://arxiv.org/abs/2506.17165v1",
        "abstract": "Generative Adversarial Networks (GAN) have shown potential in expanding\nlimited medical imaging datasets. This study explores how different ratios of\nGAN-generated and real brain tumor MRI images impact the performance of a CNN\nin classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic\nimages which were mixed with real ones at various ratios to train a custom CNN.\nThe CNN was then evaluated on a separate real-world test set. Our results\nindicate that the model maintains high sensitivity and precision in tumor\nclassification, even when trained predominantly on synthetic data. When only a\nsmall portion of GAN data was added, such as 900 real images and 100 GAN\nimages, the model achieved excellent performance, with test accuracy reaching\n95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the\nproportion of GAN images increased further, performance gradually declined.\nThis study suggests that while GANs are useful for augmenting limited datasets\nespecially when real data is scarce, too much synthetic data can introduce\nartifacts that affect the model's ability to generalize to real world cases."
    },
    {
        "date": "2025-06",
        "title": "Analyzing PDFs like Binaries: Adversarially Robust PDF Malware Analysis via Intermediate Representation and Language Model",
        "author": "Side Liu, Jiang Ming, Guodong Zhou, Xinyi Liu, Jianming Fu, and Guojun Peng",
        "link": "http://arxiv.org/abs/2506.17162v1",
        "abstract": "Malicious PDF files have emerged as a persistent threat and become a popular\nattack vector in web-based attacks. While machine learning-based PDF malware\nclassifiers have shown promise, these classifiers are often susceptible to\nadversarial attacks, undermining their reliability. To address this issue,\nrecent studies have aimed to enhance the robustness of PDF classifiers. Despite\nthese efforts, the feature engineering underlying these studies remains\noutdated. Consequently, even with the application of cutting-edge machine\nlearning techniques, these approaches fail to fundamentally resolve the issue\nof feature instability.\n  To tackle this, we propose a novel approach for PDF feature extraction and\nPDF malware detection. We introduce the PDFObj IR (PDF Object Intermediate\nRepresentation), an assembly-like language framework for PDF objects, from\nwhich we extract semantic features using a pretrained language model.\nAdditionally, we construct an Object Reference Graph to capture structural\nfeatures, drawing inspiration from program analysis. This dual approach enables\nus to analyze and detect PDF malware based on both semantic and structural\nfeatures. Experimental results demonstrate that our proposed classifier\nachieves strong adversarial robustness while maintaining an exceptionally low\nfalse positive rate of only 0.07% on baseline dataset compared to\nstate-of-the-art PDF malware classifiers."
    },
    {
        "date": "2025-06",
        "title": "Robust Training with Data Augmentation for Medical Imaging Classification",
        "author": "Josu\u00e9 Mart\u00ednez-Mart\u00ednez, Olivia Brown, Mostafa Karami, and Sheida Nabavi",
        "link": "http://arxiv.org/abs/2506.17133v1",
        "abstract": "Deep neural networks are increasingly being used to detect and diagnose\nmedical conditions using medical imaging. Despite their utility, these models\nare highly vulnerable to adversarial attacks and distribution shifts, which can\naffect diagnostic reliability and undermine trust among healthcare\nprofessionals. In this study, we propose a robust training algorithm with data\naugmentation (RTDA) to mitigate these vulnerabilities in medical image\nclassification. We benchmark classifier robustness against adversarial\nperturbations and natural variations of RTDA and six competing baseline\ntechniques, including adversarial training and data augmentation approaches in\nisolation and combination, using experimental data sets with three different\nimaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that\nRTDA achieves superior robustness against adversarial attacks and improved\ngeneralization performance in the presence of distribution shift in each image\nclassification task while maintaining high clean accuracy."
    },
    {
        "date": "2025-06",
        "title": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking",
        "author": "Teng Guo, and Jingjin Yu",
        "link": "http://arxiv.org/abs/2506.17119v1",
        "abstract": "We introduce a robust framework, RGBTrack, for real-time 6D pose estimation\nand tracking that operates solely on RGB data, thereby eliminating the need for\ndepth input for such dynamic and precise object pose tracking tasks. Building\non the FoundationPose architecture, we devise a novel binary search strategy\ncombined with a render-and-compare mechanism to efficiently infer depth and\ngenerate robust pose hypotheses from true-scale CAD models. To maintain stable\ntracking in dynamic scenarios, including rapid movements and occlusions,\nRGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman\nfilter and a state machine for proactive object pose recovery. In addition,\nRGBTrack's scale recovery module dynamically adapts CAD models of unknown scale\nusing an initial depth estimate, enabling seamless integration with modern\ngenerative reconstruction techniques. Extensive evaluations on benchmark\ndatasets demonstrate that RGBTrack's novel depth-free approach achieves\ncompetitive accuracy and real-time performance, making it a promising practical\nsolution candidate for application areas including robotics, augmented reality,\nand computer vision.\n  The source code for our implementation will be made publicly available at\nhttps://github.com/GreatenAnoymous/RGBTrack.git."
    },
    {
        "date": "2025-06",
        "title": "Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability",
        "author": "Thilina Pathirana, and Ruxandra F. Olimid",
        "link": "http://arxiv.org/abs/2506.17371v1",
        "abstract": "Multi-access Edge Computing (MEC), an enhancement of 5G, processes data\ncloser to its generation point, reducing latency and network load. However, the\ndistributed and edge-based nature of 5G-MEC presents privacy and security\nchallenges, including data exposure risks. Ensuring efficient manipulation and\nsecurity of sensitive data at the edge is crucial. To address these challenges,\nwe investigate the usage of threshold secret sharing in 5G-MEC storage, an\napproach that enhances both security and dependability. A (k,n) threshold\nsecret sharing scheme splits and stores sensitive data among n nodes, requiring\nat least k nodes for reconstruction. The solution ensures confidentiality by\nprotecting data against fewer than k colluding nodes and enhances availability\nby tolerating up to n-k failing nodes. This approach mitigates threats such as\nunauthorized access and node failures, whether accidental or intentional. We\nfurther discuss a method for selecting the convenient MEHs to store the shares,\nconsidering the MEHs' trustworthiness level as a main criterion. Although we\ndefine our proposal in the context of secret-shared data storage, it can be\nseen as an independent, standalone selection process for 5G-MEC trustworthy\nnode selection in other scenarios too."
    },
    {
        "date": "2025-06",
        "title": "Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators",
        "author": "Marco Jiralerspong, Esther Derman, Danilo Vucetic, Nikolay Malkin, Bilun Sun, Tianyu Zhang, Pierre-Luc Bacon, and Gauthier Gidel",
        "link": "http://arxiv.org/abs/2506.17007v1",
        "abstract": "A major bottleneck in scientific discovery involves narrowing a large\ncombinatorial set of objects, such as proteins or molecules, to a small set of\npromising candidates. While this process largely relies on expert knowledge,\nrecent methods leverage reinforcement learning (RL) to enhance this filtering.\nThey achieve this by estimating proxy reward functions from available datasets\nand using regularization to generate more diverse candidates. These reward\nfunctions are inherently uncertain, raising a particularly salient challenge\nfor scientific discovery. In this work, we show that existing methods, often\nframed as sampling proportional to a reward function, are inadequate and yield\nsuboptimal candidates, especially in large search spaces. To remedy this issue,\nwe take a robust RL approach and introduce a unified operator that seeks\nrobustness to the uncertainty of the proxy reward function. This general\noperator targets peakier sampling distributions while encompassing known soft\nRL operators. It also leads us to a novel algorithm that identifies\nhigher-quality, diverse candidates in both synthetic and real-world tasks.\nUltimately, our work offers a new, flexible perspective on discrete\ncompositional generation tasks. Code: https://github.com/marcojira/tgm."
    },
    {
        "date": "2025-06",
        "title": "SmartGuard: Leveraging Large Language Models for Network Attack Detection through Audit Log Analysis and Summarization",
        "author": "Hao Zhang, Shuo Shao, Song Li, Zhenyu Zhong, Yan Liu, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2506.16981v1",
        "abstract": "End-point monitoring solutions are widely deployed in today's enterprise\nenvironments to support advanced attack detection and investigation. These\nmonitors continuously record system-level activities as audit logs and provide\ndeep visibility into security events. Unfortunately, existing methods of\nsemantic analysis based on audit logs have low granularity, only reaching the\nsystem call level, making it difficult to effectively classify highly covert\nbehaviors. Additionally, existing works mainly match audit log streams with\nrule knowledge bases describing behaviors, which heavily rely on expertise and\nlack the ability to detect unknown attacks and provide interpretive\ndescriptions. In this paper, we propose SmartGuard, an automated method that\ncombines abstracted behaviors from audit event semantics with large language\nmodels. SmartGuard extracts specific behaviors (function level) from incoming\nsystem logs and constructs a knowledge graph, divides events by threads, and\ncombines event summaries with graph embeddings to achieve information diagnosis\nand provide explanatory narratives through large language models. Our\nevaluation shows that SmartGuard achieves an average F1 score of 96\\% in\nassessing malicious behaviors and demonstrates good scalability across multiple\nmodels and unknown attacks. It also possesses excellent fine-tuning\ncapabilities, allowing experts to assist in timely system updates."
    },
    {
        "date": "2025-06",
        "title": "MM-AttacKG: A Multimodal Approach to Attack Graph Construction with Large Language Models",
        "author": "Yongheng Zhang, Xinyun Zhao, Yunshan Ma, Haokai Ma, Yingxiao Guan, Guozheng Yang, Yuliang Lu, and Xiang Wang",
        "link": "http://arxiv.org/abs/2506.16968v1",
        "abstract": "Cyber Threat Intelligence (CTI) parsing aims to extract key threat\ninformation from massive data, transform it into actionable intelligence,\nenhance threat detection and defense efficiency, including attack graph\nconstruction, intelligence fusion and indicator extraction. Among these\nresearch topics, Attack Graph Construction (AGC) is essential for visualizing\nand understanding the potential attack paths of threat events from CTI reports.\nExisting approaches primarily construct the attack graphs purely from the\ntextual data to reveal the logical threat relationships between entities within\nthe attack behavioral sequence. However, they typically overlook the specific\nthreat information inherent in visual modalities, which preserves the key\nthreat details from inherently-multimodal CTI report. Therefore, we enhance the\neffectiveness of attack graph construction by analyzing visual information\nthrough Multimodal Large Language Models (MLLMs). Specifically, we propose a\nnovel framework, MM-AttacKG, which can effectively extract key information from\nthreat images and integrate it into attack graph construction, thereby\nenhancing the comprehensiveness and accuracy of attack graphs. It first employs\na threat image parsing module to extract critical threat information from\nimages and generate descriptions using MLLMs. Subsequently, it builds an\niterative question-answering pipeline tailored for image parsing to refine the\nunderstanding of threat images. Finally, it achieves content-level integration\nbetween attack graphs and image-based answers through MLLMs, completing threat\ninformation enhancement. The experimental results demonstrate that MM-AttacKG\ncan accurately identify key information in threat images and significantly\nimprove the quality of multimodal attack graph construction, effectively\naddressing the shortcomings of existing methods in utilizing image-based threat\ninformation."
    },
    {
        "date": "2025-06",
        "title": "Towards Effective Complementary Security Analysis using Large Language Models",
        "author": "Jonas Wagner, Simon M\u00fcller, Christian N\u00e4ther, Jan-Philipp Stegh\u00f6fer, and Andreas Both",
        "link": "http://arxiv.org/abs/2506.16899v1",
        "abstract": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms."
    },
    {
        "date": "2025-06",
        "title": "Robust Group Anomaly Detection for Quasi-Periodic Network Time Series",
        "author": "Kai Yang, Shaoyu Dou, Pan Luo, Xin Wang, and H. Vincent Poor",
        "link": "http://arxiv.org/abs/2506.16815v1",
        "abstract": "Many real-world multivariate time series are collected from a network of\nphysical objects embedded with software, electronics, and sensors. The\nquasi-periodic signals generated by these objects often follow a similar\nrepetitive and periodic pattern, but have variations in the period, and come in\ndifferent lengths caused by timing (synchronization) errors. Given a multitude\nof such quasi-periodic time series, can we build machine learning models to\nidentify those time series that behave differently from the majority of the\nobservations? In addition, can the models help human experts to understand how\nthe decision was made? We propose a sequence to Gaussian Mixture Model\n(seq2GMM) framework. The overarching goal of this framework is to identify\nunusual and interesting time series within a network time series database. We\nfurther develop a surrogate-based optimization algorithm that can efficiently\ntrain the seq2GMM model. Seq2GMM exhibits strong empirical performance on a\nplurality of public benchmark datasets, outperforming state-of-the-art anomaly\ndetection techniques by a significant margin. We also theoretically analyze the\nconvergence property of the proposed training algorithm and provide numerical\nresults to substantiate our theoretical claims."
    },
    {
        "date": "2025-06",
        "title": "Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning",
        "author": "Chengpeng Hu, Ziming Wang, Bo Yuan, Jialin Liu, Chengqi Zhang, and Xin Yao",
        "link": "http://arxiv.org/abs/2506.16795v1",
        "abstract": "Dynamic material handling (DMH) involves the assignment of dynamically\narriving material transporting tasks to suitable vehicles in real time for\nminimising makespan and tardiness. In real-world scenarios, historical task\nrecords are usually available, which enables the training of a decision policy\non multiple instances consisting of historical records. Recently, reinforcement\nlearning has been applied to solve DMH. Due to the occurrence of dynamic events\nsuch as new tasks, adaptability is highly required. Solving DMH is challenging\nsince constraints including task delay should be satisfied. A feedback is\nreceived only when all tasks are served, which leads to sparse reward. Besides,\nmaking the best use of limited computational resources and historical records\nfor training a robust policy is crucial. The time allocated to different\nproblem instances would highly impact the learning process. To tackle those\nchallenges, this paper proposes a novel adaptive constrained evolutionary\nreinforcement learning (ACERL) approach, which maintains a population of actors\nfor diverse exploration. ACERL accesses each actor for tackling sparse rewards\nand constraint violation to restrict the behaviour of the policy. Moreover,\nACERL adaptively selects the most beneficial training instances for improving\nthe policy. Extensive experiments on eight training and eight unseen test\ninstances demonstrate the outstanding performance of ACERL compared with\nseveral state-of-the-art algorithms. Policies trained by ACERL can schedule the\nvehicles while fully satisfying the constraints. Additional experiments on 40\nunseen noised instances show the robust performance of ACERL. Cross-validation\nfurther presents the overall effectiveness of ACREL. Besides, a rigorous\nablation study highlights the coordination and benefits of each ingredient of\nACERL."
    },
    {
        "date": "2025-06",
        "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models",
        "author": "Lei Jiang, Zixun Zhang, Zizhou Wang, Xiaobing Sun, Zhen Li, Liangli Zhen, and Xiaohua Xu",
        "link": "http://arxiv.org/abs/2506.16760v1",
        "abstract": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance\nacross multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass\nbuilt-in safety mechanisms to elicit restricted content generation. Existing\nblack-box jailbreak methods primarily rely on adversarial textual prompts or\nimage perturbations, yet these approaches are highly detectable by standard\ncontent filtering systems and exhibit low query and computational efficiency.\nIn this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),\na novel black-box jailbreak attack framework that decomposes malicious prompts\ninto semantically benign visual and textual fragments. By leveraging LVLMs'\ncross-modal reasoning abilities, CAMO covertly reconstructs harmful\ninstructions through multi-step reasoning, evading conventional detection\nmechanisms. Our approach supports adjustable reasoning complexity and requires\nsignificantly fewer queries than prior attacks, enabling both stealth and\nefficiency. Comprehensive evaluations conducted on leading LVLMs validate\nCAMO's effectiveness, showcasing robust performance and strong cross-model\ntransferability. These results underscore significant vulnerabilities in\ncurrent built-in safety mechanisms, emphasizing an urgent need for advanced,\nalignment-aware security and safety solutions in vision-language systems."
    },
    {
        "date": "2025-06",
        "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
        "author": "Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, and Shin Ishii",
        "link": "http://arxiv.org/abs/2506.16753v1",
        "abstract": "Recently, robust reinforcement learning (RL) methods designed to handle\nadversarial input observations have received significant attention, motivated\nby RL's inherent vulnerabilities. While existing approaches have demonstrated\nreasonable success, addressing worst-case scenarios over long time horizons\nrequires both minimizing the agent's cumulative rewards for adversaries and\ntraining agents to counteract them through alternating learning. However, this\nprocess introduces mutual dependencies between the agent and the adversary,\nmaking interactions with the environment inefficient and hindering the\ndevelopment of off-policy methods. In this work, we propose a novel off-policy\nmethod that eliminates the need for additional environmental interactions by\nreformulating adversarial learning as a soft-constrained optimization problem.\nOur approach is theoretically supported by the symmetric property of policy\nevaluation between the agent and the adversary. The implementation is available\nat https://github.com/nakanakakosuke/VALT_SAC."
    },
    {
        "date": "2025-06",
        "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches",
        "author": "Yun Xing, Yue Cao, Nhat Chung, Jie Zhang, Ivor Tsang, Ming-Ming Cheng, Yang Liu, Lei Ma, and Qing Guo",
        "link": "http://arxiv.org/abs/2506.16690v1",
        "abstract": "Stereo Depth estimation is a critical task in autonomous driving and\nrobotics, where inaccuracies (such as misidentifying nearby objects as distant)\ncan lead to dangerous situations. Adversarial attacks against stereo depth\nestimation can help reveal vulnerabilities before deployment. Previous work has\nshown that repeating optimized textures can effectively mislead stereo depth\nestimation in digital settings. However, our research reveals that these\nnaively repeated texture structures perform poorly in physical-world\nimplementations, i.e., when deployed as patches, limiting their practical\nutility for testing stereo depth estimation systems. In this work, for the\nfirst time, we discover that introducing regular intervals between repeated\ntextures, creating a striped structure, significantly enhances the patch attack\neffectiveness. Through extensive experimentation, we analyze how variations of\nthis novel structure influence the performance. Based on these insights, we\ndevelop a novel stereo depth attack that jointly optimizes both the striped\nstructure and texture elements. Our generated adversarial patches can be\ninserted into any scenes and successfully attack state-of-the-art stereo depth\nestimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can\nalso attack commercial RGB-D cameras (Intel RealSense) in real-world\nconditions, demonstrating their practical relevance for security assessment of\nstereo systems."
    },
    {
        "date": "2025-06",
        "title": "CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks",
        "author": "Yinghao Wu, and Liyan Zhang",
        "link": "http://arxiv.org/abs/2506.17350v1",
        "abstract": "Backdoor attacks have emerged as a critical security threat against deep\nneural networks in recent years. The majority of existing backdoor attacks\nfocus on targeted backdoor attacks, where trigger is strongly associated to\nspecific malicious behavior. Various backdoor detection methods depend on this\ninherent property and shows effective results in identifying and mitigating\nsuch targeted attacks. However, a purely untargeted attack in backdoor\nscenarios is, in some sense, self-weakening, since the target nature is what\nmakes backdoor attacks so powerful. In light of this, we introduce a novel\nConstrained Untargeted Backdoor Attack (CUBA), which combines the flexibility\nof untargeted attacks with the intentionality of targeted attacks. The\ncompromised model, when presented with backdoor images, will classify them into\nrandom classes within a constrained range of target classes selected by the\nattacker. This combination of randomness and determinedness enables the\nproposed untargeted backdoor attack to natively circumvent existing backdoor\ndefense methods. To implement the untargeted backdoor attack under controlled\nflexibility, we propose to apply logit normalization on cross-entropy loss with\nflipped one-hot labels. By constraining the logit during training, the\ncompromised model will show a uniform distribution across selected target\nclasses, resulting in controlled untargeted attack. Extensive experiments\ndemonstrate the effectiveness of the proposed CUBA on different datasets."
    },
    {
        "date": "2025-06",
        "title": "Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control",
        "author": "Yuxin Chen, Jianglan Wei, Chenfeng Xu, Boyi Li, Masayoshi Tomizuka, Andrea Bajcsy, and Ran Tian",
        "link": "http://arxiv.org/abs/2506.16565v1",
        "abstract": "World models enable robots to \"imagine\" future observations given current\nobservations and planned actions, and have been increasingly adopted as\ngeneralized dynamics models to facilitate robot learning. Despite their\npromise, these models remain brittle when encountering novel visual distractors\nsuch as objects and background elements rarely seen during training.\nSpecifically, novel distractors can corrupt action outcome predictions, causing\ndownstream failures when robots rely on the world model imaginations for\nplanning or action verification. In this work, we propose Reimagination with\nObservation Intervention (ReOI), a simple yet effective test-time strategy that\nenables world models to predict more reliable action outcomes in open-world\nscenarios where novel and unanticipated visual distractors are inevitable.\nGiven the current robot observation, ReOI first detects visual distractors by\nidentifying which elements of the scene degrade in physically implausible ways\nduring world model prediction. Then, it modifies the current observation to\nremove these distractors and bring the observation closer to the training\ndistribution. Finally, ReOI \"reimagines\" future outcomes with the modified\nobservation and reintroduces the distractors post-hoc to preserve visual\nconsistency for downstream planning and verification. We validate our approach\non a suite of robotic manipulation tasks in the context of action verification,\nwhere the verifier needs to select desired action plans based on predictions\nfrom a world model. Our results show that ReOI is robust to both\nin-distribution and out-of-distribution visual distractors. Notably, it\nimproves task success rates by up to 3x in the presence of novel distractors,\nsignificantly outperforming action verification that relies on world model\npredictions without imagination interventions."
    },
    {
        "date": "2025-06",
        "title": "One Sample is Enough to Make Conformal Prediction Robust",
        "author": "Soroush H. Zargarbashi, Mohammad Sadegh Akhondzadeh, and Aleksandar Bojchevski",
        "link": "http://arxiv.org/abs/2506.16553v1",
        "abstract": "Given any model, conformal prediction (CP) returns prediction sets guaranteed\nto include the true label with high adjustable probability. Robust CP (RCP)\nextends this to inputs with worst-case noise. A well-established approach is to\nuse randomized smoothing for RCP since it is applicable to any black-box model\nand provides smaller sets compared to deterministic methods. However, current\nsmoothing-based RCP requires many model forward passes per each input which is\ncomputationally expensive. We show that conformal prediction attains some\nrobustness even with a forward pass on a single randomly perturbed input. Using\nany binary certificate we propose a single sample robust CP (RCP1). Our\napproach returns robust sets with smaller average set size compared to SOTA\nmethods which use many (e.g. around 100) passes per input. Our key insight is\nto certify the conformal prediction procedure itself rather than individual\nscores. Our approach is agnostic to the setup (classification and regression).\nWe further extend our approach to smoothing-based robust conformal risk\ncontrol."
    },
    {
        "date": "2025-06",
        "title": "SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures",
        "author": "Marco Stadler, Michael Vierhauser, Michael Riegler, Daniel Waghubinger, and Johannes Sametinger",
        "link": "http://arxiv.org/abs/2506.16545v1",
        "abstract": "The rise of the Internet of Things and Cyber-Physical Systems has introduced\nnew challenges on ensuring secure and robust communication. The growing number\nof connected devices increases network complexity, leading to higher latency\nand traffic. Distributed computing architectures (DCAs) have gained prominence\nto address these issues. This shift has significantly expanded the attack\nsurface, requiring additional security measures to protect all components --\nfrom sensors and actuators to edge nodes and central servers. Recent incidents\nhighlight the difficulty of this task: Cyberattacks, like distributed denial of\nservice attacks, continue to pose severe threats and cause substantial damage.\nImplementing a holistic defense mechanism remains an open challenge,\nparticularly against attacks that demand both enhanced resilience and rapid\nresponse. Addressing this gap requires innovative solutions to enhance the\nsecurity of DCAs. In this work, we present our holistic self-adaptive security\nframework which combines different adaptation strategies to create\ncomprehensive and efficient defense mechanisms. We describe how to incorporate\nthe framework into a real-world use case scenario and further evaluate its\napplicability and efficiency. Our evaluation yields promising results,\nindicating great potential to further extend the research on our framework."
    },
    {
        "date": "2025-06",
        "title": "Robust Reward Modeling via Causal Rubrics",
        "author": "Pragya Srivastava, Harman Singh, Rahul Madhavan, Gandharv Patil, Sravanti Addepalli, Arun Suggala, Rengarajan Aravamudhan, Soumya Sharma, Anirban Laha, Aravindan Raghuveer, Karthikeyan Shanmugam, and Doina Precup",
        "link": "http://arxiv.org/abs/2506.16507v1",
        "abstract": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k."
    },
    {
        "date": "2025-06",
        "title": "Black-Box Privacy Attacks on Shared Representations in Multitask Learning",
        "author": "John Abascal, Nicol\u00e1s Berrios, Alina Oprea, Jonathan Ullman, Adam Smith, and Matthew Jagielski",
        "link": "http://arxiv.org/abs/2506.16460v1",
        "abstract": "Multitask learning (MTL) has emerged as a powerful paradigm that leverages\nsimilarities among multiple learning tasks, each with insufficient samples to\ntrain a standalone model, to solve them simultaneously while minimizing data\nsharing across users and organizations. MTL typically accomplishes this goal by\nlearning a shared representation that captures common structure among the tasks\nby embedding data from all tasks into a common feature space. Despite being\ndesigned to be the smallest unit of shared information necessary to effectively\nlearn patterns across multiple tasks, these shared representations can\ninadvertently leak sensitive information about the particular tasks they were\ntrained on.\n  In this work, we investigate what information is revealed by the shared\nrepresentations through the lens of inference attacks. Towards this, we propose\na novel, black-box task-inference threat model where the adversary, given the\nembedding vectors produced by querying the shared representation on samples\nfrom a particular task, aims to determine whether that task was present when\ntraining the shared representation. We develop efficient, purely black-box\nattacks on machine learning models that exploit the dependencies between\nembeddings from the same task without requiring shadow models or labeled\nreference data. We evaluate our attacks across vision and language domains for\nmultiple use cases of MTL and demonstrate that even with access only to fresh\ntask samples rather than training data, a black-box adversary can successfully\ninfer a task's inclusion in training. To complement our experiments, we provide\ntheoretical analysis of a simplified learning setting and show a strict\nseparation between adversaries with training samples and fresh samples from the\ntarget task's distribution."
    },
    {
        "date": "2025-06",
        "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models",
        "author": "Biao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Zhixuan Chu, and Yiming Li",
        "link": "http://arxiv.org/abs/2506.16447v1",
        "abstract": "Backdoor unalignment attacks against Large Language Models (LLMs) enable the\nstealthy compromise of safety alignment using a hidden trigger while evading\nnormal safety auditing. These attacks pose significant threats to the\napplications of LLMs in the real-world Large Language Model as a Service\n(LLMaaS) setting, where the deployed model is a fully black-box system that can\nonly interact through text. Furthermore, the sample-dependent nature of the\nattack target exacerbates the threat. Instead of outputting a fixed label, the\nbackdoored LLM follows the semantics of any malicious command with the hidden\ntrigger, significantly expanding the target space. In this paper, we introduce\nBEAT, a black-box defense that detects triggered samples during inference to\ndeactivate the backdoor. It is motivated by an intriguing observation (dubbed\nthe probe concatenate effect), where concatenated triggered samples\nsignificantly reduce the refusal rate of the backdoored LLM towards a malicious\nprobe, while non-triggered samples have little effect. Specifically, BEAT\nidentifies whether an input is triggered by measuring the degree of distortion\nin the output distribution of the probe before and after concatenation with the\ninput. Our method addresses the challenges of sample-dependent targets from an\nopposite perspective. It captures the impact of the trigger on the refusal\nsignal (which is sample-independent) instead of sample-specific successful\nattack behaviors. It overcomes black-box access limitations by using multiple\nsampling to approximate the output distribution. Extensive experiments are\nconducted on various backdoor attacks and LLMs (including the closed-source\nGPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.\nBesides, we also preliminarily verify that BEAT can effectively defend against\npopular jailbreak attacks, as they can be regarded as 'natural backdoors'."
    },
    {
        "date": "2025-06",
        "title": "Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks",
        "author": "Dong Nguyen Tien, and Dung D. Le",
        "link": "http://arxiv.org/abs/2506.16407v1",
        "abstract": "Visual Document Understanding (VDU) systems have achieved strong performance\nin information extraction by integrating textual, layout, and visual signals.\nHowever, their robustness under realistic adversarial perturbations remains\ninsufficiently explored. We introduce the first unified framework for\ngenerating and evaluating multi-modal adversarial attacks on OCR-based VDU\nmodels. Our method covers six gradient-based layout attack scenarios,\nincorporating manipulations of OCR bounding boxes, pixels, and texts across\nboth word and line granularities, with constraints on layout perturbation\nbudget (e.g., IoU >= 0.6) to preserve plausibility.\n  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and\nsix model families demonstrate that line-level attacks and compound\nperturbations (BBox + Pixel + Text) yield the most severe performance\ndegradation. Projected Gradient Descent (PGD)-based BBox perturbations\noutperform random-shift baselines in all investigated models. Ablation studies\nfurther validate the impact of layout budget, text modification, and\nadversarial transferability."
    },
    {
        "date": "2025-06",
        "title": "Physical-Layer Signal Injection Attacks on EV Charging Ports: Bypassing Authentication via Electrical-Level Exploits",
        "author": "Hetian Shi, Yi He, Shangru Song, Jianwei Zhuge, and Jian Mao",
        "link": "http://arxiv.org/abs/2506.16400v1",
        "abstract": "The proliferation of electric vehicles in recent years has significantly\nexpanded the charging infrastructure while introducing new security risks to\nboth vehicles and chargers. In this paper, we investigate the security of major\ncharging protocols such as SAE J1772, CCS, IEC 61851, GB/T 20234, and NACS,\nuncovering new physical signal spoofing attacks in their authentication\nmechanisms. By inserting a compact malicious device into the charger connector,\nattackers can inject fraudulent signals to sabotage the charging process,\nleading to denial of service, vehicle-induced charger lockout, and damage to\nthe chargers or the vehicle's charge management system. To demonstrate the\nfeasibility of our attacks, we propose PORTulator, a proof-of-concept (PoC)\nattack hardware, including a charger gun plugin device for injecting physical\nsignals and a wireless controller for remote manipulation. By evaluating\nPORTulator on multiple real-world chargers, we identify 7 charging standards\nused by 20 charger piles that are vulnerable to our attacks. The root cause is\nthat chargers use simple physical signals for authentication and control,\nmaking them easily spoofed by attackers. To address this issue, we propose\nenhancing authentication circuits by integrating non-resistive memory\ncomponents and utilizing dynamic high-frequency Pulse Width Modulation (PWM)\nsignals to counter such physical signal spoofing attacks."
    },
    {
        "date": "2025-06",
        "title": "SycnMapV2: Robust and Adaptive Unsupervised Segmentation",
        "author": "Heng Zhang, Zikang Wan, and Danilo Vasconcellos Vargas",
        "link": "http://arxiv.org/abs/2506.16297v2",
        "abstract": "Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture."
    },
    {
        "date": "2025-06",
        "title": "R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision",
        "author": "Weeyoung Kwon, Jeahun Sung, Minkyu Jeon, Chanho Eom, and Jihyong Oh",
        "link": "http://arxiv.org/abs/2506.16262v2",
        "abstract": "Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have achieved significant progress in photorealistic\n3D scene reconstruction and novel view synthesis. However, most existing models\nassume clean and high-resolution (HR) multi-view inputs, which limits their\nrobustness under real-world degradations such as noise, blur, low-resolution\n(LR), and weather-induced artifacts. To address these limitations, the emerging\nfield of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision\ntasks including super-resolution (SR), deblurring, weather degradation removal,\nrestoration, and enhancement into the 3D spatial domain. This survey, referred\nto as R\\textsuperscript{3}eVision, provides a comprehensive overview of robust\nrendering, restoration, and enhancement for 3D LLV by formalizing the\ndegradation-aware rendering problem and identifying key challenges related to\nspatio-temporal consistency and ill-posed optimization. Recent methods that\nintegrate LLV into neural rendering frameworks are categorized to illustrate\nhow they enable high-fidelity 3D reconstruction under adverse conditions.\nApplication domains such as autonomous driving, AR/VR, and robotics are also\ndiscussed, where reliable 3D perception from degraded inputs is critical. By\nreviewing representative methods, datasets, and evaluation protocols, this work\npositions 3D LLV as a fundamental direction for robust 3D content generation\nand scene-level reconstruction in real-world environments."
    },
    {
        "date": "2025-06",
        "title": "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models",
        "author": "Xinting Liao, Weiming Liu, Jiaming Qian, Pengyang Zhou, Jiahe Xu, Wenjie Wang, Chaochao Chen, Xiaolin Zheng, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2506.16218v2",
        "abstract": "Federated prompt learning (FPL) for vision-language models is a powerful\napproach to collaboratively adapt models across distributed clients while\npreserving data privacy. However, existing FPL approaches suffer from a\ntrade-off between performance and robustness, particularly in\nout-of-distribution (OOD) shifts, limiting their reliability in real-world\nscenarios. The inherent in-distribution (ID) data heterogeneity among different\nclients makes it more challenging to maintain this trade-off. To fill this gap,\nwe introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,\nwhich captures diverse distributions among clients using ID global prompts,\nlocal prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of\nprompts to create both class-level and distribution-level separations, which\nadapt to OOD shifts through bi-level distributionally robust optimization.\nAdditionally, FOCoOp improves the discrimination consistency among clients,\ni.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by\nsemi-unbalanced optimal transport. The extensive experiments on real-world\ndatasets demonstrate that FOCoOp effectively captures decentralized\nheterogeneous distributions and enhances robustness of different OOD shifts.\nThe project is available at GitHub."
    },
    {
        "date": "2025-06",
        "title": "From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction",
        "author": "Zhenxuan Zhang, Lipei Zhang, Yanqi Cheng, Zi Wang, Fanwen Wang, Haosen Zhang, Yue Yang, Yinzhe Wu, Jiahao Huang, Angelica I Aviles-Rivero, Zhifan Gao, Guang Yang, and Peter J. Lally",
        "link": "http://arxiv.org/abs/2506.16210v2",
        "abstract": "In motion-robust magnetic resonance imaging (MRI), slice-to-volume\nreconstruction is critical for recovering anatomically consistent 3D brain\nvolumes from 2D slices, especially under accelerated acquisitions or patient\nmotion. However, this task remains challenging due to hierarchical structural\ndisruptions. It includes local detail loss from k-space undersampling, global\nstructural aliasing caused by motion, and volumetric anisotropy. Therefore, we\npropose a progressive refinement implicit neural representation (PR-INR)\nframework. Our PR-INR unifies motion correction, structural refinement, and\nvolumetric synthesis within a geometry-aware coordinate space. Specifically, a\nmotion-aware diffusion module is first employed to generate coarse volumetric\nreconstructions that suppress motion artifacts and preserve global anatomical\nstructures. Then, we introduce an implicit detail restoration module that\nperforms residual refinement by aligning spatial coordinates with visual\nfeatures. It corrects local structures and enhances boundary precision.\nFurther, a voxel continuous-aware representation module represents the image as\na continuous function over 3D coordinates. It enables accurate inter-slice\ncompletion and high-frequency detail recovery. We evaluate PR-INR on five\npublic MRI datasets under various motion conditions (3% and 5% displacement),\nundersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental\nresults demonstrate that PR-INR outperforms state-of-the-art methods in both\nquantitative reconstruction metrics and visual quality. It further shows\ngeneralization and robustness across diverse unseen domains."
    },
    {
        "date": "2025-06",
        "title": "Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis",
        "author": "Zhenghao Xi, Xiang Liu, Yaqi Liu, Yitong Cai, and Yangyu Zheng",
        "link": "http://arxiv.org/abs/2506.16186v1",
        "abstract": "Accident detection using Closed Circuit Television (CCTV) footage is one of\nthe most imperative features for enhancing transport safety and efficient\ntraffic control. To this end, this research addresses the issues of supervised\nmonitoring and data deficiency in accident detection systems by adapting\nexcellent deep learning technologies. The motivation arises from rising\nstatistics in the number of car accidents worldwide; this calls for innovation\nand the establishment of a smart, efficient and automated way of identifying\naccidents and calling for help to save lives. Addressing the problem of the\nscarcity of data, the presented framework joins Generative Adversarial Networks\n(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model\ntraining. Video frames for accidents and non-accidents are collected from\nYouTube videos, and we perform resizing, image enhancement and image\nnormalisation pixel range adjustments. Three models are used: CNN, Fine-tuned\nConvolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best\nfor detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,\nwhile the CNN model obtained 88%. Such results show that the proposed framework\nsuits traffic safety applications due to its high real-time accident detection\ncapabilities and broad-scale applicability. This work lays the foundation for\nintelligent surveillance systems in the future for real-time traffic\nmonitoring, smart city framework, and integration of intelligent surveillance\nsystems into emergency management systems."
    },
    {
        "date": "2025-06",
        "title": "MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models",
        "author": "Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, and Chao Yi",
        "link": "http://arxiv.org/abs/2506.16157v1",
        "abstract": "Referring Expression Segmentation (RES) enables precise object segmentation\nin images based on natural language descriptions, offering high flexibility and\nbroad applicability in real-world vision tasks. Despite its impressive\nperformance, the robustness of RES models against adversarial examples remains\nlargely unexplored. While prior adversarial attack methods have explored\nadversarial robustness on conventional segmentation models, they perform poorly\nwhen directly applied to RES, failing to expose vulnerabilities in its\nmultimodal structure. Moreover, in practical open-world scenarios, users\ntypically issue multiple, diverse referring expressions to interact with the\nsame image, highlighting the need for adversarial examples that generalize\nacross varied textual inputs. To address these multimodal challenges, we\npropose a novel adversarial attack strategy termed \\textbf{Multimodal\nBidirectional Attack}, tailored for RES models. Our method introduces learnable\nproxy textual embedding perturbation and jointly performs visual-aligned\noptimization on the image modality and textual-adversarial optimization on the\ntextual modality during attack generation. This dual optimization framework\nencourages adversarial images to actively adapt to more challenging text\nembedding during optimization, thereby enhancing their cross-text\ntransferability, which refers to the ability of adversarial examples to remain\neffective under a variety of unseen or semantically diverse textual inputs.\nExtensive experiments conducted on multiple RES models and benchmark datasets\ndemonstrate the superior effectiveness of our method compared to existing\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Probing the Robustness of Large Language Models Safety to Latent Perturbations",
        "author": "Tianle Gu, Kexin Huang, Zongqi Wang, Yixu Wang, Jie Li, Yuanqi Yao, Yang Yao, Yujiu Yang, Yan Teng, and Yingchun Wang",
        "link": "http://arxiv.org/abs/2506.16078v1",
        "abstract": "Safety alignment is a key requirement for building reliable Artificial\nGeneral Intelligence. Despite significant advances in safety alignment, we\nobserve that minor latent shifts can still trigger unsafe responses in aligned\nmodels. We argue that this stems from the shallow nature of existing alignment\nmethods, which focus on surface-level refusal behaviors without sufficiently\naltering internal representations. Consequently, small shifts in hidden\nactivations can re-trigger harmful behaviors embedded in the latent space. To\nexplore the robustness of safety alignment to latent perturbations, we\nintroduce a probing method that measures the Negative Log-Likelihood of the\noriginal response generated by the model. This probe quantifies local\nsensitivity in the latent space, serving as a diagnostic tool for identifying\nvulnerable directions. Based on this signal, we construct effective jailbreak\ntrajectories, giving rise to the Activation Steering Attack (ASA). More\nimportantly, these insights offer a principled foundation for improving\nalignment robustness. To this end, we introduce Layer-wise Adversarial Patch\nTraining~(LAPT), a fine-tuning strategy that inject controlled perturbations\ninto hidden representations during training. Experimental results highlight\nthat LAPT strengthen alignment robustness without compromising general\ncapabilities. Our findings reveal fundamental flaws in current alignment\nparadigms and call for representation-level training strategies that move\nbeyond surface-level behavior supervision. Codes and results are available at\nhttps://github.com/Carol-gutianle/LatentSafety."
    },
    {
        "date": "2025-06",
        "title": "A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems",
        "author": "Kexuan Wang, and An Liu",
        "link": "http://arxiv.org/abs/2506.16072v1",
        "abstract": "Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for\nits near-optimal weighted sum rate performance. However, its practical\ndeployment in massive multi-user (MU) multiple-input multiple-output (MIMO)\northogonal frequency-division multiplexing (OFDM) systems is hindered by the\nassumption of perfect channel state information (CSI) and high computational\ncomplexity. To address these issues, we first develop a wideband stochastic\nWMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted\nsum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight\nreinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),\nwhere each SWMMSE iteration is mapped to a network layer. Specifically, its DU\nmodule integrates approximation techniques and leverages beam-domain sparsity\nas well as frequency-domain subcarrier correlation, significantly accelerating\nconvergence and reducing computational overhead. Furthermore, the RL module\nadaptively adjusts the network depth and generates compensation matrices to\nmitigate approximation errors. Simulation results under imperfect CSI\ndemonstrate that RLDDU-Net outperforms existing baselines in EWSR performance\nwhile offering superior computational and convergence efficiency."
    },
    {
        "date": "2025-06",
        "title": "Floating-Point Neural Networks Are Provably Robust Universal Approximators",
        "author": "Geonho Hwang, Wonyeol Lee, Yeachan Park, Sejun Park, and Feras Saad",
        "link": "http://arxiv.org/abs/2506.16065v1",
        "abstract": "The classical universal approximation (UA) theorem for neural networks\nestablishes mild conditions under which a feedforward neural network can\napproximate a continuous function $f$ with arbitrary accuracy. A recent result\nshows that neural networks also enjoy a more general interval universal\napproximation (IUA) theorem, in the sense that the abstract interpretation\nsemantics of the network using the interval domain can approximate the direct\nimage map of $f$ (i.e., the result of applying $f$ to a set of inputs) with\narbitrary accuracy. These theorems, however, rest on the unrealistic assumption\nthat the neural network computes over infinitely precise real numbers, whereas\ntheir software implementations in practice compute over finite-precision\nfloating-point numbers. An open question is whether the IUA theorem still holds\nin the floating-point setting.\n  This paper introduces the first IUA theorem for floating-point neural\nnetworks that proves their remarkable ability to perfectly capture the direct\nimage map of any rounded target function $f$, showing no limits exist on their\nexpressiveness. Our IUA theorem in the floating-point setting exhibits material\ndifferences from the real-valued setting, which reflects the fundamental\ndistinctions between these two computational models. This theorem also implies\nsurprising corollaries, which include (i) the existence of provably robust\nfloating-point neural networks; and (ii) the computational completeness of the\nclass of straight-line programs that use only floating-point additions and\nmultiplications for the class of all floating-point programs that halt."
    },
    {
        "date": "2025-06",
        "title": "Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network",
        "author": "Zhuo Chen, Jialing He, Jiacheng Wang, Zehui Xiong, Tao Xiang, Liehuang Zhu, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2506.16023v1",
        "abstract": "Blockchain-based steganography enables data hiding via encoding the covert\ndata into a specific blockchain transaction field. However, previous works\nfocus on the specific field-embedding methods while lacking a consideration on\nrequired field-generation embedding. In this paper, we propose a generic\nblockchain-based steganography framework (GBSF). The sender generates the\nrequired fields such as amount and fees, where the additional covert data is\nembedded to enhance the channel capacity. Based on GBSF, we design a reversible\ngenerative adversarial network (R-GAN) that utilizes the generative adversarial\nnetwork with a reversible generator to generate the required fields and encode\nadditional covert data into the input noise of the reversible generator. We\nthen explore the performance flaw of R-GAN. To further improve the performance,\nwe propose R-GAN with Counter-intuitive data preprocessing and Custom\nactivation functions, namely CCR-GAN. The counter-intuitive data preprocessing\n(CIDP) mechanism is used to reduce decoding errors in covert data, while it\nincurs gradient explosion for model convergence. The custom activation function\nnamed ClipSigmoid is devised to overcome the problem. Theoretical justification\nfor CIDP and ClipSigmoid is also provided. We also develop a mechanism named\nT2C, which balances capacity and concealment. We conduct experiments using the\ntransaction amount of the Bitcoin mainnet as the required field to verify the\nfeasibility. We then apply the proposed schemes to other transaction fields and\nblockchains to demonstrate the scalability. Finally, we evaluate capacity and\nconcealment for various blockchains and transaction fields and explore the\ntrade-off between capacity and concealment. The results demonstrate that R-GAN\nand CCR-GAN are able to enhance the channel capacity effectively and outperform\nstate-of-the-art works."
    },
    {
        "date": "2025-06",
        "title": "Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal",
        "author": "Hemanth Kannamarlapudi, and Sowmya Chintalapudi",
        "link": "http://arxiv.org/abs/2506.16000v1",
        "abstract": "Navigation is a very crucial aspect of autonomous vehicle ecosystem which\nheavily relies on collecting and processing large amounts of data in various\nstates and taking a confident and safe decision to define the next vehicle\nmaneuver. In this paper, we propose a novel architecture based on Quantum\nArtificial Intelligence by enabling quantum and AI at various levels of\nnavigation decision making and communication process in Autonomous vehicles :\nQuantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum\nreinforcement learning for navigation policy optimization and finally\npost-quantum cryptographic protocols for secure communication. Quantum neural\nnetworks uses quantum amplitude encoding to fuse data from various sensors like\nLiDAR, radar, camera, GPS and weather etc., This approach gives a unified\nquantum state representation between heterogeneous sensor modalities. Nav-Q\nmodule processes the fused quantum states through variational quantum circuits\nto learn optimal navigation policies under swift dynamic and complex\nconditions. Finally, post quantum cryptographic protocols are used to secure\ncommunication channels for both within vehicle communication and V2X (Vehicle\nto Everything) communications and thus secures the autonomous vehicle\ncommunication from both classical and quantum security threats. Thus, the\nproposed framework addresses fundamental challenges in autonomous vehicles\nnavigation by providing quantum performance and future proof security. Index\nTerms Quantum Computing, Autonomous Vehicles, Sensor Fusion"
    },
    {
        "date": "2025-06",
        "title": "Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation",
        "author": "Connor Malone, Owen Claxton, Iman Shames, and Michael Milford",
        "link": "http://arxiv.org/abs/2506.15988v1",
        "abstract": "Stand-alone Visual Place Recognition (VPR) systems have little defence\nagainst a well-designed adversarial attack, which can lead to disastrous\nconsequences when deployed for robot navigation. This paper extensively\nanalyzes the effect of four adversarial attacks common in other perception\ntasks and four novel VPR-specific attacks on VPR localization performance. We\nthen propose how to close the loop between VPR, an Adversarial Attack Detector\n(AAD), and active navigation decisions by demonstrating the performance benefit\nof simulated AADs in a novel experiment paradigm -- which we detail for the\nrobotics community to use as a system framework. In the proposed experiment\nparadigm, we see the addition of AADs across a range of detection accuracies\ncan improve performance over baseline; demonstrating a significant improvement\n-- such as a ~50% reduction in the mean along-track localization error -- can\nbe achieved with True Positive and False Positive detection rates of only 75%\nand up to 25% respectively. We examine a variety of metrics including:\nAlong-Track Error, Percentage of Time Attacked, Percentage of Time in an\n`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on\nthese results, we provide the first investigation into the efficacy of the Fast\nGradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this\nwork highlights the need for AADs in real-world systems for trustworthy\nnavigation, and informs quantitative requirements for system design."
    },
    {
        "date": "2025-06",
        "title": "Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion",
        "author": "Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, and Elena V. Epure",
        "link": "http://arxiv.org/abs/2506.15981v1",
        "abstract": "The rapid advancement of AI-based music generation tools is revolutionizing\nthe music industry but also posing challenges to artists, copyright holders,\nand providers alike. This necessitates reliable methods for detecting such\nAI-generated content. However, existing detectors, relying on either audio or\nlyrics, face key practical limitations: audio-based detectors fail to\ngeneralize to new or unseen generators and are vulnerable to audio\nperturbations; lyrics-based methods require cleanly formatted and accurate\nlyrics, unavailable in practice. To overcome these limitations, we propose a\nnovel, practically grounded approach: a multimodal, modular late-fusion\npipeline that combines automatically transcribed sung lyrics and speech\nfeatures capturing lyrics-related information within the audio. By relying on\nlyrical aspects directly from audio, our method enhances robustness, mitigates\nsusceptibility to low-level artifacts, and enables practical applicability.\nExperiments show that our method, DE-detect, outperforms existing lyrics-based\ndetectors while also being more robust to audio perturbations. Thus, it offers\nan effective, robust solution for detecting AI-generated music in real-world\nscenarios. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection."
    },
    {
        "date": "2025-06",
        "title": "TRUST: Transparent, Robust and Ultra-Sparse Trees",
        "author": "Albert Dorador",
        "link": "http://arxiv.org/abs/2506.15791v1",
        "abstract": "Piecewise-constant regression trees remain popular for their\ninterpretability, yet often lag behind black-box models like Random Forest in\npredictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and\nUltra-Sparse Trees), a novel regression tree model that combines the accuracy\nof Random Forests with the interpretability of shallow decision trees and\nsparse linear models. TRUST further enhances transparency by leveraging Large\nLanguage Models to generate tailored, user-friendly explanations. Extensive\nvalidation on synthetic and real-world benchmark datasets demonstrates that\nTRUST consistently outperforms other interpretable models -- including CART,\nLasso, and Node Harvest -- in predictive accuracy, while matching the accuracy\nof Random Forest and offering substantial gains in both accuracy and\ninterpretability over M5', a well-established model that is conceptually\nrelated."
    },
    {
        "date": "2025-06",
        "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization",
        "author": "Ranting Hu",
        "link": "http://arxiv.org/abs/2506.15654v1",
        "abstract": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization."
    },
    {
        "date": "2025-06",
        "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review",
        "author": "Salijona Dyrmishi, Mohamed Djilani, Thibault Simonetto, Salah Ghamizi, and Maxime Cordy",
        "link": "http://arxiv.org/abs/2506.15506v1",
        "abstract": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning."
    },
    {
        "date": "2025-06",
        "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory",
        "author": "Atharv Singh Patlan, Ashwin Hebbar, Pramod Viswanath, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2506.17318v1",
        "abstract": "Autonomous web navigation agents, which translate natural language\ninstructions into sequences of browser actions, are increasingly deployed for\ncomplex tasks across e-commerce, information retrieval, and content discovery.\nDue to the stateless nature of large language models (LLMs), these agents rely\nheavily on external memory systems to maintain context across interactions.\nUnlike centralized systems where context is securely stored server-side, agent\nmemory is often managed client-side or by third-party applications, creating\nsignificant security vulnerabilities. This was recently exploited to attack\nproduction systems.\n  We introduce and formalize \"plan injection,\" a novel context manipulation\nattack that corrupts these agents' internal task representations by targeting\nthis vulnerable context. Through systematic evaluation of two popular web\nagents, Browser-use and Agent-E, we show that plan injections bypass robust\nprompt injection defenses, achieving up to 3x higher attack success rates than\ncomparable prompt-based attacks. Furthermore, \"context-chained injections,\"\nwhich craft logical bridges between legitimate user goals and attacker\nobjectives, lead to a 17.7% increase in success rate for privacy exfiltration\ntasks. Our findings highlight that secure memory handling must be a first-class\nconcern in agentic systems."
    },
    {
        "date": "2025-06",
        "title": "Beyond the Scope: Security Testing of Permission Management in Team Workspace",
        "author": "Liuhuo Wan, Chuan Yan, Mark Huasong Meng, Kailong Wang, Haoyu Wang, Guangdong Bai, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2506.17317v1",
        "abstract": "Nowadays team workspaces are widely adopted for multi-user collaboration and\ndigital resource management. To further broaden real-world applications,\nmainstream team workspaces platforms, such as Google Workspace and Microsoft\nOneDrive, allow third-party applications (referred to as add-ons) to be\nintegrated into their workspaces, significantly extending the functionality of\nteam workspaces. The powerful multi-user collaboration capabilities and\nintegration of add-ons make team workspaces a central hub for managing shared\nresources and protecting them against unauthorized access. Due to the\ncollaboration features of team workspaces, add-ons involved in collaborations\nmay bypass the permission isolation enforced by the administrator, unlike in\nsingle-user permission management.\n  This paper aims to investigate the permission management landscape of team\nworkspaces add-ons. To this end, we perform an in-depth analysis of the\nenforced access control mechanism inherent in this ecosystem, considering both\nmulti-user and cross-app features. We identify three potential security risks\nthat can be exploited to cause permission escalation. We then systematically\nreveal the landscape of permission escalation risks in the current ecosystem.\nSpecifically, we propose an automated tool, TAI, to systematically test all\npossible interactions within this ecosystem. Our evaluation reveals that\npermission escalation vulnerabilities are widespread in this ecosystem, with 41\ninteractions identified as problematic. Our findings should raise an alert to\nboth the team workspaces platforms and third-party developers."
    },
    {
        "date": "2025-06",
        "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning",
        "author": "Guoguo Ai, Hezhe Qiao, Hui Yan, and Guansong Pang",
        "link": "http://arxiv.org/abs/2506.15448v1",
        "abstract": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO."
    },
    {
        "date": "2025-06",
        "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service",
        "author": "Xiasi Wang, Tianliang Yao, Simin Chen, Runqi Wang, Lei YE, Kuofeng Gao, Yi Huang, and Yuan Yao",
        "link": "http://arxiv.org/abs/2506.15755v1",
        "abstract": "Vision-Language Models (VLMs) have demonstrated great potential in real-world\napplications. While existing research primarily focuses on improving their\naccuracy, the efficiency remains underexplored. Given the real-time demands of\nmany applications and the high inference overhead of VLMs, efficiency\nrobustness is a critical issue. However, previous studies evaluate efficiency\nrobustness under unrealistic assumptions, requiring access to the model\narchitecture and parameters -- an impractical scenario in ML-as-a-service\nsettings, where VLMs are deployed via inference APIs. To address this gap, we\npropose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness\nin a realistic black-box setting. VLMInferSlow incorporates fine-grained\nefficiency modeling tailored to VLM inference and leverages zero-order\noptimization to search for adversarial examples. Experimental results show that\nVLMInferSlow generates adversarial images with imperceptible perturbations,\nincreasing the computational cost by up to 128.47%. We hope this research\nraises the community's awareness about the efficiency robustness of VLMs."
    },
    {
        "date": "2025-06",
        "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments",
        "author": "Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang",
        "link": "http://arxiv.org/abs/2506.15253v1",
        "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval."
    },
    {
        "date": "2025-06",
        "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories",
        "author": "Qingsong Yan, Qiang Wang, Kaiyong Zhao, Jie Chen, Bo Li, Xiaowen Chu, and Fei Deng",
        "link": "http://arxiv.org/abs/2506.15242v2",
        "abstract": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories."
    },
    {
        "date": "2025-06",
        "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem",
        "author": "Yanxu Mao, Tiehan Cui, Peipei Liu, Datao You, and Hongsong Zhu",
        "link": "http://arxiv.org/abs/2506.15170v1",
        "abstract": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs."
    },
    {
        "date": "2025-06",
        "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography",
        "author": "Abdur Rahman, Keerthiveena Balraj, Manojkumar Ramteke, and Anurag Singh Rathore",
        "link": "http://arxiv.org/abs/2506.15166v1",
        "abstract": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND"
    },
    {
        "date": "2025-06",
        "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation",
        "author": "Hanbit Oh, Andrea M. Salcedo-V\u00e1zquez, Ixchel G. Ramirez-Alpizar, and Yukiyasu Domae",
        "link": "http://arxiv.org/abs/2506.15157v1",
        "abstract": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy."
    },
    {
        "date": "2025-06",
        "title": "Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading",
        "author": "Zhe Wang, Yuhua Ru, Aladine Chetouani, Tina Shiang, Fang Chen, Fabian Bauer, Liping Zhang, Didier Hans, Rachid Jennane, William Ewing Palmer, Mohamed Jarraya, and Yung Hsin Chen",
        "link": "http://arxiv.org/abs/2506.15748v1",
        "abstract": "Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged\nby significant inter-observer variability and the limited robustness of deep\nlearning models, particularly near critical decision boundaries. To address\nthese limitations, this paper proposes a novel framework, Diffusion-based\nCounterfactual Augmentation (DCA), which enhances model robustness and\ninterpretability by generating targeted counterfactual examples. The method\nnavigates the latent space of a diffusion model using a Stochastic Differential\nEquation (SDE), governed by balancing a classifier-informed boundary drive with\na manifold constraint. The resulting counterfactuals are then used within a\nself-corrective learning strategy to improve the classifier by focusing on its\nspecific areas of uncertainty. Extensive experiments on the public\nOsteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)\ndatasets demonstrate that this approach significantly improves classification\naccuracy across multiple model architectures. Furthermore, the method provides\ninterpretability by visualizing minimal pathological changes and revealing that\nthe learned latent space topology aligns with clinical knowledge of KOA\nprogression. The DCA framework effectively converts model uncertainty into a\nrobust training signal, offering a promising pathway to developing more\naccurate and trustworthy automated diagnostic systems. Our code is available at\nhttps://github.com/ZWang78/DCA."
    },
    {
        "date": "2025-06",
        "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation",
        "author": "Shizhao Peng, Shoumo Li, and Tianle Tao",
        "link": "http://arxiv.org/abs/2506.15102v1",
        "abstract": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications."
    },
    {
        "date": "2025-06",
        "title": "International Security Applications of Flexible Hardware-Enabled Guarantees",
        "author": "Onni Aarne, and James Petrie",
        "link": "http://arxiv.org/abs/2506.15100v1",
        "abstract": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability."
    },
    {
        "date": "2025-06",
        "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine",
        "author": "Rasha Karakchi, Rye Stahle-Smith, Nishant Chinnasami, and Tiffany Yu",
        "link": "http://arxiv.org/abs/2506.15070v1",
        "abstract": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators."
    },
    {
        "date": "2025-06",
        "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices",
        "author": "Gargi Mitra, Mohammadreza Hallajiyan, Inji Kim, Athish Pranav Dharmalingam, Mohammed Elnawawy, Shahrear Iqbal, Karthik Pattabiraman, and Homa Alemzadeh",
        "link": "http://arxiv.org/abs/2506.15028v1",
        "abstract": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients."
    },
    {
        "date": "2025-06",
        "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety",
        "author": "Christina Q. Knight, Kaustubh Deshpande, Ved Sirdeshmukh, Meher Mankikar, Scale Red Team, SEAL Research Team, and Julian Michael",
        "link": "http://arxiv.org/abs/2506.14922v2",
        "abstract": "The rapid advancement of large language models (LLMs) introduces dual-use\ncapabilities that could both threaten and bolster national security and public\nsafety (NSPS). Models implement safeguards to protect against potential misuse\nrelevant to NSPS and allow for benign users to receive helpful information.\nHowever, current benchmarks often fail to test safeguard robustness to\npotential NSPS risks in an objective, robust way. We introduce FORTRESS: 500\nexpert-crafted adversarial prompts with instance-based rubrics of 4-7 binary\nquestions for automated evaluation across 3 domains (unclassified information\nonly): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE),\nPolitical Violence & Terrorism, and Criminal & Financial Illicit Activities,\nwith 10 total subcategories across these domains. Each prompt-rubric pair has a\ncorresponding benign version to test for model over-refusals. This evaluation\nof frontier LLMs' safeguard robustness reveals varying trade-offs between\npotential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low\naverage risk score (ARS) (14.09 out of 100) but the highest over-refusal score\n(ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but\na high average potential risk (66.29). Deepseek-R1 has the highest ARS at\n78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even\ntrade-off between potential risks and over-refusals (with an ARS of 21.69 and\nORS of 5.2). To provide policymakers and researchers with a clear understanding\nof models' potential risks, we publicly release FORTRESS at\nhttps://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a\nprivate set for evaluation."
    },
    {
        "date": "2025-06",
        "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models",
        "author": "Xinkai Zhao, Yuta Tokuoka, Junichiro Iwasawa, and Keita Oda",
        "link": "http://arxiv.org/abs/2506.14919v1",
        "abstract": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods."
    },
    {
        "date": "2025-06",
        "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion",
        "author": "Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, and Ruimao Zhang",
        "link": "http://arxiv.org/abs/2506.14769v1",
        "abstract": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
    },
    {
        "date": "2025-06",
        "title": "SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification",
        "author": "Shuo Yang, Bardh Prenkaj, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2506.14587v2",
        "abstract": "Shortcut learning undermines model generalization to out-of-distribution\ndata. While the literature attributes shortcuts to biases in superficial\nfeatures, we show that imbalances in the semantic distribution of sample\nembeddings induce spurious semantic correlations, compromising model\nrobustness. To address this issue, we propose SCISSOR (Semantic Cluster\nIntervention for Suppressing ShORtcut), a Siamese network-based debiasing\napproach that remaps the semantic space by discouraging latent clusters\nexploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR\neliminates the need for data augmentation and rewriting. We evaluate SCISSOR on\n6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and\nGYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports\n+5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay,\nand +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models\nwith ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for\nBERT on NLP. Our study redefines the landscape of model generalization by\naddressing overlooked semantic biases, establishing SCISSOR as a foundational\nframework for mitigating shortcut learning and fostering more robust,\nbias-resistant AI systems."
    },
    {
        "date": "2025-06",
        "title": "Busting the Paper Ballot: Voting Meets Adversarial Machine Learning",
        "author": "Kaleel Mahmood, Caleb Manicke, Ethan Rathbun, Aayushi Verma, Sohaib Ahmad, Nicholas Stamatakis, Laurent Michel, and Benjamin Fuller",
        "link": "http://arxiv.org/abs/2506.14582v1",
        "abstract": "We show the security risk associated with using machine learning classifiers\nin United States election tabulators. The central classification task in\nelection tabulation is deciding whether a mark does or does not appear on a\nbubble associated to an alternative in a contest on the ballot. Barretto et al.\n(E-Vote-ID 2021) reported that convolutional neural networks are a viable\noption in this field, as they outperform simple feature-based classifiers.\n  Our contributions to election security can be divided into four parts. To\ndemonstrate and analyze the hypothetical vulnerability of machine learning\nmodels on election tabulators, we first introduce four new ballot datasets.\nSecond, we train and test a variety of different models on our new datasets.\nThese models include support vector machines, convolutional neural networks (a\nbasic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third,\nusing our new datasets and trained models, we demonstrate that traditional\nwhite box attacks are ineffective in the voting domain due to gradient masking.\nOur analyses further reveal that gradient masking is a product of numerical\ninstability. We use a modified difference of logits ratio loss to overcome this\nissue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct\nattacks with the adversarial examples generated using our new methods. In\ntraditional adversarial machine learning, a high (50% or greater) attack\nsuccess rate is ideal. However, for certain elections, even a 5% attack success\nrate can flip the outcome of a race. We show such an impact is possible in the\nphysical domain. We thoroughly discuss attack realism, and the challenges and\npracticality associated with printing and scanning ballot adversarial examples."
    },
    {
        "date": "2025-06",
        "title": "Doppelganger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack",
        "author": "Daewon Kang, YeongHwan Shin, Doyeon Kim, Kyu-Hwan Jung, and Meong Hi Son",
        "link": "http://arxiv.org/abs/2506.14539v2",
        "abstract": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelganger method'' to demonstrate\nthe risk of an agent being hijacked, thereby exposing system instructions and\ninternal information. Next, we define the ''Prompt Alignment Collapse under\nAdversarial Transfer (PACAT)'' level to evaluate the vulnerability to this\nadversarial transfer attack. We also propose a ''Caution for Adversarial\nTransfer (CAT)'' prompt to counter the Doppelganger method. The experimental\nresults demonstrate that the Doppelganger method can compromise the agent's\nconsistency and expose its internal information. In contrast, CAT prompts\nenable effective defense against this adversarial attack."
    },
    {
        "date": "2025-06",
        "title": "I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs",
        "author": "Yu Qi, Lipeng Gu, Honghua Chen, Liangliang Nan, and Mingqiang Wei",
        "link": "http://arxiv.org/abs/2506.14495v1",
        "abstract": "Existing 3D visual grounding methods rely on precise text prompts to locate\nobjects within 3D scenes. Speech, as a natural and intuitive modality, offers a\npromising alternative. Real-world speech inputs, however, often suffer from\ntranscription errors due to accents, background noise, and varying speech\nrates, limiting the applicability of existing 3DVG methods. To address these\nchallenges, we propose \\textbf{SpeechRefer}, a novel 3DVG framework designed to\nenhance performance in the presence of noisy and ambiguous speech-to-text\ntranscriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and\nintroduces two key innovations. First, the Speech Complementary Module captures\nacoustic similarities between phonetically related words and highlights subtle\ndistinctions, generating complementary proposal scores from the speech signal.\nThis reduces dependence on potentially erroneous transcriptions. Second, the\nContrastive Complementary Module employs contrastive learning to align\nerroneous text features with corresponding speech features, ensuring robust\nperformance even when transcription errors dominate. Extensive experiments on\nthe SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves\nthe performance of existing 3DVG methods by a large margin, which highlights\nSpeechRefer's potential to bridge the gap between noisy speech inputs and\nreliable 3DVG, enabling more intuitive and practical multimodal systems."
    },
    {
        "date": "2025-06",
        "title": "LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops",
        "author": "Jiyuan Fu, Kaixun Jiang, Lingyi Hong, Jinglun Li, Haijing Guo, Dingkang Yang, Zhaoyu Chen, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2506.14493v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown great promise but require\nsubstantial computational resources during inference. Attackers can exploit\nthis by inducing excessive output, leading to resource exhaustion and service\ndegradation. Prior energy-latency attacks aim to increase generation time by\nbroadly shifting the output token distribution away from the EOS token, but\nthey neglect the influence of token-level Part-of-Speech (POS) characteristics\non EOS and sentence-level structural patterns on output counts, limiting their\nefficacy. To address this, we propose LingoLoop, an attack designed to induce\nMLLMs to generate excessively verbose and repetitive sequences. First, we find\nthat the POS tag of a token strongly affects the likelihood of generating an\nEOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to\npostpone EOS token generation by adjusting attention weights guided by POS\ninformation. Second, we identify that constraining output diversity to induce\nrepetitive loops is effective for sustained generation. We introduce a\nGenerative Path Pruning Mechanism that limits the magnitude of hidden states,\nencouraging the model to produce persistent loops. Extensive experiments\ndemonstrate LingoLoop can increase generated tokens by up to 30 times and\nenergy consumption by a comparable factor on models like Qwen2.5-VL-3B,\nconsistently driving MLLMs towards their maximum generation limits. These\nfindings expose significant MLLMs' vulnerabilities, posing challenges for their\nreliable deployment. The code will be released publicly following the paper's\nacceptance."
    },
    {
        "date": "2025-06",
        "title": "ReDASH: Fast and efficient Scaling in Arithmetic Garbled Circuits for Secure Outsourced Inference",
        "author": "Felix Maurer, Jonas Sander, and Thomas Eisenbarth",
        "link": "http://arxiv.org/abs/2506.14489v1",
        "abstract": "ReDash extends Dash's arithmetic garbled circuits to provide a more flexible\nand efficient framework for secure outsourced inference. By introducing a novel\ngarbled scaling gadget based on a generalized base extension for the residue\nnumber system, ReDash removes Dash's limitation of scaling exclusively by\npowers of two. This enables arbitrary scaling factors drawn from the residue\nnumber system's modular base, allowing for tailored quantization schemes and\nmore efficient model evaluation.\n  Through the new $\\text{ScaleQuant}^+$ quantization mechanism, ReDash supports\noptimized modular bases that can significantly reduce the overhead of\narithmetic operations during convolutional neural network inference. ReDash\nachieves up to a 33-fold speedup in overall inference time compared to Dash\nDespite these enhancements, ReDash preserves the robust security guarantees of\narithmetic garbling. By delivering both performance gains and quantization\nflexibility, ReDash expands the practicality of garbled convolutional neural\nnetwork inference."
    },
    {
        "date": "2025-06",
        "title": "GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies",
        "author": "Jingqi Yang, Zhilong Song, Jiawei Chen, Mingli Song, Sheng Zhou, linjun sun, Xiaogang Ouyang, Chun Chen, and Can Wang",
        "link": "http://arxiv.org/abs/2506.14477v1",
        "abstract": "The development of high-quality datasets is crucial for benchmarking and\nadvancing research in Graphical User Interface (GUI) agents. Despite their\nimportance, existing datasets are often constructed under idealized conditions,\noverlooking the diverse anomalies frequently encountered in real-world\ndeployments. To address this limitation, we introduce GUI-Robust, a novel\ndataset designed for comprehensive GUI agent evaluation, explicitly\nincorporating seven common types of anomalies observed in everyday GUI\ninteractions. Furthermore, we propose a semi-automated dataset construction\nparadigm that collects user action sequences from natural interactions via RPA\ntools and then generate corresponding step and task descriptions for these\nactions with the assistance of MLLMs. This paradigm significantly reduces\nannotation time cost by a factor of over 19 times. Finally, we assess\nstate-of-the-art GUI agents using the GUI-Robust dataset, revealing their\nsubstantial performance degradation in abnormal scenarios. We anticipate that\nour work will highlight the importance of robustness in GUI agents and inspires\nmore future research in this direction. The dataset and code are available at\nhttps://github.com/chessbean1/GUI-Robust.."
    },
    {
        "date": "2025-06",
        "title": "LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data",
        "author": "Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2506.14474v1",
        "abstract": "Large language models (LLMs) can be trained or fine-tuned on data obtained\nwithout the owner's consent. Verifying whether a specific LLM was trained on\nparticular data instances or an entire dataset is extremely challenging.\nDataset watermarking addresses this by embedding identifiable modifications in\ntraining data to detect unauthorized use. However, existing methods often lack\nstealth, making them relatively easy to detect and remove. In light of these\nlimitations, we propose LexiMark, a novel watermarking technique designed for\ntext and documents, which embeds synonym substitutions for carefully selected\nhigh-entropy words. Our method aims to enhance an LLM's memorization\ncapabilities on the watermarked text without altering the semantic integrity of\nthe text. As a result, the watermark is difficult to detect, blending\nseamlessly into the text with no visible markers, and is resistant to removal\ndue to its subtle, contextually appropriate substitutions that evade automated\nand manual detection. We evaluated our method using baseline datasets from\nrecent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral\n7B, Pythia 6.9B, as well as three smaller variants from the Pythia family\n(160M, 410M, and 1B). Our evaluation spans multiple training settings,\nincluding continued pretraining and fine-tuning scenarios. The results\ndemonstrate significant improvements in AUROC scores compared to existing\nmethods, underscoring our method's effectiveness in reliably verifying whether\nunauthorized watermarked data was used in LLM training."
    },
    {
        "date": "2025-06",
        "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control",
        "author": "Yaqiao Zhu, Hongkai Wen, Geyong Min, and Man Luo",
        "link": "http://arxiv.org/abs/2506.14391v1",
        "abstract": "Efficient traffic signal control (TSC) is essential for mitigating urban\ncongestion, yet existing reinforcement learning (RL) methods face challenges in\nscaling to large networks while maintaining global coordination. Centralized RL\nsuffers from scalability issues, while decentralized approaches often lack\nunified objectives, resulting in limited network-level efficiency. In this\npaper, we propose HiLight, a hierarchical reinforcement learning framework with\nglobal adversarial guidance for large-scale TSC. HiLight consists of a\nhigh-level Meta-Policy, which partitions the traffic network into subregions\nand generates sub-goals using a Transformer-LSTM architecture, and a low-level\nSub-Policy, which controls individual intersections with global awareness. To\nimprove the alignment between global planning and local execution, we introduce\nan adversarial training mechanism, where the Meta-Policy generates challenging\nyet informative sub-goals, and the Sub-Policy learns to surpass these targets,\nleading to more effective coordination. We evaluate HiLight across both\nsynthetic and real-world benchmarks, and additionally construct a large-scale\nManhattan network with diverse traffic conditions, including peak transitions,\nadverse weather, and holiday surges. Experimental results show that HiLight\nexhibits significant advantages in large-scale scenarios and remains\ncompetitive across standard benchmarks of varying sizes."
    },
    {
        "date": "2025-06",
        "title": "Excessive Reasoning Attack on Reasoning LLMs",
        "author": "Wai Man Si, Mingjie Li, Michael Backes, and Yang Zhang",
        "link": "http://arxiv.org/abs/2506.14374v1",
        "abstract": "Recent reasoning large language models (LLMs), such as OpenAI o1 and\nDeepSeek-R1, exhibit strong performance on complex tasks through test-time\ninference scaling. However, prior studies have shown that these models often\nincur significant computational costs due to excessive reasoning, such as\nfrequent switching between reasoning trajectories (e.g., underthinking) or\nredundant reasoning on simple questions (e.g., overthinking). In this work, we\nexpose a novel threat: adversarial inputs can be crafted to exploit excessive\nreasoning behaviors and substantially increase computational overhead without\ncompromising model utility. Therefore, we propose a novel loss framework\nconsisting of three components: (1) Priority Cross-Entropy Loss, a modification\nof the standard cross-entropy objective that emphasizes key tokens by\nleveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss,\nwhich encourages the model to initiate additional reasoning paths during\ninference; and (3) Delayed Termination Loss, which is designed to extend the\nreasoning process and defer the generation of final outputs. We optimize and\nevaluate our attack for the GSM8K and ORCA datasets on\nDeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results\ndemonstrate a 3x to 9x increase in reasoning length with comparable utility\nperformance. Furthermore, our crafted adversarial inputs exhibit\ntransferability, inducing computational overhead in o3-mini, o1-mini,\nDeepSeek-R1, and QWQ models."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Learning to Optimize with Theoretical Guarantees",
        "author": "Qingyu Song, Wei Lin, Juncheng Wang, and Hong Xu",
        "link": "http://arxiv.org/abs/2506.14263v1",
        "abstract": "Learning to optimize (L2O) is an emerging technique to solve mathematical\noptimization problems with learning-based methods. Although with great success\nin many real-world scenarios such as wireless communications, computer\nnetworks, and electronic design, existing L2O works lack theoretical\ndemonstration of their performance and robustness in out-of-distribution (OOD)\nscenarios. We address this gap by providing comprehensive proofs. First, we\nprove a sufficient condition for a robust L2O model with homogeneous\nconvergence rates over all In-Distribution (InD) instances. We assume an L2O\nmodel achieves robustness for an InD scenario. Based on our proposed\nmethodology of aligning OOD problems to InD problems, we also demonstrate that\nthe L2O model's convergence rate in OOD scenarios will deteriorate by an\nequation of the L2O model's input features. Moreover, we propose an L2O model\nwith a concise gradient-only feature construction and a novel gradient-based\nhistory modeling method. Numerical simulation demonstrates that our proposed\nmodel outperforms the state-of-the-art baseline in both InD and OOD scenarios\nand achieves up to 10 $\\times$ convergence speedup. The code of our method can\nbe found from https://github.com/NetX-lab/GoMathL2O-Official."
    },
    {
        "date": "2025-06",
        "title": "synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?",
        "author": "Johannes Flotzinger, Fabian Deuser, Achref Jaziri, Heiko Neumann, Norbert Oswald, Visvanathan Ramesh, and Thomas Braml",
        "link": "http://arxiv.org/abs/2506.14255v1",
        "abstract": "Adequate bridge inspection is increasingly challenging in many countries due\nto growing ailing stocks, compounded with a lack of staff and financial\nresources. Automating the key task of visual bridge inspection, classification\nof defects and building components on pixel level, improves efficiency,\nincreases accuracy and enhances safety in the inspection process and resulting\nbuilding assessment. Models overtaking this task must cope with an assortment\nof real-world conditions. They must be robust to variations in image quality,\nas well as background texture, as defects often appear on surfaces of diverse\ntexture and degree of weathering. dacl10k is the largest and most diverse\ndataset for real-world concrete bridge inspections. However, the dataset\nexhibits class imbalance, which leads to notably poor model performance\nparticularly when segmenting fine-grained classes such as cracks and cavities.\nThis work introduces \"synth-dacl\", a compilation of three novel dataset\nextensions based on synthetic concrete textures. These extensions are designed\nto balance class distribution in dacl10k and enhance model performance,\nespecially for crack and cavity segmentation. When incorporating the synth-dacl\nextensions, we observe substantial improvements in model robustness across 15\nperturbed test sets. Notably, on the perturbed test set, a model trained on\ndacl10k combined with all synthetic extensions achieves a 2% increase in mean\nIoU, F1 score, Recall, and Precision compared to the same model trained solely\non dacl10k."
    },
    {
        "date": "2025-06",
        "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis",
        "author": "Varun Mannam, and Zhenyu Shi",
        "link": "http://arxiv.org/abs/2506.14854v2",
        "abstract": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring."
    },
    {
        "date": "2025-06",
        "title": "Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data",
        "author": "Tatthapong Srikitrungruang, Matthew Lemon, Sina Aghaee Dabaghan Fard, Jaesung Lee, and Yuxiao Zhou",
        "link": "http://arxiv.org/abs/2506.14036v2",
        "abstract": "Accurately estimating spatially heterogeneous elasticity parameters,\nparticularly Young's modulus and Poisson's ratio, from noisy displacement\nmeasurements remains significantly challenging in inverse elasticity problems.\nExisting inverse estimation techniques are often limited by instability,\npronounced sensitivity to measurement noise, and difficulty in recovering\nabsolute-scale Young's modulus. This work presents a novel Inverse Elasticity\nPhysics-Informed Neural Network (IE-PINN) specifically designed to robustly\nreconstruct heterogeneous distributions of elasticity parameters from noisy\ndisplacement data based on linear elasticity physics. IE-PINN integrates three\ndistinct neural network architectures dedicated to separately modeling\ndisplacement fields, strain fields, and elasticity distributions, thereby\nsignificantly enhancing stability and accuracy against measurement noise.\nAdditionally, a two-phase estimation strategy is introduced: the first phase\nrecovers relative spatial distributions of Young's modulus and Poisson's ratio,\nand the second phase calibrates the absolute scale of Young's modulus using\nimposed loading boundary conditions. Additional methodological innovations,\nincluding positional encoding, sine activation functions, and a sequential\npretraining protocol, further enhance the model's performance and robustness.\nExtensive numerical experiments demonstrate that IE-PINN effectively overcomes\ncritical limitations encountered by existing methods, delivering accurate\nabsolute-scale elasticity estimations even under severe noise conditions. This\nadvancement holds substantial potential for clinical imaging diagnostics and\nmechanical characterization, where measurements typically encounter substantial\nnoise."
    },
    {
        "date": "2025-06",
        "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models",
        "author": "Quan Nguyen, Minh N. Vu, Truc Nguyen, and My T. Thai",
        "link": "http://arxiv.org/abs/2506.17292v1",
        "abstract": "Federated Learning enables collaborative learning among clients via a\ncoordinating server while avoiding direct data sharing, offering a perceived\nsolution to preserve privacy. However, recent studies on Membership Inference\nAttacks (MIAs) have challenged this notion, showing high success rates against\nunprotected training data. While local differential privacy (LDP) is widely\nregarded as a gold standard for privacy protection in data analysis, most\nstudies on MIAs either neglect LDP or fail to provide theoretical guarantees\nfor attack success rates against LDP-protected data. To address this gap, we\nderive theoretical lower bounds for the success rates of low-polynomial time\nMIAs that exploit vulnerabilities in fully connected or self-attention layers.\nWe establish that even when data are protected by LDP, privacy risks persist,\ndepending on the privacy budget. Practical evaluations on federated vision\nmodels confirm considerable privacy risks, revealing that the noise required to\nmitigate these attacks significantly degrades models' utility."
    },
    {
        "date": "2025-06",
        "title": "Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble",
        "author": "Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, and Lei Yu",
        "link": "http://arxiv.org/abs/2506.13972v1",
        "abstract": "Membership inference attacks (MIAs) pose a significant threat to the privacy\nof machine learning models and are widely used as tools for privacy assessment,\nauditing, and machine unlearning. While prior MIA research has primarily\nfocused on performance metrics such as AUC, accuracy, and TPR@low FPR - either\nby developing new methods to enhance these metrics or using them to evaluate\nprivacy solutions - we found that it overlooks the disparities among different\nattacks. These disparities, both between distinct attack methods and between\nmultiple instantiations of the same method, have crucial implications for the\nreliability and completeness of MIAs as privacy evaluation tools. In this\npaper, we systematically investigate these disparities through a novel\nframework based on coverage and stability analysis. Extensive experiments\nreveal significant disparities in MIAs, their potential causes, and their\nbroader implications for privacy evaluation. To address these challenges, we\npropose an ensemble framework with three distinct strategies to harness the\nstrengths of state-of-the-art MIAs while accounting for their disparities. This\nframework not only enables the construction of more powerful attacks but also\nprovides a more robust and comprehensive methodology for privacy evaluation."
    },
    {
        "date": "2025-06",
        "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning",
        "author": "Zhenhao Zhu, Yue Liu, Yingwei Ma, Hongcheng Gao, Nuo Chen, Yanpei Guo, Wenjie Qu, Huiying Xu, Xinzhong Zhu, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2506.13737v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated promising performance in\ncomplex tasks. However, the resource-consuming reasoning processes may be\nexploited by attackers to maliciously occupy the resources of the servers,\nleading to a crash, like the DDoS attack in cyber. To this end, we propose a\nnovel attack method on LRMs termed ExtendAttack to maliciously occupy the\nresources of servers by stealthily extending the reasoning processes of LRMs.\nConcretely, we systematically obfuscate characters within a benign prompt,\ntransforming them into a complex, poly-base ASCII representation. This compels\nthe model to perform a series of computationally intensive decoding sub-tasks\nthat are deeply embedded within the semantic structure of the query itself.\nExtensive experiments demonstrate the effectiveness of our proposed\nExtendAttack. Remarkably, it increases the length of the model's response by\nover 2.5 times for the o3 model on the HumanEval benchmark. Besides, it\npreserves the original meaning of the query and achieves comparable answer\naccuracy, showing the stealthiness."
    },
    {
        "date": "2025-06",
        "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
        "author": "Arjun Krishna, Aaditya Rastogi, and Erick Galinkin",
        "link": "http://arxiv.org/abs/2506.13726v1",
        "abstract": "The introduction of advanced reasoning capabilities have improved the\nproblem-solving performance of large language models, particularly on math and\ncoding benchmarks. However, it remains unclear whether these reasoning models\nare more or less vulnerable to adversarial prompt attacks than their\nnon-reasoning counterparts. In this work, we present a systematic evaluation of\nweaknesses in advanced reasoning models compared to similar non-reasoning\nmodels across a diverse set of prompt-based attack categories. Using\nexperimental data, we find that on average the reasoning-augmented models are\n\\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\%\nattack success rate, lower is better). However, this overall trend masks\nsignificant category-specific differences: for certain attack types the\nreasoning models are substantially \\emph{more vulnerable} (e.g., up to 32\npercentage points worse on a tree-of-attacks prompt), while for others they are\nmarkedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting\ninjection). Our findings highlight the nuanced security implications of\nadvanced reasoning in language models and emphasize the importance of\nstress-testing safety across diverse adversarial techniques."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Disentanglement by Backpropagation with Physics-Informed Variational Autoencoder",
        "author": "Ioannis Christoforos Koune, and Alice Cicirello",
        "link": "http://arxiv.org/abs/2506.13658v1",
        "abstract": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach."
    },
    {
        "date": "2025-06",
        "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning",
        "author": "Zhiqiang Li, Haiyong Bao, Menghong Guan, Hao Pan, Cheng Huang, and Hong-Ning Dai",
        "link": "http://arxiv.org/abs/2506.13612v1",
        "abstract": "Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security."
    },
    {
        "date": "2025-06",
        "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks",
        "author": "Yali Yuan, Kai Xu, Ruolin Ma, and Yuchen Zhang",
        "link": "http://arxiv.org/abs/2506.13563v1",
        "abstract": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings."
    },
    {
        "date": "2025-06",
        "title": "LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations",
        "author": "Lorenzo Bini, and Stephane Marchand-Maillet",
        "link": "http://arxiv.org/abs/2506.13344v1",
        "abstract": "Generating high-fidelity and biologically plausible synthetic single-cell RNA\nsequencing (scRNA-seq) data, especially with conditional control, is\nchallenging due to its high dimensionality, sparsity, and complex biological\nvariations. Existing generative models often struggle to capture these unique\ncharacteristics and ensure robustness to structural noise in cellular networks.\nWe introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model\nfor robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates\ngraph-based representations with a score-based diffusion model, enhanced by a\nnovel spectral adversarial perturbation mechanism on graph edge weights. Our\ncontributions are threefold: we leverage Laplacian Positional Encodings (LPEs)\nto enrich the latent space with crucial cellular relationship information; we\ndevelop a conditional score-based diffusion model for effective learning and\ngeneration from complex scRNA-seq distributions; and we employ a unique\nspectral adversarial training scheme on graph edge weights, boosting robustness\nagainst structural variations. Extensive experiments on diverse scRNA-seq\ndatasets demonstrate LapDDPM's superior performance, achieving high fidelity\nand generating biologically-plausible, cell-type-specific samples. LapDDPM sets\na new benchmark for conditional scRNA-seq data generation, offering a robust\ntool for various downstream biological applications."
    },
    {
        "date": "2025-06",
        "title": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks",
        "author": "Yuefei Lyu, Chaozhuo Li, Xi Zhang, and Tianle Zhang",
        "link": "http://arxiv.org/abs/2506.13276v1",
        "abstract": "Text-attributed graphs (TAGs) integrate textual data with graph structures,\nproviding valuable insights in applications such as social network analysis and\nrecommendation systems. Graph Neural Networks (GNNs) effectively capture both\ntopological structure and textual information in TAGs but are vulnerable to\nadversarial attacks. Existing graph injection attack (GIA) methods assume that\nattackers can directly manipulate the embedding layer, producing\nnon-explainable node embeddings. Furthermore, the effectiveness of these\nattacks often relies on surrogate models with high training costs. Thus, this\npaper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.\nOur approach leverages large language models (LLMs) to generate interpretable\ntext-level node attributes directly, ensuring attacks remain feasible in\nreal-world scenarios. We design strategies for LLM prompting that balance\nexploration and reliability to guide text generation, and propose a similarity\nassessment method to evaluate attack text effectiveness in disrupting graph\nhomophily. This method efficiently perturbs the target node with minimal\ntraining costs in a strict black-box setting, ensuring a text-level graph\ninjection attack for TAGs. Experiments on real-world TAG datasets validate the\nsuperior performance of ATAG-LLM compared to state-of-the-art embedding-level\nand text-level attack methods."
    },
    {
        "date": "2025-06",
        "title": "Building Automotive Security on Internet Standards: An Integration of DNSSEC, DANE, and DANCE to Authenticate and Authorize In-Car Services",
        "author": "Timo Salomon, Mehmet Mueller, Philipp Meyer, and Thomas C. Schmidt",
        "link": "http://arxiv.org/abs/2506.13261v1",
        "abstract": "The automotive industry is undergoing a software-as-a-service transformation\nthat enables software-defined functions and post-sale updates via cloud and\nvehicle-to-everything communication. Connectivity in cars introduces\nsignificant security challenges, as remote attacks on vehicles have become\nincreasingly prevalent. Current automotive designs call for security solutions\nthat address the entire lifetime of a vehicle. In this paper, we propose to\nauthenticate and authorize in-vehicle services by integrating DNSSEC, DANE, and\nDANCE with automotive middleware. Our approach decouples the cryptographic\nauthentication of the service from that of the service deployment with the help\nof DNSSEC and thereby largely simplifies key management. We propose to\nauthenticate in-vehicle services by certificates that are solely generated by\nthe service suppliers but published on deployment via DNSSEC TLSA records\nsolely signed by the OEM. Building on well-established Internet standards\nensures interoperability with various current and future protocols, scalable\nmanagement of credentials for millions of connected vehicles at\nwell-established security levels. We back our design proposal by a security\nanalysis using the STRIDE threat model and by evaluations in a realistic\nin-vehicle setup that demonstrate its effectiveness."
    },
    {
        "date": "2025-06",
        "title": "No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!",
        "author": "Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Christian Kroer",
        "link": "http://arxiv.org/abs/2506.13244v3",
        "abstract": "We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan."
    },
    {
        "date": "2025-06",
        "title": "Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study",
        "author": "Dang Viet Anh Nguyen, Carlos Lima Azevedo, Tomer Toledo, and Filipe Rodrigues",
        "link": "http://arxiv.org/abs/2506.13836v1",
        "abstract": "Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a\npromising approach for improving urban mobility. However, its robustness under\nreal-world disruptions such as traffic incidents remains largely underexplored.\nIn this study, we introduce T-REX, an open-source, SUMO-based simulation\nframework for training and evaluating RL-TSC methods under dynamic, incident\nscenarios. T-REX models realistic network-level performance considering\ndrivers' probabilistic rerouting, speed adaptation, and contextual\nlane-changing, enabling the simulation of congestion propagation under\nincidents. To assess robustness, we propose a suite of metrics that extend\nbeyond conventional traffic efficiency measures. Through extensive experiments\nacross synthetic and real-world networks, we showcase T-REX for the evaluation\nof several state-of-the-art RL-TSC methods under multiple real-world deployment\nparadigms. Our findings show that while independent value-based and\ndecentralized pressure-based methods offer fast convergence and generalization\nin stable traffic conditions and homogeneous networks, their performance\ndegrades sharply under incident-driven distribution shifts. In contrast,\nhierarchical coordination methods tend to offer more stable and adaptable\nperformance in large-scale, irregular networks, benefiting from their\nstructured decision-making architecture. However, this comes with the trade-off\nof slower convergence and higher training complexity. These findings highlight\nthe need for robustness-aware design and evaluation in RL-TSC research. T-REX\ncontributes to this effort by providing an open, standardized and reproducible\nplatform for benchmarking RL methods under dynamic and disruptive traffic\nscenarios."
    },
    {
        "date": "2025-06",
        "title": "Using LLMs for Security Advisory Investigations: How Far Are We?",
        "author": "Bayu Fedra Abdullah, Yusuf Sulistyo Nugroho, Brittany Reid, Raula Gaikovina Kula, Kazumasa Shimari, and Kenichi Matsumoto",
        "link": "http://arxiv.org/abs/2506.13161v1",
        "abstract": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration."
    },
    {
        "date": "2025-06",
        "title": "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online Auctions",
        "author": "Steven Su, Erik Rye, Dave Levin, and Robert Beverly",
        "link": "http://arxiv.org/abs/2506.13052v2",
        "abstract": "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers."
    },
    {
        "date": "2025-06",
        "title": "HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs",
        "author": "Zijian Zhang, Xuecheng Wu, Danlei Huang, Siyu Yan, Chong Peng, and Xuezhi Cao",
        "link": "http://arxiv.org/abs/2506.13038v2",
        "abstract": "Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains."
    },
    {
        "date": "2025-06",
        "title": "Position: Certified Robustness Does Not (Yet) Imply Model Security",
        "author": "Andrew C. Cullen, Paul Montague, Sarah M. Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2506.13024v1",
        "abstract": "While certified robustness is widely promoted as a solution to adversarial\nexamples in Artificial Intelligence systems, significant challenges remain\nbefore these techniques can be meaningfully deployed in real-world\napplications. We identify critical gaps in current research, including the\nparadox of detection without distinction, the lack of clear criteria for\npractitioners to evaluate certification schemes, and the potential security\nrisks arising from users' expectations surrounding ``guaranteed\" robustness\nclaims. This position paper is a call to arms for the certification research\ncommunity, proposing concrete steps to address these fundamental challenges and\nadvance the field toward practical applicability."
    },
    {
        "date": "2025-06",
        "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
        "author": "Nima Naderloui, Shenao Yan, Binghui Wang, Jie Fu, Wendy Hui Wang, Weiran Liu, and Yuan Hong",
        "link": "http://arxiv.org/abs/2506.13009v1",
        "abstract": "Machine unlearning focuses on efficiently removing specific data from trained\nmodels, addressing privacy and compliance concerns with reasonable costs.\nAlthough exact unlearning ensures complete data removal equivalent to\nretraining, it is impractical for large-scale models, leading to growing\ninterest in inexact unlearning methods. However, the lack of formal guarantees\nin these methods necessitates the need for robust evaluation frameworks to\nassess their privacy and effectiveness. In this work, we first identify several\nkey pitfalls of the existing unlearning evaluation frameworks, e.g., focusing\non average-case evaluation or targeting random samples for evaluation,\nincomplete comparisons with the retraining baseline. Then, we propose RULI\n(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel\nframework to address critical gaps in the evaluation of inexact unlearning\nmethods. RULI introduces a dual-objective attack to measure both unlearning\nefficacy and privacy risks at a per-sample granularity. Our findings reveal\nsignificant vulnerabilities in state-of-the-art unlearning methods, where RULI\nachieves higher attack success rates, exposing privacy risks underestimated by\nexisting methods. Built on a game-based foundation and validated through\nempirical evaluations on both image and text data (spanning tasks from\nclassification to generation), RULI provides a rigorous, scalable, and\nfine-grained methodology for evaluating unlearning techniques."
    },
    {
        "date": "2025-06",
        "title": "Open Source, Open Threats? Investigating Security Challenges in Open-Source Software",
        "author": "Seyed Ali Akhavani, Behzad Ousat, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2506.12995v1",
        "abstract": "Open-source software (OSS) has become increasingly more popular across\ndifferent domains. However, this rapid development and widespread adoption come\nwith a security cost. The growing complexity and openness of OSS ecosystems\nhave led to increased exposure to vulnerabilities and attack surfaces. This\npaper investigates the trends and patterns of reported vulnerabilities within\nOSS platforms, focusing on the implications of these findings for security\npractices. To understand the dynamics of OSS vulnerabilities, we analyze a\ncomprehensive dataset comprising 31,267 unique vulnerability reports from\nGitHub's advisory database and Snyk.io, belonging to 14,675 packages across 10\nprogramming languages. Our analysis reveals a significant surge in reported\nvulnerabilities, increasing at an annual rate of 98%, far outpacing the 25%\naverage annual growth in the number of open-source software (OSS) packages.\nAdditionally, we observe an 85% increase in the average lifespan of\nvulnerabilities across ecosystems during the studied period, indicating a\npotential decline in security. We identify the most prevalent Common Weakness\nEnumerations (CWEs) across programming languages and find that, on average,\njust seven CWEs are responsible for over 50% of all reported vulnerabilities.\nWe further examine these commonly observed CWEs and highlight\necosystem-specific trends. Notably, we find that vulnerabilities associated\nwith intentionally malicious packages comprise 49% of reports in the NPM\necosystem and 14% in PyPI, an alarming indication of targeted attacks within\npackage repositories. We conclude with an in-depth discussion of the\ncharacteristics and attack vectors associated with these malicious packages."
    },
    {
        "date": "2025-06",
        "title": "Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks",
        "author": "Erica Cai, Xi Chen, Reagan Grey Keeney, Ethan Zuckerman, Brendan O'Connor, and Przemyslaw A. Grabowicz",
        "link": "http://arxiv.org/abs/2506.12925v1",
        "abstract": "Comparative studies of news coverage are challenging to conduct because\nmethods to identify news articles about the same event in different languages\nrequire expertise that is difficult to scale. We introduce an AI-powered method\nfor identifying news articles based on an event FINGERPRINT, which is a minimal\nset of metadata required to identify critical events. Our event coverage\nidentification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),\nefficiently identifies news articles about critical world events, specifically\nterrorist attacks and several types of natural disasters. FAME does not require\ntraining data and is able to automatically and efficiently identify news\narticles that discuss an event given its fingerprint: time, location, and class\n(such as storm or flood). The method achieves state-of-the-art performance and\nscales to massive databases of tens of millions of news articles and hundreds\nof events happening globally. We use FAME to identify 27,441 articles that\ncover 470 natural disaster and terrorist attack events that happened in 2020.\nTo this end, we use a massive database of news articles in three languages from\nMediaCloud, and three widely used, expert-curated databases of critical events:\nEM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior\nliterature: coverage of disasters and terrorist attacks correlates to death\ncounts, to the GDP of a country where the event occurs, and to trade volume\nbetween the reporting country and the country where the event occurred. We\nshare our NLP annotations and cross-country media attention data to support the\nefforts of researchers and media monitoring organizations."
    },
    {
        "date": "2025-06",
        "title": "Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs",
        "author": "Lu Chen, Han Yang, Hu Wang, Yuxin Cao, Shaofeng Li, and Yuan Luo",
        "link": "http://arxiv.org/abs/2506.12875v1",
        "abstract": "Adversarial examples have attracted significant attention over the years, yet\nunderstanding their frequency-based characteristics remains insufficient. In\nthis paper, we investigate the intriguing properties of adversarial examples in\nthe frequency domain for the image classification task, with the following key\nfindings. (1) As the high-frequency components increase, the performance gap\nbetween adversarial and natural examples becomes increasingly pronounced. (2)\nThe model performance against filtered adversarial examples initially increases\nto a peak and declines to its inherent robustness. (3) In Convolutional Neural\nNetworks, mid- and high-frequency components of adversarial examples exhibit\ntheir attack capabilities, while in Transformers, low- and mid-frequency\ncomponents of adversarial examples are particularly effective. These results\nsuggest that different network architectures have different frequency\npreferences and that differences in frequency components between adversarial\nand natural examples may directly influence model robustness. Based on our\nfindings, we further conclude with three useful proposals that serve as a\nvaluable reference to the AI model security community."
    },
    {
        "date": "2025-06",
        "title": "Active Adversarial Noise Suppression for Image Forgery Localization",
        "author": "Rongxuan Peng, Shunquan Tan, Xianbo Mo, Alex C. Kot, and Jiwu Huang",
        "link": "http://arxiv.org/abs/2506.12871v1",
        "abstract": "Recent advances in deep learning have significantly propelled the development\nof image forgery localization. However, existing models remain highly\nvulnerable to adversarial attacks: imperceptible noise added to forged images\ncan severely mislead these models. In this paper, we address this challenge\nwith an Adversarial Noise Suppression Module (ANSM) that generate a defensive\nperturbation to suppress the attack effect of adversarial noise. We observe\nthat forgery-relevant features extracted from adversarial and original forged\nimages exhibit distinct distributions. To bridge this gap, we introduce\nForgery-relevant Features Alignment (FFA) as a first-stage training strategy,\nwhich reduces distributional discrepancies by minimizing the channel-wise\nKullback-Leibler divergence between these features. To further refine the\ndefensive perturbation, we design a second-stage training strategy, termed\nMask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR\nensures that the perturbation remains effective for both adversarial and\noriginal forged images, recovering forgery localization accuracy to their\noriginal level. Extensive experiments across various attack algorithms\ndemonstrate that our method significantly restores the forgery localization\nmodel's performance on adversarial images. Notably, when ANSM is applied to\noriginal forged images, the performance remains nearly unaffected. To our best\nknowledge, this is the first report of adversarial defense in image forgery\nlocalization tasks. We have released the source code and anti-forensics\ndataset."
    },
    {
        "date": "2025-06",
        "title": "TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models",
        "author": "Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Xiaochun Cao, Shouling Ji, Jiaheng Zhang, Jincai Huang, and Li Shen",
        "link": "http://arxiv.org/abs/2506.12815v1",
        "abstract": "Recent advances in Trajectory Optimization (TO) models have achieved\nremarkable success in offline reinforcement learning. However, their\nvulnerabilities against backdoor attacks are poorly understood. We find that\nexisting backdoor attacks in reinforcement learning are based on reward\nmanipulation, which are largely ineffective against the TO model due to its\ninherent sequence modeling nature. Moreover, the complexities introduced by\nhigh-dimensional action spaces further compound the challenge of action\nmanipulation. To address these gaps, we propose TrojanTO, the first\naction-level backdoor attack against TO models. TrojanTO employs alternating\ntraining to enhance the connection between triggers and target actions for\nattack effectiveness. To improve attack stealth, it utilizes precise poisoning\nvia trajectory filtering for normal performance and batch poisoning for trigger\nconsistency. Extensive evaluations demonstrate that TrojanTO effectively\nimplants backdoor attacks across diverse tasks and attack objectives with a low\nattack budget (0.3\\% of trajectories). Furthermore, TrojanTO exhibits broad\napplicability to DT, GDT, and DC, underscoring its scalability across diverse\nTO model architectures."
    },
    {
        "date": "2025-06",
        "title": "Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps",
        "author": "Mohammadreza Kouchaki, Aly Sabri Abdalla, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2506.12812v1",
        "abstract": "The open radio access network (O-RAN) architecture introduces RAN intelligent\ncontrollers (RICs) to facilitate the management and optimization of the\ndisaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL\n(DRL), are increasingly employed for designing intelligent controllers, or\nxApps, to be deployed in the near-real time (near-RT) RIC. These models often\nencounter local optima, which raise concerns about their reliability for RAN\nintelligent control. We therefore introduce Federated O-RAN enabled\nNeuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer\nxApp in parallel to the RAN controller xApps. This NE-DRL xApp framework\nenables effective exploration and exploitation in the near-RT RIC without\ndisrupting RAN operations. We implement the NE xApp along with a DRL xApp and\ndeploy them on Open AI Cellular (OAIC) platform and present numerical results\nthat demonstrate the improved robustness of xApps while effectively balancing\nthe additional computational load."
    },
    {
        "date": "2025-06",
        "title": "Predicting Genetic Mutations from Single-Cell Bone Marrow Images in Acute Myeloid Leukemia Using Noise-Robust Deep Learning Models",
        "author": "Garima Jain, Ravi Kant Gupta, Priyansh Jain, Abhijeet Patil, Ardhendu Sekhar, Gajendra Smeeta, Sanghamitra Pati, and Amit Sethi",
        "link": "http://arxiv.org/abs/2506.12798v1",
        "abstract": "In this study, we propose a robust methodology for identification of myeloid\nblasts followed by prediction of genetic mutation in single-cell images of\nblasts, tackling challenges associated with label accuracy and data noise. We\ntrained an initial binary classifier to distinguish between leukemic (blasts)\nand non-leukemic cells images, achieving 90 percent accuracy. To evaluate the\nmodels generalization, we applied this model to a separate large unlabeled\ndataset and validated the predictions with two haemato-pathologists, finding an\napproximate error rate of 20 percent in the leukemic and non-leukemic labels.\nAssuming this level of label noise, we further trained a four-class model on\nimages predicted as blasts to classify specific mutations. The mutation labels\nwere known for only a bag of cell images extracted from a single slide. Despite\nthe tumor label noise, our mutation classification model achieved 85 percent\naccuracy across four mutation classes, demonstrating resilience to label\ninconsistencies. This study highlights the capability of machine learning\nmodels to work with noisy labels effectively while providing accurate,\nclinically relevant mutation predictions, which is promising for diagnostic\napplications in areas such as haemato-pathology."
    },
    {
        "date": "2025-06",
        "title": "Unconstrained Robust Online Convex Optimization",
        "author": "Jiujia Zhang, and Ashok Cutkosky",
        "link": "http://arxiv.org/abs/2506.12781v1",
        "abstract": "This paper addresses online learning with ``corrupted'' feedback. Our learner\nis provided with potentially corrupted gradients $\\tilde g_t$ instead of the\n``true'' gradients $g_t$. We make no assumptions about how the corruptions\narise: they could be the result of outliers, mislabeled data, or even malicious\ninterference. We focus on the difficult ``unconstrained'' setting in which our\nalgorithm must maintain low regret with respect to any comparison point $u \\in\n\\mathbb{R}^d$. The unconstrained setting is significantly more challenging as\nexisting algorithms suffer extremely high regret even with very tiny amounts of\ncorruption (which is not true in the case of a bounded domain). Our algorithms\nguarantee regret $ \\|u\\|G (\\sqrt{T} + k) $ when $G \\ge \\max_t \\|g_t\\|$ is\nknown, where $k$ is a measure of the total amount of corruption. When $G$ is\nunknown we incur an extra additive penalty of $(\\|u\\|^2+G^2) k$."
    },
    {
        "date": "2025-06",
        "title": "Base3: a simple interpolation-based ensemble method for robust dynamic link prediction",
        "author": "Kondrup Emma",
        "link": "http://arxiv.org/abs/2506.12764v1",
        "abstract": "Dynamic link prediction remains a central challenge in temporal graph\nlearning, particularly in designing models that are both effective and\npractical for real-world deployment. Existing approaches often rely on complex\nneural architectures, which are computationally intensive and difficult to\ninterpret.\n  In this work, we build on the strong recurrence-based foundation of the\nEdgeBank baseline, by supplementing it with inductive capabilities. We do so by\nleveraging the predictive power of non-learnable signals from two complementary\nperspectives: historical edge recurrence, as captured by EdgeBank, and global\nnode popularity, as introduced in the PopTrack model. We propose t-CoMem, a\nlightweight memory module that tracks temporal co-occurrence patterns and\nneighborhood activity. Building on this, we introduce Base3, an\ninterpolation-based model that fuses EdgeBank, PopTrack, and t-CoMem into a\nunified scoring framework. This combination effectively bridges local and\nglobal temporal dynamics -- repetition, popularity, and context -- without\nrelying on training. Evaluated on the Temporal Graph Benchmark, Base3 achieves\nperformance competitive with state-of-the-art deep models, even outperforming\nthem on some datasets. Importantly, it considerably improves on existing\nbaselines' performance under more realistic and challenging negative sampling\nstrategies -- offering a simple yet robust alternative for temporal graph\nlearning."
    },
    {
        "date": "2025-06",
        "title": "Learning to Fuse: Modality-Aware Adaptive Scheduling for Robust Multimodal Foundation Models",
        "author": "Liam Bennett, Mason Clark, Lucas Anderson, Hana Satou, and Olivia Martinez",
        "link": "http://arxiv.org/abs/2506.12733v1",
        "abstract": "Multimodal foundation models have achieved impressive progress across a wide\nrange of vision-language tasks. However, existing approaches often adopt fixed\nor task-specific fusion strategies, neglecting the intrinsic variability of\nmodality reliability and sample complexity. In this paper, we propose\nModality-Aware Adaptive Fusion Scheduling (MA-AFS), a general framework that\nlearns to dynamically modulate the contribution of each modality on a\nper-instance basis. MA-AFS introduces a lightweight neural scheduler that\npredicts modality fusion weights by integrating visual and textual entropy\nsignals along with cross-modal agreement cues. This enables the model to\nadaptively emphasize more reliable modalities, especially under noisy, missing,\nor misaligned inputs. We formulate the fusion process as a differentiable\nscheduling mechanism, analyze its theoretical consistency and regularization\neffect, and demonstrate that it improves robustness without increasing model\ncapacity significantly. Extensive experiments on image-text retrieval,\ncaptioning, and visual question answering show that MA-AFS achieves consistent\nperformance gains over strong baselines such as CLIP, ALBEF, and BLIP.\nMoreover, MA-AFS exhibits improved robustness under modality corruption and\nenhanced generalization under domain shifts. Our work highlights the importance\nof adaptive fusion and opens a promising direction toward reliable and\nuncertainty-aware multimodal learning."
    },
    {
        "date": "2025-06",
        "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
        "author": "Yucheng Li, Surin Ahn, Huiqiang Jiang, Amir H. Abdi, Yuqing Yang, and Lili Qiu",
        "link": "http://arxiv.org/abs/2506.12707v1",
        "abstract": "Large language models (LLMs) have achieved widespread adoption across\nnumerous applications. However, many LLMs are vulnerable to malicious attacks\neven after safety alignment. These attacks typically bypass LLMs' safety\nguardrails by wrapping the original malicious instructions inside adversarial\njailbreaks prompts. Previous research has proposed methods such as adversarial\ntraining and prompt rephrasing to mitigate these safety vulnerabilities, but\nthese methods often reduce the utility of LLMs or lead to significant\ncomputational overhead and online latency. In this paper, we propose\nSecurityLingua, an effective and efficient approach to defend LLMs against\njailbreak attacks via security-oriented prompt compression. Specifically, we\ntrain a prompt compressor designed to discern the \"true intention\" of the input\nprompt, with a particular focus on detecting the malicious intentions of\nadversarial prompts. Then, in addition to the original prompt, the intention is\npassed via the system prompt to the target LLM to help it identify the true\nintention of the request. SecurityLingua ensures a consistent user experience\nby leaving the original input prompt intact while revealing the user's\npotentially malicious intention and stimulating the built-in safety guardrails\nof the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only\na negligible overhead and extra token cost compared to all existing defense\nmethods, making it an especially practical solution for LLM defense.\nExperimental results demonstrate that SecurityLingua can effectively defend\nagainst malicious attacks and maintain utility of the LLM with negligible\ncompute and latency overhead. Our code is available at\nhttps://aka.ms/SecurityLingua."
    },
    {
        "date": "2025-06",
        "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models",
        "author": "Jiaming Zhang, Xin Wang, Xingjun Ma, Lingyu Qiu, Yu-Gang Jiang, and Jitao Sang",
        "link": "http://arxiv.org/abs/2506.12706v1",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable\ncapabilities in understanding relationships between visual and textual data\nthrough joint embedding spaces. Despite their effectiveness, these models\nremain vulnerable to adversarial attacks, particularly in the image modality,\nposing significant security concerns. Building upon our previous work on\nAdversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to\nenhance adversarial robustness in VLMs without extensive parameter training, we\npresent a significant extension by introducing the Neural Augmentor framework\nfor Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations\ninclude: (1) extending AdvPT from text-only to multi-modal prompting across\nboth text and visual modalities, (2) expanding from single-layer to multi-layer\nprompt architectures, and (3) proposing a novel architecture-level redesign\nthrough our Neural Augmentor approach, which implements feature purification to\ndirectly address the distortions introduced by adversarial attacks in feature\nspace. Our NAP-Tuning approach incorporates token refiners that learn to\nreconstruct purified features through residual connections, allowing for\nmodality-specific and layer-specific feature correction.Comprehensive\nexperiments demonstrate that NAP-Tuning significantly outperforms existing\nmethods across various datasets and attack types. Notably, our approach shows\nsignificant improvements over the strongest baselines under the challenging\nAutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on\nViT-B32 architectures while maintaining competitive clean accuracy."
    },
    {
        "date": "2025-06",
        "title": "DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty",
        "author": "Mingxuan Cui, Duo Zhou, Yuxuan Han, Grani A. Hanasusanto, Qiong Wang, Huan Zhang, and Zhengyuan Zhou",
        "link": "http://arxiv.org/abs/2506.12622v1",
        "abstract": "Deep reinforcement learning (RL) has achieved significant success, yet its\napplication in real-world scenarios is often hindered by a lack of robustness\nto environmental uncertainties. To solve this challenge, some robust RL\nalgorithms have been proposed, but most are limited to tabular settings. In\nthis work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a\nnovel algorithm designed to enhance the robustness of the state-of-the-art Soft\nActor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with\nentropy against the worst possible transition model lying in an uncertainty\nset. A distributionally robust version of the soft policy iteration is derived\nwith a convergence guarantee. For settings where nominal distributions are\nunknown, such as offline RL, a generative modeling approach is proposed to\nestimate the required nominal distributions from data. Furthermore,\nexperimental results on a range of continuous control benchmark tasks\ndemonstrate our algorithm achieves up to $9.8$ times the average reward of the\nSAC baseline under common perturbations. Additionally, compared with existing\nrobust reinforcement learning algorithms, DR-SAC significantly improves\ncomputing efficiency and applicability to large-scale problems."
    },
    {
        "date": "2025-06",
        "title": "Existence of Adversarial Examples for Random Convolutional Networks via Isoperimetric Inequalities on $\\mathbb{so}(d)$",
        "author": "Amit Daniely",
        "link": "http://arxiv.org/abs/2506.12613v1",
        "abstract": "We show that adversarial examples exist for various random convolutional\nnetworks, and furthermore, that this is a relatively simple consequence of the\nisoperimetric inequality on the special orthogonal group $\\mathbb{so}(d)$. This\nextends and simplifies a recent line of work which shows similar results for\nrandom fully connected networks."
    },
    {
        "date": "2025-06",
        "title": "Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry",
        "author": "Farida Mohsen, and Ali Safa",
        "link": "http://arxiv.org/abs/2506.12536v1",
        "abstract": "Accurate rotational odometry is crucial for autonomous robotic systems,\nparticularly for small, power-constrained platforms such as drones and mobile\nrobots. This study introduces thermal-gyro fusion, a novel sensor fusion\napproach that integrates ultra-low-resolution thermal imaging with gyroscope\nreadings for rotational odometry. Unlike RGB cameras, thermal imaging is\ninvariant to lighting conditions and, when fused with gyroscopic data,\nmitigates drift which is a common limitation of inertial sensors. We first\ndevelop a multimodal data acquisition system to collect synchronized thermal\nand gyroscope data, along with rotational speed labels, across diverse\nenvironments. Subsequently, we design and train a lightweight Convolutional\nNeural Network (CNN) that fuses both modalities for rotational speed\nestimation. Our analysis demonstrates that thermal-gyro fusion enables a\nsignificant reduction in thermal camera resolution without significantly\ncompromising accuracy, thereby improving computational efficiency and memory\nutilization. These advantages make our approach well-suited for real-time\ndeployment in resource-constrained robotic systems. Finally, to facilitate\nfurther research, we publicly release our dataset as supplementary material."
    },
    {
        "date": "2025-06",
        "title": "Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning",
        "author": "Sara Rajaram, R. James Cotton, and Fabian H. Sinz",
        "link": "http://arxiv.org/abs/2506.12529v1",
        "abstract": "Preference-based Reinforcement Learning (PbRL) entails a variety of\napproaches for aligning models with human intent to alleviate the burden of\nreward engineering. However, most previous PbRL work has not investigated the\nrobustness to labeler errors, inevitable with labelers who are non-experts or\noperate under time constraints. Additionally, PbRL algorithms often target very\nspecific settings (e.g. pairwise ranked preferences or purely offline\nlearning). We introduce Similarity as Reward Alignment (SARA), a simple\ncontrastive framework that is both resilient to noisy labels and adaptable to\ndiverse feedback formats and training paradigms. SARA learns a latent\nrepresentation of preferred samples and computes rewards as similarities to the\nlearned latent. We demonstrate strong performance compared to baselines on\ncontinuous control offline RL benchmarks. We further demonstrate SARA's\nversatility in applications such as trajectory filtering for downstream tasks,\ncross-task preference transfer, and reward shaping in online learning."
    },
    {
        "date": "2025-06",
        "title": "When Forgetting Triggers Backdoors: A Clean Unlearning Attack",
        "author": "Marco Arazzi, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2506.12522v1",
        "abstract": "Machine unlearning has emerged as a key component in ensuring ``Right to be\nForgotten'', enabling the removal of specific data points from trained models.\nHowever, even when the unlearning is performed without poisoning the forget-set\n(clean unlearning), it can be exploited for stealthy attacks that existing\ndefenses struggle to detect. In this paper, we propose a novel {\\em clean}\nbackdoor attack that exploits both the model learning phase and the subsequent\nunlearning requests. Unlike traditional backdoor methods, during the first\nphase, our approach injects a weak, distributed malicious signal across\nmultiple classes. The real attack is then activated and amplified by\nselectively unlearning {\\em non-poisoned} samples. This strategy results in a\npowerful and stealthy novel attack that is hard to detect or mitigate,\nhighlighting critical vulnerabilities in current unlearning mechanisms and\nhighlighting the need for more robust defenses."
    },
    {
        "date": "2025-06",
        "title": "Exploiting AI for Attacks: On the Interplay between Adversarial AI and Offensive AI",
        "author": "Saskia Laura Schr\u00f6er, Luca Pajola, Alberto Castagnaro, Giovanni Apruzzese, and Mauro Conti",
        "link": "http://arxiv.org/abs/2506.12519v1",
        "abstract": "As Artificial Intelligence (AI) continues to evolve, it has transitioned from\na research-focused discipline to a widely adopted technology, enabling\nintelligent solutions across various sectors. In security, AI's role in\nstrengthening organizational resilience has been studied for over two decades.\nWhile much attention has focused on AI's constructive applications, the\nincreasing maturity and integration of AI have also exposed its darker\npotentials. This article explores two emerging AI-related threats and the\ninterplay between them: AI as a target of attacks (`Adversarial AI') and AI as\na means to launch attacks on any target (`Offensive AI') -- potentially even on\nanother AI. By cutting through the confusion and explaining these threats in\nplain terms, we introduce the complex and often misunderstood interplay between\nAdversarial AI and Offensive AI, offering a clear and accessible introduction\nto the challenges posed by these threats."
    },
    {
        "date": "2025-06",
        "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization",
        "author": "Filip Sondej, Yushi Yang, Miko\u0142aj Kniejski, and Marcel Windys",
        "link": "http://arxiv.org/abs/2506.12484v2",
        "abstract": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."
    },
    {
        "date": "2025-06",
        "title": "Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation",
        "author": "Alexander Geiger, Immanuel Hacker, \u00d6mer Sen, and Andreas Ulbig",
        "link": "http://arxiv.org/abs/2506.12466v1",
        "abstract": "The increasing complexity of cyberphysical power systems leads to larger\nattack surfaces to be exploited by malicious actors and a higher risk of faults\nthrough misconfiguration. We propose to meet those risks with a declarative\napproach to describe cyberphysical power systems and to automatically evaluate\nsecurity and safety controls. We leverage Semantic Web technologies as a\nwell-standardized framework, providing languages to specify ontologies, rules\nand shape constraints. We model infrastructure through an ontology which\ncombines external ontologies, architecture and data models for sufficient\nexpressivity and interoperability with external systems. The ontology can\nenrich itself through rules defined in SPARQL, allowing for the inference of\nknowledge that is not explicitly stated. Through the evaluation of SHACL shape\nconstraints we can then validate the data and verify safety and security\nconstraints. We demonstrate this concept with two use cases and illustrate how\nthis solution can be developed further in a community-driven fashion."
    },
    {
        "date": "2025-06",
        "title": "Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates",
        "author": "Chengqing Yu, Fei Wang, Chuanguang Yang, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, and Yongjun Xu",
        "link": "http://arxiv.org/abs/2506.12459v1",
        "abstract": "Multivariate Time Series Forecasting (MTSF) involves predicting future values\nof multiple interrelated time series. Recently, deep learning-based MTSF models\nhave gained significant attention for their promising ability to mine semantics\n(global and local information) within MTS data. However, these models are\npervasively susceptible to missing values caused by malfunctioning data\ncollectors. These missing values not only disrupt the semantics of MTS, but\ntheir distribution also changes over time. Nevertheless, existing models lack\nrobustness to such issues, leading to suboptimal forecasting performance. To\nthis end, in this paper, we propose Multi-View Representation Learning\n(Merlin), which can help existing models achieve semantic alignment between\nincomplete observations with different missing rates and complete observations\nin MTS. Specifically, Merlin consists of two key modules: offline knowledge\ndistillation and multi-view contrastive learning. The former utilizes a teacher\nmodel to guide a student model in mining semantics from incomplete\nobservations, similar to those obtainable from complete observations. The\nlatter improves the student model's robustness by learning from\npositive/negative data pairs constructed from incomplete observations with\ndifferent missing rates, ensuring semantic alignment across different missing\nrates. Therefore, Merlin is capable of effectively enhancing the robustness of\nexisting models against unfixed missing rates while preserving forecasting\naccuracy. Experiments on four real-world datasets demonstrate the superiority\nof Merlin."
    },
    {
        "date": "2025-06",
        "title": "On the existence of consistent adversarial attacks in high-dimensional linear classification",
        "author": "Matteo Vilucchio, Lenka Zdeborov\u00e1, and Bruno Loureiro",
        "link": "http://arxiv.org/abs/2506.12454v1",
        "abstract": "What fundamentally distinguishes an adversarial attack from a\nmisclassification due to limited model expressivity or finite data? In this\nwork, we investigate this question in the setting of high-dimensional binary\nclassification, where statistical effects due to limited data availability play\na central role. We introduce a new error metric that precisely capture this\ndistinction, quantifying model vulnerability to consistent adversarial attacks\n-- perturbations that preserve the ground-truth labels. Our main technical\ncontribution is an exact and rigorous asymptotic characterization of these\nmetrics in both well-specified models and latent space models, revealing\ndifferent vulnerability patterns compared to standard robust error measures.\nThe theoretical results demonstrate that as models become more\noverparameterized, their vulnerability to label-preserving perturbations grows,\noffering theoretical insight into the mechanisms underlying model sensitivity\nto adversarial attacks."
    },
    {
        "date": "2025-06",
        "title": "LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning",
        "author": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Yanwei Ren, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2506.12394v1",
        "abstract": "The advent of parameter-efficient fine-tuning methods has significantly\nreduced the computational burden of adapting large-scale pretrained models to\ndiverse downstream tasks. However, existing approaches often struggle to\nachieve robust performance under domain shifts while maintaining computational\nefficiency. To address this challenge, we propose Low-rAnk Regulated Gradient\nProjection (LARGO) algorithm that integrates dynamic constraints into low-rank\nadaptation methods. Specifically, LARGO incorporates parallel trainable\ngradient projections to dynamically regulate layer-wise updates, retaining the\nOut-Of-Distribution robustness of pretrained model while preserving inter-layer\nindependence. Additionally, it ensures computational efficiency by mitigating\nthe influence of gradient dependencies across layers during weight updates.\nBesides, through leveraging singular value decomposition of pretrained weights\nfor structured initialization, we incorporate an SVD-based initialization\nstrategy that minimizing deviation from pretrained knowledge. Through extensive\nexperiments on diverse benchmarks, LARGO achieves state-of-the-art performance\nacross in-domain and out-of-distribution scenarios, demonstrating improved\nrobustness under domain shifts with significantly lower computational overhead\ncompared to existing PEFT methods. The source code will be released soon."
    },
    {
        "date": "2025-06",
        "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks",
        "author": "Haoyu Zhai, Shuo Wang, Pirouz Naghavi, Qingying Hao, and Gang Wang",
        "link": "http://arxiv.org/abs/2506.12344v1",
        "abstract": "Gaussian blur is widely used to blur human faces in sensitive photos before\nthe photos are posted on the Internet. However, it is unclear to what extent\nthe blurred faces can be restored and used to re-identify the person,\nespecially under a high-blurring setting. In this paper, we explore this\nquestion by developing a deblurring method called Revelio. The key intuition is\nto leverage a generative model's memorization effect and approximate the\ninverse function of Gaussian blur for face restoration. Compared with existing\nmethods, we design the deblurring process to be identity-preserving. It uses a\nconditional Diffusion model for preliminary face restoration and then uses an\nidentity retrieval model to retrieve related images to further enhance\nfidelity. We evaluate Revelio with large public face image datasets and show\nthat it can effectively restore blurred faces, especially under a high-blurring\nsetting. It has a re-identification accuracy of 95.9%, outperforming existing\nsolutions. The result suggests that Gaussian blur should not be used for face\nanonymization purposes. We also demonstrate the robustness of this method\nagainst mismatched Gaussian kernel sizes and functions, and test preliminary\ncountermeasures and adaptive attacks to inspire future work."
    },
    {
        "date": "2025-06",
        "title": "Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models",
        "author": "Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, and Suhang Wang",
        "link": "http://arxiv.org/abs/2506.12340v2",
        "abstract": "Large vision-language models (LVLMs) have demonstrated outstanding\nperformance in many downstream tasks. However, LVLMs are trained on large-scale\ndatasets, which can pose privacy risks if training images contain sensitive\ninformation. Therefore, it is important to detect whether an image is used to\ntrain the LVLM. Recent studies have investigated membership inference attacks\n(MIAs) against LVLMs, including detecting image-text pairs and single-modality\ncontent. In this work, we focus on detecting whether a target image is used to\ntrain the target LVLM. We design simple yet effective Image Corruption-Inspired\nMembership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by\nLVLM's different sensitivity to image corruption for member and non-member\nimages. We first perform an MIA method under the white-box setting, where we\ncan obtain the embeddings of the image through the vision part of the target\nLVLM. The attacks are based on the embedding similarity between the image and\nits corrupted version. We further explore a more practical scenario where we\nhave no knowledge about target LVLMs and we can only query the target LVLMs\nwith an image and a question. We then conduct the attack by utilizing the\noutput text embeddings' similarity. Experiments on existing datasets validate\nthe effectiveness of our proposed attack methods under those two different\nsettings."
    },
    {
        "date": "2025-06",
        "title": "Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models",
        "author": "Yash Sinha, Manit Baser, Murari Mandal, Dinil Mon Divakaran, and Mohan Kankanhalli",
        "link": "http://arxiv.org/abs/2506.17279v1",
        "abstract": "Knowledge erasure in large language models (LLMs) is important for ensuring\ncompliance with data and AI regulations, safeguarding user privacy, mitigating\nbias, and misinformation. Existing unlearning methods aim to make the process\nof knowledge erasure more efficient and effective by removing specific\nknowledge while preserving overall model performance, especially for retained\ninformation. However, it has been observed that the unlearning techniques tend\nto suppress and leave the knowledge beneath the surface, thus making it\nretrievable with the right prompts. In this work, we demonstrate that\n\\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden\ninformation. We introduce a step-by-step reasoning-based black-box attack,\nSleek, that systematically exposes unlearning failures. We employ a structured\nattack framework with three core components: (1) an adversarial prompt\ngeneration strategy leveraging step-by-step reasoning built from LLM-generated\nqueries, (2) an attack mechanism that successfully recalls erased content, and\nexposes unfair suppression of knowledge intended for retention and (3) a\ncategorization of prompts as direct, indirect, and implied, to identify which\nquery types most effectively exploit unlearning weaknesses. Through extensive\nevaluations on four state-of-the-art unlearning techniques and two widely used\nLLMs, we show that existing approaches fail to ensure reliable knowledge\nremoval. Of the generated adversarial prompts, 62.5% successfully retrieved\nforgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair\nsuppression of retained knowledge. Our work highlights the persistent risks of\ninformation leakage, emphasizing the need for more robust unlearning strategies\nfor erasure."
    },
    {
        "date": "2025-06",
        "title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device",
        "author": "Chuntao Ding, Jianhang Xie, Junna Zhang, Salman Raza, Shangguang Wang, and Jiannong Cao",
        "link": "http://arxiv.org/abs/2506.12335v1",
        "abstract": "It has become mainstream to deploy Convolutional Neural Network (CNN) models\non ubiquitous Internet of Things (IoT) devices with the help of the cloud to\nprovide users with a variety of high-quality services. Most existing methods\nhave two limitations: (i) low robustness in handling corrupted image data\ncollected by IoT devices; and (ii) high consumption of computational and\ntransmission resources. To this end, we propose the Grouped NonLinear\ntransformation generation method (GroupNL), which generates diversified feature\nmaps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to\nimprove the robustness of the CNN model. Specifically, partial convolution\nfilters are designated as seed filters in a convolutional layer, and a small\nset of feature maps, i.e., seed feature maps, are first generated based on\nvanilla convolution operation. Then, we split seed feature maps into several\ngroups, each with a set of different NLFs, to generate corresponding diverse\nfeature maps with in-place nonlinear processing. Moreover, GroupNL effectively\nreduces the parameter transmission between multiple nodes during model training\nby setting the hyperparameters of NLFs to random initialization and not\nupdating them during model training, and reduces the computing resources by\nusing NLFs to generate feature maps instead of most feature maps generated\nbased on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,\nIcons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the\nproposed GroupNL outperforms other state-of-the-art methods in model robust and\ntraining acceleration. Specifically, on the Icons-50 dataset, the accuracy of\nGroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla\nResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN\nwhen trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset."
    },
    {
        "date": "2025-06",
        "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions",
        "author": "Stephen Mell, Botong Zhang, David Mell, Shuo Li, Ramya Ramalingam, Nathan Yu, Steve Zdancewic, and Osbert Bastani",
        "link": "http://arxiv.org/abs/2506.12202v1",
        "abstract": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level."
    },
    {
        "date": "2025-06",
        "title": "A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11901v1",
        "abstract": "Advantages of deep learning over traditional methods have been demonstrated\nfor radio signal classification in the recent years. However, various\nresearchers have discovered that even a small but intentional feature\nperturbation known as adversarial examples can significantly deteriorate the\nperformance of the deep learning based radio signal classification. Among\nvarious kinds of adversarial examples, universal adversarial perturbation has\ngained considerable attention due to its feature of being data independent,\nhence as a practical strategy to fool the radio signal classification with a\nhigh success rate. Therefore, in this paper, we investigate a defense system\ncalled neural rejection system to propose against universal adversarial\nperturbations, and evaluate its performance by generating white-box universal\nadversarial perturbations. We show that the proposed neural rejection system is\nable to defend universal adversarial perturbations with significantly higher\naccuracy than the undefended deep neural network."
    },
    {
        "date": "2025-06",
        "title": "Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Basil AsSadhan, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11892v1",
        "abstract": "Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples."
    }
]