[
    {
        "date": "2025-10",
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "author": "Aleksandr Oganov, Ilya Bykov, Eva Neudachina, Mishan Aliev, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin, Denis Rakitin, and Aibek Alanov",
        "link": "http://arxiv.org/abs/2510.17699v1",
        "abstract": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS."
    },
    {
        "date": "2025-10",
        "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks",
        "author": "Xu Zhang, Hao Li, and Zhichao Lu",
        "link": "http://arxiv.org/abs/2510.17687v1",
        "abstract": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and\nperception capabilities but are increasingly vulnerable to jailbreak attacks.\nWhile existing work focuses on explicit attacks, where malicious content\nresides in a single modality, recent studies reveal implicit attacks, in which\nbenign text and image inputs jointly express unsafe intent. Such joint-modal\nthreats are difficult to detect and remain underexplored, largely due to the\nscarcity of high-quality implicit data. We propose ImpForge, an automated\nred-teaming pipeline that leverages reinforcement learning with tailored reward\nmodules to generate diverse implicit samples across 14 domains. Building on\nthis dataset, we further develop CrossGuard, an intent-aware safeguard\nproviding robust and comprehensive defense against both explicit and implicit\nthreats. Extensive experiments across safe and unsafe benchmarks, implicit and\nexplicit attacks, and multiple out-of-domain settings demonstrate that\nCrossGuard significantly outperforms existing defenses, including advanced\nMLLMs and guardrails, achieving stronger security while maintaining high\nutility. This offers a balanced and practical solution for enhancing MLLM\nrobustness against real-world multimodal threats."
    },
    {
        "date": "2025-10",
        "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
        "author": "Athanasios Angelakis, Amne Mousa, Micah L. A. Heldeweg, Laurens A. Biesheuvel, Mark A. Haaksma, Jasper M. Smit, Pieter R. Tuinman, and Paul W. G. Elbers",
        "link": "http://arxiv.org/abs/2510.17650v1",
        "abstract": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and\nstructurally normal lungs in lung ultrasound (LUS) videos remains challenging\ndue to the high visual variability of non-cardiogenic inflammatory patterns\n(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This\nheterogeneity complicates automated classification as overlapping B-lines and\npleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive\nCompact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer\nvariant that removes both positional embeddings and the [CLS] token, making it\nfully permutation-invariant and suitable for unordered medical image data. To\nenhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),\nwhich permutes probe-view sequences and frame orders while preserving\nanatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95\ncritically ill patients against nine state-of-the-art baselines. Despite the\nheterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest\nvalidation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)\nand specificity (0.91), while all competing models collapsed to trivial\nclassification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with\n2.5x fewer parameters, supporting real-time clinical deployment. These results\nshow that aligning architectural design with data structure can outperform\nscale in small-data medical imaging."
    },
    {
        "date": "2025-10",
        "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
        "author": "Yuquan Xue, Guanxing Lu, Zhenyu Wu, Chuanrui Zhang, Bofang Jia, Zhengyi Gu, Yansong Tang, and Ziwei Wang",
        "link": "http://arxiv.org/abs/2510.17640v1",
        "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models."
    },
    {
        "date": "2025-10",
        "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models",
        "author": "Vincenzo Carletti, Pasquale Foggia, Carlo Mazzocca, Giuseppe Parrella, and Mario Vento",
        "link": "http://arxiv.org/abs/2510.17621v1",
        "abstract": "Federated Learning (FL) enables collaborative training of Machine Learning\n(ML) models across multiple clients while preserving their privacy. Rather than\nsharing raw data, federated clients transmit locally computed updates to train\nthe global model. Although this paradigm should provide stronger privacy\nguarantees than centralized ML, client updates remain vulnerable to privacy\nleakage. Adversaries can exploit them to infer sensitive properties about the\ntraining data or even to reconstruct the original inputs via Gradient Inversion\nAttacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to\nreconstruct training data by reversing intermediate updates using\noptimizationbased techniques. We observe that these approaches usually\nreconstruct noisy approximations of the original inputs, whose quality can be\nenhanced with specialized denoising models. This paper presents Gradient Update\nInversion with DEnoising (GUIDE), a novel methodology that leverages diffusion\nmodels as denoising tools to improve image reconstruction attacks in FL. GUIDE\ncan be integrated into any GIAs that exploits surrogate datasets, a widely\nadopted assumption in GIAs literature. We comprehensively evaluate our approach\nin two attack scenarios that use different FL algorithms, models, and datasets.\nOur results demonstrate that GUIDE integrates seamlessly with two state-ofthe-\nart GIAs, substantially improving reconstruction quality across multiple\nmetrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,\nas measured by the DreamSim metric."
    },
    {
        "date": "2025-10",
        "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection",
        "author": "Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, and Lihua Xie",
        "link": "http://arxiv.org/abs/2510.17566v1",
        "abstract": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/."
    },
    {
        "date": "2025-10",
        "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction",
        "author": "Raghu Vamshi Hemadri, Geetha Krishna Guruju, Kristi Topollai, and Anna Ewa Choromanska",
        "link": "http://arxiv.org/abs/2510.17532v1",
        "abstract": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology."
    },
    {
        "date": "2025-10",
        "title": "Cybersecurity AI: Evaluating Agentic Cybersecurity in Attack/Defense CTFs",
        "author": "Francesco Balassone, V\u00edctor Mayoral-Vilches, Stefan Rass, Martin Pinzger, Gaetano Perrone, Simon Pietro Romano, and Peter Schartner",
        "link": "http://arxiv.org/abs/2510.17521v1",
        "abstract": "We empirically evaluate whether AI systems are more effective at attacking or\ndefending in cybersecurity. Using CAI (Cybersecurity AI)'s parallel execution\nframework, we deployed autonomous agents in 23 Attack/Defense CTF\nbattlegrounds. Statistical analysis reveals defensive agents achieve 54.3%\nunconstrained patching success versus 28.3% offensive initial access\n(p=0.0193), but this advantage disappears under operational constraints: when\ndefense requires maintaining availability (23.9%) and preventing all intrusions\n(15.2%), no significant difference exists (p>0.05). Exploratory taxonomy\nanalysis suggests potential patterns in vulnerability exploitation, though\nlimited sample sizes preclude definitive conclusions. This study provides the\nfirst controlled empirical evidence challenging claims of AI attacker\nadvantage, demonstrating that defensive effectiveness critically depends on\nsuccess criteria, a nuance absent from conceptual analyses but essential for\ndeployment. These findings underscore the urgency for defenders to adopt\nopen-source Cybersecurity AI frameworks to maintain security equilibrium\nagainst accelerating offensive automation."
    },
    {
        "date": "2025-10",
        "title": "AWARE: Audio Watermarking with Adversarial Resistance to Edits",
        "author": "Kosta Pavlovi\u0107, Lazar Stanarevi\u0107, Petar Nedi\u0107, Slavko Kova\u010devi\u0107, and Igor Djurovi\u0107",
        "link": "http://arxiv.org/abs/2510.17512v1",
        "abstract": "Prevailing practice in learning-based audio watermarking is to pursue\nrobustness by expanding the set of simulated distortions during training.\nHowever, such surrogates are narrow and prone to overfitting. This paper\npresents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an\nalternative approach that avoids reliance on attack-simulation stacks and\nhandcrafted differentiable distortions. Embedding is obtained via adversarial\noptimization in the time-frequency domain under a level-proportional perceptual\nbudget. Detection employs a time-order-agnostic detector with a Bitwise Readout\nHead (BRH) that aggregates temporal evidence into one score per watermark bit,\nenabling reliable watermark decoding even under desynchronization and temporal\ncuts. Empirically, AWARE attains high audio quality and speech intelligibility\n(PESQ/STOI) and consistently low BER across various audio edits, often\nsurpassing representative state-of-the-art learning-based audio watermarking\nsystems."
    },
    {
        "date": "2025-10",
        "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
        "author": "Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, and Abbas Rahimi",
        "link": "http://arxiv.org/abs/2510.17496v1",
        "abstract": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
    },
    {
        "date": "2025-10",
        "title": "RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems",
        "author": "Keivan Faghih Niresi, Zepeng Zhang, and Olga Fink",
        "link": "http://arxiv.org/abs/2510.17396v1",
        "abstract": "Time series data are often affected by various forms of corruption, such as\nmissing values, noise, and outliers, which pose significant challenges for\ntasks such as forecasting and anomaly detection. To address these issues,\ninverse problems focus on reconstructing the original signal from corrupted\ndata by leveraging prior knowledge about its underlying structure. While deep\nlearning methods have demonstrated potential in this domain, they often require\nextensive pretraining and struggle to generalize under distribution shifts. In\nthis work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series\nLinear Inverse Problems), a novel deep prior framework that achieves high\nrecovery performance without requiring pretraining data. RINS-T leverages\nneural networks as implicit priors and integrates robust optimization\ntechniques, making it resilient to outliers while relaxing the reliance on\nGaussian noise assumptions. To further improve optimization stability and\nrobustness, we introduce three key innovations: guided input initialization,\ninput perturbation, and convex output combination techniques. Each of these\ncontributions strengthens the framework's optimization stability and\nrobustness. These advancements make RINS-T a flexible and effective solution\nfor addressing complex real-world time series challenges. Our code is available\nat https://github.com/EPFL-IMOS/RINS-T."
    },
    {
        "date": "2025-10",
        "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World",
        "author": "Wei Zhang, Zhanhao Hu, Xiao Li, Xiaopei Zhu, and Xiaolin Hu",
        "link": "http://arxiv.org/abs/2510.17322v1",
        "abstract": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses."
    },
    {
        "date": "2025-10",
        "title": "The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment",
        "author": "Eduard Marin, Jinwoo Kim, Alessio Pavoni, Mauro Conti, and Roberto Di Pietro",
        "link": "http://arxiv.org/abs/2510.17311v1",
        "abstract": "Serverless computing has rapidly emerged as a prominent cloud paradigm,\nenabling developers to focus solely on application logic without the burden of\nmanaging servers or underlying infrastructure. Public serverless repositories\nhave become key to accelerating the development of serverless applications.\nHowever, their growing popularity makes them attractive targets for\nadversaries. Despite this, the security posture of these repositories remains\nlargely unexplored, exposing developers and organizations to potential risks.\nIn this paper, we present the first comprehensive analysis of the security\nlandscape of serverless components hosted in public repositories. We analyse\n2,758 serverless components from five widely used public repositories popular\namong developers and enterprises, and 125,936 Infrastructure as Code (IaC)\ntemplates across three widely used IaC frameworks. Our analysis reveals\nsystemic vulnerabilities including outdated software packages, misuse of\nsensitive parameters, exploitable deployment configurations, susceptibility to\ntypo-squatting attacks and opportunities to embed malicious behaviour within\ncompressed serverless components. Finally, we provide practical recommendations\nto mitigate these threats."
    },
    {
        "date": "2025-10",
        "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems",
        "author": "Rishi Jha, Harold Triedman, Justin Wagle, and Vitaly Shmatikov",
        "link": "http://arxiv.org/abs/2510.17276v1",
        "abstract": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation."
    },
    {
        "date": "2025-10",
        "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses",
        "author": "Runlin Lei, Lu Yi, Mingguo He, Pengyu Qiu, Zhewei Wei, Yongchao Liu, and Chuntao Hong",
        "link": "http://arxiv.org/abs/2510.17185v1",
        "abstract": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are\npowerful approaches for learning on Text-Attributed Graphs (TAGs), a\ncomprehensive understanding of their robustness remains elusive. Current\nevaluations are fragmented, failing to systematically investigate the distinct\neffects of textual and structural perturbations across diverse models and\nattack scenarios. To address these limitations, we introduce a unified and\ncomprehensive framework to evaluate robustness in TAG learning. Our framework\nevaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten\ndatasets from four domains, under diverse text-based, structure-based, and\nhybrid perturbations in both poisoning and evasion scenarios. Our extensive\nanalysis reveals multiple findings, among which three are particularly\nnoteworthy: 1) models have inherent robustness trade-offs between text and\nstructure, 2) the performance of GNNs and RGNNs depends heavily on the text\nencoder and attack type, and 3) GraphLLMs are particularly vulnerable to\ntraining data corruption. To overcome the identified trade-offs, we introduce\nSFT-auto, a novel framework that delivers superior and balanced robustness\nagainst both textual and structural attacks within a single model. Our work\nestablishes a foundation for future research on TAG security and offers\npractical solutions for robust TAG learning in adversarial environments. Our\ncode is available at: https://github.com/Leirunlin/TGRB."
    },
    {
        "date": "2025-10",
        "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition",
        "author": "Roland Croft, Brian Du, Darcy Joseph, and Sharath Kumar",
        "link": "http://arxiv.org/abs/2510.17169v1",
        "abstract": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples."
    },
    {
        "date": "2025-10",
        "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback",
        "author": "Shinji Ito, Kevin Jamieson, Haipeng Luo, Arnab Maiti, and Taira Tsuchiya",
        "link": "http://arxiv.org/abs/2510.17103v1",
        "abstract": "We study online learning in finite-horizon episodic Markov decision processes\n(MDPs) under the challenging aggregate bandit feedback model, where the learner\nobserves only the cumulative loss incurred in each episode, rather than\nindividual losses at each state-action pair. While prior work in this setting\nhas focused exclusively on worst-case analysis, we initiate the study of\nbest-of-both-worlds (BOBW) algorithms that achieve low regret in both\nstochastic and adversarial environments. We propose the first BOBW algorithms\nfor episodic tabular MDPs with aggregate bandit feedback. In the case of known\ntransitions, our algorithms achieve $O(\\log T)$ regret in stochastic settings\nand ${O}(\\sqrt{T})$ regret in adversarial ones. Importantly, we also establish\nmatching lower bounds, showing the optimality of our algorithms in this\nsetting. We further extend our approach to unknown-transition settings by\nincorporating confidence-based techniques. Our results rely on a combination of\nFTRL over occupancy measures, self-bounding techniques, and new loss estimators\ninspired by recent advances in online shortest path problems. Along the way, we\nalso provide the first individual-gap-dependent lower bounds and demonstrate\nnear-optimal BOBW algorithms for shortest path problems with bandit feedback."
    },
    {
        "date": "2025-10",
        "title": "Watermark Robustness and Radioactivity May Be at Odds in Federated Learning",
        "author": "Leixu Huang, Zedian Shao, and Teodora Baluta",
        "link": "http://arxiv.org/abs/2510.17033v1",
        "abstract": "Federated learning (FL) enables fine-tuning large language models (LLMs)\nacross distributed data sources. As these sources increasingly include\nLLM-generated text, provenance tracking becomes essential for accountability\nand transparency. We adapt LLM watermarking for data provenance in FL where a\nsubset of clients compute local updates on watermarked data, and the server\naverages all updates into the global LLM. In this setup, watermarks are\nradioactive: the watermark signal remains detectable after fine-tuning with\nhigh confidence. The $p$-value can reach $10^{-24}$ even when as little as\n$6.6\\%$ of data is watermarked. However, the server can act as an active\nadversary that wants to preserve model utility while evading provenance\ntracking. Our observation is that updates induced by watermarked synthetic data\nappear as outliers relative to non-watermark updates. Our adversary thus\napplies strong robust aggregation that can filter these outliers, together with\nthe watermark signal. All evaluated radioactive watermarks are not robust\nagainst such an active filtering server. Our work suggests fundamental\ntrade-offs between radioactivity, robustness, and utility."
    },
    {
        "date": "2025-10",
        "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs",
        "author": "Masahiro Kaneko, and Timothy Baldwin",
        "link": "http://arxiv.org/abs/2510.17000v1",
        "abstract": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs."
    },
    {
        "date": "2025-10",
        "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
        "author": "Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, and Xiang Wang",
        "link": "http://arxiv.org/abs/2510.16926v1",
        "abstract": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement."
    },
    {
        "date": "2025-10",
        "title": "UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks",
        "author": "Mansi Phute, Matthew Hull, Haoran Wang, Alec Helbling, ShengYun Peng, Willian Lunardi, Martin Andreoni, Wenke Lee, and Polo Chau",
        "link": "http://arxiv.org/abs/2510.16923v1",
        "abstract": "Deep learning models deployed in safety critical applications like autonomous\ndriving use simulations to test their robustness against adversarial attacks in\nrealistic conditions. However, these simulations are non-differentiable,\nforcing researchers to create attacks that do not integrate simulation\nenvironmental factors, reducing attack success. To address this limitation, we\nintroduce UNDREAM, the first software framework that bridges the gap between\nphotorealistic simulators and differentiable renderers to enable end-to-end\noptimization of adversarial perturbations on any 3D objects. UNDREAM enables\nmanipulation of the environment by offering complete control over weather,\nlighting, backgrounds, camera angles, trajectories, and realistic human and\nobject movements, thereby allowing the creation of diverse scenes. We showcase\na wide array of distinct physically plausible adversarial objects that UNDREAM\nenables researchers to swiftly explore in different configurable environments.\nThis combination of photorealistic simulation and differentiable optimization\nopens new avenues for advancing research of physical adversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "ThreatIntel-Andro: Expert-Verified Benchmarking for Robust Android Malware Research",
        "author": "Hongpeng Bai, Minhong Dong, Yao Zhang, Shunzhe Zhao, Haobo Zhang, Lingyue Li, Yude Bai, and Guangquan Xu",
        "link": "http://arxiv.org/abs/2510.16835v1",
        "abstract": "The rapidly evolving Android malware ecosystem demands high-quality,\nreal-time datasets as a foundation for effective detection and defense. With\nthe widespread adoption of mobile devices across industrial systems, they have\nbecome a critical yet often overlooked attack surface in industrial\ncybersecurity. However, mainstream datasets widely used in academia and\nindustry (e.g., Drebin) exhibit significant limitations: on one hand, their\nheavy reliance on VirusTotal's multi-engine aggregation results introduces\nsubstantial label noise; on the other hand, outdated samples reduce their\ntemporal relevance. Moreover, automated labeling tools (e.g., AVClass2) suffer\nfrom suboptimal aggregation strategies, further compounding labeling errors and\npropagating inaccuracies throughout the research community."
    },
    {
        "date": "2025-10",
        "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction",
        "author": "Abdur Rahman, Mohammad Marufuzzaman, Jason Street, Haifeng Wang, Veera G. Gude, and Randy Buchanan",
        "link": "http://arxiv.org/abs/2510.16832v1",
        "abstract": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries."
    },
    {
        "date": "2025-10",
        "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation",
        "author": "Yue Liu, Zhenchang Xing, Shidong Pan, and Chakkrit Tantithamthavorn",
        "link": "http://arxiv.org/abs/2510.16823v1",
        "abstract": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs."
    },
    {
        "date": "2025-10",
        "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
        "author": "Tianyang Dou, Ming Li, Jiangying Qin, Xuan Liao, Jiageng Zhong, Armin Gruen, and Mengyi Deng",
        "link": "http://arxiv.org/abs/2510.16730v1",
        "abstract": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce."
    },
    {
        "date": "2025-10",
        "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning",
        "author": "Anthony DiMaggio, Raghav Sharma, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2510.16694v1",
        "abstract": "Secure federated learning (FL) preserves data privacy during distributed\nmodel training. However, deploying such frameworks across heterogeneous devices\nresults in performance bottlenecks, due to straggler clients with limited\ncomputational or network capabilities, slowing training for all participating\nclients. This paper introduces the first straggler mitigation technique for\nsecure aggregation with deep neural networks. We propose CLIP, a client-side\ninvariant neuron pruning technique coupled with network-aware pruning, that\naddresses compute and network bottlenecks due to stragglers during training\nwith minimal accuracy loss. Our technique accelerates secure FL training by 13%\nto 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an\naccuracy impact of between 1.3% improvement to 2.6% reduction."
    },
    {
        "date": "2025-10",
        "title": "Robust Dynamic Staffing with Predictions",
        "author": "Yiding Feng, Vahideh Manshadi, Rad Niazadeh, and Saba Neyshabouri",
        "link": "http://arxiv.org/abs/2510.16663v1",
        "abstract": "We consider a natural dynamic staffing problem in which a decision-maker\nsequentially hires workers over a finite horizon to meet an unknown demand\nrevealed at the end. Predictions about demand arrive over time and become\nincreasingly accurate, while worker availability decreases. This creates a\nfundamental trade-off between hiring early to avoid understaffing (when workers\nare more available but forecasts are less reliable) and hiring late to avoid\noverstaffing (when forecasts are more accurate but availability is lower). This\nproblem is motivated by last-mile delivery operations, where companies such as\nAmazon rely on gig-economy workers whose availability declines closer to the\noperating day.\n  To address practical limitations of Bayesian models (in particular, to remain\nagnostic to the underlying forecasting method), we study this problem under\nadversarial predictions. In this model, sequential predictions are\nadversarially chosen uncertainty intervals that (approximately) contain the\ntrue demand. The objective is to minimize worst-case staffing imbalance cost.\nOur main result is a simple and computationally efficient online algorithm that\nis minimax optimal. We first characterize the minimax cost against a restricted\nadversary via a polynomial-size linear program, then show how to emulate this\nsolution in the general case. While our base model focuses on a single demand,\nwe extend the framework to multiple demands (with egalitarian/utilitarian\nobjectives), to settings with costly reversals of hiring decisions, and to\ninconsistent prediction intervals. We also introduce a practical \"re-solving\"\nvariant of our algorithm, which we prove is also minimax optimal. Finally we\nconduct numerical experiments showing that our algorithms outperform Bayesian\nheuristics in both cost and speed, and are competitive with (approximate or\nexact) Bayesian-optimal policies when those can be computed."
    },
    {
        "date": "2025-10",
        "title": "Universal and Transferable Attacks on Pathology Foundation Models",
        "author": "Yuntian Wang, Xilin Yang, Che-Yung Shen, Nir Pillar, and Aydogan Ozcan",
        "link": "http://arxiv.org/abs/2510.16660v1",
        "abstract": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology."
    },
    {
        "date": "2025-10",
        "title": "A Versatile Framework for Designing Group-Sparse Adversarial Attacks",
        "author": "Alireza Heshmati, Saman Soleimani Roudi, Sajjad Amini, Shahrokh Ghaemmaghami, and Farokh Marvasti",
        "link": "http://arxiv.org/abs/2510.16637v1",
        "abstract": "Existing adversarial attacks often neglect perturbation sparsity, limiting\ntheir ability to model structural changes and to explain how deep neural\nnetworks (DNNs) process meaningful input patterns. We propose ATOS (Attack\nThrough Overlapping Sparsity), a differentiable optimization framework that\ngenerates structured, sparse adversarial perturbations in element-wise,\npixel-wise, and group-wise forms. For white-box attacks on image classifiers,\nwe introduce the Overlapping Smoothed L0 (OSL0) function, which promotes\nconvergence to a stationary point while encouraging sparse, structured\nperturbations. By grouping channels and adjacent pixels, ATOS improves\ninterpretability and helps identify robust versus non-robust features. We\napproximate the L-infinity gradient using the logarithm of the sum of\nexponential absolute values to tightly control perturbation magnitude. On\nCIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing\nsignificantly sparser and more structurally coherent perturbations than prior\nmethods. The structured group-wise attack highlights critical regions from the\nnetwork's perspective, providing counterfactual explanations by replacing\nclass-defining regions with robust features from the target class."
    },
    {
        "date": "2025-10",
        "title": "Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application",
        "author": "Bruno Louren\u00e7o, Pedro Ad\u00e3o, Jo\u00e3o F. Ferreira, Mario Monteiro Marques, and C\u00e1tia Vaz",
        "link": "http://arxiv.org/abs/2510.16610v1",
        "abstract": "This survey investigates how ontologies, semantic log processing, and Large\nLanguage Models (LLMs) enhance cybersecurity. Ontologies structure domain\nknowledge, enabling interoperability, data integration, and advanced threat\nanalysis. Security logs, though critical, are often unstructured and complex.\nTo address this, automated construction of Knowledge Graphs (KGs) from raw logs\nis emerging as a key strategy for organizing and reasoning over security data.\nLLMs enrich this process by providing contextual understanding and extracting\ninsights from unstructured content. This work aligns with European Union (EU)\nefforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges\nand opportunities in intelligent ontology-driven cyber defense."
    },
    {
        "date": "2025-10",
        "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense",
        "author": "Yiyang Huang, Liang Shi, Yitian Zhang, Yi Xu, and Yun Fu",
        "link": "http://arxiv.org/abs/2510.16596v1",
        "abstract": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased."
    },
    {
        "date": "2025-10",
        "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries",
        "author": "Xinfeng Li, Shengyuan Pang, Jialin Wu, Jiangyi Deng, Huanlong Zhong, Yanjiao Chen, Jie Zhang, and Wenyuan Xu",
        "link": "http://arxiv.org/abs/2510.16581v1",
        "abstract": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image\ngeneration, can be exploited to produce unsafe images. Existing safety\nmeasures, e.g., content moderation or model alignment, fail in the presence of\nwhite-box adversaries who know and can adjust model parameters, e.g., by\nfine-tuning. This paper presents a novel defensive framework, named Patronus,\nwhich equips T2I models with holistic protection to defend against white-box\nadversaries. Specifically, we design an internal moderator that decodes unsafe\ninput features into zero vectors while ensuring the decoding performance of\nbenign input features. Furthermore, we strengthen the model alignment with a\ncarefully designed non-fine-tunable learning mechanism, ensuring the T2I model\nwill not be compromised by malicious fine-tuning. We conduct extensive\nexperiments to validate the intactness of the performance on safe content\ngeneration and the effectiveness of rejecting unsafe content generation.\nResults also confirm the resilience of Patronus against various fine-tuning\nattacks by white-box adversaries."
    },
    {
        "date": "2025-10",
        "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem",
        "author": "Xiaofan Li, and Xing Gao",
        "link": "http://arxiv.org/abs/2510.16558v1",
        "abstract": "The Model Context Protocol (MCP) is an emerging open standard that enables\nAI-powered applications to interact with external tools through structured\nmetadata. A rapidly growing ecosystem has formed around MCP, including a wide\nrange of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP\nregistries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),\nand thousands of community-contributed MCP servers. Although the MCP ecosystem\nis gaining traction, there has been little systematic study of its architecture\nand associated security risks. In this paper, we present the first\ncomprehensive security analysis of the MCP ecosystem. We decompose MCP\necosystem into three core components: hosts, registries, and servers, and study\nthe interactions and trust relationships among them. Users search for servers\non registries and configure them in the host, which translates LLM-generated\noutput into external tool invocations provided by the servers and executes\nthem. Our qualitative analysis reveals that hosts lack output verification\nmechanisms for LLM-generated outputs, enabling malicious servers to manipulate\nmodel behavior and induce a variety of security threats, including but not\nlimited to sensitive data exfiltration. We uncover a wide range of\nvulnerabilities that enable attackers to hijack servers, due to the lack of a\nvetted server submission process in registries. To support our analysis, we\ncollect and analyze a dataset of 67,057 servers from six public registries. Our\nquantitative analysis demonstrates that a substantial number of servers can be\nhijacked by attackers. Finally, we propose practical defense strategies for MCP\nhosts, registries, and users. We responsibly disclosed our findings to affected\nhosts and registries."
    },
    {
        "date": "2025-10",
        "title": "$\u03c1$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching",
        "author": "Weijie Chen, Shan Tang, Yulin Tang, Xiapu Luo, Yinqian Zhang, and Weizhong Qiang",
        "link": "http://arxiv.org/abs/2510.16544v1",
        "abstract": "Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)\nthat continues to pose a significant threat to various systems. However, we\nfind that conventional load-based attacks are becoming highly ineffective on\nthe most recent architectures such as Intel Alder and Raptor Lake. In this\npaper, we present $\\rho$Hammer, a new Rowhammer framework that systematically\novercomes three core challenges impeding attacks on these new architectures.\nFirst, we design an efficient and generic DRAM address mapping\nreverse-engineering method that uses selective pairwise measurements and\nstructured deduction, enabling recovery of complex mappings within seconds on\nthe latest memory controllers. Second, to break through the activation rate\nbottleneck of load-based hammering, we introduce a novel prefetch-based\nhammering paradigm that leverages the asynchronous nature of x86 prefetch\ninstructions and is further enhanced by multi-bank parallelism to maximize\nthroughput. Third, recognizing that speculative execution causes more severe\ndisorder issues for prefetching, which cannot be simply mitigated by memory\nbarriers, we develop a counter-speculation hammering technique using\ncontrol-flow obfuscation and optimized NOP-based pseudo-barriers to maintain\nprefetch order with minimal overhead. Evaluations across four latest Intel\narchitectures demonstrate $\\rho$Hammer's breakthrough effectiveness: it induces\nup to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes\nand has a 112x higher flip rate than the load-based hammering baselines on\nComet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on\nthe latest Raptor Lake architecture, where baselines completely fail, achieving\nstable flip rates of 2,291/min and fast end-to-end exploitation."
    },
    {
        "date": "2025-10",
        "title": "Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution",
        "author": "Dimitris Stefanopoulos, and Andreas Voskou",
        "link": "http://arxiv.org/abs/2510.16443v1",
        "abstract": "This report presents the winning solution for Task 2 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The goal of the challenge was to design and train a robust\nANN-based model capable of achieving high accuracy in a binary classification\ntask on both clean and adversarial data generated with the Random Distribution\nShuffle Attack (RDSA). Our solution consists of two components: a data\ngeneration phase and a robust model training phase. In the first phase, we\nproduced 15 million artificial training samples using a custom methodology\nderived from Random Distribution Shuffle Attack (RDSA). In the second phase, we\nintroduced a robust architecture comprising (i)a Feature Embedding Block with\nshared weights among features of the same type and (ii)a Dense Fusion Tail\nresponsible for the final prediction. Training this architecture on our\nadversarial dataset achieved a mixed accuracy score of 80\\%, exceeding the\nsecond-place solution by two percentage points."
    },
    {
        "date": "2025-10",
        "title": "Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution",
        "author": "Dimitris Stefanopoulos, and Andreas Voskou",
        "link": "http://arxiv.org/abs/2510.16440v1",
        "abstract": "This report presents the winning solution for Task 1 of Colliding with\nAdversaries: A Challenge on Robust Learning in High Energy Physics Discovery at\nECML-PKDD 2025. The task required designing an adversarial attack against a\nprovided classification model that maximizes misclassification while minimizing\nperturbations. Our approach employs a multi-round gradient-based strategy that\nleverages the differentiable structure of the model, augmented with random\ninitialization and sample-mixing techniques to enhance effectiveness. The\nresulting attack achieved the best results in perturbation size and fooling\nsuccess rate, securing first place in the competition."
    },
    {
        "date": "2025-10",
        "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching",
        "author": "Aidyn Ubingazhibov, R\u00e9mi Pautrat, Iago Su\u00e1rez, Shaohui Liu, Marc Pollefeys, and Viktor Larsson",
        "link": "http://arxiv.org/abs/2510.16438v1",
        "abstract": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick."
    },
    {
        "date": "2025-10",
        "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization",
        "author": "Pulin Li, Guocheng Wu, Li Yin, Yuxin Zheng, Wei Zhang, and Yanjie Zhou",
        "link": "http://arxiv.org/abs/2510.16370v1",
        "abstract": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD."
    },
    {
        "date": "2025-10",
        "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
        "author": "Ryoto Miyamoto, Xin Fan, Fuyuko Kido, Tsuneo Matsumoto, and Hayato Yamana",
        "link": "http://arxiv.org/abs/2510.16295v1",
        "abstract": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques."
    },
    {
        "date": "2025-10",
        "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense",
        "author": "Zhehao Zhang, Weijie Xu, Shixian Cui, and Chandan K. Reddy",
        "link": "http://arxiv.org/abs/2510.16259v1",
        "abstract": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems."
    },
    {
        "date": "2025-10",
        "title": "Detecting Adversarial Fine-tuning with Auditing Agents",
        "author": "Sarah Egler, John Schulman, and Nicholas Carlini",
        "link": "http://arxiv.org/abs/2510.16255v1",
        "abstract": "Large Language Model (LLM) providers expose fine-tuning APIs that let end\nusers fine-tune their frontier LLMs. Unfortunately, it has been shown that an\nadversary with fine-tuning access to an LLM can bypass safeguards. Particularly\nconcerning, such attacks may avoid detection with datasets that are only\nimplicitly harmful. Our work studies robust detection mechanisms for\nadversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning\nauditing agent and show it can detect harmful fine-tuning prior to model\ndeployment. We provide our auditing agent with access to the fine-tuning\ndataset, as well as the fine-tuned and pre-fine-tuned models, and request the\nagent assigns a risk score for the fine-tuning job. We evaluate our detection\napproach on a diverse set of eight strong fine-tuning attacks from the\nliterature, along with five benign fine-tuned models, totaling over 1400\nindependent audits. These attacks are undetectable with basic content\nmoderation on the dataset, highlighting the challenge of the task. With the\nbest set of affordances, our auditing agent achieves a 56.2% detection rate of\nadversarial fine-tuning at a 1% false positive rate. Most promising, the\nauditor is able to detect covert cipher attacks that evade safety evaluations\nand content moderation of the dataset. While benign fine-tuning with\nunintentional subtle safety degradation remains a challenge, we establish a\nbaseline configuration for further work in this area. We release our auditing\nagent at https://github.com/safety-research/finetuning-auditor."
    },
    {
        "date": "2025-10",
        "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness",
        "author": "Longwei Wang, Ifrat Ikhtear Uddin, KC Santosh, Chaowei Zhang, Xiao Qin, and Yang Zhou",
        "link": "http://arxiv.org/abs/2510.16171v1",
        "abstract": "Adversarial examples reveal critical vulnerabilities in deep neural networks\nby exploiting their sensitivity to imperceptible input perturbations. While\nadversarial training remains the predominant defense strategy, it often incurs\nsignificant computational cost and may compromise clean-data accuracy. In this\nwork, we investigate an architectural approach to adversarial robustness by\nembedding group-equivariant convolutions-specifically, rotation- and\nscale-equivariant layers-into standard convolutional neural networks (CNNs).\nThese layers encode symmetry priors that align model behavior with structured\ntransformations in the input space, promoting smoother decision boundaries and\ngreater resilience to adversarial attacks. We propose and evaluate two\nsymmetry-aware architectures: a parallel design that processes standard and\nequivariant features independently before fusion, and a cascaded design that\napplies equivariant operations sequentially. Theoretically, we demonstrate that\nsuch models reduce hypothesis space complexity, regularize gradients, and yield\ntighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme\nValue for nEtwork Robustness) framework. Empirically, our models consistently\nimprove adversarial robustness and generalization across CIFAR-10, CIFAR-100,\nand CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial\ntraining. These findings underscore the potential of symmetry-enforcing\narchitectures as efficient and principled alternatives to data\naugmentation-based defenses."
    },
    {
        "date": "2025-10",
        "title": "The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers",
        "author": "Owais Makroo, Siva Rajesh Kasa, Sumegh Roychowdhury, Karan Gupta, Nikhil Pattisapu, Santhosh Kasa, and Sumit Negi",
        "link": "http://arxiv.org/abs/2510.16122v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a critical privacy threat by\nenabling adversaries to determine whether a specific sample was included in a\nmodel's training dataset. Despite extensive research on MIAs, systematic\ncomparisons between generative and discriminative classifiers remain limited.\nThis work addresses this gap by first providing theoretical motivation for why\ngenerative classifiers exhibit heightened susceptibility to MIAs, then\nvalidating these insights through comprehensive empirical evaluation. Our study\nencompasses discriminative, generative, and pseudo-generative text classifiers\nacross varying training data volumes, evaluated on nine benchmark datasets.\nEmploying a diverse array of MIA strategies, we consistently demonstrate that\nfully generative classifiers which explicitly model the joint likelihood\n$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe\nthat the canonical inference approach commonly used in generative classifiers\nsignificantly amplifies this privacy risk. These findings reveal a fundamental\nutility-privacy trade-off inherent in classifier design, underscoring the\ncritical need for caution when deploying generative classifiers in\nprivacy-sensitive applications. Our results motivate future research directions\nin developing privacy-preserving generative classifiers that can maintain\nutility while mitigating membership inference vulnerabilities."
    },
    {
        "date": "2025-10",
        "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
        "author": "Yi Wan, Jiuqi Wang, Liam Li, Jinsong Liu, Ruihao Zhu, and Zheqing Zhu",
        "link": "http://arxiv.org/abs/2510.15862v3",
        "abstract": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under Apache 2.0 license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS."
    },
    {
        "date": "2025-10",
        "title": "Towards Proactive Defense Against Cyber Cognitive Attacks",
        "author": "Bonnie Rushing, Mac-Rufus Umeokolo, and Shouhuai Xu",
        "link": "http://arxiv.org/abs/2510.15801v1",
        "abstract": "Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit\npsychological biases and manipulate decision-making processes. Emerging\ntechnologies, such as AI-driven disinformation and synthetic media, have\naccelerated the scale and sophistication of these threats. Prior studies\nprimarily categorize current cognitive attack tactics, lacking predictive\nmechanisms to anticipate future DIs and their malicious use in cognitive\nattacks. This paper addresses these gaps by introducing a novel predictive\nmethodology for forecasting the emergence of DIs and their malicious uses in\ncognitive attacks. We identify trends in adversarial tactics and propose\nproactive defense strategies."
    },
    {
        "date": "2025-10",
        "title": "Ambusher: Exploring the Security of Distributed SDN Controllers Through Protocol State Fuzzing",
        "author": "Jinwoo Kim, Minjae Seo, Eduard Marin, Seungsoo Lee, Jaehyun Nam, and Seungwon Shin",
        "link": "http://arxiv.org/abs/2510.15798v1",
        "abstract": "Distributed SDN (Software-Defined Networking) controllers have rapidly become\nan integral element of Wide Area Networks (WAN), particularly within SD-WAN,\nproviding scalability and fault-tolerance for expansive network\ninfrastructures. However, the architecture of these controllers introduces new\npotential attack surfaces that have thus far received inadequate attention. In\nresponse to these concerns, we introduce Ambusher, a testing tool designed to\ndiscover vulnerabilities within protocols used in distributed SDN controllers.\nAmbusher achieves this by leveraging protocol state fuzzing, which\nsystematically finds attack scenarios based on an inferred state machine. Since\nlearning states from a cluster is complicated, Ambusher proposes a novel\nmethodology that extracts a single and relatively simple state machine,\nachieving efficient state-based fuzzing. Our evaluation of Ambusher, conducted\non a real SD-WAN deployment spanning two campus networks and one enterprise\nnetwork, illustrates its ability to uncover 6 potential vulnerabilities in the\nwidely used distributed controller platform."
    },
    {
        "date": "2025-10",
        "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments",
        "author": "Sabbir M Saleh, Nazim Madhavji, and John Steinbacher",
        "link": "http://arxiv.org/abs/2510.16087v1",
        "abstract": "Security is becoming a pivotal point in cloud platforms. Several divisions,\nsuch as business organisations, health care, government, etc., have experienced\ncyber-attacks on their infrastructures. This research focuses on security\nissues within Continuous Integration and Deployment (CI/CD) pipelines in a\ncloud platform as a reaction to recent cyber breaches. This research proposes a\nblockchain-based solution to enhance CI/CD pipeline security. This research\naims to develop a framework that leverages blockchain's distributed ledger\ntechnology and tamper-resistant features to improve CI/CD pipeline security.\nThe goal is to emphasise secure software deployment by integrating threat\nmodelling frameworks and adherence to coding standards. It also aims to employ\ntools to automate security testing to detect publicly disclosed vulnerabilities\nand flaws, such as an outdated version of Java Spring Framework, a JavaScript\nlibrary from an unverified source, or a database library that allows SQL\ninjection attacks in the deployed software through the framework."
    },
    {
        "date": "2025-10",
        "title": "Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language",
        "author": "Ehud Shapiro",
        "link": "http://arxiv.org/abs/2510.15747v1",
        "abstract": "Grassroots platforms are distributed applications run by\\linebreak\ncryptographically-identified people on their networked personal devices, where\nmultiple disjoint platform instances emerge independently and coalesce when\nthey interoperate. Their foundation is the grassroots social graph, upon which\ngrassroots social networks, grassroots cryptocurrencies, and grassroots\ndemocratic federations can be built.\n  Grassroots platforms have yet to be implemented, the key challenge being\nfaulty and malicious participants: without secure programming support, correct\nparticipants cannot reliably identify each other, establish secure\ncommunication, or verify each other's code integrity.\n  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,\nlogic programming language for implementing grassroots platforms. GLP extends\nlogic programs with paired single-reader/single-writer (SRSW) logic variables,\nproviding secure communication channels among cryptographically-identified\npeople through encrypted, signed and attested messages, which enable identity\nand code integrity verification. We present GLP progressively: logic programs,\nconcurrent GLP, multiagent GLP, augmenting it with cryptographic security, and\nproviding smartphone implementation-ready specifications. We prove safety\nproperties including that GLP computations are deductions, SRSW preservation,\nacyclicity, and monotonicity. We prove multiagent GLP is grassroots and that\nGLP streams achieve blockchain security properties. We present a grassroots\nsocial graph protocol establishing authenticated peer-to-peer connections and\ndemonstrate secure grassroots social networking applications."
    },
    {
        "date": "2025-10",
        "title": "Constrained Adversarial Perturbation",
        "author": "Virendra Nishad, Bhaskar Mukhoty, Hilal AlQuabeh, Sandeep K. Shukla, and Sayak Ray Chowdhury",
        "link": "http://arxiv.org/abs/2510.15699v1",
        "abstract": "Deep neural networks have achieved remarkable success in a wide range of\nclassification tasks. However, they remain highly susceptible to adversarial\nexamples - inputs that are subtly perturbed to induce misclassification while\nappearing unchanged to humans. Among various attack strategies, Universal\nAdversarial Perturbations (UAPs) have emerged as a powerful tool for both\nstress testing model robustness and facilitating scalable adversarial training.\nDespite their effectiveness, most existing UAP methods neglect domain specific\nconstraints that govern feature relationships. Violating such constraints, such\nas debt to income ratios in credit scoring or packet flow invariants in network\ncommunication, can render adversarial examples implausible or easily\ndetectable, thereby limiting their real world applicability.\n  In this work, we advance universal adversarial attacks to constrained feature\nspaces by formulating an augmented Lagrangian based min max optimization\nproblem that enforces multiple, potentially complex constraints of varying\nimportance. We propose Constrained Adversarial Perturbation (CAP), an efficient\nalgorithm that solves this problem using a gradient based alternating\noptimization strategy. We evaluate CAP across diverse domains including\nfinance, IT networks, and cyber physical systems, and demonstrate that it\nachieves higher attack success rates while significantly reducing runtime\ncompared to existing baselines. Our approach also generalizes seamlessly to\nindividual adversarial perturbations, where we observe similar strong\nperformance gains. Finally, we introduce a principled procedure for learning\nfeature constraints directly from data, enabling broad applicability across\ndomains with structured input spaces."
    },
    {
        "date": "2025-10",
        "title": "Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images",
        "author": "Sami Belguesmia, Mohand Sa\u00efd Allili, and Assia Hamadene",
        "link": "http://arxiv.org/abs/2510.15576v1",
        "abstract": "DeepFake technology has advanced significantly in recent years, enabling the\ncreation of highly realistic synthetic face images. Existing DeepFake detection\nmethods often struggle with pose variations, occlusions, and artifacts that are\ndifficult to detect in real-world conditions. To address these challenges, we\npropose a multi-view architecture that enhances DeepFake detection by analyzing\nfacial features at multiple levels. Our approach integrates three specialized\nencoders, a global view encoder for detecting boundary inconsistencies, a\nmiddle view encoder for analyzing texture and color alignment, and a local view\nencoder for capturing distortions in expressive facial regions such as the\neyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,\nwe incorporate a face orientation encoder, trained to classify face poses,\nensuring robust detection across various viewing angles. By fusing features\nfrom these encoders, our model achieves superior performance in detecting\nmanipulated images, even under challenging pose and lighting\nconditions.Experimental results on challenging datasets demonstrate the\neffectiveness of our method, outperforming conventional single-view approaches"
    },
    {
        "date": "2025-10",
        "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems",
        "author": "Sibo Xiao",
        "link": "http://arxiv.org/abs/2510.15555v2",
        "abstract": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions."
    },
    {
        "date": "2025-10",
        "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval",
        "author": "Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, and Yuki Mitsufuji",
        "link": "http://arxiv.org/abs/2510.15543v1",
        "abstract": "Multimodal retrieval, which seeks to retrieve relevant content across\nmodalities such as text or image, supports applications from AI search to\ncontents production. Despite the success of separate-encoder approaches like\nCLIP align modality-specific embeddings with contrastive learning, recent\nmultimodal large language models (MLLMs) enable a unified encoder that directly\nprocesses composed inputs. While flexible and advanced, we identify that\nunified encoders trained with conventional contrastive learning are prone to\nlearn modality shortcut, leading to poor robustness under distribution shifts.\nWe propose a modality composition awareness framework to mitigate this issue.\nConcretely, a preference loss enforces multimodal embeddings to outperform\ntheir unimodal counterparts, while a composition regularization objective\naligns multimodal embeddings with prototypes composed from its unimodal parts.\nThese objectives explicitly model structural relationships between the composed\nrepresentation and its unimodal counterparts. Experiments on various benchmarks\nshow gains in out-of-distribution retrieval, highlighting modality composition\nawareness as a effective principle for robust composed multimodal retrieval\nwhen utilizing MLLMs as the unified encoder."
    },
    {
        "date": "2025-10",
        "title": "Adversary-Free Counterfactual Prediction via Information-Regularized Representations",
        "author": "Shiqin Tang, Rong Feng, Shuxin Zhuang, Hongzong Li, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2510.15479v1",
        "abstract": "We study counterfactual prediction under assignment bias and propose a\nmathematically grounded, information-theoretic approach that removes\ntreatment-covariate dependence without adversarial training. Starting from a\nbound that links the counterfactual-factual risk gap to mutual information, we\nlearn a stochastic representation Z that is predictive of outcomes while\nminimizing I(Z; T). We derive a tractable variational objective that\nupper-bounds the information term and couples it with a supervised decoder,\nyielding a stable, provably motivated training criterion. The framework extends\nnaturally to dynamic settings by applying the information penalty to sequential\nrepresentations at each decision time. We evaluate the method on controlled\nnumerical simulations and a real-world clinical dataset, comparing against\nrecent state-of-the-art balancing, reweighting, and adversarial baselines.\nAcross metrics of likelihood, counterfactual error, and policy evaluation, our\napproach performs favorably while avoiding the training instabilities and\ntuning burden of adversarial schemes."
    },
    {
        "date": "2025-10",
        "title": "SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models",
        "author": "Hanbin Hong, Shuya Feng, Nima Naderloui, Shenao Yan, Jingyu Zhang, Biying Liu, Ali Arastehfard, Heqing Huang, and Yuan Hong",
        "link": "http://arxiv.org/abs/2510.15476v2",
        "abstract": "Large Language Models (LLMs) have rapidly become integral to real-world\napplications, powering services across diverse sectors. However, their\nwidespread deployment has exposed critical security risks, particularly through\njailbreak prompts that can bypass model alignment and induce harmful outputs.\nDespite intense research into both attack and defense techniques, the field\nremains fragmented: definitions, threat models, and evaluation criteria vary\nwidely, impeding systematic progress and fair comparison. In this\nSystematization of Knowledge (SoK), we address these challenges by (1)\nproposing a holistic, multi-level taxonomy that organizes attacks, defenses,\nand vulnerabilities in LLM prompt security; (2) formalizing threat models and\ncost assumptions into machine-readable profiles for reproducible evaluation;\n(3) introducing an open-source evaluation toolkit for standardized, auditable\ncomparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest\nannotated dataset of jailbreak and benign prompts to date;\\footnote{The dataset\nis released at\n\\href{https://huggingface.co/datasets/youbin2014/JailbreakDB}{\\textcolor{purple}{https://huggingface.co/datasets/youbin2014/JailbreakDB}}.}\nand (5) presenting a comprehensive evaluation platform and leaderboard of\nstate-of-the-art methods \\footnote{will be released soon.}. Our work unifies\nfragmented research, provides rigorous foundations for future studies, and\nsupports the development of robust, trustworthy LLMs suitable for high-stakes\ndeployment."
    },
    {
        "date": "2025-10",
        "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows",
        "author": "Gabriele Visentin, and Patrick Cheridito",
        "link": "http://arxiv.org/abs/2510.15458v1",
        "abstract": "In this paper, we show that interventionally robust optimization problems in\ncausal models are continuous under the $G$-causal Wasserstein distance, but may\nbe discontinuous under the standard Wasserstein distance. This highlights the\nimportance of using generative models that respect the causal structure when\naugmenting data for such tasks. To this end, we propose a new normalizing flow\narchitecture that satisfies a universal approximation property for causal\nstructural models and can be efficiently trained to minimize the $G$-causal\nWasserstein distance. Empirically, we demonstrate that our model outperforms\nstandard (non-causal) generative models in data augmentation for causal\nregression and mean-variance portfolio optimization in causal factor models."
    },
    {
        "date": "2025-10",
        "title": "DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking",
        "author": "Zhiqiang Zhu, Xinbo Gao, Wen Lu, Jie Li, Zhaoyang Wang, and Mingqian Ge",
        "link": "http://arxiv.org/abs/2510.15449v1",
        "abstract": "Existing nighttime aerial trackers based on prompt learning rely solely on\nspatial localization supervision, which fails to provide fine-grained cues that\npoint to target features and inevitably produces vague prompts. This limitation\nimpairs the tracker's ability to accurately focus on the object features and\nresults in trackers still performing poorly. To address this issue, we propose\nDPTrack, a prompt-based aerial tracker designed for nighttime scenarios by\nencoding the given object's attribute features into the directional kernel\nenriched with fine-grained cues to generate precise prompts. Specifically,\ndrawing inspiration from visual bionics, DPTrack first hierarchically captures\nthe object's topological structure, leveraging topological attributes to enrich\nthe feature representation. Subsequently, an encoder condenses these\ntopology-aware features into the directional kernel, which serves as the core\nguidance signal that explicitly encapsulates the object's fine-grained\nattribute cues. Finally, a kernel-guided prompt module built on\nchannel-category correspondence attributes propagates the kernel across the\nfeatures of the search region to pinpoint the positions of target features and\nconvert them into precise prompts, integrating spatial gating for robust\nnighttime tracking. Extensive evaluations on established benchmarks demonstrate\nDPTrack's superior performance. Our code will be available at\nhttps://github.com/zzq-vipsl/DPTrack."
    },
    {
        "date": "2025-10",
        "title": "MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention",
        "author": "Nengbo Zhang, and Hann Woei Ho",
        "link": "http://arxiv.org/abs/2510.15448v1",
        "abstract": "Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for\nenabling cooperative perception and control in autonomous aerial swarms. Yet,\nvision-based recognition models relying only on RGB data often fail to capture\nthe complex spatial temporal characteristics of MAV motion, which limits their\nability to distinguish different actions. To overcome this problem, this paper\npresents MAVR-Net, a multi-view learning-based MAV action recognition\nframework. Unlike traditional single-view methods, the proposed approach\ncombines three complementary types of data, including raw RGB frames, optical\nflow, and segmentation masks, to improve the robustness and accuracy of MAV\nmotion recognition. Specifically, ResNet-based encoders are used to extract\ndiscriminative features from each view, and a multi-scale feature pyramid is\nadopted to preserve the spatiotemporal details of MAV motion patterns. To\nenhance the interaction between different views, a cross-view attention module\nis introduced to model the dependencies among various modalities and feature\nscales. In addition, a multi-view alignment loss is designed to ensure semantic\nconsistency and strengthen cross-view feature representations. Experimental\nresults on benchmark MAV action datasets show that our method clearly\noutperforms existing approaches, achieving 97.8\\%, 96.5\\%, and 92.8\\% accuracy\non the Short MAV, Medium MAV, and Long MAV datasets, respectively."
    },
    {
        "date": "2025-10",
        "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models",
        "author": "Shuang Liang, Zhihao Xu, Jialing Tao, Hui Xue, and Xiting Wang",
        "link": "http://arxiv.org/abs/2510.15430v2",
        "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. To address\nthis, existing detection methods either learn attack-specific parameters, which\nhinders generalization to unseen attacks, or rely on heuristically sound\nprinciples, which limit accuracy and efficiency. To overcome these limitations,\nwe propose Learning to Detect (LoD), a general framework that accurately\ndetects unknown jailbreak attacks by shifting the focus from attack-specific\nlearning to task-specific learning. This framework includes a Multi-modal\nSafety Concept Activation Vector module for safety-oriented representation\nlearning and a Safety Pattern Auto-Encoder module for unsupervised attack\nclassification. Extensive experiments show that our method achieves\nconsistently higher detection AUROC on diverse unknown attacks while improving\nefficiency. The code is available at\nhttps://anonymous.4open.science/r/Learning-to-Detect-51CB."
    },
    {
        "date": "2025-10",
        "title": "Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models",
        "author": "Shashank Gupta",
        "link": "http://arxiv.org/abs/2510.15429v1",
        "abstract": "This dissertation investigates how reinforcement learning (RL) methods can be\ndesigned to be safe, sample-efficient, and robust. Framed through the unifying\nperspective of contextual-bandit RL, the work addresses two major application\ndomains - ranking and recommendation, and text-to-image diffusion models. The\nfirst part of the thesis develops theory and algorithms for safe deployment in\nranking systems. An exposure-based generalisation bound is derived, leading to\na counterfactual risk-minimisation objective whose solution is guaranteed not\nto underperform the logging policy, even with sparse feedback. This guarantee\nis extended to doubly robust estimators, enabling safety even under adversarial\nor misspecified user models and offering practitioners explicit control over\npermissible utility loss. The second part turns to single-action bandits, where\nvarious off-policy estimators are unified within a baseline-correction\nframework. A closed-form optimal baseline is proposed and shown to minimise\nboth evaluation and policy-gradient variance, thereby improving off-policy\nlearning reliability. The final part examines the trade-offs between efficiency\nand effectiveness in generative RL. A systematic study of PPO and REINFORCE\nmotivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple\ndiffusion trajectories with a REINFORCE-style baseline inside PPO's clipped\nobjective. LOOP achieves PPO-level sample efficiency while producing\ngenerations that align more faithfully with textual attributes."
    },
    {
        "date": "2025-10",
        "title": "Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning",
        "author": "Chen Qian, Haoyu Zhang, Junnan Ma, Liuhong Zhu, Qingrui Cai, Yu Wang, Ruibo Song, Lv Li, Lin Mei, Xianwang Jiang, Qin Xu, Boyu Jiang, Ran Tao, Chunmiao Chen, Shufang Chen, Dongyun Liang, Qiu Guo, Jianzhong Lin, Taishan Kang, Mengtian Lu, Liyuan Fu, Ruibin Huang, Huijuan Wan, Xu Huang, Jianhua Wang, Di Guo, Hai Zhong, Jianjun Zhou, and Xiaobo Qu",
        "link": "http://arxiv.org/abs/2510.15400v1",
        "abstract": "Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging\n(multi-shot DWI) for body-wide tumor diagnostics is limited by severe\nmotion-induced phase artifacts from respiration, peristalsis, and so on,\ncompounded by multi-organ, multi-slice, multi-direction and multi-b-value\ncomplexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that\novercomes these challenges through physics-informed modeling and\nsynthetic-data-driven prompt learning. We model inter-shot phase variations as\na high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel\nmatrix reconstruction. Crucially, the algorithm's rank parameter is\nautomatically set via prompt learning trained exclusively on synthetic\nabdominal DWI data emulating physiological motion. Validated across 10,000+\nclinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)\nAchieved twice the spatial resolution of clinical single-shot DWI, enhancing\nliver lesion conspicuity; (2) Generalized to seven diverse anatomical regions\n(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single\nmodel; (3) Outperformed state-of-the-art methods in image quality, artifact\nsuppression, and noise reduction (11 radiologists' evaluations on a 5-point\nscale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points\n(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points\n(good) on knee and tumor brain. The approach eliminates navigator signals and\nrealistic data supervision, providing an interpretable, robust solution for\nhigh-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance\nsignifies transformative potential for precision oncology."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Zero-Shot Reinforcement Learning",
        "author": "Kexin Zheng, Lauriane Teyssier, Yinan Zheng, Yu Luo, and Xiayuan Zhan",
        "link": "http://arxiv.org/abs/2510.15382v1",
        "abstract": "The recent development of zero-shot reinforcement learning (RL) has opened a\nnew avenue for learning pre-trained generalist policies that can adapt to\narbitrary new tasks in a zero-shot manner. While the popular Forward-Backward\nrepresentations (FB) and related methods have shown promise in zero-shot RL, we\nempirically found that their modeling lacks expressivity and that extrapolation\nerrors caused by out-of-distribution (OOD) actions during offline learning\nsometimes lead to biased representations, ultimately resulting in suboptimal\nperformance. To address these issues, we propose Behavior-REgularizEd Zero-shot\nRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that\nsimultaneously enhances learning stability, policy extraction capability, and\nrepresentation learning quality. BREEZE introduces behavioral regularization in\nzero-shot RL policy learning, transforming policy optimization into a stable\nin-sample learning paradigm. Additionally, BREEZE extracts the policy using a\ntask-conditioned diffusion model, enabling the generation of high-quality and\nmultimodal action distributions in zero-shot RL settings. Moreover, BREEZE\nemploys expressive attention-based architectures for representation modeling to\ncapture the complex relationships between environmental dynamics. Extensive\nexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best\nor near-the-best performance while exhibiting superior robustness compared to\nprior offline zero-shot RL methods. The official implementation is available\nat: https://github.com/Whiterrrrr/BREEZE."
    },
    {
        "date": "2025-10",
        "title": "Bilinear Compressive Security",
        "author": "Axel Flinth, Hubert Orlicki, Semira Einsele, and Gerhard Wunder",
        "link": "http://arxiv.org/abs/2510.15380v1",
        "abstract": "Beyond its widespread application in signal and image processing,\n\\emph{compressed sensing} principles have been greatly applied to secure\ninformation transmission (often termed 'compressive security'). In this\nscenario, the measurement matrix $Q$ acts as a one time pad encryption key (in\ncomplex number domain) which can achieve perfect information-theoretic security\ntogether with other benefits such as reduced complexity and energy efficiency\nparticularly useful in IoT. However, unless the matrix is changed for every\nmessage it is vulnerable towards known plain text attacks: only $n$\nobservations suffices to recover a key $Q$ with $n$ columns. In this paper, we\ninvent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')\naddressing these shortcomings: In addition to the linear encoding of the\nmessage $x$ with a matrix $Q$, the sender convolves the resulting vector with a\nrandomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the\nreceiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through\nblind deconvolution. We study a rather idealized known plaintext attack for\nrecovering $Q$ from repeated observations of $y$'s for different, known $x_k$,\nwith varying and unknown $h$ ,giving Eve a number of advantages not present in\npractice. Our main result for BCS states that under a weak symmetry condition\non the filter $h$, recovering $Q$ will require extensive sampling from\ntransmissions of $\\Omega\\left(\\max\\left(n,(n/s)^2\\right)\\right)$ messages $x_k$\nif they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the\nkey. In this way, the scheme is much safer than standard compressed sensing\neven though our assumptions are much in favor towards a potential attacker."
    },
    {
        "date": "2025-10",
        "title": "Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks",
        "author": "Yuyuan Feng, Bin Ma, and Enyan Dai",
        "link": "http://arxiv.org/abs/2510.15333v1",
        "abstract": "Extensive research has highlighted the vulnerability of graph neural networks\n(GNNs) to adversarial attacks, including manipulation, node injection, and the\nrecently emerging threat of backdoor attacks. However, existing defenses\ntypically focus on a single type of attack, lacking a unified approach to\nsimultaneously defend against multiple threats. In this work, we leverage the\nflexibility of the Mixture of Experts (MoE) architecture to design a scalable\nand unified framework for defending against backdoor, edge manipulation, and\nnode injection attacks. Specifically, we propose an MI-based logic diversity\nloss to encourage individual experts to focus on distinct neighborhood\nstructures in their decision processes, thus ensuring a sufficient subset of\nexperts remains unaffected under perturbations in local structures. Moreover,\nwe introduce a robustness-aware router that identifies perturbation patterns\nand adaptively routes perturbed nodes to corresponding robust experts.\nExtensive experiments conducted under various adversarial settings demonstrate\nthat our method consistently achieves superior robustness against multiple\ngraph adversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning",
        "author": "Yiming Lin, Shang Wang, Junkai Zhou, Qiufeng Wang, Xiao-Bo Jin, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2510.15296v1",
        "abstract": "Single Positive Multi-Label Learning (SPMLL) addresses the challenging\nscenario where each training sample is annotated with only one positive label\ndespite potentially belonging to multiple categories, making it difficult to\ncapture complex label relationships and hierarchical structures. While existing\nmethods implicitly model label relationships through distance-based similarity,\nlacking explicit geometric definitions for different relationship types. To\naddress these limitations, we propose the first hyperbolic classification\nframework for SPMLL that represents each label as a hyperbolic ball rather than\na point or vector, enabling rich inter-label relationship modeling through\ngeometric ball interactions. Our ball-based approach naturally captures\nmultiple relationship types simultaneously: inclusion for hierarchical\nstructures, overlap for co-occurrence patterns, and separation for semantic\nindependence. Further, we introduce two key component innovations: a\ntemperature-adaptive hyperbolic ball classifier and a physics-inspired\ndouble-well regularization that guides balls toward meaningful configurations.\nTo validate our approach, extensive experiments on four benchmark datasets\n(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive\nperformance with superior interpretability compared to existing methods.\nFurthermore, statistical analysis reveals strong correlation between learned\nembeddings and real-world co-occurrence patterns, establishing hyperbolic\ngeometry as a more robust paradigm for structured classification under\nincomplete supervision."
    },
    {
        "date": "2025-10",
        "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
        "author": "Zhiyuan Fan, Yifeng Liu, Qingyue Zhao, Angela Yuan, and Quanquan Gu",
        "link": "http://arxiv.org/abs/2510.15262v1",
        "abstract": "Empirical scaling laws prescribe how to allocate parameters, data, and\ncompute, while maximal-update parameterization ($\\mu$P) enables learning-rate\ntransfer across widths by equalizing early-time update magnitudes. However, in\nmodern scale-invariant architectures, training quickly enters an\noptimizer-governed steady state where normalization layers create backward\nscale sensitivity and the effective learning rate becomes width dependent,\ndegrading $\\mu$P transfer. We address this by introducing a weight-decay\nscaling rule for AdamW that preserves sublayer gain across widths. Empirically,\nthe singular-value spectrum of each matrix parameter scales in norm as\n$\\sqrt{\\eta/\\lambda}$ with an approximately invariant shape; under width\nscaling $d$, we observe that the top singular value scales approximately as\n$\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$. Combining this observation with the $\\mu$P\nlearning-rate rule $\\eta_2\\propto d^{-1}$ for matrix-like parameters implies an\nempirical weight-decay scaling rule $\\lambda_2\\propto \\sqrt{d}$ that\napproximately keeps sublayer gains width invariant. Together with vector-like\nparameters trained at $\\eta_1=\\Theta_d(1)$ and $\\lambda_1=0$, this yields\n\\emph{zero-shot} transfer of both learning rate and weight decay from proxy to\ntarget widths, removing per-width sweeps. We validate the rule on LLaMA-style\nTransformers and in a minimal synthetic setting, and we provide a simple\ndiagnostic, matching top singular values, to check sublayer-gain invariance.\nOur results extend $\\mu$P beyond the near-init regime by explicitly controlling\nsteady-state scales set by the optimizer, offering a practical recipe for\nwidth-robust hyperparameter transfer under AdamW."
    },
    {
        "date": "2025-10",
        "title": "DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models",
        "author": "Yangyang Li",
        "link": "http://arxiv.org/abs/2510.15260v1",
        "abstract": "Large language models are highly sensitive to prompt wording. However,\npopular automatic prompt search methods, including InstructZero, often degrade\nunder distribution shift and adversarial evaluation because they optimize\nexpected performance under a single evaluation distribution. Consequently,\nprompts that work in one setting frequently fail to transfer. To address this,\nDRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian\noptimization. Specifically, an f-divergence ball defines an ambiguity set\naround the evaluation distribution, and a robust acquisition rule maximizes\nworst-case expected utility while retaining the query efficiency of Bayesian\nsearch. Therefore, the search explicitly targets reliability under distribution\nshift rather than average behavior alone. Experiments follow the\ninstruction-induction protocol with matched query budgets across formality\nrewriting, code debugging, and translation. For example, on BIG-Bench\ninformative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to\napproximately 85-90%, yielding an absolute gain of about 25-30 points.\nMoreover, auto-debugging shows about +25-point gains under domain shift.\nMeanwhile, stable tasks such as cause-and-effect remain above 96%, indicating\nno loss on in-distribution cases. Furthermore, improvements are consistent\nacross divergence choices and decoding temperatures. Overall, DRO-InstructZero\nconnects distributionally robust optimization with prompt learning, offering a\nplug-and-play and general approach for reliable, transferable prompt alignment\nunder real-world uncertainty."
    },
    {
        "date": "2025-10",
        "title": "Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification",
        "author": "Ynes Ineza, Muhammad A. Ullah, Abdul Serwadda, and Aurore Munyaneza",
        "link": "http://arxiv.org/abs/2510.15173v1",
        "abstract": "Voice interfaces are increasingly used in high stakes domains such as mobile\nbanking, smart home security, and hands free healthcare. Meanwhile, modern\ngenerative models have made high quality voice forgeries inexpensive and easy\nto create, eroding confidence in voice authentication alone. To strengthen\nprotection against such attacks, we present a second authentication factor that\ncombines acoustic evidence with the unique motion patterns of a speaker's lower\nface. By placing lightweight inertial sensors around the mouth to capture mouth\nopening and evolving lower facial geometry, our system records a distinct\nmotion signature with strong discriminative power across individuals. We built\na prototype and recruited 43 participants to evaluate the system under four\nconditions seated, walking on level ground, walking on stairs, and speaking\nwith different language backgrounds (native vs. non native English). Across all\nscenarios, our approach consistently achieved a median equal error rate (EER)\nof 0.01 or lower, indicating that mouth movement data remain robust under\nvariations in gait, posture, and spoken language. We discuss specific use cases\nwhere this second line of defense could provide tangible security benefits to\nvoice authentication systems."
    },
    {
        "date": "2025-10",
        "title": "Targeted Attacks and Defenses for Distributed Federated Learning in Vehicular Networks",
        "author": "Utku Demir, Tugba Erpek, Yalin E. Sagduyu, Sastry Kompella, and Mengran Xue",
        "link": "http://arxiv.org/abs/2510.15109v1",
        "abstract": "In emerging networked systems, mobile edge devices such as ground vehicles\nand unmanned aerial system (UAS) swarms collectively aggregate vast amounts of\ndata to make machine learning decisions such as threat detection in remote,\ndynamic, and infrastructure-constrained environments where power and bandwidth\nare scarce. Federated learning (FL) addresses these constraints and privacy\nconcerns by enabling nodes to share local model weights for deep neural\nnetworks instead of raw data, facilitating more reliable decision-making than\nindividual learning. However, conventional FL relies on a central server to\ncoordinate model updates in each learning round, which imposes significant\ncomputational burdens on the central node and may not be feasible due to the\nconnectivity constraints. By eliminating dependence on a central server,\ndistributed federated learning (DFL) offers scalability, resilience to node\nfailures, learning robustness, and more effective defense strategies. Despite\nthese advantages, DFL remains vulnerable to increasingly advanced and stealthy\ncyberattacks. In this paper, we design sophisticated targeted training data\npoisoning and backdoor (Trojan) attacks, and characterize the emerging\nvulnerabilities in a vehicular network. We analyze how DFL provides resilience\nagainst such attacks compared to individual learning and present effective\ndefense mechanisms to further strengthen DFL against the emerging cyber\nthreats."
    },
    {
        "date": "2025-10",
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
        "author": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, and Nanyun Peng",
        "link": "http://arxiv.org/abs/2510.14949v1",
        "abstract": "Contact languages like English exhibit rich regional variations in the form\nof dialects, which are often used by dialect speakers interacting with\ngenerative models. However, can multimodal generative models effectively\nproduce content given dialectal textual input? In this work, we study this\nquestion by constructing a new large-scale benchmark spanning six common\nEnglish dialects. We work with dialect speakers to collect and verify over 4200\nunique prompts and evaluate on 17 image and video generative models. Our\nautomatic and human evaluation results show that current state-of-the-art\nmultimodal generative models exhibit 32.26% to 48.17% performance degradation\nwhen a single dialect word is used in the prompt. Common mitigation methods\nsuch as fine-tuning and prompt rewriting can only improve dialect performance\nby small margins (< 7%), while potentially incurring significant performance\ndegradation in Standard American English (SAE). To this end, we design a\ngeneral encoder-based mitigation strategy for multimodal generative models. Our\nmethod teaches the model to recognize new dialect features while preserving SAE\nperformance. Experiments on models such as Stable Diffusion 1.5 show that our\nmethod is able to simultaneously raise performance on five dialects to be on\npar with SAE (+34.4%), while incurring near zero cost to SAE performance."
    },
    {
        "date": "2025-10",
        "title": "Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion",
        "author": "Blake Werner, Lizhi Yang, and Aaron D. Ames",
        "link": "http://arxiv.org/abs/2510.14947v2",
        "abstract": "Robust humanoid locomotion in unstructured environments requires\narchitectures that balance fast low-level stabilization with slower perceptual\ndecision-making. We show that a simple layered control architecture (LCA), a\nproprioceptive stabilizer running at high rate, coupled with a compact low-rate\nperceptual policy, enables substantially more robust performance than\nmonolithic end-to-end designs, even when using minimal perception encoders.\nThrough a two-stage training curriculum (blind stabilizer pretraining followed\nby perceptual fine-tuning), we demonstrate that layered policies consistently\noutperform one-stage alternatives in both simulation and hardware. On a Unitree\nG1 humanoid, our approach succeeds across stair and ledge tasks where one-stage\nperceptual policies fail. These results highlight that architectural separation\nof timescales, rather than network scale or complexity, is the key enabler for\nrobust perception-conditioned locomotion."
    },
    {
        "date": "2025-10",
        "title": "A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems",
        "author": "Zixuan Liu, Yi Zhao, Zhuotao Liu, Qi Li, Chuanpu Fu, Guangmeng Zhou, and Ke Xu",
        "link": "http://arxiv.org/abs/2510.14906v1",
        "abstract": "Machine Learning (ML)-based malicious traffic detection is a promising\nsecurity paradigm. It outperforms rule-based traditional detection by\nidentifying various advanced attacks. However, the robustness of these ML\nmodels is largely unexplored, thereby allowing attackers to craft adversarial\ntraffic examples that evade detection. Existing evasion attacks typically rely\non overly restrictive conditions (e.g., encrypted protocols, Tor, or\nspecialized setups), or require detailed prior knowledge of the target (e.g.,\ntraining data and model parameters), which is impractical in realistic\nblack-box scenarios. The feasibility of a hard-label black-box evasion attack\n(i.e., applicable across diverse tasks and protocols without internal target\ninsights) thus remains an open challenge. To this end, we develop\nNetMasquerade, which leverages reinforcement learning (RL) to manipulate attack\nflows to mimic benign traffic and evade detection. Specifically, we establish a\ntailored pre-trained model called Traffic-BERT, utilizing a network-specialized\ntokenizer and an attention mechanism to extract diverse benign traffic\npatterns. Subsequently, we integrate Traffic-BERT into the RL framework,\nallowing NetMasquerade to effectively manipulate malicious packet sequences\nbased on benign traffic patterns with minimal modifications. Experimental\nresults demonstrate that NetMasquerade enables both brute-force and stealthy\nattacks to evade 6 existing detection methods under 80 attack scenarios,\nachieving over 96.65% attack success rate. Notably, it can evade the methods\nthat are either empirically or certifiably robust against existing evasion\nattacks. Finally, NetMasquerade achieves low-latency adversarial traffic\ngeneration, demonstrating its practicality in real-world scenarios."
    },
    {
        "date": "2025-10",
        "title": "Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning",
        "author": "Marc Damie, Florian Hahn, Andreas Peter, and Jan Ramon",
        "link": "http://arxiv.org/abs/2510.14894v1",
        "abstract": "To preserve privacy, multi-party computation (MPC) enables executing Machine\nLearning (ML) algorithms on secret-shared or encrypted data. However, existing\nMPC frameworks are not optimized for sparse data. This makes them unsuitable\nfor ML applications involving sparse data, e.g., recommender systems or\ngenomics. Even in plaintext, such applications involve high-dimensional sparse\ndata, that cannot be processed without sparsity-related optimizations due to\nprohibitively large memory requirements.\n  Since matrix multiplication is central in ML algorithms, we propose MPC\nalgorithms to multiply secret sparse matrices. On the one hand, our algorithms\navoid the memory issues of the \"dense\" data representation of classic secure\nmatrix multiplication algorithms. On the other hand, our algorithms can\nsignificantly reduce communication costs (some experiments show a factor 1000)\nfor realistic problem sizes. We validate our algorithms in two ML applications\nin which existing protocols are impractical.\n  An important question when developing MPC algorithms is what assumptions can\nbe made. In our case, if the number of non-zeros in a row is a sensitive piece\nof information then a short runtime may reveal that the number of non-zeros is\nsmall. Existing approaches make relatively simple assumptions, e.g., that there\nis a universal upper bound to the number of non-zeros in a row. This often\ndoesn't align with statistical reality, in a lot of sparse datasets the amount\nof data per instance satisfies a power law. We propose an approach which allows\nadopting a safe upper bound on the distribution of non-zeros in rows/columns of\nsparse matrices."
    },
    {
        "date": "2025-10",
        "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks",
        "author": "Maor Reuben, Ido Mendel, Or Feldman, Moshe Kravchik, Mordehai Guri, and Rami Puzis",
        "link": "http://arxiv.org/abs/2510.14778v1",
        "abstract": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity."
    },
    {
        "date": "2025-10",
        "title": "SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services",
        "author": "Ha Xuan Son, Nguyen Quoc Anh, Phat T. Tran-Truong, Le Thanh Tuan, and Pham Thanh Nghiem",
        "link": "http://arxiv.org/abs/2510.14708v1",
        "abstract": "The Internet of Medical Things (IoMT) has revolutionized healthcare by\ntransforming medical operations into standardized, interoperable services.\nHowever, this service-oriented model introduces significant security\nvulnerabilities in device management and communication, which are especially\ncritical given the sensitivity of medical data. To address these risks, this\npaper proposes SLIE (Secure and Lightweight Identity Encryption), a novel\ncryptosystem based on Wildcard Key Derivation Identity-Based Encryption\n(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication\nthrough end-to-end encryption, hierarchical access control, and a lightweight\nkey management system designed for resource-constrained devices. It\nincorporates constant-time operations, memory obfuscation, and expiry-based key\nrevocation to counter side-channel, man-in-the-middle, and unauthorized access\nattacks, thereby ensuring compliance with standards like HIPAA and GDPR.\nEvaluations show that SLIE significantly outperforms RSA, with encryption and\ndecryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement\nin encryption speed, a 99.70% improvement in decryption speed, and an energy\nefficiency of 0.014 J/KB."
    },
    {
        "date": "2025-10",
        "title": "AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX",
        "author": "Nicolas Dutly, Friederike Groschupp, Ivan Puddu, Kari Kostiainen, and Srdjan Capkun",
        "link": "http://arxiv.org/abs/2510.14675v1",
        "abstract": "To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel\nintroduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent\ndeterministic single-stepping. In this work, we introduce AEX-NStep, the first\ninterrupt counting attack on AEX-Notify-enabled Enclaves. We show that\ndeterministic single-stepping is not required for interrupt counting attacks to\nbe practical and that, therefore, AEX-Notify does not entirely prevent such\nattacks. We specifically show that one of AEX-Notify's security guarantees,\nobfuscated forward progress, does not hold, and we introduce two new\nprobabilistic interrupt counting attacks. We use these attacks to construct a\npractical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our\nresults extend the original security analysis of AEX-Notify and inform the\ndesign of future mitigations."
    },
    {
        "date": "2025-10",
        "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
        "author": "Jihyun Yu, Yoojin Oh, Wonho Bae, Mingyu Kim, and Junhyug Noh",
        "link": "http://arxiv.org/abs/2510.14634v1",
        "abstract": "Test-time adaptation (TTA) aims to correct performance degradation of deep\nmodels under distribution shifts by updating models or inputs using unlabeled\ntest data. Input-only diffusion-based TTA methods improve robustness for\nclassification to corruptions but rely on gradient guidance, limiting\nexploration and generalization across distortion types. We propose SteeringTTA,\nan inference-only framework that adapts Feynman-Kac steering to guide\ndiffusion-based input adaptation for classification with rewards driven by\npseudo-label. SteeringTTA maintains multiple particle trajectories, steered by\na combination of cumulative top-K probabilities and an entropy schedule, to\nbalance exploration and confidence. On ImageNet-C, SteeringTTA consistently\noutperforms the baseline without any model updates or source data."
    },
    {
        "date": "2025-10",
        "title": "Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration",
        "author": "Evangelos Lamprou, Julian Dai, Grigoris Ntousakis, Martin C. Rinard, and Nikos Vasilakis",
        "link": "http://arxiv.org/abs/2510.14522v1",
        "abstract": "Software supply-chain attacks are an important and ongoing concern in the\nopen source software ecosystem. These attacks maintain the standard\nfunctionality that a component implements, but additionally hide malicious\nfunctionality activated only when the component reaches its target environment.\nLexo addresses such stealthy attacks by automatically learning and regenerating\nvulnerability-free versions of potentially malicious components. Lexo first\ngenerates a set of input-output pairs to model a component's full observable\nbehavior, which it then uses to synthesize a new version of the original\ncomponent. The new component implements the original functionality but avoids\nstealthy malicious behavior. Throughout this regeneration process, Lexo\nconsults several distinct instances of Large Language Models (LLMs), uses\ncorrectness and coverage metrics to shepherd these instances, and guardrails\ntheir results. Our evaluation on 100+ real-world packages, including high\nprofile stealthy supply-chain attacks, indicates that Lexo scales across\nmultiple domains, regenerates code efficiently (<100s on average), maintains\ncompatibility, and succeeds in eliminating malicious code in several real-world\nsupply-chain-attacks, even in cases when a state-of-the-art LLM fails to\neliminate malicious code when prompted to do so."
    },
    {
        "date": "2025-10",
        "title": "Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models",
        "author": "Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, and Kai Zhou",
        "link": "http://arxiv.org/abs/2510.14470v1",
        "abstract": "The emergence of graph foundation models (GFMs), particularly those\nincorporating language models (LMs), has revolutionized graph learning and\ndemonstrated remarkable performance on text-attributed graphs (TAGs). However,\ncompared to traditional GNNs, these LM-empowered GFMs introduce unique security\nvulnerabilities during the unsecured prompt tuning phase that remain\nunderstudied in current research. Through empirical investigation, we reveal a\nsignificant performance degradation in traditional graph backdoor attacks when\noperating in attribute-inaccessible constrained TAG systems without explicit\ntrigger node attribute optimization. To address this, we propose a novel\ndual-trigger backdoor attack framework that operates at both text-level and\nstruct-level, enabling effective attacks without explicit optimization of\ntrigger node text attributes through the strategic utilization of a\npre-established text pool. Extensive experimental evaluations demonstrate that\nour attack maintains superior clean accuracy while achieving outstanding attack\nsuccess rates, including scenarios with highly concealed single-trigger nodes.\nOur work highlights critical backdoor risks in web-deployed LM-empowered GFMs\nand contributes to the development of more robust supervision mechanisms for\nopen-source platforms in the era of foundation models."
    },
    {
        "date": "2025-10",
        "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
        "author": "Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, and Biqing Huang",
        "link": "http://arxiv.org/abs/2510.14466v1",
        "abstract": "As large language models (LLMs) rapidly advance, performance on high-resource\nlanguages (e.g., English, Chinese) is nearing saturation, yet remains\nsubstantially lower for low-resource languages (e.g., Urdu, Thai) due to\nlimited training data, machine-translation noise, and unstable cross-lingual\nalignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language\nModels), a training framework that robustly improves cross-lingual\nrepresentations under low-resource conditions while jointly strengthening\nretrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored\nRepresentation Composition Architecture), which anchors low-resource languages\nto an English semantic space via anchor-based alignment and multi-agent\ncollaborative encoding, preserving geometric stability in a shared embedding\nspace; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a\nlanguage-aware lightweight reasoning head with consistency regularization on\ntop of Arca's multilingual representations, unifying the training objective to\nenhance cross-lingual understanding, retrieval, and reasoning robustness. We\nfurther construct and release a multilingual product retrieval dataset covering\nfive Southeast Asian and two South Asian languages. Experiments across\nlow-resource benchmarks (cross-lingual retrieval, semantic similarity, and\nreasoning) show consistent gains and robustness under few-shot and\nnoise-amplified settings; ablations validate the contribution of both Arca and\nLaSR. Code will be released on GitHub and the dataset on Hugging Face."
    },
    {
        "date": "2025-10",
        "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
        "author": "Sven Jacob, Weijia Shao, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2510.14460v1",
        "abstract": "Video-based object detection plays a vital role in safety-critical\napplications. While deep learning-based object detectors have achieved\nimpressive performance, they remain vulnerable to adversarial attacks,\nparticularly those involving universal perturbations. In this work, we propose\na minimally distorted universal adversarial attack tailored for video object\ndetection, which leverages nuclear norm regularization to promote structured\nperturbations concentrated in the background. To optimize this formulation\nefficiently, we employ an adaptive, optimistic exponentiated gradient method\nthat enhances both scalability and convergence. Our results demonstrate that\nthe proposed attack outperforms both low-rank projected gradient descent and\nFrank-Wolfe based attacks in effectiveness while maintaining high stealthiness.\nAll code and data are publicly available at\nhttps://github.com/jsve96/AO-Exp-Attack."
    },
    {
        "date": "2025-10",
        "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
        "author": "Brandon Hill, and Kma Solaiman",
        "link": "http://arxiv.org/abs/2510.14389v1",
        "abstract": "Motherboard defect detection is critical for ensuring reliability in\nhigh-volume electronics manufacturing. While prior research in PCB inspection\nhas largely targeted bare-board or trace-level defects, assembly-level\ninspection of full motherboards inspection remains underexplored. In this work,\nwe present BoardVision, a reproducible framework for detecting assembly-level\ndefects such as missing screws, loose fan wiring, and surface scratches. We\nbenchmark two representative detectors - YOLOv7 and Faster R-CNN, under\ncontrolled conditions on the MiracleFactory motherboard dataset, providing the\nfirst systematic comparison in this domain. To mitigate the limitations of\nsingle models, where YOLO excels in precision but underperforms in recall and\nFaster R-CNN shows the reverse, we propose a lightweight ensemble,\nConfidence-Temporal Voting (CTV Voter), that balances precision and recall\nthrough interpretable rules. We further evaluate robustness under realistic\nperturbations including sharpness, brightness, and orientation changes,\nhighlighting stability challenges often overlooked in motherboard defect\ndetection. Finally, we release a deployable GUI-driven inspection tool that\nbridges research evaluation with operator usability. Together, these\ncontributions demonstrate how computer vision techniques can transition from\nbenchmark results to practical quality assurance for assembly-level motherboard\nmanufacturing."
    },
    {
        "date": "2025-10",
        "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
        "author": "Danish Ali, Ajmal Mian, Naveed Akhtar, and Ghulam Mubashar Hassan",
        "link": "http://arxiv.org/abs/2510.14383v1",
        "abstract": "Accurate brain tumor segmentation is significant for clinical diagnosis and\ntreatment. It is challenging due to the heterogeneity of tumor subregions.\nMamba-based State Space Models have demonstrated promising performance.\nHowever, they incur significant computational overhead due to sequential\nfeature computation across multiple spatial axes. Moreover, their robustness\nacross diverse BraTS data partitions remains largely unexplored, leaving a\ncritical gap in reliable evaluation. To address these limitations, we propose\ndual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation\nmodel that captures multi-scale long-range dependencies with minimal\ncomputational overhead. We leverage a space-filling curve to preserve spatial\nlocality during 3D-to-1D feature mapping, thereby reducing reliance on\ncomputationally expensive multi-axial feature scans. To enrich feature\nrepresentation, we propose a gated fusion module that adaptively integrates\nforward and reverse contexts, along with a quantization block that discretizes\nfeatures to improve robustness. In addition, we propose five systematic folds\non BraTS2023 for rigorous evaluation of segmentation techniques under diverse\nconditions and present detailed analysis of common failure scenarios. On the\n20\\% test set used by recent methods, our model achieves Dice improvements of\n0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor.\nEvaluations on the proposed systematic five folds demonstrate that our model\nmaintains competitive whole tumor accuracy while achieving clear average Dice\ngains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing\nstate-of-the-art. Furthermore, our model attains 15 times improvement in\nefficiency while maintaining high segmentation accuracy, highlighting its\nrobustness and computational advantage over existing approaches."
    },
    {
        "date": "2025-10",
        "title": "BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection",
        "author": "Zichen Liu, Shao Yang, and Xusheng Xiao",
        "link": "http://arxiv.org/abs/2510.14344v1",
        "abstract": "Mobile app markets host millions of apps, yet undesired behaviors (e.g.,\ndisruptive ads, illegal redirection, payment deception) remain hard to catch\nbecause they often do not rely on permission-protected APIs and can be easily\ncamouflaged via UI or metadata edits. We present BINCTX, a learning approach\nthat builds multi-modal representations of an app from (i) a global\nbytecode-as-image view that captures code-level semantics and family-style\npatterns, (ii) a contextual view (manifested actions, components, declared\npermissions, URL/IP constants) indicating how behaviors are triggered, and\n(iii) a third-party-library usage view summarizing invocation frequencies along\ninter-component call paths. The three views are embedded and fused to train a\ncontextual-aware classifier. On real-world malware and benign apps, BINCTX\nattains a macro F1 of 94.73%, outperforming strong baselines by at least\n14.92%. It remains robust under commercial obfuscation (F1 84%\npost-obfuscation) and is more resistant to adversarial samples than\nstate-of-the-art bytecode-only systems."
    },
    {
        "date": "2025-10",
        "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease",
        "author": "Yangyang Li",
        "link": "http://arxiv.org/abs/2510.14332v1",
        "abstract": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD\npatients, leading to early treatments that lessen symptoms and alleviating\nfinancial burden of health care. As one of the leading signs of AD, language\ncapability changes can be used for early diagnosis of AD. In this paper, I\ndevelop a robust classification method using hybrid word embedding and\nfine-tuned hyperparameters to achieve state-of-the-art accuracy in the early\ndetection of AD. Specifically, we create a hybrid word embedding based on word\nvectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The\nscores identify whether a sentence is fluent or not and capture semantic\ncontext of the sentences. I enrich the word embedding by adding linguistic\nfeatures to analyze syntax and semantics. Further, we input an embedded feature\nvector into logistic regression and fine tune hyperparameters throughout the\npipeline. By tuning hyperparameters of the machine learning pipeline (e.g.,\nmodel regularization parameter, learning rate and vector size of Doc2Vec, and\nvector size of ELMo), I achieve 91% classification accuracy and an Area Under\nthe Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based\non my knowledge, my model with 91% accuracy and 97% AUC outperforms the best\nexisting NLP model for AD diagnosis with an accuracy of 88% [32]. I study the\nmodel stability through repeated experiments and find that the model is stable\neven though the training data is split randomly (standard deviation of accuracy\n= 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method\nis accurate and stable. This model can be used as a large-scale screening\nmethod for AD, as well as a complementary examination for doctors to detect AD."
    },
    {
        "date": "2025-10",
        "title": "TangledFeatures: Robust Feature Selection in Highly Correlated Spaces",
        "author": "Allen Daniel Sunny",
        "link": "http://arxiv.org/abs/2510.15005v1",
        "abstract": "Feature selection is a fundamental step in model development, shaping both\npredictive performance and interpretability. Yet, most widely used methods\nfocus on predictive accuracy, and their performance degrades in the presence of\ncorrelated predictors. To address this gap, we introduce TangledFeatures, a\nframework for feature selection in correlated feature spaces. It identifies\nrepresentative features from groups of entangled predictors, reducing\nredundancy while retaining explanatory power. The resulting feature subset can\nbe directly applied in downstream models, offering a more interpretable and\nstable basis for analysis compared to traditional selection techniques. We\ndemonstrate the effectiveness of TangledFeatures on Alanine Dipeptide, applying\nit to the prediction of backbone torsional angles and show that the selected\nfeatures correspond to structurally meaningful intra-atomic distances that\nexplain variation in these angles."
    },
    {
        "date": "2025-10",
        "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
        "author": "Shivangi Yadav, and Arun Ross",
        "link": "http://arxiv.org/abs/2510.14314v1",
        "abstract": "An iris biometric system can be compromised by presentation attacks (PAs)\nwhere artifacts such as artificial eyes, printed eye images, or cosmetic\ncontact lenses are presented to the system. To counteract this, several\npresentation attack detection (PAD) methods have been developed. However, there\nis a scarcity of datasets for training and evaluating iris PAD techniques due\nto the implicit difficulties in constructing and imaging PAs. To address this,\nwe introduce the Multi-domain Image Translative Diffusion StyleGAN\n(MID-StyleGAN), a new framework for generating synthetic ocular images that\ncaptures the PA and bonafide characteristics in multiple domains such as\nbonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the\nstrengths of diffusion models and generative adversarial networks (GANs) to\nproduce realistic and diverse synthetic data. Our approach utilizes a\nmulti-domain architecture that enables the translation between bonafide ocular\nimages and different PA domains. The model employs an adaptive loss function\ntailored for ocular data to maintain domain consistency. Extensive experiments\ndemonstrate that MID-StyleGAN outperforms existing methods in generating\nhigh-quality synthetic ocular images. The generated data was used to\nsignificantly enhance the performance of PAD systems, providing a scalable\nsolution to the data scarcity problem in iris and ocular biometrics. For\nexample, on the LivDet2020 dataset, the true detect rate at 1% false detect\nrate improved from 93.41% to 98.72%, showcasing the impact of the proposed\nmethod."
    },
    {
        "date": "2025-10",
        "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
        "author": "Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2510.14312v1",
        "abstract": "A multi-agent system (MAS) powered by large language models (LLMs) can\nautomate tedious user tasks such as meeting scheduling that requires\ninter-agent collaboration. LLMs enable nuanced protocols that account for\nunstructured private data, user constraints, and preferences. However, this\ndesign introduces new risks, including misalignment and attacks by malicious\nparties that compromise agents or steal user data. In this paper, we propose\nthe Terrarium framework for fine-grained study on safety, privacy, and security\nin LLM-based MAS. We repurpose the blackboard design, an early approach in\nmulti-agent systems, to create a modular, configurable testbed for multi-agent\ncollaboration. We identify key attack vectors such as misalignment, malicious\nagents, compromised communication, and data poisoning. We implement three\ncollaborative MAS scenarios with four representative attacks to demonstrate the\nframework's flexibility. By providing tools to rapidly prototype, evaluate, and\niterate on defenses and designs, Terrarium aims to accelerate progress toward\ntrustworthy multi-agent systems."
    },
    {
        "date": "2025-10",
        "title": "Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks",
        "author": "Xinhao Deng, Jingyou Chen, Linxiao Yu, Yixiang Zhang, Zhongyi Gu, Changhao Qiu, Xiyuan Zhao, Ke Xu, and Qi Li",
        "link": "http://arxiv.org/abs/2510.14283v1",
        "abstract": "Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to\ninfer the websites visited by users, posing a serious threat to anonymous\ncommunication systems. Although recent WF techniques achieve over 90% accuracy\nin controlled experimental settings, most studies remain confined to single\nscenarios, overlooking the complexity of real-world environments. This paper\npresents the first systematic and comprehensive evaluation of existing WF\nattacks under diverse realistic conditions, including defense mechanisms,\ntraffic drift, multi-tab browsing, early-stage detection, open-world settings,\nand few-shot scenarios. Experimental results show that many WF techniques with\nstrong performance in isolated settings degrade significantly when facing other\nconditions. Since real-world environments often combine multiple challenges,\ncurrent WF attacks are difficult to apply directly in practice. This study\nhighlights the limitations of WF attacks and introduces a multidimensional\nevaluation framework, offering critical insights for developing more robust and\npractical WF attacks."
    },
    {
        "date": "2025-10",
        "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
        "author": "Kieu-Anh Truong Thi, Huy-Hieu Pham, and Duc-Trong Le",
        "link": "http://arxiv.org/abs/2510.14273v1",
        "abstract": "Domain shift in histopathology, often caused by differences in acquisition\nprocesses or data sources, poses a major challenge to the generalization\nability of deep learning models. Existing methods primarily rely on modeling\nstatistical correlations by aligning feature distributions or introducing\nstatistical variation, yet they often overlook causal relationships. In this\nwork, we propose a novel causal-inference-based framework that leverages\nsemantic features while mitigating the impact of confounders. Our method\nimplements the front-door principle by designing transformation strategies that\nexplicitly incorporate mediators and observed tissue slides. We validate our\nmethod on the CAMELYON17 dataset and a private histopathology dataset,\ndemonstrating consistent performance gains across unseen domains. As a result,\nour approach achieved up to a 7% improvement in both the CAMELYON17 dataset and\nthe private histopathology dataset, outperforming existing baselines. These\nresults highlight the potential of causal inference as a powerful tool for\naddressing domain shift in histopathology image analysis."
    },
    {
        "date": "2025-10",
        "title": "Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation",
        "author": "Jingwen Gu, Yiting He, Zhishuai Liu, and Pan Xu",
        "link": "http://arxiv.org/abs/2510.14246v1",
        "abstract": "Decision-making under distribution shift is a central challenge in\nreinforcement learning (RL), where training and deployment environments differ.\nWe study this problem through the lens of robust Markov decision processes\n(RMDPs), which optimize performance against adversarial transition dynamics.\nOur focus is the online setting, where the agent has only limited interaction\nwith the environment, making sample efficiency and exploration especially\ncritical. Policy optimization, despite its success in standard RL, remains\ntheoretically and empirically underexplored in robust RL. To bridge this gap,\nwe propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized\n\\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online\npolicy optimization method that learns robust policies with sublinear regret.\nTo enable tractable optimization within the softmax policy class, DR-RPO\nincorporates reference-policy regularization, yielding RMDP variants that are\ndoubly constrained in both transitions and policies. To scale to large\nstate-action spaces, we adopt the $d$-rectangular linear MDP formulation and\ncombine linear function approximation with an upper confidence bonus for\noptimistic exploration. We provide theoretical guarantees showing that policy\noptimization can achieve polynomial suboptimality bounds and sample efficiency\nin robust RL, matching the performance of value-based approaches. Finally,\nempirical results across diverse domains corroborate our theory and demonstrate\nthe robustness of DR-RPO."
    },
    {
        "date": "2025-10",
        "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
        "author": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, and Morteza Dehghani",
        "link": "http://arxiv.org/abs/2510.14242v1",
        "abstract": "Large Language Models (LLMs) often produce inconsistent answers when faced\nwith different phrasings of the same prompt. In this paper, we propose\nFlip-Flop Consistency ($F^2C$), an unsupervised training method that improves\nrobustness to such perturbations. $F^2C$ is composed of two key components. The\nfirst, Consensus Cross-Entropy (CCE), uses a majority vote across prompt\nvariations to create a hard pseudo-label. The second is a representation\nalignment loss that pulls lower-confidence and non-majority predictors toward\nthe consensus established by high-confidence, majority-voting variations. We\nevaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt\nvariations per dataset. On average, $F^2C$ raises observed agreement by 11.62%,\nimproves mean $F_1$ by 8.94%, and reduces performance variance across formats\nby 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively,\nincreasing $\\overline{F_1}$ and agreement while decreasing variance across most\nsource-target pairs. Finally, when trained on only a subset of prompt\nperturbations and evaluated on held-out formats, $F^2C$ consistently improves\nboth performance and agreement while reducing variance. These findings\nhighlight $F^2C$ as an effective unsupervised method for enhancing LLM\nconsistency, performance, and generalization under prompt perturbations. Code\nis available at\nhttps://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs."
    },
    {
        "date": "2025-10",
        "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction",
        "author": "Yingguang Yang, Xianghua Zeng, Qi Wu, Hao Peng, Yutong Xia, Hao Liu, Bin Chong, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2510.16035v1",
        "abstract": "Social networks have become a crucial source of real-time information for\nindividuals. The influence of social bots within these platforms has garnered\nconsiderable attention from researchers, leading to the development of numerous\ndetection technologies. However, the vulnerability and robustness of these\ndetection methods is still underexplored. Existing Graph Neural Network\n(GNN)-based methods cannot be directly applied due to the issues of limited\ncontrol over social agents, the black-box nature of bot detectors, and the\nheterogeneity of bots. To address these challenges, this paper proposes the\nfirst adversarial multi-agent Reinforcement learning framework for social Bot\ncontrol attacks (RoBCtrl) targeting GNN-based social bot detectors.\nSpecifically, we use a diffusion model to generate high-fidelity bot accounts\nby reconstructing existing account data with minor modifications, thereby\nevading detection on social platforms. To the best of our knowledge, this is\nthe first application of diffusion models to mimic the behavior of evolving\nsocial bots effectively. We then employ a Multi-Agent Reinforcement Learning\n(MARL) method to simulate bots adversarial behavior. We categorize social\naccounts based on their influence and budget. Different agents are then\nemployed to control bot accounts across various categories, optimizing the\nattachment strategy through reinforcement learning. Additionally, a\nhierarchical state abstraction based on structural entropy is designed to\naccelerate the reinforcement learning. Extensive experiments on social bot\ndetection datasets demonstrate that our framework can effectively undermine the\nperformance of GNN-based detectors."
    },
    {
        "date": "2025-10",
        "title": "RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models",
        "author": "Fanchao Meng, Jiaping Gui, Yunbo Li, and Yue Wu",
        "link": "http://arxiv.org/abs/2510.14233v1",
        "abstract": "Modern Network Intrusion Detection Systems generate vast volumes of low-level\nalerts, yet these outputs remain semantically fragmented, requiring\nlabor-intensive manual correlation with high-level adversarial behaviors.\nExisting solutions for automating this mapping-rule-based systems and machine\nlearning classifiers-suffer from critical limitations: rule-based approaches\nfail to adapt to novel attack variations, while machine learning methods lack\ncontextual awareness and treat tactic-technique mapping as a syntactic matching\nproblem rather than a reasoning task. Although Large Language Models have shown\npromise in cybersecurity tasks, preliminary experiments reveal that existing\nLLM-based methods frequently hallucinate technique names or produce\ndecontextualized mappings due to their single-step classification approach.\n  To address these challenges, we introduce RHINO, a novel framework that\ndecomposes LLM-based attack analysis into three interpretable phases mirroring\nhuman reasoning: (1) behavioral abstraction, where raw logs are translated into\ncontextualized narratives; (2) multi-role collaborative inference, generating\ncandidate techniques by evaluating behavioral evidence against MITRE ATT&CK\nknowledge; and (3) validation, cross-referencing predictions with official\nMITRE definitions to rectify hallucinations. RHINO bridges the semantic gap\nbetween low-level observations and adversarial intent while improving output\nreliability through structured reasoning.\n  We evaluate RHINO on three benchmarks across four backbone models. RHINO\nachieved high accuracy, with model performance ranging from 86.38% to 88.45%,\nresulting in relative gains from 24.25% to 76.50% across different models. Our\nresults demonstrate that RHINO significantly enhances the interpretability and\nscalability of threat analysis, offering a blueprint for deploying LLMs in\noperational security settings."
    },
    {
        "date": "2025-10",
        "title": "When Flatness Does (Not) Guarantee Adversarial Robustness",
        "author": "Nils Philipp Walter, Linara Adilova, Jilles Vreeken, and Michael Kamp",
        "link": "http://arxiv.org/abs/2510.14231v1",
        "abstract": "Despite their empirical success, neural networks remain vulnerable to small,\nadversarial perturbations. A longstanding hypothesis suggests that flat minima,\nregions of low curvature in the loss landscape, offer increased robustness.\nWhile intuitive, this connection has remained largely informal and incomplete.\nBy rigorously formalizing the relationship, we show this intuition is only\npartially correct: flatness implies local but not global adversarial\nrobustness. To arrive at this result, we first derive a closed-form expression\nfor relative flatness in the penultimate layer, and then show we can use this\nto constrain the variation of the loss in input space. This allows us to\nformally analyze the adversarial robustness of the entire network. We then show\nthat to maintain robustness beyond a local neighborhood, the loss needs to\ncurve sharply away from the data manifold. We validate our theoretical\npredictions empirically across architectures and datasets, uncovering the\ngeometric structure that governs adversarial vulnerability, and linking\nflatness to model confidence: adversarial examples often lie in large, flat\nregions where the model is confidently wrong. Our results challenge simplified\nviews of flatness and provide a nuanced understanding of its role in\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks",
        "author": "Trilok Padhi, Pinxian Lu, Abdulkadir Erol, Tanmay Sutar, Gauri Sharma, Mina Sonmez, Munmun De Choudhury, and Ugur Kursuncu",
        "link": "http://arxiv.org/abs/2510.14207v2",
        "abstract": "Large Language Model (LLM) agents are powering a growing share of interactive\nweb applications, yet remain vulnerable to misuse and harm. Prior jailbreak\nresearch has largely focused on single-turn prompts, whereas real harassment\noften unfolds over multi-turn interactions. In this work, we present the Online\nHarassment Agentic Benchmark consisting of: (i) a synthetic multi-turn\nharassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)\nsimulation informed by repeated game theory, (iii) three jailbreak methods\nattacking agents across memory, planning, and fine-tuning, and (iv) a\nmixed-methods evaluation framework. We utilize two prominent LLMs,\nLLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our\nresults show that jailbreak tuning makes harassment nearly guaranteed with an\nattack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,\nand 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal\nrate to 1-2% in both models. The most prevalent toxic behaviors are Insult with\n84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.\n31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive\ncategories such as sexual or racial harassment. Qualitative evaluation further\nreveals that attacked agents reproduce human-like aggression profiles, such as\nMachiavellian/psychopathic patterns under planning, and narcissistic tendencies\nwith memory. Counterintuitively, closed-source and open-source models exhibit\ndistinct escalation trajectories across turns, with closed-source models\nshowing significant vulnerability. Overall, our findings show that multi-turn\nand theory-grounded attacks not only succeed at high rates but also mimic\nhuman-like harassment dynamics, motivating the development of robust safety\nguardrails to ultimately keep online platforms safe and responsible."
    },
    {
        "date": "2025-10",
        "title": "Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis",
        "author": "Junyu Ren, Wensheng Gan, Guangyu Zhang, Wei Zhong, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2510.16033v1",
        "abstract": "Existing transfer fault diagnosis methods typically assume either clean data\nor sufficient domain similarity, which limits their effectiveness in industrial\nenvironments where severe noise interference and domain shifts coexist. To\naddress this challenge, we propose an information separation global-focal\nadversarial network (ISGFAN), a robust framework for cross-domain fault\ndiagnosis under noise conditions. ISGFAN is built on an information separation\narchitecture that integrates adversarial learning with an improved orthogonal\nloss to decouple domain-invariant fault representation, thereby isolating noise\ninterference and domain-specific characteristics. To further strengthen\ntransfer robustness, ISGFAN employs a global-focal domain-adversarial scheme\nthat constrains both the conditional and marginal distributions of the model.\nSpecifically, the focal domain-adversarial component mitigates\ncategory-specific transfer obstacles caused by noise in unsupervised scenarios,\nwhile the global domain classifier ensures alignment of the overall\ndistribution. Experiments conducted on three public benchmark datasets\ndemonstrate that the proposed method outperforms other prominent existing\napproaches, confirming the superiority of the ISGFAN framework. Data and code\nare available at https://github.com/JYREN-Source/ISGFAN"
    },
    {
        "date": "2025-10",
        "title": "Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks",
        "author": "Jack Vanlyssel",
        "link": "http://arxiv.org/abs/2510.14185v1",
        "abstract": "Industrial Control Systems (ICS) underpin the United States' critical\ninfrastructure, managing essential services such as power, water, and\ntransportation that are vital to national security and public safety. However,\nincreasing digital integration has exposed these systems to escalating cyber\nthreats. Historical attacks like Stuxnet and the Ukraine power grid incident\nrevealed exploitable weaknesses-poor network segmentation, outdated software,\nweak authentication, and inadequate monitoring-that persist in many U.S. ICS\nenvironments today. This paper analyzes these landmark attacks to identify\nrecurring vulnerabilities and assess their relevance to current U.S.\ninfrastructure. It argues that without immediate reforms, similar exploits\ncould lead to catastrophic disruptions and national security crises. To address\nthese risks, the paper proposes policy measures focused on implementing\nzero-trust architecture and improved network segmentation to enhance system\nresilience. These recommendations aim to guide policymakers and industry\nleaders in securing the nation's most critical operational technologies against\nfuture cyber threats."
    },
    {
        "date": "2025-10",
        "title": "High-Dimensional BWDM: A Robust Nonparametric Clustering Validation Index for Large-Scale Data",
        "author": "Mohammed Baragilly, and Hend Gabr",
        "link": "http://arxiv.org/abs/2510.14145v1",
        "abstract": "Determining the appropriate number of clusters in unsupervised learning is a\ncentral problem in statistics and data science. Traditional validity indices\nsuch as Calinski-Harabasz, Silhouette, and Davies-Bouldin-depend on\ncentroid-based distances and therefore degrade in high-dimensional or\ncontaminated data. This paper proposes a new robust, nonparametric clustering\nvalidation framework, the High-Dimensional Between-Within Distance Median\n(HD-BWDM), which extends the recently introduced BWDM criterion to\nhigh-dimensional spaces. HD-BWDM integrates random projection and principal\ncomponent analysis to mitigate the curse of dimensionality and applies trimmed\nclustering and medoid-based distances to ensure robustness against outliers. We\nderive theoretical results showing consistency and convergence under\nJohnson-Lindenstrauss embeddings. Extensive simulations demonstrate that\nHD-BWDM remains stable and interpretable under high-dimensional projections and\ncontamination, providing a robust alternative to traditional centroid-based\nvalidation criteria. The proposed method provides a theoretically grounded,\ncomputationally efficient stopping rule for nonparametric clustering in modern\nhigh-dimensional applications."
    },
    {
        "date": "2025-10",
        "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
        "author": "Edoardo Allegrini, Ananth Shreekumar, and Z. Berkay Celik",
        "link": "http://arxiv.org/abs/2510.14133v1",
        "abstract": "Agentic AI systems, which leverage multiple autonomous agents and Large\nLanguage Models (LLMs), are increasingly used to address complex, multi-step\ntasks. The safety, security, and functionality of these systems are critical,\nespecially in high-stakes applications. However, the current ecosystem of\ninter-agent communication is fragmented, with protocols such as the Model\nContext Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol\nfor coordination being analyzed in isolation. This fragmentation creates a\nsemantic gap that prevents the rigorous analysis of system properties and\nintroduces risks such as architectural misalignment and exploitable\ncoordination issues. To address these challenges, we introduce a modeling\nframework for agentic AI systems composed of two foundational models. The\nfirst, the host agent model, formalizes the top-level entity that interacts\nwith the user, decomposes tasks, and orchestrates their execution by leveraging\nexternal agents and tools. The second, the task lifecycle model, details the\nstates and transitions of individual sub-tasks from creation to completion,\nproviding a fine-grained view of task management and error handling. Together,\nthese models provide a unified semantic framework for reasoning about the\nbehavior of multi-AI agent systems. Grounded in this framework, we define 17\nproperties for the host agent and 14 for the task lifecycle, categorized into\nliveness, safety, completeness, and fairness. Expressed in temporal logic,\nthese properties enable formal verification of system behavior, detection of\ncoordination edge cases, and prevention of deadlocks and security\nvulnerabilities. Through this effort, we introduce the first rigorously\ngrounded, domain-agnostic framework for the systematic analysis, design, and\ndeployment of correct, reliable, and robust agentic AI systems."
    },
    {
        "date": "2025-10",
        "title": "Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants",
        "author": "Waqar Muhammad Ashraf, Talha Ansar, Abdulelah S. Alshehri, Peipei Chen, Ramit Debnath, and Vivek Dua",
        "link": "http://arxiv.org/abs/2510.14125v1",
        "abstract": "We introduce a neural network-driven robust optimisation framework that\nintegrates data-driven domain as a constraint into the nonlinear programming\ntechnique, addressing the overlooked issue of domain-inconsistent solutions\narising from the interaction of parametrised neural network models with\noptimisation solvers. Applied to a 1180 MW capacity combined cycle gas power\nplant, our framework delivers domain-consistent robust optimal solutions that\nachieve a verified 0.76 percentage point mean improvement in energy efficiency.\nFor the first time, scaling this efficiency gain to the global fleet of gas\npower plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with\n10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results\nunderscore the synergetic role of machine learning in delivering near-term,\nscalable decarbonisation pathways for global climate action."
    },
    {
        "date": "2025-10",
        "title": "Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments",
        "author": "Rajendra Upadhyay, Al Nahian Bin Emran, Rajendra Paudyal, Lisa Donnan, and Duminda Wijesekera",
        "link": "http://arxiv.org/abs/2510.14066v1",
        "abstract": "Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to\ncritical infrastructure and border protection by operating as rogue user\nequipment (UE) within cellular networks, consuming resources, creating\ninterference, and potentially violating restricted airspaces. This paper\npresents minimal features of the operating space, yet an end-to-end simulation\nframework to analyze detect-to-mitigate latency of such intrusions in a hybrid\nterrestrial-non-terrestrial (LEO satellite) 5G system. The system model\nincludes terrestrial gNBs, satellite backhaul (with stochastic outages), and a\ndetection logic (triggered by handover instability and signal quality\nvariance). A lockdown mechanism is invoked upon detection, with optional local\nfallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,\nspeeds, and satellite outage rates yield several insights. First, satellite\nbackhaul outages can cause arbitrarily long mitigation delays, yet, to meet\nfallback deadlines, they need to be effectively bounded. Second, while handover\ninstability was hypothesized, our results show that extra handovers have a\nnegligible effect within the range of parameters we considered. The main\nbenefit of resilience from fallback comes from the delay in limiting\nmitigation. Third, patrol UEs experience negligible collateral impact, with\nhandover rates close to terrestrial baselines. Stress scenarios further\nhighlight that fallback is indispensable in preventing extreme control-plane\nand physical security vulnerabilities: Without fallback, prolonged outages in\nthe satellite backhaul delay lockdown commands, allowing rogue UAVs to linger\ninside restricted corridors for several seconds longer. These results\nunderscore the importance of complementing non-terrestrial links with local\ncontrol to ensure robust and timely response against uncooperative UAV\nintrusions."
    },
    {
        "date": "2025-10",
        "title": "NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations",
        "author": "Junjie Nan, Jianing Li, Wei Chen, Mingkun Zhang, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2510.14025v1",
        "abstract": "Adversarial purification has achieved great success in combating adversarial\nimage perturbations, which are usually assumed to be additive. However,\nnon-additive adversarial perturbations such as blur, occlusion, and distortion\nare also common in the real world. Under such perturbations, existing\nadversarial purification methods are much less effective since they are\ndesigned to fit the additive nature. In this paper, we propose an extended\nadversarial purification framework named NAPPure, which can further handle\nnon-additive perturbations. Specifically, we first establish the generation\nprocess of an adversarial image, and then disentangle the underlying clean\nimage and perturbation parameters through likelihood maximization. Experiments\non GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the\nrobustness of image classification models against non-additive perturbations."
    },
    {
        "date": "2025-10",
        "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation",
        "author": "Abdulrahman Alhaidari, Balaji Palanisamy, and Prashant Krishnamurthy",
        "link": "http://arxiv.org/abs/2510.16024v1",
        "abstract": "Billions of dollars are lost every year in DeFi platforms by transactions\nexploiting business logic or accounting vulnerabilities. Existing defenses\nfocus on static code analysis, public mempool screening, attacker contract\ndetection, or trusted off-chain monitors, none of which prevents exploits\nsubmitted through private relays or malicious contracts that execute within the\nsame block. We present the first decentralized, fully on-chain learning\nframework that: (i) performs gas-prohibitive computation on Layer-2 to reduce\ncost, (ii) propagates verified model updates to Layer-1, and (iii) enables\ngas-bounded, low-latency inference inside smart contracts. A novel\nProof-of-Improvement (PoIm) protocol governs the training process and verifies\neach decentralized micro update as a self-verifying training transaction.\nUpdates are accepted by \\textit{PoIm} only if they demonstrably improve at\nleast one core metric (e.g., accuracy, F1-score, precision, or recall) on a\npublic benchmark without degrading any of the other core metrics, while\nadversarial proposals get financially penalized through an adaptable test set\nfor evolving threats. We develop quantization and loop-unrolling techniques\nthat enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs\n(with support for formally verified decision tree inference) within the\nEthereum block gas limit, while remaining bit-exact to their off-chain\ncounterparts, formally proven in Z3. We curate 298 unique real-world exploits\n(2020 - 2025) with 402 exploit transactions across eight EVM chains,\ncollectively responsible for \\$3.74 B in losses."
    },
    {
        "date": "2025-10",
        "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features",
        "author": "Wei Zou, Yupei Liu, Yanting Wang, Ying Chen, Neil Gong, and Jinyuan Jia",
        "link": "http://arxiv.org/abs/2510.14005v2",
        "abstract": "LLM-integrated applications are vulnerable to prompt injection attacks, where\nan attacker contaminates the input to inject malicious prompts, causing the LLM\nto follow the attacker's intent instead of the original user's. Existing prompt\ninjection detection methods often have sub-optimal performance and/or high\ncomputational overhead. In this work, we propose PIShield, a detection method\nthat is both effective and efficient. Our key observation is that the internal\nrepresentation of the final token in a prompt-extracted from a specific layer\nof the LLM, which we term the injection-critical layer-captures distinguishing\nfeatures between clean and contaminated prompts. Leveraging this insight, we\ntrain a simple linear classifier on these internal representations using a\nlabeled set of clean and contaminated prompts. We compare PIShield against 11\nbaselines across 5 diverse benchmark datasets and 8 prompt injection attacks.\nThe results demonstrate that PIShield is both highly effective and efficient,\nsubstantially outperforming existing methods. Additionally, we show that\nPIShield resists strong adaptive attacks."
    },
    {
        "date": "2025-10",
        "title": "Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach",
        "author": "Ziqing Lu, Lifeng Lai, and Weiyu Xu",
        "link": "http://arxiv.org/abs/2510.13792v1",
        "abstract": "Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged\nin many security-related applications, such as autonomous driving, financial\ndecisions, and drone/robot algorithms. In order to improve the\nrobustness/defense of RL systems against adversaries, studying various\nadversarial attacks on RL systems is very important. Most previous work\nconsidered deterministic adversarial attack strategies in MDP, which the\nrecipient (victim) agent can defeat by reversing the deterministic attacks. In\nthis paper, we propose a provably ``invincible'' or ``uncounterable'' type of\nadversarial attack on RL. The attackers apply a rate-distortion\ninformation-theoretic approach to randomly change agents' observations of the\ntransition kernel (or other properties) so that the agent gains zero or very\nlimited information about the ground-truth kernel (or other properties) during\nthe training. We derive an information-theoretic lower bound on the recipient\nagent's reward regret and show the impact of rate-distortion attacks on\nstate-of-the-art model-based and model-free algorithms. We also extend this\nnotion of an information-theoretic approach to other types of adversarial\nattack, such as state observation attacks."
    },
    {
        "date": "2025-10",
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations",
        "author": "Dominik J. M\u00fchlematter, Lin Che, Ye Hong, Martin Raubal, and Nina Wiedemann",
        "link": "http://arxiv.org/abs/2510.13774v1",
        "abstract": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion."
    },
    {
        "date": "2025-10",
        "title": "Local Information-Theoretic Security via Euclidean Geometry",
        "author": "Emmanouil M. Athanasakos, Nicholas Kalouptsidis, and Hariprasad Manjunath",
        "link": "http://arxiv.org/abs/2510.13661v1",
        "abstract": "This paper introduces a methodology based on Euclidean information theory to\ninvestigate local properties of secure communication over discrete memoryless\nwiretap channels. We formulate a constrained optimization problem that\nmaximizes a legitimate user's information rate while imposing explicit upper\nbounds on both the information leakage to an eavesdropper and the informational\ncost of encoding the secret message. By leveraging local geometric\napproximations, this inherently non-convex problem is transformed into a\ntractable quadratic programming structure. It is demonstrated that the optimal\nLagrange multipliers governing this approximated problem can be found by\nsolving a linear program. The constraints of this linear program are derived\nfrom Karush-Kuhn-Tucker conditions and are expressed in terms of the\ngeneralized eigenvalues of channel-derived matrices. This framework facilitates\nthe derivation of an analytical formula for an approximate local secrecy\ncapacity. Furthermore, we define and analyze a new class of secret local\ncontraction coefficients. These coefficients, characterized as the largest\ngeneralized eigenvalues of a matrix pencil, quantify the maximum achievable\nratio of approximate utility to approximate leakage, thus measuring the\nintrinsic local leakage efficiency of the channel. We establish bounds\nconnecting these local coefficients to their global counterparts defined over\ntrue mutual information measures. The efficacy of the proposed framework is\ndemonstrated through detailed analysis and numerical illustrations for both\ngeneral multi-mode channels and the canonical binary symmetric wiretap channel."
    },
    {
        "date": "2025-10",
        "title": "Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection",
        "author": "Akib Mohammed Khan, and Bartosz Krawczyk",
        "link": "http://arxiv.org/abs/2510.13643v1",
        "abstract": "Foundation models such as DINOv2 have shown strong performance in few-shot\nanomaly detection, yet two key questions remain unexamined: (i) how susceptible\nare these detectors to adversarial perturbations; and (ii) how well do their\nanomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a\ntraining-free deep nearest-neighbor detector over DINOv2 features, we present\none of the first systematic studies of adversarial attacks and uncertainty\nestimation in this setting. To enable white-box gradient attacks while\npreserving test-time behavior, we attach a lightweight linear head to frozen\nDINOv2 features only for crafting perturbations. Using this heuristic, we\nevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe\nconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible\nperturbations can flip nearest-neighbor relations in feature space to induce\nconfident misclassification. Complementing robustness, we probe reliability and\nfind that raw anomaly scores are poorly calibrated, revealing a gap between\nconfidence and correctness that limits safety-critical use. As a simple, strong\nbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly\nscores for uncertainty estimation. The resulting calibrated posteriors yield\nsignificantly higher predictive entropy on adversarially perturbed inputs than\non clean ones, enabling a practical flagging mechanism for attack detection\nwhile reducing calibration error (ECE). Our findings surface concrete\nvulnerabilities in DINOv2-based few-shot anomaly detectors and establish an\nevaluation protocol and baseline for robust, uncertainty-aware anomaly\ndetection. We argue that adversarial robustness and principled uncertainty\nquantification are not optional add-ons but essential capabilities if anomaly\ndetection systems are to be trustworthy and ready for real-world deployment."
    },
    {
        "date": "2025-10",
        "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
        "author": "Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, and Xipeng Qiu",
        "link": "http://arxiv.org/abs/2510.13626v1",
        "abstract": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity",
        "author": "Riccardo Santi, Riccardo Salami, and Simone Calderara",
        "link": "http://arxiv.org/abs/2510.13606v1",
        "abstract": "Nowdays, there are an abundance of portable devices capable of collecting\nlarge amounts of data and with decent computational power. This opened the\npossibility to train AI models in a distributed manner, preserving the\nparticipating clients' privacy. However, because of privacy regulations and\nsafety requirements, elimination upon necessity of a client contribution to the\nmodel has become mandatory. The cleansing process must satisfy specific\nefficacy and time requirements. In recent years, research efforts have produced\nseveral knowledge removal methods, but these require multiple communication\nrounds between the data holders and the process coordinator. This can cause the\nunavailability of an effective model up to the end of the removal process,\nwhich can result in a disservice to the system users. In this paper, we\nintroduce an innovative solution based on Task Arithmetic and the Neural\nTangent Kernel, to rapidly remove a client's influence from a model."
    },
    {
        "date": "2025-10",
        "title": "Selective Adversarial Attacks on LLM Benchmarks",
        "author": "Ivan Dubrovsky, Anastasia Orlova, Illarion Iov, Nina Gubina, Irena Gureeva, and Alexey Zaytsev",
        "link": "http://arxiv.org/abs/2510.13570v1",
        "abstract": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments."
    },
    {
        "date": "2025-10",
        "title": "Data-driven learning of feedback maps for explicit robust predictive control: an approximation theoretic view",
        "author": "Siddhartha Ganguly, Shubham Gupta, and Debasish Chatterjee",
        "link": "http://arxiv.org/abs/2510.13522v1",
        "abstract": "We establish an algorithm to learn feedback maps from data for a class of\nrobust model predictive control (MPC) problems. The algorithm accounts for the\napproximation errors due to the learning directly at the synthesis stage,\nensuring recursive feasibility by construction. The optimal control problem\nconsists of a linear noisy dynamical system, a quadratic stage and quadratic\nterminal costs as the objective, and convex constraints on the state, control,\nand disturbance sequences; the control minimizes and the disturbance maximizes\nthe objective. We proceed via two steps -- (a) Data generation: First, we\nreformulate the given minmax problem into a convex semi-infinite program and\nemploy recently developed tools to solve it in an exact fashion on grid points\nof the state space to generate (state, action) data. (b) Learning approximate\nfeedback maps: We employ a couple of approximation schemes that furnish tight\napproximations within preassigned uniform error bounds on the admissible state\nspace to learn the unknown feedback policy. The stability of the closed-loop\nsystem under the approximate feedback policies is also guaranteed under a\nstandard set of hypotheses. Two benchmark numerical examples are provided to\nillustrate the results."
    },
    {
        "date": "2025-10",
        "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
        "author": "Emily Miller, Michael Milford, Muhammad Burhan Hafez, SD Ramchurn, and Shoaib Ehsan",
        "link": "http://arxiv.org/abs/2510.13464v1",
        "abstract": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance."
    },
    {
        "date": "2025-10",
        "title": "Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts",
        "author": "Li Bai, Qingqing Ye, Xinwei Zhang, Sen Zhang, Zi Liang, Jianliang Xu, and Haibo Hu",
        "link": "http://arxiv.org/abs/2510.13451v1",
        "abstract": "Machine learning models are often vulnerable to inference attacks that expose\nsensitive information from their training data. Shadow model technique is\ncommonly employed in such attacks, such as membership inference. However, the\nneed for a large number of shadow models leads to high computational costs,\nlimiting their practical applicability. Such inefficiency mainly stems from the\nindependent training and use of these shadow models. To address this issue, we\npresent a novel shadow pool training framework SHAPOOL, which constructs\nmultiple shared models and trains them jointly within a single process. In\nparticular, we leverage the Mixture-of-Experts mechanism as the shadow pool to\ninterconnect individual models, enabling them to share some sub-networks and\nthereby improving efficiency. To ensure the shared models closely resemble\nindependent models and serve as effective substitutes, we introduce three novel\nmodules: path-choice routing, pathway regularization, and pathway alignment.\nThese modules guarantee random data allocation for pathway learning, promote\ndiversity among shared models, and maintain consistency with target models. We\nevaluate SHAPOOL in the context of various membership inference attacks and\nshow that it significantly reduces the computational cost of shadow model\nconstruction while maintaining comparable attack performance."
    },
    {
        "date": "2025-10",
        "title": "Robust Minimax Boosting with Performance Guarantees",
        "author": "Santiago Mazuelas, and Veronica Alvarez",
        "link": "http://arxiv.org/abs/2510.13445v1",
        "abstract": "Boosting methods often achieve excellent classification accuracy, but can\nexperience notable performance degradation in the presence of label noise.\nExisting robust methods for boosting provide theoretical robustness guarantees\nfor certain types of label noise, and can exhibit only moderate performance\ndegradation. However, previous theoretical results do not account for realistic\ntypes of noise and finite training sizes, and existing robust methods can\nprovide unsatisfactory accuracies, even without noise. This paper presents\nmethods for robust minimax boosting (RMBoost) that minimize worst-case error\nprobabilities and are robust to general types of label noise. In addition, we\nprovide finite-sample performance guarantees for RMBoost with respect to the\nerror obtained without noise and with respect to the best possible error (Bayes\nrisk). The experimental results corroborate that RMBoost is not only resilient\nto label noise but can also provide strong classification accuracy."
    },
    {
        "date": "2025-10",
        "title": "Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring",
        "author": "Yuxin Wang, Dennis Frauen, Jonas Schweisthal, Maresa Schr\u00f6der, and Stefan Feuerriegel",
        "link": "http://arxiv.org/abs/2510.13397v1",
        "abstract": "Dropout is common in clinical studies, with up to half of patients leaving\nearly due to side effects or other reasons. When dropout is informative (i.e.,\ndependent on survival time), it introduces censoring bias, because of which\ntreatment effect estimates are also biased. In this paper, we propose an\nassumption-lean framework to assess the robustness of conditional average\ntreatment effect (CATE) estimates in survival analysis when facing censoring\nbias. Unlike existing works that rely on strong assumptions, such as\nnon-informative censoring, to obtain point estimation, we use partial\nidentification to derive informative bounds on the CATE. Thereby, our framework\nhelps to identify patient subgroups where treatment is effective despite\ninformative censoring. We further develop a novel meta-learner that estimates\nthe bounds using arbitrary machine learning models and with favorable\ntheoretical properties, including double robustness and quasi-oracle\nefficiency. We demonstrate the practical value of our meta-learner through\nnumerical experiments and in an application to a cancer drug trial. Together,\nour framework offers a practical tool for assessing the robustness of estimated\ntreatment effects in the presence of censoring and thus promotes the reliable\nuse of survival data for evidence generation in medicine and epidemiology."
    },
    {
        "date": "2025-10",
        "title": "Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training",
        "author": "Yisen Wang, Yichuan Mo, Hongjun Wang, Junyi Li, and Zhouchen Lin",
        "link": "http://arxiv.org/abs/2510.13361v1",
        "abstract": "Despite the rapid progress of neural networks, they remain highly vulnerable\nto adversarial examples, for which adversarial training (AT) is currently the\nmost effective defense. While AT has been extensively studied, its practical\napplications expose two major limitations: natural accuracy tends to degrade\nsignificantly compared with standard training, and robustness does not transfer\nwell across attacks crafted under different norm constraints. Unlike prior\nworks that attempt to address only one issue within a single network, we\npropose to partition the overall generalization goal into multiple sub-tasks,\neach assigned to a dedicated base learner. By specializing in its designated\nobjective, each base learner quickly becomes an expert in its field. In the\nlater stages of training, we interpolate their parameters to form a\nknowledgeable global learner, while periodically redistributing the global\nparameters back to the base learners to prevent their optimization trajectories\nfrom drifting too far from the shared target. We term this framework Generalist\nand introduce three variants tailored to different application scenarios. Both\ntheoretical analysis and extensive experiments demonstrate that Generalist\nachieves lower generalization error and significantly alleviates the trade-off\nproblems compared with baseline methods. Our results suggest that Generalist\nprovides a promising step toward developing fully robust classifiers in the\nfuture."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control",
        "author": "Shingo Ayabe, Hiroshi Kera, and Kazuhiko Kawamoto",
        "link": "http://arxiv.org/abs/2510.13358v1",
        "abstract": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability."
    },
    {
        "date": "2025-10",
        "title": "Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems",
        "author": "Karthik Avinash, Nikhil Pareek, and Rishav Hada",
        "link": "http://arxiv.org/abs/2510.13351v1",
        "abstract": "The increasing deployment of Large Language Models (LLMs) across enterprise\nand mission-critical domains has underscored the urgent need for robust\nguardrailing systems that ensure safety, reliability, and compliance. Existing\nsolutions often struggle with real-time oversight, multi-modal data handling,\nand explainability -- limitations that hinder their adoption in regulated\nenvironments. Existing guardrails largely operate in isolation, focused on text\nalone making them inadequate for multi-modal, production-scale environments. We\nintroduce Protect, natively multi-modal guardrailing model designed to operate\nseamlessly across text, image, and audio inputs, designed for enterprise-grade\ndeployment. Protect integrates fine-tuned, category-specific adapters trained\nvia Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering\nfour safety dimensions: toxicity, sexism, data privacy, and prompt injection.\nOur teacher-assisted annotation pipeline leverages reasoning and explanation\ntraces to generate high-fidelity, context-aware labels across modalities.\nExperimental results demonstrate state-of-the-art performance across all safety\ndimensions, surpassing existing open and proprietary models such as WildGuard,\nLlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for\ntrustworthy, auditable, and production-ready safety systems capable of\noperating across text, image, and audio modalities."
    },
    {
        "date": "2025-10",
        "title": "Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning",
        "author": "Baogang Song, Dongdong Zhao, Jianwen Xiang, Qiben Xu, and Zizhuo Yu",
        "link": "http://arxiv.org/abs/2510.13322v1",
        "abstract": "Backdoor attacks pose a persistent security risk to deep neural networks\n(DNNs) due to their stealth and durability. While recent research has explored\nleveraging model unlearning mechanisms to enhance backdoor concealment,\nexisting attack strategies still leave persistent traces that may be detected\nthrough static analysis. In this work, we introduce the first paradigm of\nrevocable backdoor attacks, where the backdoor can be proactively and\nthoroughly removed after the attack objective is achieved. We formulate the\ntrigger optimization in revocable backdoor attacks as a bilevel optimization\nproblem: by simulating both backdoor injection and unlearning processes, the\ntrigger generator is optimized to achieve a high attack success rate (ASR)\nwhile ensuring that the backdoor can be easily erased through unlearning. To\nmitigate the optimization conflict between injection and removal objectives, we\nemploy a deterministic partition of poisoning and unlearning samples to reduce\nsampling-induced variance, and further apply the Projected Conflicting Gradient\n(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on\nCIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to\nstate-of-the-art backdoor attacks, while enabling effective removal of backdoor\nbehavior after unlearning. This work opens a new direction for backdoor attack\nresearch and presents new challenges for the security of machine learning\nsystems."
    },
    {
        "date": "2025-10",
        "title": "SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning",
        "author": "Weiqi Guo, Guanjun Liu, and Ziyuan Zhou",
        "link": "http://arxiv.org/abs/2510.13262v1",
        "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for\ncooperative and competitive tasks such as autonomous driving and strategic\ngaming. However, models trained by MADRL are vulnerable to adversarial\nperturbations on states and actions. Therefore, it is essential to investigate\nthe robustness of MADRL models from an attack perspective. Existing studies\nfocus on either state-only attacks or action-only attacks, but do not consider\nhow to effectively joint them. Simply combining state and action perturbations\nsuch as randomly perturbing states and actions does not exploit their potential\nsynergistic effects. In this paper, we propose the State-Action Joint Attack\n(SAJA) framework that has a good synergistic effects. SAJA consists of two\nimportant phases: (1) In the state attack phase, a multi-step gradient ascent\nmethod utilizes both the actor network and the critic network to compute an\nadversarial state, and (2) in the action attack phase, based on the perturbed\nstate, a second gradient ascent uses the critic network to craft the final\nadversarial action. Additionally, a heuristic regularizer measuring the\ndistance between the perturbed actions and the original clean ones is added\ninto the loss function to enhance the effectiveness of the critic's guidance.\nWe evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating\nthat (1) it outperforms and is more stealthy than state-only or action-only\nattacks, and (2) existing state or action defense methods cannot defend its\nattacks."
    },
    {
        "date": "2025-10",
        "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
        "author": "Haochuan Xu, Yun Sing Koh, Shuhuai Huang, Zirun Zhou, Di Wang, Jun Sakuma, and Jingfeng Zhang",
        "link": "http://arxiv.org/abs/2510.13237v1",
        "abstract": "Vision-Language-Action (VLA) models have achieved revolutionary progress in\nrobot learning, enabling robots to execute complex physical robot tasks from\nnatural language instructions. Despite this progress, their adversarial\nrobustness remains underexplored. In this work, we propose both adversarial\npatch attack and corresponding defense strategies for VLA models. We first\nintroduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic\nadversarial attack that generates patches directly placeable within the\ncamera's view. In comparison to prior methods, EDPA can be readily applied to\ndifferent VLA models without requiring prior knowledge of the model\narchitecture, or the controlled robotic manipulator. EDPA constructs these\npatches by (i) disrupting the semantic alignment between visual and textual\nlatent representations, and (ii) maximizing the discrepancy of latent\nrepresentations between adversarial and corresponding clean visual inputs.\nThrough the optimization of these objectives, EDPA distorts the VLA's\ninterpretation of visual information, causing the model to repeatedly generate\nincorrect actions and ultimately result in failure to complete the given\nrobotic task. To counter this, we propose an adversarial fine-tuning scheme for\nthe visual encoder, in which the encoder is optimized to produce similar latent\nrepresentations for both clean and adversarially perturbed visual inputs.\nExtensive evaluations on the widely recognized LIBERO robotic simulation\nbenchmark demonstrate that EDPA substantially increases the task failure rate\nof cutting-edge VLA models, while our proposed defense effectively mitigates\nthis degradation. The codebase is accessible via the homepage at\nhttps://edpa-attack.github.io/."
    },
    {
        "date": "2025-10",
        "title": "Searching for a Farang: Collective Security among Women in Pattaya, Thailand",
        "author": "Taylor Robinson, and Rikke Bjerg Jensen",
        "link": "http://arxiv.org/abs/2510.13162v1",
        "abstract": "We report on two months of ethnographic fieldwork in a women's centre in\nPattaya, and interviews with 76 participants. Our findings, as they relate to\ndigital security, show how (i) women in Pattaya, often working in the sex and\nmassage industries, perceived relationships with farang men as their best, and\nsometimes only, option to achieve security; (ii) the strategies used by the\nwomen to appeal to a farang involved presenting themselves online, mirroring\nhow they were being advertised by bar owners to attract customers; (iii)\nappealing to what they considered `Western ideals', the women sought out\n`Western technologies' and appropriated them for their benefit; (iv) the women\nnavigated a series of online security risks, such as scams and abuse, which\nshaped their search for a farang; (v) the women developed collective security\nthrough knowledge-sharing to protect themselves and each other in their search\nfor a farang. We situate our work in emerging digital security scholarship\nwithin marginalised contexts."
    },
    {
        "date": "2025-10",
        "title": "Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks",
        "author": "Tan Le, Van Le, and Sachin Shetty",
        "link": "http://arxiv.org/abs/2510.13136v1",
        "abstract": "Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly\nexposed to Denial of Service (DoS) attacks that compromise localization,\ncontrol and telemetry integrity. We propose a privacy-aware malware detection\nframework for indoor robotic systems, which leverages hybrid quantum computing\nand deep neural networks to counter DoS threats in CPS, while preserving\nprivacy information. By integrating quantum-enhanced feature encoding with\ndropout-optimized deep learning, our architecture achieves up to 95.2%\ndetection accuracy under privacy-constrained conditions. The system operates\nwithout handcrafted thresholds or persistent beacon data, enabling scalable\ndeployment in adversarial environments. Benchmarking reveals robust\ngeneralization, interpretability and resilience against training instability\nthrough modular circuit design. This work advances trustworthy AI for secure,\nautonomous CPS operations."
    },
    {
        "date": "2025-10",
        "title": "ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors",
        "author": "Nuntipat Narkthong, Yukui Luo, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2510.13111v1",
        "abstract": "The run-time electromagnetic (EM) emanation of microprocessors presents a\nside-channel that leaks the confidentiality of the applications running on\nthem. Many recent works have demonstrated successful attacks leveraging such\nside-channels to extract the confidentiality of diverse applications, such as\nthe key of cryptographic algorithms and the hyperparameter of neural network\nmodels. This paper proposes ShuffleV, a microarchitecture defense strategy\nagainst EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target\ndefense (MTD) philosophy, by integrating hardware units to randomly shuffle the\nexecution order of program instructions and optionally insert dummy\ninstructions, to nullify the statistical observation by attackers across\nrepetitive runs. We build ShuffleV on the open-source RISC-V core and provide\nsix design options, to suit different application scenarios. To enable rapid\nevaluation, we develop a ShuffleV simulator that can help users to (1) simulate\nthe performance overhead for each design option and (2) generate an execution\ntrace to validate the randomness of execution on their workload. We implement\nShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two\nrepresentative victim applications against EM SCAs, AES encryption, and neural\nnetwork inference. The experimental results demonstrate that ShuffleV can\nprovide automatic protection for these applications, without any user\nintervention or software modification."
    },
    {
        "date": "2025-10",
        "title": "CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing",
        "author": "Sikai Cheng, Reza Zandehshahvar, Haoruo Zhao, Daniel A. Garcia-Ulloa, Alejandro Villena-Rodriguez, Carles Navarro Manch\u00f3n, and Pascal Van Hentenryck",
        "link": "http://arxiv.org/abs/2510.12996v1",
        "abstract": "Channel state information (CSI) prediction is a promising strategy for\nensuring reliable and efficient operation of massive multiple-input\nmultiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While\ndeep learning-based methods have advanced beyond conventional model-driven and\nstatistical approaches, they remain limited in robustness to practical\nnon-Gaussian noise, generalization across diverse channel conditions, and\ncomputational efficiency. This paper introduces CSI-4CAST, a hybrid deep\nlearning architecture that integrates 4 key components, i.e., Convolutional\nneural network residuals, Adaptive correction layers, ShuffleNet blocks, and\nTransformers, to efficiently capture both local and long-range dependencies in\nCSI prediction. To enable rigorous evaluation, this work further presents a\ncomprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization\ntesting, which includes more than 300,000 samples across 3,060 realistic\nscenarios for both TDD and FDD systems. The dataset spans multiple channel\nmodels, a wide range of delay spreads and user velocities, and diverse noise\ntypes and intensity degrees. Experimental results show that CSI-4CAST achieves\nsuperior prediction accuracy with substantially lower computational cost,\noutperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,\nthe best performance among all evaluated models, while reducing FLOPs by 5x and\n3x compared to LLM4CP, the strongest baseline. In addition, evaluation over\nCSI-RRG provides valuable insights into how different channel factors affect\nthe performance and generalization capability of deep learning models. Both the\ndataset (https://huggingface.co/CSI-4CAST) and evaluation protocols\n(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a\nstandardized benchmark and to encourage further research on robust and\nefficient CSI prediction."
    },
    {
        "date": "2025-10",
        "title": "Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning",
        "author": "James Pedley, Benjamin Etheridge, Stephen J. Roberts, and Francesco Quinzan",
        "link": "http://arxiv.org/abs/2510.12939v1",
        "abstract": "Reinforcement learning (RL) policies deployed in real-world environments must\nremain reliable under adversarial perturbations. At the same time, modern deep\nRL agents are heavily over-parameterized, raising costs and fragility concerns.\nWhile pruning has been shown to improve robustness in supervised learning, its\nrole in adversarial RL remains poorly understood. We develop the first\ntheoretical framework for certified robustness under pruning in\nstate-adversarial Markov decision processes (SA-MDPs). For Gaussian and\ncategorical policies with Lipschitz networks, we prove that element-wise\npruning can only tighten certified robustness bounds; pruning never makes the\npolicy less robust. Building on this, we derive a novel three-term regret\ndecomposition that disentangles clean-task performance, pruning-induced\nperformance loss, and robustness gains, exposing a fundamental\nperformance--robustness frontier. Empirically, we evaluate magnitude and\nmicro-pruning schedules on continuous-control benchmarks with strong\npolicy-aware adversaries. Across tasks, pruning consistently uncovers\nreproducible ``sweet spots'' at moderate sparsity levels, where robustness\nimproves substantially without harming - and sometimes even enhancing - clean\nperformance. These results position pruning not merely as a compression tool\nbut as a structural intervention for robust RL."
    },
    {
        "date": "2025-10",
        "title": "Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering",
        "author": "Nil-Jana Akpinar, Chia-Jung Lee, Vanessa Murdock, and Pietro Perona",
        "link": "http://arxiv.org/abs/2510.12925v1",
        "abstract": "Large Language Models (LLMs) should answer factual questions truthfully,\ngrounded in objective knowledge, regardless of user context such as\nself-disclosed personal information, or system personalization. In this paper,\nwe present the first systematic evaluation of LLM robustness to inquiry\npersonas, i.e. user profiles that convey attributes like identity, expertise,\nor belief. While prior work has primarily focused on adversarial inputs or\ndistractors for robustness testing, we evaluate plausible, human-centered\ninquiry persona cues that users disclose in real-world interactions. We find\nthat such cues can meaningfully alter QA accuracy and trigger failure modes\nsuch as refusals, hallucinated limitations, and role confusion. These effects\nhighlight how model sensitivity to user framing can compromise factual\nreliability, and position inquiry persona testing as an effective tool for\nrobustness evaluation."
    },
    {
        "date": "2025-10",
        "title": "Robust Plant Disease Diagnosis with Few Target-Domain Samples",
        "author": "Takafumi Nogami, Satoshi Kagiwada, and Hitoshi Iyatomi",
        "link": "http://arxiv.org/abs/2510.12909v1",
        "abstract": "Various deep learning-based systems have been proposed for accurate and\nconvenient plant disease diagnosis, achieving impressive performance. However,\nrecent studies show that these systems often fail to maintain diagnostic\naccuracy on images captured under different conditions from the training\nenvironment -- an essential criterion for model robustness. Many deep learning\nmethods have shown high accuracy in plant disease diagnosis. However, they\noften struggle to generalize to images taken in conditions that differ from the\ntraining setting. This drop in performance stems from the subtle variability of\ndisease symptoms and domain gaps -- differences in image context and\nenvironment. The root cause is the limited diversity of training data relative\nto task complexity, making even advanced models vulnerable in unseen domains.\nTo tackle this challenge, we propose a simple yet highly adaptable learning\nframework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),\ngrounded in metric learning. TMPS operates under the assumption of access to a\nlimited number of labeled samples from the target (deployment) domain and\nleverages these samples effectively to improve diagnostic robustness. We assess\nTMPS on a large-scale automated plant disease diagnostic task using a dataset\ncomprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21\ndiseases and healthy instances across three crop species. By incorporating just\n10 target domain samples per disease into training, TMPS surpasses models\ntrained using the same combined source and target samples, and those fine-tuned\nwith these target samples after pre-training on source data. It achieves\naverage macro F1 score improvements of 7.3 and 3.6 points, respectively, and a\nremarkable 18.7 and 17.1 point improvement over the baseline and conventional\nmetric learning."
    },
    {
        "date": "2025-10",
        "title": "KoALA: KL-L0 Adversarial Detector via Label Agreement",
        "author": "Siqi Li, and Yasser Shoukry",
        "link": "http://arxiv.org/abs/2510.12752v1",
        "abstract": "Deep neural networks are highly susceptible to adversarial attacks, which\npose significant risks to security- and safety-critical applications. We\npresent KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,\nsemantics-free adversarial detector that requires no architectural changes or\nadversarial retraining. KoALA operates on a simple principle: it detects an\nadversarial attack when class predictions from two complementary similarity\nmetrics disagree. These metrics-KL divergence and an L0-based similarity-are\nspecifically chosen to detect different types of perturbations. The KL\ndivergence metric is sensitive to dense, low-amplitude shifts, while the\nL0-based similarity is designed for sparse, high-impact changes. We provide a\nformal proof of correctness for our approach. The only training required is a\nsimple fine-tuning step on a pre-trained image encoder using clean images to\nensure the embeddings align well with both metrics. This makes KOALA a\nlightweight, plug-and-play solution for existing models and various data\nmodalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet\nconfirm our theoretical claims. When the theorem's conditions are met, KoALA\nconsistently and effectively detects adversarial examples. On the full test\nsets, KoALA achieves a precision of 0.94 and a recall of 0.81 on\nResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on\nCLIP/Tiny-ImageNet."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection",
        "author": "Wissam Salhab, Darine Ameyed, Hamid Mcheick, and Fehmi Jaafar",
        "link": "http://arxiv.org/abs/2510.12713v1",
        "abstract": "Robustness in AI systems refers to their ability to maintain reliable and\naccurate performance under various conditions, including out-of-distribution\n(OOD) samples, adversarial attacks, and environmental changes. This is crucial\nin safety-critical systems, such as autonomous vehicles, transportation, or\nhealthcare, where malfunctions could have severe consequences. This paper\nproposes an approach to improve OOD detection without the need of labeled data,\nthereby increasing the AI systems' robustness. The proposed approach leverages\nthe principles of self-supervised learning, allowing the model to learn useful\nrepresentations from unlabeled data. Combined with graph-theoretical\ntechniques, this enables the more efficient identification and categorization\nof OOD samples. Compared to existing state-of-the-art methods, this approach\nachieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =\n0.99."
    },
    {
        "date": "2025-10",
        "title": "Hash chaining degrades security at Facebook",
        "author": "Thomas Rivasseau",
        "link": "http://arxiv.org/abs/2510.12665v1",
        "abstract": "Modern web and digital application password storage relies on password\nhashing for storage and security. Ad-hoc upgrade of password storage to keep up\nwith hash algorithm norms may be used to save costs but can introduce\nunforeseen vulnerabilities. This is the case in the password storage scheme\nused by Meta Platforms which services several billion monthly users worldwide.\nIn this paper we present the first example of an exploit which demonstrates the\nsecurity weakness of Facebook's password storage scheme, and discuss its\nimplications. Proper ethical disclosure guidelines and vendor notification were\nfollowed."
    },
    {
        "date": "2025-10",
        "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds",
        "author": "Gunwoo Kim, Taejune Park, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2510.12629v1",
        "abstract": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
    },
    {
        "date": "2025-10",
        "title": "Multi-Copy Security in Unclonable Cryptography",
        "author": "Alper \u00c7akan, Vipul Goyal, Fuyuki Kitagawa, Ryo Nishimaki, and Takashi Yamakawa",
        "link": "http://arxiv.org/abs/2510.12626v1",
        "abstract": "Unclonable cryptography leverages the quantum no-cloning principle to\ncopy-protect cryptographic functionalities. While most existing works address\nthe basic single-copy security, the stronger notion of multi-copy security\nremains largely unexplored.\n  We introduce a generic compiler that upgrades collusion-resistant unclonable\nprimitives to achieve multi-copy security, assuming only one-way functions.\nUsing this framework, we obtain the first multi-copy secure constructions of\npublic-key quantum money (termed quantum coins), single-decryptor encryption,\nunclonable encryption, and more. We also introduce an extended notion of\nquantum coins, called upgradable quantum coins, which allow weak\n(almost-public) verification under weaker assumptions and can be upgraded to\nfull public verification under stronger assumptions by the bank simply\npublishing additional classical information.\n  Along the way, we give a generic compiler that upgrades single-copy secure\nsingle-decryptor encryption to a collusion-resistant one, assuming the\nexistence of functional encryption, and construct the first multi-challenge\nsecure unclonable encryption scheme, which we believe are of independent\ninterest."
    },
    {
        "date": "2025-10",
        "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
        "author": "Siyuan Li, Aodu Wulianghai, Xi Lin, Guangyan Li, Xiang Chen, Jun Wu, and Jianhua Li",
        "link": "http://arxiv.org/abs/2510.12608v1",
        "abstract": "With the increasing integration of large language models (LLMs) into\nopen-domain writing, detecting machine-generated text has become a critical\ntask for ensuring content authenticity and trust. Existing approaches rely on\nstatistical discrepancies or model-specific heuristics to distinguish between\nLLM-generated and human-written text. However, these methods struggle in\nreal-world scenarios due to limited generalization, vulnerability to\nparaphrasing, and lack of explainability, particularly when facing stylistic\ndiversity or hybrid human-AI authorship. In this work, we propose\nStyleDecipher, a robust and explainable detection framework that revisits\nLLM-generated text detection using combined feature extractors to quantify\nstylistic differences. By jointly modeling discrete stylistic indicators and\ncontinuous stylistic representations derived from semantic embeddings,\nStyleDecipher captures distinctive style-level divergences between human and\nLLM outputs within a unified representation space. This framework enables\naccurate, explainable, and domain-agnostic detection without requiring access\nto model internals or labeled segments. Extensive experiments across five\ndiverse domains, including news, code, essays, reviews, and academic abstracts,\ndemonstrate that StyleDecipher consistently achieves state-of-the-art in-domain\naccuracy. Moreover, in cross-domain evaluations, it surpasses existing\nbaselines by up to 36.30%, while maintaining robustness against adversarial\nperturbations and mixed human-AI content. Further qualitative and quantitative\nanalysis confirms that stylistic signals provide explainable evidence for\ndistinguishing machine-generated text. Our source code can be accessed at\nhttps://github.com/SiyuanLi00/StyleDecipher."
    },
    {
        "date": "2025-10",
        "title": "Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers",
        "author": "Giacomo Bertollo, Naz Bodemir, and Jonah Burgess",
        "link": "http://arxiv.org/abs/2510.16005v1",
        "abstract": "Analyzing 500 CTF participants, this paper shows that while participants\nreadily bypassed simple AI guardrails using common techniques, layered\nmulti-step defenses still posed significant challenges, offering concrete\ninsights for building safer AI systems."
    },
    {
        "date": "2025-10",
        "title": "The Robustness of Differentiable Causal Discovery in Misspecified Scenarios",
        "author": "Huiyang Yi, Yanyan He, Duxin Chen, Mingyu Kang, He Wang, and Wenwu Yu",
        "link": "http://arxiv.org/abs/2510.12503v1",
        "abstract": "Causal discovery aims to learn causal relationships between variables from\ntargeted data, making it a fundamental task in machine learning. However,\ncausal discovery algorithms often rely on unverifiable causal assumptions,\nwhich are usually difficult to satisfy in real-world data, thereby limiting the\nbroad application of causal discovery in practical scenarios. Inspired by these\nconsiderations, this work extensively benchmarks the empirical performance of\nvarious mainstream causal discovery algorithms, which assume i.i.d. data, under\neight model assumption violations. Our experimental results show that\ndifferentiable causal discovery methods exhibit robustness under the metrics of\nStructural Hamming Distance and Structural Intervention Distance of the\ninferred graphs in commonly used challenging scenarios, except for scale\nvariation. We also provide the theoretical explanations for the performance of\ndifferentiable causal discovery methods. Finally, our work aims to\ncomprehensively benchmark the performance of recent differentiable causal\ndiscovery methods under model assumption violations, and provide the standard\nfor reasonable evaluation of causal discovery, as well as to further promote\nits application in real-world scenarios."
    },
    {
        "date": "2025-10",
        "title": "MS-GAGA: Metric-Selective Guided Adversarial Generation Attack",
        "author": "Dion J. X. Ho, Gabriel Lee Jun Rong, Niharika Shrivastava, Harshavardhan Abichandani, Pai Chet Ng, and Xiaoxiao Miao",
        "link": "http://arxiv.org/abs/2510.12468v1",
        "abstract": "We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a\ntwo-stage framework for crafting transferable and visually imperceptible\nadversarial examples against deepfake detectors in black-box settings. In Stage\n1, a dual-stream attack module generates adversarial candidates: MNTD-PGD\napplies enhanced gradient calculations optimized for small perturbation\nbudgets, while SG-PGD focuses perturbations on visually salient regions. This\ncomplementary design expands the adversarial search space and improves\ntransferability across unseen models. In Stage 2, a metric-aware selection\nmodule evaluates candidates based on both their success against black-box\nmodels and their structural similarity (SSIM) to the original image. By jointly\noptimizing transferability and imperceptibility, MS-GAGA achieves up to 27%\nhigher misclassification rates on unseen detectors compared to state-of-the-art\nattacks."
    },
    {
        "date": "2025-10",
        "title": "Attack-Specialized Deep Learning with Ensemble Fusion for Network Anomaly Detection",
        "author": "Nisith Dissanayake, and Uthayasanker Thayasivam",
        "link": "http://arxiv.org/abs/2510.12455v1",
        "abstract": "The growing scale and sophistication of cyberattacks pose critical challenges\nto network security, particularly in detecting diverse intrusion types within\nimbalanced datasets. Traditional intrusion detection systems (IDS) often\nstruggle to maintain high accuracy across both frequent and rare attacks,\nleading to increased false negatives for minority classes. To address this, we\npropose a hybrid anomaly detection framework that integrates specialized deep\nlearning models with an ensemble meta-classifier. Each model is trained to\ndetect a specific attack category, enabling tailored learning of class-specific\npatterns, while their collective outputs are fused by a Random Forest\nmeta-classifier to improve overall decision reliability. The framework is\nevaluated on the NSL-KDD benchmark, demonstrating superior performance in\nhandling class imbalance compared to conventional monolithic models. Results\nshow significant improvements in precision, recall, and F1-score across all\nattack categories, including rare classes such as User to Root (U2R). The\nproposed system achieves near-perfect detection rates with minimal false\nalarms, highlighting its robustness and generalizability. This work advances\nthe design of intrusion detection systems by combining specialization with\nensemble learning, providing an effective and scalable solution for\nsafeguarding modern networks."
    },
    {
        "date": "2025-10",
        "title": "Formal Models and Convergence Analysis for Context-Aware Security Verification",
        "author": "Ayush Chaudhary",
        "link": "http://arxiv.org/abs/2510.12440v1",
        "abstract": "We present a formal framework for context-aware security verification that\nestablishes provable guarantees for ML-enhanced adaptive systems. We introduce\ncontext-completeness - a new security property - and prove: (1) sample\ncomplexity bounds showing when adaptive verification succeeds, (2)\ninformation-theoretic limits relating context richness to detection capability,\n(3) convergence guarantees for ML-based payload generators, and (4)\ncompositional soundness bounds. We further provide a formal separation between\nstatic context-blind verifiers and context-aware adaptive verifiers: for a\nnatural family of targets, any static verifier with finite payload budget\nachieves completeness at most alpha, while a context-aware verifier with\nsufficient information achieves completeness greater than alpha. We validate\nour theoretical predictions through controlled experiments on 97,224 exploit\nsamples, demonstrating: detection accuracy improving from 58% to 69.93% with\ndataset growth, success probability increasing from 51% to 82% with context\nenrichment, training loss converging at O(1/sqrt(T)) rate, and false positive\nrate (10.19%) within theoretical bounds (12%). Our results show that\ntheoretically-grounded adaptive verification achieves provable improvements\nover static approaches under stated assumptions while maintaining soundness\nguarantees."
    },
    {
        "date": "2025-10",
        "title": "DeepTrust: Multi-Step Classification through Dissimilar Adversarial Representations for Robust Android Malware Detection",
        "author": "Daniel Pulido-Cort\u00e1zar, Daniel Gibert, and Felip Many\u00e0",
        "link": "http://arxiv.org/abs/2510.12310v1",
        "abstract": "Over the last decade, machine learning has been extensively applied to\nidentify malicious Android applications. However, such approaches remain\nvulnerable against adversarial examples, i.e., examples that are subtly\nmanipulated to fool a machine learning model into making incorrect predictions.\nThis research presents DeepTrust, a novel metaheuristic that arranges flexible\nclassifiers, like deep neural networks, into an ordered sequence where the\nfinal decision is made by a single internal model based on conditions activated\nin cascade. In the Robust Android Malware Detection competition at the 2025\nIEEE Conference SaTML, DeepTrust secured the first place and achieved\nstate-of-the-art results, outperforming the next-best competitor by up to 266%\nunder feature-space evasion attacks. This is accomplished while maintaining the\nhighest detection rate on non-adversarial malware and a false positive rate\nbelow 1%. The method's efficacy stems from maximizing the divergence of the\nlearned representations among the internal models. By using classifiers\ninducing fundamentally dissimilar embeddings of the data, the decision space\nbecomes unpredictable for an attacker. This frustrates the iterative\nperturbation process inherent to evasion attacks, enhancing system robustness\nwithout compromising accuracy on clean examples."
    },
    {
        "date": "2025-10",
        "title": "Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs",
        "author": "Blazej Manczak, Eric Lin, Francisco Eiras, James O' Neill, and Vaikkunth Mugunthan",
        "link": "http://arxiv.org/abs/2510.12255v1",
        "abstract": "Large language models (LLMs) are rapidly transitioning into medical clinical\nuse, yet their reliability under realistic, multi-turn interactions remains\npoorly understood. Existing evaluation frameworks typically assess single-turn\nquestion answering under idealized conditions, overlooking the complexities of\nmedical consultations where conflicting input, misleading context, and\nauthority influence are common. We introduce MedQA-Followup, a framework for\nsystematically evaluating multi-turn robustness in medical question answering.\nOur approach distinguishes between shallow robustness (resisting misleading\ninitial context) and deep robustness (maintaining accuracy when answers are\nchallenged across turns), while also introducing an indirect-direct axis that\nseparates contextual framing (indirect) from explicit suggestion (direct).\nUsing controlled interventions on the MedQA dataset, we evaluate five\nstate-of-the-art LLMs and find that while models perform reasonably well under\nshallow perturbations, they exhibit severe vulnerabilities in multi-turn\nsettings, with accuracy dropping from 91.2% to as low as 13.5% for Claude\nSonnet 4. Counterintuitively, indirect, context-based interventions are often\nmore harmful than direct suggestions, yielding larger accuracy drops across\nmodels and exposing a significant vulnerability for clinical deployment.\nFurther compounding analyses reveal model differences, with some showing\nadditional performance drops under repeated interventions while others\npartially recovering or even improving. These findings highlight multi-turn\nrobustness as a critical but underexplored dimension for safe and reliable\ndeployment of medical LLMs."
    },
    {
        "date": "2025-10",
        "title": "PromptLocate: Localizing Prompt Injection Attacks",
        "author": "Yuqi Jia, Yupei Liu, Zedian Shao, Jinyuan Jia, and Neil Gong",
        "link": "http://arxiv.org/abs/2510.12252v2",
        "abstract": "Prompt injection attacks deceive a large language model into completing an\nattacker-specified task instead of its intended task by contaminating its input\ndata with an injected prompt, which consists of injected instruction(s) and\ndata. Localizing the injected prompt within contaminated data is crucial for\npost-attack forensic analysis and data recovery. Despite its growing\nimportance, prompt injection localization remains largely unexplored. In this\nwork, we bridge this gap by proposing PromptLocate, the first method for\nlocalizing injected prompts. PromptLocate comprises three steps: (1) splitting\nthe contaminated data into semantically coherent segments, (2) identifying\nsegments contaminated by injected instructions, and (3) pinpointing segments\ncontaminated by injected data. We show PromptLocate accurately localizes\ninjected prompts across eight existing and eight adaptive attacks."
    },
    {
        "date": "2025-10",
        "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
        "author": "Dongsen Zhang, Zekun Li, Xu Luo, Xuannan Liu, Peipei Li, and Wenjun Xu",
        "link": "http://arxiv.org/abs/2510.15994v1",
        "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM)\nagents discover, describe, and call external tools. While MCP unlocks broad\ninteroperability, it also enlarges the attack surface by making tools\nfirst-class, composable objects with natural-language metadata, and\nstandardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end\nevaluation suite that systematically measures how well LLM agents resist\nMCP-specific attacks throughout the full tool-use pipeline: task planning, tool\ninvocation, and response handling. MSB contributes: (1) a taxonomy of 12\nattacks including name-collision, preference manipulation, prompt injections\nembedded in tool descriptions, out-of-scope parameter requests,\nuser-impersonating responses, false-error escalation, tool-transfer, retrieval\ninjection, and mixed attacks; (2) an evaluation harness that executes attacks\nby running real tools (both benign and malicious) via MCP rather than\nsimulation; and (3) a robustness metric that quantifies the trade-off between\nsecurity and performance: Net Resilient Performance (NRP). We evaluate nine\npopular LLM agents across 10 domains and 400+ tools, producing 2,000 attack\ninstances. Results reveal the effectiveness of attacks against each stage of\nMCP. Models with stronger performance are more vulnerable to attacks due to\ntheir outstanding tool calling and instruction following capabilities. MSB\nprovides a practical baseline for researchers and practitioners to study,\ncompare, and harden MCP agents."
    },
    {
        "date": "2025-10",
        "title": "Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs",
        "author": "Bowen Fan, Zhilin Guo, Xunkai Li, Yihan Zhou, Bing Zhou, Zhenjun Li, Rong-Hua Li, and Guoren Wang",
        "link": "http://arxiv.org/abs/2510.12233v1",
        "abstract": "Graph Neural Networks (GNNs) have become a pivotal framework for modeling\ngraph-structured data, enabling a wide range of applications from social\nnetwork analysis to molecular chemistry. By integrating large language models\n(LLMs), text-attributed graphs (TAGs) enhance node representations with rich\ntextual semantics, significantly boosting the expressive power of graph-based\nlearning. However, this sophisticated synergy introduces critical\nvulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both\ntheir structural topology and textual attributes. Although specialized attack\nmethods have been designed for each of these aspects, no work has yet unified\nthem into a comprehensive approach. In this work, we propose the Interpretable\nMulti-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial\nattack framework designed to orchestrate multi-level perturbations across both\ngraph structure and textual features. IMDGA utilizes three tightly integrated\nmodules to craft attacks that balance interpretability and impact, enabling a\ndeeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical\nanalysis and comprehensive empirical evaluations on diverse datasets and\narchitectures, IMDGA demonstrates superior interpretability, attack\neffectiveness, stealthiness, and robustness compared to existing methods. By\nexposing critical weaknesses in TAG representation learning, this work uncovers\na previously underexplored semantic dimension of vulnerability in Graph-LLMs,\noffering valuable insights for improving their resilience. Our code and\nresources are publicly available at\nhttps://anonymous.4open.science/r/IMDGA-7289."
    },
    {
        "date": "2025-10",
        "title": "Leaking Queries On Secure Stream Processing Systems",
        "author": "Hung Pham, Viet Vo, Tien Tuan Anh Dinh, Duc Tran, and Shuhao Zhang",
        "link": "http://arxiv.org/abs/2510.12172v2",
        "abstract": "Stream processing systems are important in modern applications in which data\narrive continuously and need to be processed in real time. Because of their\nresource and scalability requirements, many of these systems run on the cloud,\nwhich is considered untrusted. Existing works on securing databases on the\ncloud focus on protecting the data, and most systems leverage trusted hardware\nfor high performance. However, in stream processing systems, queries are as\nsensitive as the data because they contain the application logics.\n  We demonstrate that it is practical to extract the queries from stream\nprocessing systems that use Intel SGX for securing the execution engine. The\nattack performed by a malicious cloud provider is based on timing side\nchannels, and it works in two phases. In the offline phase, the attacker\nprofiles the execution time of individual stream operators, based on synthetic\ndata. This phase outputs a model that identifies individual stream operators.\nIn the online phase, the attacker isolates the operators that make up the\nquery, monitors its execution, and recovers the operators using the model in\nthe previous phase. We implement the attack based on popular data stream\nbenchmarks using SecureStream and NEXMark, and demonstrate attack success rates\nof up to 92%. We further discuss approaches that can harden streaming\nprocessing systems against our attacks without incurring high overhead."
    },
    {
        "date": "2025-10",
        "title": "Fairness-Constrained Optimization Attack in Federated Learning",
        "author": "Harsh Kasyap, Minghong Fang, Zhuqing Liu, Carsten Maple, and Somanath Tripathy",
        "link": "http://arxiv.org/abs/2510.12143v1",
        "abstract": "Federated learning (FL) is a privacy-preserving machine learning technique\nthat facilitates collaboration among participants across demographics. FL\nenables model sharing, while restricting the movement of data. Since FL\nprovides participants with independence over their training data, it becomes\nsusceptible to poisoning attacks. Such collaboration also propagates bias among\nthe participants, even unintentionally, due to different data distribution or\nhistorical bias present in the data. This paper proposes an intentional\nfairness attack, where a client maliciously sends a biased model, by increasing\nthe fairness loss while training, even considering homogeneous data\ndistribution. The fairness loss is calculated by solving an optimization\nproblem for fairness metrics such as demographic parity and equalized odds. The\nattack is insidious and hard to detect, as it maintains global accuracy even\nafter increasing the bias. We evaluate our attack against the state-of-the-art\nByzantine-robust and fairness-aware aggregation schemes over different\ndatasets, in various settings. The empirical results demonstrate the attack\nefficacy by increasing the bias up to 90\\%, even in the presence of a single\nmalicious client in the FL system."
    },
    {
        "date": "2025-10",
        "title": "Locket: Robust Feature-Locking Technique for Language Models",
        "author": "Lipeng He, Vasisht Duddu, and N. Asokan",
        "link": "http://arxiv.org/abs/2510.12117v1",
        "abstract": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to\ngenerate revenue, offering basic models for free users, and advanced models for\npaying subscribers. However, a finer-grained pay-to-unlock scheme for premium\nfeatures (e.g., math, coding) is thought to be more economically viable for the\nproviders. Such a scheme requires a feature-locking technique (FLoTE) which is\n(i) effective in refusing locked features, (ii) utility-preserving for unlocked\nfeatures, (iii) robust against evasion or unauthorized credential sharing, and\n(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,\npassword-locked models) are not robust or scalable. We present Locket, the\nfirst robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a\nnovel merging approach to attach adapters to an LLM for refusing unauthorized\nfeatures. Our comprehensive evaluation shows that Locket is effective ($100$%\nrefusal on locked features), utility-preserving ($\\leq 7$% utility degradation\nin unlocked features), robust ($\\leq 5$% attack success rate), and scales to\nmultiple features and clients."
    },
    {
        "date": "2025-10",
        "title": "Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU",
        "author": "Weixuan Li, Guang Yu, Quanjun Li, Junhua Zhou, Jiajun Chen, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, Lin Tang, and Xuhang Chen",
        "link": "http://arxiv.org/abs/2510.12084v1",
        "abstract": "Chaotic systems play a key role in modern image encryption due to their\nsensitivity to initial conditions, ergodicity, and complex dynamics. However,\nmany existing chaos-based encryption methods suffer from vulnerabilities, such\nas inadequate permutation and diffusion, and suboptimal pseudorandom\nproperties. This paper presents Kun-IE, a novel encryption framework designed\nto address these issues. The framework features two key contributions: the\ndevelopment of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a\nbroader chaotic range and superior pseudorandom sequence generation, and the\nintroduction of Kun-SCAN, a novel permutation strategy that significantly\nreduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE\nis flexible and supports encryption for images of any size. Experimental\nresults and security analyses demonstrate its robustness against various\ncryptanalytic attacks, making it a strong solution for secure image\ncommunication. The code is available at this\n\\href{https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU}{link}."
    },
    {
        "date": "2025-10",
        "title": "A Review on Domain Adaption and Generative Adversarial Networks(GANs)",
        "author": "Aashish Dhawan, and Divyanshu Mudgal",
        "link": "http://arxiv.org/abs/2510.12075v1",
        "abstract": "The major challenge in today's computer vision scenario is the availability\nof good quality labeled data. In a field of study like image classification,\nwhere data is of utmost importance, we need to find more reliable methods which\ncan overcome the scarcity of data to produce results comparable to previous\nbenchmark results. In most cases, obtaining labeled data is very difficult\nbecause of the high cost of human labor and in some cases impossible. The\npurpose of this paper is to discuss Domain Adaptation and various methods to\nimplement it. The main idea is to use a model trained on a particular dataset\nto predict on data from a different domain of the same kind, for example - a\nmodel trained on paintings of airplanes predicting on real images of airplanes"
    },
    {
        "date": "2025-10",
        "title": "Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce Applications",
        "author": "Urvashi Kishnani, and Sanchari Das",
        "link": "http://arxiv.org/abs/2510.12031v1",
        "abstract": "E-commerce mobile applications are central to global financial transactions,\nmaking their security and privacy crucial. In this study, we analyze 92\ntop-grossing Android e-commerce apps (58 U.S.-based and 34 international) using\nMobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and\ncertificate weaknesses, with approximately 92% using unsecured HTTP connections\nand an average MobSF security score of 40.92/100. Over-privileged permissions\nwere identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and\ncertificate vulnerabilities, both groups showed similar network-related issues.\nWe advocate for the adoption of stronger, standardized, and user-focused\nsecurity practices across regions."
    },
    {
        "date": "2025-10",
        "title": "Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks",
        "author": "Ansh Tiwari, and Ayush Chauhan",
        "link": "http://arxiv.org/abs/2510.12843v1",
        "abstract": "Spiking neural networks (SNNs) promise energy-efficient artificial\nintelligence on neuromorphic hardware but struggle with tasks requiring both\nfast adaptation and long-term memory, especially in continual learning. We\npropose Local Timescale Gating (LT-Gate), a neuron model that combines dual\ntime-constant dynamics with an adaptive gating mechanism. Each spiking neuron\ntracks information on a fast and a slow timescale in parallel, and a learned\ngate locally adjusts their influence. This design enables individual neurons to\npreserve slow contextual information while responding to fast signals,\naddressing the stability-plasticity dilemma. We further introduce a\nvariance-tracking regularization that stabilizes firing activity, inspired by\nbiological homeostasis. Empirically, LT-Gate yields significantly improved\naccuracy and retention in sequential learning tasks: on a challenging temporal\nclassification benchmark it achieves about 51 percent final accuracy, compared\nto about 46 percent for a recent Hebbian continual-learning baseline and lower\nfor prior SNN methods. Unlike approaches that require external replay or\nexpensive orthogonalizations, LT-Gate operates with local updates and is fully\ncompatible with neuromorphic hardware. In particular, it leverages features of\nIntel's Loihi chip (multiple synaptic traces with different decay rates) for\non-chip learning. Our results demonstrate that multi-timescale gating can\nsubstantially enhance continual learning in SNNs, narrowing the gap between\nspiking and conventional deep networks on lifelong-learning tasks."
    },
    {
        "date": "2025-10",
        "title": "Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing",
        "author": "Deeksha Hareesha Kulal, Chidozie Princewill Arannonu, Afsah Anwar, Nidhi Rastogi, and Quamar Niyaz",
        "link": "http://arxiv.org/abs/2510.11915v1",
        "abstract": "Phishing remains a critical cybersecurity threat, especially with the advent\nof large language models (LLMs) capable of generating highly convincing\nmalicious content. Unlike earlier phishing attempts which are identifiable by\ngrammatical errors, misspellings, incorrect phrasing, and inconsistent\nformatting, LLM generated emails are grammatically sound, contextually\nrelevant, and linguistically natural. These advancements make phishing emails\nincreasingly difficult to distinguish from legitimate ones, challenging\ntraditional detection mechanisms. Conventional phishing detection systems often\nfail when faced with emails crafted by LLMs or manipulated using adversarial\nperturbation techniques. To address this challenge, we propose a robust\nphishing email detection system featuring an enhanced text preprocessing\npipeline. This pipeline includes spelling correction and word splitting to\ncounteract adversarial modifications and improve detection accuracy. Our\napproach integrates widely adopted natural language processing (NLP) feature\nextraction techniques and machine learning algorithms. We evaluate our models\non publicly available datasets comprising both phishing and legitimate emails,\nachieving a detection accuracy of 94.26% and F1-score of 84.39% in model\ndeployment setting. To assess robustness, we further evaluate our models using\nadversarial phishing samples generated by four attack methods in Python\nTextAttack framework. Additionally, we evaluate models' performance against\nphishing emails generated by LLMs including ChatGPT and Llama. Results\nhighlight the resilience of models against evolving AI-powered phishing\nthreats."
    },
    {
        "date": "2025-10",
        "title": "ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty",
        "author": "Chenliang Li, Junyu Leng, Jiaxiang Li, Youbang Sun, Shixiang Chen, Shahin Shahrampour, and Alfredo Garcia",
        "link": "http://arxiv.org/abs/2510.11899v1",
        "abstract": "Robust reinforcement learning (Robust RL) seeks to handle epistemic\nuncertainty in environment dynamics, but existing approaches often rely on\nnested min--max optimization, which is computationally expensive and yields\noverly conservative policies. We propose \\textbf{Adaptive Rank Representation\n(AdaRL)}, a bi-level optimization framework that improves robustness by\naligning policy complexity with the intrinsic dimension of the task. At the\nlower level, AdaRL performs policy optimization under fixed-rank constraints\nwith dynamics sampled from a Wasserstein ball around a centroid model. At the\nupper level, it adaptively adjusts the rank to balance the bias--variance\ntrade-off, projecting policy parameters onto a low-rank manifold. This design\navoids solving adversarial worst-case dynamics while ensuring robustness\nwithout over-parameterization. Empirical results on MuJoCo continuous control\nbenchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank\nbaselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,\nParseval), but also converges toward the intrinsic rank of the underlying\ntasks. These results highlight that adaptive low-rank policy representations\nprovide an efficient and principled alternative for robust RL under model\nuncertainty."
    },
    {
        "date": "2025-10",
        "title": "Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling",
        "author": "Xiaohang Tang, Zhuowen Cheng, and Satyabrat Kumar",
        "link": "http://arxiv.org/abs/2510.11877v1",
        "abstract": "The Transformer, a highly expressive architecture for sequence modeling, has\nrecently been adapted to solve sequential decision-making, most notably through\nthe Decision Transformer (DT), which learns policies by conditioning on desired\nreturns. Yet, the adversarial robustness of reinforcement learning methods\nbased on sequence modeling remains largely unexplored. Here we introduce the\nConservative Adversarially Robust Decision Transformer (CART), to our knowledge\nthe first framework designed to enhance the robustness of DT in adversarial\nstochastic games. We formulate the interaction between the protagonist and the\nadversary at each stage as a stage game, where the payoff is defined as the\nexpected maximum value over subsequent states, thereby explicitly incorporating\nstochastic state transitions. By conditioning Transformer policies on the NashQ\nvalue derived from these stage games, CART generates policy that are\nsimultaneously less exploitable (adversarially robust) and conservative to\ntransition uncertainty. Empirically, CART achieves more accurate minimax value\nestimation and consistently attains superior worst-case returns across a range\nof adversarial stochastic games."
    },
    {
        "date": "2025-10",
        "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models",
        "author": "Dominik Schwarz",
        "link": "http://arxiv.org/abs/2510.11837v1",
        "abstract": "The security of Large Language Model (LLM) applications is fundamentally\nchallenged by \"form-first\" attacks like prompt injection and jailbreaking,\nwhere malicious instructions are embedded within user inputs. Conventional\ndefenses, which rely on post hoc output filtering, are often brittle and fail\nto address the root cause: the model's inability to distinguish trusted\ninstructions from untrusted data. This paper proposes Countermind, a\nmulti-layered security architecture intended to shift defenses from a reactive,\npost hoc posture to a proactive, pre-inference, and intra-inference enforcement\nmodel. The architecture proposes a fortified perimeter designed to structurally\nvalidate and transform all inputs, and an internal governance mechanism\nintended to constrain the model's semantic processing pathways before an output\nis generated. The primary contributions of this work are conceptual designs\nfor: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text\nCrypter intended to reduce the plaintext prompt injection attack surface,\nprovided all ingestion paths are enforced. (2) A Parameter-Space Restriction\n(PSR) mechanism, leveraging principles from representation engineering, to\ndynamically control the LLM's access to internal semantic clusters, with the\ngoal of mitigating semantic drift and dangerous emergent behaviors. (3) A\nSecure, Self-Regulating Core that uses an OODA loop and a learning security\nmodule to adapt its defenses based on an immutable audit log. (4) A Multimodal\nInput Sandbox and Context-Defense mechanisms to address threats from\nnon-textual data and long-term semantic poisoning. This paper outlines an\nevaluation plan designed to quantify the proposed architecture's effectiveness\nin reducing the Attack Success Rate (ASR) for form-first attacks and to measure\nits potential latency overhead."
    },
    {
        "date": "2025-10",
        "title": "Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning",
        "author": "Simin Li, Zihao Mao, Hanxiao Li, Zonglei Jing, Zhuohang bian, Jun Guo, Li Wang, Zhuoran Han, Ruixiao Xu, Xin Yu, Chengdong Ma, Yuqing Ma, Bo An, Yaodong Yang, Weifeng Lv, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2510.11824v1",
        "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common\npractice to tune hyperparameters in ideal simulated environments to maximize\ncooperative performance. However, policies tuned for cooperation often fail to\nmaintain robustness and resilience under real-world uncertainties. Building\ntrustworthy MARL systems requires a deep understanding of robustness, which\nensures stability under uncertainties, and resilience, the ability to recover\nfrom disruptions--a concept extensively studied in control systems but largely\noverlooked in MARL. In this paper, we present a large-scale empirical study\ncomprising over 82,620 experiments to evaluate cooperation, robustness, and\nresilience in MARL across 4 real-world environments, 13 uncertainty types, and\n15 hyperparameters. Our key findings are: (1) Under mild uncertainty,\noptimizing cooperation improves robustness and resilience, but this link\nweakens as perturbations intensify. Robustness and resilience also varies by\nalgorithm and uncertainty type. (2) Robustness and resilience do not generalize\nacross uncertainty modalities or agent scopes: policies robust to action noise\nfor all agents may fail under observation noise on a single agent. (3)\nHyperparameter tuning is critical for trustworthy MARL: surprisingly, standard\npractices like parameter sharing, GAE, and PopArt can hurt robustness, while\nearly stopping, high critic learning rates, and Leaky ReLU consistently help.\nBy optimizing hyperparameters only, we observe substantial improvement in\ncooperation, robustness and resilience across all MARL backbones, with the\nphenomenon also generalizing to robust MARL methods across these backbones.\nCode and results available at\nhttps://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark ."
    },
    {
        "date": "2025-10",
        "title": "BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing",
        "author": "Caelin Kaplan, Alexander Warnecke, and Neil Archibald",
        "link": "http://arxiv.org/abs/2510.11823v1",
        "abstract": "AI models are being increasingly integrated into real-world systems, raising\nsignificant concerns about their safety and security. Consequently, AI red\nteaming has become essential for organizations to proactively identify and\naddress vulnerabilities before they can be exploited by adversaries. While\nnumerous AI red teaming tools currently exist, practitioners face challenges in\nselecting the most appropriate tools from a rapidly expanding landscape, as\nwell as managing complex and frequently conflicting software dependencies\nacross isolated projects. Given these challenges and the relatively small\nnumber of organizations with dedicated AI red teams, there is a strong need to\nlower barriers to entry and establish a standardized environment that\nsimplifies the setup and execution of comprehensive AI model assessments.\n  Inspired by Kali Linux's role in traditional penetration testing, we\nintroduce BlackIce, an open-source containerized toolkit designed for red\nteaming Large Language Models (LLMs) and classical machine learning (ML)\nmodels. BlackIce provides a reproducible, version-pinned Docker image that\nbundles 14 carefully selected open-source tools for Responsible AI and Security\ntesting, all accessible via a unified command-line interface. With this setup,\ninitiating red team assessments is as straightforward as launching a container,\neither locally or using a cloud platform. Additionally, the image's modular\narchitecture facilitates community-driven extensions, allowing users to easily\nadapt or expand the toolkit as new threats emerge. In this paper, we describe\nthe architecture of the container image, the process used for selecting tools,\nand the types of evaluations they support."
    },
    {
        "date": "2025-10",
        "title": "A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges",
        "author": "Yuwen Cui, Guangjing Wang, Khanh Vu, Kai Wei, Kehan Shen, Zhengyuan Jiang, Xiao Han, Ning Wang, Zhuo Lu, and Yao Liu",
        "link": "http://arxiv.org/abs/2510.11804v1",
        "abstract": "The Tor network provides users with strong anonymity by routing their\ninternet traffic through multiple relays. While Tor encrypts traffic and hides\nIP addresses, it remains vulnerable to traffic analysis attacks such as the\nwebsite fingerprinting (WF) attack, achieving increasingly high fingerprinting\naccuracy even under open-world conditions. In response, researchers have\nproposed a variety of defenses, ranging from adaptive padding, traffic\nregularization, and traffic morphing to adversarial perturbation, that seek to\nobfuscate or reshape traffic traces. However, these defenses often entail\ntrade-offs between privacy, usability, and system performance. Despite\nextensive research, a comprehensive survey unifying WF datasets, attack\nmethodologies, and defense strategies remains absent. This paper fills that gap\nby systematically categorizing existing WF research into three key domains:\ndatasets, attack models, and defense mechanisms. We provide an in-depth\ncomparative analysis of techniques, highlight their strengths and limitations\nunder diverse threat models, and discuss emerging challenges such as multi-tab\nbrowsing and coarse-grained traffic features. By consolidating prior work and\nidentifying open research directions, this survey serves as a foundation for\nadvancing stronger privacy protection in Tor."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Attacks Leverage Interference Between Features in Superposition",
        "author": "Edward Stevinson, Lucas Prieto, Melih Barsbey, and Tolga Birdal",
        "link": "http://arxiv.org/abs/2510.11709v1",
        "abstract": "Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs."
    },
    {
        "date": "2025-10",
        "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving",
        "author": "Ruida Wang, Jiarui Yao, Rui Pan, Shizhe Diao, and Tong Zhang",
        "link": "http://arxiv.org/abs/2510.11769v1",
        "abstract": "Solving math problems through verifiable languages such as Lean has\nsignificantly impacted both the mathematics and computer science communities.\nCurrent state-of-the-art models are often trained with expensive online\nReinforcement Learning (RL) or expert iteration. However, these approaches rely\non fixed problem sets, which causes inefficient training and limits the model\nto tackle complex problems. To overcome these limitations, we propose GAR:\nGenerative Adversarial Reinforcement learning, a comprehensive RL training\nframework that jointly trains the problem composer and solver in an adversarial\nloop. GAR introduces an implicit curriculum learning mechanism, which aligns\ntask difficulty with the prover's evolving capability. It thereby improves the\ntraining efficiency and enables stronger performance of proving advanced\ntheorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and\nDeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of\n4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on\nProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR\nestablishes a general RL paradigm for co-evolution of problem generation and\nsolving under verifiable environments."
    },
    {
        "date": "2025-10",
        "title": "LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings",
        "author": "Ting Li, Yang Yang, Yipeng Yu, Liang Yao, Guoqing Chao, and Ruifeng Xu",
        "link": "http://arxiv.org/abs/2510.11584v1",
        "abstract": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations."
    },
    {
        "date": "2025-10",
        "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
        "author": "Yijun Hu, Bing Fan, Xin Gu, Haiqing Ren, Dongfang Liu, Heng Fan, and Libo Zhang",
        "link": "http://arxiv.org/abs/2510.11417v1",
        "abstract": "Establishing object-level correspondence between egocentric and exocentric\nviews is essential for intelligent assistants to deliver precise and intuitive\nvisual guidance. However, this task faces numerous challenges, including\nextreme viewpoint variations, occlusions, and the presence of small objects.\nExisting approaches usually borrow solutions from video object segmentation\nmodels, but still suffer from the aforementioned challenges. Recently, the\nSegment Anything Model 2 (SAM 2) has shown strong generalization capabilities\nand excellent performance in video object segmentation. Yet, when simply\napplied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe\ndifficulties due to ineffective ego-exo feature fusion and limited long-term\nmemory capacity, especially for long videos. Addressing these problems, we\npropose a novel EEC framework based on SAM 2 with long-term memories by\npresenting a dual-memory architecture and an adaptive feature routing module\ninspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features\n(i) a Memory-View MoE module which consists of a dual-branch routing mechanism\nto adaptively assign contribution weights to each expert feature along both\nchannel and spatial dimensions, and (ii) a dual-memory bank system with a\nsimple yet effective compression strategy to retain critical long-term\ninformation while eliminating redundancy. In the extensive experiments on the\nchallenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new\nstate-of-the-art results and significantly outperforms existing methods and the\nSAM 2 baseline, showcasing its strong generalization across diverse scenarios.\nOur code and model are available at https://github.com/juneyeeHu/LM-EEC."
    },
    {
        "date": "2025-10",
        "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
        "author": "Sean Oesch, Jack Hutchins, Luke Koch, and Kevin Kurian",
        "link": "http://arxiv.org/abs/2510.11398v1",
        "abstract": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat."
    },
    {
        "date": "2025-10",
        "title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training",
        "author": "Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, and Tony T. Wang",
        "link": "http://arxiv.org/abs/2510.13872v1",
        "abstract": "Simultaneously achieving robust classification and high-fidelity generative\nmodeling within a single framework presents a significant challenge. Hybrid\napproaches, such as Joint Energy-Based Models (JEM), interpret classifiers as\nEBMs but are often limited by the instability and poor sample quality inherent\nin SGLD-based training. We address these limitations by proposing a novel\ntraining framework that integrates adversarial training (AT) principles for\nboth discriminative robustness and stable generative learning. The proposed\nmethod introduces three key innovations: (1) the replacement of SGLD-based JEM\nlearning with a stable, AT-based approach that optimizes the energy function by\ndiscriminating between real data and PGD-generated contrastive samples using\nthe BCE loss; (2) synergistic adversarial training for the discriminative\ncomponent that enhances classification robustness while eliminating the need\nfor explicit gradient penalties; and (3) a two-stage training procedure to\nresolve the incompatibility between batch normalization and EBM training.\nExperiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method\nsubstantially improves adversarial robustness over existing hybrid models while\nmaintaining competitive generative performance. On ImageNet, when optimized for\ngenerative modeling, our model's generative fidelity surpasses that of BigGAN\nand approaches diffusion models, representing the first MCMC-based EBM approach\nto achieve high-quality generation on complex, high-resolution datasets. Our\napproach addresses key stability issues that have limited JEM scaling and\ndemonstrates that adversarial training can serve as an effective foundation for\nunified frameworks capable of generating and robustly classifying visual data."
    },
    {
        "date": "2025-10",
        "title": "CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks",
        "author": "Munsif Ali, Leonardo Rossi, and Massimo Bertozzi",
        "link": "http://arxiv.org/abs/2510.13869v1",
        "abstract": "Continual learning (CL) in the context of Generative Adversarial Networks\n(GANs) remains a challenging problem, particularly when it comes to learn from\na few-shot (FS) samples without catastrophic forgetting. Current most effective\nstate-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible\nquantity of new weights at each training iteration, which would become\nsignificant when considering the long term. For this reason, this paper\nintroduces \\textcolor{red}{\\textbf{\\underline{c}}}ontinual\nfew-sh\\textcolor{red}{\\textbf{\\underline{o}}}t learning with\n\\textcolor{red}{\\textbf{\\underline{lo}}}w-\\textcolor{red}{\\textbf{\\underline{r}}}ank\nadaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and\nCL together, leveraging low-rank tensors to efficiently adapt the model to\ntarget tasks while reducing even more the number of parameters required.\nApplying a vanilla LoRA implementation already permitted us to obtain pretty\ngood results. In order to optimize even further the size of the adapters, we\nchallenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for\nconvolutional layers. Finally, aware of the criticality linked to the choice of\nthe hyperparameters of LoRA, we provide an empirical study to easily find the\nbest ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on\nseveral benchmark CL and FS tasks and show that our model is efficient,\nreaching SOTA performance but with a number of resources enormously reduced.\nSource code is available on\n\\href{https://github.com/munsifali11/CoLoR-GAN}{Github."
    },
    {
        "date": "2025-10",
        "title": "TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical Image Security",
        "author": "Junhua Zhou, Quanjun Li, Weixuan Li, Guang Yu, Yihua Shao, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, and Xuhang Chen",
        "link": "http://arxiv.org/abs/2510.11301v1",
        "abstract": "The rise of digital medical imaging, like MRI and CT, demands strong\nencryption to protect patient data in telemedicine and cloud storage. Chaotic\nsystems are popular for image encryption due to their sensitivity and unique\ncharacteristics, but existing methods often lack sufficient security. This\npaper presents the Three-dimensional Diffusion Algorithm and Deep Learning\nImage Encryption system (TDADL-IE), built on three key elements. First, we\npropose an enhanced chaotic generator using an LSTM network with a 1D-Sine\nQuadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation.\nNext, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt\npermuted images. TDADL-IE is versatile for images of any size. Experiments\nconfirm its effectiveness against various security threats. The code is\navailable at\n\\href{https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}."
    },
    {
        "date": "2025-10",
        "title": "Collaborative Shadows: Distributed Backdoor Attacks in LLM-Based Multi-Agent Systems",
        "author": "Pengyu Zhu, Lijun Li, Yaxing Lyu, Li Sun, Sen Su, and Jing Shao",
        "link": "http://arxiv.org/abs/2510.11246v1",
        "abstract": "LLM-based multi-agent systems (MAS) demonstrate increasing integration into\nnext-generation applications, but their safety in backdoor attacks remains\nlargely underexplored. However, existing research has focused exclusively on\nsingle-agent backdoor attacks, overlooking the novel attack surfaces introduced\nby agent collaboration in MAS. To bridge this gap, we present the first\nDistributed Backdoor Attack tailored to MAS. We decompose the backdoor into\nmultiple distributed attack primitives that are embedded within MAS tools.\nThese primitives remain dormant individually but collectively activate only\nwhen agents collaborate in a specific sequence, thereby assembling the full\nbackdoor to execute targeted attacks such as data exfiltration. To fully assess\nthis threat, we introduce a benchmark for multi-role collaborative tasks and a\nsandboxed framework to evaluate. Extensive experiments demonstrate that our\nattack achieves an attack success rate exceeding 95% without degrading\nperformance on benign tasks. This work exposes novel backdoor attack surfaces\nthat exploit agent collaboration, underscoring the need to move beyond\nsingle-agent protection. Code and benchmark are available at\nhttps://github.com/whfeLingYu/Distributed-Backdoor-Attacks-in-MAS."
    },
    {
        "date": "2025-10",
        "title": "Attacks by Content: Automated Fact-checking is an AI Security Issue",
        "author": "Michael Schlichtkrull",
        "link": "http://arxiv.org/abs/2510.11238v1",
        "abstract": "When AI agents retrieve and reason over external documents, adversaries can\nmanipulate the data they receive to subvert their behaviour. Previous research\nhas studied indirect prompt injection, where the attacker injects malicious\ninstructions. We argue that injection of instructions is not necessary to\nmanipulate agents - attackers could instead supply biased, misleading, or false\ninformation. We term this an attack by content. Existing defenses, which focus\non detecting hidden commands, are ineffective against attacks by content. To\ndefend themselves and their users, agents must critically evaluate retrieved\ninformation, corroborating claims with external evidence and evaluating source\ntrustworthiness. We argue that this is analogous to an existing NLP task,\nautomated fact-checking, which we propose to repurpose as a cognitive\nself-defense tool for agents."
    },
    {
        "date": "2025-10",
        "title": "TraceAegis: Securing LLM-Based Agents via Hierarchical and Behavioral Anomaly Detection",
        "author": "Jiahao Liu, Bonan Ruan, Xianglin Yang, Zhiwei Lin, Yan Liu, Yang Wang, Tao Wei, and Zhenkai Liang",
        "link": "http://arxiv.org/abs/2510.11203v1",
        "abstract": "LLM-based agents have demonstrated promising adaptability in real-world\napplications. However, these agents remain vulnerable to a wide range of\nattacks, such as tool poisoning and malicious instructions, that compromise\ntheir execution flow and can lead to serious consequences like data breaches\nand financial loss. Existing studies typically attempt to mitigate such\nanomalies by predefining specific rules and enforcing them at runtime to\nenhance safety. Yet, designing comprehensive rules is difficult, requiring\nextensive manual effort and still leaving gaps that result in false negatives.\nAs agent systems evolve into complex software systems, we take inspiration from\nsoftware system security and propose TraceAegis, a provenance-based analysis\nframework that leverages agent execution traces to detect potential anomalies.\nIn particular, TraceAegis constructs a hierarchical structure to abstract\nstable execution units that characterize normal agent behaviors. These units\nare then summarized into constrained behavioral rules that specify the\nconditions necessary to complete a task. By validating execution traces against\nboth hierarchical and behavioral constraints, TraceAegis is able to effectively\ndetect abnormal behaviors. To evaluate the effectiveness of TraceAegis, we\nintroduce TraceAegis-Bench, a dataset covering two representative scenarios:\nhealthcare and corporate procurement. Each scenario includes 1,300 benign\nbehaviors and 300 abnormal behaviors, where the anomalies either violate the\nagent's execution order or break the semantic consistency of its execution\nsequence. Experimental results demonstrate that TraceAegis achieves strong\nperformance on TraceAegis-Bench, successfully identifying the majority of\nabnormal behaviors."
    },
    {
        "date": "2025-10",
        "title": "RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation",
        "author": "Vasilije Stambolic, Aritra Dhar, and Lukas Cavigelli",
        "link": "http://arxiv.org/abs/2510.11195v1",
        "abstract": "Retrieval-Augmented Generation (RAG) increases the reliability and\ntrustworthiness of the LLM response and reduces hallucination by eliminating\nthe need for model retraining. It does so by adding external data into the\nLLM's context. We develop a new class of black-box attack, RAG-Pull, that\ninserts hidden UTF characters into queries or external code repositories,\nredirecting retrieval toward malicious code, thereby breaking the models'\nsafety alignment. We observe that query and code perturbations alone can shift\nretrieval toward attacker-controlled snippets, while combined query-and-target\nperturbations achieve near-perfect success. Once retrieved, these snippets\nintroduce exploitable vulnerabilities such as remote code execution and SQL\ninjection. RAG-Pull's minimal perturbations can alter the model's safety\nalignment and increase preference towards unsafe code, therefore opening up a\nnew class of attacks on LLMs."
    },
    {
        "date": "2025-10",
        "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively",
        "author": "Peiming Li, Zhiyuan Hu, Yang Tang, Shiyu Li, and Xi Chen",
        "link": "http://arxiv.org/abs/2510.11194v1",
        "abstract": "Personalized alignment is crucial for enabling Large Language Models (LLMs)\nto engage effectively in user-centric interactions. However, current methods\nface a dual challenge: they fail to infer users' deep implicit preferences\n(including unstated goals, semantic context and risk tolerances), and they lack\nthe defensive reasoning required to navigate real-world ambiguity. This\ncognitive gap leads to responses that are superficial, brittle and\nshort-sighted. To address this, we propose Critique-Driven Reasoning Alignment\n(CDRA), which reframes alignment from a scalar reward-matching task into a\nstructured reasoning process. First, to bridge the preference inference gap, we\nintroduce the DeepPref benchmark. This dataset, comprising 3000\npreference-query pairs across 20 topics, is curated by simulating a\nmulti-faceted cognitive council that produces critique-annotated reasoning\nchains to deconstruct query semantics and reveal latent risks. Second, to\ninstill defensive reasoning, we introduce the Personalized Generative Process\nReward Model (Pers-GenPRM), which frames reward modeling as a personalized\nreasoning task. It generates a critique chain to evaluate a response's\nalignment with user preferences before outputting a final score based on this\nrationale. Ultimately, this interpretable, structured reward signal guides\npolicy model through Critique-Driven Policy Alignment, a process-level online\nreinforcement learning algorithm integrating both numerical and natural\nlanguage feedback. Experiments demonstrate that CDRA excels at discovering and\naligning with users' true preferences while executing robust reasoning. Our\ncode and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref."
    },
    {
        "date": "2025-10",
        "title": "TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code",
        "author": "Alexander Sternfeld, Andrei Kucharavy, and Ljiljana Dolamic",
        "link": "http://arxiv.org/abs/2510.11151v1",
        "abstract": "Large language Models (LLMs) have shown remarkable proficiency in code\ngeneration tasks across various programming languages. However, their outputs\noften contain subtle but critical vulnerabilities, posing significant risks\nwhen deployed in security-sensitive or mission-critical systems. This paper\nintroduces TypePilot, an agentic AI framework designed to enhance the security\nand robustness of LLM-generated code by leveraging strongly typed and\nverifiable languages, using Scala as a representative example. We evaluate the\neffectiveness of our approach in two settings: formal verification with the\nStainless framework and general-purpose secure code generation. Our experiments\nwith leading open-source LLMs reveal that while direct code generation often\nfails to enforce safety constraints, just as naive prompting for more secure\ncode, our type-focused agentic pipeline substantially mitigates input\nvalidation and injection vulnerabilities. The results demonstrate the potential\nof structured, type-guided LLM workflows to improve the SotA of the\ntrustworthiness of automated code generation in high-assurance domains."
    },
    {
        "date": "2025-10",
        "title": "CoSPED: Consistent Soft Prompt Targeted Data Extraction and Defense",
        "author": "Yang Zhuochen, Fok Kar Wai, and Thing Vrizlynn",
        "link": "http://arxiv.org/abs/2510.11137v1",
        "abstract": "Large language models have gained widespread attention recently, but their\npotential security vulnerabilities, especially privacy leakage, are also\nbecoming apparent. To test and evaluate for data extraction risks in LLM, we\nproposed CoSPED, short for Consistent Soft Prompt targeted data Extraction and\nDefense. We introduce several innovative components, including Dynamic Loss,\nAdditive Loss, Common Loss, and Self Consistency Decoding Strategy, and tested\nto enhance the consistency of the soft prompt tuning process. Through extensive\nexperimentation with various combinations, we achieved an extraction rate of\n65.2% at a 50-token prefix comparison. Our comparisons of CoSPED with other\nreference works confirm our superior extraction rates. We evaluate CoSPED on\nmore scenarios, achieving Pythia model extraction rate of 51.7% and introducing\ncross-model comparison. Finally, we explore defense through Rank-One Model\nEditing and achieve a reduction in the extraction rate to 1.6%, which proves\nthat our analysis of extraction mechanisms can directly inform effective\nmitigation strategies against soft prompt-based attacks."
    },
    {
        "date": "2025-10",
        "title": "PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities",
        "author": "Cheol-Hui Lee, Hwa-Yeon Lee, Min-Kyung Jung, and Dong-Joo Kim",
        "link": "http://arxiv.org/abs/2510.11110v1",
        "abstract": "Missing or corrupted modalities are common in physiological signal-based\nmedical applications owing to hardware constraints or motion artifacts.\nHowever, most existing methods assume the availability of all modalities,\nresulting in substantial performance degradation in the absence of any\nmodality. To overcome this limitation, this study proposes PhysioME, a robust\nframework designed to ensure reliable performance under missing modality\nconditions. PhysioME adopts: (1) a multimodal self-supervised learning approach\nthat combines contrastive learning with masked prediction; (2) a\nDual-PathNeuroNet backbone tailored to capture the temporal dynamics of each\nphysiological signal modality; and (3) a restoration decoder that reconstructs\nmissing modality tokens, enabling flexible processing of incomplete inputs. The\nexperimental results show that PhysioME achieves high consistency and\ngeneralization performance across various missing modality scenarios. These\nfindings highlight the potential of PhysioME as a reliable tool for supporting\nclinical decision-making in real-world settings with imperfect data\navailability."
    },
    {
        "date": "2025-10",
        "title": "CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization",
        "author": "Fengling Zhu, Boshi Liu, Jingyu Hua, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2510.11096v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\ntasks such as image captioning, visual question answering, and cross-modal\nreasoning by integrating visual and textual modalities. However, their\nmultimodal nature also exposes them to adversarial threats, where attackers can\nperturb either modality or both jointly to induce harmful, misleading, or\npolicy violating outputs. Existing defense strategies, such as adversarial\ntraining and input purification, face notable limitations: adversarial training\ntypically improves robustness only against known attacks while incurring high\ncomputational costs, whereas conventional purification approaches often suffer\nfrom degraded image quality and insufficient generalization to complex\nmultimodal tasks.\n  In this work, we focus on defending the visual modality, which frequently\nserves as the primary entry point for adversarial manipulation. We propose a\nsupervised diffusion based denoising framework that leverages paired\nadversarial clean image datasets to fine-tune diffusion models with\ndirectional, task specific guidance. Unlike prior unsupervised purification\nmethods such as DiffPure, our approach achieves higher quality reconstructions\nwhile significantly improving defense robustness in multimodal tasks.\nFurthermore, we incorporate prompt optimization as a complementary defense\nmechanism, enhancing resistance against diverse and unseen attack strategies.\n  Extensive experiments on image captioning and visual question answering\ndemonstrate that our method not only substantially improves robustness but also\nexhibits strong transferability to unknown adversarial attacks. These results\nhighlight the effectiveness of supervised diffusion based denoising for\nmultimodal defense, paving the way for more reliable and secure deployment of\nMLLMs in real world applications."
    },
    {
        "date": "2025-10",
        "title": "Robust Photoplethysmography Signal Denoising via Mamba Networks",
        "author": "I Chiu, Yu-Tung Liu, Kuan-Chen Wang, Hung-Yu Wei, and Yu Tsao",
        "link": "http://arxiv.org/abs/2510.11058v1",
        "abstract": "Photoplethysmography (PPG) is widely used in wearable health monitoring, but\nits reliability is often degraded by noise and motion artifacts, limiting\ndownstream applications such as heart rate (HR) estimation. This paper presents\na deep learning framework for PPG denoising with an emphasis on preserving\nphysiological information. In this framework, we propose DPNet, a Mamba-based\ndenoising backbone designed for effective temporal modeling. To further enhance\ndenoising performance, the framework also incorporates a scale-invariant\nsignal-to-distortion ratio (SI-SDR) loss to promote waveform fidelity and an\nauxiliary HR predictor (HRP) that provides physiological consistency through\nHR-based supervision. Experiments on the BIDMC dataset show that our method\nachieves strong robustness against both synthetic noise and real-world motion\nartifacts, outperforming conventional filtering and existing neural models. Our\nmethod can effectively restore PPG signals while maintaining HR accuracy,\nhighlighting the complementary roles of SI-SDR loss and HR-guided supervision.\nThese results demonstrate the potential of our approach for practical\ndeployment in wearable healthcare systems."
    },
    {
        "date": "2025-10",
        "title": "Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation",
        "author": "Zixi Wang, Yushe Cao, Yubo Huang, Jinzhu Wei, Jingzehua Xu, Shuai Zhang, and Xin Lai",
        "link": "http://arxiv.org/abs/2510.13864v1",
        "abstract": "In this paper, we propose a new method called Self-Training with Dynamic\nWeighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation\n(GDA) by addressing the challenge of smooth knowledge migration from the source\nto the target domain. Traditional GDA methods mitigate domain shift through\nintermediate domains and self-training but often suffer from inefficient\nknowledge migration or incomplete intermediate data. Our approach introduces a\ndynamic weighting mechanism that adaptively balances the loss contributions of\nthe source and target domains during training. Specifically, we design an\noptimization framework governed by a time-varying hyperparameter $\\varrho$\n(progressing from 0 to 1), which controls the strength of domain-specific\nlearning and ensures stable adaptation. The method leverages self-training to\ngenerate pseudo-labels and optimizes a weighted objective function for\niterative model updates, maintaining robustness across intermediate domains.\nExperiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the\nCover Type dataset demonstrate that STDW outperforms existing baselines.\nAblation studies further validate the critical role of $\\varrho$'s dynamic\nscheduling in achieving progressive adaptation, confirming its effectiveness in\nreducing domain bias and improving generalization. This work provides both\ntheoretical insights and a practical framework for robust gradual domain\nadaptation, with potential applications in dynamic real-world scenarios. The\ncode is available at https://github.com/Dramwig/STDW."
    },
    {
        "date": "2025-10",
        "title": "The Easy Path to Robustness: Coreset Selection using Sample Hardness",
        "author": "Pranav Ramesh, Arjun Roy, Deepak Ravikumar, Kaushik Roy, and Gopalakrishnan Srinivasan",
        "link": "http://arxiv.org/abs/2510.11018v1",
        "abstract": "Designing adversarially robust models from a data-centric perspective\nrequires understanding which input samples are most crucial for learning\nresilient features. While coreset selection provides a mechanism for efficient\ntraining on data subsets, current algorithms are designed for clean accuracy\nand fall short in preserving robustness. To address this, we propose a\nframework linking a sample's adversarial vulnerability to its\n\\textit{hardness}, which we quantify using the average input gradient norm\n(AIGN) over training. We demonstrate that \\textit{easy} samples (with low AIGN)\nare less vulnerable and occupy regions further from the decision boundary.\nLeveraging this insight, we present EasyCore, a coreset selection algorithm\nthat retains only the samples with low AIGN for training. We empirically show\nthat models trained on EasyCore-selected data achieve significantly higher\nadversarial accuracy than those trained with competing coreset methods under\nboth standard and adversarial training. As AIGN is a model-agnostic dataset\nproperty, EasyCore is an efficient and widely applicable data-centric method\nfor improving adversarial robustness. We show that EasyCore achieves up to 7\\%\nand 5\\% improvement in adversarial accuracy under standard training and TRADES\nadversarial training, respectively, compared to existing coreset methods."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Robustness in One-Stage Learning-to-Defer",
        "author": "Yannis Montreuil, Letian Yu, Axel Carlier, Lai Xing Ng, and Wei Tsang Ooi",
        "link": "http://arxiv.org/abs/2510.10988v1",
        "abstract": "Learning-to-Defer (L2D) enables hybrid decision-making by routing inputs\neither to a predictor or to external experts. While promising, L2D is highly\nvulnerable to adversarial perturbations, which can not only flip predictions\nbut also manipulate deferral decisions. Prior robustness analyses focus solely\non two-stage settings, leaving open the end-to-end (one-stage) case where\npredictor and allocation are trained jointly. We introduce the first framework\nfor adversarial robustness in one-stage L2D, covering both classification and\nregression. Our approach formalizes attacks, proposes cost-sensitive\nadversarial surrogate losses, and establishes theoretical guarantees including\n$\\mathcal{H}$, $(\\mathcal{R }, \\mathcal{F})$, and Bayes consistency.\nExperiments on benchmark datasets confirm that our methods improve robustness\nagainst untargeted and targeted attacks while preserving clean performance."
    },
    {
        "date": "2025-10",
        "title": "DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation",
        "author": "Hyeseon Ahn, Shinwoo Park, and Yo-Sub Han",
        "link": "http://arxiv.org/abs/2510.10987v1",
        "abstract": "The promise of LLM watermarking rests on a core assumption that a specific\nwatermark proves authorship by a specific model. We demonstrate that this\nassumption is dangerously flawed. We introduce the threat of watermark\nspoofing, a sophisticated attack that allows a malicious model to generate text\ncontaining the authentic-looking watermark of a trusted, victim model. This\nenables the seamless misattribution of harmful content, such as disinformation,\nto reputable sources. The key to our attack is repurposing watermark\nradioactivity, the unintended inheritance of data patterns during fine-tuning,\nfrom a discoverable trait into an attack vector. By distilling knowledge from a\nwatermarked teacher model, our framework allows an attacker to steal and\nreplicate the watermarking signal of the victim model. This work reveals a\ncritical security gap in text authorship verification and calls for a paradigm\nshift towards technologies capable of distinguishing authentic watermarks from\nexpertly imitated ones. Our code is available at\nhttps://github.com/hsannn/ditto.git."
    },
    {
        "date": "2025-10",
        "title": "APLOT: Robust Reward Modeling via Adaptive Preference Learning with Optimal Transport",
        "author": "Zhuo Li, Yuege Feng, Dandan Guo, Jinpeng Hu, Anningzhe Gao, and Xiang Wan",
        "link": "http://arxiv.org/abs/2510.10963v1",
        "abstract": "The reward model (RM) plays a crucial role in aligning Large Language Models\n(LLMs) with human preferences through Reinforcement Learning, where the\nBradley-Terry (BT) objective has been recognized as simple yet powerful,\nspecifically for pairwise preference learning. However, BT-based RMs often\nstruggle to effectively distinguish between similar preference responses,\nleading to insufficient separation between preferred and non-preferred outputs.\nConsequently, they may easily overfit easy samples and cannot generalize well\nto Out-Of-Distribution (OOD) samples, resulting in suboptimal performance. To\naddress these challenges, this paper introduces an effective enhancement to\nBT-based RMs through an adaptive margin mechanism. Specifically, we design to\ndynamically adjust the RM focus on more challenging samples through margins,\nbased on both semantic similarity and model-predicted reward differences, which\nis approached from a distributional perspective solvable with Optimal Transport\n(OT). By incorporating these factors into a principled OT cost matrix design,\nour adaptive margin enables the RM to better capture distributional differences\nbetween chosen and rejected responses, yielding significant improvements in\nperformance, convergence speed, and generalization capabilities. Experimental\nresults across multiple benchmarks demonstrate that our method outperforms\nseveral existing RM techniques, showcasing enhanced performance in both\nIn-Distribution (ID) and OOD settings. Moreover, RLHF experiments support our\npractical effectiveness in better aligning LLMs with human preferences. Our\ncode is available at https://github.com/BIRlz/APLOT"
    },
    {
        "date": "2025-10",
        "title": "Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems",
        "author": "Qizhou Peng, Yang Zheng, Yu Wen, Yanna Wu, and Yingying Du",
        "link": "http://arxiv.org/abs/2510.10937v1",
        "abstract": "Reinforcement learning (RL) has been an important machine learning paradigm\nfor solving long-horizon sequential decision-making problems under uncertainty.\nBy integrating deep neural networks (DNNs) into the RL framework, deep\nreinforcement learning (DRL) has emerged, which achieved significant success in\nvarious domains. However, the integration of DNNs also makes it vulnerable to\nadversarial attacks. Existing adversarial attack techniques mainly focus on\neither directly manipulating the environment with which a victim agent\ninteracts or deploying an adversarial agent that interacts with the victim\nagent to induce abnormal behaviors. While these techniques achieve promising\nresults, their adoption in multi-party open systems remains limited due to two\nmajor reasons: impractical assumption of full control over the environment and\ndependent on interactions with victim agents.\n  To enable adversarial attacks in multi-party open systems, in this paper, we\nredesigned an adversarial policy learning approach that can mislead\nwell-trained victim agents without requiring direct interactions with these\nagents or full control over their environments. Particularly, we propose a\nneutral agent-based approach across various task scenarios in multi-party open\nsystems. While the neutral agents seemingly are detached from the victim\nagents, indirectly influence them through the shared environment. We evaluate\nour proposed method on the SMAC platform based on Starcraft II and the\nautonomous driving simulation platform Highway-env. The experimental results\ndemonstrate that our method can launch general and effective adversarial\nattacks in multi-party open systems."
    },
    {
        "date": "2025-10",
        "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
        "author": "Zonghuan Xu, Xiang Zheng, Xingjun Ma, and Yu-Gang Jiang",
        "link": "http://arxiv.org/abs/2510.10932v1",
        "abstract": "With the growing deployment of Vision-Language-Action (VLA) models in\nreal-world embodied AI systems, their increasing vulnerability to backdoor\nattacks poses a serious safety threat. A backdoored VLA agent can be covertly\ntriggered by a pre-injected backdoor to execute adversarial actions,\npotentially causing system failures or even physical harm. Although backdoor\nattacks on VLA models have been explored, prior work has focused only on\nuntargeted attacks, leaving the more practically threatening scenario of\ntargeted manipulation unexamined. In this paper, we study targeted backdoor\nattacks on VLA models and introduce TabVLA, a novel framework that enables such\nattacks via black-box fine-tuning. TabVLA explores two deployment-relevant\ninference-time threat models: input-stream editing and in-scene triggering. It\nformulates poisoned data generation as an optimization problem to improve\nattack effectivess. Experiments with OpenVLA-7B on the LIBERO benchmark reveal\nthat the vision channel is the principal attack surface: targeted backdoors\nsucceed with minimal poisoning, remain robust across variations in trigger\ndesign, and are degraded only by positional mismatches between fine-tuning and\ninference triggers. We also investigate a potential detection-based defense\nagainst TabVLA, which reconstructs latent visual triggers from the input stream\nto flag activation-conditioned backdoor samples. Our work highlights the\nvulnerability of VLA models to targeted backdoor manipulation and underscores\nthe need for more advanced defenses."
    },
    {
        "date": "2025-10",
        "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation",
        "author": "Ruchit Rawal, Jeffrey Yang Fan Chiang, Chihao Shen, Jeffery Siyuan Tian, Aastha Mahajan, Tom Goldstein, and Yizheng Chen",
        "link": "http://arxiv.org/abs/2510.13859v1",
        "abstract": "AI coding assistants powered by large language models (LLMs) have transformed\nsoftware development, significantly boosting productivity. While existing\nbenchmarks evaluate the correctness and security of LLM-generated code, they\nare typically limited to single-turn tasks that do not reflect the iterative\nnature of real-world development. We introduce MT-Sec, the first benchmark to\nsystematically evaluate both correctness and security in multi-turn coding\nscenarios. We construct this using a synthetic data pipeline that transforms\nexisting single-turn tasks into semantically aligned multi-turn interaction\nsequences, allowing reuse of original test suites while modeling the complexity\nof real-world coding processes. We evaluate 32 open- and closed-source models,\nand three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in\n\"correct and secure\" outputs from single-turn to multi-turn settings -- even\namong state-of-the-art models. Beyond full-program generation, we also evaluate\nmodels on multi-turn code-diff generation -- an unexplored yet practically\nrelevant setting -- and find that models perform worse here, with increased\nrates of functionally incorrect and insecure outputs. Finally, we find that\nwhile agent scaffoldings boost single-turn code generation performance, they\nare not quite as effective in multi-turn evaluations. Together, these findings\nhighlight the need for benchmarks that jointly evaluate correctness and\nsecurity in multi-turn, real-world coding workflows."
    },
    {
        "date": "2025-10",
        "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts",
        "author": "Tiarnaigh Downey-Webb, Olamide Jogunola, and Oluwaseun Ajao",
        "link": "http://arxiv.org/abs/2510.15973v1",
        "abstract": "This paper presents a systematic security assessment of four prominent Large\nLanguage Models (LLMs) against diverse adversarial attack vectors. We evaluate\nPhi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack\ncategories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),\nand Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs\n1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six\nharm categories. Results demonstrate significant variations in model\nrobustness, with Llama-2 achieving the highest overall security (3.4% average\nattack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%\naverage attack success rate). We identify critical transferability patterns\nwhere GCG and TAP attacks, though ineffective against their target model\n(Llama-2), achieve substantially higher success rates when transferred to other\nmodels (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals\nsignificant differences in vulnerability across harm categories ($p < 0.001$),\nwith malicious use prompts showing the highest attack success rates (10.71%\naverage). Our findings contribute to understanding cross-model security\nvulnerabilities and provide actionable insights for developing targeted defense\nmechanisms"
    },
    {
        "date": "2025-10",
        "title": "GPS Spoofing Attack Detection in Autonomous Vehicles Using Adaptive DBSCAN",
        "author": "Ahmad Mohammadi, Reza Ahmari, Vahid Hemmati, Frederick Owusu-Ambrose, Mahmoud Nabil Mahmoud, Parham Kebria, Abdollah Homaifar, and Mehrdad Saif",
        "link": "http://arxiv.org/abs/2510.10766v1",
        "abstract": "As autonomous vehicles become an essential component of modern\ntransportation, they are increasingly vulnerable to threats such as GPS\nspoofing attacks. This study presents an adaptive detection approach utilizing\na dynamically tuned Density Based Spatial Clustering of Applications with Noise\n(DBSCAN) algorithm, designed to adjust the detection threshold ({\\epsilon}) in\nreal-time. The threshold is updated based on the recursive mean and standard\ndeviation of displacement errors between GPS and in-vehicle sensors data, but\nonly at instances classified as non-anomalous. Furthermore, an initial\nthreshold, determined from 120,000 clean data samples, ensures the capability\nto identify even subtle and gradual GPS spoofing attempts from the beginning.\nTo assess the performance of the proposed method, five different subsets from\nthe real-world Honda Research Institute Driving Dataset (HDD) are selected to\nsimulate both large and small magnitude GPS spoofing attacks. The modified\nalgorithm effectively identifies turn-by-turn, stop, overshoot, and multiple\nsmall biased spoofing attacks, achieving detection accuracies of 98.621%,\n99.960.1%, 99.880.1%, and 98.380.1%, respectively. This work provides a\nsubstantial advancement in enhancing the security and safety of AVs against GPS\nspoofing threats."
    },
    {
        "date": "2025-10",
        "title": "EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition",
        "author": "Sudipto Sarkar, Mohammad Asif Hasan, Khondokar Ashik Shahriar, Fablia Labiba, Nahian Tasnim, and Sheikh Anawarul Haq Fattah",
        "link": "http://arxiv.org/abs/2510.10765v1",
        "abstract": "Identifying drones and birds correctly is essential for keeping the skies\nsafe and improving security systems. Using the VIP CUP 2025 dataset, which\nprovides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a\nnew lightweight yet powerful model for object detection. The model improves how\nimage features are captured and understood, making detection more accurate and\nefficient. It uses smart design changes and attention layers to focus on\nimportant details while reducing the amount of computation needed. A special\ndetection head helps the model adapt to objects of different shapes and sizes.\nWe trained three versions: one using RGB images, one using IR images, and one\ncombining both. The combined model achieved the best accuracy and reliability\nwhile running fast enough for real-time use on common GPUs."
    },
    {
        "date": "2025-10",
        "title": "Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR",
        "author": "Ling Sun, Charlotte Zhu, and Shuju Shi",
        "link": "http://arxiv.org/abs/2510.10738v1",
        "abstract": "General-purpose ASR underperforms for atypical speakers, such as L2 learners,\nreinforcing bias and limiting use in education and accessibility. Using the\nCEFR-graded Speak and Improve corpus, we show that naive fine-tuning of Whisper\nreduces average WER but simultaneously widens disparities and\ndisproportionately harms lower-level learners. To address this, we propose two\nstrategies: (i) proficiency-aware multitask learning, jointly optimizing ASR\nwith proficiency classification, and (ii) targeted augmentation, applying\nspectrogram masking to low-proficiency speech to counter imbalance. These\napproaches reduce WER by up to 29.4 percent (relative) and insertion/deletion\nerrors by as much as 58.6 percent (relative). Crucially, despite the severe\nimbalance of the dataset reflecting real-world distributions, both strategies\nconsistently narrow proficiency gaps, advancing equitable ASR for L2 learners."
    },
    {
        "date": "2025-10",
        "title": "HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing",
        "author": "Rajat Bhattacharjya, Woohyeok Park, Arnab Sarkar, Hyunwoo Oh, Mohsen Imani, and Nikil Dutt",
        "link": "http://arxiv.org/abs/2510.10718v1",
        "abstract": "Direction of Arrival (DoA) estimation techniques face a critical trade-off,\nas classical methods often lack accuracy in challenging, low signal-to-noise\nratio (SNR) conditions, while modern deep learning approaches are too\nenergy-intensive and opaque for resource-constrained, safety-critical systems.\nWe introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing\n(HDC). The framework introduces two distinct feature extraction strategies --\nMean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline,\nand then reframes DoA estimation as a pattern recognition problem. This\napproach leverages HDC's inherent robustness to noise and its transparent\nalgebraic operations to bypass the expensive matrix decompositions and\n``black-box'' nature of classical and deep learning methods, respectively. Our\nevaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than\nstate-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it\nalso consumes ~93% less energy than competing neural baselines on an embedded\nNVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and\nefficiency establishes HYPERDOA as a robust and viable solution for\nmission-critical applications on edge devices."
    },
    {
        "date": "2025-10",
        "title": "Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection",
        "author": "Gaojian Wang, Feng Lin, Tong Wu, Zhisheng Yan, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.10663v1",
        "abstract": "With abundant, unlabeled real faces, how can we learn robust and transferable\nfacial representations to boost generalization across various face security\ntasks? We make the first attempt and propose FS-VFM, a scalable self-supervised\npre-training framework, to learn fundamental representations of real face\nimages. We introduce three learning objectives, namely 3C, that synergize\nmasked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM\nto encode both local patterns and global semantics of real faces. Specifically,\nwe formulate various facial masking strategies for MIM and devise a simple yet\neffective CRFR-P masking, which explicitly prompts the model to pursue\nmeaningful intra-region Consistency and challenging inter-region Coherency. We\npresent a reliable self-distillation mechanism that seamlessly couples MIM with\nID to establish underlying local-to-global Correspondence. After pre-training,\nvanilla vision transformers (ViTs) serve as universal Vision Foundation Models\nfor downstream Face Security tasks: cross-dataset deepfake detection,\ncross-domain face anti-spoofing, and unseen diffusion facial forensics. To\nefficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a\nlightweight plug-and-play bottleneck atop the frozen backbone with a novel\nreal-anchor contrastive objective. Extensive experiments on 11 public\nbenchmarks demonstrate that our FS-VFM consistently generalizes better than\ndiverse VFMs, spanning natural and facial domains, fully, weakly, and\nself-supervised paradigms, small, base, and large ViT scales, and even\noutperforms SOTA task-specific methods, while FS-Adapter offers an excellent\nefficiency-performance trade-off. The code and models are available on\nhttps://fsfm-3c.github.io/fsvfm.html."
    },
    {
        "date": "2025-10",
        "title": "ImpMIA: Leveraging Implicit Bias for Membership Inference Attack under Realistic Scenarios",
        "author": "Yuval Golbari, Navve Wasserman, Gal Vardi, and Michal Irani",
        "link": "http://arxiv.org/abs/2510.10625v2",
        "abstract": "Determining which data samples were used to train a model-known as Membership\nInference Attack (MIA)-is a well-studied and important problem with\nimplications for data privacy. Black-box methods presume access only to the\nmodel's outputs and often rely on training auxiliary reference models. While\nthey have shown strong empirical performance, they rely on assumptions that\nrarely hold in real-world settings: (i) the attacker knows the training\nhyperparameters; (ii) all available non-training samples come from the same\ndistribution as the training data; and (iii) the fraction of training data in\nthe evaluation set is known. In this paper, we demonstrate that removing these\nassumptions leads to a significant drop in the performance of black-box\nattacks. We introduce ImpMIA, a Membership Inference Attack that exploits the\nImplicit Bias of neural networks, hence removes the need to rely on any\nreference models and their assumptions. ImpMIA is a white-box attack -- a\nsetting which assumes access to model weights and is becoming increasingly\nrealistic given that many models are publicly available (e.g., via Hugging\nFace). Building on maximum-margin implicit bias theory, ImpMIA uses the\nKarush-Kuhn-Tucker (KKT) optimality conditions to identify training samples.\nThis is done by finding the samples whose gradients most strongly reconstruct\nthe trained model's parameters. As a result, ImpMIA achieves state-of-the-art\nperformance compared to both black and white box attacks in realistic settings\nwhere only the model weights and a superset of the training data are available."
    },
    {
        "date": "2025-10",
        "title": "Encoder Decoder Generative Adversarial Network Model for Stock Market Prediction",
        "author": "Bahadur Yadav, and Sanjay Kumar Mohanty",
        "link": "http://arxiv.org/abs/2510.10617v1",
        "abstract": "Forecasting stock prices remains challenging due to the volatile and\nnon-linear nature of financial markets. Despite the promise of deep learning,\nissues such as mode collapse, unstable training, and difficulty in capturing\ntemporal and feature level correlations have limited the applications of GANs\nin this domain. We propose a GRU-based Encoder-Decoder GAN (EDGAN) model that\nstrikes a balance between expressive power and simplicity. The model introduces\nkey innovations such as a temporal decoder with residual connections for\nprecise reconstruction, conditioning on static and dynamic covariates for\ncontextual learning, and a windowing mechanism to capture temporal dynamics.\nHere, the generator uses a dense encoder-decoder framework with residual GRU\nblocks. Extensive experiments on diverse stock datasets demonstrate that EDGAN\nachieves superior forecasting accuracy and training stability, even in volatile\nmarkets. It consistently outperforms traditional GAN variants in forecasting\naccuracy and convergence stability under market conditions."
    },
    {
        "date": "2025-10",
        "title": "Multi-scale Frequency-Aware Adversarial Network for Parkinson's Disease Assessment Using Wearable Sensors",
        "author": "Weiming Zhao, Xulong Wang, Jun Qi, Yun Yang, and Po Yang",
        "link": "http://arxiv.org/abs/2510.10558v1",
        "abstract": "Severity assessment of Parkinson's disease (PD) using wearable sensors offers\nan effective, objective basis for clinical management. However, general-purpose\ntime series models often lack pathological specificity in feature extraction,\nmaking it difficult to capture subtle signals highly correlated with\nPD.Furthermore, the temporal sparsity of PD symptoms causes key diagnostic\nfeatures to be easily \"diluted\" by traditional aggregation methods, further\ncomplicating assessment. To address these issues, we propose the Multi-scale\nFrequency-Aware Adversarial Multi-Instance Network (MFAM). This model enhances\nfeature specificity through a frequency decomposition module guided by medical\nprior knowledge. Furthermore, by introducing an attention-based multi-instance\nlearning (MIL) framework, the model can adaptively focus on the most\ndiagnostically valuable sparse segments.We comprehensively validated MFAM on\nboth the public PADS dataset for PD versus differential diagnosis (DD) binary\nclassification and a private dataset for four-class severity assessment.\nExperimental results demonstrate that MFAM outperforms general-purpose time\nseries models in handling complex clinical time series with specificity,\nproviding a promising solution for automated assessment of PD severity."
    },
    {
        "date": "2025-10",
        "title": "Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting",
        "author": "Heming Xia, Cunxiao Du, Rui Li, Chak Tou Leong, Yongqi Li, and Wenjie Li",
        "link": "http://arxiv.org/abs/2510.10528v1",
        "abstract": "Large reasoning models (LRMs) have demonstrated remarkable proficiency in\ntackling complex reasoning tasks through step-by-step thinking. However, such a\nlengthy reasoning process incurs substantial computational and latency\noverheads, hindering the practical deployment of these models. In this work, we\npresent a new perspective on mitigating overthinking in LRMs via black-box\nadversarial prompting. By treating both open-source LRMs and closed-source APIs\nas black-box communicators, we investigate how to elicit concise responses\nwithout sacrificing accuracy. We introduce AdvPrompt, an iterative refinement\nframework that generates high-quality adversarial prompts from diverse\nperspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt\nconsistently reduces token usage while preserving performance. Notably,\nAdvPrompt achieves a 3x reduction in average response length on simple GSM8K\nquestions for the Qwen3 model series, and delivers an average ~40% token\nreduction across four benchmarks. For closed-source APIs, AdvPrompt reduces\ntoken usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further\nanalysis reveals the generalizability of AdvPrompt across various model scales\nand families, underscoring the potential of black-box prompting as a practical\nand effective strategy for enhancing LRM efficiency."
    },
    {
        "date": "2025-10",
        "title": "SASER: Stego attacks on open-source LLMs",
        "author": "Ming Tan, Wei Li, Hu Tao, Hailong Ma, Aodi Liu, Qian Chen, and Zilong Wang",
        "link": "http://arxiv.org/abs/2510.10486v1",
        "abstract": "Open-source large language models (LLMs) have demonstrated considerable\ndominance over proprietary LLMs in resolving neural processing tasks, thanks to\nthe collaborative and sharing nature. Although full access to source codes,\nmodel parameters, and training data lays the groundwork for transparency, we\nargue that such a full-access manner is vulnerable to stego attacks, and their\nill-effects are not fully understood. In this paper, we conduct a systematic\nformalization for stego attacks on open-source LLMs by enumerating all possible\nthreat models associated with adversary objectives, knowledge, and\ncapabilities. Therein, the threat posed by adversaries with internal knowledge,\nwho inject payloads and triggers during the model sharing phase, is of\npractical interest. We go even further and propose the first stego attack on\nopen-source LLMs, dubbed SASER, which wields impacts through identifying\ntargeted parameters, embedding payloads, injecting triggers, and executing\npayloads sequentially. Particularly, SASER enhances the attack robustness\nagainst quantization-based local deployment by de-quantizing the embedded\npayloads. In addition, to achieve stealthiness, SASER devises the\nperformance-aware importance metric to identify targeted parameters with the\nleast degradation of model performance. Extensive experiments on LlaMA2-7B and\nChatGLM3-6B, without quantization, show that the stealth rate of SASER\noutperforms existing stego attacks (for general DNNs) by up to 98.1%, while\nachieving the same attack success rate (ASR) of 100%. More importantly, SASER\nimproves ASR on quantized models from 0 to 100% in all settings. We appeal for\ninvestigations on countermeasures against SASER in view of the significant\nattack effectiveness."
    },
    {
        "date": "2025-10",
        "title": "Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation",
        "author": "Chen Zhong, Yuxuan Yang, Xinyue Zhang, Ruohan Ma, Yong Guo, Gang Li, and Jupeng Li",
        "link": "http://arxiv.org/abs/2510.10462v1",
        "abstract": "Medical image segmentation annotation suffers from inter-rater variability\n(IRV) due to differences in annotators' expertise and the inherent blurriness\nof medical images. Standard approaches that simply average expert labels are\nflawed, as they discard the valuable clinical uncertainty revealed in\ndisagreements. We introduce a fundamentally new approach with our group\ndecision simulation framework, which works by mimicking the collaborative\ndecision-making process of a clinical panel. Under this framework, an Expert\nSignature Generator (ESG) learns to represent individual annotator styles in a\nunique latent space. A Simulated Consultation Module (SCM) then intelligently\ngenerates the final segmentation by sampling from this space. This method\nachieved state-of-the-art results on challenging CBCT and MRI datasets (92.11%\nand 90.72% Dice scores). By treating expert disagreement as a useful signal\ninstead of noise, our work provides a clear path toward more robust and\ntrustworthy AI systems for healthcare."
    },
    {
        "date": "2025-10",
        "title": "Testing and Enhancing Multi-Agent Systems for Robust Code Generation",
        "author": "Zongyi Lyu, Songqiang Chen, Zhenlan Ji, Liwen Wang, Shuai Wang, Daoyuan Wu, Wenxuan Wang, and Shing-Chi Cheung",
        "link": "http://arxiv.org/abs/2510.10460v1",
        "abstract": "Multi-agent systems (MASs) have emerged as a promising paradigm for automated\ncode generation, demonstrating impressive performance on established benchmarks\nby decomposing complex coding tasks across specialized agents with different\nroles. Despite their prosperous development and adoption, their robustness\nremains pressingly under-explored, raising critical concerns for real-world\ndeployment. This paper presents the first comprehensive study examining the\nrobustness of MASs for code generation through a fuzzing-based testing\napproach. By designing a fuzzing pipeline incorporating semantic-preserving\nmutation operators and a novel fitness function, we assess mainstream MASs\nacross multiple datasets and LLMs. Our findings reveal substantial robustness\nflaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they\ninitially resolved successfully after applying the semantic-preserving\nmutations. Through comprehensive failure analysis, we identify a common yet\nlargely overlooked cause of the robustness issue: miscommunications between\nplanning and coding agents, where plans lack sufficient detail and coding\nagents misinterpret intricate logic, aligning with the challenges inherent in a\nmulti-stage information transformation process. Accordingly, we also propose a\nrepairing method that encompasses multi-prompt generation and introduces a new\nmonitor agent to address this issue. Evaluation shows that our repairing method\neffectively enhances the robustness of MASs by solving 40.0%-88.9% of\nidentified failures. Our work uncovers critical robustness flaws in MASs and\nprovides effective mitigation strategies, contributing essential insights for\ndeveloping more reliable MASs for code generation."
    },
    {
        "date": "2025-10",
        "title": "Harnessing Consistency for Robust Test-Time LLM Ensemble",
        "author": "Zhichen Zeng, Qi Yu, Xiao Lin, Ruizhong Qiu, Xuying Ning, Tianxin Wei, Yuchen Yan, Jingrui He, and Hanghang Tong",
        "link": "http://arxiv.org/abs/2510.13855v1",
        "abstract": "Different large language models (LLMs) exhibit diverse strengths and\nweaknesses, and LLM ensemble serves as a promising approach to integrate their\ncomplementary capabilities. Despite substantial progress in improving ensemble\nquality, limited attention has been paid to the robustness of ensembles against\npotential erroneous signals, which often arise from heterogeneous tokenization\nschemes and varying model expertise. Our analysis shows that ensemble failures\ntypically arise from both the token level and the model level: the former\nreflects severe disagreement in token predictions, while the latter involves\nlow confidence and pronounced disparities among models. In light of this, we\npropose CoRE, a plug-and-play technique that harnesses model consistency for\nrobust LLM ensemble, which can be seamlessly integrated with diverse ensemble\nmethods. Token-level consistency captures fine-grained disagreements by\napplying a low-pass filter to downweight uncertain tokens with high\ninconsistency, often due to token misalignment, thereby improving robustness at\na granular level. Model-level consistency models global agreement by promoting\nmodel outputs with high self-confidence and minimal divergence from others,\nenhancing robustness at a coarser level. Extensive experiments across diverse\nbenchmarks, model combinations, and ensemble strategies demonstrate that CoRE\nconsistently improves ensemble performance and robustness."
    },
    {
        "date": "2025-10",
        "title": "Post-Quantum Cryptography and Quantum-Safe Security: A Comprehensive Survey",
        "author": "Gaurab Chhetri, Shriyank Somvanshi, Pavan Hebli, Shamyo Brotee, and Subasish Das",
        "link": "http://arxiv.org/abs/2510.10436v1",
        "abstract": "Post-quantum cryptography (PQC) is moving from evaluation to deployment as\nNIST finalizes standards for ML-KEM, ML-DSA, and SLH-DSA. This survey maps the\nspace from foundations to practice. We first develop a taxonomy across\nlattice-, code-, hash-, multivariate-, isogeny-, and MPC-in-the-Head families,\nsummarizing security assumptions, cryptanalysis, and standardization status. We\nthen compare performance and communication costs using representative,\nimplementation-grounded measurements, and review hardware acceleration (AVX2,\nFPGA/ASIC) and implementation security with a focus on side-channel resistance.\nBuilding upward, we examine protocol integration (TLS, DNSSEC), PKI and\ncertificate hygiene, and deployment in constrained and high-assurance\nenvironments (IoT, cloud, finance, blockchain). We also discuss complementarity\nwith quantum technologies (QKD, QRNGs) and the limits of near-term quantum\ncomputing. Throughout, we emphasize crypto-agility, hybrid migration, and\nevidence-based guidance for operators. We conclude with open problems spanning\nparameter agility, leakage-resilient implementations, and domain-specific\nrollout playbooks. This survey aims to be a practical reference for researchers\nand practitioners planning quantum-safe systems, bridging standards,\nengineering, and operations."
    }
]