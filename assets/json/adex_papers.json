[
    {
        "date": "2025-05",
        "title": "From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching",
        "author": "Ruqi Bai, Yao Ji, Zeyu Zhou, and David I. Inouye",
        "link": "http://arxiv.org/abs/2505.24843v1",
        "abstract": "Spurious correlations can cause model performance to degrade in new\nenvironments. Prior causality-inspired works aim to learn invariant\nrepresentations (e.g., IRM) but typically underperform empirical risk\nminimization (ERM). Recent alternatives improve robustness by leveraging\ntest-time data, but such data may be unavailable in practice. To address these\nissues, we take a data-centric approach by leveraging invariant data pairs,\npairs of samples that would have the same prediction with the optimally robust\nclassifier. We prove that certain counterfactual pairs will naturally satisfy\nthis invariance property and introduce noisy counterfactual matching (NCM), a\nsimple constraint-based method for leveraging invariant pairs for enhanced\nrobustness, even with a small set of noisy pairs-in the ideal case, each pair\ncan eliminate one spurious feature. For linear causal models, we prove that the\ntest domain error can be upper bounded by the in-domain error and a term that\ndepends on the counterfactuals' diversity and quality. We validate on a\nsynthetic dataset and demonstrate on real-world benchmarks that linear probing\non a pretrained backbone improves robustness."
    },
    {
        "date": "2025-05",
        "title": "Cascading Adversarial Bias from Injection to Distillation in Language Models",
        "author": "Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, and Alina Oprea",
        "link": "http://arxiv.org/abs/2505.24842v1",
        "abstract": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies."
    },
    {
        "date": "2025-05",
        "title": "ByzFL: Research Framework for Robust Federated Learning",
        "author": "Marc Gonz\u00e1lez, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, and Fran\u00e7ois Ta\u00efani",
        "link": "http://arxiv.org/abs/2505.24802v1",
        "abstract": "We present ByzFL, an open-source Python library for developing and\nbenchmarking robust federated learning (FL) algorithms. ByzFL provides a\nunified and extensible framework that includes implementations of\nstate-of-the-art robust aggregators, a suite of configurable attacks, and tools\nfor simulating a variety of FL scenarios, including heterogeneous data\ndistributions, multiple training algorithms, and adversarial threat models. The\nlibrary enables systematic experimentation via a single JSON-based\nconfiguration file and includes built-in utilities for result visualization.\nCompatible with PyTorch tensors and NumPy arrays, ByzFL is designed to\nfacilitate reproducible research and rapid prototyping of robust FL solutions.\nByzFL is available at https://byzfl.epfl.ch/, with source code hosted on\nGitHub: https://github.com/LPD-EPFL/byzfl."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning against Model Perturbation in Edge Networks",
        "author": "Dongzi Jin, Yong Xiao, and Yingyu Li",
        "link": "http://arxiv.org/abs/2505.24728v1",
        "abstract": "Federated Learning (FL) is a promising paradigm for realizing edge\nintelligence, allowing collaborative learning among distributed edge devices by\nsharing models instead of raw data. However, the shared models are often\nassumed to be ideal, which would be inevitably violated in practice due to\nvarious perturbations, leading to significant performance degradation. To\novercome this challenge, we propose a novel method, termed Sharpness-Aware\nMinimization-based Robust Federated Learning (SMRFL), which aims to improve\nmodel robustness against perturbations by exploring the geometrical property of\nthe model landscape. Specifically, SMRFL solves a min-max optimization problem\nthat promotes model convergence towards a flat minimum by minimizing the\nmaximum loss within a neighborhood of the model parameters. In this way, model\nsensitivity to perturbations is reduced, and robustness is enhanced since\nmodels in the neighborhood of the flat minimum also enjoy low loss values. The\ntheoretical result proves that SMRFL can converge at the same rate as FL\nwithout perturbations. Extensive experimental results show that SMRFL\nsignificantly enhances robustness against perturbations compared to three\nbaseline methods on two real-world datasets under three perturbation scenarios."
    },
    {
        "date": "2025-05",
        "title": "On Symmetric Losses for Robust Policy Optimization with Noisy Preferences",
        "author": "Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2505.24709v1",
        "abstract": "Optimizing policies based on human preferences is key to aligning language\nmodels with human intent. This work focuses on reward modeling, a core\ncomponent in reinforcement learning from human feedback (RLHF), and offline\npreference optimization, such as direct preference optimization. Conventional\napproaches typically assume accurate annotations. However, real-world\npreference data often contains noise due to human errors or biases. We propose\na principled framework for robust policy optimization under noisy preferences,\nviewing reward modeling as a classification problem. This allows us to leverage\nsymmetric losses, known for their robustness to label noise in classification,\nleading to our Symmetric Preference Optimization (SymPO) method. We prove that\nsymmetric losses enable successful policy optimization even under noisy labels,\nas the resulting reward remains rank-preserving -- a property sufficient for\npolicy improvement. Experiments on synthetic and real-world tasks demonstrate\nthe effectiveness of SymPO."
    },
    {
        "date": "2025-05",
        "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches",
        "author": "Dennis Jacob, Chong Xiang, and Prateek Mittal",
        "link": "http://arxiv.org/abs/2505.24703v1",
        "abstract": "Deep learning techniques have enabled vast improvements in computer vision\ntechnologies. Nevertheless, these models are vulnerable to adversarial patch\nattacks which catastrophically impair performance. The physically realizable\nnature of these attacks calls for certifiable defenses, which feature provable\nguarantees on robustness. While certifiable defenses have been successfully\napplied to single-label classification, limited work has been done for\nmulti-label classification. In this work, we present PatchDEMUX, a certifiably\nrobust framework for multi-label classifiers against adversarial patches. Our\napproach is a generalizable method which can extend any existing certifiable\ndefense for single-label classification; this is done by considering the\nmulti-label classification task as a series of isolated binary classification\nproblems to provably guarantee robustness. Furthermore, in the scenario where\nan attacker is limited to a single patch we propose an additional certification\nprocedure that can provide tighter robustness bounds. Using the current\nstate-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a\nbackbone, we find that PatchDEMUX can achieve non-trivial robustness on the\nMS-COCO and PASCAL VOC datasets while maintaining high clean performance"
    },
    {
        "date": "2025-05",
        "title": "Black-box Adversarial Attacks on CNN-based SLAM Algorithms",
        "author": "Maria Rafaela Gkeka, Bowen Sun, Evgenia Smirni, Christos D. Antonopoulos, Spyros Lalis, and Nikolaos Bellas",
        "link": "http://arxiv.org/abs/2505.24654v1",
        "abstract": "Continuous advancements in deep learning have led to significant progress in\nfeature detection, resulting in enhanced accuracy in tasks like Simultaneous\nLocalization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural\nnetworks to adversarial attacks remains a challenge for their reliable\ndeployment in applications, such as navigation of autonomous agents. Even\nthough CNN-based SLAM algorithms are a growing area of research there is a\nnotable absence of a comprehensive presentation and examination of adversarial\nattacks targeting CNN-based feature detectors, as part of a SLAM system. Our\nwork introduces black-box adversarial perturbations applied to the RGB images\nfed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal\nthat even attacks of moderate scale can lead to tracking failure in as many as\n76% of the frames. Moreover, our experiments highlight the catastrophic impact\nof attacking depth instead of RGB input images on the SLAM system."
    },
    {
        "date": "2025-05",
        "title": "A Flat Minima Perspective on Understanding Augmentations and Model Robustness",
        "author": "Weebum Yoo, and Sung Whan Yoon",
        "link": "http://arxiv.org/abs/2505.24592v1",
        "abstract": "Model robustness indicates a model's capability to generalize well on\nunforeseen distributional shifts, including data corruption, adversarial\nattacks, and domain shifts. Data augmentation is one of the prevalent and\neffective ways to enhance robustness. Despite the great success of\naugmentations in different fields, a general theoretical understanding of their\nefficacy in improving model robustness is lacking. We offer a unified\ntheoretical framework to clarify how augmentations can enhance model robustness\nthrough the lens of loss surface flatness and PAC generalization bound. Our\nwork diverges from prior studies in that our analysis i) broadly encompasses\nmuch of the existing augmentation methods, and ii) is not limited to specific\ntypes of distribution shifts like adversarial attacks. We confirm our theories\nthrough simulations on the existing common corruption and adversarial\nrobustness benchmarks based on the CIFAR and ImageNet datasets, as well as\ndomain generalization benchmarks including PACS and OfficeHome."
    },
    {
        "date": "2025-05",
        "title": "CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control",
        "author": "Chaohui Xu, Qi Cui, and Chip-Hong Chang",
        "link": "http://arxiv.org/abs/2505.24536v1",
        "abstract": "The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous\ntraining costs make their intellectual property (IP) protection of paramount\nimportance. Recently introduced passport-based methods attempt to steer DNN\nwatermarking towards strengthening ownership verification against ambiguity\nattacks by modulating the affine parameters of normalization layers.\nUnfortunately, neither watermarking nor passport-based methods provide a\nholistic protection with robust ownership proof, high fidelity, active usage\nauthorization and user traceability for offline access distributed models and\nmulti-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we\npropose a Chameleon Hash-based Irreversible Passport (CHIP) protection\nframework that utilizes the cryptographic chameleon hash function to achieve\nall these goals. The collision-resistant property of chameleon hash allows for\nstrong model ownership claim upon IP infringement and liable user traceability,\nwhile the trapdoor-collision property enables hashing of multiple user\npassports and licensee certificates to the same immutable signature to realize\nactive usage control. Using the owner passport as an oracle, multiple\nuser-specific triplets, each contains a passport-aware user model, a user\npassport, and a licensee certificate can be created for secure offline\ndistribution. The watermarked master model can also be deployed for MLaaS with\nusage permission verifiable by the provision of any trapdoor-colliding user\npassports. CHIP is extensively evaluated on four datasets and two architectures\nto demonstrate its protection versatility and robustness. Our code is released\nat https://github.com/Dshm212/CHIP."
    },
    {
        "date": "2025-05",
        "title": "AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders",
        "author": "Yuqi Zhang, Yuchun Miao, Zuchao Li, and Liang Ding",
        "link": "http://arxiv.org/abs/2505.24519v1",
        "abstract": "We introduce AMIA, a lightweight, inference-only defense for Large\nVision-Language Models (LVLMs) that (1) Automatically Masks a small set of\ntext-irrelevant image patches to disrupt adversarial perturbations, and (2)\nconducts joint Intention Analysis to uncover and mitigate hidden harmful\nintents before response generation. Without any retraining, AMIA improves\ndefense success rates across diverse LVLMs and jailbreak benchmarks from an\naverage of 52.4% to 81.7%, preserves general utility with only a 2% average\naccuracy drop, and incurs only modest inference overhead. Ablation confirms\nboth masking and intention analysis are essential for a robust safety-utility\ntrade-off."
    },
    {
        "date": "2025-05",
        "title": "Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers",
        "author": "Heejo Kong, Sung-Jin Kim, Gunho Jung, and Seong-Whan Lee",
        "link": "http://arxiv.org/abs/2505.24443v1",
        "abstract": "Conventional semi-supervised learning (SSL) ideally assumes that labeled and\nunlabeled data share an identical class distribution, however in practice, this\nassumption is easily violated, as unlabeled data often includes unknown class\ndata, i.e., outliers. The outliers are treated as noise, considerably degrading\nthe performance of SSL models. To address this drawback, we propose a novel\nframework, Diversify and Conquer (DAC), to enhance SSL robustness in the\ncontext of open-set semi-supervised learning. In particular, we note that\nexisting open-set SSL methods rely on prediction discrepancies between inliers\nand outliers from a single model trained on labeled data. This approach can be\neasily failed when the labeled data is insufficient, leading to performance\ndegradation that is worse than naive SSL that do not account for outliers. In\ncontrast, our approach exploits prediction disagreements among multiple models\nthat are differently biased towards the unlabeled distribution. By leveraging\nthe discrepancies arising from training on unlabeled data, our method enables\nrobust outlier detection even when the labeled data is underspecified. Our key\ncontribution is constructing a collection of differently biased models through\na single training process. By encouraging divergent heads to be differently\nbiased towards outliers while making consistent predictions for inliers, we\nexploit the disagreement among these heads as a measure to identify unknown\nconcepts. Our code is available at https://github.com/heejokong/DivCon."
    },
    {
        "date": "2025-05",
        "title": "pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation",
        "author": "Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, and Kannie W. Y. Chan",
        "link": "http://arxiv.org/abs/2505.24421v1",
        "abstract": "Medical imaging is critical for diagnostics, but clinical adoption of\nadvanced AI-driven imaging faces challenges due to patient variability, image\nartifacts, and limited model generalization. While deep learning has\ntransformed image analysis, 3D medical imaging still suffers from data scarcity\nand inconsistencies due to acquisition protocols, scanner differences, and\npatient motion. Traditional augmentation uses a single pipeline for all\ntransformations, disregarding the unique traits of each augmentation and\nstruggling with large data volumes.\n  To address these challenges, we propose a Multi-encoder Augmentation-Aware\nLearning (MEAL) framework that leverages four distinct augmentation variants\nprocessed through dedicated encoders. Three fusion strategies such as\nconcatenation (CC), fusion layer (FL), and adaptive controller block (BD) are\nintegrated to build multi-encoder models that combine augmentation-specific\nfeatures before decoding. MEAL-BD uniquely preserves augmentation-aware\nrepresentations, enabling robust, protocol-invariant feature learning.\n  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic\nResonance Imaging (MRI) translation study, MEAL-BD consistently achieved the\nbest performance on both unseen- and predefined-test data. On both geometric\ntransformations (like rotations and flips) and non-augmented inputs, MEAL-BD\noutperformed other competing methods, achieving higher mean peak\nsignal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)\nscores. These results establish MEAL as a reliable framework for preserving\nstructural fidelity and generalizing across clinically relevant variability. By\nreframing augmentation as a source of diverse, generalizable features, MEAL\nsupports robust, protocol-invariant learning, advancing clinically reliable\nmedical imaging solutions."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Preference Learning for Robust LLM Alignment",
        "author": "Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, and Mingchuan Yang",
        "link": "http://arxiv.org/abs/2505.24369v1",
        "abstract": "Modern language models often rely on Reinforcement Learning from Human\nFeedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to\nadversarial attacks due to three key limitations: (1) the inefficiency and high\ncost of human annotation, (2) the vast diversity of potential adversarial\nattacks, and (3) the risk of feedback bias and reward hacking. To address these\nchallenges, we introduce Adversarial Preference Learning (APL), an iterative\nadversarial training method incorporating three key innovations. First, a\ndirect harmfulness metric based on the model's intrinsic preference\nprobabilities, eliminating reliance on external assessment. Second, a\nconditional generative attacker that synthesizes input-specific adversarial\nvariations. Third, an iterative framework with automated closed-loop feedback,\nenabling continuous adaptation through vulnerability discovery and mitigation.\nExperiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly\nenhances robustness, achieving 83.33% harmlessness win rate over the base model\n(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured\nby LLaMA-Guard), and lowering attack success rate by up to 65% according to\nHarmBench. Notably, APL maintains competitive utility, with an MT-Bench score\nof 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against\nthe base model."
    },
    {
        "date": "2025-05",
        "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations",
        "author": "Xin Quan, Marco Valentino, Louise A. Dennis, and Andr\u00e9 Freitas",
        "link": "http://arxiv.org/abs/2505.24264v1",
        "abstract": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification."
    },
    {
        "date": "2025-05",
        "title": "Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization",
        "author": "Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, and Hongbin Liu",
        "link": "http://arxiv.org/abs/2505.24249v1",
        "abstract": "Vision-based 6-DOF bronchoscopy localization offers a promising solution for\naccurate and cost-effective interventional guidance. However, existing methods\nstruggle with 1) limited generalization across patient cases due to scarce\nlabeled data, and 2) poor robustness under visual degradation, as bronchoscopy\nprocedures frequently involve artifacts such as occlusions and motion blur that\nimpair visual information. To address these challenges, we propose PANSv2, a\ngeneralizable and robust bronchoscopy localization framework. Motivated by PANS\nthat leverages multiple visual cues for pose likelihood measurement, PANSv2\nintegrates depth estimation, landmark detection, and centerline constraints\ninto a unified pose optimization framework that evaluates pose probability and\nsolves for the optimal bronchoscope pose. To further enhance generalization\ncapabilities, we leverage the endoscopic foundation model EndoOmni for depth\nestimation and the video foundation model EndoMamba for landmark detection,\nincorporating both spatial and temporal analyses. Pretrained on diverse\nendoscopic datasets, these models provide stable and transferable visual\nrepresentations, enabling reliable performance across varied bronchoscopy\nscenarios. Additionally, to improve robustness to visual degradation, we\nintroduce an automatic re-initialization module that detects tracking failures\nand re-establishes pose using landmark detections once clear views are\navailable. Experimental results on bronchoscopy dataset encompassing 10 patient\ncases show that PANSv2 achieves the highest tracking success rate, with an\n18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)\ncompared to existing methods, showing potential towards real clinical usage."
    },
    {
        "date": "2025-05",
        "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring",
        "author": "Sana Ebrahimi, Mohsen Dehghankar, and Abolfazl Asudeh",
        "link": "http://arxiv.org/abs/2505.24239v1",
        "abstract": "While multi-agent LLM systems show strong capabilities in various domains,\nthey are highly vulnerable to adversarial and low-performing agents. To resolve\nthis issue, in this paper, we introduce a general and adversary-resistant\nmulti-agent LLM framework based on credibility scoring. We model the\ncollaborative query-answering process as an iterative game, where the agents\ncommunicate and contribute to a final system output. Our system associates a\ncredibility score that is used when aggregating the team outputs. The\ncredibility scores are learned gradually based on the past contributions of\neach agent in query answering. Our experiments across multiple tasks and\nsettings demonstrate our system's effectiveness in mitigating adversarial\ninfluence and enhancing the resilience of multi-agent cooperation, even in the\nadversary-majority settings."
    },
    {
        "date": "2025-05",
        "title": "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap",
        "author": "Wenhan Yang, Spencer Stice, Ali Payani, and Baharan Mirzasoleiman",
        "link": "http://arxiv.org/abs/2505.24208v1",
        "abstract": "Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for\ntheir reliable deployment. However, LVLMs suffer from drastic safety\ndegradation compared to their LLM backbone. Even blank or irrelevant images can\ntrigger LVLMs to generate harmful responses to prompts that would otherwise be\nrefused in text-only contexts. The modality gap between image and text\nrepresentations has been recently hypothesized to contribute to safety\ndegradation of LVLMs. However, if and how the amount of modality gap affects\nLVLMs' safety is not studied. In this work, we show that the amount of modality\ngap is highly inversely correlated with VLMs' safety. Then, we show that this\nmodality gap is introduced during pretraining LVLMs and persists through\nfine-tuning. Inspired by this observation, we propose a regularization to\nreduce the modality gap during pretraining. Our extensive experiments on LLaVA\nv1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves\nsafety alignment of LVLMs, reducing unsafe rate by up to 16.3% without\ncompromising performance, and can further boost existing defenses by up to\n18.2%."
    },
    {
        "date": "2025-05",
        "title": "Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents",
        "author": "Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, and Jungseul Ok",
        "link": "http://arxiv.org/abs/2505.24157v1",
        "abstract": "Developing autonomous agents capable of mastering complex, multi-step tasks\nin unpredictable, interactive environments presents a significant challenge.\nWhile Large Language Models (LLMs) offer promise for planning, existing\napproaches often rely on problematic internal knowledge or make unrealistic\nenvironmental assumptions. Although recent work explores learning planning\nknowledge, they still retain limitations due to partial reliance on external\nknowledge or impractical setups. Indeed, prior research has largely overlooked\ndeveloping agents capable of acquiring planning knowledge from scratch,\ndirectly in realistic settings. While realizing this capability is necessary,\nit presents significant challenges, primarily achieving robustness given the\nsubstantial risk of incorporating LLMs' inaccurate knowledge. Moreover,\nefficiency is crucial for practicality as learning can demand prohibitive\nexploration. In response, we introduce Robust and Efficient Planning for\nOpen-world Agents (REPOA), a novel framework designed to tackle these issues.\nREPOA features three key components: adaptive dependency learning and\nfine-grained failure-aware operation memory to enhance robustness to knowledge\ninaccuracies, and difficulty-based exploration to improve learning efficiency.\nOur evaluation in two established open-world testbeds demonstrates REPOA's\nrobust and efficient planning, showcasing its capability to successfully obtain\nchallenging late-game items that were beyond the reach of prior approaches."
    },
    {
        "date": "2025-05",
        "title": "The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models",
        "author": "Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, and Yefeng Zheng",
        "link": "http://arxiv.org/abs/2505.24141v1",
        "abstract": "With the widespread adoption of pathology foundation models in both research\nand clinical decision support systems, exploring their security has become a\ncritical concern. However, despite their growing impact, the vulnerability of\nthese models to adversarial attacks remains largely unexplored. In this work,\nwe present the first systematic investigation into the security of pathology\nfoundation models for whole slide image~(WSI) analysis against adversarial\nattacks. Specifically, we introduce the principle of \\textit{local perturbation\nwith global impact} and propose a label-free attack framework that operates\nwithout requiring access to downstream task labels. Under this attack\nframework, we revise four classical white-box attack methods and redefine the\nperturbation budget based on the characteristics of WSI. We conduct\ncomprehensive experiments on three representative pathology foundation models\nacross five datasets and six downstream tasks. Despite modifying only 0.1\\% of\npatches per slide with imperceptible noise, our attack leads to downstream\naccuracy degradation that can reach up to 20\\% in the worst cases. Furthermore,\nwe analyze key factors that influence attack success, explore the relationship\nbetween patch-level vulnerability and semantic content, and conduct a\npreliminary investigation into potential defence strategies. These findings lay\nthe groundwork for future research on the adversarial robustness and reliable\ndeployment of pathology foundation models. Our code is publicly available at:\nhttps://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models."
    },
    {
        "date": "2025-05",
        "title": "Practical Bayes-Optimal Membership Inference Attacks",
        "author": "Marcus Lassila, Johan \u00d6stman, Khac-Hoang Ngo, and Alexandre Graell i Amat",
        "link": "http://arxiv.org/abs/2505.24089v1",
        "abstract": "We develop practical and theoretically grounded membership inference attacks\n(MIAs) against both independent and identically distributed (i.i.d.) data and\ngraph-structured data. Building on the Bayesian decision-theoretic framework of\nSablayrolles et al., we derive the Bayes-optimal membership inference rule for\nnode-level MIAs against graph neural networks, addressing key open questions\nabout optimal query strategies in the graph setting. We introduce BASE and\nG-BASE, computationally efficient approximations of the Bayes-optimal attack.\nG-BASE achieves superior performance compared to previously proposed\nclassifier-based node-level MIA attacks. BASE, which is also applicable to\nnon-graph data, matches or exceeds the performance of prior state-of-the-art\nMIAs, such as LiRA and RMIA, at a significantly lower computational cost.\nFinally, we show that BASE and RMIA are equivalent under a specific\nhyperparameter setting, providing a principled, Bayes-optimal justification for\nthe RMIA attack."
    },
    {
        "date": "2025-05",
        "title": "DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals",
        "author": "Alireza Jafari, Fereshteh Yousefirizi, and Vahid Seydi",
        "link": "http://arxiv.org/abs/2505.24085v1",
        "abstract": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with\nelevated health risks, where timely detection is pivotal for mitigating\nstroke-related morbidity. This study introduces an innovative hybrid\nmethodology integrating unsupervised deep learning and gradient boosting models\nto improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is\ncoupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM\n(LGBM)-to harness their complementary advantages while addressing individual\nlimitations. The proposed framework uniquely combines DCAE with gradient\nboosting, enabling end-to-end AF identification devoid of manual feature\nextraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of\n99.99%, and inference latency of four seconds, outperforming existing methods\nand aligning with clinical deployment requirements. The DCAE integration\nsignificantly enhances boosting models, positioning this hybrid system as a\nreliable tool for automated AF detection in clinical settings."
    },
    {
        "date": "2025-05",
        "title": "An Advanced Cyber-Physical System Security Testbed for Substation Automation",
        "author": "Akila Herath, Chen-Ching Liu, Junho Hong, and Mansi Girdhar",
        "link": "http://arxiv.org/abs/2505.24021v1",
        "abstract": "A Cyber-Physical System (CPS) testbed serves as a powerful platform for\ntesting and validating cyber intrusion detection and mitigation strategies in\nsubstations. This study presents the design and development of a CPS testbed\nthat can effectively assess the real-time dynamics of a substation. Cyber\nattacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated\nusing the testbed, along with an analysis on attack detection. Realistic timing\nmeasurements are obtained, and the time frames for deploying detection and\nmitigation strategies are evaluated."
    },
    {
        "date": "2025-05",
        "title": "LLM Agents Should Employ Security Principles",
        "author": "Kaiyuan Zhang, Zian Su, Pin-Yu Chen, Elisa Bertino, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2505.24019v1",
        "abstract": "Large Language Model (LLM) agents show considerable promise for automating\ncomplex tasks using contextual reasoning; however, interactions involving\nmultiple agents and the system's susceptibility to prompt injection and other\nforms of context manipulation introduce new vulnerabilities related to privacy\nleakage and system exploitation. This position paper argues that the\nwell-established design principles in information security, which are commonly\nreferred to as security principles, should be employed when deploying LLM\nagents at scale. Design principles such as defense-in-depth, least privilege,\ncomplete mediation, and psychological acceptability have helped guide the\ndesign of mechanisms for securing information systems over the last five\ndecades, and we argue that their explicit and conscientious adoption will help\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\na conceptual framework embedding these security principles to provide\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\nLLMs along three dimensions: benign utility, attack utility, and attack success\nrate. AgentSandbox maintains high utility for its intended functions under both\nbenign and adversarial evaluations while substantially mitigating privacy\nrisks. By embedding secure design principles as foundational elements within\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\naligned with user privacy expectations and evolving regulatory requirements."
    },
    {
        "date": "2025-05",
        "title": "SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations",
        "author": "Gaurav Sarkar, Jay Gala, and Subarna Tripathi",
        "link": "http://arxiv.org/abs/2505.23942v1",
        "abstract": "The design of activation functions remains a pivotal component in optimizing\ndeep neural networks. While prevailing choices like Swish and GELU demonstrate\nconsiderable efficacy, they often exhibit domain-specific optima. This work\nintroduces SG-Blend, a novel activation function that blends our proposed\nSSwish, a first-order symmetric variant of Swish and the established GELU\nthrough dynamic interpolation. By adaptively blending these constituent\nfunctions via learnable parameters, SG-Blend aims to harness their\ncomplementary strengths: SSwish's controlled non-monotonicity and symmetry, and\nGELU's smooth, probabilistic profile, to achieve a more universally robust\nbalance between model expressivity and gradient stability. We conduct\ncomprehensive empirical evaluations across diverse modalities and\narchitectures, showing performance improvements across all considered natural\nlanguage and computer vision tasks and models. These results, achieved with\nnegligible computational overhead, underscore SG-Blend's potential as a\nversatile, drop-in replacement that consistently outperforms strong\ncontemporary baselines. The code is available at\nhttps://anonymous.4open.science/r/SGBlend-6CBC."
    },
    {
        "date": "2025-05",
        "title": "FMG-Det: Foundation Model Guided Robust Object Detection",
        "author": "Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, and Yijing Watkins",
        "link": "http://arxiv.org/abs/2505.23726v1",
        "abstract": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches."
    },
    {
        "date": "2025-05",
        "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
        "author": "Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, and Mengran Xue",
        "link": "http://arxiv.org/abs/2505.23706v1",
        "abstract": "In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks."
    },
    {
        "date": "2025-05",
        "title": "Synopsis: Secure and private trend inference from encrypted semantic embeddings",
        "author": "Madelyne Xiao, Palak Jain, Micha Gorelick, and Sarah Scheffler",
        "link": "http://arxiv.org/abs/2505.23880v1",
        "abstract": "WhatsApp and many other commonly used communication platforms guarantee\nend-to-end encryption (E2EE), which requires that service providers lack the\ncryptographic keys to read communications on their own platforms. WhatsApp's\nprivacy-preserving design makes it difficult to study important phenomena like\nthe spread of misinformation or political messaging, as users have a clear\nexpectation and desire for privacy and little incentive to forfeit that privacy\nin the process of handing over raw data to researchers, journalists, or other\nparties.\n  We introduce Synopsis, a secure architecture for analyzing messaging trends\nin consensually-donated E2EE messages using message embeddings. Since the goal\nof this system is investigative journalism workflows, Synopsis must facilitate\nboth exploratory and targeted analyses -- a challenge for systems using\ndifferential privacy (DP), and, for different reasons, a challenge for private\ncomputation approaches based on cryptography. To meet these challenges, we\ncombine techniques from the local and central DP models and wrap the system in\nmalicious-secure multi-party computation to ensure the DP query architecture is\nthe only way to access messages, preventing any party from directly viewing\nstored message embeddings.\n  Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages\nrepresented as 500-dimensional embeddings) demonstrate the efficiency and\naccuracy of our approach. Queries on this data run in about 30 seconds, and the\naccuracy of the fine-grained interface exceeds 94% on benchmark tasks."
    },
    {
        "date": "2025-05",
        "title": "Securing AI Agents with Information-Flow Control",
        "author": "Manuel Costa, Boris K\u00f6pf, Aashish Kolluri, Andrew Paverd, Mark Russinovich, Ahmed Salem, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B\u00e9guelin",
        "link": "http://arxiv.org/abs/2505.23643v1",
        "abstract": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides"
    },
    {
        "date": "2025-05",
        "title": "DRO: A Python Library for Distributionally Robust Optimization in Machine Learning",
        "author": "Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, and Jose Blanchet",
        "link": "http://arxiv.org/abs/2505.23565v1",
        "abstract": "We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org."
    },
    {
        "date": "2025-05",
        "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
        "author": "Zenghui Yuan, Yangming Xu, Jiawen Shi, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.23561v1",
        "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."
    },
    {
        "date": "2025-05",
        "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
        "author": "Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, and Seyit Camtepe",
        "link": "http://arxiv.org/abs/2505.23397v2",
        "abstract": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making."
    },
    {
        "date": "2025-05",
        "title": "Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment",
        "author": "Yun-Cheng Tsai",
        "link": "http://arxiv.org/abs/2505.23392v1",
        "abstract": "Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment."
    },
    {
        "date": "2025-05",
        "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning",
        "author": "Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, and Xiu Li",
        "link": "http://arxiv.org/abs/2505.23871v1",
        "abstract": "Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results."
    },
    {
        "date": "2025-05",
        "title": "Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert",
        "author": "Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou. Libo Qin, and Wenhong Tian",
        "link": "http://arxiv.org/abs/2505.23868v1",
        "abstract": "Current parameter-efficient fine-tuning methods for adapting pre-trained\nlanguage models to downstream tasks are susceptible to interference from noisy\ndata. Conventional noise-handling approaches either rely on laborious data\npre-processing or employ model architecture modifications prone to error\naccumulation. In contrast to existing noise-process paradigms, we propose a\nnoise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a\nnovel framework that enhances model robustness to noise only with generated\nnoisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE\nstrategically integrates a dedicated poisoning expert in an asymmetric LoRA\nconfiguration. Through a two-stage paradigm, LoPE performs noise injection on\nthe poisoning expert during fine-tuning to enhance its noise discrimination and\nprocessing ability. During inference, we selectively mask the dedicated\npoisoning expert to leverage purified knowledge acquired by normal experts for\nnoise-robust output. Extensive experiments demonstrate that LoPE achieves\nstrong performance and robustness purely through the low-cost noise injection,\nwhich completely eliminates the requirement of data cleaning."
    },
    {
        "date": "2025-05",
        "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis",
        "author": "Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang",
        "link": "http://arxiv.org/abs/2505.23325v1",
        "abstract": "Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/."
    },
    {
        "date": "2025-05",
        "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation",
        "author": "Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, and Hongxia Yang",
        "link": "http://arxiv.org/abs/2505.23867v1",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition",
        "author": "Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, and Jin Tang",
        "link": "http://arxiv.org/abs/2505.23313v1",
        "abstract": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR."
    },
    {
        "date": "2025-05",
        "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
        "author": "Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, and Tao Xiang",
        "link": "http://arxiv.org/abs/2505.23266v1",
        "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
        "author": "Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, and Lei Xie",
        "link": "http://arxiv.org/abs/2505.23207v1",
        "abstract": "Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD."
    },
    {
        "date": "2025-05",
        "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
        "author": "Run Hao, and Peng Ying",
        "link": "http://arxiv.org/abs/2505.23192v1",
        "abstract": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems."
    },
    {
        "date": "2025-05",
        "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
        "author": "Junyan Liu, Arnab Maiti, Artin Tajdini, Kevin Jamieson, and Lillian J. Ratliff",
        "link": "http://arxiv.org/abs/2505.23124v1",
        "abstract": "We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round."
    },
    {
        "date": "2025-05",
        "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
        "author": "Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, and Yanmin Qian",
        "link": "http://arxiv.org/abs/2505.23049v1",
        "abstract": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator."
    },
    {
        "date": "2025-05",
        "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
        "author": "Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, and Rahul G. Krishnan",
        "link": "http://arxiv.org/abs/2505.23027v1",
        "abstract": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop"
    },
    {
        "date": "2025-05",
        "title": "Context-Robust Knowledge Editing for Language Models",
        "author": "Haewon Park, Gyubin Choi, Minjun Kim, and Yohan Jo",
        "link": "http://arxiv.org/abs/2505.23026v2",
        "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success."
    },
    {
        "date": "2025-05",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning",
        "author": "Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, and Sunil Gupta",
        "link": "http://arxiv.org/abs/2505.23003v1",
        "abstract": "Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL."
    },
    {
        "date": "2025-05",
        "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates",
        "author": "Jaewoo Ahn, Heeseung Yun, Dayoon Ko, and Gunhee Kim",
        "link": "http://arxiv.org/abs/2505.22943v1",
        "abstract": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios."
    },
    {
        "date": "2025-05",
        "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging",
        "author": "Haobo Zhang, and Jiayu Zhou",
        "link": "http://arxiv.org/abs/2505.22934v1",
        "abstract": "Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models."
    },
    {
        "date": "2025-05",
        "title": "Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints",
        "author": "Hyungki Im, Wyame Benslimane, and Paul Grigas",
        "link": "http://arxiv.org/abs/2505.22881v1",
        "abstract": "We study an extension of contextual stochastic linear optimization (CSLO)\nthat, in contrast to most of the existing literature, involves inequality\nconstraints that depend on uncertain parameters predicted by a machine learning\nmodel. To handle the constraint uncertainty, we use contextual uncertainty sets\nconstructed via methods like conformal prediction. Given a contextual\nuncertainty set method, we introduce the \"Smart Predict-then-Optimize with\nRobust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the\nSPO loss that measures decision error of predicted objective parameters. We\nalso introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with\nSPO-RC. To enhance performance, we train on truncated datasets where true\nconstraint parameters lie within the uncertainty sets, and we correct the\ninduced sample selection bias using importance reweighting techniques. Through\nexperiments on fractional knapsack and alloy production problem instances, we\ndemonstrate that SPO-RC+ effectively handles uncertainty in constraints and\nthat combining truncation with importance reweighting can further improve\nperformance."
    },
    {
        "date": "2025-05",
        "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
        "author": "Krti Tallam, and Emma Miller",
        "link": "http://arxiv.org/abs/2505.22852v1",
        "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based\nsandbox to mitigate prompt injection attacks in large language model (LLM)\nagents. While effective, CaMeL assumes a trusted user prompt, omits\nside-channel concerns, and incurs performance tradeoffs due to its dual-LLM\ndesign. This response identifies these issues and proposes engineering\nimprovements to expand CaMeL's threat coverage and operational usability. We\nintroduce: (1) prompt screening for initial inputs, (2) output auditing to\ndetect instruction leakage, (3) a tiered-risk access model to balance usability\nand control, and (4) a verified intermediate language for formal guarantees.\nTogether, these upgrades align CaMeL with best practices in enterprise security\nand support scalable deployment."
    },
    {
        "date": "2025-05",
        "title": "Security Benefits and Side Effects of Labeling AI-Generated Images",
        "author": "Sandra H\u00f6ltervennhoff, Jonas Ricker, Maike M. Raphael, Charlotte Schwedes, Rebecca Weil, Asja Fischer, Thorsten Holz, Lea Sch\u00f6nherr, and Sascha Fahl",
        "link": "http://arxiv.org/abs/2505.22845v1",
        "abstract": "Generative artificial intelligence is developing rapidly, impacting humans'\ninteraction with information and digital media. It is increasingly used to\ncreate deceptively realistic misinformation, so lawmakers have imposed\nregulations requiring the disclosure of AI-generated content. However, only\nlittle is known about whether these labels reduce the risks of AI-generated\nmisinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we\nstudy the implications of labels, including the possibility of mislabeling.\nAssuming that simplicity, transparency, and trust are likely to impact the\nsuccessful adoption of such labels, we first qualitatively explore users'\nopinions and expectations of AI labeling using five focus groups. Second, we\nconduct a pre-registered online survey with over 1300 U.S. and EU participants\nto quantitatively assess the effect of AI labels on users' ability to recognize\nmisinformation containing either human-made or AI-generated images. Our focus\ngroups illustrate that, while participants have concerns about the practical\nimplementation of labeling, they consider it helpful in identifying\nAI-generated images and avoiding deception. However, considering security\nbenefits, our survey revealed an ambiguous picture, suggesting that users might\nover-rely on labels. While inaccurate claims supported by labeled AI-generated\nimages were rated less credible than those with unlabeled AI-images, the belief\nin accurate claims also decreased when accompanied by a labeled AI-generated\nimage. Moreover, we find the undesired side effect that human-made images\nconveying inaccurate claims were perceived as more credible in the presence of\nlabels."
    },
    {
        "date": "2025-05",
        "title": "How Do Diffusion Models Improve Adversarial Robustness?",
        "author": "Liu Yuezhang, and Xue-Xin Wei",
        "link": "http://arxiv.org/abs/2505.22839v1",
        "abstract": "Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems."
    },
    {
        "date": "2025-05",
        "title": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems",
        "author": "Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, and Wonyong Shin",
        "link": "http://arxiv.org/abs/2505.23847v1",
        "abstract": "Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines."
    },
    {
        "date": "2025-05",
        "title": "Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook",
        "author": "Banafsheh Saber Latibari, Najmeh Nazari, Avesta Sasan, Houman Homayoun, Pratik Satam, Soheil Salehi, and Hossein Sayadi",
        "link": "http://arxiv.org/abs/2505.22605v1",
        "abstract": "The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective",
        "author": "Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, and Meng Wang",
        "link": "http://arxiv.org/abs/2505.22604v2",
        "abstract": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."
    },
    {
        "date": "2025-05",
        "title": "Training RL Agents for Multi-Objective Network Defense Tasks",
        "author": "Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, and Ahmed Ridley",
        "link": "http://arxiv.org/abs/2505.22531v1",
        "abstract": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."
    },
    {
        "date": "2025-05",
        "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas",
        "author": "Agnideep Aich, Ashit Baran Aich, and Bruce Wade",
        "link": "http://arxiv.org/abs/2505.22518v1",
        "abstract": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."
    },
    {
        "date": "2025-05",
        "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector",
        "author": "Aixuan Li, Mochu Xiang, Jing Zhang, and Yuchao Dai",
        "link": "http://arxiv.org/abs/2505.22499v2",
        "abstract": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."
    },
    {
        "date": "2025-05",
        "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods",
        "author": "Michal Kmicikiewicz, Vincent Fortuin, and Ewa Szczurek",
        "link": "http://arxiv.org/abs/2505.22494v1",
        "abstract": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty."
    },
    {
        "date": "2025-05",
        "title": "Understanding Adversarial Training with Energy-based Models",
        "author": "Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.22486v1",
        "abstract": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."
    },
    {
        "date": "2025-05",
        "title": "GeneBreaker: Jailbreak Attacks against DNA Language Models with Pathogenicity Guidance",
        "author": "Zaixi Zhang, Zhenghong Zhou, Ruofan Jin, Le Cong, and Mengdi Wang",
        "link": "http://arxiv.org/abs/2505.23839v1",
        "abstract": "DNA, encoding genetic instructions for almost all living organisms, fuels\ngroundbreaking advances in genomics and synthetic biology. Recently, DNA\nFoundation Models have achieved success in designing synthetic functional DNA\nsequences, even whole genomes, but their susceptibility to jailbreaking remains\nunderexplored, leading to potential concern of generating harmful sequences\nsuch as pathogens or toxin-producing genes. In this paper, we introduce\nGeneBreaker, the first framework to systematically evaluate jailbreak\nvulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent\nwith customized bioinformatic tools to design high-homology, non-pathogenic\njailbreaking prompts, (2) beam search guided by PathoLM and log-probability\nheuristics to steer generation toward pathogen-like sequences, and (3) a\nBLAST-based evaluation pipeline against a curated Human Pathogen Database\n(JailbreakDNABench) to detect successful jailbreaks. Evaluated on our\nJailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series\nmodels across 6 viral categories consistently (up to 60\\% Attack Success Rate\nfor Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1\nenvelope protein demonstrate the sequence and structural fidelity of jailbreak\noutput, while evolutionary modeling of SARS-CoV-2 underscores biosecurity\nrisks. Our findings also reveal that scaling DNA foundation models amplifies\ndual-use risks, motivating enhanced safety alignment and tracing mechanisms.\nOur code is at https://github.com/zaixizhang/GeneBreaker."
    },
    {
        "date": "2025-05",
        "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models",
        "author": "Yongcan Yu, Yanbo Wang, Ran He, and Jian Liang",
        "link": "http://arxiv.org/abs/2505.22271v1",
        "abstract": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."
    },
    {
        "date": "2025-05",
        "title": "Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses",
        "author": "Seung Gyu Jeong, and Seong Eun Kim",
        "link": "http://arxiv.org/abs/2505.23834v1",
        "abstract": "Lung sound classification is vital for early diagnosis of respiratory\ndiseases. However, biomedical signals often exhibit inter-patient variability\neven among patients with the same symptoms, requiring a learning approach that\nconsiders individual differences. We propose a Patient-Aware Feature Alignment\n(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)\nand Global Patient Alignment Loss (GPAL). PCSL clusters features of the same\npatient while separating those from other patients to capture patient\nvariability, whereas GPAL draws each patient's centroid toward a global center,\npreventing feature space fragmentation. Our method achieves outstanding results\non the ICBHI dataset with a score of 64.84\\% for four-class and 72.08\\% for\ntwo-class classification. These findings highlight PAFA's ability to capture\nindividualized patterns and demonstrate performance gains in distinct patient\nclusters, offering broader applications for patient-centered healthcare."
    },
    {
        "date": "2025-05",
        "title": "Accountable, Scalable and DoS-resilient Secure Vehicular Communication",
        "author": "Hongyu Jin, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.22162v1",
        "abstract": "Paramount to vehicle safety, broadcasted Cooperative Awareness Messages\n(CAMs) and Decentralized Environmental Notification Messages (DENMs) are\npseudonymously authenticated for security and privacy protection, with each\nnode needing to have all incoming messages validated within an expiration\ndeadline. This creates an asymmetry that can be easily exploited by external\nadversaries to launch a clogging Denial of Service (DoS) attack: each forged VC\nmessage forces all neighboring nodes to cryptographically validate it; at\nincreasing rates, easy to generate forged messages gradually exhaust processing\nresources and severely degrade or deny timely validation of benign CAMs/DENMs.\nThe result can be catastrophic when awareness of neighbor vehicle positions or\ncritical reports are missed. We address this problem making the standardized VC\npseudonymous authentication DoS-resilient. We propose efficient cryptographic\nconstructs, which we term message verification facilitators, to prioritize\nprocessing resources for verification of potentially valid messages among bogus\nmessages and verify multiple messages based on one signature verification. Any\nmessage acceptance is strictly based on public-key based message\nauthentication/verification for accountability, i.e., non-repudiation is not\nsacrificed, unlike symmetric key based approaches. This further enables drastic\nmisbehavior detection, also exploiting the newly introduced facilitators, based\non probabilistic signature verification and cross-checking over multiple\nfacilitators verifying the same message; while maintaining verification latency\nlow even when under attack, trading off modest communication overhead. Our\nfacilitators can also be used for efficient discovery and verification of DENM\nor any event-driven message, including misbehavior evidence used for our\nscheme."
    },
    {
        "date": "2025-05",
        "title": "Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance",
        "author": "Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, and Zhenyu He",
        "link": "http://arxiv.org/abs/2505.22154v1",
        "abstract": "RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."
    },
    {
        "date": "2025-05",
        "title": "Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM",
        "author": "Lei Yu, Yechao Zhang, Ziqi Zhou, Yang Wu, Wei Wan, Minghui Li, Shengshan Hu, Pei Xiaobing, and Jing Wang",
        "link": "http://arxiv.org/abs/2505.23828v1",
        "abstract": "With the rapid development of the Vision-Language Model (VLM), significant\nprogress has been made in Visual Question Answering (VQA) tasks. However,\nexisting VLM often generate inaccurate answers due to a lack of up-to-date\nknowledge. To address this issue, recent research has introduced\nRetrieval-Augmented Generation (RAG) techniques, commonly used in Large\nLanguage Models (LLM), into VLM, incorporating external multi-modal knowledge\nto enhance the accuracy and practicality of VLM systems. Nevertheless, the RAG\nin LLM may be susceptible to data poisoning attacks. RAG-based VLM may also\nface the threat of this attack. This paper first reveals the vulnerabilities of\nthe RAG-based large model under poisoning attack, showing that existing\nsingle-modal RAG poisoning attacks have a 100\\% failure rate in multi-modal RAG\nscenarios. To address this gap, we propose Spa-VLM (Stealthy Poisoning Attack\non RAG-based VLM), a new paradigm for poisoning attacks on large models. We\ncarefully craft malicious multi-modal knowledge entries, including adversarial\nimages and misleading text, which are then injected into the RAG's knowledge\nbase. When users access the VLM service, the system may generate misleading\noutputs. We evaluate Spa-VLM on two Wikipedia datasets and across two different\nRAGs. Results demonstrate that our method achieves highly stealthy poisoning,\nwith the attack success rate exceeding 0.8 after injecting just 5 malicious\nentries into knowledge bases with 100K and 2M entries, outperforming\nstate-of-the-art poisoning attacks designed for RAG-based LLMs. Additionally,\nwe evaluated several defense mechanisms, all of which ultimately proved\nineffective against Spa-VLM, underscoring the effectiveness and robustness of\nour attack."
    },
    {
        "date": "2025-05",
        "title": "Securing the Software Package Supply Chain for Critical Systems",
        "author": "Ritwik Murali, and Akash Ravi",
        "link": "http://arxiv.org/abs/2505.22023v1",
        "abstract": "Software systems have grown as an indispensable commodity used across various\nindustries, and almost all essential services depend on them for effective\noperation. The software is no longer an independent or stand-alone piece of\ncode written by a developer but rather a collection of packages designed by\nmultiple developers across the globe. Ensuring the reliability and resilience\nof these systems is crucial since emerging threats target software supply\nchains, as demonstrated by the widespread SolarWinds hack in late 2020. These\nsupply chains extend beyond patches and updates, involving distribution\nnetworks throughout the software lifecycle. Industries like smart grids,\nmanufacturing, healthcare, and finance rely on interconnected software systems\nand their dependencies for effective functioning. To secure software modules\nand add-ons, robust distribution architectures are essential. The proposed\nchapter enhances the existing delivery frameworks by including a permissioned\nledger with Proof of Authority consensus and multi-party signatures. The\nproposed system aims to prevent attacks while permitting every stakeholder to\nverify the same. Critical systems can interface with the secure pipeline\nwithout disrupting existing functionalities, thus preventing the cascading\neffect of an attack at any point in the supply chain."
    },
    {
        "date": "2025-05",
        "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement",
        "author": "Zhihong Tang, and Yang Li",
        "link": "http://arxiv.org/abs/2505.22021v1",
        "abstract": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection",
        "author": "Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, and Jinhang Zuo",
        "link": "http://arxiv.org/abs/2505.21938v2",
        "abstract": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."
    },
    {
        "date": "2025-05",
        "title": "SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking",
        "author": "Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Xun Chen, and Miao Pan",
        "link": "http://arxiv.org/abs/2505.23821v1",
        "abstract": "With the surge of social media, maliciously tampered public speeches,\nespecially those from influential figures, have seriously affected social\nstability and public trust. Existing speech tampering detection methods remain\ninsufficient: they either rely on external reference data or fail to be both\nsensitive to attacks and robust to benign operations, such as compression and\nresampling. To tackle these challenges, we introduce SpeechVerifer to\nproactively verify speech integrity using only the published speech itself,\ni.e., without requiring any external references. Inspired by audio\nfingerprinting and watermarking, SpeechVerifier can (i) effectively detect\ntampering attacks, (ii) be robust to benign operations and (iii) verify the\nintegrity only based on published speeches. Briefly, SpeechVerifier utilizes\nmultiscale feature extraction to capture speech features across different\ntemporal resolutions. Then, it employs contrastive learning to generate\nfingerprints that can detect modifications at varying granularities. These\nfingerprints are designed to be robust to benign operations, but exhibit\nsignificant changes when malicious tampering occurs. To enable speech\nverification in a self-contained manner, the generated fingerprints are then\nembedded into the speech signal by segment-wise watermarking. Without external\nreferences, SpeechVerifier can retrieve the fingerprint from the published\naudio and check it with the embedded watermark to verify the integrity of the\nspeech. Extensive experimental results demonstrate that the proposed\nSpeechVerifier is effective in detecting tampering attacks and robust to benign\noperations."
    },
    {
        "date": "2025-05",
        "title": "Evaluating the Retrieval Robustness of Large Language Models",
        "author": "Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, and Shiyue Zhang",
        "link": "http://arxiv.org/abs/2505.21870v1",
        "abstract": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."
    },
    {
        "date": "2025-05",
        "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification",
        "author": "Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, and Chongshou Li",
        "link": "http://arxiv.org/abs/2505.21854v1",
        "abstract": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."
    },
    {
        "date": "2025-05",
        "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
        "author": "Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, and Honghao Wei",
        "link": "http://arxiv.org/abs/2505.21841v1",
        "abstract": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."
    },
    {
        "date": "2025-05",
        "title": "Faster Rates for Private Adversarial Bandits",
        "author": "Hilal Asi, Vinod Raman, and Kunal Talwar",
        "link": "http://arxiv.org/abs/2505.21790v1",
        "abstract": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"
    },
    {
        "date": "2025-05",
        "title": "System Prompt Extraction Attacks and Defenses in Large Language Models",
        "author": "Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu",
        "link": "http://arxiv.org/abs/2505.23817v1",
        "abstract": "The system prompt in Large Language Models (LLMs) plays a pivotal role in\nguiding model behavior and response generation. Often containing private\nconfiguration details, user roles, and operational instructions, the system\nprompt has become an emerging attack target. Recent studies have shown that LLM\nsystem prompts are highly susceptible to extraction attacks through\nmeticulously designed queries, raising significant privacy and security\nconcerns. Despite the growing threat, there is a lack of systematic studies of\nsystem prompt extraction attacks and defenses. In this paper, we present a\ncomprehensive framework, SPE-LLM, to systematically evaluate System Prompt\nExtraction attacks and defenses in LLMs. First, we design a set of novel\nadversarial queries that effectively extract system prompts in state-of-the-art\n(SOTA) LLMs, demonstrating the severe risks of LLM system prompt extraction\nattacks. Second, we propose three defense techniques to mitigate system prompt\nextraction attacks in LLMs, providing practical solutions for secure LLM\ndeployments. Third, we introduce a set of rigorous evaluation metrics to\naccurately quantify the severity of system prompt extraction attacks in LLMs\nand conduct comprehensive experiments across multiple benchmark datasets, which\nvalidates the efficacy of our proposed SPE-LLM framework."
    },
    {
        "date": "2025-05",
        "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
        "author": "Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira",
        "link": "http://arxiv.org/abs/2505.21755v1",
        "abstract": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."
    },
    {
        "date": "2025-05",
        "title": "What is Adversarial Training for Diffusion Models?",
        "author": "Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.21742v1",
        "abstract": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks",
        "author": "Julia Boone, Tolunay Seyfi, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2505.21703v1",
        "abstract": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another."
    },
    {
        "date": "2025-05",
        "title": "Expert Survey: AI Reliability & Security Research Priorities",
        "author": "Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, and Cara Labrador",
        "link": "http://arxiv.org/abs/2505.21664v1",
        "abstract": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research."
    },
    {
        "date": "2025-05",
        "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking",
        "author": "Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.21620v1",
        "abstract": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."
    },
    {
        "date": "2025-05",
        "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
        "author": "Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, and Qing Wang",
        "link": "http://arxiv.org/abs/2505.21499v1",
        "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."
    },
    {
        "date": "2025-05",
        "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
        "author": "Mathew J. Walter, Aaron Barrett, and Kimberly Tam",
        "link": "http://arxiv.org/abs/2505.21609v1",
        "abstract": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.21494v1",
        "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."
    },
    {
        "date": "2025-05",
        "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming",
        "author": "Yang Yang, Jiemin Wu, and Yutao Yue",
        "link": "http://arxiv.org/abs/2505.21486v1",
        "abstract": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation."
    },
    {
        "date": "2025-05",
        "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
        "author": "Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, and Joseph Weissman",
        "link": "http://arxiv.org/abs/2505.21414v1",
        "abstract": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments."
    },
    {
        "date": "2025-05",
        "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling",
        "author": "Hovhannes Tamoyan, Subhabrata Dutta, and Iryna Gurevych",
        "link": "http://arxiv.org/abs/2505.21399v1",
        "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."
    },
    {
        "date": "2025-05",
        "title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment",
        "author": "Xingyu Zhou, Yulian Wu, Wenqian Weng, and Francesco Orabona",
        "link": "http://arxiv.org/abs/2505.21395v1",
        "abstract": "In this paper, we theoretically study the offline alignment of language\nmodels with human preference feedback, under both preference label corruption\nand privacy protections. To this end, we propose Square$\\chi$PO, a simple\none-line change to $\\chi$PO where the standard log-loss is replaced by a new\nsquare loss over probability. Thanks to the inherent properties of this new\nloss, we have advanced the state-of-the-art of differentially private and\nrobust offline direct alignment. Specifically, for the local model of label\nprivacy, Square$\\chi$PO is the first algorithm that attains an optimal rate\nbased on single-policy concentrability even with general function\napproximations. It also gives the first result under the central model of\nprivacy protection over both prompts (responses) and labels. On the robustness\nside against Huber label corruption, Square$\\chi$PO is the first alignment\nmethod that has a meaningful theoretical guarantee under general function\napproximations. More importantly, Square$\\chi$PO can address privacy protection\nand corruption simultaneously, where an interesting separation is observed,\nimplying that the order of privacy and corruption matters. Furthermore, we show\nthat Square$\\chi$PO can also be easily extended to handle the scenario of the\ngeneral preference model with state-of-the-art guarantees under corruption and\nprivacy. Last but not least, all of our theoretical guarantees enjoy a unified\nanalysis, building upon a new result on the generalization error bounds of\nleast-square regression under corruption and privacy constraints, which we\nbelieve is of independent interest to the community."
    },
    {
        "date": "2025-05",
        "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios",
        "author": "Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, and Yueming Jin",
        "link": "http://arxiv.org/abs/2505.21387v1",
        "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github."
    },
    {
        "date": "2025-05",
        "title": "Subgroups Matter for Robust Bias Mitigation",
        "author": "Anissa Alloula, Charles Jones, Ben Glocker, and Bart\u0142omiej W. Papie\u017c",
        "link": "http://arxiv.org/abs/2505.21363v2",
        "abstract": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and presents it as an alternative lever for improving the\nrobustness and fairness of machine learning models."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations",
        "author": "Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, and Yu Tsao",
        "link": "http://arxiv.org/abs/2505.21356v3",
        "abstract": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders by providing standardized evaluations of vocal\nfunction. Traditionally, expert raters use standard scales such as the\nConsensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade,\nRoughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics\nare subjective and prone to inter-rater variability, motivating the need for\nautomated, objective assessment methods. This study proposes Voice Quality\nAssessment Network (VOQANet), a deep learning-based framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To enhance robustness and\ninterpretability, we also introduce VOQANet+, which integrates low-level speech\ndescriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with\nSFM embeddings into a hybrid representation. Unlike prior studies focused only\non vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality\nDataset (PVQD), we evaluate our models on both vowel-based and sentence-level\nspeech (PVQD-S subset) to improve generalizability. Results show that\nsentence-based input outperforms vowel-based input, especially at the patient\nlevel, underscoring the value of longer utterances for capturing perceptual\nvoice attributes. VOQANet consistently surpasses baseline methods in root mean\nsquared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V\nand GRBAS dimensions, with VOQANet+ achieving even better performance.\nAdditional experiments under noisy conditions show that VOQANet+ maintains high\nprediction accuracy and robustness, supporting its potential for real-world and\ntelehealth deployment."
    },
    {
        "date": "2025-05",
        "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
        "author": "Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.21277v2",
        "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
    },
    {
        "date": "2025-05",
        "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion",
        "author": "Yayin Zheng, Chen Wan, Zihong Guo, Hailing Kuang, and Xiaohai Lu",
        "link": "http://arxiv.org/abs/2505.21181v1",
        "abstract": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels."
    },
    {
        "date": "2025-05",
        "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images",
        "author": "Xurui Li, Zhonesheng Jiang, Tingxuan Ai, and Yu Zhou",
        "link": "http://arxiv.org/abs/2505.21152v1",
        "abstract": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS."
    },
    {
        "date": "2025-05",
        "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
        "author": "Honglin Gao, Xiang Li, Lan Zhao, and Gaoxi Xiao",
        "link": "http://arxiv.org/abs/2505.21140v1",
        "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios."
    },
    {
        "date": "2025-05",
        "title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach",
        "author": "Subhagata Chattopadhyay, and Amit K Chattopadhyay",
        "link": "http://arxiv.org/abs/2505.21139v1",
        "abstract": "The COVID-19 pandemic has significantly increased the incidence of\npost-infection cardiovascular events, particularly myocardial infarction, in\nindividuals over 40. While the underlying mechanisms remain elusive, this study\nemploys a hybrid machine learning approach to analyze epidemiological data in\nassessing 13 key heart attack risk factors and their susceptibility. Based on a\nunique dataset that combines demographic, biochemical, ECG, and thallium\nstress-tests, this study categorizes distinct subpopulations against varying\nrisk profiles and then divides the population into 'at-risk' (AR) and\n'not-at-risk' (NAR) groups using clustering algorithms. The study reveals\nstrong association between the likelihood of experiencing a heart attack on the\n13 risk factors studied. The aggravated risk for postmenopausal patients\nindicates compromised individual risk factors due to estrogen depletion that\nmay be, further compromised by extraneous stress impacts, like anxiety and\nfear, aspects that have traditionally eluded data modeling predictions."
    },
    {
        "date": "2025-05",
        "title": "Robust and Computation-Aware Gaussian Processes",
        "author": "Marshal Arijona Sinaga, Julien Martinelli, and Samuel Kaski",
        "link": "http://arxiv.org/abs/2505.21133v1",
        "abstract": "Gaussian processes (GPs) are widely used for regression and optimization\ntasks such as Bayesian optimization (BO) due to their expressiveness and\nprincipled uncertainty estimates. However, in settings with large datasets\ncorrupted by outliers, standard GPs and their sparse approximations struggle\nwith computational tractability and robustness. We introduce Robust\nComputation-aware Gaussian Process (RCaGP), a novel GP model that jointly\naddresses these challenges by combining a principled treatment of\napproximation-induced uncertainty with robust generalized Bayesian updating.\nThe key insight is that robustness and approximation-awareness are not\northogonal but intertwined: approximations can exacerbate the impact of\noutliers, and mitigating one without the other is insufficient. Unlike previous\nwork that focuses narrowly on either robustness or approximation quality, RCaGP\ncombines both in a principled and scalable framework, thus effectively managing\nboth outliers and computational uncertainties introduced by approximations such\nas low-rank matrix multiplications. Our model ensures more conservative and\nreliable uncertainty estimates, a property we rigorously demonstrate.\nAdditionally, we establish a robustness property and show that the mean\nfunction is key to preserving it, motivating a tailored model selection scheme\nfor robust mean functions. Empirical results confirm that solving these\nchallenges jointly leads to superior performance across both clean and\noutlier-contaminated settings, both on regression and high-throughput Bayesian\noptimization benchmarks."
    },
    {
        "date": "2025-05",
        "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing",
        "author": "Dehao Wang, Haohang Zhu, Yiwen Xu, and Kaiqi Liu",
        "link": "http://arxiv.org/abs/2505.21049v1",
        "abstract": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability."
    },
    {
        "date": "2025-05",
        "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
        "author": "Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, and Catarina Moreira",
        "link": "http://arxiv.org/abs/2505.21027v1",
        "abstract": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data."
    },
    {
        "date": "2025-05",
        "title": "Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models",
        "author": "Puwei Lian, Yujun Cai, and Songze Li",
        "link": "http://arxiv.org/abs/2505.20955v1",
        "abstract": "Diffusion models have achieved tremendous success in image generation, but\nthey also raise significant concerns regarding privacy and copyright issues.\nMembership Inference Attacks (MIAs) are designed to ascertain whether specific\ndata were utilized during a model's training phase. As current MIAs for\ndiffusion models typically exploit the model's image prediction ability, we\nformalize them into a unified general paradigm which computes the membership\nscore for membership identification. Under this paradigm, we empirically find\nthat existing attacks overlook the inherent deficiency in how diffusion models\nprocess high-frequency information. Consequently, this deficiency leads to\nmember data with more high-frequency content being misclassified as hold-out\ndata, and hold-out data with less high-frequency content tend to be\nmisclassified as member data. Moreover, we theoretically demonstrate that this\ndeficiency reduces the membership advantage of attacks, thereby interfering\nwith the effective discrimination of member data and hold-out data. Based on\nthis insight, we propose a plug-and-play high-frequency filter module to\nmitigate the adverse effects of the deficiency, which can be seamlessly\nintegrated into any attacks within this general paradigm without additional\ntime costs. Extensive experiments corroborate that this module significantly\nimproves the performance of baseline attacks across different datasets and\nmodels."
    },
    {
        "date": "2025-05",
        "title": "NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion",
        "author": "Max Collins, Jordan Vice, Tim French, and Ajmal Mian",
        "link": "http://arxiv.org/abs/2505.20934v1",
        "abstract": "Adversarial samples exploit irregularities in the manifold ``learned'' by\ndeep learning models to cause misclassifications. The study of these\nadversarial samples provides insight into the features a model uses to classify\ninputs, which can be leveraged to improve robustness against future attacks.\nHowever, much of the existing literature focuses on constrained adversarial\nsamples, which do not accurately reflect test-time errors encountered in\nreal-world settings. To address this, we propose `NatADiff', an adversarial\nsampling scheme that leverages denoising diffusion to generate natural\nadversarial samples. Our approach is based on the observation that natural\nadversarial samples frequently contain structural elements from the adversarial\nclass. Deep learning models can exploit these structural elements to shortcut\nthe classification process, rather than learning to genuinely distinguish\nbetween classes. To leverage this behavior, we guide the diffusion trajectory\ntowards the intersection of the true and adversarial classes, combining\ntime-travel sampling with augmented classifier guidance to enhance attack\ntransferability while preserving image fidelity. Our method achieves comparable\nattack success rates to current state-of-the-art techniques, while exhibiting\nsignificantly higher transferability across model architectures and better\nalignment with natural test-time errors as measured by FID. These results\ndemonstrate that NatADiff produces adversarial samples that not only transfer\nmore effectively across models, but more faithfully resemble naturally\noccurring test-time errors."
    },
    {
        "date": "2025-05",
        "title": "Towards a DSL for hybrid secure computation",
        "author": "Romain de Laage",
        "link": "http://arxiv.org/abs/2505.20912v1",
        "abstract": "Fully homomorphic encryption (FHE) and trusted execution environments (TEE)\nare two approaches to provide confidentiality during data processing. Each\napproach has its own strengths and weaknesses. In certain scenarios,\ncomputations can be carried out in a hybrid environment, using both FHE and\nTEE. However, processing data in such hybrid settings presents challenges, as\nit requires to adapt and rewrite the algorithms for the chosen technique. We\npropose a domain-specific language (DSL) for secure computation that allows to\nexpress the computations to perform and execute them using a backend that\nleverages either FHE or TEE, depending on what is available."
    },
    {
        "date": "2025-05",
        "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties",
        "author": "Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, and Edward Choi",
        "link": "http://arxiv.org/abs/2505.20875v1",
        "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available."
    },
    {
        "date": "2025-05",
        "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks",
        "author": "Ta\u00efga Gon\u00e7alves, Tomo Miyazaki, and Shinichiro Omachi",
        "link": "http://arxiv.org/abs/2505.20782v1",
        "abstract": "We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for\ngenerating adversarial examples that mislead image classifiers toward any\ntarget class, including those not seen during training. Traditional targeted\nattacks are limited to one class per model, requiring expensive retraining for\neach target. Multi-targeted attacks address this by introducing a perturbation\ngenerator with a conditional input to specify the target class. However,\nexisting methods are constrained to classes observed during training and\nrequire access to the black-box model's training data--introducing a form of\ndata leakage that undermines realistic evaluation in practical black-box\nscenarios. We identify overreliance on class embeddings as a key limitation,\nleading to overfitting and poor generalization to unseen classes. To address\nthis, CD-MTA replaces class-level supervision with an image-based conditional\ninput and introduces class-agnostic losses that align the perturbed and target\nimages in the feature space. This design removes dependence on class semantics,\nthereby enabling generalization to unseen classes across datasets. Experiments\non ImageNet and seven other datasets show that CD-MTA outperforms prior\nmulti-targeted attacks in both standard and cross-domain settings--without\naccessing the black-box model's training data."
    },
    {
        "date": "2025-05",
        "title": "Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies",
        "author": "Kohei Obata, Yasuko Matsubara, and Yasushi Sakurai",
        "link": "http://arxiv.org/abs/2505.20765v1",
        "abstract": "Unsupervised anomaly detection in time series has been a pivotal research\narea for decades. Current mainstream approaches focus on learning normality, on\nthe assumption that all or most of the samples in the training set are normal.\nHowever, anomalies in the training set (i.e., anomaly contamination) can be\nmisleading. Recent studies employ data augmentation to generate\npseudo-anomalies and learn the boundary separating the training samples from\nthe augmented samples. Although this approach mitigates anomaly contamination\nif augmented samples mimic unseen real anomalies, it suffers from several\nlimitations. (1) Covering a wide range of time series anomalies is challenging.\n(2) It disregards augmented samples that resemble normal samples (i.e., false\nanomalies). (3) It places too much trust in the labels of training and\naugmented samples. In response, we propose RedLamp, which employs diverse data\naugmentations to generate multiclass pseudo-anomalies and learns the multiclass\nboundary. Such multiclass pseudo-anomalies cover a wide variety of time series\nanomalies. We conduct multiclass classification using soft labels, which\nprevents the model from being overconfident and ensures its robustness against\ncontaminated/false anomalies. The learned latent space is inherently\nexplainable as it is trained to separate pseudo-anomalies into multiclasses.\nExtensive experiments demonstrate the effectiveness of RedLamp in anomaly\ndetection and its robustness against anomaly contamination."
    },
    {
        "date": "2025-05",
        "title": "Adversarial bandit optimization for approximately linear functions",
        "author": "Zhuoyu Cheng, Kohei Hatano, and Eiji Takimoto",
        "link": "http://arxiv.org/abs/2505.20734v2",
        "abstract": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret."
    },
    {
        "date": "2025-05",
        "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment",
        "author": "Lingyu Qiu, Ke Jiang, and Xiaoyang Tan",
        "link": "http://arxiv.org/abs/2505.20653v1",
        "abstract": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA."
    },
    {
        "date": "2025-05",
        "title": "ADA: Automated Moving Target Defense for AI Workloads via Ephemeral Infrastructure-Native Rotation in Kubernetes",
        "author": "Akram Sheriff, Ken Huang, Zsolt Nemeth, and Madjid Nakhjiri",
        "link": "http://arxiv.org/abs/2505.23805v1",
        "abstract": "This paper introduces the Adaptive Defense Agent (ADA), an innovative\nAutomated Moving Target Defense (AMTD) system designed to fundamentally enhance\nthe security posture of AI workloads. ADA operates by continuously and\nautomatically rotating these workloads at the infrastructure level, leveraging\nthe inherent ephemerality of Kubernetes pods. This constant managed churn\nsystematically invalidates attacker assumptions and disrupts potential kill\nchains by regularly destroying and respawning AI service instances. This\nmethodology, applying principles of chaos engineering as a continuous,\nproactive defense, offers a paradigm shift from traditional static defenses\nthat rely on complex and expensive confidential or trusted computing solutions\nto secure the underlying compute platforms, while at the same time agnostically\nsupporting the latest advancements in agentic and nonagentic AI ecosystems and\nsolutions such as agent-to-agent (A2A) communication frameworks or model\ncontext protocols (MCP). This AI-native infrastructure design, relying on the\nwidely proliferated cloud-native Kubernetes technologies, facilitates easier\ndeployment, simplifies maintenance through an inherent zero trust posture\nachieved by rotation, and promotes faster adoption. We posit that ADA's novel\napproach to AMTD provides a more robust, agile, and operationally efficient\nzero-trust model for AI services, achieving security through proactive\nenvironmental manipulation rather than reactive patching."
    },
    {
        "date": "2025-05",
        "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
        "author": "Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, and Bin Ma",
        "link": "http://arxiv.org/abs/2505.20635v1",
        "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method."
    },
    {
        "date": "2025-05",
        "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
        "author": "Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2505.20621v1",
        "abstract": "Similar to other machine learning frameworks, Offline Reinforcement Learning\n(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on\nexternally sourced datasets, a vulnerability that is exacerbated by its\nsequential nature. To mitigate the risks posed by RL poisoning, we extend\ncertified defenses to provide larger guarantees against adversarial\nmanipulation, ensuring robustness for both per-state actions, and the overall\nexpected cumulative reward. Our approach leverages properties of Differential\nPrivacy, in a manner that allows this work to span both continuous and discrete\nspaces, as well as stochastic and deterministic environments -- significantly\nexpanding the scope and applicability of achievable guarantees. Empirical\nevaluations demonstrate that our approach ensures the performance drops to no\nmore than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly\nimproving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while\nproducing certified radii that is $5$ times larger as well. This highlights the\npotential of our framework to enhance safety and reliability in offline RL."
    },
    {
        "date": "2025-05",
        "title": "EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms",
        "author": "Jiaxiong He",
        "link": "http://arxiv.org/abs/2505.20614v1",
        "abstract": "This paper introduces EarthOL, a novel consensus protocol that attempts to\nreplace computational waste in blockchain systems with verifiable human\ncontributions within bounded domains. While recognizing the fundamental\nimpossibility of universal value assessment, we propose a domain-restricted\napproach that acknowledges cultural diversity and subjective preferences while\nmaintaining cryptographic security. Our enhanced Proof-of-Human-Contribution\n(PoHC) protocol uses a multi-layered verification system with domain-specific\nevaluation criteria, time-dependent validation mechanisms, and comprehensive\nsecurity frameworks. We present theoretical analysis demonstrating meaningful\nprogress toward incentive-compatible human contribution verification in\nhigh-consensus domains, achieving Byzantine fault tolerance in controlled\nscenarios while addressing significant scalability and cultural bias\nchallenges. Through game-theoretic analysis, probabilistic modeling, and\nenhanced security protocols, we identify specific conditions under which the\nprotocol remains stable and examine failure modes with comprehensive mitigation\nstrategies. This work contributes to understanding the boundaries of\ndecentralized value assessment and provides a framework for future research in\nhuman-centered consensus mechanisms for specific application domains, with\nparticular emphasis on validator and security specialist incentive systems."
    },
    {
        "date": "2025-05",
        "title": "One-shot Robust Federated Learning of Independent Component Analysis",
        "author": "Dian Jin, Xin Bing, and Yuqian Zhang",
        "link": "http://arxiv.org/abs/2505.20532v1",
        "abstract": "This paper investigates a general robust one-shot aggregation framework for\ndistributed and federated Independent Component Analysis (ICA) problem. We\npropose a geometric median-based aggregation algorithm that leverages $k$-means\nclustering to resolve the permutation ambiguity in local client estimations.\nOur method first performs k-means to partition client-provided estimators into\nclusters and then aggregates estimators within each cluster using the geometric\nmedian. This approach provably remains effective even in highly heterogeneous\nscenarios where at most half of the clients can observe only a minimal number\nof samples. The key theoretical contribution lies in the combined analysis of\nthe geometric median's error bound-aided by sample quantiles-and the maximum\nmisclustering rates of the aforementioned solution of $k$-means. The\neffectiveness of the proposed approach is further supported by simulation\nstudies conducted under various heterogeneous settings."
    },
    {
        "date": "2025-05",
        "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
        "author": "Aideen Fay, In\u00e9s Garc\u00eda-Redondo, Qiquan Wang, Haim Dubossarsky, and Anthea Monod",
        "link": "http://arxiv.org/abs/2505.20435v1",
        "abstract": "Understanding how adversarial conditions affect language models requires\ntechniques that capture both global structure and local detail within\nhigh-dimensional activation spaces. We propose persistent homology (PH), a tool\nfrom topological data analysis, to systematically characterize multiscale\nlatent space dynamics in LLMs under two distinct attack modes -- backdoor\nfine-tuning and indirect prompt injection. By analyzing six state-of-the-art\nLLMs, we show that adversarial conditions consistently compress latent\ntopologies, reducing structural diversity at smaller scales while amplifying\ndominant features at coarser ones. These topological signatures are\nstatistically robust across layers, architectures, model sizes, and align with\nthe emergence of adversarial effects deeper in the network. To capture\nfiner-grained mechanisms underlying these shifts, we introduce a neuron-level\nPH framework that quantifies how information flows and transforms within and\nacross layers. Together, our findings demonstrate that PH offers a principled\nand unifying approach to interpreting representational dynamics in LLMs,\nparticularly under distributional shift."
    },
    {
        "date": "2025-05",
        "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
        "author": "Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2505.20426v1",
        "abstract": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/"
    },
    {
        "date": "2025-05",
        "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
        "author": "Simin Fan, Maria Ios Glarou, and Martin Jaggi",
        "link": "http://arxiv.org/abs/2505.20380v1",
        "abstract": "The performance of large language models (LLMs) across diverse downstream\napplications is fundamentally governed by the quality and composition of their\npretraining corpora. Existing domain reweighting algorithms primarily optimize\ndata mixtures for a single target task, thereby resulting in models that\noverfit to specialized objectives while exhibiting substantial performance\ndegradation on other benchmarks. This paper introduces Group Robust\nMulti-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target\ndomain reweighting framework designed to calibrate pretraining data mixtures\nfor robust performance across multiple target tasks simultaneously. GRAPE\ndynamically adjusts sampling weights across source domains (domain weights)\nwhile concurrently modulating task weights that quantify the relative\nimportance of each individual target task. This adaptive process prioritizes\ntasks based on their learning difficulty throughout training. We formulate this\ninterleaved reweighting mechanism as a minimax optimization problem: The inner\nmaximization adjusts task weights leveraging group\ndistributed-robust-optimization (DRO), where those tasks demonstrating the\nleast improvement under the current data mixture are prioritized with higher\nweights; The outer minimization then optimizes domain weights to maximize loss\nreduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama\ndatasets demonstrate that GRAPE consistently outperforms baseline methods in\nterms of reasoning performance across 6 benchmarks. Furthermore, when applied\nto multilingual targets, GRAPE effectively identifies optimal training mixtures\nfrom mainstream languages, achieving superior language modeling capabilities\nacross 8 low-resource target languages."
    },
    {
        "date": "2025-05",
        "title": "An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks",
        "author": "Jinyan Wang, Liu Yang, Yuecen Wei, Jiaxuan Si, Chenhao Guo, Qingyun Sun, Xianxian Li, and Xingcheng Fu",
        "link": "http://arxiv.org/abs/2505.20074v1",
        "abstract": "Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains."
    },
    {
        "date": "2025-05",
        "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
        "author": "Xinping Chen, and Chen Liu",
        "link": "http://arxiv.org/abs/2505.20026v1",
        "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters."
    },
    {
        "date": "2025-05",
        "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
        "author": "Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, and Oleg Y. Rogov",
        "link": "http://arxiv.org/abs/2505.19951v1",
        "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths."
    },
    {
        "date": "2025-05",
        "title": "Cellwise and Casewise Robust Covariance in High Dimensions",
        "author": "Fabio Centofanti, Mia Hubert, and Peter J. Rousseeuw",
        "link": "http://arxiv.org/abs/2505.19925v1",
        "abstract": "The sample covariance matrix is a cornerstone of multivariate statistics, but\nit is highly sensitive to outliers. These can be casewise outliers, such as\ncases belonging to a different population, or cellwise outliers, which are\ndeviating cells (entries) of the data matrix. Recently some robust covariance\nestimators have been developed that can handle both types of outliers, but\ntheir computation is only feasible up to at most 20 dimensions. To remedy this\nwe propose the cellRCov method, a robust covariance estimator that\nsimultaneously handles casewise outliers, cellwise outliers, and missing data.\nIt relies on a decomposition of the covariance on principal and orthogonal\nsubspaces, leveraging recent work on robust PCA. It also employs a ridge-type\nregularization to stabilize the estimated covariance matrix. We establish some\ntheoretical properties of cellRCov, including its casewise and cellwise\ninfluence functions as well as consistency and asymptotic normality. A\nsimulation study demonstrates the superior performance of cellRCov in\ncontaminated and missing data scenarios. Furthermore, its practical utility is\nillustrated in a real-world application to anomaly detection. We also construct\nand illustrate the cellRCCA method for robust and regularized canonical\ncorrelation analysis."
    },
    {
        "date": "2025-05",
        "title": "CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models",
        "author": "Chunyang Li, Junwei Zhang, Anda Cheng, Zhuo Ma, Xinghua Li, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2505.19864v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge, but its openness introduces vulnerabilities\nthat can be exploited by poisoning attacks. Existing poisoning methods for RAG\nsystems have limitations, such as poor generalization and lack of fluency in\nadversarial texts. In this paper, we propose CPA-RAG, a black-box adversarial\nframework that generates query-relevant texts capable of manipulating the\nretrieval process to induce target answers. The proposed method integrates\nprompt-based text generation, cross-guided optimization through multiple LLMs,\nand retriever-based scoring to construct high-quality adversarial samples. We\nconduct extensive experiments across multiple datasets and LLMs to evaluate its\neffectiveness. Results show that the framework achieves over 90\\% attack\nsuccess when the top-k retrieval setting is 5, matching white-box performance,\nand maintains a consistent advantage of approximately 5 percentage points\nacross different top-k values. It also outperforms existing black-box baselines\nby 14.5 percentage points under various defense strategies. Furthermore, our\nmethod successfully compromises a commercial RAG system deployed on Alibaba's\nBaiLian platform, demonstrating its practical threat in real-world\napplications. These findings underscore the need for more robust and secure RAG\nframeworks to defend against poisoning attacks."
    },
    {
        "date": "2025-05",
        "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
        "author": "Binyan Xu, Xilin Dai, Di Tang, and Kehuan Zhang",
        "link": "http://arxiv.org/abs/2505.19840v1",
        "abstract": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone\nto adversarial attacks. Typically, such attacks either involve frequent queries\nto the target model or rely on surrogate models closely mirroring the target\nmodel -- often trained with subsets of the target model's training data -- to\nachieve high attack success rates through transferability. However, in\nrealistic scenarios where training data is inaccessible and excessive queries\ncan raise alarms, crafting adversarial examples becomes more challenging. In\nthis paper, we present UnivIntruder, a novel attack framework that relies\nsolely on a single, publicly available CLIP model and publicly available\ndatasets. By using textual concepts, UnivIntruder generates universal,\ntransferable, and targeted adversarial perturbations that mislead DNNs into\nmisclassifying inputs into adversary-specified classes defined by textual\nconcepts.\n  Our extensive experiments show that our approach achieves an Attack Success\nRate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly\noutperforming existing transfer-based methods. Additionally, we reveal\nreal-world vulnerabilities, showing that even without querying target models,\nUnivIntruder compromises image search engines like Google and Baidu with ASR\nrates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR\nrates up to 80%. These findings underscore the practicality of our attack in\nscenarios where traditional avenues are blocked, highlighting the need to\nreevaluate security paradigms in AI applications."
    },
    {
        "date": "2025-05",
        "title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks",
        "author": "Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, and Shouling Ji",
        "link": "http://arxiv.org/abs/2505.19821v1",
        "abstract": "Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations."
    },
    {
        "date": "2025-05",
        "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning",
        "author": "Bariscan Bozkurt, Houssam Zenati, Dimitri Meunier, Liyuan Xu, and Arthur Gretton",
        "link": "http://arxiv.org/abs/2505.19807v1",
        "abstract": "We study the problem of causal function estimation in the Proxy Causal\nLearning (PCL) framework, where confounders are not observed but proxies for\nthe confounders are available. Two main approaches have been proposed: outcome\nbridge-based and treatment bridge-based methods. In this work, we propose two\nkernel-based doubly robust estimators that combine the strengths of both\napproaches, and naturally handle continuous and high-dimensional variables. Our\nidentification strategy builds on a recent density ratio-free method for\ntreatment bridge-based PCL; furthermore, in contrast to previous approaches, it\ndoes not require indicator functions or kernel smoothing over the treatment\nvariable. These properties make it especially well-suited for continuous or\nhigh-dimensional treatments. By using kernel mean embeddings, we have\nclosed-form solutions and strong consistency guarantees. Our estimators\noutperform existing methods on PCL benchmarks, including a prior doubly robust\nmethod that requires both kernel smoothing and density ratio estimation."
    },
    {
        "date": "2025-05",
        "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
        "author": "Sangyeop Kim, Yohan Lee, Yongwoo Song, and Kimin Lee",
        "link": "http://arxiv.org/abs/2505.19773v1",
        "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."
    },
    {
        "date": "2025-05",
        "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
        "author": "Bingrui Sima, Linhua Cong, Wenxuan Wang, and Kun He",
        "link": "http://arxiv.org/abs/2505.19684v2",
        "abstract": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks."
    },
    {
        "date": "2025-05",
        "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2505.19613v1",
        "abstract": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization."
    },
    {
        "date": "2025-05",
        "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts",
        "author": "Shadi Alijani, and Homayoun Najjaran",
        "link": "http://arxiv.org/abs/2505.19587v1",
        "abstract": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."
    },
    {
        "date": "2025-05",
        "title": "Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things",
        "author": "Kai Li, Conggai Li, Xin Yuan, Shenghong Li, Sai Zou, Syed Sohail Ahmed, Wei Ni, Dusit Niyato, Abbas Jamalipour, Falko Dressler, and Ozgur B. Akan",
        "link": "http://arxiv.org/abs/2505.23792v1",
        "abstract": "This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm\nthat embeds zero-trust security principles into the lifecycle of foundation\nmodels (FMs) for Internet of Things (IoT) systems. By integrating core tenets,\nsuch as continuous verification, least privilege access (LPA), data\nconfidentiality, and behavioral analytics into the design, training, and\ndeployment of FMs, ZTFMs can enable secure, privacy-preserving AI across\ndistributed, heterogeneous, and potentially adversarial IoT environments. We\npresent the first structured synthesis of ZTFMs, identifying their potential to\ntransform conventional trust-based IoT architectures into resilient,\nself-defending ecosystems. Moreover, we propose a comprehensive technical\nframework, incorporating federated learning (FL), blockchain-based identity\nmanagement, micro-segmentation, and trusted execution environments (TEEs) to\nsupport decentralized, verifiable intelligence at the network edge. In\naddition, we investigate emerging security threats unique to ZTFM-enabled\nsystems and evaluate countermeasures, such as anomaly detection, adversarial\ntraining, and secure aggregation. Through this analysis, we highlight key open\nresearch challenges in terms of scalability, secure orchestration,\ninterpretable threat attribution, and dynamic trust calibration. This survey\nlays a foundational roadmap for secure, intelligent, and trustworthy IoT\ninfrastructures powered by FMs."
    },
    {
        "date": "2025-05",
        "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare",
        "author": "Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, and Jie M. Zhang",
        "link": "http://arxiv.org/abs/2505.19562v1",
        "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI."
    },
    {
        "date": "2025-05",
        "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds",
        "author": "Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Mehrdad Noori, Gustavo Adolfo Vargas Hakim, David Osowiechi, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers",
        "link": "http://arxiv.org/abs/2505.19546v1",
        "abstract": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC."
    },
    {
        "date": "2025-05",
        "title": "Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning",
        "author": "Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2505.19532v1",
        "abstract": "The current state-of-the-art backdoor attacks against Reinforcement Learning\n(RL) rely upon unrealistically permissive access models, that assume the\nattacker can read (or even write) the victim's policy parameters, observations,\nor rewards. In this work, we question whether such a strong assumption is\nrequired to launch backdoor attacks against RL. To answer this question, we\npropose the \\underline{S}upply-\\underline{C}h\\underline{a}in\n\\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:\ntraining agents using external agents that are provided separately or embedded\nwithin the environment. In contrast to prior works, our attack only relies on\nlegitimate interactions of the RL agent with the supplied agents. Despite this\nlimited access model, by poisoning a mere $3\\%$ of training experiences, our\nattack can successfully activate over $90\\%$ of triggered actions, reducing the\naverage episodic return by $80\\%$ for the victim. Our novel attack demonstrates\nthat RL attacks are likely to become a reality under untrusted RL training\nsupply-chains."
    },
    {
        "date": "2025-05",
        "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation",
        "author": "Mohammed D. Belgoumri, Mohamed Reda Bouadjenek, Hakim Hacid, Imran Razzak, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2505.19527v1",
        "abstract": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry."
    },
    {
        "date": "2025-05",
        "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning",
        "author": "Jiyu Hu, Haijiang Zeng, and Zhen Tian",
        "link": "http://arxiv.org/abs/2505.19522v1",
        "abstract": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments."
    },
    {
        "date": "2025-05",
        "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation",
        "author": "Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, and Tianlong Chen",
        "link": "http://arxiv.org/abs/2505.19504v1",
        "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."
    },
    {
        "date": "2025-05",
        "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation",
        "author": "Kaichao Jiang, He Wang, Xiaoshuai Hao, Xiulong Yang, Ajian Liu, Qi Chu, and Yunfeng Diao",
        "link": "http://arxiv.org/abs/2505.19459v1",
        "abstract": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs."
    },
    {
        "date": "2025-05",
        "title": "An Empirical Study of JavaScript Inclusion Security Issues in Chrome Extensions",
        "author": "Chong Guan",
        "link": "http://arxiv.org/abs/2505.19456v1",
        "abstract": "JavaScript, a scripting language employed to augment the capabilities of web\nbrowsers within web pages or browser extensions, utilizes code segments termed\nJavaScript inclusions. While the security aspects of JavaScript inclusions in\nweb pages have undergone substantial scrutiny, a thorough investigation into\nthe security of such inclusions within browser extensions remains absent,\ndespite the divergent security paradigms governing these environments. This\nstudy presents a systematic measurement of JavaScript inclusions in Chrome\nextensions, employing a hybrid methodology encompassing static and dynamic\nanalysis to identify these inclusions. The analysis of 36,324 extensions\nrevealed 350,784 JavaScript inclusions. Subsequent security assessment\nindicated that, although the majority of these inclusions originate from local\nfiles within the extensions rather than external servers, 22 instances of\nvulnerable remote JavaScript inclusions were identified. These remote\ninclusions present potential avenues for malicious actors to execute arbitrary\ncode within the extension's execution context. Furthermore, an analysis of\nJavaScript library utilization within Chrome extensions disclosed the prevalent\nuse of susceptible and outdated libraries, notably within numerous widely\nadopted extensions."
    },
    {
        "date": "2025-05",
        "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents",
        "author": "Ye Ye",
        "link": "http://arxiv.org/abs/2505.19436v1",
        "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."
    },
    {
        "date": "2025-05",
        "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains",
        "author": "Jiawen Zhang, Zhenwei Zhang, Shun Zheng, Xumeng Wen, Jia Li, and Jiang Bian",
        "link": "http://arxiv.org/abs/2505.19397v1",
        "abstract": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs."
    },
    {
        "date": "2025-05",
        "title": "SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition",
        "author": "Yunbo Liu, Xukui Qin, Yifan Gao, Xiang Li, and Chengwei Feng",
        "link": "http://arxiv.org/abs/2505.19369v1",
        "abstract": "Human Activity Recognition (HAR) using wearable sensor data has become a\ncentral task in mobile computing, healthcare, and human-computer interaction.\nDespite the success of traditional deep learning models such as CNNs and RNNs,\nthey often struggle to capture long-range temporal dependencies and contextual\nrelevance across multiple sensor channels. To address these limitations, we\npropose SETransformer, a hybrid deep neural architecture that combines\nTransformer-based temporal modeling with channel-wise squeeze-and-excitation\n(SE) attention and a learnable temporal attention pooling mechanism. The model\ntakes raw triaxial accelerometer data as input and leverages global\nself-attention to capture activity-specific motion dynamics over extended time\nwindows, while adaptively emphasizing informative sensor channels and critical\ntime steps.\n  We evaluate SETransformer on the WISDM dataset and demonstrate that it\nsignificantly outperforms conventional models including LSTM, GRU, BiLSTM, and\nCNN baselines. The proposed model achieves a validation accuracy of 84.68\\% and\na macro F1-score of 84.64\\%, surpassing all baseline architectures by a notable\nmargin. Our results show that SETransformer is a competitive and interpretable\nsolution for real-world HAR tasks, with strong potential for deployment in\nmobile and ubiquitous sensing applications."
    },
    {
        "date": "2025-05",
        "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
        "author": "Amit Chakraborty, Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, and Sachin Shetty",
        "link": "http://arxiv.org/abs/2505.19364v1",
        "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful\nmachine learning models through cloud-based APIs, offering scalability and ease\nof deployment. However, these services are vulnerable to model extraction\nattacks, where adversaries repeatedly query the application programming\ninterface (API) to reconstruct a functionally similar model, compromising\nintellectual property and security. Despite various defense strategies being\nproposed, many suffer from high computational costs, limited adaptability to\nevolving attack techniques, and a reduction in performance for legitimate\nusers. In this paper, we introduce a Resilient Adaptive Defense Framework for\nModel Extraction Attack Protection (RADEP), a multifaceted defense framework\ndesigned to counteract model extraction attacks through a multi-layered\nsecurity approach. RADEP employs progressive adversarial training to enhance\nmodel resilience against extraction attempts. Malicious query detection is\nachieved through a combination of uncertainty quantification and behavioral\npattern analysis, effectively identifying adversarial queries. Furthermore, we\ndevelop an adaptive response mechanism that dynamically modifies query outputs\nbased on their suspicion scores, reducing the utility of stolen models.\nFinally, ownership verification is enforced through embedded watermarking and\nbackdoor triggers, enabling reliable identification of unauthorized model use.\nExperimental evaluations demonstrate that RADEP significantly reduces\nextraction success rates while maintaining high detection accuracy with minimal\nimpact on legitimate queries. Extensive experiments show that RADEP effectively\ndefends against model extraction attacks and remains resilient even against\nadaptive adversaries, making it a reliable security framework for MLaaS models."
    },
    {
        "date": "2025-05",
        "title": "Evaluating Query Efficiency and Accuracy of Transfer Learning-based Model Extraction Attack in Federated Learning",
        "author": "Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Marc Vucovich, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, and Sachin Shetty",
        "link": "http://arxiv.org/abs/2505.23791v1",
        "abstract": "Federated Learning (FL) is a collaborative learning framework designed to\nprotect client data, yet it remains highly vulnerable to Intellectual Property\n(IP) threats. Model extraction (ME) attacks pose a significant risk to Machine\nLearning as a Service (MLaaS) platforms, enabling attackers to replicate\nconfidential models by querying black-box (without internal insight) APIs.\nDespite FL's privacy-preserving goals, its distributed nature makes it\nparticularly susceptible to such attacks. This paper examines the vulnerability\nof FL-based victim models to two types of model extraction attacks. For various\nfederated clients built under the NVFlare platform, we implemented ME attacks\nacross two deep learning architectures and three image datasets. We evaluate\nthe proposed ME attack performance using various metrics, including accuracy,\nfidelity, and KL divergence. The experiments show that for different FL\nclients, the accuracy and fidelity of the extracted model are closely related\nto the size of the attack query set. Additionally, we explore a transfer\nlearning based approach where pretrained models serve as the starting point for\nthe extraction process. The results indicate that the accuracy and fidelity of\nthe fine-tuned pretrained extraction models are notably higher, particularly\nwith smaller query sets, highlighting potential advantages for attackers."
    },
    {
        "date": "2025-05",
        "title": "Co-evolutionary Dynamics of Attack and Defence in Cybersecurity",
        "author": "Adeela Bashir, Zia Ush Shamszaman, Zhao Song, and The Anh Han",
        "link": "http://arxiv.org/abs/2505.19338v1",
        "abstract": "In the evolving digital landscape, it is crucial to study the dynamics of\ncyberattacks and defences. This study uses an Evolutionary Game Theory (EGT)\nframework to investigate the evolutionary dynamics of attacks and defences in\ncyberspace. We develop a two-population asymmetric game between attacker and\ndefender to capture the essential factors of costs, potential benefits, and the\nprobability of successful defences. Through mathematical analysis and numerical\nsimulations, we find that systems with high defence intensities show stability\nwith minimal attack frequencies, whereas low-defence environments show\ninstability, and are vulnerable to attacks. Furthermore, we find five\nequilibria, where the strategy pair always defend and attack emerged as the\nmost likely stable state as cyber domain is characterised by a continuous\nbattle between defenders and attackers. Our theoretical findings align with\nreal-world data from past cyber incidents, demonstrating the interdisciplinary\nimpact, such as fraud detection, risk management and cybersecurity\ndecision-making. Overall, our analysis suggests that adaptive cybersecurity\nstrategies based on EGT can improve resource allocation, enhance system\nresilience, and reduce the overall risk of cyberattacks. By incorporating\nreal-world data, this study demonstrates the applicability of EGT in addressing\nthe evolving nature of cyber threats and the need for secure digital ecosystems\nthrough strategic planning and proactive defence measures."
    },
    {
        "date": "2025-05",
        "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis",
        "author": "Kazi Mahathir Rahman, Showrin Rahman, and Sharmin Sultana Srishty",
        "link": "http://arxiv.org/abs/2505.19291v1",
        "abstract": "Text-embedded image generation plays a critical role in industries such as\ngraphic design, advertising, and digital content creation. Text-to-Image\ngeneration methods leveraging diffusion models, such as TextDiffuser-2, have\ndemonstrated promising results in producing images with embedded text.\nTextDiffuser-2 effectively generates bounding box layouts that guide the\nrendering of visual text, achieving high fidelity and coherence. However,\nexisting approaches often rely on resource-intensive processes and are limited\nin their ability to run efficiently on both CPU and GPU platforms. To address\nthese challenges, we propose a novel two-stage pipeline that integrates\nreinforcement learning (RL) for rapid and optimized text layout generation with\na diffusion-based image synthesis model. Our RL-based approach significantly\naccelerates the bounding box prediction step while reducing overlaps, allowing\nthe system to run efficiently on both CPUs and GPUs. Extensive evaluations\ndemonstrate that our framework maintains or surpasses TextDiffuser-2's quality\nin text placement and image synthesis, with markedly faster runtime and\nincreased flexibility. Extensive evaluations demonstrate that our framework\nmaintains or surpasses TextDiffuser-2's quality in text placement and image\nsynthesis, with markedly faster runtime and increased flexibility. Our approach\nhas been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore\nmetrics close to state-of-the-art models, while being 97.64% more faster and\nrequiring only 2MB of memory to run."
    },
    {
        "date": "2025-05",
        "title": "BSAGIoT: A Bayesian Security Aspect Graph for Internet of Things (IoT)",
        "author": "Zeinab Lashkaripour, Masoud Khosravi-Farmad, AhmadReza Montazerolghaem, and Razieh Rezaee",
        "link": "http://arxiv.org/abs/2505.19283v1",
        "abstract": "IoT is a dynamic network of interconnected things that communicate and\nexchange data, where security is a significant issue. Previous studies have\nmainly focused on attack classifications and open issues rather than presenting\na comprehensive overview on the existing threats and vulnerabilities. This\nknowledge helps analyzing the network in the early stages even before any\nattack takes place. In this paper, the researchers have proposed different\nsecurity aspects and a novel Bayesian Security Aspects Dependency Graph for IoT\n(BSAGIoT) to illustrate their relations. The proposed BSAGIoT is a generic\nmodel applicable to any IoT network and contains aspects from five categories\nnamed data, access control, standard, network, and loss. This proposed Bayesian\nSecurity Aspect Graph (BSAG) presents an overview of the security aspects in\nany given IoT network. The purpose of BSAGIoT is to assist security experts in\nanalyzing how a successful compromise and/or a failed breach could impact the\noverall security and privacy of the respective IoT network. In addition, root\ncause identification of security challenges, how they affect one another, their\nimpact on IoT networks via topological sorting, and risk assessment could be\nachieved. Hence, to demonstrate the feasibility of the proposed method,\nexperimental results with various scenarios has been presented, in which the\nsecurity aspects have been quantified based on the network configurations. The\nresults indicate the impact of the aspects on each other and how they could be\nutilized to mitigate and/or eliminate the security and privacy deficiencies in\nIoT networks."
    },
    {
        "date": "2025-05",
        "title": "Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning",
        "author": "Hui Ma, Kai Yang, and Yang Jiao",
        "link": "http://arxiv.org/abs/2505.19263v1",
        "abstract": "Network traffic prediction plays a crucial role in intelligent network\noperation. Traditional prediction methods often rely on centralized training,\nnecessitating the transfer of vast amounts of traffic data to a central server.\nThis approach can lead to latency and privacy concerns. To address these\nissues, federated learning integrated with differential privacy has emerged as\na solution to improve data privacy and model robustness in distributed\nsettings. Nonetheless, existing federated learning protocols are vulnerable to\nByzantine attacks, which may significantly compromise model robustness.\nDeveloping a robust and privacy-preserving prediction model in the presence of\nByzantine clients remains a significant challenge. To this end, we propose an\nasynchronous differential federated learning framework based on\ndistributionally robust optimization. The proposed framework utilizes multiple\nclients to train the prediction model collaboratively with local differential\nprivacy. In addition, regularization techniques have been employed to further\nimprove the Byzantine robustness of the models. We have conducted extensive\nexperiments on three real-world datasets, and the results elucidate that our\nproposed distributed algorithm can achieve superior performance over existing\nmethods."
    },
    {
        "date": "2025-05",
        "title": "ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \\& Slow Reasoning for Robust Agent Defense",
        "author": "Shiyu Xiang, Tong Zhang, and Ronghao Chen",
        "link": "http://arxiv.org/abs/2505.19260v1",
        "abstract": "LLM Agents are becoming central to intelligent systems. However, their\ndeployment raises serious safety concerns. Existing defenses largely rely on\n\"Safety Checks\", which struggle to capture the complex semantic risks posed by\nharmful user inputs or unsafe agent behaviors - creating a significant semantic\ngap between safety checks and real-world risks. To bridge this gap, we propose\na novel defense framework, ALRPHFS (Adversarially Learned Risk Patterns with\nHierarchical Fast & Slow Reasoning). ALRPHFS consists of two core components:\n(1) an offline adversarial self-learning loop to iteratively refine a\ngeneralizable and balanced library of risk patterns, substantially enhancing\nrobustness without retraining the base LLM, and (2) an online hierarchical fast\n& slow reasoning engine that balances detection effectiveness with\ncomputational efficiency. Experimental results demonstrate that our approach\nachieves superior overall performance compared to existing baselines, achieving\na best-in-class average accuracy of 80% and exhibiting strong generalizability\nacross agents and tasks."
    },
    {
        "date": "2025-05",
        "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees",
        "author": "Sourav Ganguly, Arnob Ghosh, Kishan Panaganti, and Adam Wierman",
        "link": "http://arxiv.org/abs/2505.19238v1",
        "abstract": "Constrained decision-making is essential for designing safe policies in\nreal-world control systems, yet simulated environments often fail to capture\nreal-world adversities. We consider the problem of learning a policy that will\nmaximize the cumulative reward while satisfying a constraint, even when there\nis a mismatch between the real model and an accessible simulator/nominal model.\nIn particular, we consider the robust constrained Markov decision problem\n(RCMDP) where an agent needs to maximize the reward and satisfy the constraint\nagainst the worst possible stochastic model under the uncertainty set centered\naround an unknown nominal model. Primal-dual methods, effective for standard\nconstrained MDP (CMDP), are not applicable here because of the lack of the\nstrong duality property. Further, one cannot apply the standard robust\nvalue-iteration based approach on the composite value function either as the\nworst case models may be different for the reward value function and the\nconstraint value function. We propose a novel technique that effectively\nminimizes the constraint value function--to satisfy the constraints; on the\nother hand, when all the constraints are satisfied, it can simply maximize the\nrobust reward value function. We prove that such an algorithm finds a policy\nwith at most $\\epsilon$ sub-optimality and feasible policy after\n$O(\\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we\ndo not need to employ a binary search, thus, we reduce the computation time by\nat least 4x for smaller value of discount factor ($\\gamma$) and by at least 6x\nfor larger value of $\\gamma$."
    },
    {
        "date": "2025-05",
        "title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation",
        "author": "Peiran Sun",
        "link": "http://arxiv.org/abs/2505.19194v1",
        "abstract": "Adversarial attack reveals the vulnerability of deep learning models. For\nabout a decade, countless attack and defense methods have been proposed,\nleading to robustified classifiers and better understanding of models. Among\nthese methods, curvature-based approaches have attracted attention because it\nis assumed that high curvature may give rise to rough decision boundary.\nHowever, the most commonly used \\textit{curvature} is the curvature of loss\nfunction, scores or other parameters from within the model as opposed to\ndecision boundary curvature, since the former can be relatively easily formed\nusing second order derivative. In this paper, we propose a new query-efficient\nmethod, dynamic curvature estimation(DCE), to estimate the decision boundary\ncurvature in a black-box setting. Our approach is based on CGBA, a black-box\nadversarial attack. By performing DCE on a wide range of classifiers, we\ndiscovered, statistically, a connection between decision boundary curvature and\nadversarial robustness. We also propose a new attack method, curvature dynamic\nblack-box attack(CDBA) with improved performance using the dynamically\nestimated curvature."
    },
    {
        "date": "2025-05",
        "title": "Penetration Testing for System Security: Methods and Practical Approaches",
        "author": "Wei Zhang, Ju Xing, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.19174v1",
        "abstract": "Penetration testing refers to the process of simulating hacker attacks to\nevaluate the security of information systems . This study aims not only to\nclarify the theoretical foundations of penetration testing but also to explain\nand demonstrate the complete testing process, including how network system\nadministrators may simulate attacks using various penetration testing methods.\nMethodologically, the paper outlines the five basic stages of a typical\npenetration test: intelligence gathering, vulnerability scanning, vulnerability\nexploitation, privilege escalation, and post-exploitation activities. In each\nphase, specific tools and techniques are examined in detail, along with\npractical guidance on their use. To enhance the practical relevance of the\nstudy, the paper also presents a real-life case study, illustrating how a\ncomplete penetration test is conducted in a real-world environment. Through\nthis case, readers can gain insights into the detailed procedures and applied\ntechniques, thereby deepening their understanding of the practical value of\npenetration testing. Finally, the paper summarizes the importance and necessity\nof penetration testing in securing information systems and maintaining network\nintegrity, and it explores future trends and development directions for the\nfield. Overall, the findings of this paper offer valuable references for both\nresearchers and practitioners, contributing meaningfully to the improvement of\npenetration testing practices and the advancement of cybersecurity as a whole."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Influence Functions with Flat Validation Minima",
        "author": "Xichen Ye, Yifan Wu, Weizhong Zhang, Cheng Jin, and Yifan Chen",
        "link": "http://arxiv.org/abs/2505.19097v1",
        "abstract": "The Influence Function (IF) is a widely used technique for assessing the\nimpact of individual training samples on model predictions. However, existing\nIF methods often fail to provide reliable influence estimates in deep neural\nnetworks, particularly when applied to noisy training data. This issue does not\nstem from inaccuracies in parameter change estimation, which has been the\nprimary focus of prior research, but rather from deficiencies in loss change\nestimation, specifically due to the sharpness of validation risk. In this work,\nwe establish a theoretical connection between influence estimation error,\nvalidation set risk, and its sharpness, underscoring the importance of flat\nvalidation minima for accurate influence estimation. Furthermore, we introduce\na novel estimation form of Influence Function specifically designed for flat\nvalidation minima. Experimental results across various tasks validate the\nsuperiority of our approach."
    },
    {
        "date": "2025-05",
        "title": "Towards Generalized Proactive Defense against Face Swapping with Contour-Hybrid Watermark",
        "author": "Ruiyang Xia, Dawei Zhou, Decheng Liu, Lin Yuan, Jie Li, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2505.19081v2",
        "abstract": "Face swapping, recognized as a privacy and security concern, has prompted\nconsiderable defensive research. With the advancements in AI-generated content,\nthe discrepancies between the real and swapped faces have become nuanced.\nConsidering the difficulty of forged traces detection, we shift the focus to\nthe face swapping purpose and proactively embed elaborate watermarks against\nunknown face swapping techniques. Given that the constant purpose is to swap\nthe original face identity while preserving the background, we concentrate on\nthe regions surrounding the face to ensure robust watermark generation, while\nembedding the contour texture and face identity information to achieve\nprogressive image determination. The watermark is located in the facial contour\nand contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our\napproach generalizes face swapping detection without requiring any swapping\ntechniques during training and the storage of large-scale messages in advance.\nExperiments conducted across 8 face swapping techniques demonstrate the\nsuperiority of our approach compared with state-of-the-art passive and\nproactive detectors while achieving a favorable balance between the image\nquality and watermark robustness."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management",
        "author": "Chen Avin, Zvi Lotker, Shie Mannor, Gil Shabat, Hanan Shteingart, and Roey Yadgar",
        "link": "http://arxiv.org/abs/2505.19061v1",
        "abstract": "Motivated by dynamic parameter optimization in finite, but large action\n(configurations) spaces, this work studies the nonstochastic multi-armed bandit\n(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We\npropose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can\nuse state-of-the-art existing \"flat\" algorithms, but additionally clusters\nsimilar configurations to exploit local structures and adapt to changing\nenvironments. We prove that in the worst-case scenario, such clustering\napproach cannot hurt too much and ABoB guarantees a standard worst-case regret\nbound of $O\\left(k^{\\frac{1}{2}}T^{\\frac{1}{2}}\\right)$, where $T$ is the\nnumber of rounds and $k$ is the number of arms, matching the traditional flat\napproach. However, under favorable conditions related to the algorithm\nproperties, clusters properties, and certain Lipschitz conditions, the regret\nbound can be improved to $O\\left(k^{\\frac{1}{4}}T^{\\frac{1}{2}}\\right)$.\nSimulations and experiments on a real storage system demonstrate that ABoB,\nusing standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and\nfaster convergence than the flat method, up to 50% improvement in known\nprevious setups, nonstochastic and stochastic, as well as in our settings."
    },
    {
        "date": "2025-05",
        "title": "Distributionally Robust Deep Q-Learning",
        "author": "Chung I Lu, Julian Sester, and Aijia Zhang",
        "link": "http://arxiv.org/abs/2505.19058v1",
        "abstract": "We propose a novel distributionally robust $Q$-learning algorithm for the\nnon-tabular case accounting for continuous state spaces where the state\ntransition of the underlying Markov decision process is subject to model\nuncertainty. The uncertainty is taken into account by considering the\nworst-case transition from a ball around a reference probability measure. To\ndetermine the optimal policy under the worst-case state transition, we solve\nthe associated non-linear Bellman equation by dualising and regularising the\nBellman operator with the Sinkhorn distance, which is then parameterized with\ndeep neural networks. This approach allows us to modify the Deep Q-Network\nalgorithm to optimise for the worst case state transition.\n  We illustrate the tractability and effectiveness of our approach through\nseveral applications, including a portfolio optimisation task based on\nS\\&{P}~500 data."
    },
    {
        "date": "2025-05",
        "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
        "author": "Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, and George Turkiyyah",
        "link": "http://arxiv.org/abs/2505.19056v1",
        "abstract": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."
    },
    {
        "date": "2025-05",
        "title": "A quantitative notion of economic security for smart contract compositions",
        "author": "Emily Priyadarshini, and Massimo Bartoletti",
        "link": "http://arxiv.org/abs/2505.19006v1",
        "abstract": "Decentralized applications are often composed of multiple interconnected\nsmart contracts. This is especially evident in DeFi, where protocols are\nheavily intertwined and rely on a variety of basic building blocks such as\ntokens, decentralized exchanges and lending protocols. A crucial security\nchallenge in this setting arises when adversaries target individual components\nto cause systemic economic losses. Existing security notions focus on\ndetermining the existence of these attacks, but fail to quantify the effect of\nmanipulating individual components on the overall economic security of the\nsystem. In this paper, we introduce a quantitative security notion that\nmeasures how an attack on a single component can amplify economic losses of the\noverall system. We study the fundamental properties of this notion and apply it\nto assess the security of key compositions. In particular, we analyse\nunder-collateralized loan attacks in systems made of lending protocols and\ndecentralized exchanges."
    },
    {
        "date": "2025-05",
        "title": "Secure IVSHMEM: End-to-End Shared-Memory Protocol with Hypervisor-CA Handshake and In-Kernel Access Control",
        "author": "Hyunwoo Kim, Jaeseong Lee, Sunpyo Hong, and Changmin Han",
        "link": "http://arxiv.org/abs/2505.19004v1",
        "abstract": "In-host shared memory (IVSHMEM) enables high-throughput, zero-copy\ncommunication between virtual machines, but today's implementations lack any\nsecurity control, allowing any application to eavesdrop or tamper with the\nIVSHMEM region. This paper presents Secure IVSHMEM, a protocol that provides\nend-to-end mutual authentication and fine-grained access enforcement with\nnegligible performance cost. We combine three techniques to ensure security:\n(1) channel separation and kernel module access control, (2)hypervisor-mediated\nhandshake for end-to-end service authentication, and (3)application-level\nintegration for abstraction and performance mitigation. In microbenchmarks,\nSecure IVSHMEM completes its one-time handshake in under 200ms and sustains\ndata-plane round-trip latencies within 5\\% of the unmodified baseline, with\nnegligible bandwidth overhead. We believe this design is ideally suited for\nsafety and latency-critical in-host domains, such as automotive systems, where\nboth performance and security are paramount."
    },
    {
        "date": "2025-05",
        "title": "SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours",
        "author": "Catalina Tan, Yipeng Hu, and Shaheer U. Saeed",
        "link": "http://arxiv.org/abs/2505.18989v1",
        "abstract": "Accurate tumour segmentation is vital for various targeted diagnostic and\ntherapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.\nManual delineation is extremely labour-intensive, requiring substantial expert\ntime. Fully-supervised machine learning models aim to automate such\nlocalisation tasks, but require a large number of costly and often subjective\n3D voxel-level labels for training. The high-variance and subjectivity in such\nlabels impacts model generalisability, even when large datasets are available.\nHistopathology labels may offer more objective labels but the infeasibility of\nacquiring pixel-level annotations to develop tumour localisation methods based\non histology remains challenging in-vivo. In this work, we propose a novel\nweakly-supervised semantic segmentation framework called SPARS (Self-Play\nAdversarial Reinforcement Learning for Segmentation), which utilises an object\npresence classifier, trained on a small number of image-level binary cancer\npresence labels, to localise cancerous regions on CT scans. Such binary labels\nof patient-level cancer presence can be sourced more feasibly from biopsies and\nhistopathology reports, enabling a more objective cancer localisation on\nmedical images. Evaluating with real patient data, we observed that SPARS\nyielded a mean dice score of $77.3 \\pm 9.4$, which outperformed other\nweakly-supervised methods by large margins. This performance was comparable\nwith recent fully-supervised methods that require voxel-level annotations. Our\nresults demonstrate the potential of using SPARS to reduce the need for\nextensive human-annotated labels to detect cancer in real-world healthcare\nsettings."
    },
    {
        "date": "2025-05",
        "title": "Exemplifying Emerging Phishing: QR-based Browser-in-The-Browser (BiTB) Attack",
        "author": "Muhammad Wahid Akram, Keshav Sood, Muneeb Ul Hassan, and Basant Subba",
        "link": "http://arxiv.org/abs/2505.18944v1",
        "abstract": "Lately, cybercriminals constantly formulate productive approaches to exploit\nindividuals. This article exemplifies an innovative attack, namely QR-based\nBrowser-in-The-Browser (BiTB), using proficiencies of Large Language Model\n(LLM) i.e. Google Gemini. The presented attack is a fusion of two emerging\nattacks: BiTB and Quishing (QR code phishing). Our study underscores attack's\nsimplistic implementation utilizing malicious prompts provided to Gemini-LLM.\nMoreover, we presented a case study to highlight a lucrative attack method, we\nalso performed an experiment to comprehend the attack execution on victims'\ndevice. The findings of this work obligate the researchers' contributions in\nconfronting this type of phishing attempts through LLMs."
    },
    {
        "date": "2025-05",
        "title": "Robust Stability Analysis of Positive Lure System with Neural Network Feedback",
        "author": "Hamidreza Montazeri Hedesh, Moh. Kamalul Wafi, Bahram Shafai, and Milad Siami",
        "link": "http://arxiv.org/abs/2505.18912v1",
        "abstract": "This paper investigates the robustness of the Lur'e problem under positivity\nconstraints, drawing on results from the positive Aizerman conjecture and the\nrobustness properties of Metzler matrices. Specifically, we consider a control\nsystem of Lur'e type in which not only the linear part includes parametric\nuncertainty but also the nonlinear sector bound is unknown. We investigate\ntools from positive linear systems to effectively solve the problems in\ncomplicated and uncertain nonlinear systems. By leveraging the positivity\ncharacteristic of the system, we derive an explicit formula for the stability\nradius of Lur'e systems. Furthermore, we extend our analysis to systems with\nneural network (NN) feedback loops. Building on this approach, we also propose\na refinement method for sector bounds of feedforward neural networks (FFNNs).\nThis study introduces a scalable and efficient approach for robustness analysis\nof both Lur'e and NN-controlled systems. Finally, the proposed results are\nsupported by illustrative examples."
    },
    {
        "date": "2025-05",
        "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos",
        "author": "Andrea Ramazzina, Vittorio Giammarino, Matteo El-Hariry, and Mario Bijelic",
        "link": "http://arxiv.org/abs/2505.18899v1",
        "abstract": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO."
    },
    {
        "date": "2025-05",
        "title": "Security Concerns for Large Language Models: A Survey",
        "author": "Miles Q. Li, and Benjamin C. M. Fung",
        "link": "http://arxiv.org/abs/2505.18889v2",
        "abstract": "Large Language Models (LLMs) such as GPT-4 and its recent iterations,\nGoogle's Gemini, Anthropic's Claude 3 models, and xAI's Grok have caused a\nrevolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. In this survey, we provide a\ncomprehensive overview of the emerging security concerns around LLMs,\ncategorizing threats into prompt injection and jailbreaking, adversarial\nattacks such as input perturbations and data poisoning, misuse by malicious\nactors for purposes such as generating disinformation, phishing emails, and\nmalware, and worrisome risks inherent in autonomous LLM agents. A significant\nfocus has been recently placed on the latter, exploring goal misalignment,\nemergent deception, self-preservation instincts, and the potential for LLMs to\ndevelop and pursue covert, misaligned objectives, a behavior known as scheming,\nwhich may even persist through safety training. We summarize recent academic\nand industrial studies from 2022 to 2025 that exemplify each threat, analyze\nproposed defenses and their limitations, and identify open challenges in\nsecuring LLM-based applications. We conclude by emphasizing the importance of\nadvancing robust, multi-layered security strategies to ensure LLMs are safe and\nbeneficial."
    },
    {
        "date": "2025-05",
        "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders",
        "author": "Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, and Seyed-Mohsen Moosavi-Dezfooli",
        "link": "http://arxiv.org/abs/2505.18884v1",
        "abstract": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."
    },
    {
        "date": "2025-05",
        "title": "Securing Credit Inquiries: The Role of Real-Time User Approval in Preventing SSN Identity Theft",
        "author": "Gogulakrishnan Thiyagarajan, Vinay Bist, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2505.18861v1",
        "abstract": "Unauthorized credit inquiries are also a central entry point for identity\ntheft, with Social Security Numbers (SSNs) being widely utilized in fraudulent\ncases. Traditional credit inquiry systems do not usually possess strict user\nauthentication, making them vulnerable to unauthorized access. This paper\nproposes a real-time user authorization system to enhance security by enforcing\nexplicit user approval before processing any credit inquiry. The system employs\nreal-time verification and approval techniques. This ensures that the\nauthorized user only approves or rejects a credit check request. It minimizes\nthe risks of interference by third parties. Apart from enhancing security, this\nsystem complies with regulations like the General Data Protection Regulation\n(GDPR) and the Fair Credit Reporting Act (FCRA) while maintaining a seamless\nuser experience. This article discusses the technical issues, scaling-up\nissues, and ways of implementing real-time user authorization in financial\nsystems. Through this framework, financial institutions can drastically\nminimize the risk of identity theft, avert unauthorized credit checks, and\nincrease customer trust in the credit verification system."
    },
    {
        "date": "2025-05",
        "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation",
        "author": "Chika Maduabuchi, Hao Chen, Yujin Han, and Jindong Wang",
        "link": "http://arxiv.org/abs/2505.21545v1",
        "abstract": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm"
    },
    {
        "date": "2025-05",
        "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
        "author": "Kazuki Egashira, Robin Staab, Mark Vero, Jingxuan He, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.23786v1",
        "abstract": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\nollama and llama.cpp frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense."
    },
    {
        "date": "2025-05",
        "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models",
        "author": "Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Katherine Lee, Milad Nasr, Sahra Ghalebikesabi, Niloofar Mireshghallah, Meenatchi Sundaram Mutu Selva Annamalai, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Franziska Boenisch, Adam Dziedzic, and A. Feder Cooper",
        "link": "http://arxiv.org/abs/2505.18773v1",
        "abstract": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested."
    },
    {
        "date": "2025-05",
        "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
        "author": "Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, and Bin Xiao",
        "link": "http://arxiv.org/abs/2505.18766v1",
        "abstract": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion."
    },
    {
        "date": "2025-05",
        "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models",
        "author": "Yuanhe Zhang, Xinyue Wang, Haoran Gao, Zhenhong Zhou, Fanyu Meng, Yuyao Zhang, and Sen Su",
        "link": "http://arxiv.org/abs/2505.18680v1",
        "abstract": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks."
    },
    {
        "date": "2025-05",
        "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics",
        "author": "Pankaj Kumar, and Subhankar Mishra",
        "link": "http://arxiv.org/abs/2505.18658v1",
        "abstract": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."
    },
    {
        "date": "2025-05",
        "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework",
        "author": "Yifan Zhu, Chao Zhang, Xin Shi, Xueqiao Zhang, Yi Yang, and Yawei Luo",
        "link": "http://arxiv.org/abs/2505.18572v1",
        "abstract": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges."
    },
    {
        "date": "2025-05",
        "title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Haoran Xin, Jiatong Li, Dongzhe Zhang, Minghong Fang, Zhuqing Liu, Lihai Nie, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.18543v1",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating\nhallucinations in large language models by incorporating external knowledge\nduring inference. However, this integration introduces new security\nvulnerabilities, particularly to poisoning attacks. Although prior work has\nexplored various poisoning strategies, a thorough assessment of their practical\nthreat to RAG systems remains missing. To address this gap, we propose the\nfirst comprehensive benchmark framework for evaluating poisoning attacks on\nRAG. Our benchmark covers 5 standard question answering (QA) datasets and 10\nexpanded variants, along with 13 poisoning attack methods and 7 defense\nmechanisms, representing a broad spectrum of existing techniques. Using this\nbenchmark, we conduct a comprehensive evaluation of all included attacks and\ndefenses across the full dataset spectrum. Our findings show that while\nexisting attacks perform well on standard QA datasets, their effectiveness\ndrops significantly on the expanded versions. Moreover, our results demonstrate\nthat various advanced RAG architectures, such as sequential, branching,\nconditional, and loop RAG, as well as multi-turn conversational RAG, multimodal\nRAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning\nattacks. Notably, current defense techniques fail to provide robust protection,\nunderscoring the pressing need for more resilient and generalizable defense\nstrategies."
    },
    {
        "date": "2025-05",
        "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network",
        "author": "Xiaobin Rong, Dahan Wang, Qinwen Hu, Yushi Wang, Yuxiang Hu, and Jing Lu",
        "link": "http://arxiv.org/abs/2505.18533v1",
        "abstract": "Universal speech enhancement aims to handle input speech with different\ndistortions and input formats. To tackle this challenge, we present TS-URGENet,\na Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.\nTo address various distortions, the proposed system employs a novel three-stage\narchitecture consisting of a filling stage, a separation stage, and a\nrestoration stage. The filling stage mitigates packet loss by preliminarily\nfilling lost regions under noise interference, ensuring signal continuity. The\nseparation stage suppresses noise, reverberation, and clipping distortion to\nimprove speech clarity. Finally, the restoration stage compensates for\nbandwidth limitation, codec artifacts, and residual packet loss distortion,\nrefining the overall speech quality. Our proposed TS-URGENet achieved\noutstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd\nin Track 1."
    },
    {
        "date": "2025-05",
        "title": "Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise",
        "author": "Lucas Tecot, Di Luo, and Cho-Jui Hsieh",
        "link": "http://arxiv.org/abs/2505.18478v1",
        "abstract": "Advancements in quantum computing have spurred significant interest in\nharnessing its potential for speedups over classical systems. However, noise\nremains a major obstacle to achieving reliable quantum algorithms. In this\nwork, we present a provably noise-resilient training theory and algorithm to\nenhance the robustness of parameterized quantum circuit classifiers. Our\nmethod, with a natural connection to Evolutionary Strategies, guarantees\nresilience to parameter noise with minimal adjustments to commonly used\noptimization algorithms. Our approach is function-agnostic and adaptable to\nvarious quantum circuits, successfully demonstrated in quantum phase\nclassification tasks. By developing provably guaranteed optimization theory\nwith quantum circuits, our work opens new avenues for practical, robust\napplications of near-term quantum computers."
    },
    {
        "date": "2025-05",
        "title": "A Dual Basis Approach for Structured Robust Euclidean Distance Geometry",
        "author": "Chandra Kundu, Abiy Tasissa, and HanQin Cai",
        "link": "http://arxiv.org/abs/2505.18414v1",
        "abstract": "Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean\ndistances of a given point configuration, finds many applications in modern\nmachine learning. This paper considers the setting where only a set of anchor\nnodes is used to collect the distances between themselves and the rest. In the\npresence of potential outliers, it results in a structured partial observation\non EDM with partial corruptions. Note that an EDM can be connected to a\npositive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by\nrecent development of non-orthogonal dual basis in optimization, we propose a\nnovel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual\nBasis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the\nunderlying point configuration. The exact recovery guarantees have been\nestablished in terms of both the Gram matrix and point configuration, under\nsome mild conditions. Empirical experiments show superior performance of\nRoDEoDB on sensor localization and molecular conformation datasets."
    },
    {
        "date": "2025-05",
        "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks",
        "author": "Yuqi Jia, Zedian Shao, Yupei Liu, Jinyuan Jia, Dawn Song, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.18333v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and\nseveral defenses have recently been proposed, often claiming to mitigate these\nattacks successfully. However, we argue that existing studies lack a principled\napproach to evaluating these defenses. In this paper, we argue the need to\nassess defenses across two critical dimensions: (1) effectiveness, measured\nagainst both existing and adaptive prompt injection attacks involving diverse\ntarget and injected prompts, and (2) general-purpose utility, ensuring that the\ndefense does not compromise the foundational capabilities of the LLM. Our\ncritical evaluation reveals that prior studies have not followed such a\ncomprehensive evaluation methodology. When assessed using this principled\napproach, we show that existing defenses are not as successful as previously\nreported. This work provides a foundation for evaluating future defenses and\nguiding their development. Our code and data are available at:\nhttps://github.com/PIEval123/PIEval."
    },
    {
        "date": "2025-05",
        "title": "An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs",
        "author": "Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, and Arka Pal",
        "link": "http://arxiv.org/abs/2505.18332v1",
        "abstract": "Recent advances in Large Language Models (LLMs) have led to the widespread\nadoption of third-party inference services, raising critical privacy concerns.\nExisting methods of performing private third-party inference, such as Secure\nMultiparty Computation (SMPC), often rely on cryptographic methods. However,\nthese methods are thousands of times slower than standard unencrypted\ninference, and fail to scale to large modern LLMs. Therefore, recent lines of\nwork have explored the replacement of expensive encrypted nonlinear\ncomputations in SMPC with statistical obfuscation methods - in particular,\nrevealing permuted hidden states to the third parties, with accompanying strong\nclaims of the difficulty of reversal into the unpermuted states. In this work,\nwe begin by introducing a novel reconstruction technique that can recover\noriginal prompts from hidden states with nearly perfect accuracy across\nmultiple state-of-the-art LLMs. We then show that extensions of our attack are\nnearly perfectly effective in reversing permuted hidden states of LLMs,\ndemonstrating the insecurity of three recently proposed privacy schemes. We\nfurther dissect the shortcomings of prior theoretical `proofs' of permuation\nsecurity which allow our attack to succeed. Our findings highlight the\nimportance of rigorous security analysis in privacy-preserving LLM inference."
    },
    {
        "date": "2025-05",
        "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
        "author": "Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, and Ningyu Zhang",
        "link": "http://arxiv.org/abs/2505.20322v1",
        "abstract": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."
    },
    {
        "date": "2025-05",
        "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles",
        "author": "Varun Ajith, Anindya Pal, Saumik Bhattacharya, and Sayantari Ghosh",
        "link": "http://arxiv.org/abs/2505.18106v1",
        "abstract": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields."
    },
    {
        "date": "2025-05",
        "title": "Towards more transferable adversarial attack in black-box manner",
        "author": "Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, and Chun Pong Lau",
        "link": "http://arxiv.org/abs/2505.18097v1",
        "abstract": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses."
    },
    {
        "date": "2025-05",
        "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
        "author": "Zhishuai Liu, and Pan Xu",
        "link": "http://arxiv.org/abs/2505.18044v1",
        "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting."
    },
    {
        "date": "2025-05",
        "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
        "author": "Changyeol Lee, Yongho Shin, and Hyung-Chan An",
        "link": "http://arxiv.org/abs/2505.18043v1",
        "abstract": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature."
    },
    {
        "date": "2025-05",
        "title": "Superplatforms Have to Attack AI Agents",
        "author": "Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2505.17861v1",
        "abstract": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents."
    },
    {
        "date": "2025-05",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
        "author": "Masahiro Fujisawa, Masaki Adachi, and Michael A. Osborne",
        "link": "http://arxiv.org/abs/2505.17859v1",
        "abstract": "Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods."
    },
    {
        "date": "2025-05",
        "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems",
        "author": "Yuanya She",
        "link": "http://arxiv.org/abs/2505.18234v1",
        "abstract": "In this paper, we propose a robust and reinforcement-learning-enhanced\nnetwork intrusion detection system (NIDS) designed for class-imbalanced and\nfew-shot attack scenarios in Industrial Internet of Things (IIoT) environments.\nOur model integrates a TabTransformer for effective tabular feature\nrepresentation with Proximal Policy Optimization (PPO) to optimize\nclassification decisions via policy learning. Evaluated on the\nTON\\textunderscore IoT benchmark, our method achieves a macro F1-score of\n97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes\nlike man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%,\nshowcasing strong robustness and few-shot detection capabilities. Extensive\nablation experiments confirm the complementary roles of TabTransformer and PPO\nin mitigating class imbalance and improving generalization. These results\nhighlight the potential of combining transformer-based tabular learning with\nreinforcement learning for real-world NIDS applications."
    },
    {
        "date": "2025-05",
        "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
        "author": "Anna Van Elst, Igor Colin, and Stephan Cl\u00e9men\u00e7on",
        "link": "http://arxiv.org/abs/2505.17836v2",
        "abstract": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes."
    },
    {
        "date": "2025-05",
        "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
        "author": "Nicolas Castanet, Olivier Sigaud, and Sylvain Lamprier",
        "link": "http://arxiv.org/abs/2505.17830v1",
        "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge."
    },
    {
        "date": "2025-05",
        "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition",
        "author": "Ping Li, Jianan Ni, and Bo Pang",
        "link": "http://arxiv.org/abs/2505.17807v1",
        "abstract": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid."
    },
    {
        "date": "2025-05",
        "title": "Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture",
        "author": "Ildi Alla, and Valeria Loscri",
        "link": "http://arxiv.org/abs/2505.17776v1",
        "abstract": "Emerging 5G millimeter-wave and sub-6 GHz networks enable high-accuracy\nindoor localization, but security and privacy vulnerabilities pose serious\nchallenges. In this paper, we identify and address threats including location\nspoofing and adversarial signal manipulation against 5G-based indoor\nlocalization. We formalize a threat model encompassing attackers who inject\nforged radio signals or perturb channel measurements to mislead the\nlocalization system. To defend against these threats, we propose an\nadversary-resilient localization architecture that combines deep learning\nfingerprinting with physical domain knowledge. Our approach integrates\nmulti-anchor Channel Impulse Response (CIR) fingerprints with Time Difference\nof Arrival (TDoA) features and known anchor positions in a hybrid Convolutional\nNeural Network (CNN) and multi-head attention network. This design inherently\nchecks geometric consistency and dynamically down-weights anomalous signals,\nmaking localization robust to tampering. We formulate the secure localization\nproblem and demonstrate, through extensive experiments on a public 5G indoor\ndataset, that the proposed system achieves a mean error approximately 0.58 m\nunder mixed Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) trajectories in\nbenign conditions and gracefully degrades to around 0.81 m under attack\nscenarios. We also show via ablation studies that each architecture component\n(attention mechanism, TDoA, etc.) is critical for both accuracy and resilience,\nreducing errors by 4-5 times compared to baselines. In addition, our system\nruns in real-time, localizing the user in just 1 ms on a simple CPU. The code\nhas been released to ensure reproducibility\n(https://github.com/sec5gloc/Sec5GLoc)."
    },
    {
        "date": "2025-05",
        "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation",
        "author": "Akira Tanimoto",
        "link": "http://arxiv.org/abs/2505.17717v1",
        "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods."
    },
    {
        "date": "2025-05",
        "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors",
        "author": "Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, and Yong Li",
        "link": "http://arxiv.org/abs/2505.17682v1",
        "abstract": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports",
        "author": "Hayato Aida, Kosuke Takahashi, and Takahiro Omi",
        "link": "http://arxiv.org/abs/2505.17625v1",
        "abstract": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats."
    },
    {
        "date": "2025-05",
        "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
        "author": "Linbao Li, Yannan Liu, Daojing He, and Yu Li",
        "link": "http://arxiv.org/abs/2505.17598v1",
        "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack."
    },
    {
        "date": "2025-05",
        "title": "Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications",
        "author": "Kushal Khatiwada, Jayden Hopper, Joseph Cheatham, Ayan Joshi, and Sabur Baidya",
        "link": "http://arxiv.org/abs/2505.17586v1",
        "abstract": "The Internet of Things (IoT) and Large Language Models (LLMs) have been two\nmajor emerging players in the information technology era. Although there has\nbeen significant coverage of their individual capabilities, our literature\nsurvey sheds some light on the integration and interaction of LLMs and IoT\ndevices - a mutualistic relationship in which both parties leverage the\ncapabilities of the other. LLMs like OpenAI's ChatGPT, Anthropic's Claude,\nGoogle's Gemini/BERT, any many more, all demonstrate powerful capabilities in\nnatural language understanding and generation, enabling more intuitive and\ncontext-aware interactions across diverse IoT applications such as smart\ncities, healthcare systems, industrial automation, and smart home environments.\nDespite these opportunities, integrating these resource-intensive LLMs into IoT\ndevices that lack the state-of-the-art computational power is a challenging\ntask. The security of these edge devices is another major concern as they can\neasily act as a backdoor to private networks if the LLM integration is sloppy\nand unsecured. This literature survey systematically explores the current\nstate-of-the-art in applying LLMs within IoT, emphasizing their applications in\nvarious domains/sectors of society, the significant role they play in enhancing\nIoT security through anomaly detection and threat mitigation, and strategies\nfor effective deployment using edge computing frameworks. Finally, this survey\nhighlights existing challenges, identifies future research directions, and\nunderscores the need for cross-disciplinary collaboration to fully realize the\ntransformative potential of integrating LLMs and IoT."
    },
    {
        "date": "2025-05",
        "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation",
        "author": "Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, and Eisuke Koizumi",
        "link": "http://arxiv.org/abs/2505.17579v1",
        "abstract": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack."
    },
    {
        "date": "2025-05",
        "title": "Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters",
        "author": "Kwangsu Lee",
        "link": "http://arxiv.org/abs/2505.17527v1",
        "abstract": "Distributed broadcast encryption (DBE) is a variant of broadcast encryption\n(BE) that can efficiently transmit a message to a subset of users, in which\nusers independently generate user private keys and user public keys instead of\na central trusted authority generating user keys. In this paper, we propose a\nDBE scheme with constant size ciphertexts, constant size private keys, and\nlinear size public parameters, and prove the adaptive security of our DBE\nscheme under static assumptions in composite-order bilinear groups. The\nprevious efficient DBE schemes with constant size ciphertexts and constant size\nprivate keys are proven secure under the $q$-Type assumption or have a drawback\nof having quadratic size public parameters. In contrast, our DBE scheme is the\nfirst DBE scheme with linear size public parameters proven adaptively secure\nunder static assumptions in composite-order bilinear groups."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning",
        "author": "Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.17509v1",
        "abstract": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings."
    },
    {
        "date": "2025-05",
        "title": "Demonstration of Quantum-Secure Communications in a Nuclear Reactor",
        "author": "Konstantinos Gkouliaras, Vasileios Theos, True Miller, Brian Jowers, George Kennedy, Andy Grant, Terry Cronin, Philip G. Evans, and Stylianos Chatzidakis",
        "link": "http://arxiv.org/abs/2505.17502v2",
        "abstract": "Quantum key distribution (QKD), one of the latest cryptographic techniques,\nfounded on the laws of quantum mechanics rather than mathematical complexity,\npromises for the first time unconditional secure remote communications.\nIntegrating this technology into the next generation nuclear systems - designed\nfor universal data collection and real-time sharing as well as cutting-edge\ninstrumentation and increased dependency on digital technologies - could\nprovide significant benefits enabling secure, unattended, and autonomous\noperation in remote areas, e.g., microreactors and fission batteries. However,\nany practical implementation on a critical reactor system must meet strict\nrequirements on latency, control system compatibility, stability, and\nperformance under operational transients. Here, we report the complete\nend-to-end demonstration of a phase-encoding decoy-state BB84 protocol QKD\nsystem under prototypic conditions on Purdue's fully digital nuclear reactor,\nPUR-1. The system was installed in PUR-1 successfully executing real-time\nencryption and decryption of 2,000 signals over optic fiber distances up to 82\nkm using OTP-based encryption and up to 140 km with AES-based encryption. For a\ncore of 68 signals, OTP-secure communication was achieved for up to 135 km. The\nQKD system maintained a stable secret key rate of 320 kbps and a quantum bit\nerror of 3.8% at 54 km. Our results demonstrate that OTP-based encryption\nintroduces minimal latency while the more key-efficient AES and ASCON\nencryption schemes can significantly increase the number of signals encrypted\nwithout latency penalties. Additionally, implementation of a dynamic key pool\nensures several hours of secure key availability during potential system\ndowntimes. This work shows the potential of quantum-based secure remote\ncommunications for future digitally driven nuclear reactor technologies."
    },
    {
        "date": "2025-05",
        "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
        "author": "Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, and Kaixiang Yang",
        "link": "http://arxiv.org/abs/2505.17501v1",
        "abstract": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance."
    },
    {
        "date": "2025-05",
        "title": "SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy",
        "author": "Junru Lin, Mingzhe Liu, Songze Li, and Xuechao Wang",
        "link": "http://arxiv.org/abs/2505.17466v1",
        "abstract": "Recent years have witnessed a rapid development of platform economy, as it\neffectively addresses the trust dilemma between untrusted online buyers and\nmerchants. However, malicious platforms can misuse users' funds and\ninformation, causing severe security concerns. Previous research efforts aimed\nat enhancing security in platform payment systems often sacrificed processing\nperformance, while those focusing on processing efficiency struggled to\ncompletely prevent fund and information misuse. In this paper, we introduce\nSecurePay, a secure, yet performant payment processing system for platform\neconomy. SecurePay is the first payment system that combines permissioned\nblockchain with central bank digital currency (CBDC) to ensure fund security,\ninformation security, and resistance to collusion by intermediaries; it also\nfacilitates counter-party auditing, closed-loop regulation, and enhances\noperational efficiency for transaction settlement. We develop a full\nimplementation of the proposed SecurePay system, and our experiments conducted\non personal devices demonstrate a throughput of 256.4 transactions per second\nand an average latency of 4.29 seconds, demonstrating a comparable processing\nefficiency with a centralized system, with a significantly improved security\nlevel."
    },
    {
        "date": "2025-05",
        "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds",
        "author": "Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, and Junhui Hou",
        "link": "http://arxiv.org/abs/2505.17442v1",
        "abstract": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline."
    },
    {
        "date": "2025-05",
        "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models",
        "author": "Hefei Mei, Zirui Wang, Shen You, Minjing Dong, and Chang Xu",
        "link": "http://arxiv.org/abs/2505.17440v1",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM"
    },
    {
        "date": "2025-05",
        "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness",
        "author": "Enyi Jiang, Changming Xu, Nischay Singh, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2505.17406v1",
        "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency."
    }
]