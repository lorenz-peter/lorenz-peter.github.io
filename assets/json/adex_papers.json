[
    {
        "date": "2025-04",
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "author": "Rijul Magu, Arka Dutta, Sean Kim, Ashiqur R. KhudaBukhsh, and Munmun De Choudhury",
        "link": "http://arxiv.org/abs/2504.06160v2",
        "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Training of Reward Models",
        "author": "Alexander Bukharin, Haifeng Qian, Shengyang Sun, Adithya Renduchintala, Soumye Singhal, Zhilin Wang, Oleksii Kuchaiev, Olivier Delalleau, and Tuo Zhao",
        "link": "http://arxiv.org/abs/2504.06141v1",
        "abstract": "Reward modeling has emerged as a promising approach for the scalable\nalignment of language models. However, contemporary reward models (RMs) often\nlack robustness, awarding high rewards to low-quality, out-of-distribution\n(OOD) samples. This can lead to reward hacking, where policies exploit\nunintended shortcuts to maximize rewards, undermining alignment. To address\nthis challenge, we introduce Adv-RM, a novel adversarial training framework\nthat automatically identifies adversarial examples -- responses that receive\nhigh rewards from the target RM but are OOD and of low quality. By leveraging\nreinforcement learning, Adv-RM trains a policy to generate adversarial examples\nthat reliably expose vulnerabilities in large state-of-the-art reward models\nsuch as Nemotron 340B RM. Incorporating these adversarial examples into the\nreward training process improves the robustness of RMs, mitigating reward\nhacking and enhancing downstream performance in RLHF. We demonstrate that\nAdv-RM significantly outperforms conventional RM training, increasing stability\nand enabling more effective RLHF training in both synthetic and real-data\nsettings."
    },
    {
        "date": "2025-04",
        "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions",
        "author": "Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, and Konghui Guo",
        "link": "http://arxiv.org/abs/2504.06121v1",
        "abstract": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Security Analysis of Thumbnail-Preserving Image Encryption and a New Framework",
        "author": "Dong Xie, Zhiyang Li, Shuangxi Guo, Fulong Chen, and Peng Hu",
        "link": "http://arxiv.org/abs/2504.06083v1",
        "abstract": "As a primary encryption primitive balancing the privacy and searchability of\ncloud storage images, thumbnail preserving encryption (TPE) enables users to\nquickly identify the privacy personal image on the cloud and request this image\nfrom the owner through a secure channel. In this paper, we have found that two\ndifferent plaintext images may produce the same thumbnail. It results in the\nfailure of search strategy because the collision of thumbnail occurs. To\naddress this serious security issues, we conduct an in-depth analysis on the\ncollision probabilities of thumbnails, and then propose a new TPE framework,\ncalled multi-factor thumbnail preserving encryption (MFTPE). It starts from the\ncollision probability of two blocks, extend to the probabilities of two images\nand ultimately to N images. Then, we in detail describe three specific MFTPE\nconstructions preserving different combinations of factors, i.e., the sum and\nthe geometric mean, the sum and the range, and the sum and the weighted mean.\nThe theoretical and experimental results demonstrate that the proposed MFTPE\nreduces the probability of thumbnails, exhibits strong robustness, and also\neffectively resists face detection and noise attacks."
    },
    {
        "date": "2025-04",
        "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining",
        "author": "Mrityunjoy Gain, Kitae Kim, Avi Deb Raha, Apurba Adhikary, Eui-Nam Huh, Zhu Han, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2504.06004v1",
        "abstract": "In this paper, we propose the FedFeat+ framework, which distinctively\nseparates feature extraction from classification. We develop a two-tiered model\ntraining process: following local training, clients transmit their weights and\nsome features extracted from the feature extractor from the final local epochs\nto the server. The server aggregates these models using the FedAvg method and\nsubsequently retrains the global classifier utilizing the shared features. The\nclassifier retraining process enhances the model's understanding of the\nholistic view of the data distribution, ensuring better generalization across\ndiverse datasets. This improved generalization enables the classifier to\nadaptively influence the feature extractor during subsequent local training\nepochs. We establish a balance between enhancing model accuracy and\nsafeguarding individual privacy through the implementation of differential\nprivacy mechanisms. By incorporating noise into the feature vectors shared with\nthe server, we ensure that sensitive data remains confidential. We present a\ncomprehensive convergence analysis, along with theoretical reasoning regarding\nperformance enhancement and privacy preservation. We validate our approach\nthrough empirical evaluations conducted on benchmark datasets, including\nCIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering\nto stringent privacy guarantees. The experimental results demonstrate that the\nFedFeat+ framework, despite using only a lightweight two-layer CNN classifier,\noutperforms the FedAvg method in both IID and non-IID scenarios, achieving\naccuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10,\nCIFAR-100, and Fashion-MNIST datasets."
    },
    {
        "date": "2025-04",
        "title": "Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis",
        "author": "Jixuan Wu, Lei Xie, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.05968v2",
        "abstract": "Smart contracts are a secure and trustworthy application that plays a vital\nrole in decentralized applications in various fields such as insurance,the\ninternet, and gaming. However, in recent years, smart contract security\nbreaches have occurred frequently, and due to their financial properties, they\nhave caused huge economic losses, such as the most famous security incident\n\"The DAO\" which caused a loss of over $60 million in Ethereum. This has drawn a\nlot of attention from all sides. Writing a secure smart contract is now a\ncritical issue.This paper focuses on Ether smart contracts and explains the\nmain components of Ether, smart contract architecture and mechanism.The\nenvironment used in this paper is the Ethernet environment, using remix online\ncompilation platform and Solidity language, according to the four security\nevents of American Chain, The DAO, Parity and KotET, the principles of integer\noverflow attack, reentrant attack, access control attack and denial of service\nattack are studied and analyzed accordingly, and the scenarios of these\nvulnerabilities are reproduced, and the measures to prevent them are given.\nFinally, preventive measures are given. In addition, the principles of short\naddress attack, early transaction attack and privileged function exposure\nattack are also introduced in detail, and security measures are proposed.As\nvulnerabilities continue to emerge, their classification will also evolve. The\nanalysis and research of the current vulnerabilities are also to lay a solid\nfoundation for avoiding more vulnerabilities."
    },
    {
        "date": "2025-04",
        "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
        "author": "Kuntian Zhang, Simin Yu, Yaoshu Wang, Makoto Onizuka, and Chuan Xiao",
        "link": "http://arxiv.org/abs/2504.05945v1",
        "abstract": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs."
    },
    {
        "date": "2025-04",
        "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching",
        "author": "Weijun Li, Ansh Arora, Xuanli He, Mark Dras, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2504.05902v1",
        "abstract": "The exponential increase in the parameters of Deep Neural Networks (DNNs) has\nsignificantly raised the cost of independent training, particularly for\nresource-constrained entities. As a result, there is a growing reliance on\nopen-source models. However, the opacity of training processes exacerbates\nsecurity risks, making these models more vulnerable to malicious threats, such\nas backdoor attacks, while simultaneously complicating defense mechanisms.\nMerging homogeneous models has gained attention as a cost-effective\npost-training defense. However, we notice that existing strategies, such as\nweight averaging, only partially mitigate the influence of poisoned parameters\nand remain ineffective in disrupting the pervasive spurious correlations\nembedded across model parameters. We propose a novel module-switching strategy\nto break such spurious correlations within the model's propagation path. By\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\nour approach against backdoor attacks targeting text and vision domains. Our\nmethod achieves effective backdoor mitigation even when incorporating a couple\nof compromised models, e.g., reducing the average attack success rate (ASR) to\n22% compared to 31.9% with the best-performing baseline on SST-2."
    },
    {
        "date": "2025-04",
        "title": "Channel State Information Analysis for Jamming Attack Detection in Static and Dynamic UAV Networks -- An Experimental Study",
        "author": "Pavlo Mykytyn, Ronald Chitauro, Zoya Dyka, and Peter Langendoerfer",
        "link": "http://arxiv.org/abs/2504.05832v1",
        "abstract": "Networks built on the IEEE 802.11 standard have experienced rapid growth in\nthe last decade. Their field of application is vast, including smart home\napplications, Internet of Things (IoT), and short-range high throughput static\nand dynamic inter-vehicular communication networks. Within such networks,\nChannel State Information (CSI) provides a detailed view of the state of the\ncommunication channel and represents the combined effects of multipath\npropagation, scattering, phase shift, fading, and power decay. In this work, we\ninvestigate the problem of jamming attack detection in static and dynamic\nvehicular networks. We utilize ESP32-S3 modules to set up a communication\nnetwork between an Unmanned Aerial Vehicle (UAV) and a Ground Control Station\n(GCS), to experimentally test the combined effects of a constant jammer on\nrecorded CSI parameters, and the feasibility of jamming detection through CSI\nanalysis in static and dynamic communication scenarios."
    },
    {
        "date": "2025-04",
        "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models",
        "author": "Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, and Lin Wang",
        "link": "http://arxiv.org/abs/2504.05815v1",
        "abstract": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/."
    },
    {
        "date": "2025-04",
        "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
        "author": "Hao Zhang, Yanping Zha, Qingwei Zhuang, Zhenfeng Shao, and Jiayi Ma",
        "link": "http://arxiv.org/abs/2504.05795v2",
        "abstract": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios."
    },
    {
        "date": "2025-04",
        "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators",
        "author": "Xitao Li, Haijun Wang, Jiang Wu, and Ting Liu",
        "link": "http://arxiv.org/abs/2504.05689v1",
        "abstract": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods."
    },
    {
        "date": "2025-04",
        "title": "kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization",
        "author": "Keren Shao, Ke Chen, Matthew Baas, and Shlomo Dubnov",
        "link": "http://arxiv.org/abs/2504.05686v1",
        "abstract": "Robustness is critical in zero-shot singing voice conversion (SVC). This\npaper introduces two novel methods to strengthen the robustness of the kNN-VC\nframework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic\nemphasis, resulting in dull sounds and ringing artifacts. To address this, we\nleverage the bijection between WavLM, pitch contours, and spectrograms to\nperform additive synthesis, integrating the resulting waveform into the model\nto mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a\nkey perceptual factor in SVC. To enhance smoothness, we propose a new distance\nmetric that filters out unsuitable kNN candidates and optimize the summing\nweights of the candidates during inference. Although our techniques are built\non the kNN-VC framework for implementation convenience, they are broadly\napplicable to general concatenative neural synthesis models. Experimental\nresults validate the effectiveness of these modifications in achieving robust\nSVC. Demo: http://knnsvc.com Code: https://github.com/SmoothKen/knn-svc"
    },
    {
        "date": "2025-04",
        "title": "Secure Smart Contract with Control Flow Integrity",
        "author": "Zhiyang Chen, Sidi Mohamed Beillahi, Pasha Barahimi, Cyrus Minwalla, Han Du, Andreas Veneris, and Fan Long",
        "link": "http://arxiv.org/abs/2504.05509v1",
        "abstract": "Smart contracts power decentralized financial (DeFi) services but are\nvulnerable to complex security exploits that can lead to significant financial\nlosses. Existing security measures often fail to adequately protect these\ncontracts due to the composability of DeFi protocols and the increasing\nsophistication of attacks. Through a large-scale empirical study of historical\ntransactions from the 30 hacked DeFi protocols, we discovered that while benign\ntransactions typically exhibit a limited number of unique control flows, in\nstark contrast, attack transactions consistently introduce novel, previously\nunobserved control flows. Building on these insights, we developed CrossGuard,\na novel framework that enforces control flow integrity in real-time to secure\nsmart contracts. Crucially, CrossGuard does not require prior knowledge of\nspecific hacks; instead, it dynamically enforces control flow whitelisting\npolicies and applies simplification heuristics at runtime. This approach\nmonitors and prevents potential attacks by reverting all transactions that do\nnot adhere to the established control flow whitelisting rules. Our evaluation\ndemonstrates that CrossGuard effectively blocks 28 of the 30 analyzed attacks\nwhen configured only once prior to contract deployment, maintaining a low false\npositive rate of 0.28% and minimal additional gas costs. These results\nunderscore the efficacy of applying control flow integrity to smart contracts,\nsignificantly enhancing security beyond traditional methods and addressing the\nevolving threat landscape in the DeFi ecosystem."
    },
    {
        "date": "2025-04",
        "title": "SelfMAD: Enhancing Generalization and Robustness in Morphing Attack Detection via Self-Supervised Learning",
        "author": "Marija Ivanovska, Leon Todorov, Naser Damer, Deepak Kumar Jain, Peter Peer, and Vitomir \u0160truc",
        "link": "http://arxiv.org/abs/2504.05504v1",
        "abstract": "With the continuous advancement of generative models, face morphing attacks\nhave become a significant challenge for existing face verification systems due\nto their potential use in identity fraud and other malicious activities.\nContemporary Morphing Attack Detection (MAD) approaches frequently rely on\nsupervised, discriminative models trained on examples of bona fide and morphed\nimages. These models typically perform well with morphs generated with\ntechniques seen during training, but often lead to sub-optimal performance when\nsubjected to novel unseen morphing techniques. While unsupervised models have\nbeen shown to perform better in terms of generalizability, they typically\nresult in higher error rates, as they struggle to effectively capture features\nof subtle artifacts. To address these shortcomings, we present SelfMAD, a novel\nself-supervised approach that simulates general morphing attack artifacts,\nallowing classifiers to learn generic and robust decision boundaries without\noverfitting to the specific artifacts induced by particular face morphing\nmethods. Through extensive experiments on widely used datasets, we demonstrate\nthat SelfMAD significantly outperforms current state-of-the-art MADs, reducing\nthe detection error by more than 64% in terms of EER when compared to the\nstrongest unsupervised competitor, and by more than 66%, when compared to the\nbest performing discriminative MAD model, tested in cross-morph settings. The\nsource code for SelfMAD is available at https://github.com/LeonTodorov/SelfMAD."
    },
    {
        "date": "2025-04",
        "title": "Towards Zero Trust Security in Connected Vehicles: A Comprehensive Survey",
        "author": "Malak Annabi, Abdelhafid Zeroual, and Nadhir Messai",
        "link": "http://arxiv.org/abs/2504.05485v1",
        "abstract": "Zero Trust is the new cybersecurity model that challenges the traditional one\nby promoting continuous verification of users, devices, and applications,\nwhatever their position or origin. This model is critical for reducing the\nattack surface and preventing lateral movement without relying on implicit\ntrust. Adopting the zero trust principle in Intelligent Transportation Systems\n(ITS), especially in the context of connected vehicles (CVs), presents an\nadequate solution in the face of increasing cyber threats, thereby\nstrengthening the ITS environment. This paper offers an understanding of Zero\nTrust security through a comprehensive review of existing literature,\nprinciples, and challenges. It specifically examines its applications in\nemerging technologies, particularly within connected vehicles, addressing\npotential issues and cyber threats faced by CVs. Inclusion/exclusion criteria\nfor the systematic literature review were planned alongside a bibliometric\nanalysis. Moreover, keyword co-occurrence analysis was done, which indicates\ntrends and general themes for the Zero Trust model, Zero Trust implementation,\nand Zero Trust application. Furthermore, the paper explores various ZT models\nproposed in the literature for connected vehicles, shedding light on the\nchallenges associated with their integration into CV systems. Future directions\nof this research will focus on incorporating Zero Trust principles within\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication\nparadigms. This initiative intends to enhance the security posture and safety\nprotocols within interconnected vehicular networks. The proposed research seeks\nto address the unique cybersecurity vulnerabilities inherent in the highly\ndynamic nature of vehicular communication systems."
    },
    {
        "date": "2025-04",
        "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability",
        "author": "Mohammad Hossein Najafi, Mohammad Morsali, Mohammadreza Pashanejad, Saman Soleimani Roudi, Mohammad Norouzi, and Saeed Bagheri Shouraki",
        "link": "http://arxiv.org/abs/2504.05483v1",
        "abstract": "Deep neural networks for medical image classification often fail to\ngeneralize consistently in clinical practice due to violations of the i.i.d.\nassumption and opaque decision-making. This paper examines interpretability in\ndeep neural networks fine-tuned for fracture detection by evaluating model\nperformance against adversarial attack and comparing interpretability methods\nto fracture regions annotated by an orthopedic surgeon. Our findings prove that\nrobust models yield explanations more aligned with clinically meaningful areas,\nindicating that robustness encourages anatomically relevant feature\nprioritization. We emphasize the value of interpretability for facilitating\nhuman-AI collaboration, in which models serve as assistants under a\nhuman-in-the-loop paradigm: clinically plausible explanations foster trust,\nenable error correction, and discourage reliance on AI for high-stakes\ndecisions. This paper investigates robustness and interpretability as\ncomplementary benchmarks for bridging the gap between benchmark performance and\nsafe, actionable clinical deployment."
    },
    {
        "date": "2025-04",
        "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking",
        "author": "Omar De Mitri, Ruyu Wang, and Marco F. Huber",
        "link": "http://arxiv.org/abs/2504.05456v1",
        "abstract": "Generative Adversarial Networks (GANs) have shown impressive results in\nvarious image synthesis tasks. Vast studies have demonstrated that GANs are\nmore powerful in feature and expression learning compared to other generative\nmodels and their latent space encodes rich semantic information. However, the\ntremendous performance of GANs heavily relies on the access to large-scale\ntraining data and deteriorates rapidly when the amount of data is limited. This\npaper aims to provide an overview of GANs, its variants and applications in\nvarious vision tasks, focusing on addressing the limited data issue. We analyze\nstate-of-the-art GANs in limited data regime with designed experiments, along\nwith presenting various methods attempt to tackle this problem from different\nperspectives. Finally, we further elaborate on remaining challenges and trends\nfor future research."
    },
    {
        "date": "2025-04",
        "title": "Adversarial KA",
        "author": "Sviatoslav Dzhenzher, and Michael H. Freedman",
        "link": "http://arxiv.org/abs/2504.05255v1",
        "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs."
    },
    {
        "date": "2025-04",
        "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
        "author": "Jon Guti\u00e9rrez Zaballa, Koldo Basterretxea, and Javier Echanobe",
        "link": "http://arxiv.org/abs/2504.05119v1",
        "abstract": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM."
    },
    {
        "date": "2025-04",
        "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy",
        "author": "Ronghui Zhang, Dakang Lyu, Tengfei Li, Yunfan Wu, Ujjal Manandhar, Benfei Wang, Junzhou Chen, Bolin Gao, Danwei Wang, and Yiqiu Tan",
        "link": "http://arxiv.org/abs/2504.05112v1",
        "abstract": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions."
    },
    {
        "date": "2025-04",
        "title": "AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends",
        "author": "Victor Monzon Baeza, Ra\u00fal Parada, Laura Concha Salor, and Carlos Monzo",
        "link": "http://arxiv.org/abs/2504.05071v1",
        "abstract": "The integration of Artificial Intelligence (AI) in military communications\nand networking is reshaping modern defense strategies, enhancing secure data\nexchange, real-time situational awareness, and autonomous decision-making. This\nsurvey explores how AI-driven technologies improve tactical communication\nnetworks, radar-based data transmission, UAV-assisted relay systems, and\nelectronic warfare resilience. The study highlights AI applications in adaptive\nsignal processing, multi-agent coordination for network optimization,\nradar-assisted target tracking, and AI-driven electronic countermeasures. Our\nwork introduces a novel three-criteria evaluation methodology. It\nsystematically assesses AI applications based on general system objectives,\ncommunications constraints in the military domain, and critical tactical\nenvironmental factors. We analyze key AI techniques for different types of\nlearning applied to multi-domain network interoperability and distributed data\ninformation fusion in military operations. We also address challenges such as\nadversarial AI threats, the real-time adaptability of autonomous communication\nnetworks, and the limitations of current AI models under battlefield\nconditions. Finally, we discuss emerging trends in self-healing networks,\nAI-augmented decision support systems, and intelligent spectrum allocation. We\nprovide a structured roadmap for future AI-driven defense communications and\nnetworking research."
    },
    {
        "date": "2025-04",
        "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
        "author": "Peng Liu, Heng-Chao Li, Sen Lei, Nanqing Liu, Bin Feng, and Xiao Wu",
        "link": "http://arxiv.org/abs/2504.04935v1",
        "abstract": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes."
    },
    {
        "date": "2025-04",
        "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models",
        "author": "Justus Westerhoff, Erblina Purellku, Jakob Hackstein, Leo Pinetzki, and Lorenz Hufe",
        "link": "http://arxiv.org/abs/2504.04893v1",
        "abstract": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
    },
    {
        "date": "2025-04",
        "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
        "author": "Roie Kazoom, Raz Lapid, Moshe Sipper, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2504.04858v1",
        "abstract": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks."
    },
    {
        "date": "2025-04",
        "title": "SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement",
        "author": "Zuying Xie, Changtao Miao, Ajian Liu, Jiabao Guo, Feng Li, Dan Guo, and Yunfeng Diao",
        "link": "http://arxiv.org/abs/2504.04818v1",
        "abstract": "Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods."
    },
    {
        "date": "2025-04",
        "title": "Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection",
        "author": "Liuji Chen, Hao Gao, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.04809v1",
        "abstract": "Tool learning serves as a powerful auxiliary mechanism that extends the\ncapabilities of large language models (LLMs), enabling them to tackle complex\ntasks requiring real-time relevance or high precision operations. Behind its\npowerful capabilities lie some potential security issues. However, previous\nwork has primarily focused on how to make the output of the invoked tools\nincorrect or malicious, with little attention given to the manipulation of tool\nselection. To fill this gap, we introduce, for the first time, a black-box\ntext-based attack that can significantly increase the probability of the target\ntool being selected in this paper. We propose a two-level text perturbation\nattack witha coarse-to-fine granularity, attacking the text at both the word\nlevel and the character level. We conduct comprehensive experiments that\ndemonstrate the attacker only needs to make some perturbations to the tool's\ntextual information to significantly increase the possibility of the target\ntool being selected and ranked higher among the candidate tools. Our research\nreveals the vulnerability of the tool selection process and paves the way for\nfuture research on protecting this process."
    },
    {
        "date": "2025-04",
        "title": "Unsupervised Estimation of Nonlinear Audio Effects: Comparing Diffusion-Based and Adversarial approaches",
        "author": "Eloi Moliner, Michal \u0160vento, Alec Wright, Lauri Juvela, Pavel Rajmic, and Vesa V\u00e4lim\u00e4ki",
        "link": "http://arxiv.org/abs/2504.04751v1",
        "abstract": "Accurately estimating nonlinear audio effects without access to paired\ninput-output signals remains a challenging problem.This work studies\nunsupervised probabilistic approaches for solving this task. We introduce a\nmethod, novel for this application, based on diffusion generative models for\nblind system identification, enabling the estimation of unknown nonlinear\neffects using black- and gray-box models. This study compares this method with\na previously proposed adversarial approach, analyzing the performance of both\nmethods under different parameterizations of the effect operator and varying\nlengths of available effected recordings.Through experiments on guitar\ndistortion effects, we show that the diffusion-based approach provides more\nstable results and is less sensitive to data availability, while the\nadversarial approach is superior at estimating more pronounced distortion\neffects. Our findings contribute to the robust unsupervised blind estimation of\naudio effects, demonstrating the potential of diffusion models for system\nidentification in music technology."
    },
    {
        "date": "2025-04",
        "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models",
        "author": "Yoojin Jung, and Byung Cheol Song",
        "link": "http://arxiv.org/abs/2504.04747v1",
        "abstract": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments."
    },
    {
        "date": "2025-04",
        "title": "On the Robustness of GUI Grounding Models Against Image Attacks",
        "author": "Haoren Zhao, Tianyi Chen, and Zhen Wang",
        "link": "http://arxiv.org/abs/2504.04716v1",
        "abstract": "Graphical User Interface (GUI) grounding models are crucial for enabling\nintelligent agents to understand and interact with complex visual interfaces.\nHowever, these models face significant robustness challenges in real-world\nscenarios due to natural noise and adversarial perturbations, and their\nrobustness remains underexplored. In this study, we systematically evaluate the\nrobustness of state-of-the-art GUI grounding models, such as UGround, under\nthree conditions: natural noise, untargeted adversarial attacks, and targeted\nadversarial attacks. Our experiments, which were conducted across a wide range\nof GUI environments, including mobile, desktop, and web interfaces, have\nclearly demonstrated that GUI grounding models exhibit a high degree of\nsensitivity to adversarial perturbations and low-resolution conditions. These\nfindings provide valuable insights into the vulnerabilities of GUI grounding\nmodels and establish a strong benchmark for future research aimed at enhancing\ntheir robustness in practical applications. Our code is available at\nhttps://github.com/ZZZhr-1/Robust_GUI_Grounding."
    },
    {
        "date": "2025-04",
        "title": "AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing",
        "author": "Lingyue Fu, Ting Long, Jianghao Lin, Wei Xia, Xinyi Dai, Ruiming Tang, Yasheng Wang, Weinan Zhang, and Yong Yu",
        "link": "http://arxiv.org/abs/2504.04706v1",
        "abstract": "Knowledge Tracing (KT) monitors students' knowledge states and simulates\ntheir responses to question sequences. Existing KT models typically follow a\nsingle-step training paradigm, which leads to discrepancies with the multi-step\ninference process required in real-world simulations, resulting in significant\nerror accumulation. This accumulation of error, coupled with the issue of data\nsparsity, can substantially degrade the performance of recommendation models in\nthe intelligent tutoring systems. To address these challenges, we propose a\nnovel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT),\nwhich, for the first time, focuses on the multi-step KT task. More\nspecifically, AdvKT leverages adversarial learning paradigm involving a\ngenerator and a discriminator. The generator mimics high-reward responses,\neffectively reducing error accumulation across multiple steps, while the\ndiscriminator provides feedback to generate synthetic data. Additionally, we\ndesign specialized data augmentation techniques to enrich the training data\nwith realistic variations, ensuring that the model generalizes well even in\nscenarios with sparse data. Experiments conducted on four real-world datasets\ndemonstrate the superiority of AdvKT over existing KT models, showcasing its\nability to address both error accumulation and data sparsity issues\neffectively."
    },
    {
        "date": "2025-04",
        "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection",
        "author": "Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, and Yanning Zhang",
        "link": "http://arxiv.org/abs/2504.04495v1",
        "abstract": "With the increasing adoption of video anomaly detection in intelligent\nsurveillance domains, conventional visual-based detection approaches often\nstruggle with information insufficiency and high false-positive rates in\ncomplex environments. To address these limitations, we present a novel weakly\nsupervised framework that leverages audio-visual collaboration for robust video\nanomaly detection. Capitalizing on the exceptional cross-modal representation\nlearning capabilities of Contrastive Language-Image Pretraining (CLIP) across\nvisual, audio, and textual domains, our framework introduces two major\ninnovations: an efficient audio-visual fusion that enables adaptive cross-modal\nintegration through lightweight parametric adaptation while maintaining the\nfrozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances\ntext embeddings with key multimodal information based on the semantic\ncorrelation between audio-visual features and textual labels, significantly\nimproving CLIP's generalization for the video anomaly detection task. Moreover,\nto enhance robustness against modality deficiency during inference, we further\ndevelop an uncertainty-driven feature distillation module that synthesizes\naudio-visual representations from visual-only inputs. This module employs\nuncertainty modeling based on the diversity of audio-visual features to\ndynamically emphasize challenging features during the distillation process. Our\nframework demonstrates superior performance across multiple benchmarks, with\naudio integration significantly boosting anomaly detection accuracy in various\nscenarios. Notably, with unimodal data enhanced by uncertainty-driven\ndistillation, our approach consistently outperforms current unimodal VAD\nmethods."
    },
    {
        "date": "2025-04",
        "title": "Selective Masking Adversarial Attack on Automatic Speech Recognition Systems",
        "author": "Zheng Fang, Shenyi Zhang, Tao Wang, Bowen Li, Lingchen Zhao, and Zhangyi Wang",
        "link": "http://arxiv.org/abs/2504.04394v1",
        "abstract": "Extensive research has shown that Automatic Speech Recognition (ASR) systems\nare vulnerable to audio adversarial attacks. Current attacks mainly focus on\nsingle-source scenarios, ignoring dual-source scenarios where two people are\nspeaking simultaneously. To bridge the gap, we propose a Selective Masking\nAdversarial attack, namely SMA attack, which ensures that one audio source is\nselected for recognition while the other audio source is muted in dual-source\nscenarios. To better adapt to the dual-source scenario, our SMA attack\nconstructs the normal dual-source audio from the muted audio and selected\naudio. SMA attack initializes the adversarial perturbation with a small\nGaussian noise and iteratively optimizes it using a selective masking\noptimization algorithm. Extensive experiments demonstrate that the SMA attack\ncan generate effective and imperceptible audio adversarial examples in the\ndual-source scenario, achieving an average success rate of attack of 100% and\nsignal-to-noise ratio of 37.15dB on Conformer-CTC, outperforming the baselines."
    },
    {
        "date": "2025-04",
        "title": "WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems",
        "author": "Sameera K. M., Vinod P., Anderson Rocha, Rafidha Rehiman K. A., and Mauro Conti",
        "link": "http://arxiv.org/abs/2504.04367v1",
        "abstract": "In the era of data expansion, ensuring data privacy has become increasingly\ncritical, posing significant challenges to traditional AI-based applications.\nIn addition, the increasing adoption of IoT devices has introduced significant\ncybersecurity challenges, making traditional Network Intrusion Detection\nSystems (NIDS) less effective against evolving threats, and privacy concerns\nand regulatory restrictions limit their deployment. Federated Learning (FL) has\nemerged as a promising solution, allowing decentralized model training while\nmaintaining data privacy to solve these issues. However, despite implementing\nprivacy-preserving technologies, FL systems remain vulnerable to adversarial\nattacks. Furthermore, data distribution among clients is not heterogeneous in\nthe FL scenario. We propose WeiDetect, a two-phase, server-side defense\nmechanism for FL-based NIDS that detects malicious participants to address\nthese challenges. In the first phase, local models are evaluated using a\nvalidation dataset to generate validation scores. These scores are then\nanalyzed using a Weibull distribution, identifying and removing malicious\nmodels. We conducted experiments to evaluate the effectiveness of our approach\nin diverse attack settings. Our evaluation included two popular datasets,\nCIC-Darknet2020 and CSE-CIC-IDS2018, tested under non-IID data distributions.\nOur findings highlight that WeiDetect outperforms state-of-the-art defense\napproaches, improving higher target class recall up to 70% and enhancing the\nglobal model's F1 score by 1% to 14%."
    },
    {
        "date": "2025-04",
        "title": "A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects",
        "author": "Aos Mulahuwaish, Basheer Qolomany, Kevin Gyorick, Jacques Bou Abdo, Mohammed Aledhari, Junaid Qadir, Kathleen Carley, and Ala Al-Fuqaha",
        "link": "http://arxiv.org/abs/2504.04311v1",
        "abstract": "In today's digital era, the Internet, especially social media platforms,\nplays a significant role in shaping public opinions, attitudes, and beliefs.\nUnfortunately, the credibility of scientific information sources is often\nundermined by the spread of misinformation through various means, including\ntechnology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep\nfakes. This manipulation of public discourse serves antagonistic business\nagendas and compromises civil society. In response to this challenge, a new\nscientific discipline has emerged: social cybersecurity."
    },
    {
        "date": "2025-04",
        "title": "Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement",
        "author": "Anastasis Kratsios, Xiaofei Shi, Qiang Sun, and Zhanhao Zhang",
        "link": "http://arxiv.org/abs/2504.04300v1",
        "abstract": "We present a general computational framework for solving continuous-time\nfinancial market equilibria under minimal modeling assumptions while\nincorporating realistic financial frictions, such as trading costs, and\nsupporting multiple interacting agents. Inspired by generative adversarial\nnetworks (GANs), our approach employs a novel generative deep reinforcement\nlearning framework with a decoupling feedback system embedded in the\nadversarial training loop, which we term as the \\emph{reinforcement link}. This\narchitecture stabilizes the training dynamics by incorporating feedback from\nthe discriminator. Our theoretically guided feedback mechanism enables the\ndecoupling of the equilibrium system, overcoming challenges that hinder\nconventional numerical algorithms. Experimentally, our algorithm not only\nlearns but also provides testable predictions on how asset returns and\nvolatilities emerge from the endogenous trading behavior of market\nparticipants, where traditional analytical methods fall short. The design of\nour model is further supported by an approximation guarantee."
    },
    {
        "date": "2025-04",
        "title": "Impact of Error Rate Misreporting on Resource Allocation in Multi-tenant Quantum Computing and Defense",
        "author": "Subrata Das, and Swaroop Ghosh",
        "link": "http://arxiv.org/abs/2504.04285v1",
        "abstract": "Cloud-based quantum service providers allow multiple users to run programs on\nshared hardware concurrently to maximize resource utilization and minimize\noperational costs. This multi-tenant computing (MTC) model relies on the error\nparameters of the hardware for fair qubit allocation and scheduling, as\nerror-prone qubits can degrade computational accuracy asymmetrically for users\nsharing the hardware. To maintain low error rates, quantum providers perform\nperiodic hardware calibration, often relying on third-party calibration\nservices. If an adversary within this calibration service misreports error\nrates, the allocator can be misled into making suboptimal decisions even when\nthe physical hardware remains unchanged. We demonstrate such an attack model in\nwhich an adversary strategically misreports qubit error rates to reduce\nhardware throughput, and probability of successful trial (PST) for two\npreviously proposed allocation frameworks, i.e. Greedy and Community-Based\nDynamic Allocation Partitioning (COMDAP). Experimental results show that\nadversarial misreporting increases execution latency by 24% and reduces PST by\n7.8%. We also propose to identify inconsistencies in reported error rates by\nanalyzing statistical deviations in error rates across calibration cycles."
    },
    {
        "date": "2025-04",
        "title": "AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System",
        "author": "Chuadhry Mujeeb Ahmed",
        "link": "http://arxiv.org/abs/2504.04187v1",
        "abstract": "Malicious examples are crucial for evaluating the robustness of machine\nlearning algorithms under attack, particularly in Industrial Control Systems\n(ICS). However, collecting normal and attack data in ICS environments is\nchallenging due to the scarcity of testbeds and the high cost of human\nexpertise. Existing datasets are often limited by the domain expertise of\npractitioners, making the process costly and inefficient. The lack of\ncomprehensive attack pattern data poses a significant problem for developing\nrobust anomaly detection methods. In this paper, we propose a novel approach\nthat combines data-centric and design-centric methodologies to generate attack\npatterns using large language models (LLMs). Our results demonstrate that the\nattack patterns generated by LLMs not only surpass the quality and quantity of\nthose created by human experts but also offer a scalable solution that does not\nrely on expensive testbeds or pre-existing attack examples. This multi-agent\nbased approach presents a promising avenue for enhancing the security and\nresilience of ICS environments."
    },
    {
        "date": "2025-04",
        "title": "Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible",
        "author": "Chaoyi Lu, Yiding Sun, Pengbo Li, and Zhichuan Yang",
        "link": "http://arxiv.org/abs/2504.04081v2",
        "abstract": "As an emerging paradigm of federated learning, asynchronous federated\nlearning offers significant speed advantages over traditional synchronous\nfederated learning. Unlike synchronous federated learning, which requires\nwaiting for all clients to complete updates before aggregation, asynchronous\nfederated learning aggregates the models that have arrived in realtime, greatly\nimproving training speed. However, this mechanism also introduces the issue of\nclient model version inconsistency. When the differences between models of\ndifferent versions during aggregation become too large, it may lead to\nconflicts, thereby reducing the models accuracy. To address this issue, this\npaper proposes an asynchronous federated learning version correction algorithm\nbased on knowledge distillation, named FedADT. FedADT applies knowledge\ndistillation before aggregating gradients, using the latest global model to\ncorrect outdated information, thus effectively reducing the negative impact of\noutdated gradients on the training process. Additionally, FedADT introduces an\nadaptive weighting function that adjusts the knowledge distillation weight\naccording to different stages of training, helps mitigate the misleading\neffects caused by the poorer performance of the global model in the early\nstages of training. This method significantly improves the overall performance\nof asynchronous federated learning without adding excessive computational\noverhead. We conducted experimental comparisons with several classical\nalgorithms, and the results demonstrate that FedADT achieves significant\nimprovements over other asynchronous methods and outperforms all methods in\nterms of convergence speed."
    },
    {
        "date": "2025-04",
        "title": "Scalable Robust Bayesian Co-Clustering with Compositional ELBOs",
        "author": "Ashwin Vinod, and Chandrajit Bajaj",
        "link": "http://arxiv.org/abs/2504.04079v2",
        "abstract": "Co-clustering exploits the duality of instances and features to\nsimultaneously uncover meaningful groups in both dimensions, often\noutperforming traditional clustering in high-dimensional or sparse data\nsettings. Although recent deep learning approaches successfully integrate\nfeature learning and cluster assignment, they remain susceptible to noise and\ncan suffer from posterior collapse within standard autoencoders. In this paper,\nwe present the first fully variational Co-clustering framework that directly\nlearns row and column clusters in the latent space, leveraging a doubly\nreparameterized ELBO to improve gradient signal-to-noise separation. Our\nunsupervised model integrates a Variational Deep Embedding with a Gaussian\nMixture Model (GMM) prior for both instances and features, providing a built-in\nclustering mechanism that naturally aligns latent modes with row and column\nclusters. Furthermore, our regularized end-to-end noise learning Compositional\nELBO architecture jointly reconstructs the data while regularizing against\nnoise through the KL divergence, thus gracefully handling corrupted or missing\ninputs in a single training pipeline. To counteract posterior collapse, we\nintroduce a scale modification that increases the encoder's latent means only\nin the reconstruction pathway, preserving richer latent representations without\ninflating the KL term. Finally, a mutual information-based cross-loss ensures\ncoherent co-clustering of rows and columns. Empirical results on diverse\nreal-world datasets from multiple modalities, numerical, textual, and\nimage-based, demonstrate that our method not only preserves the advantages of\nprior Co-clustering approaches but also exceeds them in accuracy and\nrobustness, particularly in high-dimensional or noisy settings."
    },
    {
        "date": "2025-04",
        "title": "Deep-Learning-Directed Preventive Dynamic Security Control via Coordinated Demand Response",
        "author": "Amin Masoumi, and Mert Korkali",
        "link": "http://arxiv.org/abs/2504.04059v1",
        "abstract": "Unlike common faults, three-phase short-circuit faults in power systems pose\nsignificant challenges. These faults can lead to out-of-step (OOS) conditions\nand jeopardize the system's dynamic security. The rapid dynamics of these\nfaults often exceed the time of protection actions, thus limiting the\neffectiveness of corrective schemes. This paper proposes an end-to-end\ndeep-learning-based mechanism, namely, a convolutional neural network with an\nattention mechanism, to predict OOS conditions early and enhance the system's\nfault resilience. The results of the study demonstrate the effectiveness of the\nproposed algorithm in terms of early prediction and robustness against such\nfaults in various operating conditions."
    },
    {
        "date": "2025-04",
        "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
        "author": "Ehsanul Kabir, Lucas Craig, and Shagufta Mehnaz",
        "link": "http://arxiv.org/abs/2504.04033v1",
        "abstract": "As machine learning (ML) technologies become more prevalent in\nprivacy-sensitive areas like healthcare and finance, eventually incorporating\nsensitive information in building data-driven algorithms, it is vital to\nscrutinize whether these data face any privacy leakage risks. One potential\nthreat arises from an adversary querying trained models using the public,\nnon-sensitive attributes of entities in the training data to infer their\nprivate, sensitive attributes, a technique known as the attribute inference\nattack. This attack is particularly deceptive because, while it may perform\npoorly in predicting sensitive attributes across the entire dataset, it excels\nat predicting the sensitive attributes of records from a few vulnerable groups,\na phenomenon known as disparate vulnerability. This paper illustrates that an\nadversary can take advantage of this disparity to carry out a series of new\nattacks, showcasing a threat level beyond previous imagination. We first\ndevelop a novel inference attack called the disparity inference attack, which\ntargets the identification of high-risk groups within the dataset. We then\nintroduce two targeted variations of the attribute inference attack that can\nidentify and exploit a vulnerable subset of the training data, marking the\nfirst instances of targeted attacks in this category, achieving significantly\nhigher accuracy than untargeted versions. We are also the first to introduce a\nnovel and effective disparity mitigation technique that simultaneously\npreserves model performance and prevents any risk of targeted attacks."
    },
    {
        "date": "2025-04",
        "title": "Practical Poisoning Attacks against Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Yuxi Chen, Minghong Fang, Zhuqing Liu, Lihai Nie, Tong Li, and Zheli Liu",
        "link": "http://arxiv.org/abs/2504.03957v1",
        "abstract": "Large language models (LLMs) have demonstrated impressive natural language\nprocessing abilities but face challenges such as hallucination and outdated\nknowledge. Retrieval-Augmented Generation (RAG) has emerged as a\nstate-of-the-art approach to mitigate these issues. While RAG enhances LLM\noutputs, it remains vulnerable to poisoning attacks. Recent studies show that\ninjecting poisoned text into the knowledge database can compromise RAG systems,\nbut most existing attacks assume that the attacker can insert a sufficient\nnumber of poisoned texts per query to outnumber correct-answer texts in\nretrieval, an assumption that is often unrealistic. To address this limitation,\nwe propose CorruptRAG, a practical poisoning attack against RAG systems in\nwhich the attacker injects only a single poisoned text, enhancing both\nfeasibility and stealth. Extensive experiments across multiple datasets\ndemonstrate that CorruptRAG achieves higher attack success rates compared to\nexisting baselines."
    },
    {
        "date": "2025-04",
        "title": "Analysis of Robustness of a Large Game Corpus",
        "author": "Mahsa Bazzaz, and Seth Cooper",
        "link": "http://arxiv.org/abs/2504.03940v1",
        "abstract": "Procedural content generation via machine learning (PCGML) in games involves\nusing machine learning techniques to create game content such as maps and\nlevels. 2D tile-based game levels have consistently served as a standard\ndataset for PCGML because they are a simplified version of game levels while\nmaintaining the specific constraints typical of games, such as being solvable.\nIn this work, we highlight the unique characteristics of game levels, including\ntheir structured discrete data nature, the local and global constraints\ninherent in the games, and the sensitivity of the game levels to small changes\nin input. We define the robustness of data as a measure of sensitivity to small\nchanges in input that cause a change in output, and we use this measure to\nanalyze and compare these levels to state-of-the-art machine learning datasets,\nshowcasing the subtle differences in their nature. We also constructed a large\ndataset from four games inspired by popular classic tile-based games that\nshowcase these characteristics and address the challenge of sparse data in\nPCGML by providing a significantly larger dataset than those currently\navailable."
    },
    {
        "date": "2025-04",
        "title": "Commit-Reveal$^2$: Randomized Reveal Order Mitigates Last-Revealer Attacks in Commit-Reveal",
        "author": "Suheyon Lee, and Euisin Gee",
        "link": "http://arxiv.org/abs/2504.03936v1",
        "abstract": "Randomness generation is a fundamental component in blockchain systems,\nessential for tasks such as validator selection, zero-knowledge proofs, and\ndecentralized finance operations. Traditional Commit-Reveal mechanisms provide\nsimplicity and security but are susceptible to last revealer attacks, where an\nadversary can manipulate the random outcome by withholding their reveal. To\naddress this vulnerability, we propose the Commit-Reveal$^2$ protocol, which\nemploys a two-layer Commit-Reveal process to randomize the reveal order and\nmitigate the risk of such attacks. Additionally, we introduces a method to\nleverage off-chain networks to optimize communication costs and enhance\nefficiency. We implement a prototype of the proposed mechanism and publicly\nrelease the code to facilitate practical adoption and further research."
    },
    {
        "date": "2025-04",
        "title": "Secure Federated XGBoost with CUDA-accelerated Homomorphic Encryption via NVIDIA FLARE",
        "author": "Ziyue Xu, Yuan-Ting Hsieh, Zhihong Zhang, Holger R. Roth, Chester Chen, Yan Cheng, and Andrew Feng",
        "link": "http://arxiv.org/abs/2504.03909v1",
        "abstract": "Federated learning (FL) enables collaborative model training across\ndecentralized datasets. NVIDIA FLARE's Federated XGBoost extends the popular\nXGBoost algorithm to both vertical and horizontal federated settings,\nfacilitating joint model development without direct data sharing. However, the\ninitial implementation assumed mutual trust over the sharing of intermediate\ngradient statistics produced by the XGBoost algorithm, leaving potential\nvulnerabilities to honest-but-curious adversaries. This work introduces \"Secure\nFederated XGBoost\", an efficient solution to mitigate these risks. We implement\nsecure federated algorithms for both vertical and horizontal scenarios,\naddressing diverse data security patterns. To secure the messages, we leverage\nhomomorphic encryption (HE) to protect sensitive information during training. A\nnovel plugin and processor interface seamlessly integrates HE into the\nFederated XGBoost pipeline, enabling secure aggregation over ciphertexts. We\npresent both CPU-based and CUDA-accelerated HE plugins, demonstrating\nsignificant performance gains. Notably, our CUDA-accelerated HE implementation\nachieves up to 30x speedups in vertical Federated XGBoost compared to existing\nthird-party solutions. By securing critical computation steps and encrypting\nsensitive assets, Secure Federated XGBoost provides robust data privacy\nguarantees, reinforcing the fundamental benefits of federated learning while\nmaintaining high performance."
    },
    {
        "date": "2025-04",
        "title": "The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning",
        "author": "Virilo Tejedor, Cristina Zuheros, Carlos Pel\u00e1ez-Gonz\u00e1lez, David Herrera-Poyatos, Andr\u00e9s Herrera-Poyatos, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2504.03823v1",
        "abstract": "Large Language Models (LLMs) offer powerful capabilities in text generation\nand are increasingly adopted across a wide range of domains. However, their\nopen accessibility and fine-tuning capabilities pose new security threats. This\nadvance generates new challenges in terms of security and control over the\nsystems that use these models. We hypothesize that LLMs can be designed,\nadapted, and used maliciously, so their extensive and confident use entails\nrisks that should be taken into account. In this paper, we introduce H-Elena, a\nTrojan-infected version of a Falcon-7B derived Python coding assistant by\nmalicious fine-tuning. H-Elena embeds a payload for data theft and replicates\nitself through an infection mechanism triggered during training code\ngeneration. H-Elena, derived from \"Hacked-Elena\", alludes to the mythical\nTrojan Horse symbolizing its ability to infiltrate and cause damage stealthily\nfrom within. It has been obtained by fine-tuning the Falcon LLM, altering the\nneural network weights. The malicious behavior in H-Elena is activated under\ncertain conditions and has the capability to replicate and propagate a\nmalicious payload through the interactions of the infected model. We carried\nout experiments and comparative analysis between Elena and H-Elena, its\ntrojanized counterpart. We illustrate the potential of this type of virus and\nthe necessity of developing more robust and secure methods for the training and\ndeployment of LLM. Our experiments show that H-Elena retains strong assistant\nperformance while coveringtly executing and spreading malicious behavior. This\nwork demonstrates how LLMs can become self-propagating threats and highlights\nthe urgent need for robust validation and monitoring practices in LLM\ndevelopment and deployment."
    },
    {
        "date": "2025-04",
        "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds",
        "author": "Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, and Francis Engelmann",
        "link": "http://arxiv.org/abs/2504.03602v1",
        "abstract": "Registering human meshes to 3D point clouds is essential for applications\nsuch as augmented reality and human-robot interaction but often yields\nimprecise results due to noise and background clutter in real-world data. We\nintroduce a hybrid approach that incorporates body-part segmentation into the\nmesh fitting process, enhancing both human pose estimation and segmentation\naccuracy. Our method first assigns body part labels to individual points, which\nthen guide a two-step SMPL-X fitting: initial pose and orientation estimation\nusing body part centroids, followed by global refinement of the point cloud\nalignment. Additionally, we demonstrate that the fitted human mesh can refine\nbody part labels, leading to improved segmentation. Evaluations on the\ncluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that\nour approach significantly outperforms prior methods in both pose estimation\nand segmentation accuracy. Code and results are available on our project\nwebsite: https://segfit.github.io"
    },
    {
        "date": "2025-04",
        "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
        "author": "Alexander Windmann, Henrik Steude, Daniel Boschmann, and Oliver Niggemann",
        "link": "http://arxiv.org/abs/2504.03494v1",
        "abstract": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility."
    },
    {
        "date": "2025-04",
        "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models",
        "author": "Mirko Borszukovszki, Ivo Pascal de Jong, and Matias Valdenegro-Toro",
        "link": "http://arxiv.org/abs/2504.03440v1",
        "abstract": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments."
    },
    {
        "date": "2025-04",
        "title": "SoK: Attacks on Modern Card Payments",
        "author": "Xenia Hofmeier, David Basin, Ralf Sasse, and Jorge Toro-Pozo",
        "link": "http://arxiv.org/abs/2504.03363v1",
        "abstract": "EMV is the global standard for smart card payments and its NFC-based version,\nEMV contactless, is widely used, also for mobile payments. In this\nsystematization of knowledge, we examine attacks on the EMV contactless\nprotocol. We provide a comprehensive framework encompassing its desired\nsecurity properties and adversary models. We also identify and categorize a\ncomprehensive collection of protocol flaws and show how different subsets\nthereof can be combined into attacks. In addition to this systematization, we\nexamine the underlying reasons for the many attacks against EMV and point to a\nbetter way forward."
    },
    {
        "date": "2025-04",
        "title": "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning Against Data Poisoning Attacks on Non-IID Data",
        "author": "Hongliang Zhang, Jiguo Yu, Fenghua Xu, Chunqiang Hu, Yongzhao Zhang, Xiaofen Wang, Zhongyuan Yu, and Xiaosong Zhang",
        "link": "http://arxiv.org/abs/2504.03173v1",
        "abstract": "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions."
    },
    {
        "date": "2025-04",
        "title": "Bayesian Optimization of Robustness Measures Using Randomized GP-UCB-based Algorithms under Input Uncertainty",
        "author": "Yu Inatsu",
        "link": "http://arxiv.org/abs/2504.03172v1",
        "abstract": "Bayesian optimization based on Gaussian process upper confidence bound\n(GP-UCB) has a theoretical guarantee for optimizing black-box functions.\nBlack-box functions often have input uncertainty, but even in this case, GP-UCB\ncan be extended to optimize evaluation measures called robustness measures.\nHowever, GP-UCB-based methods for robustness measures include a trade-off\nparameter $\\beta$, which must be excessively large to achieve theoretical\nvalidity, just like the original GP-UCB. In this study, we propose a new method\ncalled randomized robustness measure GP-UCB (RRGP-UCB), which samples the\ntrade-off parameter $\\beta$ from a probability distribution based on a\nchi-squared distribution and avoids explicitly specifying $\\beta$. The expected\nvalue of $\\beta$ is not excessively large. Furthermore, we show that RRGP-UCB\nprovides tight bounds on the expected value of regret based on the optimal\nsolution and estimated solutions. Finally, we demonstrate the usefulness of the\nproposed method through numerical experiments."
    },
    {
        "date": "2025-04",
        "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
        "author": "Xin Jin, Simon Niklaus, Zhoutong Zhang, Zhihao Xia, Chunle Guo, Yuting Yang, Jiawen Chen, and Chongyi Li",
        "link": "http://arxiv.org/abs/2504.03136v1",
        "abstract": "Denoising is a crucial step in many video processing pipelines such as in\ninteractive editing, where high quality, speed, and user control are essential.\nWhile recent approaches achieve significant improvements in denoising quality\nby leveraging deep learning, they are prone to unexpected failures due to\ndiscrepancies between training data distributions and the wide variety of noise\npatterns found in real-world videos. These methods also tend to be slow and\nlack user control. In contrast, traditional denoising methods perform reliably\non in-the-wild videos and run relatively quickly on modern hardware. However,\nthey require manually tuning parameters for each input video, which is not only\ntedious but also requires skill. We bridge the gap between these two paradigms\nby proposing a differentiable denoising pipeline based on traditional methods.\nA neural network is then trained to predict the optimal denoising parameters\nfor each specific input, resulting in a robust and efficient approach that also\nsupports user control."
    },
    {
        "date": "2025-04",
        "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge",
        "author": "Kahim Wong, Jicheng Zhou, Kemou Li, Yain-Whar Si, Xiaowei Wu, and Jiantao Zhou",
        "link": "http://arxiv.org/abs/2504.03128v1",
        "abstract": "The proliferation of AI-generated content brings significant concerns on the\nforensic and security issues such as source tracing, copyright protection, etc,\nhighlighting the need for effective watermarking technologies. Font-based text\nwatermarking has emerged as an effective solution to embed information, which\ncould ensure copyright, traceability, and compliance of the generated text\ncontent. Existing font watermarking methods usually neglect essential font\nknowledge, which leads to watermarked fonts of low quality and limited\nembedding capacity. These methods are also vulnerable to real-world\ndistortions, low-resolution fonts, and inaccurate character segmentation. In\nthis paper, we introduce FontGuard, a novel font watermarking model that\nharnesses the capabilities of font models and language-guided contrastive\nlearning. Unlike previous methods that focus solely on the pixel-level\nalteration, FontGuard modifies fonts by altering hidden style features,\nresulting in better font quality upon watermark embedding. We also leverage the\nfont manifold to increase the embedding capacity of our proposed method by\ngenerating substantial font variants closely resembling the original font.\nFurthermore, in the decoder, we employ an image-text contrastive learning to\nreconstruct the embedded bits, which can achieve desirable robustness against\nvarious real-world transmission distortions. FontGuard outperforms\nstate-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under\nsynthetic, cross-media, and online social network distortions, respectively,\nwhile improving the visual quality by 52.7% in terms of LPIPS. Moreover,\nFontGuard uniquely allows the generation of watermarked fonts for unseen fonts\nwithout re-training the network. The code and dataset are available at\nhttps://github.com/KAHIMWONG/FontGuard."
    },
    {
        "date": "2025-04",
        "title": "SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections",
        "author": "Prashant Kumar, Dheeraj Vattikonda, Kshitij Madhav Bhat, Kunal Dargan, and Prem Kalra",
        "link": "http://arxiv.org/abs/2504.03089v1",
        "abstract": "The widespread adoption of learning-based methods for the LiDAR makes\nautonomous vehicles vulnerable to adversarial attacks through adversarial\n\\textit{point injections (PiJ)}. It poses serious security challenges for\nnavigation and map generation. Despite its critical nature, no major work\nexists that studies learning-based attacks on LiDAR-based SLAM. Our work\nproposes SLACK, an end-to-end deep generative adversarial model to attack LiDAR\nscans with several point injections without deteriorating LiDAR quality. To\nfacilitate SLACK, we design a novel yet simple autoencoder that augments\ncontrastive learning with segmentation-based attention for precise\nreconstructions. SLACK demonstrates superior performance on the task of\n\\textit{point injections (PiJ)} compared to the best baselines on KITTI and\nCARLA-64 dataset while maintaining accurate scan quality. We qualitatively and\nquantitatively demonstrate PiJ attacks using a fraction of LiDAR points. It\nseverely degrades navigation and map quality without deteriorating the LiDAR\nscan quality."
    },
    {
        "date": "2025-04",
        "title": "Integrating Identity-Based Identification against Adaptive Adversaries in Federated Learning",
        "author": "Jakub Kacper Szelag, Ji-Jian Chin, Lauren Ansell, and Sook-Chin Yip",
        "link": "http://arxiv.org/abs/2504.03077v1",
        "abstract": "Federated Learning (FL) has recently emerged as a promising paradigm for\nprivacy-preserving, distributed machine learning. However, FL systems face\nsignificant security threats, particularly from adaptive adversaries capable of\nmodifying their attack strategies to evade detection. One such threat is the\npresence of Reconnecting Malicious Clients (RMCs), which exploit FLs open\nconnectivity by reconnecting to the system with modified attack strategies. To\naddress this vulnerability, we propose integration of Identity-Based\nIdentification (IBI) as a security measure within FL environments. By\nleveraging IBI, we enable FL systems to authenticate clients based on\ncryptographic identity schemes, effectively preventing previously disconnected\nmalicious clients from re-entering the system. Our approach is implemented\nusing the TNC-IBI (Tan-Ng-Chin) scheme over elliptic curves to ensure\ncomputational efficiency, particularly in resource-constrained environments\nlike Internet of Things (IoT). Experimental results demonstrate that\nintegrating IBI with secure aggregation algorithms, such as Krum and Trimmed\nMean, significantly improves FL robustness by mitigating the impact of RMCs. We\nfurther discuss the broader implications of IBI in FL security, highlighting\nresearch directions for adaptive adversary detection, reputation-based\nmechanisms, and the applicability of identity-based cryptographic frameworks in\ndecentralized FL architectures. Our findings advocate for a holistic approach\nto FL security, emphasizing the necessity of proactive defence strategies\nagainst evolving adaptive adversarial threats."
    },
    {
        "date": "2025-04",
        "title": "Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization",
        "author": "Siqi Wang, Aoming Liu, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2504.02996v1",
        "abstract": "Multi-source Domain Generalization (DG) aims to improve model robustness to\nnew distributions. However, DG methods often overlook the effect of label\nnoise, which can confuse a model during training, reducing performance. Limited\nprior work has analyzed DG method's noise-robustness, typically focused on an\nanalysis of existing methods rather than new solutions. In this paper, we\ninvestigate this underexplored space, where models are evaluated under both\ndistribution shifts and label noise, which we refer to as Noise-Aware\nGeneralization (NAG). A natural solution to address label noise would be to\ncombine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL\nmethods aim to detect distribution shifts in a class's samples, i.e., they\nassume that distribution shifts often correspond to label noise. However, in\nNAG distribution shifts can be due to label noise or domain shifts, breaking\nthe assumptions used by LNL methods. A naive solution is to make a similar\nassumption made by many DG methods, where we presume to have domain labels\nduring training, enabling us to isolate the two types of shifts. However, this\nignores valuable cross-domain information. Specifically, our proposed DL4ND\napproach improves noise detection by taking advantage of the observation that\nnoisy samples that may appear indistinguishable within a single domain often\nshow greater variation when compared across domains. Experiments show that\nDL4ND significantly improves performance across four diverse datasets, offering\na promising direction for tackling NAG."
    },
    {
        "date": "2025-04",
        "title": "Multi-Screaming-Channel Attacks: Frequency Diversity for Enhanced Attacks",
        "author": "Jeremy Guillaume, Maxime Pelcat, Amor Nafkha, and Rub\u00e9n Salvador",
        "link": "http://arxiv.org/abs/2504.02979v1",
        "abstract": "Side-channel attacks consist of retrieving internal data from a victim system\nby analyzing its leakage, which usually requires proximity to the victim in the\nrange of a few millimetres. Screaming channels are EM side channels transmitted\nat a distance of a few meters. They appear on mixed-signal devices integrating\nan RF module on the same silicon die as the digital part. Consequently, the\nside channels are modulated by legitimate RF signal carriers and appear at the\nharmonics of the digital clock frequency. While initial works have only\nconsidered collecting leakage at these harmonics, late work has demonstrated\nthat the leakage is also present at frequencies other than these harmonics.\nThis result significantly increases the number of available frequencies to\nperform a screaming-channel attack, which can be convenient in an environment\nwhere multiple harmonics are polluted. This work studies how this diversity of\nfrequencies carrying leakage can be used to improve attack performance. We\nfirst study how to combine multiple frequencies. Second, we demonstrate that\nfrequency combination can improve attack performance and evaluate this\nimprovement according to the performance of the combined frequencies. Finally,\nwe demonstrate the interest of frequency combination in attacks at 15 and, for\nthe first time to the best of our knowledge, at 30 meters. One last important\nobservation is that this frequency combination divides by 2 the number of\ntraces needed to reach a given attack performance."
    },
    {
        "date": "2025-04",
        "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection",
        "author": "Divya Velayudhan, Abdelfatah Ahmed, Mohamad Alansari, Neha Gour, Abderaouf Behouch, Taimur Hassan, Syed Talal Wasim, Nabil Maalej, Muzammal Naseer, Juergen Gall, Mohammed Bennamoun, Ernesto Damiani, and Naoufel Werghi",
        "link": "http://arxiv.org/abs/2504.02823v1",
        "abstract": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/."
    },
    {
        "date": "2025-04",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
        "author": "Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, and Chengchung Shi",
        "link": "http://arxiv.org/abs/2504.03784v2",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset."
    },
    {
        "date": "2025-04",
        "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning",
        "author": "Ramin Zarei Sabzevar, Hamed Mohammadzadeh, Tahmineh Tavakoli, and Ahad Harati",
        "link": "http://arxiv.org/abs/2504.03782v1",
        "abstract": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp"
    },
    {
        "date": "2025-04",
        "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions",
        "author": "PeiJie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, and Feng Zhang",
        "link": "http://arxiv.org/abs/2504.02623v1",
        "abstract": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
    },
    {
        "date": "2025-04",
        "title": "Variational Online Mirror Descent for Robust Learning in Schr\u00f6dinger Bridge",
        "author": "Dong-Sig Han, Jaein Kim, Hee Bin Yoo, and Byoung-Tak Zhang",
        "link": "http://arxiv.org/abs/2504.02618v2",
        "abstract": "Sch\\\"odinger bridge (SB) has evolved into a universal class of probabilistic\ngenerative models. In practice, however, estimated learning signals are often\nuncertain, and the reliability promised by existing methods is often based on\nspeculative optimal-case scenarios. Recent studies regarding the Sinkhorn\nalgorithm through mirror descent (MD) have gained attention, revealing\ngeometric insights into solution acquisition of the SB problems. In this paper,\nwe propose a variational online MD (OMD) framework for the SB problems, which\nprovides further stability to SB solvers. We formally prove convergence and a\nregret bound for the novel OMD formulation of SB acquisition. As a result, we\npropose a simulation-free SB algorithm called Variational Mirrored\nSchr\\\"odinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of\nthe Gaussian mixture parameterization for Schr\\\"odinger potentials. Based on\nthe Wasserstein gradient flow theory, the algorithm offers tractable learning\ndynamics that precisely approximate each OMD step. In experiments, we validate\nthe performance of the proposed VMSB algorithm across an extensive suite of\nbenchmarks. VMSB consistently outperforms contemporary SB solvers on a range of\nSB problems, demonstrating the robustness predicted by our theory."
    },
    {
        "date": "2025-04",
        "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
        "author": "Liangbo Ning, Wenqi Fan, and Qing Li",
        "link": "http://arxiv.org/abs/2504.02458v1",
        "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN."
    },
    {
        "date": "2025-04",
        "title": "Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection",
        "author": "Aidan Tiruvan",
        "link": "http://arxiv.org/abs/2504.02432v1",
        "abstract": "Robust low-rank approximation under row-wise adversarial corruption can be\nachieved with a single pass, randomized procedure that detects and removes\noutlier rows by thresholding their projected norms. We propose a scalable,\nnon-iterative algorithm that efficiently recovers the underlying low-rank\nstructure in the presence of row-wise adversarial corruption. By first\ncompressing the data with a Johnson Lindenstrauss projection, our approach\npreserves the geometry of clean rows while dramatically reducing\ndimensionality. Robust statistical techniques based on the median and median\nabsolute deviation then enable precise identification and removal of outlier\nrows with abnormally high norms. The subsequent rank-k approximation achieves\nnear-optimal error bounds with a one pass procedure that scales linearly with\nthe number of observations. Empirical results confirm that combining random\nsketches with robust statistics yields efficient, accurate decompositions even\nin the presence of large fractions of corrupted rows."
    },
    {
        "date": "2025-04",
        "title": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning",
        "author": "Zihao Zhang, Xunkai Li, Rong-Hua Li, Bing Zhou, Zhenjun Li, and Guoren Wang",
        "link": "http://arxiv.org/abs/2504.02343v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) and the proliferation of\nText-Attributed Graphs (TAGs) across various domains have positioned\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\nthereby enhancing the representational capacity of Graph Neural Networks\n(GNNs). However, the field faces significant challenges: (1) the absence of a\nunified framework to systematize the diverse optimization perspectives arising\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\nrobust method capable of handling real-world TAGs, which often suffer from\ntexts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\ndomain-adaptive framework that not only organizes existing methodologies but\nalso paves the way for future advancements in the field. Building on this\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\nLLM-based text propagation and text augmentation to mitigate text sparsity,\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\nedge reconfiguration strategies to address edge sparsity. Our extensive\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\nbaselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\nperformance improvement of UltraTAG-S also rises, which underscores the\neffectiveness and robustness of UltraTAG-S."
    },
    {
        "date": "2025-04",
        "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing",
        "author": "Seif Mzoughi, Mohamed Elshafeia, and Foutse Khomh",
        "link": "http://arxiv.org/abs/2504.02335v1",
        "abstract": "Image segmentation is critical for applications such as medical imaging,\naugmented reality, and video surveillance. However, segmentation models often\nlack robustness, making them vulnerable to adversarial perturbations from\nsubtle image distortions. In this work, we propose SegRMT, a metamorphic\ntesting approach that leverages genetic algorithms (GA) to optimize sequences\nof spatial and spectral transformations while preserving image fidelity via a\npredefined PSNR threshold. Using the Cityscapes dataset, our method generates\nadversarial examples that effectively challenge the DeepLabV3 segmentation\nmodel. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection\nover Union (mIoU) to 6.4%, outperforming other adversarial baselines that\ndecrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial\ntraining, SegRMT boosts model performance, achieving mIoU improvements up to\n73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to\n53.8%, compared to only 2%-10% for other methods. These findings demonstrate\nthat SegRMT not only simulates realistic image distortions but also enhances\nthe robustness of segmentation models, making it a valuable tool for ensuring\nreliable performance in safety-critical applications."
    },
    {
        "date": "2025-04",
        "title": "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance",
        "author": "Bo Yuan, Yulin Chen, Yin Zhang, and Wei Jiang",
        "link": "http://arxiv.org/abs/2504.02901v1",
        "abstract": "Learning from noisy labels (LNL) is a challenge that arises in many\nreal-world scenarios where collected training data can contain incorrect or\ncorrupted labels. Most existing solutions identify noisy labels and adopt\nactive learning to query human experts on them for denoising. In the era of\nlarge language models (LLMs), although we can reduce the human effort to\nimprove these methods, their performances are still subject to accurately\nseparating the clean and noisy samples from noisy data. In this paper, we\npropose an innovative collaborative learning framework NoiseAL based on active\nlearning to combine LLMs and small models (SMs) for learning from noisy labels.\nDuring collaborative training, we first adopt two SMs to form a co-prediction\nnetwork and propose a dynamic-enhanced threshold strategy to divide the noisy\ndata into different subsets, then select the clean and noisy samples from these\nsubsets to feed the active annotator LLMs to rectify noisy samples. Finally, we\nemploy different optimization objectives to conquer subsets with different\ndegrees of label noises. Extensive experiments on synthetic and real-world\nnoise datasets further demonstrate the superiority of our framework over\nstate-of-the-art baselines."
    },
    {
        "date": "2025-04",
        "title": "Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism",
        "author": "Shourya Goel, Himanshi Tibrewal, Anant Jain, Anshul Pundhir, and Pravendra Singh",
        "link": "http://arxiv.org/abs/2504.02213v1",
        "abstract": "Federated learning (FL) has gained increasing attention due to\nprivacy-preserving collaborative training on decentralized clients, mitigating\nthe need to upload sensitive data to a central server directly. Nonetheless,\nrecent research has underscored the risk of exposing private data to\nadversaries, even within FL frameworks. In general, existing methods sacrifice\nperformance while ensuring resistance to privacy leakage in FL. We overcome\nthese issues and generate diverse models at a global server through the\nproposed stochastic bidirectional parameter update mechanism. Using diverse\nmodels, we improved the generalization and feature representation in the FL\nsetup, which also helped to improve the robustness of the model against privacy\nleakage without hurting the model's utility. We use global models from past FL\nrounds to follow systematic perturbation in parameter space at the server to\nensure model generalization and resistance against privacy attacks. We generate\ndiverse models (in close neighborhoods) for each client by using systematic\nperturbations in model parameters at a fine-grained level (i.e., altering each\nconvolutional filter across the layers of the model) to improve the\ngeneralization and security perspective. We evaluated our proposed approach on\nfour benchmark datasets to validate its superiority. We surpassed the\nstate-of-the-art methods in terms of model utility and robustness towards\nprivacy leakage. We have proven the effectiveness of our method by evaluating\nperformance using several quantitative and qualitative results."
    },
    {
        "date": "2025-04",
        "title": "FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets",
        "author": "Rushi Jayeshkumar Babaria, Minzhao Lyu, Gustavo Batista, and Vijay Sivaraman",
        "link": "http://arxiv.org/abs/2504.02174v1",
        "abstract": "Network traffic classification is of great importance for network operators\nin their daily routines, such as analyzing the usage patterns of multimedia\napplications and optimizing network configurations. Internet service providers\n(ISPs) that operate high-speed links expect network flow classifiers to\naccurately classify flows early, using the minimal number of necessary initial\npackets per flow. These classifiers must also be robust to packet sequence\ndisorders in candidate flows and capable of detecting unseen flow types that\nare not within the existing classification scope, which are not well achieved\nby existing methods. In this paper, we develop FastFlow, a time-series flow\nclassification method that accurately classifies network flows as one of the\nknown types or the unknown type, which dynamically selects the minimal number\nof packets to balance accuracy and efficiency. Toward the objectives, we first\ndevelop a flow representation process that converts packet streams at both\nper-packet and per-slot granularity for precise packet statistics with\nrobustness to packet sequence disorders. Second, we develop a sequential\ndecision-based classification model that leverages LSTM architecture trained\nwith reinforcement learning. Our model makes dynamic decisions on the minimal\nnumber of time-series data points per flow for the confident classification as\none of the known flow types or an unknown one. We evaluated our method on\npublic datasets and demonstrated its superior performance in early and accurate\nflow classification. Deployment insights on the classification of over 22.9\nmillion flows across seven application types and 33 content providers in a\ncampus network over one week are discussed, showing that FastFlow requires an\naverage of only 8.37 packets and 0.5 seconds to classify the application type\nof a flow with over 91% accuracy and over 96% accuracy for the content\nproviders."
    },
    {
        "date": "2025-04",
        "title": "Exploring the Privacy and Security Challenges Faced by Migrant Domestic Workers in Chinese Smart Homes",
        "author": "Shijing He, Xiao Zhan, Yaxiong Lei, Yueyan Liu, Ruba Abu-Salma, and Jose Such",
        "link": "http://arxiv.org/abs/2504.02149v1",
        "abstract": "The growing use of smart home devices poses considerable privacy and security\nchallenges, especially for individuals like migrant domestic workers (MDWs) who\nmay be surveilled by their employers. This paper explores the privacy and\nsecurity challenges experienced by MDWs in multi-user smart homes through\nin-depth semi-structured interviews with 26 MDWs and 5 staff members of\nagencies that recruit and/or train domestic workers in China. Our findings\nreveal that the relationships between MDWs, their employers, and agencies are\ncharacterized by significant power imbalances, influenced by Chinese cultural\nand social factors (such as Confucianism and collectivism), as well as legal\nones. Furthermore, the widespread and normalized use of surveillance\ntechnologies in China, particularly in public spaces, exacerbates these power\nimbalances, reinforcing a sense of constant monitoring and control. Drawing on\nour findings, we provide recommendations to domestic worker agencies and\npolicymakers to address the privacy and security challenges facing MDWs in\nChinese smart homes."
    },
    {
        "date": "2025-04",
        "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits",
        "author": "Brandon Radosevich, and John Halloran",
        "link": "http://arxiv.org/abs/2504.03767v1",
        "abstract": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/leidosinc/McpSafetyScanner"
    },
    {
        "date": "2025-04",
        "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds",
        "author": "Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, Sicheng Zhu, Furong Huang, and Tudor Dumitras",
        "link": "http://arxiv.org/abs/2504.02142v1",
        "abstract": "Group robustness has become a major concern in machine learning (ML) as\nconventional training paradigms were found to produce high error on minority\ngroups. Without explicit group annotations, proposed solutions rely on\nheuristics that aim to identify and then amplify the minority samples during\ntraining. In our work, we first uncover a critical shortcoming of these\nmethods: an inability to distinguish legitimate minority samples from poison\nsamples in the training set. By amplifying poison samples as well, group\nrobustness methods inadvertently boost the success rate of an adversary --\ne.g., from $0\\%$ without amplification to over $97\\%$ with it. Notably, we\nsupplement our empirical evidence with an impossibility result proving this\ninability of a standard heuristic under some assumptions. Moreover,\nscrutinizing recent poisoning defenses both in centralized and federated\nlearning, we observe that they rely on similar heuristics to identify which\nsamples should be eliminated as poisons. In consequence, minority samples are\neliminated along with poisons, which damages group robustness -- e.g., from\n$55\\%$ without the removal of the minority samples to $41\\%$ with it. Finally,\nas they pursue opposing goals using similar heuristics, our attempt to\nalleviate the trade-off by combining group robustness methods and poisoning\ndefenses falls short. By exposing this tension, we also hope to highlight how\nbenchmark-driven ML scholarship can obscure the trade-offs among different\nmetrics with potentially detrimental consequences."
    },
    {
        "date": "2025-04",
        "title": "Robust Channel Estimation for Optical Wireless Communications Using Neural Network",
        "author": "Dianxin Luan, and John Thompson",
        "link": "http://arxiv.org/abs/2504.02134v1",
        "abstract": "Optical Wireless Communication (OWC) has gained significant attention due to\nits high-speed data transmission and throughput. Optical wireless channels are\noften assumed to be flat, but we evaluate frequency selective channels to\nconsider high data rate optical wireless or very dispersive environments. To\naddress this for optical scenarios, this paper presents a robust channel\nestimation framework with low-complexity to mitigate frequency-selective\neffects, then to improve system reliability and performance. This channel\nestimation framework contains a neural network that can estimate general\noptical wireless channels without prior channel information about the\nenvironment. Based on this estimate and the corresponding delay spread, one of\nseveral candidate offline-trained neural networks will be activated to predict\nthis channel. Simulation results demonstrate that the proposed method has\nimproved and robust normalized mean square error (NMSE) and bit error rate\n(BER) performance compared to conventional estimation methods while maintaining\ncomputational efficiency. These findings highlight the potential of neural\nnetwork solutions in enhancing the performance of OWC systems under indoor\nchannel conditions."
    },
    {
        "date": "2025-04",
        "title": "Base Station Certificate and Multi-Factor Authentication for Cellular Radio Control Communication Security",
        "author": "Sourav Purification, Simeon Wuthier, Jinoh Kim, Ikkyun Kim, and Sang-Yoon Chang",
        "link": "http://arxiv.org/abs/2504.02133v1",
        "abstract": "Current cellular networking remains vulnerable to malicious fake base\nstations due to the lack of base station authentication mechanism or even a key\nto enable authentication. We design and build a base station certificate\n(certifying the base station's public key and location) and a multi-factor\nauthentication (making use of the certificate and the information transmitted\nin the online radio control communications) to secure the authenticity and\nmessage integrity of the base station control communications. We advance beyond\nthe state-of-the-art research by introducing greater authentication factors\n(and analyzing their individual security properties and benefits), and by using\nblockchain to deliver the base station digital certificate offline (enabling\ngreater key length or security strength and computational or networking\nefficiency). We design the certificate construction, delivery, and the\nmulti-factor authentication use on the user equipment. The user verification\ninvolves multiple factors verified through the ledger database, the location\nsensing (GPS in our implementation), and the cryptographic signature\nverification of the cellular control communication (SIB1 broadcasting). We\nanalyze our scheme's security, performance, and the fit to the existing\nstandardized networking protocols. Our work involves the implementation of\nbuilding on X.509 certificate (adapted), smart contract-based blockchain,\n5G-standardized RRC control communications, and software-defined radios. Our\nanalyses show that our scheme effectively defends against more security threats\nand can enable stronger security, i.e., ECDSA with greater key lengths.\nFurthermore, our scheme enables computing and energy to be more than three\ntimes efficient than the previous research on the mobile user equipment."
    },
    {
        "date": "2025-04",
        "title": "On Model Protection in Federated Learning against Eavesdropping Attacks",
        "author": "Dipankar Maity, and Kushal Chakrabarti",
        "link": "http://arxiv.org/abs/2504.02114v1",
        "abstract": "In this study, we investigate the protection offered by federated learning\nalgorithms against eavesdropping adversaries. In our model, the adversary is\ncapable of intercepting model updates transmitted from clients to the server,\nenabling it to create its own estimate of the model. Unlike previous research,\nwhich predominantly focuses on safeguarding client data, our work shifts\nattention protecting the client model itself. Through a theoretical analysis,\nwe examine how various factors, such as the probability of client selection,\nthe structure of local objective functions, global aggregation at the server,\nand the eavesdropper's capabilities, impact the overall level of protection. We\nfurther validate our findings through numerical experiments, assessing the\nprotection by evaluating the model accuracy achieved by the adversary. Finally,\nwe compare our results with methods based on differential privacy, underscoring\ntheir limitations in this specific context."
    },
    {
        "date": "2025-04",
        "title": "A Systematic Review of Security Communication Strategies: Guidelines and Open Challenges",
        "author": "Carolina Carreira, Alexandra Mendes, Jo\u00e3o F. Ferreira, and Nicolas Christin",
        "link": "http://arxiv.org/abs/2504.02109v1",
        "abstract": "Cybersecurity incidents such as data breaches have become increasingly\ncommon, affecting millions of users and organizations worldwide. The complexity\nof cybersecurity threats challenges the effectiveness of existing security\ncommunication strategies. Through a systematic review of over 3,400 papers, we\nidentify specific user difficulties including information overload, technical\njargon comprehension, and balancing security awareness with comfort. Our\nfindings reveal consistent communication paradoxes: users require technical\ndetails for credibility yet struggle with jargon and need risk awareness\nwithout experiencing anxiety. We propose seven evidence-based guidelines to\nimprove security communication and identify critical research gaps including\nlimited studies with older adults, children, and non-US populations,\ninsufficient longitudinal research, and limited protocol sharing for\nreproducibility. Our guidelines emphasize user-centric communication adapted to\ncultural and demographic differences while ensuring security advice remains\nactionable. This work contributes to more effective security communication\npractices that enable users to recognize and respond to cybersecurity threats\nappropriately."
    },
    {
        "date": "2025-04",
        "title": "Chunking Attacks on File Backup Services using Content-Defined Chunking",
        "author": "Boris Alexeev, Colin Percival, and Yan X Zhang",
        "link": "http://arxiv.org/abs/2504.02095v1",
        "abstract": "Systems such as file backup services often use content-defined chunking (CDC)\nalgorithms, especially those based on rolling hash techniques, to split files\ninto chunks in a way that allows for data deduplication. These chunking\nalgorithms often depend on per-user parameters in an attempt to avoid leaking\ninformation about the data being stored. We present attacks to extract these\nchunking parameters and discuss protocol-agnostic attacks and loss of security\nonce the parameters are breached (including when these parameters are not setup\nat all, which is often available as an option). Our parameter-extraction\nattacks themselves are protocol-specific but their ideas are generalizable to\nmany potential CDC schemes."
    },
    {
        "date": "2025-04",
        "title": "Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses",
        "author": "Zhengchun Shang, and Wenlan Wei",
        "link": "http://arxiv.org/abs/2504.02080v1",
        "abstract": "Large Language Models (LLMs) are increasingly popular, powering a wide range\nof applications. Their widespread use has sparked concerns, especially through\njailbreak attacks that bypass safety measures to produce harmful content.\n  In this paper, we present a comprehensive security analysis of large language\nmodels (LLMs), addressing critical research questions on the evolution and\ndeterminants of model safety.\n  Specifically, we begin by identifying the most effective techniques for\ndetecting jailbreak attacks. Next, we investigate whether newer versions of\nLLMs offer improved security compared to their predecessors. We also assess the\nimpact of model size on overall security and explore the potential benefits of\nintegrating multiple defense strategies to enhance model robustness.\n  Our study evaluates both open-source models (e.g., LLaMA and Mistral) and\nclosed-source systems (e.g., GPT-4) by employing four state-of-the-art attack\ntechniques and assessing the efficacy of three new defensive approaches."
    },
    {
        "date": "2025-04",
        "title": "CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection",
        "author": "Diego Cajaraville-Aboy, Marta Moure-Garrido, Carlos Beis-Penedo, Carlos Garcia-Rubio, Rebeca P. D\u00edaz-Redondo, Celeste Campo, Ana Fern\u00e1ndez-Vilas, and Manuel Fern\u00e1ndez-Veiga",
        "link": "http://arxiv.org/abs/2504.01882v1",
        "abstract": "The use of DNS over HTTPS (DoH) tunneling by an attacker to hide malicious\nactivity within encrypted DNS traffic poses a serious threat to network\nsecurity, as it allows malicious actors to bypass traditional monitoring and\nintrusion detection systems while evading detection by conventional traffic\nanalysis techniques. Machine Learning (ML) techniques can be used to detect DoH\ntunnels; however, their effectiveness relies on large datasets containing both\nbenign and malicious traffic. Sharing such datasets across entities is\nchallenging due to privacy concerns. In this work, we propose CO-DEFEND\n(Continuous Decentralized Federated Learning for Secure DoH-Based Threat\nDetection), a Decentralized Federated Learning (DFL) framework that enables\nmultiple entities to collaboratively train a classification machine learning\nmodel while preserving data privacy and enhancing resilience against single\npoints of failure. The proposed DFL framework, which is scalable and\nprivacy-preserving, is based on a federation process that allows multiple\nentities to train online their local models using incoming DoH flows in real\ntime as they are processed by the entity. In addition, we adapt four classical\nmachine learning algorithms, Support Vector Machines (SVM), Logistic Regression\n(LR), Decision Trees (DT), and Random Forest (RF), for federated scenarios,\ncomparing their results with more computationally complex alternatives such as\nneural networks. We compare our proposed method by using the dataset\nCIRA-CIC-DoHBrw-2020 with existing machine learning approaches to demonstrate\nits effectiveness in detecting malicious DoH tunnels and the benefits it\nbrings."
    },
    {
        "date": "2025-04",
        "title": "An Approach to Technical AGI Safety and Security",
        "author": "Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, S\u00e9bastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, and Anca Dragan",
        "link": "http://arxiv.org/abs/2504.01849v1",
        "abstract": "Artificial General Intelligence (AGI) promises transformative benefits but\nalso presents significant risks. We develop an approach to address the risk of\nharms consequential enough to significantly harm humanity. We identify four\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\nwe focus on technical approaches to misuse and misalignment. For misuse, our\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\nby proactively identifying dangerous capabilities, and implementing robust\nsecurity, access restrictions, monitoring, and model safety mitigations. To\naddress misalignment, we outline two lines of defense. First, model-level\nmitigations such as amplified oversight and robust training can help to build\nan aligned model. Second, system-level security measures such as monitoring and\naccess control can mitigate harm even if the model is misaligned. Techniques\nfrom interpretability, uncertainty estimation, and safer design patterns can\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\nthese ingredients could be combined to produce safety cases for AGI systems."
    },
    {
        "date": "2025-04",
        "title": "Autonomous optical navigation for DESTINY+: Enhancing misalignment robustness in flyby observations with a rotating telescope",
        "author": "Takayuki Hosonuma, Takeshi Miyabara, Naoya Ozaki, Ko Ishibashi, Yuta Suzaki, Peng Hong, Masayuki Ohta, and Takeshi Takashima",
        "link": "http://arxiv.org/abs/2504.01835v1",
        "abstract": "DESTINY+ is an upcoming JAXA Epsilon medium-class mission to flyby multiple\nasteroids including Phaethon. As an asteroid flyby observation instrument, a\ntelescope mechanically capable of single-axis rotation, named TCAP, is mounted\non the spacecraft to track and observe the target asteroids during flyby. As in\npast flyby missions utilizing rotating telescopes, TCAP is also used as a\nnavigation camera for autonomous optical navigation during the closest-approach\nphase. To mitigate the degradation of the navigation accuracy, past missions\nperformed calibration of the navigation camera's alignment before starting\noptical navigation. However, such calibration requires significant operational\ntime to complete and imposes constraints on the operation sequence. From the\nabove background, the DESTINY+ team has studied the possibility of reducing\noperational costs by allowing TCAP alignment errors to remain. This paper\ndescribes an autonomous optical navigation algorithm robust to the misalignment\nof rotating telescopes, proposed in this context. In the proposed method, the\nmisalignment of the telescope is estimated simultaneously with the spacecraft's\norbit relative to the flyby target. To deal with the nonlinearity between the\nmisalignment and the observation value, the proposed method utilizes the\nunscented Kalman filter, instead of the extended Kalman filter widely used in\npast studies. The proposed method was evaluated with numerical simulations on a\nPC and with hardware-in-the-loop simulation, taking the Phaethon flyby in the\nDESTINY+ mission as an example. The validation results suggest that the\nproposed method can mitigate the misalignment-induced degradation of the\noptical navigation accuracy with reasonable computational costs suited for\nonboard computers."
    },
    {
        "date": "2025-04",
        "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
        "author": "Huayang Huang, Xiangye Jin, Jiaxu Miao, and Yu Wu",
        "link": "http://arxiv.org/abs/2504.01819v1",
        "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks."
    },
    {
        "date": "2025-04",
        "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
        "author": "Chaohu Liu, Tianyi Gui, Yu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2504.01735v1",
        "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."
    },
    {
        "date": "2025-04",
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
        "author": "Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu",
        "link": "http://arxiv.org/abs/2504.01724v2",
        "abstract": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/."
    },
    {
        "date": "2025-04",
        "title": "TransforMerger: Transformer-based Voice-Gesture Fusion for Robust Human-Robot Communication",
        "author": "Petr Vanc, and Karla Stepanova",
        "link": "http://arxiv.org/abs/2504.01708v1",
        "abstract": "As human-robot collaboration advances, natural and flexible communication\nmethods are essential for effective robot control. Traditional methods relying\non a single modality or rigid rules struggle with noisy or misaligned data as\nwell as with object descriptions that do not perfectly fit the predefined\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\ntransformer-based reasoning model that infers a structured action command for\nrobotic manipulation based on fused voice and gesture inputs. Our approach\nmerges multimodal data into a single unified sentence, which is then processed\nby the language model. We employ probabilistic embeddings to handle uncertainty\nand we integrate contextual scene understanding to resolve ambiguous references\n(e.g., gestures pointing to multiple objects or vague verbal cues like \"this\").\nWe evaluate TransforMerger in simulated and real-world experiments,\ndemonstrating its robustness to noise, misalignment, and missing information.\nOur results show that TransforMerger outperforms deterministic baselines,\nespecially in scenarios requiring more contextual knowledge, enabling more\nrobust and flexible human-robot communication. Code and datasets are available\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger."
    },
    {
        "date": "2025-04",
        "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
        "author": "Junjie Chen, Yuecong Xu, Haosheng Li, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01668v1",
        "abstract": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack."
    },
    {
        "date": "2025-04",
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "author": "Haosheng Li, Junjie Chen, Yuecong Xu, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01659v2",
        "abstract": "Unsupervised domain adaptation (UDA) frameworks have shown good\ngeneralization capabilities for 3D point cloud semantic segmentation models on\nclean data. However, existing works overlook adversarial robustness when the\nsource domain itself is compromised. To comprehensively explore the robustness\nof the UDA frameworks, we first design a stealthy adversarial point cloud\ngeneration attack that can significantly contaminate datasets with only minor\nperturbations to the point cloud surface. Based on that, we propose a novel\ndataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds.\nWith the generated corrupted data, we further develop the Adversarial\nAdaptation Framework (AAF) as the countermeasure. Specifically, by extending\nthe key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss)\nand utilizing a decoder branch, our approach enables the model to focus on\nlong-tail classes during the pre-training phase and leverages high-confidence\ndecoded point cloud information to restore point cloud structures during the\nadaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where\nthe results demonstrate that our AAF method can mitigate performance\ndegradation under source adversarial perturbations for UDA in the 3D point\ncloud segmentation application."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
        "author": "Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, and Giorgio Buttazzo",
        "link": "http://arxiv.org/abs/2504.01632v1",
        "abstract": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks."
    },
    {
        "date": "2025-04",
        "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution",
        "author": "Zhuoran Yang, Jie Peng, Zhen Tan, Tianlong Chen, and Yanyong Zhang",
        "link": "http://arxiv.org/abs/2504.01533v1",
        "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs."
    },
    {
        "date": "2025-04",
        "title": "A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown L\u00e9vy Process Dynamics",
        "author": "Qihao Ye, Xiaochuan Tian, and Yuhua Zhu",
        "link": "http://arxiv.org/abs/2504.01482v1",
        "abstract": "This paper develops a model-based framework for continuous-time policy\nevaluation (CTPE) in reinforcement learning, incorporating both Brownian and\nL\\'evy noise to model stochastic dynamics influenced by rare and extreme\nevents. Our approach formulates the policy evaluation problem as solving a\npartial integro-differential equation (PIDE) for the value function with\nunknown coefficients. A key challenge in this setting is accurately recovering\nthe unknown coefficients in the stochastic dynamics, particularly when driven\nby L\\'evy processes with heavy tail effects. To address this, we propose a\nrobust numerical approach that effectively handles both unbiased and censored\ntrajectory datasets. This method combines maximum likelihood estimation with an\niterative tail correction mechanism, improving the stability and accuracy of\ncoefficient recovery. Additionally, we establish a theoretical bound for the\npolicy evaluation error based on coefficient recovery error. Through numerical\nexperiments, we demonstrate the effectiveness and robustness of our method in\nrecovering heavy-tailed L\\'evy dynamics and verify the theoretical error\nanalysis in policy evaluation."
    },
    {
        "date": "2025-04",
        "title": "Emerging Cyber Attack Risks of Medical AI Agents",
        "author": "Jianing Qiu, Lin Li, Jiankai Sun, Hao Wei, Zhe Xu, Kyle Lam, and Wu Yuan",
        "link": "http://arxiv.org/abs/2504.03759v1",
        "abstract": "Large language models (LLMs)-powered AI agents exhibit a high level of\nautonomy in addressing medical and healthcare challenges. With the ability to\naccess various tools, they can operate within an open-ended action space.\nHowever, with the increase in autonomy and ability, unforeseen risks also\narise. In this work, we investigated one particular risk, i.e., cyber attack\nvulnerability of medical AI agents, as agents have access to the Internet\nthrough web browsing tools. We revealed that through adversarial prompts\nembedded on webpages, cyberattackers can: i) inject false information into the\nagent's response; ii) they can force the agent to manipulate recommendation\n(e.g., healthcare products and services); iii) the attacker can also steal\nhistorical conversations between the user and agent, resulting in the leak of\nsensitive/private medical information; iv) furthermore, the targeted agent can\nalso cause a computer system hijack by returning a malicious URL in its\nresponse. Different backbone LLMs were examined, and we found such cyber\nattacks can succeed in agents powered by most mainstream LLMs, with the\nreasoning models such as DeepSeek-R1 being the most vulnerable."
    },
    {
        "date": "2025-04",
        "title": "Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense",
        "author": "Haibo Zhang, Zhihua Yao, Kouichi Sakurai, and Takeshi Saitoh",
        "link": "http://arxiv.org/abs/2504.01399v1",
        "abstract": "In the rapidly evolving field of artificial intelligence, machine learning\nemerges as a key technology characterized by its vast potential and inherent\nrisks. The stability and reliability of these models are important, as they are\nfrequent targets of security threats. Adversarial attacks, first rigorously\ndefined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability:\nthey can trick machine learning models into making incorrect predictions by\napplying nearly invisible perturbations to images. Although many studies have\nfocused on constructing sophisticated defensive mechanisms to mitigate such\nattacks, they often overlook the substantial time and computational costs of\ntraining and maintaining these models. Ideally, a defense method should be able\nto generalize across various, even unseen, adversarial attacks with minimal\noverhead. Building on our previous work on image-to-image translation-based\ndefenses, this study introduces an improved model that incorporates residual\nblocks to enhance generalizability. The proposed method requires training only\na single model, effectively defends against diverse attack types, and is\nwell-transferable between different target models. Experiments show that our\nmodel can restore the classification accuracy from near zero to an average of\n72\\% while maintaining competitive performance compared to state-of-the-art\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores",
        "author": "Zhe Jiang, Sam Ainsworth, and Timothy Jones",
        "link": "http://arxiv.org/abs/2504.01380v1",
        "abstract": "High-performance security guarantees rely on hardware support. Generic\nprogrammable support for fine-grained instruction analysis has gained broad\ninterest in the literature as a fundamental building block for the security of\nfuture processors. Yet, implementation in real out-of-order (OoO) superscalar\nprocessors presents tough challenges that cannot be explored in highly abstract\nsimulators. We detail the challenges of implementing complex programmable\npathways without critical paths or contention. We then introduce FireGuard, the\nfirst implementation of fine-grained instruction analysis on a real OoO\nsuperscalar processor. We establish an end-to-end system, including\nmicroarchitecture, SoC, ISA and programming model. Experiments show that our\nsolution simultaneously ensures both security and performance of the system,\nwith parallel scalability. We examine the feasibility of building FireGuard\ninto modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F,\nwhere less than 1% silicon area is introduced. The Repo. of FireGuard's source\ncode: https://github.com/SEU-ACAL/reproduce-FireGuard-DAC-25."
    },
    {
        "date": "2025-04",
        "title": "Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification",
        "author": "Akil Raj Subedi, Taniya Shah, Aswani Kumar Cherukuri, and Thanos Vasilakos",
        "link": "http://arxiv.org/abs/2504.01345v1",
        "abstract": "Social media platforms like Twitter have increasingly relied on Natural\nLanguage Processing NLP techniques to analyze and understand the sentiments\nexpressed in the user generated content. One such state of the art NLP model is\nBidirectional Encoder Representations from Transformers BERT which has been\nwidely adapted in sentiment analysis. BERT is susceptible to adversarial\nattacks. This paper aims to scrutinize the inherent vulnerabilities of such\nmodels in Twitter sentiment analysis. It aims to formulate a framework for\nconstructing targeted adversarial texts capable of deceiving these models,\nwhile maintaining stealth. In contrast to conventional methodologies, such as\nImportance Reweighting, this framework core idea resides in its reliance on\ngradients to prioritize the importance of individual words within the text. It\nuses a whitebox approach to attain fine grained sensitivity, pinpointing words\nthat exert maximal influence on the classification outcome. This paper is\norganized into three interdependent phases. It starts with fine-tuning a\npre-trained BERT model on Twitter data. It then analyzes gradients of the model\nto rank words on their importance, and iteratively replaces those with feasible\ncandidates until an acceptable solution is found. Finally, it evaluates the\neffectiveness of the adversarial text against the custom trained sentiment\nclassification model. This assessment would help in gauging the capacity of the\nadversarial text to successfully subvert classification without raising any\nalarm."
    },
    {
        "date": "2025-04",
        "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
        "author": "Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, and Kin-Man Lam",
        "link": "http://arxiv.org/abs/2504.01308v2",
        "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."
    },
    {
        "date": "2025-04",
        "title": "TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification",
        "author": "Kimia haghjooei, and Mansoor Rezghi",
        "link": "http://arxiv.org/abs/2504.01228v1",
        "abstract": "Deep learning models have achieved remarkable success in computer vision but\nremain vulnerable to adversarial attacks, particularly in black-box settings\nwhere model details are unknown. Existing adversarial attack methods(even those\nworks with key frames) often treat video data as simple vectors, ignoring their\ninherent multi-dimensional structure, and require a large number of queries,\nmaking them inefficient and detectable. In this paper, we propose\n\\textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages\nthe multi-dimensional properties of video data by representing videos as\nfourth-order tensors. By exploiting low-rank attack, our method significantly\nreduces the search space and the number of queries needed to generate\nadversarial examples in black-box settings. Experimental results on standard\nvideo classification datasets demonstrate that \\textbf{TenAd} effectively\ngenerates imperceptible adversarial perturbations while achieving higher attack\nsuccess rates and query efficiency compared to state-of-the-art methods. Our\napproach outperforms existing black-box adversarial attacks in terms of success\nrate, query efficiency, and perturbation imperceptibility, highlighting the\npotential of tensor-based methods for adversarial attacks on video models."
    },
    {
        "date": "2025-04",
        "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01220v1",
        "abstract": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal."
    },
    {
        "date": "2025-04",
        "title": "GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01213v1",
        "abstract": "Although contactless fingerprints offer user comfort, they are more\nvulnerable to spoofing. The current solution for anti-spoofing in the area of\ncontactless fingerprints relies on domain adaptation learning, limiting their\ngeneralization and scalability. To address these limitations, we introduce\nGRU-AUNet, a domain adaptation approach that integrates a Swin\nTransformer-based UNet architecture with GRU-enhanced attention mechanisms, a\nDynamic Filter Network in the bottleneck, and a combined Focal and Contrastive\nLoss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet\ndemonstrates robust resilience against presentation attacks, achieving an\naverage BPCER of 0.09\\% and APCER of 1.2\\% in the CLARKSON, COLFISPOOF, and\nIIITD datasets, outperforming state-of-the-art domain adaptation methods."
    },
    {
        "date": "2025-04",
        "title": "Performative Drift Resistant Classification Using Generative Domain Adversarial Networks",
        "author": "Maciej Makowski, Brandon Gower-Winter, and Georg Krempl",
        "link": "http://arxiv.org/abs/2504.01135v1",
        "abstract": "Performative Drift is a special type of Concept Drift that occurs when a\nmodel's predictions influence the future instances the model will encounter. In\nthese settings, retraining is not always feasible. In this work, we instead\nfocus on drift understanding as a method for creating drift-resistant\nclassifiers. To achieve this, we introduce the Generative Domain Adversarial\nNetwork (GDAN) which combines both Domain and Generative Adversarial Networks.\nUsing GDAN, domain-invariant representations of incoming data are created and a\ngenerative network is used to reverse the effects of performative drift. Using\nsemi-real and synthetic data generators, we empirically evaluate GDAN's ability\nto provide drift-resistant classification. Initial results are promising with\nGDAN limiting performance degradation over several timesteps. Additionally,\nGDAN's generative network can be used in tandem with other models to limit\ntheir performance degradation in the presence of performative drift. Lastly, we\nhighlight the relationship between model retraining and the unpredictability of\nperformative drift, providing deeper insights into the challenges faced when\nusing traditional Concept Drift mitigation strategies in the performative\nsetting."
    },
    {
        "date": "2025-04",
        "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation",
        "author": "Wenjun Zeng, Dana Kurniawan, Ryan Mullins, Yuchi Liu, Tamoghna Saha, Dirichi Ike-Njoku, Jindong Gu, Yiwen Song, Cai Xu, Jingjing Zhou, Aparna Joshi, Shravan Dheep, Mani Malek, Hamid Palangi, Joon Baek, Rick Pereira, and Karthik Narasimhan",
        "link": "http://arxiv.org/abs/2504.01081v2",
        "abstract": "We introduce ShieldGemma 2, a 4B parameter image content moderation model\nbuilt on Gemma 3. This model provides robust safety risk predictions across the\nfollowing key harm categories: Sexually Explicit, Violence \\& Gore, and\nDangerous Content for synthetic images (e.g. output of any image generation\nmodel) and natural images (e.g. any image input to a Vision-Language Model). We\nevaluated on both internal and external benchmarks to demonstrate\nstate-of-the-art performance compared to LlavaGuard\n\\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base\nGemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we\npresent a novel adversarial data generation pipeline which enables a\ncontrolled, diverse, and robust image generation. ShieldGemma 2 provides an\nopen image moderation tool to advance multimodal safety and responsible AI\ndevelopment."
    },
    {
        "date": "2025-04",
        "title": "Safety and Security Risk Mitigation in Satellite Missions via Attack-Fault-Defense Trees",
        "author": "Reza Soltani, Pablo Diale, Milan Lopuha\u00e4-Zwakenberg, and Mari\u00eblle Stoelinga",
        "link": "http://arxiv.org/abs/2504.00988v1",
        "abstract": "Cyber-physical systems, such as self-driving cars or digitized electrical\ngrids, often involve complex interactions between security, safety, and\ndefense. Proper risk management strategies must account for these three\ncritical domains and their interaction because the failure to address one\ndomain can exacerbate risks in the others, leading to cascading effects that\ncompromise the overall system resilience. This work presents a case study from\nAscentio Technologies, a mission-critical system company in Argentina\nspecializing in aerospace, where the interplay between safety, security, and\ndefenses is critical for ensuring the resilience and reliability of their\nsystems. The main focus will be on the Ground Segment for the satellite project\ncurrently developed by the company. Analyzing safety, security, and defense\nmechanisms together in the Ground Segment of a satellite project is crucial\nbecause these domains are deeply interconnected--for instance, a security\nbreach could disable critical safety functions, or a safety failure could\ncreate opportunities for attackers to exploit vulnerabilities, amplifying the\nrisks to the entire system. This paper showcases the application of the\nAttack-Fault-Defense Tree (AFDT) framework, which integrates attack trees,\nfault trees, and defense mechanisms into a unified model. AFDT provides an\nintuitive visual language that facilitates interdisciplinary collaboration,\nenabling experts from various fields to better assess system vulnerabilities\nand defenses. By applying AFDT to the Ground Segment of the satellite project,\nwe demonstrate how qualitative analyses can be performed to identify weaknesses\nand enhance the overall system's security and safety. This case highlights the\nimportance of jointly analyzing attacks, faults, and defenses to improve\nresilience in complex cyber-physical environments."
    },
    {
        "date": "2025-04",
        "title": "S3C2 Summit 2024-08: Government Secure Supply Chain Summit",
        "author": "Courtney Miller, William Enck, Yasemin Acar, Michel Cukier, Alexandros Kapravelos, Christian Kastner, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.00924v1",
        "abstract": "Supply chain security has become a very important vector to consider when\ndefending against adversary attacks. Due to this, more and more developers are\nkeen on improving their supply chains to make them more robust against future\nthreats. On August 29, 2024 researchers from the Secure Software Supply Chain\nCenter (S3C2) gathered 14 practitioners from 10 government agencies to discuss\nthe state of supply chain security. The goal of the summit is to share insights\nbetween companies and developers alike to foster new collaborations and ideas\nmoving forward. Through this meeting, participants were questions on best\npractices and thoughts how to improve things for the future. In this paper we\nsummarize the responses and discussions of the summit."
    },
    {
        "date": "2025-04",
        "title": "TAMIS: Tailored Membership Inference Attacks on Synthetic Data",
        "author": "Paul Andrey, Batiste Le Bars, and Marc Tommasi",
        "link": "http://arxiv.org/abs/2504.00758v1",
        "abstract": "Membership Inference Attacks (MIA) enable to empirically assess the privacy\nof a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA\nagainst differentially-private synthetic data generation methods that rely on\ngraphical models. This attack builds upon MAMA-MIA, a recently-published\nstate-of-the-art method. It lowers its computational cost and requires less\nattacker knowledge. Our attack is the product of a two-fold improvement. First,\nwe recover the graphical model having generated a synthetic dataset by using\nsolely that dataset, rather than shadow-modeling over an auxiliary one. This\nproves less costly and more performant. Second, we introduce a more\nmathematically-grounded attack score, that provides a natural threshold for\nbinary predictions. In our experiments, TAMIS achieves better or similar\nperformance as MAMA-MIA on replicas of the SNAKE challenge."
    },
    {
        "date": "2025-04",
        "title": "Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning Under Zero-Inflated Distribution",
        "author": "Songran Bai, Yuheng Ji, Yue Liu, Xingwei Zhang, Xiaolong Zheng, and Daniel Dajun Zeng",
        "link": "http://arxiv.org/abs/2504.00721v1",
        "abstract": "Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is\ncrucial for urban risk management tasks, including crime prediction and traffic\naccident profiling. However, SGL models are vulnerable to adversarial attacks,\ncompromising their practical utility. While adversarial training (AT) has been\nwidely used to bolster model robustness, our study finds that traditional AT\nexacerbates performance disparities between majority and minority classes under\nZID, potentially leading to irreparable losses due to underreporting critical\nrisk events. In this paper, we first demonstrate the smaller top-k gradients\nand lower separability of minority class are key factors contributing to this\ndisparity. To address these issues, we propose MinGRE, a framework for Minority\nClass Gradients and Representations Enhancement. MinGRE employs a\nmulti-dimensional attention mechanism to reweight spatiotemporal gradients,\nminimizing the gradient distribution discrepancies across classes.\nAdditionally, we introduce an uncertainty-guided contrastive loss to improve\nthe inter-class separability and intra-class compactness of minority\nrepresentations with higher uncertainty. Extensive experiments demonstrate that\nthe MinGRE framework not only significantly reduces the performance disparity\nacross classes but also achieves enhanced robustness compared to existing\nbaselines. These findings underscore the potential of our method in fostering\nthe development of more equitable and robust models."
    },
    {
        "date": "2025-04",
        "title": "Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models",
        "author": "Alireza Aghabagherloo, Aydin Abadi, Sumanta Sarkar, Vishnu Asutosh Dasu, and Bart Preneel",
        "link": "http://arxiv.org/abs/2504.00638v1",
        "abstract": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy."
    },
    {
        "date": "2025-04",
        "title": "Geometric Median Matching for Robust k-Subset Selection from Noisy Data",
        "author": "Anish Acharya, Sujay Sanghavi, Alexandros G. Dimakis, and Inderjit S Dhillon",
        "link": "http://arxiv.org/abs/2504.00564v2",
        "abstract": "Data pruning -- the combinatorial task of selecting a small and\nrepresentative subset from a large dataset, is crucial for mitigating the\nenormous computational costs associated with training data-hungry modern deep\nlearning models at scale. Since large scale data collections are invariably\nnoisy, developing data pruning strategies that remain robust even in the\npresence of corruption is critical in practice. However, existing data pruning\nmethods often fail under high corruption rates due to their reliance on\nempirical mean estimation, which is highly sensitive to outliers.\n  In response, we propose Geometric Median (GM) Matching, a novel k-subset\nselection strategy that leverages Geometric Median -- a robust estimator with\nan optimal breakdown point of 1/2; to enhance resilience against noisy data.\nOur method iteratively selects a k-subset such that the mean of the subset\napproximates the GM of the (potentially) noisy dataset, ensuring robustness\neven under arbitrary corruption. We provide theoretical guarantees, showing\nthat GM Matching enjoys an improved O(1/k) convergence rate -- a quadratic\nimprovement over random sampling, even under arbitrary corruption. Extensive\nexperiments across image classification and image generation tasks demonstrate\nthat GM Matching consistently outperforms existing pruning approaches,\nparticularly in high-corruption settings and at high pruning rates; making it a\nstrong baseline for robust data pruning."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Curriculum Graph-Free Knowledge Distillation for Graph Neural Networks",
        "author": "Yuang Jia, Xiaojuan Shan, Jun Xia, Guancheng Wan, Yuchen Zhang, Wenke Huang, Mang Ye, and Stan Z. Li",
        "link": "http://arxiv.org/abs/2504.00540v2",
        "abstract": "Data-free Knowledge Distillation (DFKD) is a method that constructs\npseudo-samples using a generator without real data, and transfers knowledge\nfrom a teacher model to a student by enforcing the student to overcome\ndimensional differences and learn to mimic the teacher's outputs on these\npseudo-samples. In recent years, various studies in the vision domain have made\nnotable advancements in this area. However, the varying topological structures\nand non-grid nature of graph data render the methods from the vision domain\nineffective. Building upon prior research into differentiable methods for graph\nneural networks, we propose a fast and high-quality data-free knowledge\ndistillation approach in this paper. Without compromising distillation quality,\nthe proposed graph-free KD method (ACGKD) significantly reduces the spatial\ncomplexity of pseudo-graphs by leveraging the Binary Concrete distribution to\nmodel the graph structure and introducing a spatial complexity tuning\nparameter. This approach enables efficient gradient computation for the graph\nstructure, thereby accelerating the overall distillation process. Additionally,\nACGKD eliminates the dimensional ambiguity between the student and teacher\nmodels by increasing the student's dimensions and reusing the teacher's\nclassifier. Moreover, it equips graph knowledge distillation with a CL-based\nstrategy to ensure the student learns graph structures progressively. Extensive\nexperiments demonstrate that ACGKD achieves state-of-the-art performance in\ndistilling knowledge from GNNs without training data."
    },
    {
        "date": "2025-04",
        "title": "Robust LiDAR-Camera Calibration with 2D Gaussian Splatting",
        "author": "Shuyi Zhou, Shuxiang Xie, Ryoichi Ishikawa, and Takeshi Oishi",
        "link": "http://arxiv.org/abs/2504.00525v1",
        "abstract": "LiDAR-camera systems have become increasingly popular in robotics recently. A\ncritical and initial step in integrating the LiDAR and camera data is the\ncalibration of the LiDAR-camera system. Most existing calibration methods rely\non auxiliary target objects, which often involve complex manual operations,\nwhereas targetless methods have yet to achieve practical effectiveness.\nRecognizing that 2D Gaussian Splatting (2DGS) can reconstruct geometric\ninformation from camera image sequences, we propose a calibration method that\nestimates LiDAR-camera extrinsic parameters using geometric constraints. The\nproposed method begins by reconstructing colorless 2DGS using LiDAR point\nclouds. Subsequently, we update the colors of the Gaussian splats by minimizing\nthe photometric loss. The extrinsic parameters are optimized during this\nprocess. Additionally, we address the limitations of the photometric loss by\nincorporating the reprojection and triangulation losses, thereby enhancing the\ncalibration robustness and accuracy."
    },
    {
        "date": "2025-04",
        "title": "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning",
        "author": "Jie Ma, Zhitao Gao, Qi Chai, Jun Liu, Pinghui Wang, Jing Tao, and Zhou Su",
        "link": "http://arxiv.org/abs/2504.00487v2",
        "abstract": "Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning\ntask requiring intelligent systems to answer natural language queries based on\npaired audio-video inputs accurately. However, existing AVQA approaches often\nsuffer from overfitting to dataset biases, leading to poor robustness.\nMoreover, current datasets may not effectively diagnose these methods. To\naddress these challenges, we first introduce a novel dataset, FortisAVQA,\nconstructed in two stages: (1) rephrasing questions in the test split of the\npublic MUSIC-AVQA dataset and (2) introducing distribution shifts across\nquestions. The first stage expands the test space with greater diversity, while\nthe second enables a refined robustness evaluation across rare, frequent, and\noverall question distributions. Second, we introduce a robust Multimodal\nAudio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle\ncollaborative debiasing strategy to mitigate bias learning. Experimental\nresults demonstrate that our architecture achieves state-of-the-art performance\non FortisAVQA, with a notable improvement of 7.81\\%. Extensive ablation studies\non both datasets validate the effectiveness of our debiasing components.\nAdditionally, our evaluation reveals the limited robustness of existing\nmultimodal QA methods. We also verify the plug-and-play capability of our\nstrategy by integrating it with various baseline models across both datasets.\nOur dataset and code are available at https://github.com/reml-group/fortisavqa."
    },
    {
        "date": "2025-04",
        "title": "Efficient Near-Optimal Algorithm for Online Shortest Paths in Directed Acyclic Graphs with Bandit Feedback Against Adaptive Adversaries",
        "author": "Arnab Maiti, Zhiyuan Fan, Kevin Jamieson, Lillian J. Ratliff, and Gabriele Farina",
        "link": "http://arxiv.org/abs/2504.00461v1",
        "abstract": "In this paper, we study the online shortest path problem in directed acyclic\ngraphs (DAGs) under bandit feedback against an adaptive adversary. Given a DAG\n$G = (V, E)$ with a source node $v_{\\mathsf{s}}$ and a sink node\n$v_{\\mathsf{t}}$, let $X \\subseteq \\{0,1\\}^{|E|}$ denote the set of all paths\nfrom $v_{\\mathsf{s}}$ to $v_{\\mathsf{t}}$. At each round $t$, we select a path\n$\\mathbf{x}_t \\in X$ and receive bandit feedback on our loss $\\langle\n\\mathbf{x}_t, \\mathbf{y}_t \\rangle \\in [-1,1]$, where $\\mathbf{y}_t$ is an\nadversarially chosen loss vector. Our goal is to minimize regret with respect\nto the best path in hindsight over $T$ rounds. We propose the first\ncomputationally efficient algorithm to achieve a near-minimax optimal regret\nbound of $\\tilde O(\\sqrt{|E|T\\log |X|})$ with high probability against any\nadaptive adversary, where $\\tilde O(\\cdot)$ hides logarithmic factors in the\nnumber of edges $|E|$. Our algorithm leverages a novel loss estimator and a\ncentroid-based decomposition in a nontrivial manner to attain this regret\nbound.\n  As an application, we show that our algorithm for DAGs provides\nstate-of-the-art efficient algorithms for $m$-sets, extensive-form games, the\nColonel Blotto game, shortest walks in directed graphs, hypercubes, and\nmulti-task multi-armed bandits, achieving improved high-probability regret\nguarantees in all these settings."
    },
    {
        "date": "2025-04",
        "title": "Mixture-of-Attack-Experts with Class Regularization for Unified Physical-Digital Face Attack Detection",
        "author": "Shunxin Chen, Ajian Liu, Junze Zheng, Jun Wan, Kailai Peng, Sergio Escalera, and Zhen Lei",
        "link": "http://arxiv.org/abs/2504.00458v1",
        "abstract": "Facial recognition systems in real-world scenarios are susceptible to both\ndigital and physical attacks. Previous methods have attempted to achieve\nclassification by learning a comprehensive feature space. However, these\nmethods have not adequately accounted for the inherent characteristics of\nphysical and digital attack data, particularly the large intra class variation\nin attacks and the small inter-class variation between live and fake faces. To\naddress these limitations, we propose the Fine-Grained MoE with Class-Aware\nRegularization CLIP framework (FG-MoE-CLIP-CAR), incorporating key improvements\nat both the feature and loss levels. At the feature level, we employ a Soft\nMixture of Experts (Soft MoE) architecture to leverage different experts for\nspecialized feature processing. Additionally, we refine the Soft MoE to capture\nmore subtle differences among various types of fake faces. At the loss level,\nwe introduce two constraint modules: the Disentanglement Module (DM) and the\nCluster Distillation Module (CDM). The DM enhances class separability by\nincreasing the distance between the centers of live and fake face classes.\nHowever, center-to-center constraints alone are insufficient to ensure\ndistinctive representations for individual features. Thus, we propose the CDM\nto further cluster features around their respective class centers while\nmaintaining separation from other classes. Moreover, specific attacks that\nsignificantly deviate from common attack patterns are often overlooked. To\naddress this issue, our distance calculation prioritizes more distant features.\nExperimental results on two unified physical-digital attack datasets\ndemonstrate that the proposed method achieves state-of-the-art (SOTA)\nperformance."
    },
    {
        "date": "2025-04",
        "title": "FA^{3}-CLIP: Frequency-Aware Cues Fusion and Attack-Agnostic Prompt Learning for Unified Face Attack Detection",
        "author": "Yongze Li, Ning Li, Ajian Liu, Hui Ma, Liying Yang, Xihong Chen, Zhiyao Liang, Yanyan Liang, Jun Wan, and Zhen Lei",
        "link": "http://arxiv.org/abs/2504.00454v1",
        "abstract": "Facial recognition systems are vulnerable to physical (e.g., printed photos)\nand digital (e.g., DeepFake) face attacks. Existing methods struggle to\nsimultaneously detect physical and digital attacks due to: 1) significant\nintra-class variations between these attack types, and 2) the inadequacy of\nspatial information alone to comprehensively capture live and fake cues. To\naddress these issues, we propose a unified attack detection model termed\nFrequency-Aware and Attack-Agnostic CLIP (FA\\textsuperscript{3}-CLIP), which\nintroduces attack-agnostic prompt learning to express generic live and fake\ncues derived from the fusion of spatial and frequency features, enabling\nunified detection of live faces and all categories of attacks. Specifically,\nthe attack-agnostic prompt module generates generic live and fake prompts\nwithin the language branch to extract corresponding generic representations\nfrom both live and fake faces, guiding the model to learn a unified feature\nspace for unified attack detection. Meanwhile, the module adaptively generates\nthe live/fake conditional bias from the original spatial and frequency\ninformation to optimize the generic prompts accordingly, reducing the impact of\nintra-class variations. We further propose a dual-stream cues fusion framework\nin the vision branch, which leverages frequency information to complement\nsubtle cues that are difficult to capture in the spatial domain. In addition, a\nfrequency compression block is utilized in the frequency stream, which reduces\nredundancy in frequency features while preserving the diversity of crucial\ncues. We also establish new challenging protocols to facilitate unified face\nattack detection effectiveness. Experimental results demonstrate that the\nproposed method significantly improves performance in detecting physical and\ndigital face attacks, achieving state-of-the-art results."
    },
    {
        "date": "2025-04",
        "title": "Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation",
        "author": "Lan Sun, Songpengcheng Xia, Jiarui Yang, and Ling Pei",
        "link": "http://arxiv.org/abs/2504.00438v1",
        "abstract": "The proliferation of wearable technology has established multi-device\necosystems comprising smartphones, smartwatches, and headphones as critical\nenablers for ubiquitous pedestrian localization. However, traditional\npedestrian dead reckoning (PDR) struggles with diverse motion modes, while\ndata-driven methods, despite improving accuracy, often lack robustness due to\ntheir reliance on a single-device setup. Therefore, a promising solution is to\nfully leverage existing wearable devices to form a flexiwear bodynet for robust\nand accurate pedestrian localization. This paper presents Suite-IN++, a deep\nlearning framework for flexiwear bodynet-based pedestrian localization.\nSuite-IN++ integrates motion data from wearable devices on different body\nparts, using contrastive learning to separate global and local motion features.\nIt fuses global features based on the data reliability of each device to\ncapture overall motion trends and employs an attention mechanism to uncover\ncross-device correlations in local features, extracting motion details helpful\nfor accurate localization. To evaluate our method, we construct a real-life\nflexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and\nAirPods) across diverse walking modes and device configurations. Experimental\nresults demonstrate that Suite-IN++ achieves superior localization accuracy and\nrobustness, significantly outperforming state-of-the-art models in real-life\npedestrian tracking scenarios."
    },
    {
        "date": "2025-04",
        "title": "Unleashing the Power of Pre-trained Encoders for Universal Adversarial Attack Detection",
        "author": "Yinghe Zhang, Chi Liu, Shuai Zhou, Sheng Shen, and Peng Gui",
        "link": "http://arxiv.org/abs/2504.00429v1",
        "abstract": "Adversarial attacks pose a critical security threat to real-world AI systems\nby injecting human-imperceptible perturbations into benign samples to induce\nmisclassification in deep learning models. While existing detection methods,\nsuch as Bayesian uncertainty estimation and activation pattern analysis, have\nachieved progress through feature engineering, their reliance on handcrafted\nfeature design and prior knowledge of attack patterns limits generalization\ncapabilities and incurs high engineering costs. To address these limitations,\nthis paper proposes a lightweight adversarial detection framework based on the\nlarge-scale pre-trained vision-language model CLIP. Departing from conventional\nadversarial feature characterization paradigms, we innovatively adopt an\nanomaly detection perspective. By jointly fine-tuning CLIP's dual visual-text\nencoders with trainable adapter networks and learnable prompts, we construct a\ncompact representation space tailored for natural images. Notably, our\ndetection architecture achieves substantial improvements in generalization\ncapability across both known and unknown attack patterns compared to\ntraditional methods, while significantly reducing training overhead. This study\nprovides a novel technical pathway for establishing a parameter-efficient and\nattack-agnostic defense paradigm, markedly enhancing the robustness of vision\nsystems against evolving adversarial threats."
    },
    {
        "date": "2025-04",
        "title": "CopyQNN: Quantum Neural Network Extraction Attack under Varying Quantum Noise",
        "author": "Zhenxiao Fu, Leyi Zhao, Xuhong Zhang, Yilun Xu, Gang Huang, and Fan Chen",
        "link": "http://arxiv.org/abs/2504.00366v1",
        "abstract": "Quantum Neural Networks (QNNs) have shown significant value across domains,\nwith well-trained QNNs representing critical intellectual property often\ndeployed via cloud-based QNN-as-a-Service (QNNaaS) platforms. Recent work has\nexamined QNN model extraction attacks using classical and emerging quantum\nstrategies. These attacks involve adversaries querying QNNaaS platforms to\nobtain labeled data for training local substitute QNNs that replicate the\nfunctionality of cloud-based models. However, existing approaches have largely\noverlooked the impact of varying quantum noise inherent in noisy\nintermediate-scale quantum (NISQ) computers, limiting their effectiveness in\nreal-world settings. To address this limitation, we propose the CopyQNN\nframework, which employs a three-step data cleaning method to eliminate noisy\ndata based on its noise sensitivity. This is followed by the integration of\ncontrastive and transfer learning within the quantum domain, enabling efficient\ntraining of substitute QNNs using a limited but cleaned set of queried data.\nExperimental results on NISQ computers demonstrate that a practical\nimplementation of CopyQNN significantly outperforms state-of-the-art QNN\nextraction attacks, achieving an average performance improvement of 8.73%\nacross all tasks while reducing the number of required queries by 90x, with\nonly a modest increase in hardware overhead."
    },
    {
        "date": "2025-04",
        "title": "Integrated LLM-Based Intrusion Detection with Secure Slicing xApp for Securing O-RAN-Enabled Wireless Network Deployments",
        "author": "Joshua Moore, Aly Sabri Abdalla, Prabesh Khanal, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2504.00341v1",
        "abstract": "The Open Radio Access Network (O-RAN) architecture is reshaping\ntelecommunications by promoting openness, flexibility, and intelligent\nclosed-loop optimization. By decoupling hardware and software and enabling\nmulti-vendor deployments, O-RAN reduces costs, enhances performance, and allows\nrapid adaptation to new technologies. A key innovation is intelligent network\nslicing, which partitions networks into isolated slices tailored for specific\nuse cases or quality of service requirements. The RAN Intelligent Controller\nfurther optimizes resource allocation, ensuring efficient utilization and\nimproved service quality for user equipment (UEs). However, the modular and\ndynamic nature of O-RAN expands the threat surface, necessitating advanced\nsecurity measures to maintain network integrity, confidentiality, and\navailability. Intrusion detection systems have become essential for identifying\nand mitigating attacks. This research explores using large language models\n(LLMs) to generate security recommendations based on the temporal traffic\npatterns of connected UEs. The paper introduces an LLM-driven intrusion\ndetection framework and demonstrates its efficacy through experimental\ndeployments, comparing non fine-tuned and fine-tuned models for task-specific\naccuracy."
    },
    {
        "date": "2025-03",
        "title": "Federated Learning for Cross-Domain Data Privacy: A Distributed Approach to Secure Collaboration",
        "author": "Yiwei Zhang, Jie Liu, Jiawei Wang, Lu Dai, Fan Guo, and Guohui Cai",
        "link": "http://arxiv.org/abs/2504.00282v1",
        "abstract": "This paper proposes a data privacy protection framework based on federated\nlearning, which aims to realize effective cross-domain data collaboration under\nthe premise of ensuring data privacy through distributed learning. Federated\nlearning greatly reduces the risk of privacy breaches by training the model\nlocally on each client and sharing only model parameters rather than raw data.\nThe experiment verifies the high efficiency and privacy protection ability of\nfederated learning under different data sources through the simulation of\nmedical, financial, and user data. The results show that federated learning can\nnot only maintain high model performance in a multi-domain data environment but\nalso ensure effective protection of data privacy. The research in this paper\nprovides a new technical path for cross-domain data collaboration and promotes\nthe application of large-scale data analysis and machine learning while\nprotecting privacy."
    },
    {
        "date": "2025-03",
        "title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
        "author": "Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Flemming, and Tianlong Chen",
        "link": "http://arxiv.org/abs/2504.00218v1",
        "abstract": "Most discussions about Large Language Model (LLM) safety have focused on\nsingle-agent settings but multi-agent LLM systems now create novel adversarial\nrisks because their behavior depends on communication between agents and\ndecentralized reasoning. In this work, we innovatively focus on attacking\npragmatic systems that have constrains such as limited token bandwidth, latency\nbetween message delivery, and defense mechanisms. We design a\n$\\textit{permutation-invariant adversarial attack}$ that optimizes prompt\ndistribution across latency and bandwidth-constraint network topologies to\nbypass distributed safety mechanisms within the system. Formulating the attack\npath as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the\nnovel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage\ngraph-based optimization to maximize attack success rate while minimizing\ndetection risk. Evaluating across models including $\\texttt{Llama}$,\n$\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on\nvarious datasets like $\\texttt{JailBreakBench}$ and\n$\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up\nto $7\\times$, exposing critical vulnerabilities in multi-agent systems.\nMoreover, we demonstrate that existing defenses, including variants of\n$\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack,\nemphasizing the urgent need for multi-agent specific safety mechanisms."
    },
    {
        "date": "2025-03",
        "title": "Few-Shot Generation of Brain Tumors for Secure and Fair Data Sharing",
        "author": "Yongyi Shi, and Ge Wang",
        "link": "http://arxiv.org/abs/2504.00150v1",
        "abstract": "Leveraging multi-center data for medical analytics presents challenges due to\nprivacy concerns and data heterogeneity. While distributed approaches such as\nfederated learning has gained traction, they remain vulnerable to privacy\nbreaches, particularly in sensitive domains like medical imaging. Generative\nmodels, such as diffusion models, enhance privacy by synthesizing realistic\ndata. However, they are prone to memorization, especially when trained on small\ndatasets. This study proposes a decentralized few-shot generative model (DFGM)\nto synthesize brain tumor images while fully preserving privacy. DFGM\nharmonizes private tumor data with publicly shareable healthy images from\nmultiple medical centers, constructing a new dataset by blending tumor\nforegrounds with healthy backgrounds. This approach ensures stringent privacy\nprotection and enables controllable, high-quality synthesis by preserving both\nthe healthy backgrounds and tumor foregrounds. We assess DFGM's effectiveness\nin brain tumor segmentation using a UNet, achieving Dice score improvements of\n3.9% for data augmentation and 4.6% for fairness on a separate dataset."
    },
    {
        "date": "2025-03",
        "title": "Value of Information-based Deceptive Path Planning Under Adversarial Interventions",
        "author": "Wesley A. Suttle, Jesse Milzman, Mustafa O. Karabag, Brian M. Sadler, and Ufuk Topcu",
        "link": "http://arxiv.org/abs/2503.24284v1",
        "abstract": "Existing methods for deceptive path planning (DPP) address the problem of\ndesigning paths that conceal their true goal from a passive, external observer.\nSuch methods do not apply to problems where the observer has the ability to\nperform adversarial interventions to impede the path planning agent. In this\npaper, we propose a novel Markov decision process (MDP)-based model for the DPP\nproblem under adversarial interventions and develop new value of information\n(VoI) objectives to guide the design of DPP policies. Using the VoI objectives\nwe propose, path planning agents deceive the adversarial observer into choosing\nsuboptimal interventions by selecting trajectories that are of low\ninformational value to the observer. Leveraging connections to the linear\nprogramming theory for MDPs, we derive computationally efficient solution\nmethods for synthesizing policies for performing DPP under adversarial\ninterventions. In our experiments, we illustrate the effectiveness of the\nproposed solution method in achieving deceptiveness under adversarial\ninterventions and demonstrate the superior performance of our approach to both\nexisting DPP methods and conservative path planning approaches on illustrative\ngridworld problems."
    },
    {
        "date": "2025-03",
        "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
        "author": "Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, and Huimin Cui",
        "link": "http://arxiv.org/abs/2503.24191v1",
        "abstract": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed."
    },
    {
        "date": "2025-03",
        "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization",
        "author": "Yingrui Ji, Xi Xiao, Gaofei Chen, Hao Xu, Chenrui Ma, Lijing Zhu, Aokun Liang, and Jiansheng Chen",
        "link": "http://arxiv.org/abs/2503.24182v1",
        "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\nin cross-modal tasks such as zero-shot image classification and text-image\nretrieval by effectively aligning visual and textual representations. However,\nthe theoretical foundations underlying CLIP's strong generalization remain\nunclear. In this work, we address this gap by proposing the Cross-modal\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\noptimization. Under this view, the model maximizes shared cross-modal\ninformation while discarding modality-specific redundancies, thereby preserving\nessential semantic alignment across modalities. Building on this insight, we\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\nthat explicitly enforces these IB principles during training. CIBR introduces a\npenalty term to discourage modality-specific redundancy, thereby enhancing\nsemantic alignment between image and text features. We validate CIBR on\nextensive vision-language benchmarks, including zero-shot classification across\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\nThe results show consistent performance gains over standard CLIP. These\nfindings provide the first theoretical understanding of CLIP's generalization\nthrough the IB lens. They also demonstrate practical improvements, offering\nguidance for future cross-modal representation learning."
    },
    {
        "date": "2025-03",
        "title": "Pay More Attention to the Robustness of Prompt for Instruction Data Mining",
        "author": "Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, and Huaimin Wang",
        "link": "http://arxiv.org/abs/2503.24028v1",
        "abstract": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification",
        "author": "Chenqi Guo, Mengshuo Rong, Qianli Feng, Rongfan Feng, and Yinglong Ma",
        "link": "http://arxiv.org/abs/2503.24017v1",
        "abstract": "Crossmodal knowledge distillation (KD) aims to enhance a unimodal student\nusing a multimodal teacher model. In particular, when the teacher's modalities\ninclude the student's, additional complementary information can be exploited to\nimprove knowledge transfer. In supervised image classification, image datasets\ntypically include class labels that represent high-level concepts, suggesting a\nnatural avenue to incorporate textual cues for crossmodal KD. However, these\nlabels rarely capture the deeper semantic structures in real-world visuals and\ncan lead to label leakage if used directly as inputs, ultimately limiting KD\nperformance. To address these issues, we propose a multi-teacher crossmodal KD\nframework that integrates CLIP image embeddings with learnable WordNet-relaxed\ntext embeddings under a hierarchical loss. By avoiding direct use of exact\nclass names and instead using semantically richer WordNet expansions, we\nmitigate label leakage and introduce more diverse textual cues. Experiments\nshow that this strategy significantly boosts student performance, whereas noisy\nor overly precise text embeddings hinder distillation efficiency.\nInterpretability analyses confirm that WordNet-relaxed prompts encourage\nheavier reliance on visual features over textual shortcuts, while still\neffectively incorporating the newly introduced textual cues. Our method\nachieves state-of-the-art or second-best results on six public datasets,\ndemonstrating its effectiveness in advancing crossmodal KD."
    },
    {
        "date": "2025-03",
        "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
        "author": "Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2503.23924v1",
        "abstract": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."
    },
    {
        "date": "2025-03",
        "title": "A Channel-Triggered Backdoor Attack on Wireless Semantic Image Reconstruction",
        "author": "Jialin Wan, Nan Cheng, and Jinglong Shen",
        "link": "http://arxiv.org/abs/2503.23866v1",
        "abstract": "Despite the transformative impact of deep learning (DL) on wireless\ncommunication systems through data-driven end-to-end (E2E) learning, the\nsecurity vulnerabilities of these systems have been largely overlooked. Unlike\nthe extensively studied image domain, limited research has explored the threat\nof backdoor attacks on the reconstruction of symbols in semantic communication\n(SemCom) systems. Previous work has investigated such backdoor attacks at the\ninput level, but these approaches are infeasible in applications with strict\ninput control. In this paper, we propose a novel attack paradigm, termed\nChannel-Triggered Backdoor Attack (CT-BA), where the backdoor trigger is a\nspecific wireless channel. This attack leverages fundamental physical layer\ncharacteristics, making it more covert and potentially more threatening\ncompared to previous input-level attacks. Specifically, we utilize channel gain\nwith different fading distributions or channel noise with different power\nspectral densities as potential triggers. This approach establishes\nunprecedented attack flexibility as the adversary can select backdoor triggers\nfrom both fading characteristics and noise variations in diverse channel\nenvironments. Moreover, during the testing phase, CT-BA enables automatic\ntrigger activation through natural channel variations without requiring active\nadversary participation. We evaluate the robustness of CT-BA on a ViT-based\nJoint Source-Channel Coding (JSCC) model across three datasets: MNIST,\nCIFAR-10, and ImageNet. Furthermore, we apply CT-BA to three typical E2E SemCom\nsystems: BDJSCC, ADJSCC, and JSCCOFDM. Experimental results demonstrate that\nour attack achieves near-perfect attack success rate (ASR) while maintaining\neffective stealth. Finally, we discuss potential defense mechanisms against\nsuch attacks."
    },
    {
        "date": "2025-03",
        "title": "Distance Estimation to Support Assistive Drones for the Visually Impaired using Robust Calibration",
        "author": "Suman Raj, Bhavani A Madhabhavi, Madhav Kumar, Prabhav Gupta, and Yogesh Simmhan",
        "link": "http://arxiv.org/abs/2504.01988v1",
        "abstract": "Autonomous navigation by drones using onboard sensors, combined with deep\nlearning and computer vision algorithms, is impacting a number of domains. We\nexamine the use of drones to autonomously assist Visually Impaired People\n(VIPs) in navigating outdoor environments while avoiding obstacles. Here, we\npresent NOVA, a robust calibration technique using depth maps to estimate\nabsolute distances to obstacles in a campus environment. NOVA uses a\ndynamic-update method that can adapt to adversarial scenarios. We compare NOVA\nwith SOTA depth map approaches, and with geometric and regression-based\nbaseline models, for distance estimation to VIPs and other obstacles in diverse\nand dynamic conditions. We also provide exhaustive evaluations to validate the\nrobustness and generalizability of our methods. NOVA predicts distances to VIP\nwith an error <30cm and to different obstacles like cars and bicycles with a\nmaximum of 60cm error, which are better than the baselines. NOVA also clearly\nout-performs SOTA depth map methods, by upto 5.3-14.6x."
    },
    {
        "date": "2025-03",
        "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
        "author": "Jingzheng Li, Xianglong Liu, Shikui Wei, Zhijun Chen, Bing Li, Qing Guo, Xianqi Yang, Yanjun Pu, and Jiakai Wang",
        "link": "http://arxiv.org/abs/2503.23708v2",
        "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment."
    },
    {
        "date": "2025-03",
        "title": "Security Analysis of Chain-FS service",
        "author": "Vanessa Teague, and Arash Mirzaei",
        "link": "http://arxiv.org/abs/2503.23627v1",
        "abstract": "We examine the security of a cloud storage service that makes very strong\nclaims about the ``trustless'' nature of its security. We find that, although\nstored files are end-to-end encrypted, the encryption method allows for\neffective dictionary attacks by a malicious server when passwords only just\nmeet the minimum length required. Furthermore, the file sharing function simply\nsends the decryption passwords to the server with no protection other than TLS."
    },
    {
        "date": "2025-03",
        "title": "Buffer is All You Need: Defending Federated Learning against Backdoor Attacks under Non-iids via Buffering",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23511v1",
        "abstract": "Federated Learning (FL) is a popular paradigm enabling clients to jointly\ntrain a global model without sharing raw data. However, FL is known to be\nvulnerable towards backdoor attacks due to its distributed nature. As\nparticipants, attackers can upload model updates that effectively compromise\nFL. What's worse, existing defenses are mostly designed under\nindependent-and-identically-distributed (iid) settings, hence neglecting the\nfundamental non-iid characteristic of FL. Here we propose FLBuff for tackling\nbackdoor attacks even under non-iids. The main challenge for such defenses is\nthat non-iids bring benign and malicious updates closer, hence harder to\nseparate. FLBuff is inspired by our insight that non-iids can be modeled as\nomni-directional expansion in representation space while backdoor attacks as\nuni-directional. This leads to the key design of FLBuff, i.e., a\nsupervised-contrastive-learning model extracting penultimate-layer\nrepresentations to create a large in-between buffer layer. Comprehensive\nevaluations demonstrate that FLBuff consistently outperforms state-of-the-art\ndefenses."
    },
    {
        "date": "2025-03",
        "title": "Revisiting the Relationship between Adversarial and Clean Training: Why Clean Training Can Make Adversarial Training Better",
        "author": "MingWei Zhou, and Xiaobing Pei",
        "link": "http://arxiv.org/abs/2504.00038v1",
        "abstract": "Adversarial training (AT) is an effective technique for enhancing adversarial\nrobustness, but it usually comes at the cost of a decline in generalization\nability. Recent studies have attempted to use clean training to assist\nadversarial training, yet there are contradictions among the conclusions. We\ncomprehensively summarize the representative strategies and, with a focus on\nthe multi - view hypothesis, provide a unified explanation for the\ncontradictory phenomena among different studies. In addition, we conduct an in\n- depth analysis of the knowledge combinations transferred from clean - trained\nmodels to adversarially - trained models in previous studies, and find that\nthey can be divided into two categories: reducing the learning difficulty and\nproviding correct guidance. Based on this finding, we propose a new idea of\nleveraging clean training to further improve the performance of advanced AT\nmethods.We reveal that the problem of generalization degradation faced by AT\npartly stems from the difficulty of adversarial training in learning certain\nsample features, and this problem can be alleviated by making full use of clean\ntraining."
    },
    {
        "date": "2025-03",
        "title": "Codehacks: A Dataset of Adversarial Tests for Competitive Programming Problems Obtained from Codeforces",
        "author": "Max Hort, and Leon Moonen",
        "link": "http://arxiv.org/abs/2503.23466v1",
        "abstract": "Software is used in critical applications in our day-to-day life and it is\nimportant to ensure its correctness. One popular approach to assess correctness\nis to evaluate software on tests. If a test fails, it indicates a fault in the\nsoftware under test; if all tests pass correctly, one may assume that the\nsoftware is correct. However, the reliability of these results depends on the\ntest suite considered, and there is a risk of false negatives (i.e. software\nthat passes all available tests but contains bugs because some cases are not\ntested). Therefore, it is important to consider error-inducing test cases when\nevaluating software.\n  To support data-driven creation of such a test-suite, which is especially of\ninterest for testing software synthesized from large language models, we curate\na dataset (Codehacks) of programming problems together with corresponding\nerror-inducing test cases (i.e., \"hacks\"). This dataset is collected from the\nwild, in particular, from the Codeforces online judge platform. The dataset\ncomprises 288,617 hacks for 5,578 programming problems, each with a natural\nlanguage description, as well as the source code for 2,196 submitted solutions\nto these problems that can be broken with their corresponding hacks.\n  Keywords: competitive programming, language model, dataset"
    },
    {
        "date": "2025-03",
        "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
        "author": "Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, and Zhi Wang",
        "link": "http://arxiv.org/abs/2503.23388v1",
        "abstract": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
    },
    {
        "date": "2025-03",
        "title": "Two Heads Are Better than One: Model-Weight and Latent-Space Analysis for Federated Learning on Non-iid Data against Poisoning Attacks",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23288v1",
        "abstract": "Federated Learning is a popular paradigm that enables remote clients to\njointly train a global model without sharing their raw data. However, FL has\nbeen shown to be vulnerable towards model poisoning attacks due to its\ndistributed nature. Particularly, attackers acting as participants can upload\narbitrary model updates that effectively compromise the global model of FL.\nWhile extensive research has been focusing on fighting against these attacks,\nwe find that most of them assume data at remote clients are under iid while in\npractice they are inevitably non-iid. Our benchmark evaluations reveal that\nexisting defenses generally fail to live up to their reputation when applied to\nvarious non-iid scenarios. In this paper, we propose a novel approach,\nGeminiGuard, that aims to address such a significant gap. We design GeminiGuard\nto be lightweight, versatile, and unsupervised so that it aligns well with the\npractical requirements of deploying such defenses. The key challenge from\nnon-iids is that they make benign model updates look more similar to malicious\nones. GeminiGuard is mainly built on two fundamental observations: (1) existing\ndefenses based on either model-weight analysis or latent-space analysis face\nlimitations in covering different MPAs and non-iid scenarios, and (2)\nmodel-weight and latent-space analysis are sufficiently different yet\npotentially complementary methods as MPA defenses. We hence incorporate a novel\nmodel-weight analysis component as well as a custom latent-space analysis\ncomponent in GeminiGuard, aiming to further enhance its defense performance. We\nconduct extensive experiments to evaluate our defense across various settings,\ndemonstrating its effectiveness in countering multiple types of untargeted and\ntargeted MPAs, including adaptive ones. Our comprehensive evaluations show that\nGeminiGuard consistently outperforms SOTA defenses under various settings."
    },
    {
        "date": "2025-03",
        "title": "Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions",
        "author": "Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2503.23278v2",
        "abstract": "The Model Context Protocol (MCP) is a standardized interface designed to\nenable seamless interaction between AI models and external tools and resources,\nbreaking down data silos and facilitating interoperability across diverse\nsystems. This paper provides a comprehensive overview of MCP, focusing on its\ncore components, workflow, and the lifecycle of MCP servers, which consists of\nthree key phases: creation, operation, and update. We analyze the security and\nprivacy risks associated with each phase and propose strategies to mitigate\npotential threats. The paper also examines the current MCP landscape, including\nits adoption by industry leaders and various use cases, as well as the tools\nand platforms supporting its integration. We explore future directions for MCP,\nhighlighting the challenges and opportunities that will influence its adoption\nand evolution within the broader AI ecosystem. Finally, we offer\nrecommendations for MCP stakeholders to ensure its secure and sustainable\ndevelopment as the AI landscape continues to evolve."
    },
    {
        "date": "2025-03",
        "title": "Comprehensive Survey towards Security Authentication Methods for Satellite Communication Systems",
        "author": "Yunfei Meng, Changbo Ke, and Zhiqiu Huang",
        "link": "http://arxiv.org/abs/2503.23277v1",
        "abstract": "Satellite communication systems (SatCom) is a brand-new network that uses\nartificial Earth satellites as relay stations to provide communication services\nsuch as broadband Internet access to various users on land, sea, air and in\nspace. It features wide coverage, relatively high transmission rates and strong\nanti-interference capabilities. Security authentication is of crucial\nsignificance for the stable operation and widespread application of satellite\ncommunication systems. It can effectively prevent unauthorized access, ensuring\nthat only users and devices that pass security authentication can access the\nsatellite network. It also ensures the confidentiality, integrity, and\navailability of data during transmission and storage, preventing data from\nbeing stolen, tampered with, or damaged. By means of literature research and\ncomparative analysis, this paper carries out on a comprehensive survey towards\nthe security authentication methods used by SatCom. This paper first summarizes\nthe existing SatCom authentication methods as five categories, namely, those\nbased on cryptography, Blockchain, satellite orbital information, the AKA\nprotocol and physical hardware respectively. Subsequently, a comprehensive\ncomparative analysis is carried out on the above-mentioned five categories of\nsecurity authentication methods from four dimensions, i.e., security,\nimplementation difficulty and cost, applicable scenarios and real-time\nperformance, and the final comparison results are following obtained. Finally,\nprospects are made for several important future research directions of security\nauthentication methods for SatCom, laying a well foundation for further\ncarrying on the related research works."
    },
    {
        "date": "2025-03",
        "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition",
        "author": "Shihao Cheng, Jinlu Zhang, Yue Liu, and Zhigang Tu",
        "link": "http://arxiv.org/abs/2503.23266v1",
        "abstract": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments."
    },
    {
        "date": "2025-03",
        "title": "Mismatch-Robust Underwater Acoustic Localization Using A Differentiable Modular Forward Model",
        "author": "Dariush Kari, Yongjie Zhuang, and Andrew C. Singer",
        "link": "http://arxiv.org/abs/2503.23260v1",
        "abstract": "In this paper, we study the underwater acoustic localization in the presence\nof environmental mismatch. Especially, we exploit a pre-trained neural network\nfor the acoustic wave propagation in a gradient-based optimization framework to\nestimate the source location. To alleviate the effect of mismatch between the\ntraining data and the test data, we simultaneously optimize over the network\nweights at the inference time, and provide conditions under which this method\nis effective. Moreover, we introduce a physics-inspired modularity in the\nforward model that enables us to learn the path lengths of the multipath\nstructure in an end-to-end training manner without access to the specific path\nlabels. We investigate the validity of the assumptions in a simple yet\nillustrative environment model."
    },
    {
        "date": "2025-03",
        "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
        "author": "Shih-Han Chan",
        "link": "http://arxiv.org/abs/2503.23250v1",
        "abstract": "Security threats like prompt injection attacks pose significant risks to\napplications that integrate Large Language Models (LLMs), potentially leading\nto unauthorized actions such as API misuse. Unlike previous approaches that aim\nto detect these attacks on a best-effort basis, this paper introduces a novel\nmethod that appends an Encrypted Prompt to each user prompt, embedding current\npermissions. These permissions are verified before executing any actions (such\nas API calls) generated by the LLM. If the permissions are insufficient, the\nLLM's actions will not be executed, ensuring safety. This approach guarantees\nthat only actions within the scope of the current permissions from the LLM can\nproceed. In scenarios where adversarial prompts are introduced to mislead the\nLLM, this method ensures that any unauthorized actions from LLM wouldn't be\nexecuted by verifying permissions in Encrypted Prompt. Thus, threats like\nprompt injection attacks that trigger LLM to generate harmful actions can be\neffectively mitigated."
    },
    {
        "date": "2025-03",
        "title": "Agent-Based Modeling and Deep Neural Networks for Establishing Digital Twins of Secure Facilities under Sensing Restrictions",
        "author": "Chathika Gunaratne, Mason Stott, Debraj De, Gautam Malviya Thakur, and Chris Young",
        "link": "http://arxiv.org/abs/2503.23147v1",
        "abstract": "Digital twin technologies help practitioners simulate, monitor, and predict\nundesirable outcomes in-silico, while avoiding the cost and risks of conducting\nlive simulation exercises. Virtual reality (VR) based digital twin technologies\nare especially useful when monitoring human Patterns of Life (POL) in secure\nnuclear facilities, where live simulation exercises are too dangerous and\ncostly to ever perform. However, the high-security status of such facilities\nmay restrict modelers from deploying human activity sensors for data\ncollection. This problem was encountered when deploying MetaPOL, a digital twin\nsystem to prevent insider threat or sabotage of secure facilities, at a secure\nnuclear reactor facility at Oak Ridge National Laboratory (ORNL). This\nchallenge was addressed using an agent-based model (ABM), driven by anecdotal\nevidence of facility personnel POL, to generate synthetic movement\ntrajectories. These synthetic trajectories were then used to train deep neural\nnetwork surrogates for next location and stay duration prediction to drive NPCs\nin the VR environment. In this study, we evaluate the efficacy of this\ntechnique for establishing NPC movement within MetaPOL and the ability to\ndistinguish NPC movement during normal operations from that during a simulated\nemergency response. Our results demonstrate the success of using a multi-layer\nperceptron for next location prediction and mixture density network for stay\nduration prediction to predict the ABM generated trajectories. We also find\nthat NPC movement in the VR environment driven by the deep neural networks\nunder normal operations remain significantly different to that seen when\nsimulating responses to a simulated emergency scenario."
    },
    {
        "date": "2025-03",
        "title": "AuditVotes: A Framework Towards More Deployable Certified Robustness for Graph Neural Networks",
        "author": "Yuni Lai, Yulin Zhu, Yixuan Sun, Yulun Wu, Bin Xiao, Gaolei Li, Jianhua Li, and Kai Zhou",
        "link": "http://arxiv.org/abs/2503.22998v1",
        "abstract": "Despite advancements in Graph Neural Networks (GNNs), adaptive attacks\ncontinue to challenge their robustness. Certified robustness based on\nrandomized smoothing has emerged as a promising solution, offering provable\nguarantees that a model's predictions remain stable under adversarial\nperturbations within a specified range. However, existing methods face a\ncritical trade-off between accuracy and robustness, as achieving stronger\nrobustness requires introducing greater noise into the input graph. This\nexcessive randomization degrades data quality and disrupts prediction\nconsistency, limiting the practical deployment of certifiably robust GNNs in\nreal-world scenarios where both accuracy and robustness are essential. To\naddress this challenge, we propose \\textbf{AuditVotes}, the first framework to\nachieve both high clean accuracy and certifiably robust accuracy for GNNs. It\nintegrates randomized smoothing with two key components,\n\\underline{au}gmentation and con\\underline{dit}ional smoothing, aiming to\nimprove data quality and prediction consistency. The augmentation, acting as a\npre-processing step, de-noises the randomized graph, significantly improving\ndata quality and clean accuracy. The conditional smoothing, serving as a\npost-processing step, employs a filtering function to selectively count votes,\nthereby filtering low-quality predictions and improving voting consistency.\nExtensive experimental results demonstrate that AuditVotes significantly\nenhances clean accuracy, certified robustness, and empirical robustness while\nmaintaining high computational efficiency. Notably, compared to baseline\nrandomized smoothing, AuditVotes improves clean accuracy by $437.1\\%$ and\ncertified accuracy by $409.3\\%$ when the attacker can arbitrarily insert $20$\nedges on the Cora-ML datasets, representing a substantial step toward deploying\ncertifiably robust GNNs in real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Federated Learning Through Secure Cluster-Weighted Client Aggregation",
        "author": "Kanishka Ranaweera, Azadeh Ghari Neiat, Xiao Liu, Bipasha Kashyap, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2503.22971v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm in machine\nlearning, enabling collaborative model training across decentralized devices\nwithout the need for raw data sharing. In FL, a global model is trained\niteratively on local datasets residing on individual devices, each contributing\nto the model's improvement. However, the heterogeneous nature of these local\ndatasets, stemming from diverse user behaviours, device capabilities, and data\ndistributions, poses a significant challenge. The inherent heterogeneity in\nfederated learning gives rise to various issues, including model performance\ndiscrepancies, convergence challenges, and potential privacy concerns. As the\nglobal model progresses through rounds of training, the disparities in local\ndata quality and quantity can impede the overall effectiveness of federated\nlearning systems. Moreover, maintaining fairness and privacy across diverse\nuser groups becomes a paramount concern. To address this issue, this paper\nintroduces a novel FL framework, ClusterGuardFL, that employs dissimilarity\nscores, k-means clustering, and reconciliation confidence scores to dynamically\nassign weights to client updates. The dissimilarity scores between global and\nlocal models guide the formation of clusters, with cluster size influencing the\nweight allocation. Within each cluster, a reconciliation confidence score is\ncalculated for individual data points, and a softmax layer generates customized\nweights for clients. These weights are utilized in the aggregation process,\nenhancing the model's robustness and privacy. Experimental results demonstrate\nthe efficacy of the proposed approach in achieving improved model performance\nin diverse datasets."
    },
    {
        "date": "2025-03",
        "title": "Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes",
        "author": "Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu",
        "link": "http://arxiv.org/abs/2503.22935v1",
        "abstract": "In recent years, the rapid increase of security vulnerabilities has caused\nmajor challenges in managing them. One critical task in vulnerability\nmanagement is tracing the patches that fix a vulnerability. By accurately\ntracing the patching commits, security stakeholders can precisely identify\naffected software components, determine vulnerable and fixed versions, assess\nthe severity etc., which facilitates rapid deployment of mitigations. However,\nprevious work has shown that the patch information is often missing in\nvulnerability databases, including both the National Vulnerability Databases\n(NVD) and the GitHub Advisory Database, which increases the risk of delayed\nmitigation, incorrect vulnerability assessment, and potential exploits.\n  Although existing work has proposed several approaches for patch tracing,\nthey suffer from two major challenges: (1) the lack of scalability to the\nfull-repository level, and (2) the lack of study on how to model the semantic\nsimilarity between the CVE and the full diff code. Upon identifying this gap,\nwe propose SITPatchTracer, a scalable full-repo full-context retrieval system\nfor security vulnerability patch tracing. SITPatchTracer leverages\nElasticSearch, learning-to-rank, and a hierarchical embedding approach based on\nGritLM, a top-ranked LLM for text embedding with unlimited context length and\nfast inference speed. The evaluation of SITPatchTracer shows that it achieves a\nhigh recall on both evaluated datasets. SITPatchTracer's recall not only\noutperforms several existing works (PatchFinder, PatchScout, VFCFinder), but\nalso Voyage, the SOTA commercial code embedding API by 13\\% and 28\\%."
    },
    {
        "date": "2025-03",
        "title": "Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use",
        "author": "Nicholas Roth, Christopher Hidey, Lucas Spangher, William F. Arnold, Chang Ye, Nick Masiewicki, Jinoo Baek, Peter Grabowski, and Eugene Ie",
        "link": "http://arxiv.org/abs/2503.22931v2",
        "abstract": "In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems."
    },
    {
        "date": "2025-03",
        "title": "Nested Stochastic Gradient Descent for (Generalized) Sinkhorn Distance-Regularized Distributionally Robust Optimization",
        "author": "Yufeng Yang, Yi Zhou, and Zhaosong Lu",
        "link": "http://arxiv.org/abs/2503.22923v1",
        "abstract": "Distributionally robust optimization (DRO) is a powerful technique to train\nrobust models against data distribution shift. This paper aims to solve\nregularized nonconvex DRO problems, where the uncertainty set is modeled by a\nso-called generalized Sinkhorn distance and the loss function is nonconvex and\npossibly unbounded. Such a distance allows to model uncertainty of\ndistributions with different probability supports and divergence functions. For\nthis class of regularized DRO problems, we derive a novel dual formulation\ntaking the form of nested stochastic programming, where the dual variable\ndepends on the data sample. To solve the dual problem, we provide theoretical\nevidence to design a nested stochastic gradient descent (SGD) algorithm, which\nleverages stochastic approximation to estimate the nested stochastic gradients.\nWe study the convergence rate of nested SGD and establish polynomial iteration\nand sample complexities that are independent of the data size and parameter\ndimension, indicating its potential for solving large-scale DRO problems. We\nconduct numerical experiments to demonstrate the efficiency and robustness of\nthe proposed algorithm."
    },
    {
        "date": "2025-03",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
        "author": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, and Diana Marculescu",
        "link": "http://arxiv.org/abs/2503.22879v2",
        "abstract": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."
    },
    {
        "date": "2025-03",
        "title": "RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation",
        "author": "Feng Lin, Dong Jae Kim, Zhenhao Li, Jinqiu Yang, Tse-Hsun, and Chen",
        "link": "http://arxiv.org/abs/2503.22851v2",
        "abstract": "When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR."
    },
    {
        "date": "2025-03",
        "title": "Tropical Bisectors and Carlini-Wagner Attacks",
        "author": "Gillian Grindstaff, Julia Lindberg, Daniela Schkoda, Miruna-Stefana Sorea, and Ruriko Yoshida",
        "link": "http://arxiv.org/abs/2503.22653v1",
        "abstract": "Pasque et al. showed that using a tropical symmetric metric as an activation\nfunction in the last layer can improve the robustness of convolutional neural\nnetworks (CNNs) against state-of-the-art attacks, including the Carlini-Wagner\nattack. This improvement occurs when the attacks are not specifically adapted\nto the non-differentiability of the tropical layer. Moreover, they showed that\nthe decision boundary of a tropical CNN is defined by tropical bisectors. In\nthis paper, we explore the combinatorics of tropical bisectors and analyze how\nthe tropical embedding layer enhances robustness against Carlini-Wagner\nattacks. We prove an upper bound on the number of linear segments the decision\nboundary of a tropical CNN can have. We then propose a refined version of the\nCarlini-Wagner attack, specifically tailored for the tropical architecture.\nComputational experiments with MNIST and LeNet5 showcase our attacks improved\nsuccess rate."
    },
    {
        "date": "2025-03",
        "title": "Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines",
        "author": "Jayaprakashreddy Cheenepalli, John D. Hastings, Khandaker Mamun Ahmed, and Chad Fenner",
        "link": "http://arxiv.org/abs/2503.22612v1",
        "abstract": "This study evaluates the adoption of DevSecOps among small and medium-sized\nenterprises (SMEs), identifying key challenges, best practices, and future\ntrends. Through a mixed methods approach backed by the Technology Acceptance\nModel (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data\nfrom 405 SME professionals, revealing that while 68% have implemented\nDevSecOps, adoption is hindered by technical complexity (41%), resource\nconstraints (35%), and cultural resistance (38%). Despite strong leadership\nprioritization of security (73%), automation gaps persist, with only 12% of\norganizations conducting security scans per commit.\n  Our findings highlight a growing integration of security tools, particularly\nAPI security (63%) and software composition analysis (62%), although container\nsecurity adoption remains low (34%). Looking ahead, SMEs anticipate artificial\nintelligence and machine learning to significantly influence DevSecOps,\nunderscoring the need for proactive adoption of AI-driven security\nenhancements. Based on our findings, this research proposes strategic best\npractices to enhance CI/CD pipeline security including automation,\nleadership-driven security culture, and cross-team collaboration."
    },
    {
        "date": "2025-03",
        "title": "Robust Offline Imitation Learning Through State-level Trajectory Stitching",
        "author": "Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, and Jie Chen",
        "link": "http://arxiv.org/abs/2503.22524v1",
        "abstract": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance."
    },
    {
        "date": "2025-03",
        "title": "Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets",
        "author": "Adri\u00e1n Detavernier, and Jasper De Bock",
        "link": "http://arxiv.org/abs/2503.22418v1",
        "abstract": "Based on existing ideas in the field of imprecise probabilities, we present a\nnew approach for assessing the reliability of the individual predictions of a\ngenerative probabilistic classifier. We call this approach robustness\nquantification, compare it to uncertainty quantification, and demonstrate that\nit continues to work well even for classifiers that are learned from small\ntraining sets that are sampled from a shifted distribution."
    },
    {
        "date": "2025-03",
        "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision",
        "author": "Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, and Xi Zhang an Hongliang Ren",
        "link": "http://arxiv.org/abs/2503.22394v1",
        "abstract": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
        "author": "Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2503.22232v2",
        "abstract": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
    },
    {
        "date": "2025-03",
        "title": "Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces",
        "author": "Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, and Sunghoon Im",
        "link": "http://arxiv.org/abs/2503.22209v1",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces."
    },
    {
        "date": "2025-03",
        "title": "Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models",
        "author": "YangTian Yan, and Jinyu Tian",
        "link": "http://arxiv.org/abs/2503.22205v1",
        "abstract": "Deep neural networks (DNNs) are susceptible to Universal Adversarial\nPerturbations (UAPs), which are instance agnostic perturbations that can\ndeceive a target model across a wide range of samples. Unlike instance-specific\nadversarial examples, UAPs present a greater challenge as they must generalize\nacross different samples and models. Generating UAPs typically requires access\nto numerous examples, which is a strong assumption in real-world tasks. In this\npaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by\nexploiting the intrinsic vulnerabilities of deep models. We analyze a series of\npopular deep models composed of linear and nonlinear layers with a Lipschitz\nconstant of 1, revealing that the vulnerability of these models is\npredominantly influenced by their linear components. Based on this observation,\nwe leverage the ill-conditioned nature of the linear components by aligning the\nUAP with the right singular vectors corresponding to the maximum singular value\nof each linear layer. Remarkably, our method achieves highly competitive\nperformance in attacking popular image classification deep models without using\nany image samples. We also evaluate the black-box attack performance of our\nmethod, showing that it matches the state-of-the-art baseline for data-free\nmethods on models that conform to our theoretical framework. Beyond the\ndata-free assumption, IntriUAP also operates under a weaker assumption, where\nthe adversary only can access a few of the victim model's layers. Experiments\ndemonstrate that the attack success rate decreases by only 4% when the\nadversary has access to just 50% of the linear layers in the victim model."
    },
    {
        "date": "2025-03",
        "title": "T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning",
        "author": "Seong-Hyeon Hwang, Minsu Kim, and Steven Euijong Whang",
        "link": "http://arxiv.org/abs/2503.22163v1",
        "abstract": "We study model confidence calibration in class-incremental learning, where\nmodels learn from sequential tasks with different class sets. While existing\nworks primarily focus on accuracy, maintaining calibrated confidence has been\nlargely overlooked. Unfortunately, most post-hoc calibration techniques are not\ndesigned to work with the limited memories of old-task data typical in\nclass-incremental learning, as retaining a sufficient validation set would be\nimpractical. Thus, we propose T-CIL, a novel temperature scaling approach for\nclass-incremental learning without a validation set for old tasks, that\nleverages adversarially perturbed exemplars from memory. Directly using\nexemplars is inadequate for temperature optimization, since they are already\nused for training. The key idea of T-CIL is to perturb exemplars more strongly\nfor old tasks than for the new task by adjusting the perturbation direction\nbased on feature distance, with the single magnitude determined using the\nnew-task validation set. This strategy makes the perturbation magnitude\ncomputed from the new task also applicable to old tasks, leveraging the\ntendency that the accuracy of old tasks is lower than that of the new task. We\nempirically show that T-CIL significantly outperforms various baselines in\nterms of calibration on real datasets and can be integrated with existing\nclass-incremental learning techniques with minimal impact on accuracy."
    },
    {
        "date": "2025-03",
        "title": "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
        "author": "Dinil Mon Divakaran",
        "link": "http://arxiv.org/abs/2503.22161v1",
        "abstract": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead."
    },
    {
        "date": "2025-03",
        "title": "SoK: Security Analysis of Blockchain-based Cryptocurrency",
        "author": "Zekai Liu, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2503.22156v1",
        "abstract": "Cryptocurrency is a novel exploration of a form of currency that proposes a\ndecentralized electronic payment scheme based on blockchain technology and\ncryptographic theory. While cryptocurrency has the security characteristics of\nbeing distributed and tamper-proof, increasing market demand has led to a rise\nin malicious transactions and attacks, thereby exposing cryptocurrency to\nvulnerabilities, privacy issues, and security threats. Particularly concerning\nare the emerging types of attacks and threats, which have made securing\ncryptocurrency increasingly urgent. Therefore, this paper classifies existing\ncryptocurrency security threats and attacks into five fundamental categories\nbased on the blockchain infrastructure and analyzes in detail the vulnerability\nprinciples exploited by each type of threat and attack. Additionally, the paper\nexamines the attackers' logic and methods and successfully reproduces the\nvulnerabilities. Furthermore, the author summarizes the existing detection and\ndefense solutions and evaluates them, all of which provide important references\nfor ensuring the security of cryptocurrency. Finally, the paper discusses the\nfuture development trends of cryptocurrency, as well as the public challenges\nit may face."
    },
    {
        "date": "2025-03",
        "title": "Non-control-Data Attacks and Defenses: A review",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.22765v1",
        "abstract": "In recent years, non-control-data attacks have be come a research hotspot in\nthe field of network security, driven\n  by the increasing number of defense methods against control-flow\n  hijacking attacks. These attacks exploit memory vulnerabilities\n  to modify non-control data within a program, thereby altering its\n  behavior without compromising control-flow integrity. Research\n  has shown that non-control-data attacks can be just as damaging\n  as control-flow hijacking attacks and are even Turing complete,\n  making them a serious security threat. However, despite being\n  discovered long ago, the threat of non-control-data attacks has\n  not been adequately addressed. In this review, we first classify\n  non-control-data attacks into two categories based on their\n  evolution: security-sensitive function attacks and data-oriented\n  programming (DOP) attacks. Subsequently, based on the non control-data attack\nmodel, we categorize existing defense methods\n  into three main strategies: memory safety, data confidentiality,\n  and data integrity protection. We then analyze recent defense\n  techniques specifically designed for DOP attacks. Finally, we\n  identify the key challenges hindering the widespread adoption\n  of defenses against non-control-data attacks and explore future\n  research directions in this field."
    },
    {
        "date": "2025-03",
        "title": "Information Theoretic One-Time Programs from Geometrically Local $\\text{QNC}_0$ Adversaries",
        "author": "Lev Stambler",
        "link": "http://arxiv.org/abs/2503.22016v2",
        "abstract": "We show how to construct simulation secure one-time memories, and thus\none-time programs, without computational assumptions in the presence of\nconstraints on quantum hardware. Specifically, we build one-time memories from\nrandom linear codes and quantum random access codes (QRACs) when constrained to\nnon-adaptive, constant depth, and $D$-dimensional geometrically-local quantum\ncircuit for some constant $D$. We place no restrictions on the adversary's\nclassical computational power, number of qubits it can use, or the coherence\ntime of its qubits. Notably, our construction can still be secure even in the\npresence of fault tolerant quantum computation as long as the input qubits are\nencoded in a non-fault tolerant manner (e.g. encoded as high energy states in\nnon-ideal hardware). Unfortunately though, our construction requires decoding\nrandom linear codes and thus does not run in polynomial time. We leave open the\nquestion of whether one can construct a polynomial time information\ntheoretically secure one-time memory from geometrically local quantum circuits.\n  Of potentially independent interest, we develop a progress bound for\ninformation leakage via collision entropy (Renyi entropy of order $2$) along\nwith a few key technical lemmas for a \"mutual information\" for collision\nentropies. We also develop new bounds on how much information a specific $2\n\\mapsto 1$ QRAC can leak about its input, which may be of independent interest\nas well."
    },
    {
        "date": "2025-03",
        "title": "SandboxEval: Towards Securing Test Environment for Untrusted Code",
        "author": "Rafiqul Rabin, Jesse Hostetler, Sean McGregor, Brett Weir, and Nick Judd",
        "link": "http://arxiv.org/abs/2504.00018v1",
        "abstract": "While large language models (LLMs) are powerful assistants in programming\ntasks, they may also produce malicious code. Testing LLM-generated code\ntherefore poses significant risks to assessment infrastructure tasked with\nexecuting untrusted code. To address these risks, this work focuses on\nevaluating the security and confidentiality properties of test environments,\nreducing the risk that LLM-generated code may compromise the assessment\ninfrastructure. We introduce SandboxEval, a test suite featuring manually\ncrafted test cases that simulate real-world safety scenarios for LLM assessment\nenvironments in the context of untrusted code execution. The suite evaluates\nvulnerabilities to sensitive information exposure, filesystem manipulation,\nexternal communication, and other potentially dangerous operations in the\ncourse of assessment activity. We demonstrate the utility of SandboxEval by\ndeploying it on an open-source implementation of Dyff, an established AI\nassessment framework used to evaluate the safety of LLMs at scale. We show,\nfirst, that the test suite accurately describes limitations placed on an LLM\noperating under instructions to generate malicious code. Second, we show that\nthe test results provide valuable insights for developers seeking to harden\nassessment infrastructure and identify risks associated with LLM execution\nactivities."
    },
    {
        "date": "2025-03",
        "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment",
        "author": "Hamed Babaei Giglou, Jennifer D'Souza, Oliver Karras, and S\u00f6ren Auer",
        "link": "http://arxiv.org/abs/2503.21902v1",
        "abstract": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Poster Abstract: Time Attacks using Kernel Vulnerabilities",
        "author": "Muhammad Abdullah Soomro, Adeel Nasrullah, and Fatima Muhammad Anwar",
        "link": "http://arxiv.org/abs/2503.21891v1",
        "abstract": "Timekeeping is a fundamental component of modern computing; however, the\nsecurity of system time remains an overlooked attack surface, leaving critical\nsystems vulnerable to manipulation."
    },
    {
        "date": "2025-03",
        "title": "OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation",
        "author": "Mallika Garg, Debashis Ghosh, and Pyari Mohan Pradhan",
        "link": "http://arxiv.org/abs/2503.21723v1",
        "abstract": "Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets."
    },
    {
        "date": "2025-03",
        "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
        "author": "Jiahe Qian, Yaoyu Fang, Jinkui Hao, and Bo Zhou",
        "link": "http://arxiv.org/abs/2503.21695v1",
        "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches."
    },
    {
        "date": "2025-03",
        "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
        "author": "Satvik Verma, Qun Wang, and E. Wes Bethel",
        "link": "http://arxiv.org/abs/2503.21674v1",
        "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity."
    },
    {
        "date": "2025-03",
        "title": "Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems",
        "author": "Huacheng Li, Jingyong Su, and Kai Wang",
        "link": "http://arxiv.org/abs/2503.21496v1",
        "abstract": "The rapid development of network technologies and industrial intelligence has\naugmented the connectivity and intelligence within the automotive industry.\nNotably, in the Internet of Vehicles (IoV), the Controller Area Network (CAN),\nwhich is crucial for the communication of electronic control units but lacks\ninbuilt security measures, has become extremely vulnerable to severe\ncybersecurity threats. Meanwhile, the efficacy of Intrusion Detection Systems\n(IDS) is hampered by the scarcity of sufficient attack data for robust model\ntraining. To overcome this limitation, we introduce a novel methodology\nleveraging the Restricted Boltzmann Machine (RBM) to generate synthetic CAN\nattack data, thereby producing training datasets with a more balanced sample\ndistribution. Specifically, we design a CAN Data Processing Module for\ntransforming raw CAN data into an RBM-trainable format, and a Negative Sample\nGeneration Module to generate data reflecting the distribution of CAN data\nframes denoting network intrusions. Experimental results show the generated\ndata significantly improves IDS performance, with CANet accuracy rising from\n0.6477 to 0.9725 and EfficientNet from 0.1067 to 0.1555. Code is available at\nhttps://github.com/wangkai-tech23/CANDataSynthetic."
    },
    {
        "date": "2025-03",
        "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time",
        "author": "Zhaojun Nan, Yunchu Han, Sheng Zhou, and Zhisheng Niu",
        "link": "http://arxiv.org/abs/2503.21476v1",
        "abstract": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices."
    },
    {
        "date": "2025-03",
        "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
        "author": "Ryan Marinelli, Josef Pichlmeier, and Tamas Bisztray",
        "link": "http://arxiv.org/abs/2503.21464v1",
        "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
    },
    {
        "date": "2025-03",
        "title": "AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model",
        "author": "Sen Zhang, Qingqing Ye, Haibo Hu, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2503.21426v1",
        "abstract": "The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks."
    },
    {
        "date": "2025-03",
        "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
        "author": "Cheng Wang, Yiwei Wang, Yujun Cai, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2503.21315v1",
        "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets."
    },
    {
        "date": "2025-03",
        "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
        "author": "Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2503.21305v1",
        "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings."
    },
    {
        "date": "2025-03",
        "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
        "author": "Yongxu Wang, Weiyun Yi, Xinhao Kong, and Wanting Li",
        "link": "http://arxiv.org/abs/2503.21257v1",
        "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub."
    },
    {
        "date": "2025-03",
        "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing",
        "author": "Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.21236v1",
        "abstract": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
        "author": "Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, and Seong Tae Kim",
        "link": "http://arxiv.org/abs/2503.21164v1",
        "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."
    },
    {
        "date": "2025-03",
        "title": "How to Secure Existing C and C++ Software without Memory Safety",
        "author": "\u00dalfar Erlingsson",
        "link": "http://arxiv.org/abs/2503.21145v1",
        "abstract": "The most important security benefit of software memory safety is easy to\nstate: for C and C++ software, attackers can exploit most bugs and\nvulnerabilities to gain full, unfettered control of software behavior, whereas\nthis is not true for most bugs in memory-safe software.\n  Fortunately, this security benefit -- most bugs don't give attackers full\ncontrol -- can be had for unmodified C/C++ software, without per-application\neffort.\n  This doesn't require trying to establish memory safety; instead, it is\nsufficient to eliminate most of the combinatorial ways in which software with\ncorrupted memory can execute. To eliminate these interleavings, there already\nexist practical compiler and runtime mechanisms that incur little overhead and\nneed no special hardware or platform support.\n  Each of the mechanisms described here is already in production use, at scale,\non one or more platforms. By supporting their combined use in development\ntoolchains, the security of all C and C++ software against remote code\nexecution attacks can be rapidly, and dramatically, improved."
    },
    {
        "date": "2025-03",
        "title": "Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid",
        "author": "Junfei Wang, and Pirathayini Srikantha",
        "link": "http://arxiv.org/abs/2503.20976v1",
        "abstract": "Real-time price signals and power generation levels (disaggregated or\naggregated) are commonly made available to the public by Independent System\nOperators (ISOs) to promote efficiency and transparency. However, they may\ninadvertently reveal crucial private information about the power grid, such as\nthe cost functions of generators. Adversaries can exploit these vulnerabilities\nfor strategic bidding, potentially leading to financial losses for power market\nparticipants and consumers. In this paper, we prove the existence of a\nclosed-form solution for recovering coefficients in cost functions when LMPs\nand disaggregated power generation data are available. Additionally, we\nestablish the convergence conditions for inference the quadratic coefficients\nof cost functions when LMPs and aggregated generation data are given. Our\ntheoretical analysis provides the conditions under which the algorithm is\nguaranteed to converge, and our experiments demonstrate the efficacy of this\nmethod on IEEE benchmark systems, including 14-bus and 30-bus and 118-bus\nsystems."
    },
    {
        "date": "2025-03",
        "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
        "author": "Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, and Lydia Y. Chen",
        "link": "http://arxiv.org/abs/2503.20952v1",
        "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse"
    },
    {
        "date": "2025-03",
        "title": "Prototype Guided Backdoor Defense",
        "author": "Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, and Narayanan P J",
        "link": "http://arxiv.org/abs/2503.20925v1",
        "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}."
    },
    {
        "date": "2025-03",
        "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
        "author": "Usama Zafar, Andr\u00e9 Teixeira, and Salman Toor",
        "link": "http://arxiv.org/abs/2503.20884v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems."
    },
    {
        "date": "2025-03",
        "title": "DR-PETS: Learning-Based Control With Planning in Adversarial Environments",
        "author": "Hozefa Jesawada, Antonio Acernese, Giovanni Russo, and Carmen Del Vecchio",
        "link": "http://arxiv.org/abs/2503.20660v2",
        "abstract": "Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates."
    },
    {
        "date": "2025-03",
        "title": "Robust Flower Cluster Matching Using The Unscented Transform",
        "author": "Andy Chu, Rashik Shrestha, Yu Gu, and Jason N. Gross",
        "link": "http://arxiv.org/abs/2503.20631v1",
        "abstract": "Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments."
    },
    {
        "date": "2025-03",
        "title": "$\u03b2$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
        "author": "Haci Ismail Aslan, Philipp Wiesner, Ping Xiong, and Odej Kao",
        "link": "http://arxiv.org/abs/2503.20630v1",
        "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance."
    },
    {
        "date": "2025-03",
        "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, and Yue Gao",
        "link": "http://arxiv.org/abs/2503.20844v1",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms."
    },
    {
        "date": "2025-03",
        "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.20613v1",
        "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks."
    },
    {
        "date": "2025-03",
        "title": "Feature Statistics with Uncertainty Help Adversarial Robustness",
        "author": "Ran Wang, Xinlei Zhou, Rihao Li, Meng Hu, Wenhui Wu, and Yuheng Jia",
        "link": "http://arxiv.org/abs/2503.20583v1",
        "abstract": "Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN."
    },
    {
        "date": "2025-03",
        "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling",
        "author": "Vinzenz Uhr, Ivan Diaz, Christian Rummel, and Richard McKinley",
        "link": "http://arxiv.org/abs/2503.20571v1",
        "abstract": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs)."
    },
    {
        "date": "2025-03",
        "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks",
        "author": "Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, and Xian Wei",
        "link": "http://arxiv.org/abs/2503.20454v1",
        "abstract": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates."
    },
    {
        "date": "2025-03",
        "title": "Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive Energy Forecasting with Missing Data",
        "author": "Akylas Stratigakos, and Panagiotis Andrianesis",
        "link": "http://arxiv.org/abs/2503.20410v1",
        "abstract": "Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data."
    },
    {
        "date": "2025-03",
        "title": "I'm Sorry Dave: How the old world of personnel security can inform the new world of AI insider risk",
        "author": "Paul Martin, and Sarah Mercer",
        "link": "http://arxiv.org/abs/2504.00012v2",
        "abstract": "Organisations are rapidly adopting artificial intelligence (AI) tools to\nperform tasks previously undertaken by people. The potential benefits are\nenormous. Separately, some organisations deploy personnel security measures to\nmitigate the security risks arising from trusted human insiders. Unfortunately,\nthere is no meaningful interplay between the rapidly evolving domain of AI and\nthe traditional world of personnel security. This is a problem. The complex\nrisks from human insiders are hard enough to understand and manage, despite\nmany decades of effort. The emerging security risks from AI insiders are even\nmore opaque. Both sides need all the help they can get. Some of the concepts\nand approaches that have proved useful in dealing with human insiders are also\napplicable to the emerging risks from AI insiders."
    },
    {
        "date": "2025-03",
        "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
        "author": "Francesco Micheli, Efe C. Balta, Anastasios Tsiamis, and John Lygeros",
        "link": "http://arxiv.org/abs/2503.20341v1",
        "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method."
    },
    {
        "date": "2025-03",
        "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach",
        "author": "Jiahao Qin, Feng Liu, and Lu Zong",
        "link": "http://arxiv.org/abs/2503.22729v1",
        "abstract": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation."
    },
    {
        "date": "2025-03",
        "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
        "author": "Tao Wu, and Tie Luo",
        "link": "http://arxiv.org/abs/2503.20310v1",
        "abstract": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures."
    },
    {
        "date": "2025-03",
        "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
        "author": "Hongye Cao, Fan Feng, Jing Huo, Shangdong Yang, Meng Fang, Tianpei Yang, and Yang Gao",
        "link": "http://arxiv.org/abs/2503.20285v1",
        "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency."
    },
    {
        "date": "2025-03",
        "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
        "author": "Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2503.20257v1",
        "abstract": "As Machine Learning (ML) evolves, the complexity and sophistication of\nsecurity threats against this paradigm continue to grow as well, threatening\ndata privacy and model integrity. In response, Machine Unlearning (MU) is a\nrecent technology that aims to remove the influence of specific data from a\ntrained model, enabling compliance with privacy regulations and user requests.\nThis can be done for privacy compliance (e.g., GDPR's right to be forgotten) or\nmodel refinement. However, the intersection between classical threats in ML and\nMU remains largely unexplored. In this Systematization of Knowledge (SoK), we\nprovide a structured analysis of security threats in ML and their implications\nfor MU. We analyze four major attack classes, namely, Backdoor Attacks,\nMembership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks,\nwe investigate their impact on MU and propose a novel classification based on\nhow they are usually used in this context. Finally, we identify open\nchallenges, including ethical considerations, and explore promising future\nresearch directions, paving the way for future research in secure and\nprivacy-preserving Machine Unlearning."
    },
    {
        "date": "2025-03",
        "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
        "author": "Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, and Robby T. Tan",
        "link": "http://arxiv.org/abs/2503.20211v1",
        "abstract": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness of Kernel Ridge Regression Using the Cauchy Loss Function",
        "author": "Hongwei Wen, Annika Betken, and Wouter Koolen",
        "link": "http://arxiv.org/abs/2503.20120v1",
        "abstract": "Robust regression aims to develop methods for estimating an unknown\nregression function in the presence of outliers, heavy-tailed distributions, or\ncontaminated data, which can severely impact performance. Most existing\ntheoretical results in robust regression assume that the noise has a finite\nabsolute mean, an assumption violated by certain distributions, such as Cauchy\nand some Pareto noise. In this paper, we introduce a generalized Cauchy noise\nframework that accommodates all noise distributions with finite moments of any\norder, even when the absolute mean is infinite. Within this framework, we study\nthe \\textit{kernel Cauchy ridge regressor} (\\textit{KCRR}), which minimizes a\nregularized empirical Cauchy risk to achieve robustness. To derive the\n$L_2$-risk bound for KCRR, we establish a connection between the excess Cauchy\nrisk and $L_2$-risk for sufficiently large scale parameters of the Cauchy loss,\nwhich reveals that these two risks are equivalent. Furthermore, under the\nassumption that the regression function satisfies H\\\"older smoothness, we\nderive excess Cauchy risk bounds for KCRR, showing improved performance as the\nscale parameter decreases. By considering the twofold effect of the scale\nparameter on the excess Cauchy risk and its equivalence with the $L_2$-risk, we\nestablish the almost minimax-optimal convergence rate for KCRR in terms of\n$L_2$-risk, highlighting the robustness of the Cauchy loss in handling various\ntypes of noise. Finally, we validate the effectiveness of KCRR through\nexperiments on both synthetic and real-world datasets under diverse noise\ncorruption scenarios."
    },
    {
        "date": "2025-03",
        "title": "ARGO-SLSA: Software Supply Chain Security in Argo Workflows",
        "author": "Mohomed Thariq, and Indrajith Ekanayake",
        "link": "http://arxiv.org/abs/2503.20079v1",
        "abstract": "Distributed systems widely adopt microservice architecture to handle growing\ncomplexity and scale. This approach breaks applications into independent,\nloosely coupled services. Kubernetes has become the de facto standard for\nmanaging microservices, and automating complex, multi-step workflows is a\ncommon requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine\nfor managing these workflows in an automated fashion. These workflows generate\nartifacts such as executables, logs, container images, and packages, which\noften require proper management through software supply chain security.\nHowever, Argo Workflows does not include built-in functionality for frameworks\nlike Supply-chain Levels for Software Artifacts (SLSA), which is essential for\nensuring artifact integrity, traceability, and security. This gap compels\npractitioners to rely on external tools to meet software supply chain security\nstandards. In response, this paper proposes a Kubernetes-native controller\nbuilt on top of existing open-source Argo Workflows to enhance artifact\nsecurity. By generating cryptographic signing and provenance attestations, the\ncontroller enables Argo Workflows to comply with SLSA standards. We demonstrate\nthat implementations can provide such cryptographic signing and provenance\nattestations for artifacts produced by the controller, allowing software\nartifacts built with Argo Workflows to adhere to SLSA requirements. The\nproposed validation model evaluates the proof of concept of the controller,\nincluding its ability to reconcile workflows, detect pods associated with\nworkflow nodes, operate without disrupting existing operations, enforce\nintegrity, and monitor software artifacts."
    }
]