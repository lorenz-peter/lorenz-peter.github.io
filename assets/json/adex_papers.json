[
    {
        "date": "2025-03",
        "title": "Value of Information-based Deceptive Path Planning Under Adversarial Interventions",
        "author": "Wesley A. Suttle, Jesse Milzman, Mustafa O. Karabag, Brian M. Sadler, and Ufuk Topcu",
        "link": "http://arxiv.org/abs/2503.24284v1",
        "abstract": "Existing methods for deceptive path planning (DPP) address the problem of\ndesigning paths that conceal their true goal from a passive, external observer.\nSuch methods do not apply to problems where the observer has the ability to\nperform adversarial interventions to impede the path planning agent. In this\npaper, we propose a novel Markov decision process (MDP)-based model for the DPP\nproblem under adversarial interventions and develop new value of information\n(VoI) objectives to guide the design of DPP policies. Using the VoI objectives\nwe propose, path planning agents deceive the adversarial observer into choosing\nsuboptimal interventions by selecting trajectories that are of low\ninformational value to the observer. Leveraging connections to the linear\nprogramming theory for MDPs, we derive computationally efficient solution\nmethods for synthesizing policies for performing DPP under adversarial\ninterventions. In our experiments, we illustrate the effectiveness of the\nproposed solution method in achieving deceptiveness under adversarial\ninterventions and demonstrate the superior performance of our approach to both\nexisting DPP methods and conservative path planning approaches on illustrative\ngridworld problems."
    },
    {
        "date": "2025-03",
        "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
        "author": "Shuoming Zhang, Jiacheng Zhao, Ruiyuan Xu, Xiaobing Feng, and Huimin Cui",
        "link": "http://arxiv.org/abs/2503.24191v1",
        "abstract": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed."
    },
    {
        "date": "2025-03",
        "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization",
        "author": "Yingrui Ji, Xi Xiao, Gaofei Chen, Hao Xu, Chenrui Ma, Lijing Zhu, Aokun Liang, and Jiansheng Chen",
        "link": "http://arxiv.org/abs/2503.24182v1",
        "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\nin cross-modal tasks such as zero-shot image classification and text-image\nretrieval by effectively aligning visual and textual representations. However,\nthe theoretical foundations underlying CLIP's strong generalization remain\nunclear. In this work, we address this gap by proposing the Cross-modal\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\noptimization. Under this view, the model maximizes shared cross-modal\ninformation while discarding modality-specific redundancies, thereby preserving\nessential semantic alignment across modalities. Building on this insight, we\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\nthat explicitly enforces these IB principles during training. CIBR introduces a\npenalty term to discourage modality-specific redundancy, thereby enhancing\nsemantic alignment between image and text features. We validate CIBR on\nextensive vision-language benchmarks, including zero-shot classification across\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\nThe results show consistent performance gains over standard CLIP. These\nfindings provide the first theoretical understanding of CLIP's generalization\nthrough the IB lens. They also demonstrate practical improvements, offering\nguidance for future cross-modal representation learning."
    },
    {
        "date": "2025-03",
        "title": "Pay More Attention to the Robustness of Prompt for Instruction Data Mining",
        "author": "Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, and Huaimin Wang",
        "link": "http://arxiv.org/abs/2503.24028v1",
        "abstract": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification",
        "author": "Chenqi Guo, Mengshuo Rong, Qianli Feng, Rongfan Feng, and Yinglong Ma",
        "link": "http://arxiv.org/abs/2503.24017v1",
        "abstract": "Crossmodal knowledge distillation (KD) aims to enhance a unimodal student\nusing a multimodal teacher model. In particular, when the teacher's modalities\ninclude the student's, additional complementary information can be exploited to\nimprove knowledge transfer. In supervised image classification, image datasets\ntypically include class labels that represent high-level concepts, suggesting a\nnatural avenue to incorporate textual cues for crossmodal KD. However, these\nlabels rarely capture the deeper semantic structures in real-world visuals and\ncan lead to label leakage if used directly as inputs, ultimately limiting KD\nperformance. To address these issues, we propose a multi-teacher crossmodal KD\nframework that integrates CLIP image embeddings with learnable WordNet-relaxed\ntext embeddings under a hierarchical loss. By avoiding direct use of exact\nclass names and instead using semantically richer WordNet expansions, we\nmitigate label leakage and introduce more diverse textual cues. Experiments\nshow that this strategy significantly boosts student performance, whereas noisy\nor overly precise text embeddings hinder distillation efficiency.\nInterpretability analyses confirm that WordNet-relaxed prompts encourage\nheavier reliance on visual features over textual shortcuts, while still\neffectively incorporating the newly introduced textual cues. Our method\nachieves state-of-the-art or second-best results on six public datasets,\ndemonstrating its effectiveness in advancing crossmodal KD."
    },
    {
        "date": "2025-03",
        "title": "Model Hemorrhage and the Robustness Limits of Large Language Models",
        "author": "Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2503.23924v1",
        "abstract": "Large language models (LLMs) demonstrate strong performance across natural\nlanguage processing tasks, yet undergo significant performance degradation when\nmodified for deployment through quantization, pruning, or decoding strategy\nadjustments. We define this phenomenon as model hemorrhage - performance\ndecline caused by parameter alterations and architectural changes. Through\nsystematic analysis of various LLM frameworks, we identify key vulnerability\npatterns: layer expansion frequently disrupts attention mechanisms, compression\ntechniques induce information loss cascades, and decoding adjustments amplify\nprediction divergences. Our investigation reveals transformer architectures\nexhibit inherent robustness thresholds that determine hemorrhage severity\nacross modification types. We propose three mitigation strategies:\ngradient-aware pruning preserves critical weight pathways, dynamic quantization\nscaling maintains activation integrity, and decoding calibration aligns\ngeneration trajectories with original model distributions. This work\nestablishes foundational metrics for evaluating model stability during\nadaptation, providing practical guidelines for maintaining performance while\nenabling efficient LLM deployment. Our findings advance understanding of neural\nnetwork resilience under architectural transformations, particularly for\nlarge-scale language models."
    },
    {
        "date": "2025-03",
        "title": "A Channel-Triggered Backdoor Attack on Wireless Semantic Image Reconstruction",
        "author": "Jialin Wan, Nan Cheng, and Jinglong Shen",
        "link": "http://arxiv.org/abs/2503.23866v1",
        "abstract": "Despite the transformative impact of deep learning (DL) on wireless\ncommunication systems through data-driven end-to-end (E2E) learning, the\nsecurity vulnerabilities of these systems have been largely overlooked. Unlike\nthe extensively studied image domain, limited research has explored the threat\nof backdoor attacks on the reconstruction of symbols in semantic communication\n(SemCom) systems. Previous work has investigated such backdoor attacks at the\ninput level, but these approaches are infeasible in applications with strict\ninput control. In this paper, we propose a novel attack paradigm, termed\nChannel-Triggered Backdoor Attack (CT-BA), where the backdoor trigger is a\nspecific wireless channel. This attack leverages fundamental physical layer\ncharacteristics, making it more covert and potentially more threatening\ncompared to previous input-level attacks. Specifically, we utilize channel gain\nwith different fading distributions or channel noise with different power\nspectral densities as potential triggers. This approach establishes\nunprecedented attack flexibility as the adversary can select backdoor triggers\nfrom both fading characteristics and noise variations in diverse channel\nenvironments. Moreover, during the testing phase, CT-BA enables automatic\ntrigger activation through natural channel variations without requiring active\nadversary participation. We evaluate the robustness of CT-BA on a ViT-based\nJoint Source-Channel Coding (JSCC) model across three datasets: MNIST,\nCIFAR-10, and ImageNet. Furthermore, we apply CT-BA to three typical E2E SemCom\nsystems: BDJSCC, ADJSCC, and JSCCOFDM. Experimental results demonstrate that\nour attack achieves near-perfect attack success rate (ASR) while maintaining\neffective stealth. Finally, we discuss potential defense mechanisms against\nsuch attacks."
    },
    {
        "date": "2025-03",
        "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
        "author": "Jingzheng Li, Xianglong Liu, Shikui Wei, Zhijun Chen, Bing Li, Qing Guo, Xianqi Yang, Yanjun Pu, and Jiakai Wang",
        "link": "http://arxiv.org/abs/2503.23708v1",
        "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment."
    },
    {
        "date": "2025-03",
        "title": "Security Analysis of Chain-FS service",
        "author": "Vanessa Teague, and Arash Mirzaei",
        "link": "http://arxiv.org/abs/2503.23627v1",
        "abstract": "We examine the security of a cloud storage service that makes very strong\nclaims about the ``trustless'' nature of its security. We find that, although\nstored files are end-to-end encrypted, the encryption method allows for\neffective dictionary attacks by a malicious server when passwords only just\nmeet the minimum length required. Furthermore, the file sharing function simply\nsends the decryption passwords to the server with no protection other than TLS."
    },
    {
        "date": "2025-03",
        "title": "Buffer is All You Need: Defending Federated Learning against Backdoor Attacks under Non-iids via Buffering",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23511v1",
        "abstract": "Federated Learning (FL) is a popular paradigm enabling clients to jointly\ntrain a global model without sharing raw data. However, FL is known to be\nvulnerable towards backdoor attacks due to its distributed nature. As\nparticipants, attackers can upload model updates that effectively compromise\nFL. What's worse, existing defenses are mostly designed under\nindependent-and-identically-distributed (iid) settings, hence neglecting the\nfundamental non-iid characteristic of FL. Here we propose FLBuff for tackling\nbackdoor attacks even under non-iids. The main challenge for such defenses is\nthat non-iids bring benign and malicious updates closer, hence harder to\nseparate. FLBuff is inspired by our insight that non-iids can be modeled as\nomni-directional expansion in representation space while backdoor attacks as\nuni-directional. This leads to the key design of FLBuff, i.e., a\nsupervised-contrastive-learning model extracting penultimate-layer\nrepresentations to create a large in-between buffer layer. Comprehensive\nevaluations demonstrate that FLBuff consistently outperforms state-of-the-art\ndefenses."
    },
    {
        "date": "2025-03",
        "title": "Codehacks: A Dataset of Adversarial Tests for Competitive Programming Problems Obtained from Codeforces",
        "author": "Max Hort, and Leon Moonen",
        "link": "http://arxiv.org/abs/2503.23466v1",
        "abstract": "Software is used in critical applications in our day-to-day life and it is\nimportant to ensure its correctness. One popular approach to assess correctness\nis to evaluate software on tests. If a test fails, it indicates a fault in the\nsoftware under test; if all tests pass correctly, one may assume that the\nsoftware is correct. However, the reliability of these results depends on the\ntest suite considered, and there is a risk of false negatives (i.e. software\nthat passes all available tests but contains bugs because some cases are not\ntested). Therefore, it is important to consider error-inducing test cases when\nevaluating software.\n  To support data-driven creation of such a test-suite, which is especially of\ninterest for testing software synthesized from large language models, we curate\na dataset (Codehacks) of programming problems together with corresponding\nerror-inducing test cases (i.e., \"hacks\"). This dataset is collected from the\nwild, in particular, from the Codeforces online judge platform. The dataset\ncomprises 288,617 hacks for 5,578 programming problems, each with a natural\nlanguage description, as well as the source code for 2,196 submitted solutions\nto these problems that can be broken with their corresponding hacks.\n  Keywords: competitive programming, language model, dataset"
    },
    {
        "date": "2025-03",
        "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
        "author": "Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, and Zhi Wang",
        "link": "http://arxiv.org/abs/2503.23388v1",
        "abstract": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
    },
    {
        "date": "2025-03",
        "title": "Two Heads Are Better than One: Model-Weight and Latent-Space Analysis for Federated Learning on Non-iid Data against Poisoning Attacks",
        "author": "Xingyu Lyu, Ning Wang, Yang Xiao, Shixiong Li, Tao Li, Danjue Chen, and Yimin Chen",
        "link": "http://arxiv.org/abs/2503.23288v1",
        "abstract": "Federated Learning is a popular paradigm that enables remote clients to\njointly train a global model without sharing their raw data. However, FL has\nbeen shown to be vulnerable towards model poisoning attacks due to its\ndistributed nature. Particularly, attackers acting as participants can upload\narbitrary model updates that effectively compromise the global model of FL.\nWhile extensive research has been focusing on fighting against these attacks,\nwe find that most of them assume data at remote clients are under iid while in\npractice they are inevitably non-iid. Our benchmark evaluations reveal that\nexisting defenses generally fail to live up to their reputation when applied to\nvarious non-iid scenarios. In this paper, we propose a novel approach,\nGeminiGuard, that aims to address such a significant gap. We design GeminiGuard\nto be lightweight, versatile, and unsupervised so that it aligns well with the\npractical requirements of deploying such defenses. The key challenge from\nnon-iids is that they make benign model updates look more similar to malicious\nones. GeminiGuard is mainly built on two fundamental observations: (1) existing\ndefenses based on either model-weight analysis or latent-space analysis face\nlimitations in covering different MPAs and non-iid scenarios, and (2)\nmodel-weight and latent-space analysis are sufficiently different yet\npotentially complementary methods as MPA defenses. We hence incorporate a novel\nmodel-weight analysis component as well as a custom latent-space analysis\ncomponent in GeminiGuard, aiming to further enhance its defense performance. We\nconduct extensive experiments to evaluate our defense across various settings,\ndemonstrating its effectiveness in countering multiple types of untargeted and\ntargeted MPAs, including adaptive ones. Our comprehensive evaluations show that\nGeminiGuard consistently outperforms SOTA defenses under various settings."
    },
    {
        "date": "2025-03",
        "title": "Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions",
        "author": "Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2503.23278v1",
        "abstract": "The Model Context Protocol (MCP) is a standardized interface designed to\nenable seamless interaction between AI models and external tools and resources,\nbreaking down data silos and facilitating interoperability across diverse\nsystems. This paper provides a comprehensive overview of MCP, focusing on its\ncore components, workflow, and the lifecycle of MCP servers, which consists of\nthree key phases: creation, operation, and update. We analyze the security and\nprivacy risks associated with each phase and propose strategies to mitigate\npotential threats. The paper also examines the current MCP landscape, including\nits adoption by industry leaders and various use cases, as well as the tools\nand platforms supporting its integration. We explore future directions for MCP,\nhighlighting the challenges and opportunities that will influence its adoption\nand evolution within the broader AI ecosystem. Finally, we offer\nrecommendations for MCP stakeholders to ensure its secure and sustainable\ndevelopment as the AI landscape continues to evolve."
    },
    {
        "date": "2025-03",
        "title": "Comprehensive Survey towards Security Authentication Methods for Satellite Communication Systems",
        "author": "Yunfei Meng, Changbo Ke, and Zhiqiu Huang",
        "link": "http://arxiv.org/abs/2503.23277v1",
        "abstract": "Satellite communication systems (SatCom) is a brand-new network that uses\nartificial Earth satellites as relay stations to provide communication services\nsuch as broadband Internet access to various users on land, sea, air and in\nspace. It features wide coverage, relatively high transmission rates and strong\nanti-interference capabilities. Security authentication is of crucial\nsignificance for the stable operation and widespread application of satellite\ncommunication systems. It can effectively prevent unauthorized access, ensuring\nthat only users and devices that pass security authentication can access the\nsatellite network. It also ensures the confidentiality, integrity, and\navailability of data during transmission and storage, preventing data from\nbeing stolen, tampered with, or damaged. By means of literature research and\ncomparative analysis, this paper carries out on a comprehensive survey towards\nthe security authentication methods used by SatCom. This paper first summarizes\nthe existing SatCom authentication methods as five categories, namely, those\nbased on cryptography, Blockchain, satellite orbital information, the AKA\nprotocol and physical hardware respectively. Subsequently, a comprehensive\ncomparative analysis is carried out on the above-mentioned five categories of\nsecurity authentication methods from four dimensions, i.e., security,\nimplementation difficulty and cost, applicable scenarios and real-time\nperformance, and the final comparison results are following obtained. Finally,\nprospects are made for several important future research directions of security\nauthentication methods for SatCom, laying a well foundation for further\ncarrying on the related research works."
    },
    {
        "date": "2025-03",
        "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition",
        "author": "Shihao Cheng, Jinlu Zhang, Yue Liu, and Zhigang Tu",
        "link": "http://arxiv.org/abs/2503.23266v1",
        "abstract": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments."
    },
    {
        "date": "2025-03",
        "title": "Mismatch-Robust Underwater Acoustic Localization Using A Differentiable Modular Forward Model",
        "author": "Dariush Kari, Yongjie Zhuang, and Andrew C. Singer",
        "link": "http://arxiv.org/abs/2503.23260v1",
        "abstract": "In this paper, we study the underwater acoustic localization in the presence\nof environmental mismatch. Especially, we exploit a pre-trained neural network\nfor the acoustic wave propagation in a gradient-based optimization framework to\nestimate the source location. To alleviate the effect of mismatch between the\ntraining data and the test data, we simultaneously optimize over the network\nweights at the inference time, and provide conditions under which this method\nis effective. Moreover, we introduce a physics-inspired modularity in the\nforward model that enables us to learn the path lengths of the multipath\nstructure in an end-to-end training manner without access to the specific path\nlabels. We investigate the validity of the assumptions in a simple yet\nillustrative environment model."
    },
    {
        "date": "2025-03",
        "title": "Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions",
        "author": "Shih-Han Chan",
        "link": "http://arxiv.org/abs/2503.23250v1",
        "abstract": "Security threats like prompt injection attacks pose significant risks to\napplications that integrate Large Language Models (LLMs), potentially leading\nto unauthorized actions such as API misuse. Unlike previous approaches that aim\nto detect these attacks on a best-effort basis, this paper introduces a novel\nmethod that appends an Encrypted Prompt to each user prompt, embedding current\npermissions. These permissions are verified before executing any actions (such\nas API calls) generated by the LLM. If the permissions are insufficient, the\nLLM's actions will not be executed, ensuring safety. This approach guarantees\nthat only actions within the scope of the current permissions from the LLM can\nproceed. In scenarios where adversarial prompts are introduced to mislead the\nLLM, this method ensures that any unauthorized actions from LLM wouldn't be\nexecuted by verifying permissions in Encrypted Prompt. Thus, threats like\nprompt injection attacks that trigger LLM to generate harmful actions can be\neffectively mitigated."
    },
    {
        "date": "2025-03",
        "title": "Agent-Based Modeling and Deep Neural Networks for Establishing Digital Twins of Secure Facilities under Sensing Restrictions",
        "author": "Chathika Gunaratne, Mason Stott, Debraj De, Gautam Malviya Thakur, and Chris Young",
        "link": "http://arxiv.org/abs/2503.23147v1",
        "abstract": "Digital twin technologies help practitioners simulate, monitor, and predict\nundesirable outcomes in-silico, while avoiding the cost and risks of conducting\nlive simulation exercises. Virtual reality (VR) based digital twin technologies\nare especially useful when monitoring human Patterns of Life (POL) in secure\nnuclear facilities, where live simulation exercises are too dangerous and\ncostly to ever perform. However, the high-security status of such facilities\nmay restrict modelers from deploying human activity sensors for data\ncollection. This problem was encountered when deploying MetaPOL, a digital twin\nsystem to prevent insider threat or sabotage of secure facilities, at a secure\nnuclear reactor facility at Oak Ridge National Laboratory (ORNL). This\nchallenge was addressed using an agent-based model (ABM), driven by anecdotal\nevidence of facility personnel POL, to generate synthetic movement\ntrajectories. These synthetic trajectories were then used to train deep neural\nnetwork surrogates for next location and stay duration prediction to drive NPCs\nin the VR environment. In this study, we evaluate the efficacy of this\ntechnique for establishing NPC movement within MetaPOL and the ability to\ndistinguish NPC movement during normal operations from that during a simulated\nemergency response. Our results demonstrate the success of using a multi-layer\nperceptron for next location prediction and mixture density network for stay\nduration prediction to predict the ABM generated trajectories. We also find\nthat NPC movement in the VR environment driven by the deep neural networks\nunder normal operations remain significantly different to that seen when\nsimulating responses to a simulated emergency scenario."
    },
    {
        "date": "2025-03",
        "title": "AuditVotes: A Framework Towards More Deployable Certified Robustness for Graph Neural Networks",
        "author": "Yuni Lai, Yulin Zhu, Yixuan Sun, Yulun Wu, Bin Xiao, Gaolei Li, Jianhua Li, and Kai Zhou",
        "link": "http://arxiv.org/abs/2503.22998v1",
        "abstract": "Despite advancements in Graph Neural Networks (GNNs), adaptive attacks\ncontinue to challenge their robustness. Certified robustness based on\nrandomized smoothing has emerged as a promising solution, offering provable\nguarantees that a model's predictions remain stable under adversarial\nperturbations within a specified range. However, existing methods face a\ncritical trade-off between accuracy and robustness, as achieving stronger\nrobustness requires introducing greater noise into the input graph. This\nexcessive randomization degrades data quality and disrupts prediction\nconsistency, limiting the practical deployment of certifiably robust GNNs in\nreal-world scenarios where both accuracy and robustness are essential. To\naddress this challenge, we propose \\textbf{AuditVotes}, the first framework to\nachieve both high clean accuracy and certifiably robust accuracy for GNNs. It\nintegrates randomized smoothing with two key components,\n\\underline{au}gmentation and con\\underline{dit}ional smoothing, aiming to\nimprove data quality and prediction consistency. The augmentation, acting as a\npre-processing step, de-noises the randomized graph, significantly improving\ndata quality and clean accuracy. The conditional smoothing, serving as a\npost-processing step, employs a filtering function to selectively count votes,\nthereby filtering low-quality predictions and improving voting consistency.\nExtensive experimental results demonstrate that AuditVotes significantly\nenhances clean accuracy, certified robustness, and empirical robustness while\nmaintaining high computational efficiency. Notably, compared to baseline\nrandomized smoothing, AuditVotes improves clean accuracy by $437.1\\%$ and\ncertified accuracy by $409.3\\%$ when the attacker can arbitrarily insert $20$\nedges on the Cora-ML datasets, representing a substantial step toward deploying\ncertifiably robust GNNs in real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Federated Learning Through Secure Cluster-Weighted Client Aggregation",
        "author": "Kanishka Ranaweera, Azadeh Ghari Neiat, Xiao Liu, Bipasha Kashyap, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2503.22971v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm in machine\nlearning, enabling collaborative model training across decentralized devices\nwithout the need for raw data sharing. In FL, a global model is trained\niteratively on local datasets residing on individual devices, each contributing\nto the model's improvement. However, the heterogeneous nature of these local\ndatasets, stemming from diverse user behaviours, device capabilities, and data\ndistributions, poses a significant challenge. The inherent heterogeneity in\nfederated learning gives rise to various issues, including model performance\ndiscrepancies, convergence challenges, and potential privacy concerns. As the\nglobal model progresses through rounds of training, the disparities in local\ndata quality and quantity can impede the overall effectiveness of federated\nlearning systems. Moreover, maintaining fairness and privacy across diverse\nuser groups becomes a paramount concern. To address this issue, this paper\nintroduces a novel FL framework, ClusterGuardFL, that employs dissimilarity\nscores, k-means clustering, and reconciliation confidence scores to dynamically\nassign weights to client updates. The dissimilarity scores between global and\nlocal models guide the formation of clusters, with cluster size influencing the\nweight allocation. Within each cluster, a reconciliation confidence score is\ncalculated for individual data points, and a softmax layer generates customized\nweights for clients. These weights are utilized in the aggregation process,\nenhancing the model's robustness and privacy. Experimental results demonstrate\nthe efficacy of the proposed approach in achieving improved model performance\nin diverse datasets."
    },
    {
        "date": "2025-03",
        "title": "Improving the Context Length and Efficiency of Code Retrieval for Tracing Security Vulnerability Fixes",
        "author": "Xueqing Liu, Jiangrui Zheng, Guanqun Yang, Siyan Wen, and Qiushi Liu",
        "link": "http://arxiv.org/abs/2503.22935v1",
        "abstract": "In recent years, the rapid increase of security vulnerabilities has caused\nmajor challenges in managing them. One critical task in vulnerability\nmanagement is tracing the patches that fix a vulnerability. By accurately\ntracing the patching commits, security stakeholders can precisely identify\naffected software components, determine vulnerable and fixed versions, assess\nthe severity etc., which facilitates rapid deployment of mitigations. However,\nprevious work has shown that the patch information is often missing in\nvulnerability databases, including both the National Vulnerability Databases\n(NVD) and the GitHub Advisory Database, which increases the risk of delayed\nmitigation, incorrect vulnerability assessment, and potential exploits.\n  Although existing work has proposed several approaches for patch tracing,\nthey suffer from two major challenges: (1) the lack of scalability to the\nfull-repository level, and (2) the lack of study on how to model the semantic\nsimilarity between the CVE and the full diff code. Upon identifying this gap,\nwe propose SITPatchTracer, a scalable full-repo full-context retrieval system\nfor security vulnerability patch tracing. SITPatchTracer leverages\nElasticSearch, learning-to-rank, and a hierarchical embedding approach based on\nGritLM, a top-ranked LLM for text embedding with unlimited context length and\nfast inference speed. The evaluation of SITPatchTracer shows that it achieves a\nhigh recall on both evaluated datasets. SITPatchTracer's recall not only\noutperforms several existing works (PatchFinder, PatchScout, VFCFinder), but\nalso Voyage, the SOTA commercial code embedding API by 13\\% and 28\\%."
    },
    {
        "date": "2025-03",
        "title": "Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use",
        "author": "Nicholas Roth, Christopher Hidey, Lucas Spangher, William F. Arnold, Chang Ye, Nick Masiewicki, Jinoo Baek, Peter Grabowski, and Eugene Ie",
        "link": "http://arxiv.org/abs/2503.22931v2",
        "abstract": "In this paper, we propose a novel factored agent architecture designed to\novercome the limitations of traditional single-agent systems in agentic AI. Our\napproach decomposes the agent into two specialized components: (1) a large\nlanguage model (LLM) that serves as a high level planner and in-context\nlearner, which may use dynamically available information in user prompts, (2) a\nsmaller language model which acts as a memorizer of tool format and output.\nThis decoupling addresses prevalent issues in monolithic designs, including\nmalformed, missing, and hallucinated API fields, as well as suboptimal planning\nin dynamic environments. Empirical evaluations demonstrate that our factored\narchitecture significantly improves planning accuracy and error resilience,\nwhile elucidating the inherent trade-off between in-context learning and static\nmemorization. These findings suggest that a factored approach is a promising\npathway for developing more robust and adaptable agentic AI systems."
    },
    {
        "date": "2025-03",
        "title": "Nested Stochastic Gradient Descent for (Generalized) Sinkhorn Distance-Regularized Distributionally Robust Optimization",
        "author": "Yufeng Yang, Yi Zhou, and Zhaosong Lu",
        "link": "http://arxiv.org/abs/2503.22923v1",
        "abstract": "Distributionally robust optimization (DRO) is a powerful technique to train\nrobust models against data distribution shift. This paper aims to solve\nregularized nonconvex DRO problems, where the uncertainty set is modeled by a\nso-called generalized Sinkhorn distance and the loss function is nonconvex and\npossibly unbounded. Such a distance allows to model uncertainty of\ndistributions with different probability supports and divergence functions. For\nthis class of regularized DRO problems, we derive a novel dual formulation\ntaking the form of nested stochastic programming, where the dual variable\ndepends on the data sample. To solve the dual problem, we provide theoretical\nevidence to design a nested stochastic gradient descent (SGD) algorithm, which\nleverages stochastic approximation to estimate the nested stochastic gradients.\nWe study the convergence rate of nested SGD and establish polynomial iteration\nand sample complexities that are independent of the data size and parameter\ndimension, indicating its potential for solving large-scale DRO problems. We\nconduct numerical experiments to demonstrate the efficiency and robustness of\nthe proposed algorithm."
    },
    {
        "date": "2025-03",
        "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models",
        "author": "Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, and Diana Marculescu",
        "link": "http://arxiv.org/abs/2503.22879v1",
        "abstract": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba."
    },
    {
        "date": "2025-03",
        "title": "RobuNFR: Evaluating the Robustness of Large Language Models on Non-Functional Requirements Aware Code Generation",
        "author": "Feng Lin, Dong Jae Kim, Zhenhao Li, Jinqiu Yang, Tse-Husn, and Chen",
        "link": "http://arxiv.org/abs/2503.22851v1",
        "abstract": "When using LLMs to address Non-Functional Requirements (NFRs), developers may\nbehave differently (e.g., expressing the same NFR in different words). Robust\nLLMs should output consistent results across these variations; however, this\naspect remains underexplored. We propose RobuNFR for evaluating the robustness\nof LLMs in NFR-aware code generation across four NFR dimensions: design,\nreadability, reliability, and performance, using three methodologies: prompt\nvariation, regression testing, and diverse workflows. Our experiments show that\nRobuNFR reveals robustness issues in the tested LLMs when considering NFRs in\ncode generation. Specifically, under prompt variation, including NFRs leads to\na decrease in Pass@1 by up to 39 percent and an increase in the standard\ndeviation from 0.48 to 2.48 compared to the baseline without NFRs (i.e.,\nFunction-Only). While incorporating NFRs generally improves overall NFR\nmetrics, it also results in higher prompt sensitivity. In regression settings,\nsome LLMs exhibit differences across versions, with improvements in one aspect\n(e.g., reduced code smells) often accompanied by regressions in another (e.g.,\ndecreased correctness), revealing inconsistencies that challenge their\nrobustness. When varying workflows, the tested LLMs show significantly\ndifferent NFR-aware code generation capabilities between two workflows: (1)\nintegrating NFRs and functional requirements into the initial prompt and (2)\nenhancing Function-Only-generated code with the same NFR."
    },
    {
        "date": "2025-03",
        "title": "Tropical Bisectors and Carlini-Wagner Attacks",
        "author": "Gillian Grindstaff, Julia Lindberg, Daniela Schkoda, Miruna-Stefana Sorea, and Ruriko Yoshida",
        "link": "http://arxiv.org/abs/2503.22653v1",
        "abstract": "Pasque et al. showed that using a tropical symmetric metric as an activation\nfunction in the last layer can improve the robustness of convolutional neural\nnetworks (CNNs) against state-of-the-art attacks, including the Carlini-Wagner\nattack. This improvement occurs when the attacks are not specifically adapted\nto the non-differentiability of the tropical layer. Moreover, they showed that\nthe decision boundary of a tropical CNN is defined by tropical bisectors. In\nthis paper, we explore the combinatorics of tropical bisectors and analyze how\nthe tropical embedding layer enhances robustness against Carlini-Wagner\nattacks. We prove an upper bound on the number of linear segments the decision\nboundary of a tropical CNN can have. We then propose a refined version of the\nCarlini-Wagner attack, specifically tailored for the tropical architecture.\nComputational experiments with MNIST and LeNet5 showcase our attacks improved\nsuccess rate."
    },
    {
        "date": "2025-03",
        "title": "Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines",
        "author": "Jayaprakashreddy Cheenepalli, John D. Hastings, Khandaker Mamun Ahmed, and Chad Fenner",
        "link": "http://arxiv.org/abs/2503.22612v1",
        "abstract": "This study evaluates the adoption of DevSecOps among small and medium-sized\nenterprises (SMEs), identifying key challenges, best practices, and future\ntrends. Through a mixed methods approach backed by the Technology Acceptance\nModel (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data\nfrom 405 SME professionals, revealing that while 68% have implemented\nDevSecOps, adoption is hindered by technical complexity (41%), resource\nconstraints (35%), and cultural resistance (38%). Despite strong leadership\nprioritization of security (73%), automation gaps persist, with only 12% of\norganizations conducting security scans per commit.\n  Our findings highlight a growing integration of security tools, particularly\nAPI security (63%) and software composition analysis (62%), although container\nsecurity adoption remains low (34%). Looking ahead, SMEs anticipate artificial\nintelligence and machine learning to significantly influence DevSecOps,\nunderscoring the need for proactive adoption of AI-driven security\nenhancements. Based on our findings, this research proposes strategic best\npractices to enhance CI/CD pipeline security including automation,\nleadership-driven security culture, and cross-team collaboration."
    },
    {
        "date": "2025-03",
        "title": "Robust Offline Imitation Learning Through State-level Trajectory Stitching",
        "author": "Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, and Jie Chen",
        "link": "http://arxiv.org/abs/2503.22524v1",
        "abstract": "Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance."
    },
    {
        "date": "2025-03",
        "title": "Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets",
        "author": "Adri\u00e1n Detavernier, and Jasper De Bock",
        "link": "http://arxiv.org/abs/2503.22418v1",
        "abstract": "Based on existing ideas in the field of imprecise probabilities, we present a\nnew approach for assessing the reliability of the individual predictions of a\ngenerative probabilistic classifier. We call this approach robustness\nquantification, compare it to uncertainty quantification, and demonstrate that\nit continues to work well even for classifiers that are learned from small\ntraining sets that are sampled from a shifted distribution."
    },
    {
        "date": "2025-03",
        "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision",
        "author": "Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, and Xi Zhang an Hongliang Ren",
        "link": "http://arxiv.org/abs/2503.22394v1",
        "abstract": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Preserving Secure Neighbor Discovery for Wireless Networks",
        "author": "Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2503.22232v2",
        "abstract": "Traditional Neighbor Discovery (ND) and Secure Neighbor Discovery (SND) are\nkey elements for network functionality. SND is a hard problem, satisfying not\nonly typical security properties (authentication, integrity) but also\nverification of direct communication, which involves distance estimation based\non time measurements and device coordinates. Defeating relay attacks, also\nknown as \"wormholes\", leading to stealthy Byzantine links and significant\ndegradation of communication and adversarial control, is key in many wireless\nnetworked systems. However, SND is not concerned with privacy; it necessitates\nrevealing the identity and location of the device(s) participating in the\nprotocol execution. This can be a deterrent for deployment, especially\ninvolving user-held devices in the emerging Internet of Things (IoT) enabled\nsmart environments. To address this challenge, we present a novel\nPrivacy-Preserving Secure Neighbor Discovery (PP-SND) protocol, enabling\ndevices to perform SND without revealing their actual identities and locations,\neffectively decoupling discovery from the exposure of sensitive information. We\nuse Homomorphic Encryption (HE) for computing device distances without\nrevealing their actual coordinates, as well as employing a pseudonymous device\nauthentication to hide identities while preserving communication integrity.\nPP-SND provides SND [1] along with pseudonymity, confidentiality, and\nunlinkability. Our presentation here is not specific to one wireless\ntechnology, and we assess the performance of the protocols (cryptographic\noverhead) on a Raspberry Pi 4 and provide a security and privacy analysis."
    },
    {
        "date": "2025-03",
        "title": "Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces",
        "author": "Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, and Sunghoon Im",
        "link": "http://arxiv.org/abs/2503.22209v1",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces."
    },
    {
        "date": "2025-03",
        "title": "Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models",
        "author": "YangTian Yan, and Jinyu Tian",
        "link": "http://arxiv.org/abs/2503.22205v1",
        "abstract": "Deep neural networks (DNNs) are susceptible to Universal Adversarial\nPerturbations (UAPs), which are instance agnostic perturbations that can\ndeceive a target model across a wide range of samples. Unlike instance-specific\nadversarial examples, UAPs present a greater challenge as they must generalize\nacross different samples and models. Generating UAPs typically requires access\nto numerous examples, which is a strong assumption in real-world tasks. In this\npaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), by\nexploiting the intrinsic vulnerabilities of deep models. We analyze a series of\npopular deep models composed of linear and nonlinear layers with a Lipschitz\nconstant of 1, revealing that the vulnerability of these models is\npredominantly influenced by their linear components. Based on this observation,\nwe leverage the ill-conditioned nature of the linear components by aligning the\nUAP with the right singular vectors corresponding to the maximum singular value\nof each linear layer. Remarkably, our method achieves highly competitive\nperformance in attacking popular image classification deep models without using\nany image samples. We also evaluate the black-box attack performance of our\nmethod, showing that it matches the state-of-the-art baseline for data-free\nmethods on models that conform to our theoretical framework. Beyond the\ndata-free assumption, IntriUAP also operates under a weaker assumption, where\nthe adversary only can access a few of the victim model's layers. Experiments\ndemonstrate that the attack success rate decreases by only 4% when the\nadversary has access to just 50% of the linear layers in the victim model."
    },
    {
        "date": "2025-03",
        "title": "T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning",
        "author": "Seong-Hyeon Hwang, Minsu Kim, and Steven Euijong Whang",
        "link": "http://arxiv.org/abs/2503.22163v1",
        "abstract": "We study model confidence calibration in class-incremental learning, where\nmodels learn from sequential tasks with different class sets. While existing\nworks primarily focus on accuracy, maintaining calibrated confidence has been\nlargely overlooked. Unfortunately, most post-hoc calibration techniques are not\ndesigned to work with the limited memories of old-task data typical in\nclass-incremental learning, as retaining a sufficient validation set would be\nimpractical. Thus, we propose T-CIL, a novel temperature scaling approach for\nclass-incremental learning without a validation set for old tasks, that\nleverages adversarially perturbed exemplars from memory. Directly using\nexemplars is inadequate for temperature optimization, since they are already\nused for training. The key idea of T-CIL is to perturb exemplars more strongly\nfor old tasks than for the new task by adjusting the perturbation direction\nbased on feature distance, with the single magnitude determined using the\nnew-task validation set. This strategy makes the perturbation magnitude\ncomputed from the new task also applicable to old tasks, leveraging the\ntendency that the accuracy of old tasks is lower than that of the new task. We\nempirically show that T-CIL significantly outperforms various baselines in\nterms of calibration on real datasets and can be integrated with existing\nclass-incremental learning techniques with minimal impact on accuracy."
    },
    {
        "date": "2025-03",
        "title": "Traffic Modeling for Network Security and Privacy: Challenges Ahead",
        "author": "Dinil Mon Divakaran",
        "link": "http://arxiv.org/abs/2503.22161v1",
        "abstract": "Traffic analysis using machine learning and deep learning models has made\nsignificant progress over the past decades. These models address various tasks\nin network security and privacy, including detection of anomalies and attacks,\ncountering censorship, etc. They also reveal privacy risks to users as\ndemonstrated by the research on LLM token inference as well as fingerprinting\n(and counter-fingerprinting) of user-visiting websites, IoT devices, and\ndifferent applications. However, challenges remain in securing our networks\nfrom threats and attacks. After briefly reviewing the tasks and recent ML\nmodels in network security and privacy, we discuss the challenges that lie\nahead."
    },
    {
        "date": "2025-03",
        "title": "SoK: Security Analysis of Blockchain-based Cryptocurrency",
        "author": "Zekai Liu, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2503.22156v1",
        "abstract": "Cryptocurrency is a novel exploration of a form of currency that proposes a\ndecentralized electronic payment scheme based on blockchain technology and\ncryptographic theory. While cryptocurrency has the security characteristics of\nbeing distributed and tamper-proof, increasing market demand has led to a rise\nin malicious transactions and attacks, thereby exposing cryptocurrency to\nvulnerabilities, privacy issues, and security threats. Particularly concerning\nare the emerging types of attacks and threats, which have made securing\ncryptocurrency increasingly urgent. Therefore, this paper classifies existing\ncryptocurrency security threats and attacks into five fundamental categories\nbased on the blockchain infrastructure and analyzes in detail the vulnerability\nprinciples exploited by each type of threat and attack. Additionally, the paper\nexamines the attackers' logic and methods and successfully reproduces the\nvulnerabilities. Furthermore, the author summarizes the existing detection and\ndefense solutions and evaluates them, all of which provide important references\nfor ensuring the security of cryptocurrency. Finally, the paper discusses the\nfuture development trends of cryptocurrency, as well as the public challenges\nit may face."
    },
    {
        "date": "2025-03",
        "title": "Non-control-Data Attacks and Defenses: A review",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.22765v1",
        "abstract": "In recent years, non-control-data attacks have be come a research hotspot in\nthe field of network security, driven\n  by the increasing number of defense methods against control-flow\n  hijacking attacks. These attacks exploit memory vulnerabilities\n  to modify non-control data within a program, thereby altering its\n  behavior without compromising control-flow integrity. Research\n  has shown that non-control-data attacks can be just as damaging\n  as control-flow hijacking attacks and are even Turing complete,\n  making them a serious security threat. However, despite being\n  discovered long ago, the threat of non-control-data attacks has\n  not been adequately addressed. In this review, we first classify\n  non-control-data attacks into two categories based on their\n  evolution: security-sensitive function attacks and data-oriented\n  programming (DOP) attacks. Subsequently, based on the non control-data attack\nmodel, we categorize existing defense methods\n  into three main strategies: memory safety, data confidentiality,\n  and data integrity protection. We then analyze recent defense\n  techniques specifically designed for DOP attacks. Finally, we\n  identify the key challenges hindering the widespread adoption\n  of defenses against non-control-data attacks and explore future\n  research directions in this field."
    },
    {
        "date": "2025-03",
        "title": "Information Theoretic One-Time Programs from Geometrically Local $\\text{QNC}_0$ Adversaries",
        "author": "Lev Stambler",
        "link": "http://arxiv.org/abs/2503.22016v2",
        "abstract": "We show how to construct simulation secure one-time memories, and thus\none-time programs, without computational assumptions in the presence of\nconstraints on quantum hardware. Specifically, we build one-time memories from\nrandom linear codes and quantum random access codes (QRACs) when constrained to\nnon-adaptive, constant depth, and $D$-dimensional geometrically-local quantum\ncircuit for some constant $D$. We place no restrictions on the adversary's\nclassical computational power, number of qubits it can use, or the coherence\ntime of its qubits. Notably, our construction can still be secure even in the\npresence of fault tolerant quantum computation as long as the input qubits are\nencoded in a non-fault tolerant manner (e.g. encoded as high energy states in\nnon-ideal hardware). Unfortunately though, our construction requires decoding\nrandom linear codes and thus does not run in polynomial time. We leave open the\nquestion of whether one can construct a polynomial time information\ntheoretically secure one-time memory from geometrically local quantum circuits.\n  Of potentially independent interest, we develop a progress bound for\ninformation leakage via collision entropy (Renyi entropy of order $2$) along\nwith a few key technical lemmas for a \"mutual information\" for collision\nentropies. We also develop new bounds on how much information a specific $2\n\\mapsto 1$ QRAC can leak about its input, which may be of independent interest\nas well."
    },
    {
        "date": "2025-03",
        "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment",
        "author": "Hamed Babaei Giglou, Jennifer D'Souza, Oliver Karras, and S\u00f6ren Auer",
        "link": "http://arxiv.org/abs/2503.21902v1",
        "abstract": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications."
    },
    {
        "date": "2025-03",
        "title": "Poster Abstract: Time Attacks using Kernel Vulnerabilities",
        "author": "Muhammad Abdullah Soomro, Adeel Nasrullah, and Fatima Muhammad Anwar",
        "link": "http://arxiv.org/abs/2503.21891v1",
        "abstract": "Timekeeping is a fundamental component of modern computing; however, the\nsecurity of system time remains an overlooked attack surface, leaving critical\nsystems vulnerable to manipulation."
    },
    {
        "date": "2025-03",
        "title": "OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation",
        "author": "Mallika Garg, Debashis Ghosh, and Pyari Mohan Pradhan",
        "link": "http://arxiv.org/abs/2503.21723v1",
        "abstract": "Occlusion is one of the challenging issues when estimating 3D hand pose. This\nproblem becomes more prominent when hand interacts with an object or two hands\nare involved. In the past works, much attention has not been given to these\noccluded regions. But these regions contain important and beneficial\ninformation that is vital for 3D hand pose estimation. Thus, in this paper, we\npropose an occlusion robust and accurate method for the estimation of 3D\nhand-object pose from the input RGB image. Our method includes first localising\nthe hand joints using a CNN based model and then refining them by extracting\ncontextual information. The self attention transformer then identifies the\nspecific joints along with the hand identity. This helps the model to identify\nthe hand belongingness of a particular joint which helps to detect the joint\neven in the occluded region. Further, these joints with hand identity are then\nused to estimate the pose using cross attention mechanism. Thus, by identifying\nthe joints in the occluded region, the obtained network becomes robust to\nocclusion. Hence, this network achieves state-of-the-art results when evaluated\non the InterHand2.6M, HO3D and H$_2$O3D datasets."
    },
    {
        "date": "2025-03",
        "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation",
        "author": "Jiahe Qian, Yaoyu Fang, Jinkui Hao, and Bo Zhou",
        "link": "http://arxiv.org/abs/2503.21695v1",
        "abstract": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches."
    },
    {
        "date": "2025-03",
        "title": "Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base",
        "author": "Satvik Verma, Qun Wang, and E. Wes Bethel",
        "link": "http://arxiv.org/abs/2503.21674v1",
        "abstract": "The widespread adoption of Internet of Things (IoT) devices has introduced\nsignificant cybersecurity challenges, particularly with the increasing\nfrequency and sophistication of Distributed Denial of Service (DDoS) attacks.\nTraditional machine learning (ML) techniques often fall short in detecting such\nattacks due to the complexity of blended and evolving patterns. To address\nthis, we propose a novel framework leveraging On-Device Large Language Models\n(ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for\nintelligent IoT network attack detection. By implementing feature ranking\ntechniques and constructing both long and short KBs tailored to model\ncapacities, the proposed framework ensures efficient and accurate detection of\nDDoS attacks while overcoming computational and privacy limitations. Simulation\nresults demonstrate that the optimized framework achieves superior accuracy\nacross diverse attack types, especially when using compact models in edge\ncomputing environments. This work provides a scalable and secure solution for\nreal-time IoT security, advancing the applicability of edge intelligence in\ncybersecurity."
    },
    {
        "date": "2025-03",
        "title": "Advancing CAN Network Security through RBM-Based Synthetic Attack Data Generation for Intrusion Detection Systems",
        "author": "Huacheng Li, Jingyong Su, and Kai Wang",
        "link": "http://arxiv.org/abs/2503.21496v1",
        "abstract": "The rapid development of network technologies and industrial intelligence has\naugmented the connectivity and intelligence within the automotive industry.\nNotably, in the Internet of Vehicles (IoV), the Controller Area Network (CAN),\nwhich is crucial for the communication of electronic control units but lacks\ninbuilt security measures, has become extremely vulnerable to severe\ncybersecurity threats. Meanwhile, the efficacy of Intrusion Detection Systems\n(IDS) is hampered by the scarcity of sufficient attack data for robust model\ntraining. To overcome this limitation, we introduce a novel methodology\nleveraging the Restricted Boltzmann Machine (RBM) to generate synthetic CAN\nattack data, thereby producing training datasets with a more balanced sample\ndistribution. Specifically, we design a CAN Data Processing Module for\ntransforming raw CAN data into an RBM-trainable format, and a Negative Sample\nGeneration Module to generate data reflecting the distribution of CAN data\nframes denoting network intrusions. Experimental results show the generated\ndata significantly improves IDS performance, with CANet accuracy rising from\n0.6477 to 0.9725 and EfficientNet from 0.1067 to 0.1555. Code is available at\nhttps://github.com/wangkai-tech23/CANDataSynthetic."
    },
    {
        "date": "2025-03",
        "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time",
        "author": "Zhaojun Nan, Yunchu Han, Sheng Zhou, and Zhisheng Niu",
        "link": "http://arxiv.org/abs/2503.21476v1",
        "abstract": "In edge intelligence systems, deep neural network (DNN) partitioning and data\noffloading can provide real-time task inference for resource-constrained mobile\ndevices. However, the inference time of DNNs is typically uncertain and cannot\nbe precisely determined in advance, presenting significant challenges in\nensuring timely task processing within deadlines. To address the uncertain\ninference time, we propose a robust optimization scheme to minimize the total\nenergy consumption of mobile devices while meeting task probabilistic\ndeadlines. The scheme only requires the mean and variance information of the\ninference time, without any prediction methods or distribution functions. The\nproblem is formulated as a mixed-integer nonlinear programming (MINLP) that\ninvolves jointly optimizing the DNN model partitioning and the allocation of\nlocal CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first\ndecompose the original problem into two subproblems: resource allocation and\nDNN model partitioning. Subsequently, the two subproblems with probability\nconstraints are equivalently transformed into deterministic optimization\nproblems using the chance-constrained programming (CCP) method. Finally, the\nconvex optimization technique and the penalty convex-concave procedure (PCCP)\ntechnique are employed to obtain the optimal solution of the resource\nallocation subproblem and a stationary point of the DNN model partitioning\nsubproblem, respectively. The proposed algorithm leverages real-world data from\npopular hardware platforms and is evaluated on widely used DNN models.\nExtensive simulations show that our proposed algorithm effectively addresses\nthe inference time uncertainty with probabilistic deadline guarantees while\nminimizing the energy consumption of mobile devices."
    },
    {
        "date": "2025-03",
        "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection",
        "author": "Ryan Marinelli, Josef Pichlmeier, and Tamas Bisztray",
        "link": "http://arxiv.org/abs/2503.21464v1",
        "abstract": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main."
    },
    {
        "date": "2025-03",
        "title": "AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model",
        "author": "Sen Zhang, Qingqing Ye, Haibo Hu, and Jianliang Xu",
        "link": "http://arxiv.org/abs/2503.21426v1",
        "abstract": "The skip-gram model (SGM), which employs a neural network to generate node\nvectors, serves as the basis for numerous popular graph embedding techniques.\nHowever, since the training datasets contain sensitive linkage information, the\nparameters of a released SGM may encode private information and pose\nsignificant privacy risks. Differential privacy (DP) is a rigorous standard for\nprotecting individual privacy in data analysis. Nevertheless, when applying\ndifferential privacy to skip-gram in graphs, it becomes highly challenging due\nto the complex link relationships, which potentially result in high sensitivity\nand necessitate substantial noise injection. To tackle this challenge, we\npresent AdvSGM, a differentially private skip-gram for graphs via adversarial\ntraining. Our core idea is to leverage adversarial training to privatize\nskip-gram while improving its utility. Towards this end, we develop a novel\nadversarial training module by devising two optimizable noise terms that\ncorrespond to the parameters of a skip-gram. By fine-tuning the weights between\nmodules within AdvSGM, we can achieve differentially private gradient updates\nwithout additional noise injection. Extensive experimental results on six\nreal-world graph datasets show that AdvSGM preserves high data utility across\ndifferent downstream tasks."
    },
    {
        "date": "2025-03",
        "title": "Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack",
        "author": "Cheng Wang, Yiwei Wang, Yujun Cai, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2503.21315v1",
        "abstract": "Retrieval-augmented generation (RAG) systems enhance large language models by\nincorporating external knowledge, addressing issues like outdated internal\nknowledge and hallucination. However, their reliance on external knowledge\nbases makes them vulnerable to corpus poisoning attacks, where adversarial\npassages can be injected to manipulate retrieval results. Existing methods for\ncrafting such passages, such as random token replacement or training inversion\nmodels, are often slow and computationally expensive, requiring either access\nto retriever's gradients or large computational resources. To address these\nlimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), an\nefficient black-box method that leverages two key properties of retrievers:\ninsensitivity to token order and bias towards influential tokens. By focusing\non these characteristics, DIGA dynamically adjusts its genetic operations to\ngenerate effective adversarial passages with significantly reduced time and\nmemory usage. Our experimental evaluation shows that DIGA achieves superior\nefficiency and scalability compared to existing methods, while maintaining\ncomparable or better attack success rates across multiple datasets."
    },
    {
        "date": "2025-03",
        "title": "DeBackdoor: A Deductive Framework for Detecting Backdoor Attacks on Deep Models with Limited Data",
        "author": "Dorde Popovic, Amin Sadeghi, Ting Yu, Sanjay Chawla, and Issa Khalil",
        "link": "http://arxiv.org/abs/2503.21305v1",
        "abstract": "Backdoor attacks are among the most effective, practical, and stealthy\nattacks in deep learning. In this paper, we consider a practical scenario where\na developer obtains a deep model from a third party and uses it as part of a\nsafety-critical system. The developer wants to inspect the model for potential\nbackdoors prior to system deployment. We find that most existing detection\ntechniques make assumptions that are not applicable to this scenario. In this\npaper, we present a novel framework for detecting backdoors under realistic\nrestrictions. We generate candidate triggers by deductively searching over the\nspace of possible triggers. We construct and optimize a smoothed version of\nAttack Success Rate as our search objective. Starting from a broad class of\ntemplate attacks and just using the forward pass of a deep model, we reverse\nengineer the backdoor attack. We conduct extensive evaluation on a wide range\nof attacks, models, and datasets, with our technique performing almost\nperfectly across these settings."
    },
    {
        "date": "2025-03",
        "title": "OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation",
        "author": "Yongxu Wang, Weiyun Yi, Xinhao Kong, and Wanting Li",
        "link": "http://arxiv.org/abs/2503.21257v1",
        "abstract": "With the rapid development of embodied intelligence, leveraging large-scale\nhuman data for high-level imitation learning on humanoid robots has become a\nfocal point of interest in both academia and industry. However, applying\nhumanoid robots to precision operation domains remains challenging due to the\ncomplexities they face in perception and control processes, the long-standing\nphysical differences in morphology and actuation mechanisms between humanoid\nrobots and humans, and the lack of task-relevant features obtained from\negocentric vision. To address the issue of covariate shift in imitation\nlearning, this paper proposes an imitation learning algorithm tailored for\nhumanoid robots. By focusing on the primary task objectives, filtering out\nbackground information, and incorporating channel feature fusion with spatial\nattention mechanisms, the proposed algorithm suppresses environmental\ndisturbances and utilizes a dynamic weight update strategy to significantly\nimprove the success rate of humanoid robots in accomplishing target tasks.\nExperimental results demonstrate that the proposed method exhibits robustness\nand scalability across various typical task scenarios, providing new ideas and\napproaches for autonomous learning and control in humanoid robots. The project\nwill be open-sourced on GitHub."
    },
    {
        "date": "2025-03",
        "title": "Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing",
        "author": "Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.21236v1",
        "abstract": "Large-scale image retrieval using deep hashing has become increasingly\npopular due to the exponential growth of image data and the remarkable feature\nextraction capabilities of deep neural networks (DNNs). However, deep hashing\nmethods are vulnerable to malicious attacks, including adversarial and backdoor\nattacks. It is worth noting that these attacks typically involve altering the\nquery images, which is not a practical concern in real-world scenarios. In this\npaper, we point out that even clean query images can be dangerous, inducing\nmalicious target retrieval results, like undesired or illegal images. To the\nbest of our knowledge, we are the first to study data \\textbf{p}oisoning\n\\textbf{a}ttacks against \\textbf{d}eep \\textbf{hash}ing\n\\textbf{(\\textit{PADHASH})}. Specifically, we first train a surrogate model to\nsimulate the behavior of the target deep hashing model. Then, a strict gradient\nmatching strategy is proposed to generate the poisoned images. Extensive\nexperiments on different models, datasets, hash methods, and hash code lengths\ndemonstrate the effectiveness and generality of our attack method."
    },
    {
        "date": "2025-03",
        "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples",
        "author": "Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, and Seong Tae Kim",
        "link": "http://arxiv.org/abs/2503.21164v1",
        "abstract": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs."
    },
    {
        "date": "2025-03",
        "title": "How to Secure Existing C and C++ Software without Memory Safety",
        "author": "\u00dalfar Erlingsson",
        "link": "http://arxiv.org/abs/2503.21145v1",
        "abstract": "The most important security benefit of software memory safety is easy to\nstate: for C and C++ software, attackers can exploit most bugs and\nvulnerabilities to gain full, unfettered control of software behavior, whereas\nthis is not true for most bugs in memory-safe software.\n  Fortunately, this security benefit -- most bugs don't give attackers full\ncontrol -- can be had for unmodified C/C++ software, without per-application\neffort.\n  This doesn't require trying to establish memory safety; instead, it is\nsufficient to eliminate most of the combinatorial ways in which software with\ncorrupted memory can execute. To eliminate these interleavings, there already\nexist practical compiler and runtime mechanisms that incur little overhead and\nneed no special hardware or platform support.\n  Each of the mechanisms described here is already in production use, at scale,\non one or more platforms. By supporting their combined use in development\ntoolchains, the security of all C and C++ software against remote code\nexecution attacks can be rapidly, and dramatically, improved."
    },
    {
        "date": "2025-03",
        "title": "Generator Cost Coefficients Inference Attack via Exploitation of Locational Marginal Prices in Smart Grid",
        "author": "Junfei Wang, and Pirathayini Srikantha",
        "link": "http://arxiv.org/abs/2503.20976v1",
        "abstract": "Real-time price signals and power generation levels (disaggregated or\naggregated) are commonly made available to the public by Independent System\nOperators (ISOs) to promote efficiency and transparency. However, they may\ninadvertently reveal crucial private information about the power grid, such as\nthe cost functions of generators. Adversaries can exploit these vulnerabilities\nfor strategic bidding, potentially leading to financial losses for power market\nparticipants and consumers. In this paper, we prove the existence of a\nclosed-form solution for recovering coefficients in cost functions when LMPs\nand disaggregated power generation data are available. Additionally, we\nestablish the convergence conditions for inference the quadratic coefficients\nof cost functions when LMPs and aggregated generation data are given. Our\ntheoretical analysis provides the conditions under which the algorithm is\nguaranteed to converge, and our experiments demonstrate the efficacy of this\nmethod on IEEE benchmark systems, including 14-bus and 30-bus and 118-bus\nsystems."
    },
    {
        "date": "2025-03",
        "title": "TS-Inverse: A Gradient Inversion Attack Tailored for Federated Time Series Forecasting Models",
        "author": "Caspar Meijer, Jiyue Huang, Shreshtha Sharma, Elena Lazovik, and Lydia Y. Chen",
        "link": "http://arxiv.org/abs/2503.20952v1",
        "abstract": "Federated learning (FL) for time series forecasting (TSF) enables clients\nwith privacy-sensitive time series (TS) data to collaboratively learn accurate\nforecasting models, for example, in energy load prediction. Unfortunately,\nprivacy risks in FL persist, as servers can potentially reconstruct clients'\ntraining data through gradient inversion attacks (GIA). Although GIA is\ndemonstrated for image classification tasks, little is known about time series\nregression tasks. In this paper, we first conduct an extensive empirical study\non inverting TS data across 4 TSF models and 4 datasets, identifying the unique\nchallenges of reconstructing both observations and targets of TS data. We then\npropose TS-Inverse, a novel GIA that improves the inversion of TS data by (i)\nlearning a gradient inversion model that outputs quantile predictions, (ii) a\nunique loss function that incorporates periodicity and trend regularization,\nand (iii) regularization according to the quantile predictions. Our evaluations\ndemonstrate a remarkable performance of TS-Inverse, achieving at least a 2x-10x\nimprovement in terms of the sMAPE metric over existing GIA methods on TS data.\nCode repository: https://github.com/Capsar/ts-inverse"
    },
    {
        "date": "2025-03",
        "title": "Prototype Guided Backdoor Defense",
        "author": "Venkat Adithya Amula, Sunayana Samavedam, Saurabh Saini, Avani Gupta, and Narayanan P J",
        "link": "http://arxiv.org/abs/2503.20925v1",
        "abstract": "Deep learning models are susceptible to {\\em backdoor attacks} involving\nmalicious attackers perturbing a small subset of training data with a {\\em\ntrigger} to causes misclassifications. Various triggers have been used,\nincluding semantic triggers that are easily realizable without requiring the\nattacker to manipulate the image. The emergence of generative AI has eased the\ngeneration of varied poisoned samples. Robustness across types of triggers is\ncrucial to effective defense. We propose Prototype Guided Backdoor Defense\n(PGBD), a robust post-hoc defense that scales across different trigger types,\nincluding previously unsolved semantic triggers. PGBD exploits displacements in\nthe geometric spaces of activations to penalize movements toward the trigger.\nThis is done using a novel sanitization loss of a post-hoc fine-tuning step.\nThe geometric approach scales easily to all types of attacks. PGBD achieves\nbetter performance across all settings. We also present the first defense\nagainst a new semantic attack on celebrity face images. Project page:\n\\hyperlink{https://venkatadithya9.github.io/pgbd.github.io/}{this https URL}."
    },
    {
        "date": "2025-03",
        "title": "Robust Federated Learning Against Poisoning Attacks: A GAN-Based Defense Framework",
        "author": "Usama Zafar, Andr\u00e9 Teixeira, and Salman Toor",
        "link": "http://arxiv.org/abs/2503.20884v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndecentralized devices without sharing raw data, but it remains vulnerable to\npoisoning attacks that compromise model integrity. Existing defenses often rely\non external datasets or predefined heuristics (e.g. number of malicious\nclients), limiting their effectiveness and scalability. To address these\nlimitations, we propose a privacy-preserving defense framework that leverages a\nConditional Generative Adversarial Network (cGAN) to generate synthetic data at\nthe server for authenticating client updates, eliminating the need for external\ndatasets. Our framework is scalable, adaptive, and seamlessly integrates into\nFL workflows. Extensive experiments on benchmark datasets demonstrate its\nrobust performance against a variety of poisoning attacks, achieving high True\nPositive Rate (TPR) and True Negative Rate (TNR) of malicious and benign\nclients, respectively, while maintaining model accuracy. The proposed framework\noffers a practical and effective solution for securing federated learning\nsystems."
    },
    {
        "date": "2025-03",
        "title": "DR-PETS: Learning-Based Control With Planning in Adversarial Environments",
        "author": "Hozefa Jesawada, Antonio Acernese, Giovanni Russo, and Carmen Del Vecchio",
        "link": "http://arxiv.org/abs/2503.20660v2",
        "abstract": "Ensuring robustness against epistemic, possibly adversarial, perturbations is\nessential for reliable real-world decision-making. While the Probabilistic\nEnsembles with Trajectory Sampling (PETS) algorithm inherently handles\nuncertainty via ensemble-based probabilistic models, it lacks guarantees\nagainst structured adversarial or worst-case uncertainty distributions. To\naddress this, we propose DR-PETS, a distributionally robust extension of PETS\nthat certifies robustness against adversarial perturbations. We formalize\nuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-aware\nplanning through a min-max optimization framework. While PETS passively\naccounts for stochasticity, DR-PETS actively optimizes robustness via a\ntractable convex approximation integrated into PETS planning loop. Experiments\non pendulum stabilization and cart-pole balancing show that DR-PETS certifies\nrobustness against adversarial parameter perturbations, achieving consistent\nperformance in worst-case scenarios where PETS deteriorates."
    },
    {
        "date": "2025-03",
        "title": "Robust Flower Cluster Matching Using The Unscented Transform",
        "author": "Andy Chu, Rashik Shrestha, Yu Gu, and Jason N. Gross",
        "link": "http://arxiv.org/abs/2503.20631v1",
        "abstract": "Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments."
    },
    {
        "date": "2025-03",
        "title": "$\u03b2$-GNN: A Robust Ensemble Approach Against Graph Structure Perturbation",
        "author": "Haci Ismail Aslan, Philipp Wiesner, Ping Xiong, and Odej Kao",
        "link": "http://arxiv.org/abs/2503.20630v1",
        "abstract": "Graph Neural Networks (GNNs) are playing an increasingly important role in\nthe efficient operation and security of computing systems, with applications in\nworkload scheduling, anomaly detection, and resource management. However, their\nvulnerability to network perturbations poses a significant challenge. We\npropose $\\beta$-GNN, a model enhancing GNN robustness without sacrificing clean\ndata performance. $\\beta$-GNN uses a weighted ensemble, combining any GNN with\na multi-layer perceptron. A learned dynamic weight, $\\beta$, modulates the\nGNN's contribution. This $\\beta$ not only weights GNN influence but also\nindicates data perturbation levels, enabling proactive mitigation. Experimental\nresults on diverse datasets show $\\beta$-GNN's superior adversarial accuracy\nand attack severity quantification. Crucially, $\\beta$-GNN avoids perturbation\nassumptions, preserving clean data structure and performance."
    },
    {
        "date": "2025-03",
        "title": "Robust Deep Reinforcement Learning in Robotics via Adaptive Gradient-Masked Adversarial Attacks",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, Yong Cui, and Yue Gao",
        "link": "http://arxiv.org/abs/2503.20844v1",
        "abstract": "Deep reinforcement learning (DRL) has emerged as a promising approach for\nrobotic control, but its realworld deployment remains challenging due to its\nvulnerability to environmental perturbations. Existing white-box adversarial\nattack methods, adapted from supervised learning, fail to effectively target\nDRL agents as they overlook temporal dynamics and indiscriminately perturb all\nstate dimensions, limiting their impact on long-term rewards. To address these\nchallenges, we propose the Adaptive Gradient-Masked Reinforcement (AGMR)\nAttack, a white-box attack method that combines DRL with a gradient-based soft\nmasking mechanism to dynamically identify critical state dimensions and\noptimize adversarial policies. AGMR selectively allocates perturbations to the\nmost impactful state features and incorporates a dynamic adjustment mechanism\nto balance exploration and exploitation during training. Extensive experiments\ndemonstrate that AGMR outperforms state-of-the-art adversarial attack methods\nin degrading the performance of the victim agent and enhances the victim\nagent's robustness through adversarial defense mechanisms."
    },
    {
        "date": "2025-03",
        "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement Learning",
        "author": "Zongyuan Zhang, Tianyang Duan, Zheng Lin, Dong Huang, Zihan Fang, Zekai Sun, Ling Xiong, Hongbin Liang, Heming Cui, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.20613v1",
        "abstract": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks."
    },
    {
        "date": "2025-03",
        "title": "Feature Statistics with Uncertainty Help Adversarial Robustness",
        "author": "Ran Wang, Xinlei Zhou, Rihao Li, Meng Hu, Wenhui Wu, and Yuheng Jia",
        "link": "http://arxiv.org/abs/2503.20583v1",
        "abstract": "Despite the remarkable success of deep neural networks (DNNs), the security\nthreat of adversarial attacks poses a significant challenge to the reliability\nof DNNs. By introducing randomness into different parts of DNNs, stochastic\nmethods can enable the model to learn some uncertainty, thereby improving model\nrobustness efficiently. In this paper, we theoretically discover a universal\nphenomenon that adversarial attacks will shift the distributions of feature\nstatistics. Motivated by this theoretical finding, we propose a robustness\nenhancement module called Feature Statistics with Uncertainty (FSU). It\nresamples channel-wise feature means and standard deviations of examples from\nmultivariate Gaussian distributions, which helps to reconstruct the attacked\nexamples and calibrate the shifted distributions. The calibration recovers some\ndomain characteristics of the data for classification, thereby mitigating the\ninfluence of perturbations and weakening the ability of attacks to deceive\nmodels. The proposed FSU module has universal applicability in training,\nattacking, predicting and fine-tuning, demonstrating impressive robustness\nenhancement ability at trivial additional time cost. For example, against\npowerful optimization-based CW attacks, by incorporating FSU into attacking and\npredicting phases, it endows many collapsed state-of-the-art models with\n50%-80% robust accuracy on CIFAR10, CIFAR100 and SVHN."
    },
    {
        "date": "2025-03",
        "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling",
        "author": "Vinzenz Uhr, Ivan Diaz, Christian Rummel, and Richard McKinley",
        "link": "http://arxiv.org/abs/2503.20571v1",
        "abstract": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs)."
    },
    {
        "date": "2025-03",
        "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks",
        "author": "Yangqi Feng, Shing-Ho J. Lin, Baoyuan Gao, and Xian Wei",
        "link": "http://arxiv.org/abs/2503.20454v1",
        "abstract": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates."
    },
    {
        "date": "2025-03",
        "title": "Learning Data-Driven Uncertainty Set Partitions for Robust and Adaptive Energy Forecasting with Missing Data",
        "author": "Akylas Stratigakos, and Panagiotis Andrianesis",
        "link": "http://arxiv.org/abs/2503.20410v1",
        "abstract": "Short-term forecasting models typically assume the availability of input data\n(features) when they are deployed and in use. However, equipment failures,\ndisruptions, cyberattacks, may lead to missing features when such models are\nused operationally, which could negatively affect forecast accuracy, and result\nin suboptimal operational decisions. In this paper, we use adaptive robust\noptimization and adversarial machine learning to develop forecasting models\nthat seamlessly handle missing data operationally. We propose linear- and\nneural network-based forecasting models with parameters that adapt to available\nfeatures, combining linear adaptation with a novel algorithm for learning\ndata-driven uncertainty set partitions. The proposed adaptive models do not\nrely on identifying historical missing data patterns and are suitable for\nreal-time operations under stringent time constraints. Extensive numerical\nexperiments on short-term wind power forecasting considering horizons from 15\nminutes to 4 hours ahead illustrate that our proposed adaptive models are on\npar with imputation when data are missing for very short periods (e.g., when\nonly the latest measurement is missing) whereas they significantly outperform\nimputation when data are missing for longer periods. We further provide\ninsights by showcasing how linear adaptation and data-driven partitions (even\nwith a few subsets) approach the performance of the optimal, yet impractical,\nmethod of retraining for every possible realization of missing data."
    },
    {
        "date": "2025-03",
        "title": "Wasserstein Distributionally Robust Bayesian Optimization with Continuous Context",
        "author": "Francesco Micheli, Efe C. Balta, Anastasios Tsiamis, and John Lygeros",
        "link": "http://arxiv.org/abs/2503.20341v1",
        "abstract": "We address the challenge of sequential data-driven decision-making under\ncontext distributional uncertainty. This problem arises in numerous real-world\nscenarios where the learner optimizes black-box objective functions in the\npresence of uncontrollable contextual variables. We consider the setting where\nthe context distribution is uncertain but known to lie within an ambiguity set\ndefined as a ball in the Wasserstein distance. We propose a novel algorithm for\nWasserstein Distributionally Robust Bayesian Optimization that can handle\ncontinuous context distributions while maintaining computational tractability.\nOur theoretical analysis combines recent results in self-normalized\nconcentration in Hilbert spaces and finite-sample bounds for distributionally\nrobust optimization to establish sublinear regret bounds that match\nstate-of-the-art results. Through extensive comparisons with existing\napproaches on both synthetic and real-world problems, we demonstrate the\nsimplicity, effectiveness, and practical applicability of our proposed method."
    },
    {
        "date": "2025-03",
        "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach",
        "author": "Jiahao Qin, Feng Liu, and Lu Zong",
        "link": "http://arxiv.org/abs/2503.22729v1",
        "abstract": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation."
    },
    {
        "date": "2025-03",
        "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks",
        "author": "Tao Wu, and Tie Luo",
        "link": "http://arxiv.org/abs/2503.20310v1",
        "abstract": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures."
    },
    {
        "date": "2025-03",
        "title": "Model-Based Offline Reinforcement Learning with Adversarial Data Augmentation",
        "author": "Hongye Cao, Fan Feng, Jing Huo, Shangdong Yang, Meng Fang, Tianpei Yang, and Yang Gao",
        "link": "http://arxiv.org/abs/2503.20285v1",
        "abstract": "Model-based offline Reinforcement Learning (RL) constructs environment models\nfrom offline datasets to perform conservative policy optimization. Existing\napproaches focus on learning state transitions through ensemble models,\nrollouting conservative estimation to mitigate extrapolation errors. However,\nthe static data makes it challenging to develop a robust policy, and offline\nagents cannot access the environment to gather new data. To address these\nchallenges, we introduce Model-based Offline Reinforcement learning with\nAdversariaL data augmentation (MORAL). In MORAL, we replace the fixed horizon\nrollout by employing adversaria data augmentation to execute alternating\nsampling with ensemble models to enrich training data. Specifically, this\nadversarial process dynamically selects ensemble models against policy for\nbiased sampling, mitigating the optimistic estimation of fixed models, thus\nrobustly expanding the training data for policy optimization. Moreover, a\ndifferential factor is integrated into the adversarial process for\nregularization, ensuring error minimization in extrapolations. This\ndata-augmented optimization adapts to diverse offline tasks without rollout\nhorizon tuning, showing remarkable applicability. Extensive experiments on D4RL\nbenchmark demonstrate that MORAL outperforms other model-based offline RL\nmethods in terms of policy learning and sample efficiency."
    },
    {
        "date": "2025-03",
        "title": "How Secure is Forgetting? Linking Machine Unlearning to Machine Learning Attacks",
        "author": "Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2503.20257v1",
        "abstract": "As Machine Learning (ML) evolves, the complexity and sophistication of\nsecurity threats against this paradigm continue to grow as well, threatening\ndata privacy and model integrity. In response, Machine Unlearning (MU) is a\nrecent technology that aims to remove the influence of specific data from a\ntrained model, enabling compliance with privacy regulations and user requests.\nThis can be done for privacy compliance (e.g., GDPR's right to be forgotten) or\nmodel refinement. However, the intersection between classical threats in ML and\nMU remains largely unexplored. In this Systematization of Knowledge (SoK), we\nprovide a structured analysis of security threats in ML and their implications\nfor MU. We analyze four major attack classes, namely, Backdoor Attacks,\nMembership Inference Attacks (MIA), Adversarial Attacks, and Inversion Attacks,\nwe investigate their impact on MU and propose a novel classification based on\nhow they are usually used in this context. Finally, we identify open\nchallenges, including ethical considerations, and explore promising future\nresearch directions, paving the way for future research in secure and\nprivacy-preserving Machine Unlearning."
    },
    {
        "date": "2025-03",
        "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
        "author": "Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, and Robby T. Tan",
        "link": "http://arxiv.org/abs/2503.20211v1",
        "abstract": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness of Kernel Ridge Regression Using the Cauchy Loss Function",
        "author": "Hongwei Wen, Annika Betken, and Wouter Koolen",
        "link": "http://arxiv.org/abs/2503.20120v1",
        "abstract": "Robust regression aims to develop methods for estimating an unknown\nregression function in the presence of outliers, heavy-tailed distributions, or\ncontaminated data, which can severely impact performance. Most existing\ntheoretical results in robust regression assume that the noise has a finite\nabsolute mean, an assumption violated by certain distributions, such as Cauchy\nand some Pareto noise. In this paper, we introduce a generalized Cauchy noise\nframework that accommodates all noise distributions with finite moments of any\norder, even when the absolute mean is infinite. Within this framework, we study\nthe \\textit{kernel Cauchy ridge regressor} (\\textit{KCRR}), which minimizes a\nregularized empirical Cauchy risk to achieve robustness. To derive the\n$L_2$-risk bound for KCRR, we establish a connection between the excess Cauchy\nrisk and $L_2$-risk for sufficiently large scale parameters of the Cauchy loss,\nwhich reveals that these two risks are equivalent. Furthermore, under the\nassumption that the regression function satisfies H\\\"older smoothness, we\nderive excess Cauchy risk bounds for KCRR, showing improved performance as the\nscale parameter decreases. By considering the twofold effect of the scale\nparameter on the excess Cauchy risk and its equivalence with the $L_2$-risk, we\nestablish the almost minimax-optimal convergence rate for KCRR in terms of\n$L_2$-risk, highlighting the robustness of the Cauchy loss in handling various\ntypes of noise. Finally, we validate the effectiveness of KCRR through\nexperiments on both synthetic and real-world datasets under diverse noise\ncorruption scenarios."
    },
    {
        "date": "2025-03",
        "title": "ARGO-SLSA: Software Supply Chain Security in Argo Workflows",
        "author": "Mohomed Thariq, and Indrajith Ekanayake",
        "link": "http://arxiv.org/abs/2503.20079v1",
        "abstract": "Distributed systems widely adopt microservice architecture to handle growing\ncomplexity and scale. This approach breaks applications into independent,\nloosely coupled services. Kubernetes has become the de facto standard for\nmanaging microservices, and automating complex, multi-step workflows is a\ncommon requirement in Kubernetes. Argo Workflows is a Kubernetes-native engine\nfor managing these workflows in an automated fashion. These workflows generate\nartifacts such as executables, logs, container images, and packages, which\noften require proper management through software supply chain security.\nHowever, Argo Workflows does not include built-in functionality for frameworks\nlike Supply-chain Levels for Software Artifacts (SLSA), which is essential for\nensuring artifact integrity, traceability, and security. This gap compels\npractitioners to rely on external tools to meet software supply chain security\nstandards. In response, this paper proposes a Kubernetes-native controller\nbuilt on top of existing open-source Argo Workflows to enhance artifact\nsecurity. By generating cryptographic signing and provenance attestations, the\ncontroller enables Argo Workflows to comply with SLSA standards. We demonstrate\nthat implementations can provide such cryptographic signing and provenance\nattestations for artifacts produced by the controller, allowing software\nartifacts built with Argo Workflows to adhere to SLSA requirements. The\nproposed validation model evaluates the proof of concept of the controller,\nincluding its ability to reconcile workflows, detect pods associated with\nworkflow nodes, operate without disrupting existing operations, enforce\nintegrity, and monitor software artifacts."
    },
    {
        "date": "2025-03",
        "title": "A proposal for an incident regime that tracks and counters threats to national security posed by AI systems",
        "author": "Alejandro Ortega",
        "link": "http://arxiv.org/abs/2503.19887v1",
        "abstract": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regie\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal,, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security. Our\nproposal is timely, given ongoing policy interest in the potential national\nsecurity threats posed by AI systems."
    },
    {
        "date": "2025-03",
        "title": "RCC-PFL: Robust Client Clustering under Noisy Labels in Personalized Federated Learning",
        "author": "Abdulmoneam Ali, and Ahmed Arafa",
        "link": "http://arxiv.org/abs/2503.19886v1",
        "abstract": "We address the problem of cluster identity estimation in a personalized\nfederated learning (PFL) setting in which users aim to learn different personal\nmodels. The backbone of effective learning in such a setting is to cluster\nusers into groups whose objectives are similar. A typical approach in the\nliterature is to achieve this by training users' data on different proposed\npersonal models and assign them to groups based on which model achieves the\nlowest value of the users' loss functions. This process is to be done\niteratively until group identities converge. A key challenge in such a setting\narises when users have noisy labeled data, which may produce misleading values\nof their loss functions, and hence lead to ineffective clustering. To overcome\nthis challenge, we propose a label-agnostic data similarity-based clustering\nalgorithm, coined RCC-PFL, with three main advantages: the cluster identity\nestimation procedure is independent from the training labels; it is a one-shot\nclustering algorithm performed prior to the training; and it requires fewer\ncommunication rounds and less computation compared to iterative-based\nclustering methods. We validate our proposed algorithm using various models and\ndatasets and show that it outperforms multiple baselines in terms of average\naccuracy and variance reduction."
    },
    {
        "date": "2025-03",
        "title": "Bitstream Collisions in Neural Image Compression via Adversarial Perturbations",
        "author": "Jordan Madden, Lhamo Dorje, and Xiaohua Li",
        "link": "http://arxiv.org/abs/2503.19817v1",
        "abstract": "Neural image compression (NIC) has emerged as a promising alternative to\nclassical compression techniques, offering improved compression ratios. Despite\nits progress towards standardization and practical deployment, there has been\nminimal exploration into it's robustness and security. This study reveals an\nunexpected vulnerability in NIC - bitstream collisions - where semantically\ndifferent images produce identical compressed bitstreams. Utilizing a novel\nwhitebox adversarial attack algorithm, this paper demonstrates that adding\ncarefully crafted perturbations to semantically different images can cause\ntheir compressed bitstreams to collide exactly. The collision vulnerability\nposes a threat to the practical usability of NIC, particularly in\nsecurity-critical applications. The cause of the collision is analyzed, and a\nsimple yet effective mitigation method is presented."
    },
    {
        "date": "2025-03",
        "title": "SITA: Structurally Imperceptible and Transferable Adversarial Attacks for Stylized Image Generation",
        "author": "Jingdan Kang, Haoxin Yang, Yan Cai, Huaidong Zhang, Xuemiao Xu, Yong Du, and Shengfeng He",
        "link": "http://arxiv.org/abs/2503.19791v1",
        "abstract": "Image generation technology has brought significant advancements across\nvarious fields but has also raised concerns about data misuse and potential\nrights infringements, particularly with respect to creating visual artworks.\nCurrent methods aimed at safeguarding artworks often employ adversarial\nattacks. However, these methods face challenges such as poor transferability,\nhigh computational costs, and the introduction of noticeable noise, which\ncompromises the aesthetic quality of the original artwork. To address these\nlimitations, we propose a Structurally Imperceptible and Transferable\nAdversarial (SITA) attacks. SITA leverages a CLIP-based destylization loss,\nwhich decouples and disrupts the robust style representation of the image. This\ndisruption hinders style extraction during stylized image generation, thereby\nimpairing the overall stylization process. Importantly, SITA eliminates the\nneed for a surrogate diffusion model, leading to significantly reduced\ncomputational overhead. The method's robust style feature disruption ensures\nhigh transferability across diverse models. Moreover, SITA introduces\nperturbations by embedding noise within the imperceptible structural details of\nthe image. This approach effectively protects against style extraction without\ncompromising the visual quality of the artwork. Extensive experiments\ndemonstrate that SITA offers superior protection for artworks against\nunauthorized use in stylized generation. It significantly outperforms existing\nmethods in terms of transferability, computational efficiency, and noise\nimperceptibility. Code is available at https://github.com/A-raniy-day/SITA."
    },
    {
        "date": "2025-03",
        "title": "PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch",
        "author": "Abhishek Ghosh, Ajay Nayak, Ashish Panwar, and Arkaprava Basu",
        "link": "http://arxiv.org/abs/2503.19779v1",
        "abstract": "CUDA Graphs -- a recent hardware feature introduced for NVIDIA GPUs -- aim to\nreduce CPU launch overhead by capturing and launching a series of GPU tasks\n(kernels) as a DAG. However, deploying CUDA Graphs faces several challenges\ntoday due to the static structure of a graph. It also incurs performance\noverhead due to data copy. In fact, we show a counter-intuitive result --\ndeploying CUDA Graphs hurts performance in many cases.\n  We introduce PyGraph, a novel approach to automatically harness the power of\nCUDA Graphs within PyTorch2. Driven by three key observations, PyGraph embodies\nthree novel optimizations: it enables wider deployment of CUDA Graphs, reduces\nGPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based\non a cost-benefit analysis. PyGraph seamlessly integrates with PyTorch2's\ncompilation toolchain, enabling efficient use of CUDA Graphs without manual\nmodifications to the code. We evaluate PyGraph across various machine learning\nbenchmarks, demonstrating substantial performance improvements over PyTorch2."
    },
    {
        "date": "2025-03",
        "title": "A Managed Tokens Service for Securely Keeping and Distributing Grid Tokens",
        "author": "Shreyas Bhat, and Dave Dykstra",
        "link": "http://arxiv.org/abs/2503.19768v1",
        "abstract": "Fermilab is transitioning authentication and authorization for grid\noperations to using bearer tokens based on the WLCG Common JWT (JSON Web Token)\nProfile. One of the functionalities that Fermilab experimenters rely on is the\nability to automate batch job submission, which in turn depends on the ability\nto securely refresh and distribute the necessary credentials to experiment job\nsubmit points. Thus, with the transition to using tokens for grid operations,\nwe needed to create a service that would obtain, refresh, and distribute tokens\nfor experimenters' use. This service would avoid the need for experimenters to\nbe experts in obtaining their own tokens and would better protect the most\nsensitive long-lived credentials. Further, the service needed to be widely\nscalable, as Fermilab hosts many experiments, each of which would need their\nown credentials. To address these issues, we created and deployed a Managed\nTokens Service. The service is written in Go, taking advantage of that\nlanguage's native concurrency primitives to easily be able to scale operations\nas we onboard experiments. The service uses as its first credentials a set of\nkerberos keytabs, stored on the same secure machine that the Managed Tokens\nservice runs on. These kerberos credentials allow the service to use htgettoken\nvia condor_vault_storer to store vault tokens in the HTCondor credential\nmanagers (credds) that run on the batch system scheduler machines (HTCondor\nschedds); as well as downloading a local, shorter-lived copy of the vault\ntoken. The kerberos credentials are then also used to distribute copies of the\nlocally-stored vault tokens to experiment submit points."
    },
    {
        "date": "2025-03",
        "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion",
        "author": "Pihai Sun, Junjun Jiang, Yuanqi Yao, Youyu Chen, Wenbo Zhao, Kui Jiang, and Xianming Liu",
        "link": "http://arxiv.org/abs/2503.19739v2",
        "abstract": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
    },
    {
        "date": "2025-03",
        "title": "On What Depends the Robustness of Multi-source Models to Missing Data in Earth Observation?",
        "author": "Francisco Mena, Diego Arenas, Miro Miranda, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2503.19719v1",
        "abstract": "In recent years, the development of robust multi-source models has emerged in\nthe Earth Observation (EO) field. These are models that leverage data from\ndiverse sources to improve predictive accuracy when there is missing data.\nDespite these advancements, the factors influencing the varying effectiveness\nof such models remain poorly understood. In this study, we evaluate the\npredictive performance of six state-of-the-art multi-source models in\npredicting scenarios where either a single data source is missing or only a\nsingle source is available. Our analysis reveals that the efficacy of these\nmodels is intricately tied to the nature of the task, the complementarity among\ndata sources, and the model design. Surprisingly, we observe instances where\nthe removal of certain data sources leads to improved predictive performance,\nchallenging the assumption that incorporating all available data is always\nbeneficial. These findings prompt critical reflections on model complexity and\nthe necessity of all collected data sources, potentially shaping the way for\nmore streamlined approaches in EO applications."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Graphical Lasso: A Robust Scheme for Non-Stationary Mean Data",
        "author": "Samuel Rey, Ernesto Curbelo, Luca Martino, Fernando Llorente, and Antonio G. Marques",
        "link": "http://arxiv.org/abs/2503.19651v1",
        "abstract": "This work addresses the problem of graph learning from data following a\nGaussian Graphical Model (GGM) with a time-varying mean. Graphical Lasso (GL),\nthe standard method for estimating sparse precision matrices, assumes that the\nobserved data follows a zero-mean Gaussian distribution. However, this\nassumption is often violated in real-world scenarios where the mean evolves\nover time due to external influences, trends, or regime shifts. When the mean\nis not properly accounted for, applying GL directly can lead to estimating a\nbiased precision matrix, hence hindering the graph learning task. To overcome\nthis limitation, we propose Graphical Lasso with Adaptive Targeted Adaptive\nImportance Sampling (GL-ATAIS), an iterative method that jointly estimates the\ntime-varying mean and the precision matrix. Our approach integrates Bayesian\ninference with frequentist estimation, leveraging importance sampling to obtain\nan estimate of the mean while using a regularized maximum likelihood estimator\nto infer the precision matrix. By iteratively refining both estimates, GL-ATAIS\nmitigates the bias introduced by time-varying means, leading to more accurate\ngraph recovery. Our numerical evaluation demonstrates the impact of properly\naccounting for time-dependent means and highlights the advantages of GL-ATAIS\nover standard GL in recovering the true graph structure."
    },
    {
        "date": "2025-03",
        "title": "Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs",
        "author": "J\u00e9r\u00e9my Thibault, Joseph Lenormand, and Catalin Hritcu",
        "link": "http://arxiv.org/abs/2503.19609v1",
        "abstract": "Researchers aim to build secure compilation chains enforcing that if there is\nno attack a source context can mount against a source program then there is\nalso no attack an adversarial target context can mount against the compiled\nprogram. Proving that these compilation chains are secure is, however,\nchallenging, and involves a non-trivial back-translation step: for any attack a\ntarget context mounts against the compiled program one has to exhibit a source\ncontext mounting the same attack against the source program. We describe a\nnovel back-translation technique, which results in simpler proofs that can be\nmore easily mechanized in a proof assistant. Given a finite set of finite trace\nprefixes, capturing the interaction recorded during an attack between a target\ncontext and the compiled program, we build a call-return tree that we\nback-translate into a source context producing the same trace prefixes. We use\nstate in the generated source context to record the current location in the\ncall-return tree. The back-translation is done in several small steps, each\nadding to the tree new information describing how the location should change\ndepending on how the context regains control. To prove this back-translation\ncorrect we give semantics to every intermediate call-return tree language,\nusing ghost state to store information and explicitly enforce execution\ninvariants. We prove several small forward simulations, basically seeing the\nback-translation as a verified nanopass compiler. Thanks to this modular\nstructure, we are able to mechanize this complex back-translation and its\ncorrectness proof in the Rocq prover without too much effort."
    },
    {
        "date": "2025-03",
        "title": "Boosting the Transferability of Audio Adversarial Examples with Acoustic Representation Optimization",
        "author": "Weifei Jin, Junjie Su, Hejia Wang, Yulin Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2503.19591v1",
        "abstract": "With the widespread application of automatic speech recognition (ASR)\nsystems, their vulnerability to adversarial attacks has been extensively\nstudied. However, most existing adversarial examples are generated on specific\nindividual models, resulting in a lack of transferability. In real-world\nscenarios, attackers often cannot access detailed information about the target\nmodel, making query-based attacks unfeasible. To address this challenge, we\npropose a technique called Acoustic Representation Optimization that aligns\nadversarial perturbations with low-level acoustic characteristics derived from\nspeech representation models. Rather than relying on model-specific,\nhigher-layer abstractions, our approach leverages fundamental acoustic\nrepresentations that remain consistent across diverse ASR architectures. By\nenforcing an acoustic representation loss to guide perturbations toward these\nrobust, lower-level representations, we enhance the cross-model transferability\nof adversarial examples without degrading audio quality. Our method is\nplug-and-play and can be integrated with any existing attack methods. We\nevaluate our approach on three modern ASR models, and the experimental results\ndemonstrate that our method significantly improves the transferability of\nadversarial examples generated by previous methods while preserving the audio\nquality."
    },
    {
        "date": "2025-03",
        "title": "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models",
        "author": "Dahyun Jung, Seungyoon Lee, Hyeonseok Moon, Chanjun Park, and Heuiseok Lim",
        "link": "http://arxiv.org/abs/2503.19540v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced interactions between users and models. These advancements concurrently\nunderscore the need for rigorous safety evaluations due to the manifestation of\nsocial biases, which can lead to harmful societal impacts. Despite these\nconcerns, existing benchmarks may overlook the intrinsic weaknesses of LLMs,\nwhich can generate biased responses even with simple adversarial instructions.\nTo address this critical gap, we introduce a new benchmark, Fairness Benchmark\nin LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can\nsustain fairness even when exposed to prompts constructed to induce bias. To\nthoroughly evaluate the robustness of LLMs, we integrate prompts that amplify\npotential biases into the fairness assessment. Comparative experiments between\nFLEX and existing benchmarks demonstrate that traditional evaluations may\nunderestimate the inherent risks in models. This highlights the need for more\nstringent LLM evaluation benchmarks to guarantee safety and fairness."
    },
    {
        "date": "2025-03",
        "title": "Towards Imperceptible Adversarial Attacks for Time Series Classification with Local Perturbations and Frequency Analysis",
        "author": "Wenwei Gu, Renyi Zhong, Jianping Zhang, and Michael R. Lyu",
        "link": "http://arxiv.org/abs/2503.19519v1",
        "abstract": "Adversarial attacks in time series classification (TSC) models have recently\ngained attention due to their potential to compromise model robustness.\nImperceptibility is crucial, as adversarial examples detected by the human\nvision system (HVS) can render attacks ineffective. Many existing methods fail\nto produce high-quality imperceptible examples, often generating perturbations\nwith more perceptible low-frequency components, like square waves, and global\nperturbations that reduce stealthiness. This paper aims to improve the\nimperceptibility of adversarial attacks on TSC models by addressing frequency\ncomponents and time series locality. We propose the Shapelet-based\nFrequency-domain Attack (SFAttack), which uses local perturbations focused on\ntime series shapelets to enhance discriminative information and stealthiness.\nAdditionally, we introduce a low-frequency constraint to confine perturbations\nto high-frequency components, enhancing imperceptibility."
    },
    {
        "date": "2025-03",
        "title": "SparSamp: Efficient Provably Secure Steganography Based on Sparse Sampling",
        "author": "Yaofei Wang, Gang Pei, Kejiang Chen, Jinyang Ding, Chao Pan, Weilong Pang, Donghui Hu, and Weiming Zhang",
        "link": "http://arxiv.org/abs/2503.19499v1",
        "abstract": "Steganography embeds confidential data within seemingly innocuous\ncommunications. Provable security in steganography, a long-sought goal, has\nbecome feasible with deep generative models. However, existing methods face a\ncritical trade-off between security and efficiency. This paper introduces\nSparSamp, an efficient provably secure steganography method based on sparse\nsampling. SparSamp embeds messages by combining them with pseudo-random numbers\nto obtain message-derived random numbers for sampling. It enhances extraction\naccuracy and embedding capacity by increasing the sampling intervals and making\nthe sampling process sparse. SparSamp preserves the original probability\ndistribution of the generative model, thus ensuring security. It introduces\nonly $O(1)$ additional complexity per sampling step, enabling the fastest\nembedding speed without compromising generation speed. SparSamp is designed to\nbe plug-and-play; message embedding can be achieved by simply replacing the\nsampling component of an existing generative model with SparSamp. We\nimplemented SparSamp in text, image, and audio generation models. It can\nachieve embedding speeds of up to 755 bits/second with GPT-2, 5046 bits/second\nwith DDPM, and 9,223 bits/second with WaveRNN."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Time-of-Flight Depth Denoising with Confidence-Aware Diffusion Model",
        "author": "Changyong He, Jin Zeng, Jiawei Zhang, and Jiajie Guo",
        "link": "http://arxiv.org/abs/2503.19448v1",
        "abstract": "Time-of-Flight (ToF) sensors efficiently capture scene depth, but the\nnonlinear depth construction procedure often results in extremely large noise\nvariance or even invalid areas. Recent methods based on deep neural networks\n(DNNs) achieve enhanced ToF denoising accuracy but tend to struggle when\npresented with severe noise corruption due to limited prior knowledge of ToF\ndata distribution. In this paper, we propose DepthCAD, a novel ToF denoising\napproach that ensures global structural smoothness by leveraging the rich prior\nknowledge in Stable Diffusion and maintains local metric accuracy by steering\nthe diffusion process with confidence guidance. To adopt the pretrained image\ndiffusion model to ToF depth denoising, we apply the diffusion on raw ToF\ncorrelation measurements with dynamic range normalization before converting to\ndepth maps. Experimental results validate the state-of-the-art performance of\nthe proposed scheme, and the evaluation on real data further verifies its\nrobustness against real-world ToF noise."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
        "author": "Hengyu Wu, and Yang Cao",
        "link": "http://arxiv.org/abs/2503.19338v1",
        "abstract": "The adoption of the Large Language Model (LLM) has accelerated dramatically\nsince the ChatGPT from OpenAI went online in November 2022. Recent advances in\nLarge Multimodal Models (LMMs), which process diverse data types and enable\ninteraction through various channels, have expanded beyond the text-to-text\nlimitations of early LLMs, attracting significant and concurrent attention from\nboth researchers and industry. While LLMs and LMMs are starting to spread\nwidely, concerns about their privacy risks are increasing as well. Membership\nInference Attacks (MIAs), techniques used to determine whether a particular\ndata point was part of a model's training set, serve as a key metric for\nassessing the privacy vulnerabilities of machine learning models. Hu et al.\nshow that various machine learning algorithms are vulnerable to MIA. Despite\nextensive studies on MIAs in traditional models, there remains a lack of\nsystematic surveys addressing their effectiveness and implications in modern\nlarge-scale models like LLMs and LMMs. In this paper, we systematically\nreviewed recent studies of MIA against LLMs and LMMs. We analyzed and\ncategorized each attack based on their methodology and scenario and discussed\nthe limitations in existing research. Additionally, we examine privacy concerns\nassociated with the fine-tuning process. Finally, we provided some suggestions\nfor future research in this direction."
    },
    {
        "date": "2025-03",
        "title": "Efficient Adversarial Detection Frameworks for Vehicle-to-Microgrid Services in Edge Computing",
        "author": "Ahmed Omara, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2503.19318v1",
        "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into\nmicrogrid control systems, the risk of malicious actors exploiting\nvulnerabilities in Machine Learning (ML) algorithms to disrupt power generation\nand distribution grows. Detection models to identify adversarial attacks need\nto meet the constraints of edge environments, where computational power and\nmemory are often limited. To address this issue, we propose a novel strategy\nthat optimizes detection models for Vehicle-to-Microgrid (V2M) edge\nenvironments without compromising performance against inference and evasion\nattacks. Our approach integrates model design and compression into a unified\nprocess and results in a highly compact detection model that maintains high\naccuracy. We evaluated our method against four benchmark evasion attacks-Fast\nGradient Sign Method (FGSM), Basic Iterative Method (BIM), Carlini & Wagner\nmethod (C&W) and Conditional Generative Adversarial Network (CGAN) method-and\ntwo knowledge-based attacks, white-box and gray-box. Our optimized model\nreduces memory usage from 20MB to 1.3MB, inference time from 3.2 seconds to 0.9\nseconds, and GPU utilization from 5% to 2.68%."
    },
    {
        "date": "2025-03",
        "title": "SoK: How Robust is Audio Watermarking in Generative AI models?",
        "author": "Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, and Qiben Yan",
        "link": "http://arxiv.org/abs/2503.19176v2",
        "abstract": "Audio watermarking is increasingly used to verify the provenance of\nAI-generated content, enabling applications such as detecting AI-generated\nspeech, protecting music IP, and defending against voice cloning. To be\neffective, audio watermarks must resist removal attacks that distort signals to\nevade detection. While many schemes claim robustness, these claims are\ntypically tested in isolation and against a limited set of attacks. A\nsystematic evaluation against diverse removal attacks is lacking, hindering\npractical deployment. In this paper, we investigate whether recent watermarking\nschemes that claim robustness can withstand a broad range of removal attacks.\nFirst, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we\nsummarize their underlying technologies and potential vulnerabilities. We then\npresent a large-scale empirical study to assess their robustness. To support\nthis, we build an evaluation framework encompassing 22 types of removal attacks\n(109 configurations) including signal-level, physical-level, and AI-induced\ndistortions. We reproduce 9 watermarking schemes using open-source code,\nidentify 8 new highly effective attacks, and highlight 11 key findings that\nexpose the fundamental limitations of these methods across 3 public datasets.\nOur results reveal that none of the surveyed schemes can withstand all tested\ndistortions. This evaluation offers a comprehensive view of how current\nwatermarking methods perform under real-world threats. Our demo and code are\navailable at https://sokaudiowm.github.io/."
    },
    {
        "date": "2025-03",
        "title": "MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks",
        "author": "Wenhao You, Bryan Hooi, Yiwei Wang, Youke Wang, Zong Ke, Ming-Hsuan Yang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.19134v1",
        "abstract": "While safety mechanisms have significantly progressed in filtering harmful\ntext inputs, MLLMs remain vulnerable to multimodal jailbreaks that exploit\ntheir cross-modal reasoning capabilities. We present MIRAGE, a novel multimodal\njailbreak framework that exploits narrative-driven context and role immersion\nto circumvent safety mechanisms in Multimodal Large Language Models (MLLMs). By\nsystematically decomposing the toxic query into environment, role, and action\ntriplets, MIRAGE constructs a multi-turn visual storytelling sequence of images\nand text using Stable Diffusion, guiding the target model through an engaging\ndetective narrative. This process progressively lowers the model's defences and\nsubtly guides its reasoning through structured contextual cues, ultimately\neliciting harmful responses. In extensive experiments on the selected datasets\nwith six mainstream MLLMs, MIRAGE achieves state-of-the-art performance,\nimproving attack success rates by up to 17.5% over the best baselines.\nMoreover, we demonstrate that role immersion and structured semantic\nreconstruction can activate inherent model biases, facilitating the model's\nspontaneous violation of ethical safeguards. These results highlight critical\nweaknesses in current multimodal safety mechanisms and underscore the urgent\nneed for more robust defences against cross-modal threats."
    },
    {
        "date": "2025-03",
        "title": "Paving the way for scientific foundation models: enhancing generalization and robustness in PDEs with constraint-aware pre-training",
        "author": "Amin Totounferoush, Serge Kotchourko, Michael W. Mahoney, and Steffen Staab",
        "link": "http://arxiv.org/abs/2503.19081v1",
        "abstract": "Partial differential equations (PDEs) govern a wide range of physical\nsystems, but solving them efficiently remains a major challenge. The idea of a\nscientific foundation model (SciFM) is emerging as a promising tool for\nlearning transferable representations across diverse domains. However, SciFMs\nrequire large amounts of solution data, which may be scarce or computationally\nexpensive to generate. To maximize generalization while reducing data\ndependence, we propose incorporating PDE residuals into pre-training either as\nthe sole learning signal or in combination with data loss to compensate for\nlimited or infeasible training data. We evaluate this constraint-aware\npre-training across three key benchmarks: (i) generalization to new physics,\nwhere material properties, e.g., the diffusion coefficient, is shifted with\nrespect to the training distribution; (ii) generalization to entirely new PDEs,\nrequiring adaptation to different operators; and (iii) robustness against noisy\nfine-tuning data, ensuring stability in real-world applications. Our results\nshow that pre-training with PDE constraints significantly enhances\ngeneralization, outperforming models trained solely on solution data across all\nbenchmarks. These findings prove the effectiveness of our proposed\nconstraint-aware pre-training as a crucial component for SciFMs, providing a\nscalable approach to data-efficient, generalizable PDE solvers."
    },
    {
        "date": "2025-03",
        "title": "Graph-Level Label-Only Membership Inference Attack against Graph Neural Networks",
        "author": "Jiazhu Dai, and Yubing Lu",
        "link": "http://arxiv.org/abs/2503.19070v2",
        "abstract": "Graph neural networks (GNNs) are widely used for graph-structured data but\nare vulnerable to membership inference attacks (MIAs) in graph classification\ntasks, which determine if a graph was part of the training dataset, potentially\ncausing data leakage. Existing MIAs rely on prediction probability vectors, but\nthey become ineffective when only prediction labels are available. We propose a\nGraph-level Label-Only Membership Inference Attack (GLO-MIA), which is based on\nthe intuition that the target model's predictions on training data are more\nstable than those on testing data. GLO-MIA generates a set of perturbed graphs\nfor target graph by adding perturbations to its effective features and queries\nthe target model with the perturbed graphs to get their prediction labels,\nwhich are then used to calculate robustness score of the target graph. Finally,\nby comparing the robustness score with a predefined threshold, the membership\nof the target graph can be inferred correctly with high probability. Our\nevaluation on three datasets and four GNN models shows that GLO-MIA achieves an\nattack accuracy of up to 0.825, outperforming baseline work by 8.5% and closely\nmatching the performance of probability-based MIAs, even with only prediction\nlabels."
    },
    {
        "date": "2025-03",
        "title": "strideSEA: A STRIDE-centric Security Evaluation Approach",
        "author": "Alvi Jawad, Jason Jaskolka, Ashraf Matrawy, and Mohamed Ibnkahla",
        "link": "http://arxiv.org/abs/2503.19030v1",
        "abstract": "Microsoft's STRIDE methodology is at the forefront of threat modeling,\nsupporting the increasingly critical quality attribute of security in\nsoftware-intensive systems. However, in a comprehensive security evaluation\nprocess, the general consensus is that the STRIDE classification is only useful\nfor threat elicitation, isolating threat modeling from the other security\nevaluation activities involved in a secure software development life cycle\n(SDLC). We present strideSEA, a STRIDE-centric Security Evaluation Approach\nthat integrates STRIDE as the central classification scheme into the security\nactivities of threat modeling, attack scenario analysis, risk analysis, and\ncountermeasure recommendation that are conducted alongside software engineering\nactivities in secure SDLCs. The application of strideSEA is demonstrated in a\nreal-world online immunization system case study. Using STRIDE as a single\nunifying thread, we bind existing security evaluation approaches in the four\nsecurity activities of strideSEA to analyze (1) threats using Microsoft threat\nmodeling tool, (2) attack scenarios using attack trees, (3) systemic risk using\nNASA's defect detection and prevention (DDP) technique, and (4) recommend\ncountermeasures based on their effectiveness in reducing the most critical\nrisks using DDP. The results include a detailed quantitative assessment of the\nsecurity of the online immunization system with a clear definition of the role\nand advantages of integrating STRIDE in the evaluation process. Overall, the\nunified approach in strideSEA enables a more structured security evaluation\nprocess, allowing easier identification and recommendation of countermeasures,\nthus supporting the security requirements and eliciting design considerations,\ninforming the software development life cycle of future software-based\ninformation systems."
    },
    {
        "date": "2025-03",
        "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
        "author": "Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, and Sahin Albayrak",
        "link": "http://arxiv.org/abs/2503.18903v1",
        "abstract": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications."
    },
    {
        "date": "2025-03",
        "title": "Secure Edge Computing Reference Architecture for Data-driven Structural Health Monitoring: Lessons Learned from Implementation and Benchmarking",
        "author": "Sheikh Muhammad Farjad, Sandeep Reddy Patllola, Yonas Kassa, George Grispos, and Robin Gandhi",
        "link": "http://arxiv.org/abs/2503.18857v1",
        "abstract": "Structural Health Monitoring (SHM) plays a crucial role in maintaining aging\nand critical infrastructure, supporting applications such as smart cities and\ndigital twinning. These applications demand machine learning models capable of\nprocessing large volumes of real-time sensor data at the network edge. However,\nexisting approaches often neglect the challenges of deploying machine learning\nmodels at the edge or are constrained by vendor-specific platforms. This paper\nintroduces a scalable and secure edge-computing reference architecture tailored\nfor data-driven SHM. We share practical insights from deploying this\narchitecture at the Memorial Bridge in New Hampshire, US, referred to as the\nLiving Bridge project. Our solution integrates a commercial data acquisition\nsystem with off-the-shelf hardware running an open-source edge-computing\nplatform, remotely managed and scaled through cloud services. To support the\ndevelopment of data-driven SHM systems, we propose a resource consumption\nbenchmarking framework called edgeOps to evaluate the performance of machine\nlearning models on edge devices. We study this framework by collecting resource\nutilization data for machine learning models typically used in SHM applications\non two different edge computing hardware platforms. edgeOps was specifically\nstudied on off-the-shelf Linux and ARM-based edge devices. Our findings\ndemonstrate the impact of platform and model selection on system performance,\nproviding actionable guidance for edge-based SHM system design."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
        "author": "Wenxi Chen, Raymond A. Yeh, Shaoshuai Mou, and Yan Gu",
        "link": "http://arxiv.org/abs/2503.18784v1",
        "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles",
        "author": "Der-Hau Lee",
        "link": "http://arxiv.org/abs/2503.18752v1",
        "abstract": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers."
    },
    {
        "date": "2025-03",
        "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting",
        "author": "Lijiang Li, Jinglu Wang, Xiang Ming, and Yan Lu",
        "link": "http://arxiv.org/abs/2503.18718v1",
        "abstract": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time."
    },
    {
        "date": "2025-03",
        "title": "Robust face recognition based on the wing loss and the $\\ell_1$ regularization",
        "author": "Yaoyao Yun, and Jianwen Xu",
        "link": "http://arxiv.org/abs/2503.18652v1",
        "abstract": "In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition."
    },
    {
        "date": "2025-03",
        "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling",
        "author": "Kunyang Li, and Ming Hou",
        "link": "http://arxiv.org/abs/2503.18631v1",
        "abstract": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions."
    },
    {
        "date": "2025-03",
        "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
        "author": "Hadi Mohammadi, Ehsan Nazerfard, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2503.18569v1",
        "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning."
    },
    {
        "date": "2025-03",
        "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
        "author": "Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, and Luping Zhou",
        "link": "http://arxiv.org/abs/2503.18536v1",
        "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module."
    },
    {
        "date": "2025-03",
        "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
        "author": "Jiate Li, Meng Pang, Yun Dong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.18503v1",
        "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training."
    },
    {
        "date": "2025-03",
        "title": "Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study",
        "author": "Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.18487v1",
        "abstract": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
        "author": "Chenfei Liao, Kaiyu Lei, Xu Zheng, Junha Moon, Zhixiong Wang, Yixuan Wang, Danda Pani Paudel, Luc Van Gool, and Xuming Hu",
        "link": "http://arxiv.org/abs/2503.18445v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "author": "Wen Bai, Yi Wong, Xiao Qiao, and Chin Pang Ho",
        "link": "http://arxiv.org/abs/2503.18436v1",
        "abstract": "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity."
    },
    {
        "date": "2025-03",
        "title": "RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data",
        "author": "Xudong Mou, Rui Wang, Bo Li, Tianyu Wo, Jie Sun, Hui Wang, and Xudong Liu",
        "link": "http://arxiv.org/abs/2503.18385v1",
        "abstract": "The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https://github.com/ruiking04/RoCA."
    },
    {
        "date": "2025-03",
        "title": "Attacking and Improving the Tor Directory Protocol",
        "author": "Zhongtang Luo, Adithya Bhat, Kartik Nayak, and Aniket Kate",
        "link": "http://arxiv.org/abs/2503.18345v1",
        "abstract": "The Tor network enhances clients' privacy by routing traffic through an\noverlay network of volunteered intermediate relays. Tor employs a distributed\nprotocol among nine hard-coded Directory Authority (DA) servers to securely\ndisseminate information about these relays to produce a new consensus document\nevery hour. With a straightforward voting mechanism to ensure consistency, the\nprotocol is expected to be secure even when a minority of those authorities get\ncompromised. However, the current consensus protocol is flawed: it allows an\nequivocation attack that enables only a single compromised authority to create\na valid consensus document with malicious relays. Importantly the vulnerability\nis not innocuous: We demonstrate that the compromised authority can effectively\ntrick a targeted client into using the equivocated consensus document in an\nundetectable manner. Moreover, even if we have archived Tor consensus documents\navailable since its beginning, we cannot be sure that no client was ever\ntricked.\n  We propose a two-stage solution to deal with this exploit. In the short term,\nwe have developed and deployed TorEq, a monitor to detect such exploits\nreactively: the Tor clients can refer to the monitor before updating the\nconsensus to ensure no equivocation. To solve the problem proactively, we first\ndefine the Tor DA consensus problem as the interactive consistency (IC) problem\nfrom the distributed computing literature. We then design DirCast, a novel\nsecure Byzantine Broadcast protocol that requires minimal code change from the\ncurrent Tor DA code base. Our protocol has near-optimal efficiency that uses\noptimistically five rounds and at most nine rounds to reach an agreement in the\ncurrent nine-authority system. We are communicating with the Tor security team\nto incorporate the solutions into the Tor project."
    },
    {
        "date": "2025-03",
        "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
        "author": "Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, and Tsuyoshi Takatani",
        "link": "http://arxiv.org/abs/2503.18341v1",
        "abstract": "Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning."
    },
    {
        "date": "2025-03",
        "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD",
        "author": "Paul K. Mandal",
        "link": "http://arxiv.org/abs/2503.18290v1",
        "abstract": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences."
    },
    {
        "date": "2025-03",
        "title": "Enhance GNNs with Reliable Confidence Estimation via Adversarial Calibration Learning",
        "author": "Yilong Wang, Jiahao Zhang, Tianxiang Zhao, and Suhang Wang",
        "link": "http://arxiv.org/abs/2503.18235v1",
        "abstract": "Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios."
    },
    {
        "date": "2025-03",
        "title": "Literature Review: Cyber Security Monitoring in Maritime",
        "author": "Risto Vaarandi, Leonidas Tsiopoulos, Gabor Visky, Muaan Ur Rehman, and Hayretdin Bahsi",
        "link": "http://arxiv.org/abs/2503.18173v1",
        "abstract": "In recent years, many cyber incidents have happened in the maritime sector,\ntargeting the information technology (IT) and operational technology (OT)\ninfrastructure. Although several systematization-of-knowledge papers have been\npublished in the maritime field, none of the previous studies has focused on\ncyber security monitoring, which aims at timely detection of cyber attacks with\nautomated methods. The current article addresses this research gap and surveys\nthe methods, algorithms, tools and architectures used for cyber security\nmonitoring in the maritime sector. For the survey, a systematic literature\nreview of cyber security monitoring studies is conducted in this article,\nfollowing the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) protocol. The first contribution of this article is the\nbibliometric analysis of related literature and the identification of the main\nresearch themes in previous works. For that purpose, our article presents a\ntaxonomy for existing studies which highlights the main properties of maritime\ncyber security monitoring research. The second contribution of this article is\nan in-depth analysis of previous works and the identification of research gaps\nand limitations in existing literature. Based on our findings, we outline\nfuture research directions for cyber security monitoring in the maritime field."
    },
    {
        "date": "2025-03",
        "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
        "author": "Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, and An-An Liu",
        "link": "http://arxiv.org/abs/2503.17987v1",
        "abstract": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}"
    },
    {
        "date": "2025-03",
        "title": "Generative Data Imputation for Sparse Learner Performance Data Using Generative Adversarial Imputation Networks",
        "author": "Liang Zhang, Jionghao Lin, John Sabatini, Diego Zapata-Rivera, Carol Forsyth, Yang Jiang, John Hollander, Xiangen Hu, and Arthur C. Graesser",
        "link": "http://arxiv.org/abs/2503.18982v1",
        "abstract": "Learner performance data collected by Intelligent Tutoring Systems (ITSs),\nsuch as responses to questions, is essential for modeling and predicting\nlearners' knowledge states. However, missing responses due to skips or\nincomplete attempts create data sparsity, challenging accurate assessment and\npersonalized instruction. To address this, we propose a generative imputation\napproach using Generative Adversarial Imputation Networks (GAIN). Our method\nfeatures a three-dimensional (3D) framework (learners, questions, and\nattempts), flexibly accommodating various sparsity levels. Enhanced by\nconvolutional neural networks and optimized with a least squares loss function,\nthe GAIN-based method aligns input and output dimensions to question-attempt\nmatrices along the learners' dimension. Extensive experiments using datasets\nfrom AutoTutor Adult Reading Comprehension (ARC), ASSISTments, and MATHia\ndemonstrate that our approach significantly outperforms tensor factorization\nand alternative GAN methods in imputation accuracy across different attempt\nscenarios. Bayesian Knowledge Tracing (BKT) further validates the effectiveness\nof the imputed data by estimating learning parameters: initial knowledge\n(P(L0)), learning rate (P(T)), guess rate (P(G)), and slip rate (P(S)). Results\nindicate the imputed data enhances model fit and closely mirrors original\ndistributions, capturing underlying learning behaviors reliably.\nKullback-Leibler (KL) divergence assessments confirm minimal divergence,\nshowing the imputed data preserves essential learning characteristics\neffectively. These findings underscore GAIN's capability as a robust imputation\ntool in ITSs, alleviating data sparsity and supporting adaptive, individualized\ninstruction, ultimately leading to more precise and responsive learner\nassessments and improved educational outcomes."
    },
    {
        "date": "2025-03",
        "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
        "author": "Dong Zhao, Jinlong Li, Shuang Wang, Mengyao Wu, Qi Zang, Nicu Sebe, and Zhun Zhong",
        "link": "http://arxiv.org/abs/2503.17940v2",
        "abstract": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."
    },
    {
        "date": "2025-03",
        "title": "Understanding and Mitigating Side and Covert Channel Vulnerabilities Introduced by RowHammer Defenses",
        "author": "F. Nisa Bostanc\u0131, O\u011fuzhan Canpolat, Ataberk Olgun, \u0130smail Emir Y\u00fcksel, Mohammad Sadrosadati, A. Giray Ya\u011fl\u0131k\u00e7\u0131, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2503.17891v1",
        "abstract": "DRAM chips are vulnerable to read disturbance phenomena (e.g., RowHammer and\nRowPress), where repeatedly accessing or keeping open a DRAM row causes\nbitflips in nearby rows, due to DRAM density scaling. Attackers can leverage\nRowHammer bitflips in real systems to take over systems and leak data.\nConsequently, many prior works propose mitigations, including recent DDR\nspecifications introducing new mitigation frameworks (e.g., PRAC and RFM). For\nrobustness, it is timely and critical to analyze other security implications\nthat widely-adopted RowHammer mitigations can introduce. Unfortunately, no\nprior work analyzes the timing channel vulnerabilities introduced by RowHammer\nmitigations. In this work, we present the first analysis and evaluation of\ntiming channel vulnerabilities introduced by RowHammer mitigations. Our key\nobservation is that RowHammer mitigations' preventive actions have two features\nthat enable timing channels. First, preventive actions often reduce DRAM\nbandwidth availability because they block access to DRAM, thereby delaying\nregular memory requests and resulting in increased memory latencies. Second,\npreventive actions can be triggered on demand as they depend on memory access\npatterns. We systematically analyze two latest industry mitigations and\nintroduce LeakyHammer, a new class of attacks that leverage the RowHammer\nmitigation-induced memory latency differences to establish communication\nchannels between processes and leak secrets. First, we build two covert channel\nattacks exploiting two state-of-the-art RowHammer mitigations, providing 41.9\nKbps and 54.0 Kbps channel capacity. Second, we demonstrate a proof-of-concept\nwebsite fingerprinting attack that can identify visited websites based on the\nRowHammer mitigation behavior. We discuss 3 mitigations against LeakyHammer and\nshow that fundamentally mitigating LeakyHammer induces significant performance\noverheads."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Mitigating DDoS Attacks with AI: A Survey",
        "author": "Alexandru Apostu, Silviu Gheorghe, Andrei H\u00eeji, Nicolae Cleju, Andrei P\u0103tra\u015fcu, Cristian Rusu, Radu Ionescu, and Paul Irofti",
        "link": "http://arxiv.org/abs/2503.17867v1",
        "abstract": "Distributed Denial of Service attacks represent an active cybersecurity\nresearch problem. Recent research shifted from static rule-based defenses\ntowards AI-based detection and mitigation. This comprehensive survey covers\nseveral key topics. Preeminently, state-of-the-art AI detection methods are\ndiscussed. An in-depth taxonomy based on manual expert hierarchies and an\nAI-generated dendrogram are provided, thus settling DDoS categorization\nambiguities. An important discussion on available datasets follows, covering\ndata format options and their role in training AI detection methods together\nwith adversarial training and examples augmentation. Beyond detection, AI based\nmitigation techniques are surveyed as well. Finally, multiple open research\ndirections are proposed."
    },
    {
        "date": "2025-03",
        "title": "NVBleed: Covert and Side-Channel Attacks on NVIDIA Multi-GPU Interconnect",
        "author": "Yicheng Zhang, Ravan Nazaraliyev, Sankha Baran Dutta, Andres Marquez, Kevin Barker, and Nael Abu-Ghazaleh",
        "link": "http://arxiv.org/abs/2503.17847v1",
        "abstract": "Multi-GPU systems are becoming increasingly important in highperformance\ncomputing (HPC) and cloud infrastructure, providing acceleration for\ndata-intensive applications, including machine learning workloads. These\nsystems consist of multiple GPUs interconnected through high-speed networking\nlinks such as NVIDIA's NVLink. In this work, we explore whether the\ninterconnect on such systems can offer a novel source of leakage, enabling new\nforms of covert and side-channel attacks. Specifically, we reverse engineer the\noperations of NVlink and identify two primary sources of leakage: timing\nvariations due to contention and accessible performance counters that disclose\ncommunication patterns. The leakage is visible remotely and even across VM\ninstances in the cloud, enabling potentially dangerous attacks. Building on\nthese observations, we develop two types of covert-channel attacks across two\nGPUs, achieving a bandwidth of over 70 Kbps with an error rate of 4.78% for the\ncontention channel. We develop two end-to-end crossGPU side-channel attacks:\napplication fingerprinting (including 18 high-performance computing and deep\nlearning applications) and 3D graphics character identification within Blender,\na multi-GPU rendering application. These attacks are highly effective,\nachieving F1 scores of up to 97.78% and 91.56%, respectively. We also discover\nthat leakage surprisingly occurs across Virtual Machines on the Google Cloud\nPlatform (GCP) and demonstrate a side-channel attack on Blender, achieving F1\nscores exceeding 88%. We also explore potential defenses such as managing\naccess to counters and reducing the resolution of the clock to mitigate the two\nsources of leakage."
    },
    {
        "date": "2025-03",
        "title": "Connectedness: a dimension of security bug severity assessment for measuring uncertainty",
        "author": "Shue Long Chan",
        "link": "http://arxiv.org/abs/2503.17813v1",
        "abstract": "Current frameworks for evaluating security bug severity, such as the Common\nVulnerability Scoring System (CVSS), prioritize the ratio of exploitability to\nimpact. This paper suggests that the above approach measures the \"known knowns\"\nbut inadequately addresses the \"known unknowns\" especially when there exist\nmultiple possible exploit paths and side effects, which introduce significant\nuncertainty. This paper introduces the concept of connectedness, which measures\nhow strongly a security bug is connected with different entities, thereby\nreflecting the uncertainty of impact and the exploit potential. This work\nhighlights the critical but underappreciated role connectedness plays in\nseverity assessments."
    },
    {
        "date": "2025-03",
        "title": "Design and implementation of a novel cryptographically secure pseudorandom number generator",
        "author": "Juan Di Mauro, Eduardo Salazar, and Hugo D. Scolnik",
        "link": "http://arxiv.org/abs/2503.17767v1",
        "abstract": "The aim of this paper is to present a new design for a pseudorandom number\ngenerator (PRNG) that is cryptographically secure, passes all of the usual\nstatistical tests referenced in the literature and hence generates high quality\nrandom sequences, that is compact and easy to implement in practice, of\nportable design and offering reasonable execution times. Our procedure achieves\nthose objectives through the use of a sequence of modular exponentiations\nfollowed by the application of Feistel-like boxes that mix up bits using a\nnonlinear function. The results of extensive statistical tests on sequences of\nabout 2^40 bits in size generated by our algorithm are also presented."
    },
    {
        "date": "2025-03",
        "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "author": "Jie Zhang, Zhongqi Wang, Shiguang Shan, and Xilin Chen",
        "link": "http://arxiv.org/abs/2503.17724v1",
        "abstract": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA."
    },
    {
        "date": "2025-03",
        "title": "Measuring the Robustness of Audio Deepfake Detectors",
        "author": "Xiang Li, Pin-Yu Chen, and Wenqi Wei",
        "link": "http://arxiv.org/abs/2503.17577v1",
        "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings."
    },
    {
        "date": "2025-03",
        "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
        "author": "Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, and Hassan Mansour",
        "link": "http://arxiv.org/abs/2503.17351v1",
        "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."
    },
    {
        "date": "2025-03",
        "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
        "author": "John Naulty, Eason Chen, Joy Wang, George Digkas, and Kostas Chalkias",
        "link": "http://arxiv.org/abs/2503.17302v1",
        "abstract": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
    },
    {
        "date": "2025-03",
        "title": "UAV Resilience Against Stealthy Attacks",
        "author": "Arthur Amorim, Max Taylor, Trevor Kann, Gary T. Leavens, William L. Harrison, and Lance Joneckis",
        "link": "http://arxiv.org/abs/2503.17298v1",
        "abstract": "Unmanned aerial vehicles (UAVs) depend on untrusted software components to\nautomate dangerous or critical missions, making them a desirable target for\nattacks. Some work has been done to prevent an attacker who has either\ncompromised a ground control station or parts of a UAV's software from\nsabotaging the vehicle, but not both. We present an architecture running a UAV\nsoftware stack with runtime monitoring and seL4-based software isolation that\nprevents attackers from both exploiting software bugs and utilizing stealthy\nattacks. Our architecture retrofits legacy UAVs and secures the popular MAVLink\nprotocol, making wide adoption possible."
    },
    {
        "date": "2025-03",
        "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
        "author": "Jan Rabenseifner, Sven Klaassen, Jannis Kueck, and Philipp Bach",
        "link": "http://arxiv.org/abs/2503.17290v2",
        "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
    },
    {
        "date": "2025-03",
        "title": "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies",
        "author": "Ronan Mouchoux, and Fran\u00e7ois Moerman",
        "link": "http://arxiv.org/abs/2503.17219v1",
        "abstract": "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains."
    },
    {
        "date": "2025-03",
        "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization",
        "author": "Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, and Wang Lu",
        "link": "http://arxiv.org/abs/2503.17211v1",
        "abstract": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."
    },
    {
        "date": "2025-03",
        "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
        "author": "Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, and Ada Sedova",
        "link": "http://arxiv.org/abs/2503.17173v1",
        "abstract": "The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments."
    },
    {
        "date": "2025-03",
        "title": "Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers",
        "author": "Gaojie Jin, Tianjin Huang, Ronghui Mu, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2503.17172v1",
        "abstract": "Recent studies have identified a critical challenge in deep neural networks\n(DNNs) known as ``robust fairness\", where models exhibit significant\ndisparities in robust accuracy across different classes. While prior work has\nattempted to address this issue in adversarial robustness, the study of\nworst-class certified robustness for smoothed classifiers remains unexplored.\nOur work bridges this gap by developing a PAC-Bayesian bound for the\nworst-class error of smoothed classifiers. Through theoretical analysis, we\ndemonstrate that the largest eigenvalue of the smoothed confusion matrix\nfundamentally influences the worst-class error of smoothed classifiers. Based\non this insight, we introduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy\nof the smoothed classifier and further improve its worst-class certified\nrobustness. We provide extensive experimental validation across multiple\ndatasets and model architectures to demonstrate the effectiveness of our\napproach."
    },
    {
        "date": "2025-03",
        "title": "Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes",
        "author": "Orkun Furat, Sabrina Weber, Johannes Schubert, Ren\u00e9 Rekers, Maximilian Luczak, Erik Glatt, Andreas Wiegmann, J\u00fcrgen Janek, Anja Bielefeld, and Volker Schmidt",
        "link": "http://arxiv.org/abs/2503.17171v1",
        "abstract": "This paper presents a computational method for generating virtual 3D\nmorphologies of functional materials using low-parametric stochastic geometry\nmodels, i.e., digital twins, calibrated with 2D microscopy images. These\ndigital twins allow systematic parameter variations to simulate various\nmorphologies, that can be deployed for virtual materials testing by means of\nspatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to\ngenerate realistic 3D morphologies. However, GANs often comprise of numerous\nuninterpretable parameters make systematic variation of morphologies for\nvirtual materials testing challenging. In contrast, low-parametric stochastic\ngeometry models (e.g., based on Gaussian random fields) enable targeted\nvariation but may struggle to mimic complex morphologies. Combining GANs with\nadvanced stochastic geometry models (e.g., excursion sets of more general\nrandom fields) addresses these limitations, allowing model calibration solely\nfrom 2D image data. This approach is demonstrated by generating a digital twin\nof all-solid-state battery (ASSB) cathodes. Since the digital twins are\nparametric, they support systematic exploration of structural scenarios and\ntheir macroscopic properties. The proposed method facilitates simulation\nstudies for optimizing 3D morphologies, benefiting not only ASSB cathodes but\nalso other materials with similar structures."
    },
    {
        "date": "2025-03",
        "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving",
        "author": "Alexandra Arzberger, and Ramin Tavakoli Kolagari",
        "link": "http://arxiv.org/abs/2503.17168v2",
        "abstract": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults."
    },
    {
        "date": "2025-03",
        "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks",
        "author": "Ekaterina Dmitrieva, and Maksim Kaledin",
        "link": "http://arxiv.org/abs/2503.17141v1",
        "abstract": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software simplifying downstream speech tasks. Still, modern Deep\nLearning (DL) solutions often require high amount of computational resources\nwhat makes their usage on low-resource devices challenging. We present\nHiFi-Stream, an optimized version of recently published HiFi++ model. Our\nexperiments demonstrate that HiFiStream saves most of the qualities of the\noriginal model despite its size and computational complexity: the lightest\nversion has only around 490k parameters which is 3.5x reduction in comparison\nto the original HiFi++ making it one of the smallest and fastest models\navailable. The model is evaluated in streaming setting where it demonstrates\nits superior performance in comparison to modern baselines."
    },
    {
        "date": "2025-03",
        "title": "EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations",
        "author": "Tadeu Freitas, Erick Silva, Rehana Yasmin, Ali Shoker, Manuel E. Correia, Rolando Martins, and Paulo Esteves-Verissimo",
        "link": "http://arxiv.org/abs/2503.16984v1",
        "abstract": "Vehicle cybersecurity has emerged as a critical concern, driven by the\ninnovation in the automotive industry, e.g., automomous, electric, or\nconnnected vehicles. Current efforts to address these challenges are\nconstrained by the limited computational resources of vehicles and the reliance\non connected infrastructures. This motivated the foundation of Vehicle Security\nOperations Centers (VSOCs) that extend IT-based Security Operations Centers\n(SOCs) to cover the entire automotive ecosystem, both the in-vehicle and\noff-vehicle scopes. Security Orchestration, Automation, and Response (SOAR)\ntools are considered key for impelementing an effective cybersecurity solution.\nHowever, existing state-of-the-art solutions depend on infrastructure networks\nsuch as 4G, 5G, and WiFi, which often face scalability and congestion issues.\nTo address these limitations, we propose a novel SOAR architecture EVSOAR that\nleverages the EV charging stations for connectivity and computing to enhance\nvehicle cybersecurity. Our EV-specific SOAR architecture enables real-time\nanalysis and automated responses to cybersecurity threats closer to the EV,\nreducing the cellular latency, bandwidth, and interference limitations. Our\nexperimental results demonstrate a significant improvement in latency,\nstability, and scalability through the infrastructure and the capacity to\ndeploy computationally intensive applications, that are otherwise infeasible\nwithin the resource constraints of individual vehicles."
    },
    {
        "date": "2025-03",
        "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
        "author": "Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, and Hang Su",
        "link": "http://arxiv.org/abs/2503.16975v1",
        "abstract": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."
    },
    {
        "date": "2025-03",
        "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
        "author": "Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, and Yi Yang",
        "link": "http://arxiv.org/abs/2503.16964v1",
        "abstract": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery."
    },
    {
        "date": "2025-03",
        "title": "CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.16950v1",
        "abstract": "Stack-based memory corruption vulnerabilities have\n  long been exploited by attackers to execute arbitrary code\n  or perform unauthorized memory operations. Various defense\n  mechanisms have been introduced to mitigate stack memory\n  errors, but they typically focus on specific attack types, incur\n  substantial performance overhead, or suffer from compatibility\n  limitations.In this paper, we present CleanStack, an efficient,\n  highly compatible, and comprehensive stack protection mech anism. CleanStack\nisolates stack objects influenced by external\n  input from other safe stack objects, thereby preventing attackers\n  from modifying return addresses via controlled stack objects.\n  Additionally, by randomizing the placement of tainted stack\n  objects within the Unclean Stack, CleanStack mitigates non control data\nattacks by preventing attackers from predicting the\n  stack layout.A key component of CleanStack is the identifica tion of tainted\nstack objects. We analyze both static program\n  analysis and heuristic methods for this purpose. To maximize\n  compatibility, we adopt a heuristic approach and implement\n  CleanStack within the LLVM compiler framework, applying it to\n  SPEC CPU2017 benchmarks and a real-world application.Our\n  security evaluation demonstrates that CleanStack significantly\n  reduces the exploitability of stack-based memory errors by\n  providing a dual-stack system with isolation and randomization.\n  Performance evaluation results indicate that CleanStack incurs\n  an execution overhead of only 1.73% on the SPEC CPU2017\n  benchmark while introducing a minimal memory overhead of\n  just 0.04%. Compared to existing stack protection techniques,\n  CleanStack achieves an optimal balance between protection\n  coverage, runtime overhead, and compatibility, making it one\n  of the most comprehensive and efficient stack security solutions\n  to date."
    },
    {
        "date": "2025-03",
        "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
        "author": "Shayne Longpre, Kevin Klyman, Ruth E. Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M. Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca S. Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, and Arvind Narayanan",
        "link": "http://arxiv.org/abs/2503.16861v2",
        "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
    },
    {
        "date": "2025-03",
        "title": "Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic",
        "author": "Yali Yuan, Qianqi Niu, and Yachao Yuan",
        "link": "http://arxiv.org/abs/2503.16847v1",
        "abstract": "Flow correlation attacks is an efficient network attacks, aiming to expose\nthose who use anonymous network services, such as Tor. Conducting such attacks\nduring the early stages of network communication is particularly critical for\nscenarios demanding rapid decision-making, such as cybercrime detection or\nfinancial fraud prevention. Although recent studies have made progress in flow\ncorrelation attacks techniques, research specifically addressing flow\ncorrelation with early network traffic flow remains limited. Moreover, due to\nfactors such as model complexity, training costs, and real-time requirements,\nexisting technologies cannot be directly applied to flow correlation with early\nnetwork traffic flow. In this paper, we propose flow correlation attack with\nearly network traffic, named Early-MFC, based on multi-view triplet networks.\nThe proposed approach extracts multi-view traffic features from the payload at\nthe transport layer and the Inter-Packet Delay. It then integrates multi-view\nflow information, converting the extracted features into shared embeddings. By\nleveraging techniques such as metric learning and contrastive learning, the\nmethod optimizes the embeddings space by ensuring that similar flows are mapped\ncloser together while dissimilar flows are positioned farther apart. Finally,\nBayesian decision theory is applied to determine flow correlation, enabling\nhigh-accuracy flow correlation with early network traffic flow. Furthermore, we\ninvestigate flow correlation attacks under extra-early network traffic flow\nconditions. To address this challenge, we propose Early-MFC+, which utilizes\npayload data to construct embedded feature representations, ensuring robust\nperformance even with minimal packet availability."
    },
    {
        "date": "2025-03",
        "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
        "author": "Massa Baali, Xiang Li, Hao Chen, Rita Singh, and Bhiksha Raj",
        "link": "http://arxiv.org/abs/2503.16718v1",
        "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released."
    },
    {
        "date": "2025-03",
        "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents",
        "author": "Mihaela-Larisa Clement, M\u00f3nika Farsang, Felix Resch, and Radu Grosu",
        "link": "http://arxiv.org/abs/2503.16711v1",
        "abstract": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task."
    },
    {
        "date": "2025-03",
        "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
        "author": "Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, and Yushun Dong",
        "link": "http://arxiv.org/abs/2503.16693v1",
        "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine\nLearning as a Service (GMLaaS) platforms, yet they remain vulnerable to\ngraph-based model extraction attacks (MEAs), where adversaries reconstruct\nsurrogate models by querying the victim model. Existing defense mechanisms,\nsuch as watermarking and fingerprinting, suffer from poor real-time\nperformance, susceptibility to evasion, or reliance on post-attack\nverification, making them inadequate for handling the dynamic characteristics\nof graph-based MEA variants. To address these limitations, we propose ATOM, a\nnovel real-time MEA detection framework tailored for GNNs. ATOM integrates\nsequential modeling and reinforcement learning to dynamically detect evolving\nattack patterns, while leveraging $k$-core embedding to capture the structural\nproperties, enhancing detection precision. Furthermore, we provide theoretical\nanalysis to characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate that ATOM\noutperforms existing approaches in detection performance, maintaining stable\nacross different time steps, thereby offering a more effective defense\nmechanism for GMLaaS environments."
    },
    {
        "date": "2025-03",
        "title": "Input-Triggered Hardware Trojan Attack on Spiking Neural Networks",
        "author": "Spyridon Raptis, Paul Kling, Ioannis Kaskampas, Ihsen Alouani, and Haralampos-G. Stratigopoulos",
        "link": "http://arxiv.org/abs/2503.21793v1",
        "abstract": "Neuromorphic computing based on spiking neural networks (SNNs) is emerging as\na promising alternative to traditional artificial neural networks (ANNs),\noffering unique advantages in terms of low power consumption. However, the\nsecurity aspect of SNNs is under-explored compared to their ANN counterparts.\nAs the increasing reliance on AI systems comes with unique security risks and\nchallenges, understanding the vulnerabilities and threat landscape is essential\nas neuromorphic computing matures. In this effort, we propose a novel\ninput-triggered Hardware Trojan (HT) attack for SNNs. The HT mechanism is\ncondensed in the area of one neuron. The trigger mechanism is an input message\ncrafted in the spiking domain such that a selected neuron produces a malicious\nspike train that is not met in normal settings. This spike train triggers a\nmalicious modification in the neuron that forces it to saturate, firing\npermanently and failing to recover to its resting state even when the input\nactivity stops. The excessive spikes pollute the network and produce misleading\ndecisions. We propose a methodology to select an appropriate neuron and to\ngenerate the input pattern that triggers the HT payload. The attack is\nillustrated by simulation on three popular benchmarks in the neuromorphic\ncommunity. We also propose a hardware implementation for an analog spiking\nneuron and a digital SNN accelerator, demonstrating that the HT has a\nnegligible area and power footprint and, thereby, can easily evade detection."
    },
    {
        "date": "2025-03",
        "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
        "author": "Qiankun Shi, Jie Peng, Kun Yuan, Xiao Wang, and Qing Ling",
        "link": "http://arxiv.org/abs/2503.16337v1",
        "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight."
    },
    {
        "date": "2025-03",
        "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
        "author": "Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, and Jizhao Liu",
        "link": "http://arxiv.org/abs/2503.16287v1",
        "abstract": "The rapid development of low-Earth orbit (LEO) satellite constellations and\nsatellite communication systems has elevated the importance of secure video\ntransmission, which is the key to applications such as remote sensing, disaster\nrelief, and secure information exchange. In this context, three serious issues\narise concerning real-time encryption of videos on satellite embedded devices:\n(a) the challenge of achieving real-time performance; (b) the limitations posed\nby the constrained computing performance of satellite payloads; and (c) the\npotential for excessive power consumption leading to overheating, thereby\nescalating safety risks. To overcome these challenges, this study introduced a\nnovel approach for encrypting videos by employing two 1D chaotic maps, which\nwas deployed on a satellite for the first time. The experiment on the satellite\nconfirms that our scheme is suitable for complex satellite environments. In\naddition, the proposed chaotic maps were implemented on a Field Programmable\nGate Array (FPGA) platform, and simulation results showed consistency with\nthose obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B\ndemonstrate exceptional real-time performance and low power consumption,\nvalidating both the hardware feasibility and the stability of our design.\nRigorous statistical testing also confirms the scheme's resilience against a\nvariety of attacks, underscoring its potential for secure, real-time data\ntransmission in satellite communication systems."
    },
    {
        "date": "2025-03",
        "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
        "author": "Jo\u00e3o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cin\u00e0, Carlos Cotrini, Lea Sch\u00f6nherr, and Joachim M. Buhmann",
        "link": "http://arxiv.org/abs/2503.16271v1",
        "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
    },
    {
        "date": "2025-03",
        "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "author": "Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, and Chenjun Ma",
        "link": "http://arxiv.org/abs/2503.16266v1",
        "abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from\nmodels, leading to privacy leakage, particularly in facial recognition systems.\nAlthough many studies have enhanced the effectiveness of white-box MIAs, less\nattention has been paid to improving efficiency and utility under limited\nattacker capabilities. Existing black-box MIAs necessitate an impractical\nnumber of queries, incurring significant overhead. Therefore, we analyze the\nlimitations of existing MIAs and introduce Surrogate Model-based Inversion with\nLong-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient\nMIA for the black-box setting. We begin by analyzing the initialization of MIAs\nfrom a data distribution perspective and propose a long-tailed surrogate\ntraining method to obtain high-quality initial points. We then enhance the\nattack's effectiveness by employing the gradient-free black-box optimization\nalgorithm selected by NGOpt. Our experiments show that SMILE outperforms\nexisting state-of-the-art black-box MIAs while requiring only about 5% of the\nquery overhead."
    },
    {
        "date": "2025-03",
        "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
        "author": "Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath",
        "link": "http://arxiv.org/abs/2503.16248v1",
        "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible."
    },
    {
        "date": "2025-03",
        "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2503.16179v1",
        "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."
    },
    {
        "date": "2025-03",
        "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
        "author": "Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, and Vincent Guigue",
        "link": "http://arxiv.org/abs/2503.16161v1",
        "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
    },
    {
        "date": "2025-03",
        "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
        "author": "Marek Wodzinski, and Henning M\u00fcller",
        "link": "http://arxiv.org/abs/2503.16075v1",
        "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
    },
    {
        "date": "2025-03",
        "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
        "author": "Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, and Prisca Chinazor Amajuoyi",
        "link": "http://arxiv.org/abs/2503.16047v2",
        "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
    },
    {
        "date": "2025-03",
        "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
        "author": "Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2503.16023v1",
        "abstract": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
    },
    {
        "date": "2025-03",
        "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
        "author": "Junsung Park, Hwijeong Lee, Inha Kang, and Hyunjung Shim",
        "link": "http://arxiv.org/abs/2503.15910v2",
        "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness."
    },
    {
        "date": "2025-03",
        "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
        "author": "Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, and Bo Li",
        "link": "http://arxiv.org/abs/2503.15754v1",
        "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems."
    },
    {
        "date": "2025-03",
        "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
        "author": "Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar, and Prospero C. Naval Jr",
        "link": "http://arxiv.org/abs/2503.15726v1",
        "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications."
    },
    {
        "date": "2025-03",
        "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "author": "Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, and Chao Shen",
        "link": "http://arxiv.org/abs/2503.15404v1",
        "abstract": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."
    },
    {
        "date": "2025-03",
        "title": "Robustness of Nonlinear Representation Learning",
        "author": "Simon Buchholz, and Bernhard Sch\u00f6lkopf",
        "link": "http://arxiv.org/abs/2503.15355v1",
        "abstract": "We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes."
    },
    {
        "date": "2025-03",
        "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
        "author": "Dominik Macko, Robert Moro, and Ivan Srba",
        "link": "http://arxiv.org/abs/2503.15128v1",
        "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
    },
    {
        "date": "2025-03",
        "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
        "author": "Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, and Siyuan Huang",
        "link": "http://arxiv.org/abs/2503.15082v1",
        "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs."
    },
    {
        "date": "2025-03",
        "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
        "author": "Jiazhu Dai, and Haoyu Sun",
        "link": "http://arxiv.org/abs/2503.14922v1",
        "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples."
    },
    {
        "date": "2025-03",
        "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
        "author": "Jingyi Liao, Xun Xu, Yongyi Su, Rong-Cheng Tu, Yifan Liu, Dacheng Tao, and Xulei Yang",
        "link": "http://arxiv.org/abs/2503.14910v1",
        "abstract": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition",
        "author": "Seyed Mojtaba Mohasel, and Hamidreza Koosha",
        "link": "http://arxiv.org/abs/2503.14873v1",
        "abstract": "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness Tradeoff in Fine-Tuning",
        "author": "Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak, Yohan Beugin, Eric Pauley, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.14836v1",
        "abstract": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."
    },
    {
        "date": "2025-03",
        "title": "Robust Transmission of Punctured Text with Large Language Model-based Recovery",
        "author": "Sojeong Park, Hyeonho Noh, and Hyun Jong Yang",
        "link": "http://arxiv.org/abs/2503.14831v1",
        "abstract": "With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions."
    },
    {
        "date": "2025-03",
        "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation",
        "author": "Shiyi Jiang, Farshad Firouzi, and Krishnendu Chakrabarty",
        "link": "http://arxiv.org/abs/2503.16542v1",
        "abstract": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data."
    },
    {
        "date": "2025-03",
        "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
        "author": "Prashant Kulkarni, and Assaf Namer",
        "link": "http://arxiv.org/abs/2503.15560v1",
        "abstract": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security."
    },
    {
        "date": "2025-03",
        "title": "Robust Object Detection of Underwater Robot based on Domain Generalization",
        "author": "Pinhao Song",
        "link": "http://arxiv.org/abs/2503.19929v1",
        "abstract": "Object detection aims to obtain the location and the category of specific\nobjects in a given image, which includes two tasks: classification and\nlocation. In recent years, researchers tend to apply object detection to\nunderwater robots equipped with vision systems to complete tasks including\nseafood fishing, fish farming, biodiversity monitoring and so on. However, the\ndiversity and complexity of underwater environments bring new challenges to\nobject detection. First, aquatic organisms tend to live together, which leads\nto severe occlusion. Second, theaquatic organisms are good at hiding\nthemselves, which have a similar color to the background. Third, the various\nwater quality and changeable and extreme lighting conditions lead to the\ndistorted, low contrast, blue or green images obtained by the underwater\ncamera, resulting in domain shift. And the deep model is generally vulnerable\nto facing domain shift. Fourth, the movement of the underwater robot leads to\nthe blur of the captured image and makes the water muddy, which results in low\nvisibility of the water. This paper investigates the problems brought by the\nunderwater environment mentioned above, and aims to design a high-performance\nand robust underwater object detector."
    },
    {
        "date": "2025-03",
        "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
        "author": "Rohan Menon, Nicola Franco, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.14751v1",
        "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures."
    },
    {
        "date": "2025-03",
        "title": "A Comprehensive Study of LLM Secure Code Generation",
        "author": "Shih-Chieh Dai, Jun Xu, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2503.15554v1",
        "abstract": "LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work."
    },
    {
        "date": "2025-03",
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "author": "Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat, and Huan Liu",
        "link": "http://arxiv.org/abs/2503.15552v1",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts."
    },
    {
        "date": "2025-03",
        "title": "Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection",
        "author": "Leonardo Henrique de Melo, Gustavo de Carvalho Bertoli, Michele Nogueira, Aldri Luiz dos Santos, and Louren\u00e7o Alves Pereira Junior",
        "link": "http://arxiv.org/abs/2503.14618v1",
        "abstract": "Distributed denial-of-service (DDoS) attacks remain a critical threat to\nInternet services, causing costly disruptions. While machine learning (ML) has\nshown promise in DDoS detection, current solutions struggle with multi-domain\nenvironments where attacks must be detected across heterogeneous networks and\norganizational boundaries. This limitation severely impacts the practical\ndeployment of ML-based defenses in real-world settings.\n  This paper introduces Anomaly-Flow, a novel framework that addresses this\ncritical gap by combining Federated Learning (FL) with Generative Adversarial\nNetworks (GANs) for privacy-preserving, multi-domain DDoS detection. Our\nproposal enables collaborative learning across diverse network domains while\npreserving data privacy through synthetic flow generation. Through extensive\nevaluation across three distinct network datasets, Anomaly-Flow achieves an\naverage F1-score of $0.747$, outperforming baseline models. Importantly, our\nframework enables organizations to share attack detection capabilities without\nexposing sensitive network data, making it particularly valuable for critical\ninfrastructure and privacy-sensitive sectors.\n  Beyond immediate technical contributions, this work provides insights into\nthe challenges and opportunities in multi-domain DDoS detection, establishing a\nfoundation for future research in collaborative network defense systems. Our\nfindings have important implications for academic research and industry\npractitioners working to deploy practical ML-based security solutions."
    },
    {
        "date": "2025-03",
        "title": "Doubly robust identification of treatment effects from multiple environments",
        "author": "Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, and Fanny Yang",
        "link": "http://arxiv.org/abs/2503.14459v1",
        "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods."
    },
    {
        "date": "2025-03",
        "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
        "author": "Murong Yue, and Ziyu Yao",
        "link": "http://arxiv.org/abs/2503.15551v1",
        "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
    },
    {
        "date": "2025-03",
        "title": "Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory",
        "author": "Lucas Gnecco-Heredia, Matteo Sammut, Muni Sreenivas Pydi, Rafael Pinot, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2503.14299v1",
        "abstract": "Randomization as a mean to improve the adversarial robustness of machine\nlearning models has recently attracted significant attention. Unfortunately,\nmuch of the theoretical analysis so far has focused on binary classification,\nproviding only limited insights into the more complex multiclass setting. In\nthis paper, we take a step toward closing this gap by drawing inspiration from\nthe field of graph theory. Our analysis focuses on discrete data distributions,\nallowing us to cast the adversarial risk minimization problems within the\nwell-established framework of set packing problems. By doing so, we are able to\nidentify three structural conditions on the support of the data distribution\nthat are necessary for randomization to improve robustness. Furthermore, we are\nable to construct several data distributions where (contrarily to binary\nclassification) switching from a deterministic to a randomized solution\nsignificantly reduces the optimal adversarial risk. These findings highlight\nthe crucial role randomization can play in enhancing robustness to adversarial\nattacks in multiclass classification."
    },
    {
        "date": "2025-03",
        "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
        "author": "Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa, Alexander L\u00f6ser, Erik Rodner, and Felix A. Gers",
        "link": "http://arxiv.org/abs/2503.14572v1",
        "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes."
    },
    {
        "date": "2025-03",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "author": "Adam \u0160torek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim, Prashast Srivastava, and Suman Jana",
        "link": "http://arxiv.org/abs/2503.14281v1",
        "abstract": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
    },
    {
        "date": "2025-03",
        "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
        "author": "Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2503.14247v1",
        "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM"
    },
    {
        "date": "2025-03",
        "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
        "author": "Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, and Wei-Shi Zheng",
        "link": "http://arxiv.org/abs/2503.14198v1",
        "abstract": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat."
    },
    {
        "date": "2025-03",
        "title": "Towards properties of adversarial image perturbations",
        "author": "Egor Kuznetsov, Kirill Aistov, and Maxim Koroteev",
        "link": "http://arxiv.org/abs/2503.14111v1",
        "abstract": "Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization."
    },
    {
        "date": "2025-03",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "author": "Yuchen Niu, and Siew-Kei Lam",
        "link": "http://arxiv.org/abs/2503.14006v1",
        "abstract": "Automated insulin delivery (AID) systems have emerged as a significant\ntechnological advancement in diabetes care. These systems integrate a\ncontinuous glucose monitor, an insulin pump, and control algorithms to automate\ninsulin delivery, reducing the burden of self-management and offering enhanced\nglucose control. However, the increasing reliance on wireless connectivity and\nsoftware control has exposed AID systems to critical security risks that could\nresult in life-threatening treatment errors. This review first presents a\ncomprehensive examination of the security landscape, covering technical\nvulnerabilities, legal frameworks, and commercial product considerations, and\nan analysis of existing research on attack vectors, defence mechanisms, as well\nas evaluation methods and resources for AID systems. Despite recent\nadvancements, several open challenges remain in achieving secure AID systems,\nparticularly in standardising security evaluation frameworks and developing\ncomprehensive, lightweight, and adaptive defence strategies. As one of the most\nwidely adopted and extensively studied physiologic closed-loop control systems,\nthis review serves as a valuable reference for understanding security\nchallenges and solutions applicable to analogous medical systems."
    },
    {
        "date": "2025-03",
        "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
        "author": "Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui",
        "link": "http://arxiv.org/abs/2503.13962v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."
    },
    {
        "date": "2025-03",
        "title": "Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels",
        "author": "Yujia Tong, Yuze Wang, Jingling Yuan, and Chuang Hu",
        "link": "http://arxiv.org/abs/2503.13917v1",
        "abstract": "Model quantization enables efficient deployment of deep neural networks on\nedge devices through low-bit parameter representation, yet raises critical\nchallenges for implementing machine unlearning (MU) under data privacy\nregulations. Existing MU methods designed for full-precision models fail to\naddress two fundamental limitations in quantized networks: 1) Noise\namplification from label mismatch during data processing, and 2) Gradient\nimbalance between forgotten and retained data during training. These issues are\nexacerbated by quantized models' constrained parameter space and discrete\noptimization. We propose Q-MUL, the first dedicated unlearning framework for\nquantized models. Our method introduces two key innovations: 1) Similar Labels\nassignment replaces random labels with semantically consistent alternatives to\nminimize noise injection, and 2) Adaptive Gradient Reweighting dynamically\naligns parameter update contributions from forgotten and retained data. Through\nsystematic analysis of quantized model vulnerabilities, we establish\ntheoretical foundations for these mechanisms. Extensive evaluations on\nbenchmark datasets demonstrate Q-MUL's superiority over existing approaches."
    },
    {
        "date": "2025-03",
        "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation",
        "author": "Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, and Yanye Lu",
        "link": "http://arxiv.org/abs/2503.13895v1",
        "abstract": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available."
    },
    {
        "date": "2025-03",
        "title": "Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception",
        "author": "Jinge Ma, Jiangpeng He, and Fengqing Zhu",
        "link": "http://arxiv.org/abs/2503.13869v1",
        "abstract": "3D perception plays a crucial role in real-world applications such as\nautonomous driving, robotics, and AR/VR. In practical scenarios, 3D perception\nmodels must continuously adapt to new data and emerging object categories, but\nretraining from scratch incurs prohibitive costs. Therefore, adopting\nclass-incremental learning (CIL) becomes particularly essential. However,\nreal-world 3D point cloud data often include corrupted samples, which poses\nsignificant challenges for existing CIL methods and leads to more severe\nforgetting on corrupted data. To address these challenges, we consider the\nscenario in which a CIL model can be updated using point clouds with unknown\ncorruption to better simulate real-world conditions. Inspired by Farthest Point\nSampling, we propose a novel exemplar selection strategy that effectively\npreserves intra-class diversity when selecting replay exemplars, mitigating\nforgetting induced by data corruption. Furthermore, we introduce a point cloud\ndownsampling-based replay method to utilize the limited replay buffer memory\nmore efficiently, thereby further enhancing the model's continual learning\nability. Extensive experiments demonstrate that our method improves the\nperformance of replay-based CIL baselines by 2% to 11%, proving its\neffectiveness and promising potential for real-world 3D applications."
    },
    {
        "date": "2025-03",
        "title": "Text-Guided Image Invariant Feature Learning for Robust Image Watermarking",
        "author": "Muhammad Ahtesham, and Xin Zhong",
        "link": "http://arxiv.org/abs/2503.13805v1",
        "abstract": "Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking."
    },
    {
        "date": "2025-03",
        "title": "Web Artifact Attacks Disrupt Vision Language Models",
        "author": "Maan Qraitem, Piotr Teterwak, Kate Saenko, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2503.13652v1",
        "abstract": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "author": "Johan Edstedt",
        "link": "http://arxiv.org/abs/2503.13433v1",
        "abstract": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1."
    },
    {
        "date": "2025-03",
        "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
        "author": "Nhi Pham, Bernt Schiele, Adam Kortylewski, and Jonas Fischer",
        "link": "http://arxiv.org/abs/2503.13429v1",
        "abstract": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness."
    },
    {
        "date": "2025-03",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "author": "Ripan Kumar Kundu, Matthew Denton, Genova Mongalo, Prasad Calyam, and Khaza Anuarul Hoque",
        "link": "http://arxiv.org/abs/2503.13419v1",
        "abstract": "The synergy between virtual reality (VR) and artificial intelligence (AI),\nspecifically deep learning (DL)-based cybersickness detection models, has\nushered in unprecedented advancements in immersive experiences by automatically\ndetecting cybersickness severity and adaptively various mitigation techniques,\noffering a smooth and comfortable VR experience. While this DL-enabled\ncybersickness detection method provides promising solutions for enhancing user\nexperiences, it also introduces new risks since these models are vulnerable to\nadversarial attacks; a small perturbation of the input data that is visually\nundetectable to human observers can fool the cybersickness detection model and\ntrigger unexpected mitigation, thus disrupting user immersive experiences (UIX)\nand even posing safety risks. In this paper, we present a new type of VR\nattack, i.e., a cybersickness attack, which successfully stops the triggering\nof cybersickness mitigation by fooling DL-based cybersickness detection models\nand dramatically hinders the UIX. Next, we propose a novel explainable\nartificial intelligence (XAI)-guided cybersickness attack detection framework\nto detect such attacks in VR to ensure UIX and a comfortable VR experience. We\nevaluate the proposed attack and the detection framework using two\nstate-of-the-art open-source VR cybersickness datasets: Simulation 2021 and\nGameplay dataset. Finally, to verify the effectiveness of our proposed method,\nwe implement the attack and the XAI-based detection using a testbed with a\ncustom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and\nperform a user study. Our study shows that such an attack can dramatically\nhinder the UIX. However, our proposed XAI-guided cybersickness attack detection\ncan successfully detect cybersickness attacks and trigger the proper\nmitigation, effectively reducing VR cybersickness."
    },
    {
        "date": "2025-03",
        "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
        "author": "Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, and Libo Qin",
        "link": "http://arxiv.org/abs/2503.13413v3",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
    },
    {
        "date": "2025-03",
        "title": "Follow-the-Regularized-Leader with Adversarial Constraints",
        "author": "Ricardo N. Ferreira, and Cl\u00e1udia Soares",
        "link": "http://arxiv.org/abs/2503.13366v2",
        "abstract": "Constrained Online Convex Optimization (COCO) can be seen as a generalization\nof the standard Online Convex Optimization (OCO) framework. At each round, a\ncost function and constraint function are revealed after a learner chooses an\naction. The goal is to minimize both the regret and cumulative constraint\nviolation (CCV) against an adaptive adversary. We show for the first time that\nis possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV,\nimproving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\~{O}\n\\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively."
    },
    {
        "date": "2025-03",
        "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
        "author": "Marcello Iotti, Paolo Davini, Jost von Hardenberg, and Giuseppe Zappa",
        "link": "http://arxiv.org/abs/2503.13316v1",
        "abstract": "To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature."
    },
    {
        "date": "2025-03",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "author": "Tianxing Fu, Jia Hu, Geyong Min, and Zi Wang",
        "link": "http://arxiv.org/abs/2503.13255v1",
        "abstract": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models while ensuring their data remains private and\nsecure. Blockchain technology further enhances FL by providing stronger\nsecurity, a transparent audit trail, and protection against data tampering and\nmodel manipulation. Most blockchain-secured FL systems rely on conventional\nconsensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while\nProof-of-Stake (PoS) improves energy efficiency but risks centralization as it\ninherently favors participants with larger stakes. Recently, learning-based\nconsensus has emerged as an alternative by replacing cryptographic tasks with\nmodel training to save energy. However, this approach introduces potential\nprivacy vulnerabilities, as the training process may inadvertently expose\nsensitive information through gradient sharing and model updates. To address\nthese challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT)\nconsensus mechanism. This method leverages the zero-knowledge succinct\nnon-interactive argument of knowledge proof (zk-SNARK) protocol to validate\nparticipants' contributions based on their model performance, effectively\neliminating the inefficiencies of traditional consensus methods and mitigating\nthe privacy risks posed by learning-based consensus. We analyze our system's\nsecurity, demonstrating its capacity to prevent the disclosure of sensitive\ninformation about local models or training data to untrusted parties during the\nentire FL process. Extensive experiments demonstrate that our system is robust\nagainst privacy and Byzantine attacks while maintaining accuracy and utility\nwithout trade-offs, scalable across various blockchain settings, and efficient\nin both computation and communication."
    },
    {
        "date": "2025-03",
        "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
        "author": "Tong Zhou, Shijin Duan, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Shaolei Ren, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2503.13224v1",
        "abstract": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security."
    },
    {
        "date": "2025-03",
        "title": "Robust Decision-Making Via Free Energy Minimization",
        "author": "Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, and Giovanni Russo",
        "link": "http://arxiv.org/abs/2503.13223v1",
        "abstract": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments."
    }
]