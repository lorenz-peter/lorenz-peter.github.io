[
    {
        "date": "2025-05",
        "title": "Securing WiFi Fingerprint-based Indoor Localization Systems from Malicious Access Points",
        "author": "Fariha Tanjim Shifat, Sayma Sarwar Ela, and Mosarrat Jahan",
        "link": "http://arxiv.org/abs/2505.07724v1",
        "abstract": "WiFi fingerprint-based indoor localization schemes deliver highly accurate\nlocation data by matching the received signal strength indicator (RSSI) with an\noffline database using machine learning (ML) or deep learning (DL) models.\nHowever, over time, RSSI values degrade due to the malicious behavior of access\npoints (APs), causing low positional accuracy due to RSSI value mismatch with\nthe offline database. Existing literature lacks detection of malicious APs in\nthe online phase and mitigating their effects. This research addresses these\nlimitations and proposes a long-term reliable indoor localization scheme by\nincorporating malicious AP detection and their effect mitigation techniques.\nThe proposed scheme uses a Light Gradient-Boosting Machine (LGBM) classifier to\nestimate locations and integrates simple yet efficient techniques to detect\nmalicious APs based on online query data. Subsequently, a mitigation technique\nis incorporated that updates the offline database and online queries by\nimputing stable values for malicious APs using LGBM Regressors. Additionally,\nwe introduce a noise addition mechanism in the offline database to capture the\ndynamic environmental effects. Extensive experimental evaluation shows that the\nproposed scheme attains a detection accuracy above 95% for each attack type.\nThe mitigation strategy effectively restores the system's performance nearly to\nits original state when no malicious AP is present. The noise addition module\nreduces localization errors by nearly 16%. Furthermore, the proposed solution\nis lightweight, reducing the execution time by approximately 94% compared to\nthe existing methods."
    },
    {
        "date": "2025-05",
        "title": "Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy",
        "author": "Gleb Molodtsov, Daniil Medyakov, Sergey Skorik, Nikolas Khachaturov, Shahane Tigranyan, Vladimir Aletov, Aram Avetisyan, Martin Tak\u00e1\u010d, and Aleksandr Beznosikov",
        "link": "http://arxiv.org/abs/2505.07614v1",
        "abstract": "Recent advancements in machine learning have improved performance while also\nincreasing computational demands. While federated and distributed setups\naddress these issues, their structure is vulnerable to malicious influences. In\nthis paper, we address a specific threat, Byzantine attacks, where compromised\nclients inject adversarial updates to derail global convergence. We combine the\ntrust scores concept with trial function methodology to dynamically filter\noutliers. Our methods address the critical limitations of previous approaches,\nallowing functionality even when Byzantine nodes are in the majority. Moreover,\nour algorithms adapt to widely used scaled methods like Adam and RMSProp, as\nwell as practical scenarios, including local training and partial\nparticipation. We validate the robustness of our methods by conducting\nextensive experiments on both synthetic and real ECG data collected from\nmedical institutions. Furthermore, we provide a broad theoretical analysis of\nour algorithms and their extensions to aforementioned practical setups. The\nconvergence guarantees of our methods are comparable to those of classical\nalgorithms developed without Byzantine interference."
    },
    {
        "date": "2025-05",
        "title": "SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models",
        "author": "Huining Cui, and Wei Liu",
        "link": "http://arxiv.org/abs/2505.07584v1",
        "abstract": "The increasing deployment of large language models in security-sensitive\ndomains necessitates rigorous evaluation of their resilience against\nadversarial prompt-based attacks. While previous benchmarks have focused on\nsecurity evaluations with limited and predefined attack domains, such as\ncybersecurity attacks, they often lack a comprehensive assessment of\nintent-driven adversarial prompts and the consideration of real-life\nscenario-based multi-turn attacks. To address this gap, we present\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\nmodel assessment: one-off attack, successive attack, successive reverse attack,\nalternative attack, sequential ascending attack with escalating threat levels\nand sequential descending attack with diminishing threat levels. In addition,\nwe introduce a dataset customized for the benchmark, which incorporates both\nneutral and malicious prompts, categorised across seven security domains and\nsixteen attack techniques. In applying this benchmark, we systematically\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\ninsights into the strengths and weaknesses of modern large language models in\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\npublicly available at\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\nwhich provides a groundwork for advancing research in large language model\nsecurity."
    },
    {
        "date": "2025-05",
        "title": "Security through the Eyes of AI: How Visualization is Shaping Malware Detection",
        "author": "Asmitha K. A., Matteo Brosolo, Serena Nicolazzo, Antonino Nocera, Vinod P., Rafidha Rehiman K. A., and Muhammed Shafi K. P",
        "link": "http://arxiv.org/abs/2505.07574v1",
        "abstract": "Malware, a persistent cybersecurity threat, increasingly targets\ninterconnected digital systems such as desktop, mobile, and IoT platforms\nthrough sophisticated attack vectors. By exploiting these vulnerabilities,\nattackers compromise the integrity and resilience of modern digital ecosystems.\nTo address this risk, security experts actively employ Machine Learning or Deep\nLearning-based strategies, integrating static, dynamic, or hybrid approaches to\ncategorize malware instances. Despite their advantages, these methods have\ninherent drawbacks and malware variants persistently evolve with increased\nsophistication, necessitating advancements in detection strategies.\nVisualization-based techniques are emerging as scalable and interpretable\nsolutions for detecting and understanding malicious behaviors across diverse\nplatforms including desktop, mobile, IoT, and distributed systems as well as\nthrough analysis of network packet capture files. In this comprehensive survey\nof more than 100 high-quality research articles, we evaluate existing\nvisualization-based approaches applied to malware detection and classification.\nAs a first contribution, we propose a new all-encompassing framework to study\nthe landscape of visualization-based malware detection techniques. Within this\nframework, we systematically analyze state-of-the-art approaches across the\ncritical stages of the malware detection pipeline. By analyzing not only the\nsingle techniques but also how they are combined to produce the final solution,\nwe shed light on the main challenges in visualization-based approaches and\nprovide insights into the advancements and potential future directions in this\ncritical field."
    },
    {
        "date": "2025-05",
        "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework",
        "author": "Sarah de Boer, Hartmut H\u00e4ntze, Kiran Vaidhya Venkadesh, Myrthe A. D. Buser, Gabriel E. Humpire Mamani, Lina Xu, Lisa C. Adams, Jawed Nawabi, Keno K. Bressem, Bram van Ginneken, Mathias Prokop, and Alessa Hering",
        "link": "http://arxiv.org/abs/2505.07573v1",
        "abstract": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation."
    },
    {
        "date": "2025-05",
        "title": "GRADA: Graph-based Reranker against Adversarial Documents Attack",
        "author": "Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2505.07546v1",
        "abstract": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy."
    },
    {
        "date": "2025-05",
        "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection",
        "author": "Juan E. Tapia, Fabian Stockhardt, L\u00e1zaro Janier Gonz\u00e1lez-Soler, and Christoph Busch",
        "link": "http://arxiv.org/abs/2505.07540v1",
        "abstract": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images."
    },
    {
        "date": "2025-05",
        "title": "Post-Quantum Secure Decentralized Random Number Generation Protocol with Two Rounds of Communication in the Standard Model",
        "author": "Pham Nhat Minh, and Khuong Nguyen-An",
        "link": "http://arxiv.org/abs/2505.07536v1",
        "abstract": "Randomness plays a vital role in numerous applications, including simulation,\ncryptography, distributed systems, and gaming. Consequently, extensive research\nhas been conducted to generate randomness. One such method is to design a\ndecentralized random number generator (DRNG), a protocol that enables multiple\nparticipants to collaboratively generate random outputs that must be publicly\nverifiable. However, existing DRNGs are either not secure against quantum\ncomputers or depend on the random oracle model (ROM) to achieve security. In\nthis paper, we design a DRNG based on lattice-based publicly verifiable secret\nsharing (PVSS) that is post-quantum secure and proven secure in the standard\nmodel. Additionally, our DRNG requires only two rounds of communication to\ngenerate a single (pseudo)random value and can tolerate up to any t < n/2\ndishonest participants. To our knowledge, the proposed DRNG construction is the\nfirst to achieve all these properties."
    },
    {
        "date": "2025-05",
        "title": "From Search To Sampling: Generative Models For Robust Algorithmic Recourse",
        "author": "Prateek Garg, Lokesh Nagalapatti, and Sunita Sarawagi",
        "link": "http://arxiv.org/abs/2505.07351v1",
        "abstract": "Algorithmic Recourse provides recommendations to individuals who are\nadversely impacted by automated model decisions, on how to alter their profiles\nto achieve a favorable outcome. Effective recourse methods must balance three\nconflicting goals: proximity to the original profile to minimize cost,\nplausibility for realistic recourse, and validity to ensure the desired\noutcome. We show that existing methods train for these objectives separately\nand then search for recourse through a joint optimization over the recourse\ngoals during inference, leading to poor recourse recommendations. We introduce\nGenRe, a generative recourse model designed to train the three recourse\nobjectives jointly. Training such generative models is non-trivial due to lack\nof direct recourse supervision. We propose efficient ways to synthesize such\nsupervision and further show that GenRe's training leads to a consistent\nestimator. Unlike most prior methods, that employ non-robust gradient descent\nbased search during inference, GenRe simply performs a forward sampling over\nthe generative model to produce minimum cost recourse, leading to superior\nperformance across multiple metrics. We also demonstrate GenRe provides the\nbest trade-off between cost, plausibility and validity, compared to\nstate-of-art baselines. Our code is available at:\nhttps://github.com/prateekgargx/genre."
    },
    {
        "date": "2025-05",
        "title": "Assessing the Latency of Network Layer Security in 5G Networks",
        "author": "Sotiris Michaelides, Jonathan Mucke, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.07328v1",
        "abstract": "In contrast to its predecessors, 5G supports a wide range of commercial,\nindustrial, and critical infrastructure scenarios. One key feature of 5G,\nultra-reliable low latency communication, is particularly appealing to such\nscenarios for its real-time capabilities. However, 5G's enhanced security,\nmostly realized through optional security controls, imposes additional overhead\non the network performance, potentially hindering its real-time capabilities.\nTo better assess this impact and guide operators in choosing between different\noptions, we measure the latency overhead of IPsec when applied over the N3 and\nthe service-based interfaces to protect user and control plane data,\nrespectively. Furthermore, we evaluate whether WireGuard constitutes an\nalternative to reduce this overhead. Our findings show that IPsec, if\nconfigured correctly, has minimal latency impact and thus is a prime candidate\nto secure real-time critical scenarios."
    },
    {
        "date": "2025-05",
        "title": "On the Robustness of Reward Models for Language Model Alignment",
        "author": "Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, and James Thorne",
        "link": "http://arxiv.org/abs/2505.07271v1",
        "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."
    },
    {
        "date": "2025-05",
        "title": "Adaptive, Robust and Scalable Bayesian Filtering for Online Learning",
        "author": "Gerardo Duran-Martin",
        "link": "http://arxiv.org/abs/2505.07267v1",
        "abstract": "In this thesis, we introduce Bayesian filtering as a principled framework for\ntackling diverse sequential machine learning problems, including online\n(continual) learning, prequential (one-step-ahead) forecasting, and contextual\nbandits. To this end, this thesis addresses key challenges in applying Bayesian\nfiltering to these problems: adaptivity to non-stationary environments,\nrobustness to model misspecification and outliers, and scalability to the\nhigh-dimensional parameter space of deep neural networks. We develop novel\ntools within the Bayesian filtering framework to address each of these\nchallenges, including: (i) a modular framework that enables the development\nadaptive approaches for online learning; (ii) a novel, provably robust filter\nwith similar computational cost to standard filters, that employs Generalised\nBayes; and (iii) a set of tools for sequentially updating model parameters\nusing approximate second-order optimisation methods that exploit the\noverparametrisation of high-dimensional parametric models such as neural\nnetworks. Theoretical analysis and empirical results demonstrate the improved\nperformance of our methods in dynamic, high-dimensional, and misspecified\nmodels."
    },
    {
        "date": "2025-05",
        "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments",
        "author": "Chetan Pathade, and Shubham Patil",
        "link": "http://arxiv.org/abs/2505.07188v1",
        "abstract": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data."
    },
    {
        "date": "2025-05",
        "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models",
        "author": "Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.07167v1",
        "abstract": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."
    },
    {
        "date": "2025-05",
        "title": "AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation",
        "author": "Heqing Ren, Chao Feng, Alberto Huertas, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2505.07149v1",
        "abstract": "Traditional machine learning (ML) raises serious privacy concerns, while\nfederated learning (FL) mitigates the risk of data leakage by keeping data on\nlocal devices. However, the training process of FL can still leak sensitive\ninformation, which adversaries may exploit to infer private data. One of the\nmost prominent threats is the membership inference attack (MIA), where the\nadversary aims to determine whether a particular data record was part of the\ntraining set.\n  This paper addresses this problem through a two-stage defense called\nAugMixCloak. The core idea is to apply data augmentation and principal\ncomponent analysis (PCA)-based information fusion to query images, which are\ndetected by perceptual hashing (pHash) as either identical to or highly similar\nto images in the training set. Experimental results show that AugMixCloak\nsuccessfully defends against both binary classifier-based MIA and metric-based\nMIA across five datasets and various decentralized FL (DFL) topologies.\nCompared with regularization-based defenses, AugMixCloak demonstrates stronger\nprotection. Compared with confidence score masking, AugMixCloak exhibits better\ngeneralization."
    },
    {
        "date": "2025-05",
        "title": "Standing Firm in 5G: A Single-Round, Dropout-Resilient Secure Aggregation for Federated Learning",
        "author": "Yiwei Zhang, Rouzbeh Behnia, Imtiaz Karim, Attila A. Yavuz, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2505.07148v1",
        "abstract": "Federated learning (FL) is well-suited to 5G networks, where many mobile\ndevices generate sensitive edge data. Secure aggregation protocols enhance\nprivacy in FL by ensuring that individual user updates reveal no information\nabout the underlying client data. However, the dynamic and large-scale nature\nof 5G-marked by high mobility and frequent dropouts-poses significant\nchallenges to the effective adoption of these protocols. Existing protocols\noften require multi-round communication or rely on fixed infrastructure,\nlimiting their practicality. We propose a lightweight, single-round secure\naggregation protocol designed for 5G environments. By leveraging base stations\nfor assisted computation and incorporating precomputation, key-homomorphic\npseudorandom functions, and t-out-of-k secret sharing, our protocol ensures\nefficiency, robustness, and privacy. Experiments show strong security\nguarantees and significant gains in communication and computation efficiency,\nmaking the approach well-suited for real-world 5G FL deployments."
    },
    {
        "date": "2025-05",
        "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization",
        "author": "Jitesh Joshi, and Youngjun Cho",
        "link": "http://arxiv.org/abs/2505.07013v1",
        "abstract": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live"
    },
    {
        "date": "2025-05",
        "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding",
        "author": "Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, and Yu-Jou Hsiao",
        "link": "http://arxiv.org/abs/2505.06991v1",
        "abstract": "This report presents our semantic segmentation framework developed by team\nACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which\nfocuses on parsing outdoor scenes into nine semantic categories under\nreal-world conditions. Our method integrates a Swin Transformer backbone\nenhanced with Rotary Position Embedding (RoPE) for improved spatial\ngeneralization, alongside a Color Shift Estimation-and-Correction module\ndesigned to compensate for illumination inconsistencies in natural\nenvironments. To further improve training stability, we adopt a quantile-based\ndenoising strategy that downweights the top 2.5\\% of highest-error pixels,\ntreating them as noise and suppressing their influence during optimization.\nEvaluated on the official GOOSE test set, our approach achieved a mean\nIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness of\ncombining color correction, positional encoding, and error-aware denoising in\nrobust semantic segmentation."
    },
    {
        "date": "2025-05",
        "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition",
        "author": "Md. Naimur Asif Borno, Md Sakib Hossain Shovon, MD Hanif Sikder, Iffat Firozy Rimi, Tahani Jaser Alahmadi, and Mohammad Ali Moni",
        "link": "http://arxiv.org/abs/2505.06982v1",
        "abstract": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection."
    },
    {
        "date": "2025-05",
        "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended Version)",
        "author": "James Tobler, Hira Taqdees Syeda, and Toby Murray",
        "link": "http://arxiv.org/abs/2505.06958v1",
        "abstract": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice."
    },
    {
        "date": "2025-05",
        "title": "RedTeamLLM: an Agentic AI framework for offensive security",
        "author": "Brian Challita, and Pierre Parrend",
        "link": "http://arxiv.org/abs/2505.06913v1",
        "abstract": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated."
    },
    {
        "date": "2025-05",
        "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
        "author": "Mihyeon Kim, Juhyoung Park, and Youngbin Kim",
        "link": "http://arxiv.org/abs/2505.06889v1",
        "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy."
    },
    {
        "date": "2025-05",
        "title": "NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks",
        "author": "Shunyao Wang, Ming Cheng, and Christina Dan Wang",
        "link": "http://arxiv.org/abs/2505.06864v1",
        "abstract": "Stochastic Discount Factor (SDF) models provide a unified framework for asset\npricing and risk assessment, yet traditional formulations struggle to\nincorporate unstructured textual information. We introduce NewsNet-SDF, a novel\ndeep learning framework that seamlessly integrates pretrained language model\nembeddings with financial time series through adversarial networks. Our\nmultimodal architecture processes financial news using GTE-multilingual models,\nextracts temporal patterns from macroeconomic data via LSTM networks, and\nnormalizes firm characteristics, fusing these heterogeneous information sources\nthrough an innovative adversarial training mechanism. Our dataset encompasses\napproximately 2.5 million news articles and 10,000 unique securities,\naddressing the computational challenges of processing and aligning text data\nwith financial time series. Empirical evaluations on U.S. equity data\n(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with\na Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%\nimprovement versus traditional SDF implementations, and a 74% reduction in\npricing errors compared to the Fama-French five-factor model. In comprehensive\ncomparisons, our deep learning approach consistently outperforms traditional,\nmodern, and other neural asset pricing models across all key metrics. Ablation\nstudies confirm that text embeddings contribute significantly more to model\nperformance than macroeconomic features, with news-derived principal components\nranking among the most influential determinants of SDF dynamics. These results\nvalidate the effectiveness of our multimodal deep learning approach in\nintegrating unstructured text with traditional financial data for more accurate\nasset pricing, providing new insights for digital intelligent decision-making\nin financial technology."
    },
    {
        "date": "2025-05",
        "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection",
        "author": "Xia Du, Jiajie Zhu, Jizhe Zhou, Chi-man Pun, Zheng Lin, Cong Wu, Zhe Chen, and Jun Luo",
        "link": "http://arxiv.org/abs/2505.06860v1",
        "abstract": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use."
    },
    {
        "date": "2025-05",
        "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2505.06831v1",
        "abstract": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios."
    },
    {
        "date": "2025-05",
        "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification",
        "author": "Dipayan Saha, Hasan Al Shaikh, Shams Tarek, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2505.06821v1",
        "abstract": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints",
        "author": "Abhishek Sinha, and Rahul Vaze",
        "link": "http://arxiv.org/abs/2505.06709v1",
        "abstract": "We revisit the Online Convex Optimization problem with adversarial\nconstraints (COCO) where, in each round, a learner is presented with a convex\ncost function and a convex constraint function, both of which may be chosen\nadversarially. The learner selects actions from a convex decision set in an\nonline fashion, with the goal of minimizing both regret and the cumulative\nconstraint violation (CCV) over a horizon of $T$ rounds. The best-known policy\nfor this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.\nIn this paper, we present a surprising improvement that achieves a\nsignificantly smaller CCV by trading it off with regret. Specifically, for any\nbounded convex cost and constraint functions, we propose an online policy that\nachieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$\nCCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a\ntunable parameter. We achieve this result by first considering the special case\nof $\\textsf{Constrained Expert}$ problem where the decision set is a\nprobability simplex and the cost and constraint functions are linear.\nLeveraging a new adaptive small-loss regret bound, we propose an efficient\npolicy for the $\\textsf{Constrained Expert}$ problem, that attains\n$O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV,\nwhere $N$ is the number of experts. The original problem is then reduced to the\n$\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an\nadditional smoothness assumption, we propose an efficient gradient-based policy\nattaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$\nCCV."
    },
    {
        "date": "2025-05",
        "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels",
        "author": "Xuefeng Jiang, Jia Li, Nannan Wu, Zhiyuan Wu, Xujing Li, Sheng Sun, Gang Xu, Yuwei Wang, Qi Li, and Min Liu",
        "link": "http://arxiv.org/abs/2505.06684v1",
        "abstract": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench."
    },
    {
        "date": "2025-05",
        "title": "Practical Reasoning Interruption Attacks on Reasoning Large Language Models",
        "author": "Yu Cui, and Cong Zuo",
        "link": "http://arxiv.org/abs/2505.06643v1",
        "abstract": "Reasoning large language models (RLLMs) have demonstrated outstanding\nperformance across a variety of tasks, yet they also expose numerous security\nvulnerabilities. Most of these vulnerabilities have centered on the generation\nof unsafe content. However, recent work has identified a distinct\n\"thinking-stopped\" vulnerability in DeepSeek-R1: under adversarial prompts, the\nmodel's reasoning process ceases at the system level and produces an empty\nfinal answer. Building upon this vulnerability, researchers developed a novel\nprompt injection attack, termed reasoning interruption attack, and also offered\nan initial analysis of its root cause. Through extensive experiments, we verify\nthe previous analyses, correct key errors based on three experimental findings,\nand present a more rigorous explanation of the fundamental causes driving the\nvulnerability. Moreover, existing attacks typically require over 2,000 tokens,\nimpose significant overhead, reduce practicality, and are easily detected. To\novercome these limitations, we propose the first practical reasoning\ninterruption attack. It succeeds with just 109 tokens by exploiting our newly\nuncovered \"reasoning token overflow\" (RTO) effect to overwrite the model's\nfinal answer, forcing it to return an invalid response. Experimental results\ndemonstrate that our proposed attack is highly effective. Furthermore, we\ndiscover that the method for triggering RTO differs between the official\nDeepSeek-R1 release and common unofficial deployments. As a broadened\napplication of RTO, we also construct a novel jailbreak attack that enables the\ntransfer of unsafe content within the reasoning tokens into final answer,\nthereby exposing it to the user. Our work carries significant implications for\nenhancing the security of RLLMs."
    },
    {
        "date": "2025-05",
        "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles",
        "author": "Rathin Chandra Shit, and Sharmila Subudhi",
        "link": "http://arxiv.org/abs/2505.06632v1",
        "abstract": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy."
    },
    {
        "date": "2025-05",
        "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation",
        "author": "Yuqin Lan",
        "link": "http://arxiv.org/abs/2505.06612v1",
        "abstract": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models."
    },
    {
        "date": "2025-05",
        "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
        "author": "Dongyoon Yang, Jihu Lee, and Yongdai Kim",
        "link": "http://arxiv.org/abs/2505.06580v1",
        "abstract": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios."
    },
    {
        "date": "2025-05",
        "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data",
        "author": "Ad\u00e8le H. Ribeiro, and Dominik Heider",
        "link": "http://arxiv.org/abs/2505.06542v1",
        "abstract": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making."
    },
    {
        "date": "2025-05",
        "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations",
        "author": "Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, and Thomas Poulet",
        "link": "http://arxiv.org/abs/2505.06502v1",
        "abstract": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper."
    },
    {
        "date": "2025-05",
        "title": "An In-kernel Forensics Engine for Investigating Evasive Attacks",
        "author": "Javad Zhandi, Lalchandra Rampersaud, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2505.06498v1",
        "abstract": "Over the years, adversarial attempts against critical services have become\nmore effective and sophisticated in launching low-profile attacks. This trend\nhas always been concerning. However, an even more alarming trend is the\nincreasing difficulty of collecting relevant evidence about these attacks and\nthe involved threat actors in the early stages before significant damage is\ndone. This issue puts defenders at a significant disadvantage, as it becomes\nexceedingly difficult to understand the attack details and formulate an\nappropriate response. Developing robust forensics tools to collect evidence\nabout modern threats has never been easy. One main challenge is to provide a\nrobust trade-off between achieving sufficient visibility while leaving minimal\ndetectable artifacts. This paper will introduce LASE, an open-source\nLow-Artifact Forensics Engine to perform threat analysis and forensics in\nWindows operating system. LASE augments current analysis tools by providing\ndetailed, system-wide monitoring capabilities while minimizing detectable\nartifacts. We designed multiple deployment scenarios, showing LASE's potential\nin evidence gathering and threat reasoning in a real-world setting. By making\nLASE and its execution trace data available to the broader research community,\nthis work encourages further exploration in the field by reducing the\nengineering costs for threat analysis and building a longitudinal behavioral\nanalysis catalog for diverse security domains."
    },
    {
        "date": "2025-05",
        "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
        "author": "Jiawei Guo, and Haipeng Cai",
        "link": "http://arxiv.org/abs/2505.06493v1",
        "abstract": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning."
    },
    {
        "date": "2025-05",
        "title": "Learning from the Good Ones: Risk Profiling-Based Defenses Against Evasion Attacks on DNNs",
        "author": "Mohammed Elnawawy, Gargi Mitra, Shahrear Iqbal, and Karthik Pattabiraman",
        "link": "http://arxiv.org/abs/2505.06477v1",
        "abstract": "Safety-critical applications such as healthcare and autonomous vehicles use\ndeep neural networks (DNN) to make predictions and infer decisions. DNNs are\nsusceptible to evasion attacks, where an adversary crafts a malicious data\ninstance to trick the DNN into making wrong decisions at inference time.\nExisting defenses that protect DNNs against evasion attacks are either static\nor dynamic. Static defenses are computationally efficient but do not adapt to\nthe evolving threat landscape, while dynamic defenses are adaptable but suffer\nfrom an increased computational overhead. To combine the best of both worlds,\nin this paper, we propose a novel risk profiling framework that uses a\nrisk-aware strategy to selectively train static defenses using victim instances\nthat exhibit the most resilient features and are hence more resilient against\nan evasion attack. We hypothesize that training existing defenses on instances\nthat are less vulnerable to the attack enhances the adversarial detection rate\nby reducing false negatives. We evaluate the efficacy of our risk-aware\nselective training strategy on a blood glucose management system that\ndemonstrates how training static anomaly detectors indiscriminately may result\nin an increased false negative rate, which could be life-threatening in\nsafety-critical applications. Our experiments show that selective training on\nthe less vulnerable patients achieves a recall increase of up to 27.5\\% with\nminimal impact on precision compared to indiscriminate training."
    },
    {
        "date": "2025-05",
        "title": "\"vcd2df\" -- Leveraging Data Science Insights for Hardware Security Research",
        "author": "Calvin Deutschbein, and Jimmy Ostler",
        "link": "http://arxiv.org/abs/2505.06470v1",
        "abstract": "In this work, we hope to expand the universe of security practitioners of\nopen-source hardware by creating a bridge from hardware design languages (HDLs)\nto data science languages like Python and R through libraries that converge VCD\n(value change dump) files into data frames, the expected input type of the\nmodern data science tools. We show how insights can be derived in high-level\nlanguages from register transfer level (RTL) trace data. Additional, we show a\npromising future direction in hardware security research leveraging the\nparallelism of the Spark DataFrame."
    },
    {
        "date": "2025-05",
        "title": "Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning",
        "author": "Syed Mhamudul Hasan, Hussein Zangoti, Iraklis Anagnostopoulos, and Abdur R. Shahid",
        "link": "http://arxiv.org/abs/2505.06454v1",
        "abstract": "Recent studies have shown that sponge attacks can significantly increase the\nenergy consumption and inference latency of deep neural networks (DNNs).\nHowever, prior work has focused primarily on computer vision and natural\nlanguage processing tasks, overlooking the growing use of lightweight AI models\nin sensing-based applications on resource-constrained devices, such as those in\nInternet of Things (IoT) environments. These attacks pose serious threats of\nenergy depletion and latency degradation in systems where limited battery\ncapacity and real-time responsiveness are critical for reliable operation. This\npaper makes two key contributions. First, we present the first systematic\nexploration of energy-latency sponge attacks targeting sensing-based AI models.\nUsing wearable sensing-based AI as a case study, we demonstrate that sponge\nattacks can substantially degrade performance by increasing energy consumption,\nleading to faster battery drain, and by prolonging inference latency. Second,\nto mitigate such attacks, we investigate model pruning, a widely adopted\ncompression technique for resource-constrained AI, as a potential defense. Our\nexperiments show that pruning-induced sparsity significantly improves model\nresilience against sponge poisoning. We also quantify the trade-offs between\nmodel efficiency and attack resilience, offering insights into the security\nimplications of model compression in sensing-based AI systems deployed in IoT\nenvironments."
    },
    {
        "date": "2025-05",
        "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving",
        "author": "Ming Liu, Siyuan Liang, Koushik Howlader, Liwen Wang, Dacheng Tao, and Wensheng Zhang",
        "link": "http://arxiv.org/abs/2505.06413v1",
        "abstract": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems."
    },
    {
        "date": "2025-05",
        "title": "Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2505.06409v1",
        "abstract": "As AI models scale to billions of parameters and operate with increasing\nautonomy, ensuring their safe, reliable operation demands engineering-grade\nsecurity and assurance frameworks. This paper presents an enterprise-level,\nrisk-aware, security-by-design approach for large-scale autonomous AI systems,\nintegrating standardized threat metrics, adversarial hardening techniques, and\nreal-time anomaly detection into every phase of the development lifecycle. We\ndetail a unified pipeline - from design-time risk assessments and secure\ntraining protocols to continuous monitoring and automated audit logging - that\ndelivers provable guarantees of model behavior under adversarial and\noperational stress. Case studies in national security, open-source model\ngovernance, and industrial automation demonstrate measurable reductions in\nvulnerability and compliance overhead. Finally, we advocate cross-sector\ncollaboration - uniting engineering teams, standards bodies, and regulatory\nagencies - to institutionalize these technical safeguards within a resilient,\nend-to-end assurance ecosystem for the next generation of AI."
    },
    {
        "date": "2025-05",
        "title": "Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers",
        "author": "Massimiliano Albanese, Xinming Ou, Kevin Lybarger, Daniel Lende, and Dmitry Goldgof",
        "link": "http://arxiv.org/abs/2505.06394v1",
        "abstract": "Security Operations Centers (SOCs) face growing challenges in managing\ncybersecurity threats due to an overwhelming volume of alerts, a shortage of\nskilled analysts, and poorly integrated tools. Human-AI collaboration offers a\npromising path to augment the capabilities of SOC analysts while reducing their\ncognitive overload. To this end, we introduce an AI-driven human-machine\nco-teaming paradigm that leverages large language models (LLMs) to enhance\nthreat intelligence, alert triage, and incident response workflows. We present\na vision in which LLM-based AI agents learn from human analysts the tacit\nknowledge embedded in SOC operations, enabling the AI agents to improve their\nperformance on SOC tasks through this co-teaming. We invite SOCs to collaborate\nwith us to further develop this process and uncover replicable patterns where\nhuman-AI co-teaming yields measurable improvements in SOC productivity."
    },
    {
        "date": "2025-05",
        "title": "Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms",
        "author": "Adrien Chan-Hon-Tong, Aur\u00e9lien Plyer, Baptiste Cadalen, and Laurent Serre",
        "link": "http://arxiv.org/abs/2505.06389v1",
        "abstract": "Sensor-based guidance is required for long-range platforms. To bypass the\nstructural limitation of classical registration on reference image framework,\nwe offer in this paper to encode a stack of images of the scene into a deep\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\nwhen the scene can or can not be snowy)."
    },
    {
        "date": "2025-05",
        "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal",
        "author": "Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2505.06381v1",
        "abstract": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet)."
    },
    {
        "date": "2025-05",
        "title": "Offensive Security for AI Systems: Concepts, Practices, and Applications",
        "author": "Josh Harguess, and Chris M. Ward",
        "link": "http://arxiv.org/abs/2505.06380v1",
        "abstract": "As artificial intelligence (AI) systems become increasingly adopted across\nsectors, the need for robust, proactive security strategies is paramount.\nTraditional defensive measures often fall short against the unique and evolving\nthreats facing AI-driven technologies, making offensive security an essential\napproach for identifying and mitigating risks. This paper presents a\ncomprehensive framework for offensive security in AI systems, emphasizing\nproactive threat simulation and adversarial testing to uncover vulnerabilities\nthroughout the AI lifecycle. We examine key offensive security techniques,\nincluding weakness and vulnerability assessment, penetration testing, and red\nteaming, tailored specifically to address AI's unique susceptibilities. By\nsimulating real-world attack scenarios, these methodologies reveal critical\ninsights, informing stronger defensive strategies and advancing resilience\nagainst emerging threats. This framework advances offensive AI security from\ntheoretical concepts to practical, actionable methodologies that organizations\ncan implement to strengthen their AI systems against emerging threats."
    },
    {
        "date": "2025-05",
        "title": "Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment",
        "author": "Muhy Eddin Za'ter, Amir Sajad, and Bri-Mathias Hodge",
        "link": "http://arxiv.org/abs/2505.06207v1",
        "abstract": "This paper introduces a novel approach to the power system security\nassessment using Multi-Task Learning (MTL), and reformulating the problem as a\nmulti-label classification task. The proposed MTL framework simultaneously\nassesses static, voltage, transient, and small-signal stability, improving both\naccuracy and interpretability with respect to the most state of the art machine\nlearning methods. It consists of a shared encoder and multiple decoders,\nenabling knowledge transfer between stability tasks. Experiments on the IEEE\n68-bus system demonstrate a measurable superior performance of the proposed\nmethod compared to the extant state-of-the-art approaches."
    },
    {
        "date": "2025-05",
        "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients",
        "author": "Jinsheng Yuan, Yuhang Hao, Weisi Guo, Yun Wu, and Chongyan Gu",
        "link": "http://arxiv.org/abs/2505.06335v1",
        "abstract": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design."
    },
    {
        "date": "2025-05",
        "title": "NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines",
        "author": "Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, and Amit Sheth",
        "link": "http://arxiv.org/abs/2505.06333v1",
        "abstract": "In modern assembly pipelines, identifying anomalies is crucial in ensuring\nproduct quality and operational efficiency. Conventional single-modality\nmethods fail to capture the intricate relationships required for precise\nanomaly prediction in complex predictive environments with abundant data and\nmultiple modalities. This paper proposes a neurosymbolic AI and fusion-based\napproach for multimodal anomaly prediction in assembly pipelines. We introduce\na time series and image-based fusion model that leverages decision-level fusion\ntechniques. Our research builds upon three primary novel approaches in\nmultimodal learning: time series and image-based decision-level fusion\nmodeling, transfer learning for fusion, and knowledge-infused learning. We\nevaluate the novel method using our derived and publicly available multimodal\ndataset and conduct comprehensive ablation studies to assess the impact of our\npreprocessing techniques and fusion model compared to traditional baselines.\nThe results demonstrate that a neurosymbolic AI-based fusion approach that uses\ntransfer learning can effectively harness the complementary strengths of time\nseries and image data, offering a robust and interpretable approach for anomaly\nprediction in assembly pipelines with enhanced performance. \\noindent The\ndatasets, codes to reproduce the results, supplementary materials, and demo are\navailable at https://github.com/ChathurangiShyalika/NSF-MAP."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies",
        "author": "Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, and Junliang Du",
        "link": "http://arxiv.org/abs/2505.06145v1",
        "abstract": "Few-shot text classification has important application value in low-resource\nenvironments. This paper proposes a strategy that combines adaptive\nfine-tuning, contrastive learning, and regularization optimization to improve\nthe classification performance of Transformer-based models. Experiments on the\nFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform\nwell in few-shot tasks, especially in the 5-shot setting, which can more\neffectively capture text features and improve classification accuracy. The\nexperiment also found that there are significant differences in the\nclassification difficulty of different relationship categories. Some categories\nhave fuzzy semantic boundaries or complex feature distributions, making it\ndifficult for the standard cross entropy loss to learn the discriminative\ninformation required to distinguish categories. By introducing contrastive loss\nand regularization loss, the generalization ability of the model is enhanced,\neffectively alleviating the overfitting problem in few-shot environments. In\naddition, the research results show that the use of Transformer models or\ngenerative architectures with stronger self-attention mechanisms can help\nimprove the stability and accuracy of few-shot classification."
    },
    {
        "date": "2025-05",
        "title": "Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation",
        "author": "Julian F. Schumann, Jeroen Hagenus, Frederik Baymler Mathiesen, and Arkady Zgonnikov",
        "link": "http://arxiv.org/abs/2505.06134v1",
        "abstract": "Trajectory prediction is a key element of autonomous vehicle systems,\nenabling them to anticipate and react to the movements of other road users.\nEvaluating the robustness of prediction models against adversarial attacks is\nessential to ensure their reliability in real-world traffic. However, current\napproaches tend to focus on perturbing the past positions of surrounding\nagents, which can generate unrealistic scenarios and overlook critical\nvulnerabilities. This limitation may result in overly optimistic assessments of\nmodel performance in real-world conditions.\n  In this work, we demonstrate that perturbing not just past but also future\nstates of adversarial agents can uncover previously undetected weaknesses and\nthereby provide a more rigorous evaluation of model robustness. Our novel\napproach incorporates dynamic constraints and preserves tactical behaviors,\nenabling more effective and realistic adversarial attacks. We introduce new\nperformance measures to assess the realism and impact of these adversarial\ntrajectories. Testing our method on a state-of-the-art prediction model\nrevealed significant increases in prediction errors and collision rates under\nadversarial conditions. Qualitative analysis further showed that our attacks\ncan expose critical weaknesses, such as the inability of the model to detect\npotential collisions in what appear to be safe predictions. These results\nunderscore the need for more comprehensive adversarial testing to better\nevaluate and improve the reliability of trajectory prediction models for\nautonomous vehicles."
    },
    {
        "date": "2025-05",
        "title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations",
        "author": "Shuaiyi Huang, Mara Levy, Anubhav Gupta, Daniel Ekpo, Ruijie Zheng, and Abhinav Shrivastava",
        "link": "http://arxiv.org/abs/2505.06079v1",
        "abstract": "Preference feedback collected by human or VLM annotators is often noisy,\npresenting a significant challenge for preference-based reinforcement learning\nthat relies on accurate preference labels. To address this challenge, we\npropose TREND, a novel framework that integrates few-shot expert demonstrations\nwith a tri-teaching strategy for effective noise mitigation. Our method trains\nthree reward models simultaneously, where each model views its small-loss\npreference pairs as useful knowledge and teaches such useful pairs to its peer\nnetwork for updating the parameters. Remarkably, our approach requires as few\nas one to three expert demonstrations to achieve high performance. We evaluate\nTREND on various robotic manipulation tasks, achieving up to 90% success rates\neven with noise levels as high as 40%, highlighting its effective robustness in\nhandling noisy preference feedback. Project page:\nhttps://shuaiyihuang.github.io/publications/TREND."
    },
    {
        "date": "2025-05",
        "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
        "author": "Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, and Christof Monz",
        "link": "http://arxiv.org/abs/2505.06027v1",
        "abstract": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning."
    },
    {
        "date": "2025-05",
        "title": "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward Secure Inference in FinTech Applications",
        "author": "Faneela, Baraq Ghaleb, Jawad Ahmad, William J. Buchanan, and Sana Ullah Jan",
        "link": "http://arxiv.org/abs/2505.05920v1",
        "abstract": "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems."
    },
    {
        "date": "2025-05",
        "title": "A Taxonomy of Attacks and Defenses in Split Learning",
        "author": "Aqsa Shabbir, Halil \u0130brahim Kanpak, Alptekin K\u00fcp\u00e7\u00fc, and Sinem Sav",
        "link": "http://arxiv.org/abs/2505.05872v1",
        "abstract": "Split Learning (SL) has emerged as a promising paradigm for distributed deep\nlearning, allowing resource-constrained clients to offload portions of their\nmodel computation to servers while maintaining collaborative learning. However,\nrecent research has demonstrated that SL remains vulnerable to a range of\nprivacy and security threats, including information leakage, model inversion,\nand adversarial attacks. While various defense mechanisms have been proposed, a\nsystematic understanding of the attack landscape and corresponding\ncountermeasures is still lacking. In this study, we present a comprehensive\ntaxonomy of attacks and defenses in SL, categorizing them along three key\ndimensions: employed strategies, constraints, and effectiveness. Furthermore,\nwe identify key open challenges and research gaps in SL based on our\nsystematization, highlighting potential future directions."
    },
    {
        "date": "2025-05",
        "title": "Intrusion Detection System Using Deep Learning for Network Security",
        "author": "Soham Chatterjee, Satvik Chaudhary, and Aswani Kumar Cherukuri",
        "link": "http://arxiv.org/abs/2505.05810v1",
        "abstract": "As the number of cyberattacks and their particualr nature escalate, the need\nfor effective intrusion detection systems (IDS) has become indispensable for\nensuring the security of contemporary networks. Adaptive and more sophisticated\nthreats are often beyond the reach of traditional approaches to intrusion\ndetection and access control. This paper proposes an experimental evaluation of\nIDS models based on deep learning techniques, focusing on the classification of\nnetwork traffic into malicious and benign categories. We analyze and retrain an\nassortment of architectures, such as Convolutional Neural Networks (CNN),\nArtificial Neural Networks (ANN), and LSTM models. Each model was tested based\non a real dataset simulated in a multi-faceted and everchanging network traffic\nenvironment. Among the tested models, the best achieved an accuracy of 96\npercent, underscoring the potential of deep learning models in improving\nefficiency and rapid response in IDS systems. The goal of the research is to\ndemonstrate the effectiveness of distinct architectures and their corresponding\ntrade-offs to enhance framework development for adaptive IDS solutions and\nimprove overall network security."
    },
    {
        "date": "2025-05",
        "title": "Efficient Full-Stack Private Federated Deep Learning with Post-Quantum Security",
        "author": "Yiwei Zhang, Rouzbeh Behnia, Attila A. Yavuz, Reza Ebrahimi, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2505.05751v1",
        "abstract": "Federated learning (FL) enables collaborative model training while preserving\nuser data privacy by keeping data local. Despite these advantages, FL remains\nvulnerable to privacy attacks on user updates and model parameters during\ntraining and deployment. Secure aggregation protocols have been proposed to\nprotect user updates by encrypting them, but these methods often incur high\ncomputational costs and are not resistant to quantum computers. Additionally,\ndifferential privacy (DP) has been used to mitigate privacy leakages, but\nexisting methods focus on secure aggregation or DP, neglecting their potential\nsynergies. To address these gaps, we introduce Beskar, a novel framework that\nprovides post-quantum secure aggregation, optimizes computational overhead for\nFL settings, and defines a comprehensive threat model that accounts for a wide\nspectrum of adversaries. We also integrate DP into different stages of FL\ntraining to enhance privacy protection in diverse scenarios. Our framework\nprovides a detailed analysis of the trade-offs between security, performance,\nand model accuracy, representing the first thorough examination of secure\naggregation protocols combined with various DP approaches for post-quantum\nsecure FL. Beskar aims to address the pressing privacy and security issues FL\nwhile ensuring quantum-safety and robust performance."
    },
    {
        "date": "2025-05",
        "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
        "author": "Julia Shuieh, Prasann Singhal, Apaar Shanker, John Heyer, George Pu, and Samuel Denton",
        "link": "http://arxiv.org/abs/2505.05704v1",
        "abstract": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."
    },
    {
        "date": "2025-05",
        "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology",
        "author": "Fuyao Chen, Yuexi Du, Tal Zeevi, Nicha C. Dvornek, and John A. Onofrey",
        "link": "http://arxiv.org/abs/2505.05689v1",
        "abstract": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging."
    },
    {
        "date": "2025-05",
        "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval",
        "author": "Alexander Most, Joseph Winjum, Ayan Biswas, Shawn Jones, Nishath Rajiv Ranasinghe, Dan O'Malley, and Manish Bhattarai",
        "link": "http://arxiv.org/abs/2505.05666v1",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments."
    },
    {
        "date": "2025-05",
        "title": "On Corruption-Robustness in Performative Reinforcement Learning",
        "author": "Vasilis Pollatos, Debmalya Mandal, and Goran Radanovic",
        "link": "http://arxiv.org/abs/2505.05609v1",
        "abstract": "In performative Reinforcement Learning (RL), an agent faces a\npolicy-dependent environment: the reward and transition functions depend on the\nagent's policy. Prior work on performative RL has studied the convergence of\nrepeated retraining approaches to a performatively stable policy. In the finite\nsample regime, these approaches repeatedly solve for a saddle point of a\nconvex-concave objective, which estimates the Lagrangian of a regularized\nversion of the reinforcement learning problem. In this paper, we aim to extend\nsuch repeated retraining approaches, enabling them to operate under corrupted\ndata. More specifically, we consider Huber's $\\epsilon$-contamination model,\nwhere an $\\epsilon$ fraction of data points is corrupted by arbitrary\nadversarial noise. We propose a repeated retraining approach based on\nconvex-concave optimization under corrupted gradients and a novel\nproblem-specific robust mean estimator for the gradients. We prove that our\napproach exhibits last-iterate convergence to an approximately stable policy,\nwith the approximation error linear in $\\sqrt{\\epsilon}$. We experimentally\ndemonstrate the importance of accounting for corruption in performative RL."
    },
    {
        "date": "2025-05",
        "title": "QUIC-Exfil: Exploiting QUIC's Server Preferred Address Feature to Perform Data Exfiltration Attacks",
        "author": "Thomas Gr\u00fcbl, Weijie Niu, Jan von der Assen, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2505.05292v1",
        "abstract": "The QUIC protocol is now widely adopted by major tech companies and accounts\nfor a significant fraction of today's Internet traffic. QUIC's multiplexing\ncapabilities, encrypted headers, dynamic IP address changes, and encrypted\nparameter negotiations make the protocol not only more efficient, secure, and\ncensorship-resistant, but also practically unmanageable by firewalls. This\nopens doors for attackers who may exploit certain traits of the QUIC protocol\nto perform targeted attacks, such as data exfiltration attacks. Whereas\nexisting data exfiltration techniques, such as TLS and DNS-based exfiltration,\ncan be detected on a firewall level, QUIC-based data exfiltration is more\ndifficult to detect, since changes in IP addresses and ports are inherent to\nthe protocol's normal behavior. To show the feasibility of a QUIC-based data\nexfiltration attack, we introduce a novel method leveraging the server\npreferred address feature of the QUIC protocol and, thus, allows an attacker to\nexfiltrate sensitive data from an infected machine to a malicious server,\ndisguised as a server-side connection migration. The attack is implemented as a\nproof of concept tool in Rust. We evaluated the performance of five anomaly\ndetection classifiers - Random Forest, Multi-Layer Perceptron, Support Vector\nMachine, Autoencoder, and Isolation Forest - trained on datasets collected from\nthree network traffic scenarios. The classifiers were trained on over 700K\nbenign and malicious QUIC packets and 786 connection migration events, but were\nunable to detect the data exfiltration attempts. Furthermore, post-analysis of\nthe traffic captures did not reveal any identifiable fingerprint. As part of\nour evaluation, we also interviewed five leading firewall vendors and found\nthat, as of today, no major firewall vendor implements functionality capable of\ndistinguishing between benign and malicious QUIC connection migrations."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration",
        "author": "Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, and George Vouros",
        "link": "http://arxiv.org/abs/2505.05262v1",
        "abstract": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks."
    },
    {
        "date": "2025-05",
        "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality",
        "author": "Chara Podimata",
        "link": "http://arxiv.org/abs/2505.05211v1",
        "abstract": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems."
    },
    {
        "date": "2025-05",
        "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks",
        "author": "Yixin Cheng, Hongcheng Guo, Yangming Li, and Leonid Sigal",
        "link": "http://arxiv.org/abs/2505.05190v2",
        "abstract": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."
    },
    {
        "date": "2025-05",
        "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting",
        "author": "Elad Feldman, Jacob Shams, Dudi Biton, Alfred Chen, Shaoyuan Xie, Satoru Koda, Yisroel Mirsky, Asaf Shabtai, Yuval Elovici, and Ben Nassi",
        "link": "http://arxiv.org/abs/2505.05183v1",
        "abstract": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection."
    },
    {
        "date": "2025-05",
        "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP",
        "author": "Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, and James Bailey",
        "link": "http://arxiv.org/abs/2505.05528v1",
        "abstract": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."
    },
    {
        "date": "2025-05",
        "title": "Integrating Communication, Sensing, and Security: Progress and Prospects of PLS in ISAC Systems",
        "author": "Waqas Aman, El-Mehdi Illi, Marwa Qaraqe, and Saif Al-Kuwari",
        "link": "http://arxiv.org/abs/2505.05090v1",
        "abstract": "The sixth generation of wireless networks defined several key performance\nindicators (KPIs) for assessing its networks, mainly in terms of reliability,\ncoverage, and sensing. In this regard, remarkable attention has been paid\nrecently to the integrated sensing and communication (ISAC) paradigm as an\nenabler for efficiently and jointly performing communication and sensing using\nthe same spectrum and hardware resources. On the other hand, ensuring\ncommunication and data security has been an imperative requirement for wireless\nnetworks throughout their evolution. The physical-layer security (PLS) concept\npaved the way to catering to the security needs in wireless networks in a\nsustainable way while guaranteeing theoretically secure transmissions,\nindependently of the computational capacity of adversaries. Therefore, it is of\nparamount importance to consider a balanced trade-off between communication\nreliability, sensing, and security in future networks, such as the 5G and\nbeyond, and the 6G. In this paper, we provide a comprehensive and system-wise\nreview of designed secure ISAC systems from a PLS point of view. In particular,\nthe impact of various physical-layer techniques, schemes, and wireless\ntechnologies to ensure the sensing-security trade-off is studied from the\nsurveyed work. Furthermore, the amalgamation of PLS and ISAC is analyzed in a\nbroader impact by considering attacks targeting data confidentiality,\ncommunication covertness, and sensing spoofing. The paper also serves as a\ntutorial by presenting several theoretical foundations on ISAC and PLS, which\nrepresent a practical guide for readers to develop novel secure ISAC network\ndesigns."
    },
    {
        "date": "2025-05",
        "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
        "author": "Jaehyun Jeon, Jang Han Yoon, Min Soo Kim, Sumin Shim, Yejin Choi, Hanbin Kim, and Youngjae Yu",
        "link": "http://arxiv.org/abs/2505.05026v2",
        "abstract": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly."
    },
    {
        "date": "2025-05",
        "title": "Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought",
        "author": "Mingfei Zeng, Ming Xie, Xixi Zheng, Chunhai Li, Chuan Zhang, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2505.06307v1",
        "abstract": "The rapid development of Internet of Things (IoT) technology has transformed\npeople's way of life and has a profound impact on both production and daily\nactivities. However, with the rapid advancement of IoT technology, the security\nof IoT devices has become an unavoidable issue in both research and\napplications. Although some efforts have been made to detect or mitigate IoT\nsecurity vulnerabilities, they often struggle to adapt to the complexity of IoT\nenvironments, especially when dealing with dynamic security scenarios. How to\nautomatically, efficiently, and accurately understand these vulnerabilities\nremains a challenge. To address this, we propose an IoT security assistant\ndriven by Large Language Model (LLM), which enhances the LLM's understanding of\nIoT security vulnerabilities and related threats. The aim of the ICoT method we\npropose is to enable the LLM to understand security issues by breaking down the\nvarious dimensions of security vulnerabilities and generating responses\ntailored to the user's specific needs and expertise level. By incorporating\nICoT, LLM can gradually analyze and reason through complex security scenarios,\nresulting in more accurate, in-depth, and personalized security recommendations\nand solutions. Experimental results show that, compared to methods relying\nsolely on LLM, our proposed LLM-driven IoT security assistant significantly\nimproves the understanding of IoT security issues through the ICoT approach and\nprovides personalized solutions based on the user's identity, demonstrating\nhigher accuracy and reliability."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection",
        "author": "Xuesong Liu, Tianyu Hao, and Emmett J. Ientilucci",
        "link": "http://arxiv.org/abs/2505.05008v1",
        "abstract": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios."
    },
    {
        "date": "2025-05",
        "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
        "author": "Brian Choi, Shu Wang, Isabelle Choi, and Kun Sun",
        "link": "http://arxiv.org/abs/2505.04977v1",
        "abstract": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy."
    },
    {
        "date": "2025-05",
        "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators",
        "author": "Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, and Xue Bin Peng",
        "link": "http://arxiv.org/abs/2505.04961v1",
        "abstract": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w."
    },
    {
        "date": "2025-05",
        "title": "RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models",
        "author": "Zhenhua Xu, Zhebo Wang, Maike Li, Wenpeng Xing, Chunqiang Hu, Chen Zhi, and Meng Han",
        "link": "http://arxiv.org/abs/2505.06304v1",
        "abstract": "Recent advances in large language models (LLMs) have underscored the\nimportance of safeguarding intellectual property rights through robust\nfingerprinting techniques. Traditional fingerprint verification approaches\ntypically focus on a single model, seeking to improve the robustness of its\nfingerprint.However, these single-model methods often struggle to capture\nintrinsic commonalities across multiple related models. In this paper, we\npropose RAP-SM (Robust Adversarial Prompt via Shadow Models), a novel framework\nthat extracts a public fingerprint for an entire series of LLMs. Experimental\nresults demonstrate that RAP-SM effectively captures the intrinsic\ncommonalities among different models while exhibiting strong adversarial\nrobustness. Our findings suggest that RAP-SM presents a valuable avenue for\nscalable fingerprint verification, offering enhanced protection against\npotential model breaches in the era of increasingly prevalent LLMs."
    },
    {
        "date": "2025-05",
        "title": "Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition",
        "author": "Xiaozhou Ye, and Kevin I-Kai Wang",
        "link": "http://arxiv.org/abs/2505.06301v1",
        "abstract": "Cross-user variability in Human Activity Recognition (HAR) remains a critical\nchallenge due to differences in sensor placement, body dynamics, and behavioral\npatterns. Traditional methods often fail to capture biomechanical invariants\nthat persist across users, limiting their generalization capability. We propose\nan Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)\nframework that integrates anatomical correlation knowledge into a unified graph\nneural network (GNN) architecture. By modeling three biomechanically motivated\nrelationships together-Interconnected Units, Analogous Units, and Lateral\nUnits-our method encodes domain-invariant features while addressing\nuser-specific variability through Variational Edge Feature Extractor. A\nGradient Reversal Layer (GRL) enforces adversarial domain generalization,\nensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and\nDSADS datasets demonstrate state-of-the-art performance. Our work bridges\nbiomechanical principles with graph-based adversarial learning by integrating\ninformation fusion techniques. This fusion of information underpins our unified\nand generalized model for cross-user HAR."
    },
    {
        "date": "2025-05",
        "title": "CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability",
        "author": "Taisuke Kobayashi",
        "link": "http://arxiv.org/abs/2505.04897v1",
        "abstract": "Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction."
    },
    {
        "date": "2025-05",
        "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on Memory",
        "author": "MD Mahady Hassan, Shanto Roy, and Reza Rahaeimehr",
        "link": "http://arxiv.org/abs/2505.04896v1",
        "abstract": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
    },
    {
        "date": "2025-05",
        "title": "FedRE: Robust and Effective Federated Learning with Privacy Preference",
        "author": "Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, and Ruixuan Li",
        "link": "http://arxiv.org/abs/2505.04889v1",
        "abstract": "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods."
    },
    {
        "date": "2025-05",
        "title": "Robust ML Auditing using Prior Knowledge",
        "author": "Jade Garcia Bourr\u00e9e, Augustin Godinot, Martijn De Vos, Milos Vujasinovic, Sayan Biswas, Gilles Tredan, Erwan Le Merrer, and Anne-Marie Kermarrec",
        "link": "http://arxiv.org/abs/2505.04796v1",
        "abstract": "The rapid adoption of ML decision-making systems across products and services\nhas led to a set of regulations on how such systems should behave and be built.\nAmong all the technical challenges to enforcing these regulations, one crucial,\nyet under-explored problem is the risk of manipulation while these systems are\nbeing audited for fairness. This manipulation occurs when a platform\ndeliberately alters its answers to a regulator to pass an audit without\nmodifying its answers to other users. In this paper, we introduce a novel\napproach to manipulation-proof auditing by taking into account the auditor's\nprior knowledge of the task solved by the platform. We first demonstrate that\nregulators must not rely on public priors (e.g. a public dataset), as platforms\ncould easily fool the auditor in such cases. We then formally establish the\nconditions under which an auditor can prevent audit manipulations using prior\nknowledge about the ground truth. Finally, our experiments with two standard\ndatasets exemplify the maximum level of unfairness a platform can hide before\nbeing detected as malicious. Our formalization and generalization of\nmanipulation-proof auditing with a prior opens up new research directions for\nmore robust fairness audits."
    },
    {
        "date": "2025-05",
        "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World",
        "author": "Bangyan Liao, Zhenjun Zhao, Haoang Li, Yi Zhou, Yingping Zeng, Hao Li, and Peidong Liu",
        "link": "http://arxiv.org/abs/2505.04788v1",
        "abstract": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n``soft'' association scheme, realized via a truncated multi-selection error,\nthat allows for joint estimation of VPs' locations and line-VP associations.\nThis approach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called \\textbf{GlobustVP}), which independently searches for one VP and\nits associated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that\n\\textbf{GlobustVP} achieves a favorable balance between efficiency, robustness,\nand global optimality compared to previous works. The code is publicly\navailable at https://github.com/WU-CVGL/GlobustVP."
    },
    {
        "date": "2025-05",
        "title": "Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain",
        "author": "Spyridon Raptis, and Haralampos-G. Stratigopoulos",
        "link": "http://arxiv.org/abs/2505.06299v1",
        "abstract": "As Spiking Neural Networks (SNNs) gain traction across various applications,\nunderstanding their security vulnerabilities becomes increasingly important. In\nthis work, we focus on the adversarial attacks, which is perhaps the most\nconcerning threat. An adversarial attack aims at finding a subtle input\nperturbation to fool the network's decision-making. We propose two novel\nadversarial attack algorithms for SNNs: an input-specific attack that crafts\nadversarial samples from specific dataset inputs and a universal attack that\ngenerates a reusable patch capable of inducing misclassification across most\ninputs, thus offering practical feasibility for real-time deployment. The\nalgorithms are gradient-based operating in the spiking domain proving to be\neffective across different evaluation metrics, such as adversarial accuracy,\nstealthiness, and generation time. Experimental results on two widely used\nneuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our\nproposed attacks surpass in all metrics all existing state-of-the-art methods.\nAdditionally, we present the first demonstration of adversarial attack\ngeneration in the sound domain using the SHD dataset."
    },
    {
        "date": "2025-05",
        "title": "Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting",
        "author": "Shai Feldman, Stephen Bates, and Yaniv Romano",
        "link": "http://arxiv.org/abs/2505.04733v1",
        "abstract": "We introduce a framework for robust uncertainty quantification in situations\nwhere labeled training data are corrupted, through noisy or missing labels. We\nbuild on conformal prediction, a statistical tool for generating prediction\nsets that cover the test label with a pre-specified probability. The validity\nof conformal prediction, however, holds under the i.i.d assumption, which does\nnot hold in our setting due to the corruptions in the data. To account for this\ndistribution shift, the privileged conformal prediction (PCP) method proposed\nleveraging privileged information (PI) -- additional features available only\nduring training -- to re-weight the data distribution, yielding valid\nprediction sets under the assumption that the weights are accurate. In this\nwork, we analyze the robustness of PCP to inaccuracies in the weights. Our\nanalysis indicates that PCP can still yield valid uncertainty estimates even\nwhen the weights are poorly estimated. Furthermore, we introduce uncertain\nimputation (UI), a new conformal method that does not rely on weight\nestimation. Instead, we impute corrupted labels in a way that preserves their\nuncertainty. Our approach is supported by theoretical guarantees and validated\nempirically on both synthetic and real benchmarks. Finally, we show that these\ntechniques can be integrated into a triply robust framework, ensuring\nstatistically valid predictions as long as at least one underlying method is\nvalid."
    },
    {
        "date": "2025-05",
        "title": "Qualitative Analysis of $\u03c9$-Regular Objectives on Robust MDPs",
        "author": "Ali Asadi, Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, and Ali Shafiee",
        "link": "http://arxiv.org/abs/2505.04539v1",
        "abstract": "Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states."
    },
    {
        "date": "2025-05",
        "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation",
        "author": "Edward Humes, Xiaomin Lin, Uttej Kallakuri, and Tinoosh Mohsenin",
        "link": "http://arxiv.org/abs/2505.04529v1",
        "abstract": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU."
    },
    {
        "date": "2025-05",
        "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective Encryption",
        "author": "Mohammad Waquas Usmani, Susmit Shannigrahi, and Michael Zink",
        "link": "http://arxiv.org/abs/2505.04466v1",
        "abstract": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
    },
    {
        "date": "2025-05",
        "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
        "author": "Xiaoyu Xu, Minxin Du, Qingqing Ye, and Haibo Hu",
        "link": "http://arxiv.org/abs/2505.04416v1",
        "abstract": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."
    },
    {
        "date": "2025-05",
        "title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning",
        "author": "Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2505.04339v1",
        "abstract": "DBSCAN, a well-known density-based clustering algorithm, has gained\nwidespread popularity and usage due to its effectiveness in identifying\nclusters of arbitrary shapes and handling noisy data. However, it encounters\nchallenges in producing satisfactory cluster results when confronted with\ndatasets of varying density scales, a common scenario in real-world\napplications. In this paper, we propose a novel Adaptive and Robust DBSCAN with\nMulti-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,\nwe model the initial dataset as a two-level encoding tree and categorize the\ndata vertices into distinct density partitions according to the information\nuncertainty determined in the encoding tree. Each partition is then assigned to\nan agent to find the best clustering parameters without manual assistance. The\nallocation is density-adaptive, enabling AR-DBSCAN to effectively handle\ndiverse density distributions within the dataset by utilizing distinct agents\nfor different partitions. Second, a multi-agent deep reinforcement learning\nguided automatic parameter searching process is designed. The process of\nadjusting the parameter search direction by perceiving the clustering\nenvironment is modeled as a Markov decision process. Using a weakly-supervised\nreward training policy network, each agent adaptively learns the optimal\nclustering parameters by interacting with the clusters. Third, a recursive\nsearch mechanism adaptable to the data's scale is presented, enabling efficient\nand controlled exploration of large parameter spaces. Extensive experiments are\nconducted on nine artificial datasets and a real-world dataset. The results of\noffline and online tasks show that AR-DBSCAN not only improves clustering\naccuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,\nbut also is capable of robustly finding dominant parameters."
    },
    {
        "date": "2025-05",
        "title": "Guardians of the Web: The Evolution and Future of Website Information Security",
        "author": "Md Saiful Islam, and Li Xiangdong",
        "link": "http://arxiv.org/abs/2505.04308v1",
        "abstract": "Website information security has become a critical concern in the digital\nage. This article explores the evolution of website information security,\nexamining its historical development, current practices, and future directions.\nThe early beginnings from the 1960s to the 1980s laid the groundwork for modern\ncybersecurity, with the development of ARPANET, TCP/IP, public-key\ncryptography, and the first antivirus programs. The 1990s marked a\ntransformative era, driven by the commercialization of the Internet and the\nemergence of web-based services. As the Internet grew, so did the range and\nsophistication of cyber threats, leading to advancements in security\ntechnologies such as the Secure Sockets Layer (SSL) protocol, password\nprotection, and firewalls. Current practices in website information security\ninvolve a multi-layered approach, including encryption, secure coding\npractices, regular security audits, and user education. The future of website\ninformation security is expected to be shaped by emerging technologies such as\nartificial intelligence, blockchain, and quantum computing, as well as the\nincreasing importance of international cooperation and standardization efforts.\nAs cyber threats continue to evolve, ongoing research and innovation in website\ninformation security will be essential to protect sensitive information and\nmaintain trust in the digital world."
    },
    {
        "date": "2025-05",
        "title": "Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets",
        "author": "Mateo Lopez-Ledezma, and Gissel Velarde",
        "link": "http://arxiv.org/abs/2505.04204v1",
        "abstract": "Cybersecurity has become essential worldwide and at all levels, concerning\nindividuals, institutions, and governments. A basic principle in cybersecurity\nis to be always alert. Therefore, automation is imperative in processes where\nthe volume of daily operations is large. Several cybersecurity applications can\nbe addressed as binary classification problems, including anomaly detection,\nfraud detection, intrusion detection, spam detection, or malware detection. We\npresent three experiments. In the first experiment, we evaluate single\nclassifiers including Random Forests, Light Gradient Boosting Machine, eXtreme\nGradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting\nDecision Tree. In the second experiment, we test different sampling techniques\nincluding over-sampling, under-sampling, Synthetic Minority Over-sampling\nTechnique, and Self-Paced Ensembling. In the last experiment, we evaluate\nSelf-Paced Ensembling and its number of base classifiers. We found that\nimbalance learning techniques had positive and negative effects, as reported in\nrelated studies. Thus, these techniques should be applied with caution.\nBesides, we found different best performers for each dataset. Therefore, we\nrecommend testing single classifiers and imbalance learning techniques for each\nnew dataset and application involving imbalanced datasets as is the case in\nseveral cyber security applications."
    },
    {
        "date": "2025-05",
        "title": "Trajectory Entropy Reinforcement Learning for Predictable and Robust Control",
        "author": "Bang You, Chenxu Wang, and Huaping Liu",
        "link": "http://arxiv.org/abs/2505.04193v1",
        "abstract": "Simplicity is a critical inductive bias for designing data-driven\ncontrollers, especially when robustness is important. Despite the impressive\nresults of deep reinforcement learning in complex control tasks, it is prone to\ncapturing intricate and spurious correlations between observations and actions,\nleading to failure under slight perturbations to the environment. To tackle\nthis problem, in this work we introduce a novel inductive bias towards simple\npolicies in reinforcement learning. The simplicity inductive bias is introduced\nby minimizing the entropy of entire action trajectories, corresponding to the\nnumber of bits required to describe information in action trajectories after\nthe agent observes state trajectories. Our reinforcement learning agent,\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\ntrajectory entropy while maximizing rewards. We show that the trajectory\nentropy can be effectively estimated by learning a variational parameterized\naction prediction model, and use the prediction model to construct an\ninformation-regularized reward function. Furthermore, we construct a practical\nalgorithm that enables the joint optimization of models, including the policy\nand the prediction model. Experimental evaluations on several high-dimensional\nlocomotion tasks show that our learned policies produce more cyclical and\nconsistent action trajectories, and achieve superior performance, and\nrobustness to noise and dynamic changes than the state-of-the-art."
    },
    {
        "date": "2025-05",
        "title": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling",
        "author": "AbdulAziz AbdulGhaffar, and Ashraf Matrawy",
        "link": "http://arxiv.org/abs/2505.04101v1",
        "abstract": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases."
    },
    {
        "date": "2025-05",
        "title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks",
        "author": "Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, and Dezhong Peng",
        "link": "http://arxiv.org/abs/2505.04046v1",
        "abstract": "Recently, trustworthy multi-view learning has attracted extensive attention\nbecause evidence learning can provide reliable uncertainty estimation to\nenhance the credibility of multi-view predictions. Existing trusted multi-view\nlearning methods implicitly assume that multi-view data is secure. In practice,\nhowever, in safety-sensitive applications such as autonomous driving and\nsecurity monitoring, multi-view data often faces threats from adversarial\nperturbations, thereby deceiving or disrupting multi-view learning models. This\ninevitably leads to the adversarial unreliability problem (AUP) in trusted\nmulti-view learning. To overcome this tricky problem, we propose a novel\nmulti-view learning framework, namely Reliable Disentanglement Multi-view\nLearning (RDML). Specifically, we first propose evidential disentanglement\nlearning to decompose each view into clean and adversarial parts under the\nguidance of corresponding evidences, which is extracted by a pretrained\nevidence extractor. Then, we employ the feature recalibration module to\nmitigate the negative impact of adversarial perturbations and extract potential\ninformative features from them. Finally, to further ignore the irreparable\nadversarial interferences, a view-level evidential attention mechanism is\ndesigned. Extensive experiments on multi-view classification tasks with\nadversarial attacks show that our RDML outperforms the state-of-the-art\nmulti-view learning methods by a relatively large margin."
    },
    {
        "date": "2025-05",
        "title": "MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models",
        "author": "Soheil Zibakhsh Shabgahi, Yaman Jandali, and Farinaz Koushanfar",
        "link": "http://arxiv.org/abs/2505.04015v1",
        "abstract": "This paper proposes MergeGuard, a novel methodology for mitigation of AI\nTrojan attacks. Trojan attacks on AI models cause inputs embedded with triggers\nto be misclassified to an adversary's target class, posing a significant threat\nto model usability trained by an untrusted third party. The core of MergeGuard\nis a new post-training methodology for linearizing and merging fully connected\nlayers which we show simultaneously improves model generalizability and\nperformance. Our Proof of Concept evaluation on Transformer models demonstrates\nthat MergeGuard maintains model accuracy while decreasing trojan attack success\nrate, outperforming commonly used (post-training) Trojan mitigation by\nfine-tuning methodologies."
    },
    {
        "date": "2025-05",
        "title": "Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]",
        "author": "David Chu, Aditya Balasubramanian, Dee Bao, Natacha Crooks, Heidi Howard, Lucky E. Katahanas, and Soujanya Ponnapalli",
        "link": "http://arxiv.org/abs/2505.04014v1",
        "abstract": "Today, users can \"lift-and-shift\" unmodified applications into modern,\nVM-based Trusted Execution Environments (TEEs) in order to gain hardware-based\nsecurity guarantees. However, TEEs do not protect applications against disk\nrollback attacks, where persistent storage can be reverted to an earlier state\nafter a crash; existing rollback resistance solutions either only support a\nsubset of applications or require code modification. Our key insight is that\nrestoring disk consistency after a rollback attack guarantees rollback\nresistance for any application. We present Rollbaccine, a device mapper that\nprovides automatic rollback resistance for all applications by provably\npreserving disk consistency. Rollbaccine intercepts and replicates writes to\ndisk, restores lost state from backups during recovery, and minimizes overheads\nby taking advantage of the weak, multi-threaded semantics of disk operations.\nAcross benchmarks over two real applications (PostgreSQL and HDFS) and two file\nsystems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the\nfsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the\nstate-of-the-art, non-automatic rollback resistant solution by $208\\times$."
    },
    {
        "date": "2025-05",
        "title": "AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience",
        "author": "Shamnad Mohamed Shaffi, Sunish Vengathattil, Jezeena Nikarthil Sidhick, and Resmi Vijayan",
        "link": "http://arxiv.org/abs/2505.03945v1",
        "abstract": "Cloud security concerns have been greatly realized in recent years due to the\nincrease of complicated threats in the computing world. Many traditional\nsolutions do not work well in real-time to detect or prevent more complex\nthreats. Artificial intelligence is today regarded as a revolution in\ndetermining a protection plan for cloud data architecture through machine\nlearning, statistical visualization of computing infrastructure, and detection\nof security breaches followed by counteraction. These AI-enabled systems make\nwork easier as more network activities are scrutinized, and any anomalous\nbehavior that might be a precursor to a more serious breach is prevented. This\npaper examines ways AI can enhance cloud security by applying predictive\nanalytics, behavior-based security threat detection, and AI-stirring\nencryption. It also outlines the problems of the previous security models and\nhow AI overcomes them. For a similar reason, issues like data privacy, biases\nin the AI model, and regulatory compliance are also covered. So, AI improves\nthe protection of cloud computing contexts; however, more efforts are needed in\nthe subsequent phases to extend the technology's reliability, modularity, and\nethical aspects. This means that AI can be blended with other new computing\ntechnologies, including blockchain, to improve security frameworks further. The\npaper discusses the current trends in securing cloud data architecture using AI\nand presents further research and application directions."
    },
    {
        "date": "2025-05",
        "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders",
        "author": "Chethan Krishnamurthy Ramanaik, Arjun Roy, and Eirini Ntoutsi",
        "link": "http://arxiv.org/abs/2505.03646v1",
        "abstract": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples."
    },
    {
        "date": "2025-05",
        "title": "Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection",
        "author": "Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2505.03610v1",
        "abstract": "3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets."
    },
    {
        "date": "2025-05",
        "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets",
        "author": "Charita Dellaporta, Patrick O'Hara, and Theodoros Damoulas",
        "link": "http://arxiv.org/abs/2505.03585v1",
        "abstract": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification."
    },
    {
        "date": "2025-05",
        "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
        "author": "Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Beto de Paola, Dominik Gabi, James Crnkovich, Jean-Christophe Testud, Kat He, Rashnil Chaturvedi, Wu Zhou, and Joshua Saxe",
        "link": "http://arxiv.org/abs/2505.03574v1",
        "abstract": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails."
    },
    {
        "date": "2025-05",
        "title": "Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication",
        "author": "Chenguang Liu, Jianjun Chen, Yunfei Chen, Yubei He, Zhuangkun Wei, Hongjian Sun, Haiyan Lu, and Qi Hao",
        "link": "http://arxiv.org/abs/2505.03528v1",
        "abstract": "Cooperative perception, leveraging shared information from multiple vehicles\nvia vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous\ndriving to alleviate the limitation of single-vehicle perception. Existing\nworks have explored the effects of V2V communication impairments on perception\nprecision, but they lack generalization to different levels of impairments. In\nthis work, we propose a joint weighting and denoising framework, Coop-WD, to\nenhance cooperative perception subject to V2V channel impairments. In this\nframework, the self-supervised contrastive model and the conditional diffusion\nprobabilistic model are adopted hierarchically for vehicle-level and\npixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is\nproposed to selectively deactivate denoising to reduce processing overhead.\nRician fading, non-stationarity, and time-varying distortion are considered.\nSimulation results demonstrate that the proposed Coop-WD outperforms\nconventional benchmarks in all types of channels. Qualitative analysis with\nvisual examples further proves the superiority of our proposed method. The\nproposed Coop-WD-eco achieves up to 50% reduction in computational cost under\nsevere distortion while maintaining comparable accuracy as channel conditions\nimprove."
    },
    {
        "date": "2025-05",
        "title": "Uncovering the Limitations of Model Inversion Evaluation -- Benchmarks and Connection to Type-I Adversarial Attacks",
        "author": "Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, and Ngai-Man Cheung",
        "link": "http://arxiv.org/abs/2505.03519v2",
        "abstract": "Model Inversion (MI) attacks aim to reconstruct information of private\ntraining data by exploiting access to machine learning models. The most common\nevaluation framework for MI attacks/defenses relies on an evaluation model that\nhas been utilized to assess progress across almost all MI attacks and defenses\nproposed in recent years. In this paper, for the first time, we present an\nin-depth study of MI evaluation. Firstly, we construct the first comprehensive\nhuman-annotated dataset of MI attack samples, based on 28 setups of different\nMI attacks, defenses, private and public datasets. Secondly, using our dataset,\nwe examine the accuracy of the MI evaluation framework and reveal that it\nsuffers from a significant number of false positives. These findings raise\nquestions about the previously reported success rates of SOTA MI attacks.\nThirdly, we analyze the causes of these false positives, design controlled\nexperiments, and discover the surprising effect of Type I adversarial features\non MI evaluation, as well as adversarial transferability, highlighting a\nrelationship between two previously distinct research areas. Our findings\nsuggest that the performance of SOTA MI attacks has been overestimated, with\nthe actual privacy leakage being significantly less than previously reported.\nIn conclusion, we highlight critical limitations in the widely used MI\nevaluation framework and present our methods to mitigate false positive rates.\nWe remark that prior research has shown that Type I adversarial attacks are\nvery challenging, with no existing solution. Therefore, we urge to consider\nhuman evaluation as a primary MI evaluation framework rather than merely a\nsupplement as in previous MI research. We also encourage further work on\ndeveloping more robust and reliable automatic evaluation frameworks."
    },
    {
        "date": "2025-05",
        "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models",
        "author": "Zihan Wang, Hongwei Li, Rui Zhang, Wenbo Jiang, Kangjie Chen, Tianwei Zhang, Qingchuan Zhao, and Guowen Xu",
        "link": "http://arxiv.org/abs/2505.03501v1",
        "abstract": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness"
    },
    {
        "date": "2025-05",
        "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)",
        "author": "Faiz Taleb, Ivan Gazeau, and Maryline Laurent",
        "link": "http://arxiv.org/abs/2505.03490v1",
        "abstract": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models."
    },
    {
        "date": "2025-05",
        "title": "Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in Voice Authentication Systems",
        "author": "Alireza Mohammadi, Keshav Sood, Dhananjay Thiruvady, and Asef Nazari",
        "link": "http://arxiv.org/abs/2505.03455v1",
        "abstract": "Voice authentication systems remain susceptible to two major threats:\nbackdoor triggered attacks and targeted data poisoning attacks. This dual\nvulnerability is critical because conventional solutions typically address each\nthreat type separately, leaving systems exposed to adversaries who can exploit\nboth attacks simultaneously. We propose a unified defense framework that\neffectively addresses both BTA and TDPA. Our framework integrates a frequency\nfocused detection mechanism that flags covert pitch boosting and sound masking\nbackdoor attacks in near real time, followed by a convolutional neural network\nthat addresses TDPA. This dual layered defense approach utilizes\nmultidimensional acoustic features to isolate anomalous signals without\nrequiring costly model retraining. In particular, our PBSM detection mechanism\ncan seamlessly integrate into existing voice authentication pipelines and scale\neffectively for large scale deployments. Experimental results on benchmark\ndatasets and their compression with the state of the art algorithm demonstrate\nthat our PBSM detection mechanism outperforms the state of the art. Our\nframework reduces attack success rates to as low as five to fifteen percent\nwhile maintaining a recall rate of up to ninety five percent in recognizing\nTDPA."
    },
    {
        "date": "2025-05",
        "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis",
        "author": "Fouad Trad, and Ali Chehab",
        "link": "http://arxiv.org/abs/2505.03451v1",
        "abstract": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses."
    },
    {
        "date": "2025-05",
        "title": "Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks",
        "author": "Sun Haoxuan, Hong Yan, Zhan Jiahui, Chen Haoxing, Lan Jun, Zhu Huijia, Wang Weiqiang, Zhang Liqing, and Zhang Jianfu",
        "link": "http://arxiv.org/abs/2505.03435v1",
        "abstract": "The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification."
    },
    {
        "date": "2025-05",
        "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense",
        "author": "Kirill Lukyanov, Mikhail Drobyshevskiy, Georgii Sazonov, Mikhail Soloviov, and Ilya Makarov",
        "link": "http://arxiv.org/abs/2505.03424v1",
        "abstract": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}"
    },
    {
        "date": "2025-05",
        "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
        "author": "Saleh Zare Zade, Yao Qiang, Xiangyu Zhou, Hui Zhu, Mohammad Amin Roshani, Prashant Khanduri, and Dongxiao Zhu",
        "link": "http://arxiv.org/abs/2505.03392v1",
        "abstract": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}."
    },
    {
        "date": "2025-05",
        "title": "Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples",
        "author": "Jian-Wei Li, and Wen-Ze Shao",
        "link": "http://arxiv.org/abs/2505.03383v1",
        "abstract": "Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods."
    },
    {
        "date": "2025-05",
        "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection",
        "author": "June-Woo Kim, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, and Chan-Mo Yang",
        "link": "http://arxiv.org/abs/2505.03359v1",
        "abstract": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment."
    },
    {
        "date": "2025-05",
        "title": "A Chaos Driven Metric for Backdoor Attack Detection",
        "author": "Hema Karnam Surendrababu, and Nithin Nagaraj",
        "link": "http://arxiv.org/abs/2505.03208v1",
        "abstract": "The advancement and adoption of Artificial Intelligence (AI) models across\ndiverse domains have transformed the way we interact with technology. However,\nit is essential to recognize that while AI models have introduced remarkable\nadvancements, they also present inherent challenges such as their vulnerability\nto adversarial attacks. The current work proposes a novel defense mechanism\nagainst one of the most significant attack vectors of AI models - the backdoor\nattack via data poisoning of training datasets. In this defense technique, an\nintegrated approach that combines chaos theory with manifold learning is\nproposed. A novel metric - Precision Matrix Dependency Score (PDS) that is\nbased on the conditional variance of Neurochaos features is formulated. The PDS\nmetric has been successfully evaluated to distinguish poisoned samples from\nnon-poisoned samples across diverse datasets."
    },
    {
        "date": "2025-05",
        "title": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks",
        "author": "Qi Qin, Xinye Cao, Guoshun Nan, Sihan Chen, Rushan Li, Li Su, Haitao Du, Qimei Cui, Pengxuan Mao, Xiaofeng Tao, and Tony Q. S. Quek",
        "link": "http://arxiv.org/abs/2505.03161v2",
        "abstract": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community."
    },
    {
        "date": "2025-05",
        "title": "Robust Fairness Vision-Language Learning for Medical Image Analysis",
        "author": "Sparsh Bansal, Mingyang Wu, Xin Wang, and Shu Hu",
        "link": "http://arxiv.org/abs/2505.03153v1",
        "abstract": "The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC."
    },
    {
        "date": "2025-05",
        "title": "Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models",
        "author": "Hoang Cuong Nguyen, Shahroz Tariq, Mohan Baruwal Chhetri, and Bao Quoc Vo",
        "link": "http://arxiv.org/abs/2505.03147v1",
        "abstract": "This work evaluates the performance of Cyber Threat Intelligence (CTI)\nextraction methods in identifying attack techniques from threat reports\navailable on the web using the MITRE ATT&CK framework. We analyse four\nconfigurations utilising state-of-the-art tools, including the Threat Report\nATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as\nLlama2. Our findings reveal significant challenges, including class imbalance,\noverfitting, and domain-specific complexity, which impede accurate technique\nextraction. To mitigate these issues, we propose a novel two-step pipeline:\nfirst, an LLM summarises the reports, and second, a retrained SciBERT model\nprocesses a rebalanced dataset augmented with LLM-generated data. This approach\nachieves an improvement in F1-scores compared to baseline models, with several\nattack techniques surpassing an F1-score of 0.90. Our contributions enhance the\nefficiency of web-based CTI systems and support collaborative cybersecurity\noperations in an interconnected digital landscape, paving the way for future\nresearch on integrating human-AI collaboration platforms."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems",
        "author": "Abdul Mustafa, Muhammad Talha Khan, Muhammad Azmi Umer, Zaki Masood, and Chuadhry Mujeeb Ahmed",
        "link": "http://arxiv.org/abs/2505.03120v1",
        "abstract": "Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable\nto adversarial attacks. It is crucial for an IDS to learn to recognize\nadversarial examples before malicious entities exploit them. In this paper, we\ngenerated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We\nvalidate the generalization and scalability of the adversarial samples to\ntackle a broad range of real attacks on Industrial Control Systems (ICS). We\nevaluated the impact by assessing multiple attacks generated using the proposed\nmethod. The model trained with adversarial samples detected attacks with 95%\naccuracy on real-world attack data not used during training. The study was\nconducted using an operational secure water treatment (SWaT) testbed."
    },
    {
        "date": "2025-05",
        "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering",
        "author": "Joshua Owotogbe",
        "link": "http://arxiv.org/abs/2505.03096v1",
        "abstract": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey",
        "author": "Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, and Aman Raj",
        "link": "http://arxiv.org/abs/2505.03084v1",
        "abstract": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world."
    },
    {
        "date": "2025-05",
        "title": "Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles",
        "author": "Hanlin Chen, Simin Chen, Wenyu Li, Wei Yang, and Yiheng Feng",
        "link": "http://arxiv.org/abs/2505.03850v1",
        "abstract": "As a safety-critical cyber-physical system, cybersecurity and related safety\nissues for Autonomous Vehicles (AVs) have been important research topics for a\nwhile. Among all the modules on AVs, perception is one of the most accessible\nattack surfaces, as drivers and AVs have no control over the outside\nenvironment. Most current work targeting perception security for AVs focuses on\nperception correctness. In this work, we propose an impact analysis based on\ninference time attacks for autonomous vehicles. We demonstrate in a simulation\nsystem that such inference time attacks can also threaten the safety of both\nthe ego vehicle and other traffic participants."
    },
    {
        "date": "2025-05",
        "title": "AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks",
        "author": "Ilyas Oulkadda, and Julien Perez",
        "link": "http://arxiv.org/abs/2505.06267v1",
        "abstract": "The widespread adoption of Large Language Models (LLMs) for code generation,\nexemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM\nto assist in code completion tasks} surpassing a million users, highlights the\ntransformative potential of these tools in improving developer productivity.\nHowever, this rapid growth also underscores critical concerns regarding the\nquality, safety, and reliability of the code they generate. As Code-LLMs\nevolve, they face significant challenges, including the diminishing returns of\nmodel scaling and the scarcity of new, high-quality training data. To address\nthese issues, this paper introduces Adversarial Knowledge Distillation (AKD), a\nnovel approach that leverages adversarially generated synthetic datasets to\ndistill the capabilities of larger models into smaller, more efficient ones. By\nsystematically stress-testing and refining the reasoning capabilities of\nCode-LLMs, AKD provides a framework for enhancing model robustness,\nreliability, and security while improving their parameter-efficiency. We\nbelieve this work represents a critical step toward ensuring dependable\nautomated code generation within the constraints of existing data and the\ncost-efficiency of model execution."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation",
        "author": "Anjila Budathoki, and Manish Dhakal",
        "link": "http://arxiv.org/abs/2505.02971v1",
        "abstract": "Adversarial attacks have been fairly explored for computer vision and\nvision-language models. However, the avenue of adversarial attack for the\nvision language segmentation models (VLSMs) is still under-explored, especially\nfor medical image analysis.\n  Thus, we have investigated the robustness of VLSMs against adversarial\nattacks for 2D medical images with different modalities with radiology,\nphotography, and endoscopy. The main idea of this project was to assess the\nrobustness of the fine-tuned VLSMs specially in the medical domain setting to\naddress the high risk scenario.\n  First, we have fine-tuned pre-trained VLSMs for medical image segmentation\nwith adapters.\n  Then, we have employed adversarial attacks -- projected gradient descent\n(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to\ndetermine its robustness against adversaries.\n  We have reported models' performance decline to analyze the adversaries'\nimpact.\n  The results exhibit significant drops in the DSC and IoU scores after the\nintroduction of these adversaries. Furthermore, we also explored universal\nperturbation but were not able to find for the medical images.\n  \\footnote{https://github.com/anjilab/secure-private-ai}"
    },
    {
        "date": "2025-05",
        "title": "Single-Sample and Robust Online Resource Allocation",
        "author": "Rohan Ghuge, Sahil Singla, and Yifan Wang",
        "link": "http://arxiv.org/abs/2505.02963v1",
        "abstract": "Online Resource Allocation problem is a central problem in many areas of\nComputer Science, Operations Research, and Economics. In this problem, we\nsequentially receive $n$ stochastic requests for $m$ kinds of shared resources,\nwhere each request can be satisfied in multiple ways, consuming different\namounts of resources and generating different values. The goal is to achieve a\n$(1-\\epsilon)$-approximation to the hindsight optimum, where $\\epsilon>0$ is a\nsmall constant, assuming each resource has a large budget.\n  In this paper, we investigate the learnability and robustness of online\nresource allocation. Our primary contribution is a novel Exponential Pricing\nalgorithm with the following properties: 1. It requires only a \\emph{single\nsample} from each of the $n$ request distributions to achieve a\n$(1-\\epsilon)$-approximation for online resource allocation with large budgets.\nSuch an algorithm was previously unknown, even with access to polynomially many\nsamples, as prior work either assumed full distributional knowledge or was\nlimited to i.i.d.\\,or random-order arrivals. 2. It is robust to corruptions in\nthe outliers model and the value augmentation model. Specifically, it maintains\nits $(1 - \\epsilon)$-approximation guarantee under both these robustness\nmodels, resolving the open question posed in Argue, Gupta, Molinaro, and Singla\n(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures\nincentive compatibility.\n  The intuition behind our Exponential Pricing algorithm is that the price of a\nresource should adjust exponentially as it is overused or underused. It differs\nfrom conventional approaches that use an online learning algorithm for item\npricing. This departure guarantees that the algorithm will never run out of any\nresource, but loses the usual no-regret properties of online learning\nalgorithms, necessitating a new analytical approach."
    },
    {
        "date": "2025-05",
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "author": "Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2505.02824v1",
        "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance."
    },
    {
        "date": "2025-05",
        "title": "Acoustic Side-Channel Attacks on a Computer Mouse",
        "author": "Mauro Conti, Marin Duroyon, Gabriele Orazi, and Gene Tsudik",
        "link": "http://arxiv.org/abs/2505.02725v1",
        "abstract": "Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using\naudio emitted from a computing devices and their peripherals. Attacks targeting\nkeyboards are popular and have been explored in the literature. However,\nsimilar attacks targeting other human interface peripherals, such as computer\nmice, are under-explored. To this end, this paper considers security leakage\nvia acoustic signals emanating from normal mouse usage. We first confirm\nfeasibility of such attacks by showing a proof-of-concept attack that\nclassifies four mouse movements with 97% accuracy in a controlled environment.\nWe then evolve the attack towards discerning twelve unique mouse movements\nusing a smartphone to record the experiment. Using Machine Learning (ML)\ntechniques, the model is trained on an experiment with six participants to be\ngeneralizable and discern among twelve movements with 94% accuracy. In\naddition, we experiment with an attack that detects a user action of closing a\nfull-screen window on a laptop. Achieving an accuracy of 91%, this experiment\nhighlights exploiting audio leakage from computer mouse movements in a\nrealistic scenario."
    },
    {
        "date": "2025-05",
        "title": "Robustness questions the interpretability of graph neural networks: what to do?",
        "author": "Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, and Ilya Makarov",
        "link": "http://arxiv.org/abs/2505.02566v1",
        "abstract": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications."
    },
    {
        "date": "2025-05",
        "title": "Antifragility of RIS-assisted Communication Systems under Jamming Attacks",
        "author": "Mounir Bensalem, Thomas R\u00f6thig, and Admela Jukan",
        "link": "http://arxiv.org/abs/2505.02565v1",
        "abstract": "Antifragility of communication systems is defined as measure of benefits\ngained from the adverse events and variability of its environment. In this\npaper, we introduce the notion of antifragility in Reconfigurable Intelligent\nSurface (RIS) assisted communication systems affected by a jamming attack. We\nanalyzed the antifragility of the two hop systems, where the wireless path\ncontains source node, RIS, destination node, and a eavesdropping/jamming node.\nWe propose and analyze the antifragility performance for several jamming\nmodels, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude\nshifting. Our paper shows that antifragility throughput can indeed be achieved\nunder certain power thresholds and for various jamming models. In particular,\nhigh jamming power combined with low baseline data rates yields an antifragile\ngain factor of approximately five times. The results confirm that\nreconfigurable intelligent surfaces, when coupled with an antifragile design\nphilosophy, can convert hostile interference from a liability into a throughput\ngain."
    },
    {
        "date": "2025-05",
        "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "author": "Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, and Peng Hu",
        "link": "http://arxiv.org/abs/2505.02549v2",
        "abstract": "Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE."
    },
    {
        "date": "2025-05",
        "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
        "author": "Aiman Farooq, Azad Singh, Deepak Mishra, and Santanu Chaudhury",
        "link": "http://arxiv.org/abs/2505.02529v1",
        "abstract": "Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care."
    },
    {
        "date": "2025-05",
        "title": "Bayesian Robust Aggregation for Federated Learning",
        "author": "Aleksandr Karakulev, Usama Zafar, Salman Toor, and Prashant Singh",
        "link": "http://arxiv.org/abs/2505.02490v1",
        "abstract": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "Economic Security of Multiple Shared Security Protocols",
        "author": "Abhimanyu Nag, Dhruv Bodani, and Abhishek Kumar",
        "link": "http://arxiv.org/abs/2505.03843v2",
        "abstract": "As restaking protocols gain adoption across blockchain ecosystems, there is a\nneed for Actively Validated Services (AVSs) to span multiple Shared Security\nProviders (SSPs). This leads to stake fragmentation which introduces new\ncomplications where an adversary may compromise an AVS by targeting its weakest\nSSP. In this paper, we formalize the Multiple SSP Problem and analyze two\narchitectures : an isolated fragmented model called Model $\\mathbb{M}$ and a\nshared unified model called Model $\\mathbb{S}$, through a convex optimization\nand game-theoretic lens. We derive utility bounds, attack cost conditions, and\nmarket equilibrium that describes protocol security for both models. Our\nresults show that while Model $\\mathbb{M}$ offers deployment flexibility, it\ninherits lowest-cost attack vulnerabilities, whereas Model $\\mathbb{S}$\nachieves tighter security guarantees through single validator sets and\naggregated slashing logic. We conclude with future directions of work including\nan incentive-compatible stake rebalancing allocation in restaking ecosystems."
    },
    {
        "date": "2025-05",
        "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
        "author": "Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, and Ganesh Ramakrishnan",
        "link": "http://arxiv.org/abs/2505.02433v1",
        "abstract": "We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs."
    },
    {
        "date": "2025-05",
        "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training",
        "author": "Fares B. Mehouachi, and Saif Eddin Jabari",
        "link": "http://arxiv.org/abs/2505.02360v1",
        "abstract": "Adversarial training is a cornerstone of robust deep learning, but fast\nmethods like the Fast Gradient Sign Method (FGSM) often suffer from\nCatastrophic Overfitting (CO), where models become robust to single-step\nattacks but fail against multi-step variants. While existing solutions rely on\nnoise injection, regularization, or gradient clipping, we propose a novel\nsolution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent\nunder the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we\ndevelop a framework for generalized $l^p$ attack as a fixed point problem and\ncraft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to\n$l^{\\infty}$. This leads to our core insight: CO emerges when highly\nconcentrated gradients where information localizes in few dimensions interact\nwith aggressive norm constraints. By quantifying gradient concentration through\nParticipation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM\nthat automatically tunes the training norm based on gradient information.\nExtensive experiments demonstrate that this approach achieves strong robustness\nwithout requiring additional regularization or noise injection, providing a\nnovel and theoretically-principled pathway to mitigate the CO problem."
    },
    {
        "date": "2025-05",
        "title": "Temporal Robustness in Discrete Time Linear Dynamical Systems",
        "author": "Nilava Metya, and Arunesh Sinha",
        "link": "http://arxiv.org/abs/2505.02347v1",
        "abstract": "Discrete time linear dynamical systems, including Markov chains, have found\nmany applications. However, in some problems, there is uncertainty about the\ntime horizon for which the system runs. This creates uncertainty about the cost\n(or reward) incurred based on the state distribution when the system stops.\nGiven past data samples of how long a system ran, we propose to theoretically\nanalyze a distributional robust cost estimation task in a Wasserstein ambiguity\nset, instead of learning a probability distribution from a few samples. Towards\nthis, we show an equivalence between a discrete time Markov Chain on a\nprobability simplex and a global asymptotic stable (GAS) discrete time linear\ndynamical system, allowing us to base our study on a GAS system only. Then, we\nprovide various polynomial time algorithms and hardness results for different\ncases in our theoretical study, including a fundamental result about\nWasserstein distance based polytope."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
        "author": "Daisuke Yamada, Harit Vishwakarma, and Ramya Korlakai Vinayak",
        "link": "http://arxiv.org/abs/2505.02299v1",
        "abstract": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control."
    },
    {
        "date": "2025-05",
        "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
        "author": "Dyuman Aditya, Junning Huang, Nico Bohlinger, Piotr Kicki, Krzysztof Walas, Jan Peters, Matteo Luperto, and Davide Tateo",
        "link": "http://arxiv.org/abs/2505.02272v1",
        "abstract": "Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam"
    },
    {
        "date": "2025-05",
        "title": "Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees",
        "author": "Andrew Quijano, Spyros T. Halkidis, Kevin Gallagher, Kemal Akkaya, and Nikolaos Samaras",
        "link": "http://arxiv.org/abs/2505.02224v1",
        "abstract": "A decision tree is an easy-to-understand tool that has been widely used for\nclassification tasks. On the one hand, due to privacy concerns, there has been\nan urgent need to create privacy-preserving classifiers that conceal the user's\ninput from the classifier. On the other hand, with the rise of cloud computing,\ndata owners are keen to reduce risk by outsourcing their model, but want\nsecurity guarantees that third parties cannot steal their decision tree model.\nTo address these issues, Joye and Salehi introduced a theoretical protocol that\nefficiently evaluates decision trees while maintaining privacy by leveraging\ntheir comparison protocol that is resistant to timing attacks. However, their\napproach was not only inefficient but also prone to side-channel attacks.\nTherefore, in this paper, we propose a new decision tree inference protocol in\nwhich the model is shared and evaluated among multiple entities. We partition\nour decision tree model by each level to be stored in a new entity we refer to\nas a \"level-site.\" Utilizing this approach, we were able to gain improved\naverage run time for classifier evaluation for a non-complete tree, while also\nhaving strong mitigations against side-channel attacks."
    },
    {
        "date": "2025-05",
        "title": "Robust AI-Generated Face Detection with Imbalanced Data",
        "author": "Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, and Shu Hu",
        "link": "http://arxiv.org/abs/2505.02182v1",
        "abstract": "Deepfakes, created using advanced AI techniques such as Variational\nAutoencoder and Generative Adversarial Networks, have evolved from research and\nentertainment applications into tools for malicious activities, posing\nsignificant threats to digital trust. Current deepfake detection techniques\nhave evolved from CNN-based methods focused on local artifacts to more advanced\napproaches using vision transformers and multimodal models like CLIP, which\ncapture global anomalies and improve cross-domain generalization. Despite\nrecent progress, state-of-the-art deepfake detectors still face major\nchallenges in handling distribution shifts from emerging generative models and\naddressing severe class imbalance between authentic and fake samples in\ndeepfake datasets, which limits their robustness and detection accuracy. To\naddress these challenges, we propose a framework that combines dynamic loss\nreweighting and ranking-based optimization, which achieves superior\ngeneralization and performance under imbalanced dataset conditions. The code is\navailable at https://github.com/Purdue-M2/SP_CUP."
    },
    {
        "date": "2025-05",
        "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection",
        "author": "Samuel Webster, and Adam Czajka",
        "link": "http://arxiv.org/abs/2505.02176v1",
        "abstract": "Saliency-guided training, which directs model learning to important regions\nof images, has demonstrated generalization improvements across various\nbiometric presentation attack detection (PAD) tasks. This paper presents its\nfirst application to fingerprint PAD. We conducted a 50-participant study to\ncreate a dataset of 800 human-annotated fingerprint perceptually-important\nmaps, explored alongside algorithmically-generated \"pseudosaliency,\" including\nminutiae-based, image quality-based, and autoencoder-based saliency maps.\nEvaluating on the 2021 Fingerprint Liveness Detection Competition testing set,\nwe explore various configurations within five distinct training scenarios to\nassess the impact of saliency-guided training on accuracy and generalization.\nOur findings demonstrate the effectiveness of saliency-guided training for\nfingerprint PAD in both limited and large data contexts, and we present a\nconfiguration capable of earning the first place on the LivDet-2021 benchmark.\nOur results highlight saliency-guided training's promise for increased model\ngeneralization capabilities, its effectiveness when data is limited, and its\npotential to scale to larger datasets in fingerprint PAD. All collected\nsaliency data and trained models are released with the paper to support\nreproducible research."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets",
        "author": "Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, and Ruixuan Li",
        "link": "http://arxiv.org/abs/2505.02118v3",
        "abstract": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."
    },
    {
        "date": "2025-05",
        "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
        "author": "Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, and Qifeng Chen",
        "link": "http://arxiv.org/abs/2505.02094v1",
        "abstract": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness."
    },
    {
        "date": "2025-05",
        "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
        "author": "Christian Schroeder de Witt",
        "link": "http://arxiv.org/abs/2505.02077v1",
        "abstract": "Decentralized AI agents will soon interact across internet platforms,\ncreating security challenges beyond traditional cybersecurity and AI safety\nframeworks. Free-form protocols are essential for AI's task generalization but\nenable new threats like secret collusion and coordinated swarm attacks. Network\neffects can rapidly spread privacy breaches, disinformation, jailbreaks, and\ndata poisoning, while multi-agent dispersion and stealth optimization help\nadversaries evade oversightcreating novel persistent threats at a systemic\nlevel. Despite their critical importance, these security challenges remain\nunderstudied, with research fragmented across disparate fields including AI\nsecurity, multi-agent learning, complex systems, cybersecurity, game theory,\ndistributed systems, and technical AI governance. We introduce\n\\textbf{multi-agent security}, a new field dedicated to securing networks of\ndecentralized AI agents against threats that emerge or amplify through their\ninteractionswhether direct or indirect via shared environmentswith each other,\nhumans, and institutions, and characterize fundamental security-performance\ntrade-offs. Our preliminary work (1) taxonomizes the threat landscape arising\nfrom interacting AI agents, (2) surveys security-performance tradeoffs in\ndecentralized AI systems, and (3) proposes a unified research agenda addressing\nopen challenges in designing secure agent systems and interaction environments.\nBy identifying these gaps, we aim to guide research in this critical area to\nunlock the socioeconomic potential of large-scale agent deployment on the\ninternet, foster public trust, and mitigate national security risks in critical\ninfrastructure and defense contexts."
    },
    {
        "date": "2025-05",
        "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification",
        "author": "Yi Han",
        "link": "http://arxiv.org/abs/2505.02073v1",
        "abstract": "As time series classification (TSC) gains prominence, ensuring robust TSC\nmodels against adversarial attacks is crucial. While adversarial defense is\nwell-studied in Computer Vision (CV), the TSC field has primarily relied on\nadversarial training (AT), which is computationally expensive. In this paper,\nfive data augmentation-based defense methods tailored for time series are\ndeveloped, with the most computationally intensive method among them increasing\nthe computational resources by only 14.07% compared to the original TSC model.\nMoreover, the deployment process for these methods is straightforward. By\nleveraging these advantages of our methods, we create two combined methods. One\nof these methods is an ensemble of all the proposed techniques, which not only\nprovides better defense performance than PGD-based AT but also enhances the\ngeneralization ability of TSC models. Moreover, the computational resources\nrequired for our ensemble are less than one-third of those required for\nPGD-based AT. These methods advance robust TSC in data mining. Furthermore, as\nfoundation models are increasingly explored for time series feature learning,\nour work provides insights into integrating data augmentation-based adversarial\ndefense with large-scale pre-trained models in future research."
    },
    {
        "date": "2025-05",
        "title": "A Comprehensive Analysis of Adversarial Attacks against Spam Filters",
        "author": "Esra Hoto\u011flu, Sevil Sen, and Burcu Can",
        "link": "http://arxiv.org/abs/2505.03831v1",
        "abstract": "Deep learning has revolutionized email filtering, which is critical to\nprotect users from cyber threats such as spam, malware, and phishing. However,\nthe increasing sophistication of adversarial attacks poses a significant\nchallenge to the effectiveness of these filters. This study investigates the\nimpact of adversarial attacks on deep learning-based spam detection systems\nusing real-world datasets. Six prominent deep learning models are evaluated on\nthese datasets, analyzing attacks at the word, character sentence, and\nAI-generated paragraph-levels. Novel scoring functions, including spam weights\nand attention weights, are introduced to improve attack effectiveness. This\ncomprehensive analysis sheds light on the vulnerabilities of spam filters and\ncontributes to efforts to improve their security against evolving adversarial\nthreats."
    },
    {
        "date": "2025-05",
        "title": "Triple-identity Authentication: The Future of Secure Access",
        "author": "Suyun Borjigin",
        "link": "http://arxiv.org/abs/2505.02004v1",
        "abstract": "In a typical authentication process, the local system verifies the user's\nidentity using a stored hash value generated by a cross-system hash algorithm.\nThis article shifts the research focus from traditional password encryption to\nthe establishment of gatekeeping mechanisms for effective interactions between\na system and the outside world. Here, we propose a triple-identity\nauthentication system to achieve this goal. Specifically, this local system\nopens the inner structure of its hash algorithm to all user credentials,\nincluding the login name, login password, and authentication password. When a\nlogin credential is entered, the local system hashes it and then creates a\nunique identifier using intermediate hash elements randomly selected from the\nopen algorithm. Importantly, this locally generated unique identifier (rather\nthan the stored hash produced by the open algorithm) is utilized to verify the\nuser's combined identity, which is generated by combining the entered\ncredential with the International Mobile Equipment Identity and the\nInternational Mobile Subscriber Identity. The verification process is\nimplemented at each interaction point: the login name field, the login password\nfield, and the server's authentication point. Thus, within the context of this\ntriple-identity authentication system, we establish a robust gatekeeping\nmechanism for system interactions, ultimately providing a level of security\nthat is equivalent to multi-factor authentication."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images",
        "author": "Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, and Jaya Sreevalsan-Nair",
        "link": "http://arxiv.org/abs/2505.01884v2",
        "abstract": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)"
    },
    {
        "date": "2025-05",
        "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
        "author": "Trisanth Srinivasan, and Santosh Patapati",
        "link": "http://arxiv.org/abs/2505.01881v1",
        "abstract": "Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems."
    },
    {
        "date": "2025-05",
        "title": "PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework",
        "author": "Daniel Commey, and Garth V. Crosby",
        "link": "http://arxiv.org/abs/2505.01866v1",
        "abstract": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy, but its classical cryptographic underpinnings are vulnerable to\nquantum attacks. This vulnerability is particularly critical in sensitive\ndomains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure\nBlockchain-based Federated Learning), a framework integrating post-quantum\ncryptography (PQC) with blockchain verification to secure FL against quantum\nadversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly\nDilithium) signatures to authenticate model updates and leverage optimized\nsmart contracts for decentralized validation. Extensive evaluations on diverse\ndatasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient\ncryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms)\nwith a fixed signature size of 3309 Bytes. Blockchain integration incurs a\nmanageable overhead, with average transaction times around 4.8 s and gas usage\nper update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the\ncryptographic overhead relative to transaction time remains minimal (around\n0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the\nbottleneck in blockchain-based FL. The system maintains competitive model\naccuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with\nround times showing sublinear growth with increasing client numbers. Our\nopen-source implementation and reproducible benchmarks validate the feasibility\nof deploying long-term, quantum-resistant security in practical FL systems."
    },
    {
        "date": "2025-05",
        "title": "Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp",
        "author": "Eran Aizikovich, Dudu Mimran, Edita Grolman, Yuval Elovici, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2505.01816v1",
        "abstract": "The Open Radio Access Network (O-RAN) architecture is revolutionizing\ncellular networks with its open, multi-vendor design and AI-driven management,\naiming to enhance flexibility and reduce costs. Although it has many\nadvantages, O-RAN is not threat-free. While previous studies have mainly\nexamined vulnerabilities arising from O-RAN's intelligent components, this\npaper is the first to focus on the security challenges and vulnerabilities\nintroduced by transitioning from single-operator to multi-operator RAN\narchitectures. This shift increases the risk of untrusted third-party operators\nmanaging different parts of the network. To explore these vulnerabilities and\ntheir potential mitigation, we developed an open-access testbed environment\nthat integrates a wireless network simulator with the official O-RAN Software\nCommunity (OSC) RAN intelligent component (RIC) cluster. This environment\nenables realistic, live data collection and serves as a platform for\ndemonstrating APATE (adversarial perturbation against traffic efficiency), an\nevasion attack in which a malicious cell manipulates its reported key\nperformance indicators (KPIs) and deceives the O-RAN traffic steering to gain\nunfair allocations of user equipment (UE). To ensure that O-RAN's legitimate\nactivity continues, we introduce MARRS (monitoring adversarial RAN reports), a\ndetection framework based on a long-short term memory (LSTM) autoencoder (AE)\nthat learns contextual features across the network to monitor malicious\ntelemetry (also demonstrated in our testbed). Our evaluation showed that by\nexecuting APATE, an attacker can obtain a 248.5% greater UE allocation than it\nwas supposed to in a benign scenario. In addition, the MARRS detection method\nwas also shown to successfully classify malicious cell activity, achieving\naccuracy of 99.2% and an F1 score of 0.978."
    },
    {
        "date": "2025-05",
        "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge",
        "author": "Core Francisco Park, Zechen Zhang, and Hidenori Tanaka",
        "link": "http://arxiv.org/abs/2505.01812v1",
        "abstract": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT."
    },
    {
        "date": "2025-05",
        "title": "Backdoor Attacks Against Patch-based Mixture of Experts",
        "author": "Cedric Chan, Jona te Lintelo, and Stjepan Picek",
        "link": "http://arxiv.org/abs/2505.01811v1",
        "abstract": "As Deep Neural Networks (DNNs) continue to require larger amounts of data and\ncomputational power, Mixture of Experts (MoE) models have become a popular\nchoice to reduce computational complexity. This popularity increases the\nimportance of considering the security of MoE architectures. Unfortunately, the\nsecurity of models using a MoE architecture has not yet gained much attention\ncompared to other DNN models. In this work, we investigate the vulnerability of\npatch-based MoE (pMoE) models for image classification against backdoor\nattacks. We examine multiple trigger generation methods and Fine-Pruning as a\ndefense. To better understand a pMoE model's vulnerability to backdoor attacks,\nwe investigate which factors affect the model's patch selection. Our work shows\nthat pMoE models are highly susceptible to backdoor attacks. More precisely, we\nachieve high attack success rates of up to 100% with visible triggers and a 2%\npoisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally,\nwe show that pruning itself is ineffective as a defense but that fine-tuning\ncan remove the backdoor almost completely. Our results show that fine-tuning\nthe model for five epochs reduces the attack success rate to 2.1% whilst\nsacrificing 1.4% accuracy."
    },
    {
        "date": "2025-05",
        "title": "ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability",
        "author": "Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Fang Chen, and Jianlong Zhou",
        "link": "http://arxiv.org/abs/2505.06258v1",
        "abstract": "Attribution algorithms are essential for enhancing the interpretability and\ntrustworthiness of deep learning models by identifying key features driving\nmodel decisions. Existing frameworks, such as InterpretDL and OmniXAI,\nintegrate multiple attribution methods but suffer from scalability limitations,\nhigh coupling, theoretical constraints, and lack of user-friendly\nimplementations, hindering neural network transparency and interoperability. To\naddress these challenges, we propose Attribution-Based Explainability (ABE), a\nunified framework that formalizes Fundamental Attribution Methods and\nintegrates state-of-the-art attribution algorithms while ensuring compliance\nwith attribution axioms. ABE enables researchers to develop novel attribution\ntechniques and enhances interpretability through four customizable modules:\nRobustness, Interpretability, Validation, and Data & Model. This framework\nprovides a scalable, extensible foundation for advancing attribution-based\nexplainability and fostering transparent AI systems. Our code is available at:\nhttps://github.com/LMBTough/ABE-XAI."
    },
    {
        "date": "2025-05",
        "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement",
        "author": "Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, and Hongliang Ren",
        "link": "http://arxiv.org/abs/2505.01766v1",
        "abstract": "Surgical workflow recognition is vital for automating tasks, supporting\ndecision-making, and training novice surgeons, ultimately improving patient\nsafety and standardizing procedures. However, data corruption can lead to\nperformance degradation due to issues like occlusion from bleeding or smoke in\nsurgical scenes and problems with data storage and transmission. In this case,\nwe explore a robust graph-based multimodal approach to integrating vision and\nkinematic data to enhance accuracy and reliability. Vision data captures\ndynamic surgical scenes, while kinematic data provides precise movement\ninformation, overcoming limitations of visual recognition under adverse\nconditions. We propose a multimodal Graph Representation network with\nAdversarial feature Disentanglement (GRAD) for robust surgical workflow\nrecognition in challenging scenarios with domain shifts or corrupted data.\nSpecifically, we introduce a Multimodal Disentanglement Graph Network that\ncaptures fine-grained visual information while explicitly modeling the complex\nrelationships between vision and kinematic embeddings through graph-based\nmessage modeling. To align feature spaces across modalities, we propose a\nVision-Kinematic Adversarial framework that leverages adversarial training to\nreduce modality gaps and improve feature consistency. Furthermore, we design a\nContextual Calibrated Decoder, incorporating temporal and contextual priors to\nenhance robustness against domain shifts and corrupted data. Extensive\ncomparative and ablation experiments demonstrate the effectiveness of our model\nand proposed modules. Moreover, our robustness experiments show that our method\neffectively handles data corruption during storage and transmission, exhibiting\nexcellent stability and robustness. Our approach aims to advance automated\nsurgical workflow recognition, addressing the complexities and dynamism\ninherent in surgical procedures."
    },
    {
        "date": "2025-05",
        "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm",
        "author": "Sarvesh Shashidhar, Ritik, Nachiketa Patil, Suraj Racha, and Ganesh Ramakrishnan",
        "link": "http://arxiv.org/abs/2505.01706v1",
        "abstract": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present."
    },
    {
        "date": "2025-05",
        "title": "Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning",
        "author": "Aditya Shinde, and Prashant Doshi",
        "link": "http://arxiv.org/abs/2505.03817v1",
        "abstract": "This paper presents a holistic approach to attacker preference modeling from\nsystem-level audit logs using inverse reinforcement learning (IRL). Adversary\nmodeling is an important capability in cybersecurity that lets defenders\ncharacterize behaviors of potential attackers, which enables attribution to\nknown cyber adversary groups. Existing approaches rely on documenting an\never-evolving set of attacker tools and techniques to track known threat\nactors. Although attacks evolve constantly, attacker behavioral preferences are\nintrinsic and less volatile. Our approach learns the behavioral preferences of\ncyber adversaries from forensics data on their tools and techniques. We model\nthe attacker as an expert decision-making agent with unknown behavioral\npreferences situated in a computer host. We leverage attack provenance graphs\nof audit logs to derive a state-action trajectory of the attack. We test our\napproach on open datasets of audit logs containing real attack data. Our\nresults demonstrate for the first time that low-level forensics data can\nautomatically reveal an adversary's subjective preferences, which serves as an\nadditional dimension to modeling and documenting cyber adversaries. Attackers'\npreferences tend to be invariant despite their different tools and indicate\npredispositions that are inherent to the attacker. As such, these inferred\npreferences can potentially serve as unique behavioral signatures of attackers\nand improve threat attribution."
    },
    {
        "date": "2025-05",
        "title": "Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security",
        "author": "Andrew Adiletta, Zane Weissman, Fatemeh Khojasteh Dana, Berk Sunar, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2505.01518v1",
        "abstract": "The increasing density of modern DRAM has heightened its vulnerability to\nRowhammer attacks, which induce bit flips by repeatedly accessing specific\nmemory rows. This paper presents an analysis of bit flip patterns generated by\nadvanced Rowhammer techniques that bypass existing hardware defenses. First, we\ninvestigate the phenomenon of adjacent bit flips--where two or more physically\nneighboring bits are corrupted simultaneously--and demonstrate they occur with\nsignificantly higher frequency than previously documented. We also show that if\nmultiple bits flip within a byte, they are more likely to be adjacent than\nrandomly distributed: for example, if 4 bits flip within a byte, there is an\n87% chance that they are all adjacent. We also demonstrate that bit flips\nwithin a row will naturally cluster together likely due to the underlying\nphysics of the attack. We then investigate two fault injection attacks enabled\nby multiple adjacent or nearby bit flips. First, we show how these correlated\nflips enable efficient cryptographic signature correction attacks, successfully\nrecovering ECDSA private keys from OpenSSL implementations where single-bit\napproaches would be unfeasible. Second, we introduce a targeted attack against\nlarge language models by exploiting Rowhammer-induced corruptions in tokenizer\ndictionaries of GGUF model files. This attack effectively rewrites safety\ninstructions in system prompts by swapping safety-critical tokens with benign\nalternatives, circumventing model guardrails while maintaining normal\nfunctionality in other contexts. Our experimental results across multiple DRAM\nconfigurations reveal that current memory protection schemes are inadequate\nagainst these sophisticated attack vectors, which can achieve their objectives\nwith precise, minimal modifications rather than random corruption."
    },
    {
        "date": "2025-05",
        "title": "Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration",
        "author": "Khushbu Mehboob Shaikh, and Georgios Giannakopoulos",
        "link": "http://arxiv.org/abs/2505.01514v1",
        "abstract": "The rapid digitalization of communication systems has elevated Interactive\nVoice Response (IVR) technologies to become critical interfaces for customer\nengagement. With Artificial Intelligence (AI) now driving these platforms,\nensuring secure, compliant, and ethically designed development practices is\nmore imperative than ever. AI-powered IVRs leverage Natural Language Processing\n(NLP) and Machine Learning (ML) to personalize interactions, automate service\ndelivery, and optimize user experiences. However, these innovations expose\nsystems to heightened risks, including data privacy breaches, AI decision\nopacity, and model security vulnerabilities. This paper analyzes the evolution\nof IVRs from static code-based designs to adaptive AI-driven systems,\npresenting a cybersecurity-centric perspective. We propose a practical\ngovernance framework that embeds agile security principles, compliance with\nglobal data legislation, and user-centric ethics. Emphasizing\nprivacy-by-design, adaptive risk modeling, and transparency, the paper argues\nthat ethical AI integration is not a feature but a strategic imperative.\nThrough this multidimensional lens, we highlight how modern IVRs can transition\nfrom communication tools to intelligent, secure, and accountable digital\nfrontlines-resilient against emerging threats and aligned with societal\nexpectations."
    },
    {
        "date": "2025-05",
        "title": "Machine Learning for Cyber-Attack Identification from Traffic Flows",
        "author": "Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, and Huihui Wang",
        "link": "http://arxiv.org/abs/2505.01489v1",
        "abstract": "This paper presents our simulation of cyber-attacks and detection strategies\non the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual\nmachines and the OPNSense firewall, along with traffic dynamics from SUMO and\nexploitation via the Metasploit framework. We try to answer the research\nquestions: are we able to identify cyber attacks by only analyzing traffic flow\npatterns. In this research, the cyber attacks are focused particularly when\nlights are randomly turned all green or red at busy intersections by\nadversarial attackers. Despite challenges stemming from imbalanced data and\noverlapping traffic patterns, our best model shows 85\\% accuracy when detecting\nintrusions purely using traffic flow statistics. Key indicators for successful\ndetection included occupancy, jam length, and halting durations."
    },
    {
        "date": "2025-05",
        "title": "Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability",
        "author": "Anass Grini, Oumaima Taheri, Btissam El Khamlichi, and Amal El Fallah-Seghrouchni",
        "link": "http://arxiv.org/abs/2505.01328v1",
        "abstract": "While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications."
    },
    {
        "date": "2025-05",
        "title": "Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams",
        "author": "Xinyu Li, Xuebin Ren, Shusen Yang, Liang Shi, and Chia-Mu Yu",
        "link": "http://arxiv.org/abs/2505.01292v1",
        "abstract": "Local Differential Privacy (LDP) enables massive data collection and analysis\nwhile protecting end users' privacy against untrusted aggregators. It has been\napplied to various data types (e.g., categorical, numerical, and graph data)\nand application settings (e.g., static and streaming). Recent findings indicate\nthat LDP protocols can be easily disrupted by poisoning or manipulation\nattacks, which leverage injected/corrupted fake users to send crafted data\nconforming to the LDP reports. However, current attacks primarily target static\nprotocols, neglecting the security of LDP protocols in the streaming settings.\nOur research fills the gap by developing novel fine-grained manipulation\nattacks to LDP protocols for data streams. By reviewing the attack surfaces in\nexisting algorithms, We introduce a unified attack framework with composable\nmodules, which can manipulate the LDP estimated stream toward a target stream.\nOur attack framework can adapt to state-of-the-art streaming LDP algorithms\nwith different analytic tasks (e.g., frequency and mean) and LDP models\n(event-level, user-level, w-event level). We validate our attacks theoretically\nand through extensive experiments on real-world datasets, and finally explore a\npossible defense mechanism for mitigating these attacks."
    },
    {
        "date": "2025-05",
        "title": "Watermark Overwriting Attack on StegaStamp algorithm",
        "author": "I. F. Serzhenko, L. A. Khaertdinova, M. A. Pautov, and A. V. Antsiferova",
        "link": "http://arxiv.org/abs/2505.01474v1",
        "abstract": "This paper presents an attack method on the StegaStamp watermarking algorithm\nthat completely removes watermarks from an image with minimal quality loss,\ndeveloped as part of the NeurIPS \"Erasing the invisible\" competition."
    },
    {
        "date": "2025-05",
        "title": "Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain",
        "author": "Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, and Qingming Huang",
        "link": "http://arxiv.org/abs/2505.01267v1",
        "abstract": "The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods."
    },
    {
        "date": "2025-05",
        "title": "A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture",
        "author": "Najmus Sakib Sizan, Md. Abu Layek, and Khondokar Fida Hasan",
        "link": "http://arxiv.org/abs/2505.01196v1",
        "abstract": "To improve crop forecasting and provide farmers with actionable data-driven\ninsights, we propose a novel approach integrating IoT, machine learning, and\nblockchain technologies. Using IoT, real-time data from sensor networks\ncontinuously monitor environmental conditions and soil nutrient levels,\nsignificantly improving our understanding of crop growth dynamics. Our study\ndemonstrates the exceptional accuracy of the Random Forest model, achieving a\n99.45\\% accuracy rate in predicting optimal crop types and yields, thereby\noffering precise crop projections and customized recommendations. To ensure the\nsecurity and integrity of the sensor data used for these forecasts, we\nintegrate the Ethereum blockchain, which provides a robust and secure platform.\nThis ensures that the forecasted data remain tamper-proof and reliable.\nStakeholders can access real-time and historical crop projections through an\nintuitive online interface, enhancing transparency and facilitating informed\ndecision-making. By presenting multiple predicted crop scenarios, our system\nenables farmers to optimize production strategies effectively. This integrated\napproach promises significant advances in precision agriculture, making crop\nforecasting more accurate, secure, and user-friendly."
    },
    {
        "date": "2025-05",
        "title": "Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks",
        "author": "M. Saeid HaghighiFard, and Sinem Coleri",
        "link": "http://arxiv.org/abs/2505.01186v1",
        "abstract": "Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies."
    },
    {
        "date": "2025-05",
        "title": "Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms",
        "author": "Mehrdad Asadi, Roxana R\u0103dulescu, and Ann Now\u00e9",
        "link": "http://arxiv.org/abs/2505.01181v1",
        "abstract": "Swarming systems, such as for example multi-drone networks, excel at\ncooperative tasks like monitoring, surveillance, or disaster assistance in\ncritical environments, where autonomous agents make decentralized decisions in\norder to fulfill team-level objectives in a robust and efficient manner.\nUnfortunately, team-level coordinated strategies in the wild are vulnerable to\ndata poisoning attacks, resulting in either inaccurate coordination or\nadversarial behavior among the agents. To address this challenge, we contribute\na framework that investigates the effects of such data poisoning attacks, using\nexplainable AI methods. We model the interaction among agents using\nevolutionary intelligence, where an optimal coalition strategically emerges to\nperform coordinated tasks. Then, through a rigorous evaluation, the swarm model\nis systematically poisoned using data manipulation attacks. We showcase the\napplicability of explainable AI methods to quantify the effects of poisoning on\nthe team strategy and extract footprint characterizations that enable\ndiagnosing. Our findings indicate that when the model is poisoned above 10%,\nnon-optimal strategies resulting in inefficient cooperation can be identified."
    },
    {
        "date": "2025-05",
        "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
        "author": "Francisco Aguilera-Mart\u00ednez, and Fernando Berzal",
        "link": "http://arxiv.org/abs/2505.01177v1",
        "abstract": "As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges."
    },
    {
        "date": "2025-05",
        "title": "Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability",
        "author": "Zhaoyang Ma, Zhihao Wu, Wang Lu, Xin Gao, Jinghang Yue, Taolin Zhang, Lipo Wang, Youfang Lin, and Jing Wang",
        "link": "http://arxiv.org/abs/2505.01168v1",
        "abstract": "The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch."
    },
    {
        "date": "2025-05",
        "title": "Active Sybil Attack and Efficient Defense Strategy in IPFS DHT",
        "author": "V. H. M. Netto, T. Cholez, and C. L. Ignat",
        "link": "http://arxiv.org/abs/2505.01139v1",
        "abstract": "The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P)\nstorage that relies on Kademlia, a Distributed Hash Table (DHT) structure\ncommonly used in P2P systems for its proved scalability. However, DHTs are\nknown to be vulnerable to Sybil attacks, in which a single entity controls\nmultiple malicious nodes. Recent studies have shown that IPFS is affected by a\npassive content eclipse attack, leveraging Sybils, in which adversarial nodes\nhide received indexed information from other peers, making the content appear\nunavailable. Fortunately, the latest mitigation strategy coupling an attack\ndetection based on statistical tests and a wider publication strategy upon\ndetection was able to circumvent it.\n  In this work, we present a new active attack, with malicious nodes responding\nwith semantically correct but intentionally false data, exploiting both an\noptimized placement of Sybils to stay below the detection threshold and an\nearly trigger of the content discovery termination in Kubo, the main IPFS\nimplementation. Our attack achieves to completely eclipse content on the latest\nKubo release. When evaluated against the most recent known mitigation, it\nsuccessfully denies access to the target content in approximately 80\\% of\nlookup attempts.\n  To address this vulnerability, we propose a new mitigation called\nSR-DHT-Store, which enables efficient, Sybil-resistant content publication\nwithout relying on attack detection but instead on a systematic and precise use\nof region-based queries, defined by a dynamically computed XOR distance to the\ntarget ID. SR-DHT-Store can be combined with other defense mechanisms resulting\nin a defense strategy that completely mitigates both passive and active Sybil\nattacks at a lower overhead, while allowing an incremental deployment."
    },
    {
        "date": "2025-05",
        "title": "Risk Analysis and Design Against Adversarial Actions",
        "author": "Marco C. Campi, Algo Car\u00e8, Luis G. Crespo, Simone Garatti, and Federico A. Ramponi",
        "link": "http://arxiv.org/abs/2505.01130v1",
        "abstract": "Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework."
    },
    {
        "date": "2025-05",
        "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models",
        "author": "Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, and Matt Fredrikson",
        "link": "http://arxiv.org/abs/2505.01050v1",
        "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs."
    },
    {
        "date": "2025-05",
        "title": "Quantum Support Vector Regression for Robust Anomaly Detection",
        "author": "Kilian Tscharke, Maximilian Wendlinger, Sebastian Issel, and Pascal Debus",
        "link": "http://arxiv.org/abs/2505.01012v2",
        "abstract": "Anomaly Detection (AD) is critical in data analysis, particularly within the\ndomain of IT security. In recent years, Machine Learning (ML) algorithms have\nemerged as a powerful tool for AD in large-scale data. In this study, we\nexplore the potential of quantum ML approaches, specifically quantum kernel\nmethods, for the application to robust AD. We build upon previous work on\nQuantum Support Vector Regression (QSVR) for semisupervised AD by conducting a\ncomprehensive benchmark on IBM quantum hardware using eleven datasets. Our\nresults demonstrate that QSVR achieves strong classification performance and\neven outperforms the noiseless simulation on two of these datasets. Moreover,\nwe investigate the influence of - in the NISQ-era inevitable - quantum noise on\nthe performance of the QSVR. Our findings reveal that the model exhibits\nrobustness to depolarizing, phase damping, phase flip, and bit flip noise,\nwhile amplitude damping and miscalibration noise prove to be more disruptive.\nFinally, we explore the domain of Quantum Adversarial Machine Learning and\ndemonstrate that QSVR is highly vulnerable to adversarial attacks and that\nnoise does not improve the adversarial robustness of the model."
    },
    {
        "date": "2025-05",
        "title": "Attack and defense techniques in large language models: A survey and new perspectives",
        "author": "Zhiyu Liao, Kang Chen, Yuanguo Lin, Kangkang Li, Yunxuan Liu, Hefeng Chen, Xingwang Huang, and Yuanhui Yu",
        "link": "http://arxiv.org/abs/2505.00976v1",
        "abstract": "Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications."
    },
    {
        "date": "2025-05",
        "title": "FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection",
        "author": "Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, and Cuicui Luo",
        "link": "http://arxiv.org/abs/2505.00941v2",
        "abstract": "Time series anomaly detection is critical for system monitoring and risk\nidentification, across various domains, such as finance and healthcare.\nHowever, for most reconstruction-based approaches, detecting anomalies remains\na challenge due to the complexity of sequential patterns in time series data.\nOn the one hand, reconstruction-based techniques are susceptible to\ncomputational deviation stemming from anomalies, which can lead to impure\nrepresentations of normal sequence patterns. On the other hand, they often\nfocus on the time-domain dependencies of time series, while ignoring the\nalignment of frequency information beyond the time domain. To address these\nchallenges, we propose a novel Frequency-augmented Convolutional Transformer\n(FreCT). FreCT utilizes patch operations to generate contrastive views and\nemploys an improved Transformer architecture integrated with a convolution\nmodule to capture long-term dependencies while preserving local topology\ninformation. The introduced frequency analysis based on Fourier transformation\ncould enhance the model's ability to capture crucial characteristics beyond the\ntime domain. To protect the training quality from anomalies and improve the\nrobustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and\nabsolute error to optimize consistency information in both time and frequency\ndomains. Extensive experiments on four public datasets demonstrate that FreCT\noutperforms existing methods in identifying anomalies."
    },
    {
        "date": "2025-05",
        "title": "Robust Root Cause Diagnosis using In-Distribution Interventions",
        "author": "Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, and Amit Sharma",
        "link": "http://arxiv.org/abs/2505.00930v1",
        "abstract": "Diagnosing the root cause of an anomaly in a complex interconnected system is\na pressing problem in today's cloud services and industrial operations. We\npropose In-Distribution Interventions (IDI), a novel algorithm that predicts\nroot cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes\nshould take on anomalous values; 2) **Fix:** had the root cause nodes assumed\nusual values, the target node would not have been anomalous. Prior methods of\nassessing the fix condition rely on counterfactuals inferred from a Structural\nCausal Model (SCM) trained on historical data. But since anomalies are rare and\nfall outside the training distribution, the fitted SCMs yield unreliable\ncounterfactual estimates. IDI overcomes this by relying on interventional\nestimates obtained by solely probing the fitted SCM at in-distribution inputs.\nWe present a theoretical analysis comparing and bounding the errors in\nassessing the fix condition using interventional and counterfactual estimates.\nWe then conduct experiments by systematically varying the SCM's complexity to\ndemonstrate the cases where IDI's interventional approach outperforms the\ncounterfactual approach and vice versa. Experiments on both synthetic and\nPetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies\ntrue root causes more accurately and robustly than nine existing\nstate-of-the-art RCD baselines. Code is released at\nhttps://github.com/nlokeshiisc/IDI_release."
    },
    {
        "date": "2025-05",
        "title": "Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting",
        "author": "Zayn Wang, Frank Pu, Vinci Cheung, and Robert Hao",
        "link": "http://arxiv.org/abs/2505.00888v2",
        "abstract": "As new project upgrading the blockchain industry, novel forms of attack\nchallenges developers to rethink about the design of their innovations. In the\ngrowth stage of the development, Decentralized Autonomous Organizations (DAO)\nintroduces different approaches in managing fund through voting in governance\ntokens. However, relying on tokens as a weight for voting introduces\nopportunities for hackers to manipulate voting results through flash loan,\nallowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to\nexecute through the smart contract. In this research, we learned different\ndefense mechanism against the flash loan attack, and their weakness in\naccessibility that compromise the security of different blockchain projects.\nBased on our observation, we propose a new defensing structure and apply it\nwith cases."
    },
    {
        "date": "2025-05",
        "title": "Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting",
        "author": "Tianya Zhao, Ningning Wang, Junqing Zhang, and Xuyu Wang",
        "link": "http://arxiv.org/abs/2505.00881v1",
        "abstract": "While supervised deep neural networks (DNNs) have proven effective for device\nauthentication via radio frequency (RF) fingerprinting, they are hindered by\ndomain shift issues and the scarcity of labeled data. The success of large\nlanguage models has led to increased interest in unsupervised pre-trained\nmodels (PTMs), which offer better generalization and do not require labeled\ndatasets, potentially addressing the issues mentioned above. However, the\ninherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently\nexplored. In this paper, we thoroughly investigate data-free backdoor attacks\non such PTMs in RF fingerprinting, focusing on a practical scenario where\nattackers lack access to downstream data, label information, and training\nprocesses. To realize the backdoor attack, we carefully design a set of\ntriggers and predefined output representations (PORs) for the PTMs. By mapping\ntriggers and PORs through backdoor training, we can implant backdoor behaviors\ninto the PTMs, thereby introducing vulnerabilities across different downstream\nRF fingerprinting tasks without requiring prior knowledge. Extensive\nexperiments demonstrate the wide applicability of our proposed attack to\nvarious input domains, protocols, and PTMs. Furthermore, we explore potential\ndetection and defense methods, demonstrating the difficulty of fully\nsafeguarding against our proposed backdoor attack."
    },
    {
        "date": "2025-05",
        "title": "Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme",
        "author": "Sarah Flanery, Anson Trapani, Christiana Chamon, and Leyla Nazhandali",
        "link": "http://arxiv.org/abs/2505.00858v1",
        "abstract": "This study investigates a duality approach to information leak detection in\nthe generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme.\nWhile previous work by Chamon and Kish sampled voltages at zero-current\ninstances, this research explores sampling currents at zero-voltage crossings.\nThe objective is to determine if this dual approach can reveal information\nleaks in non-equilibrium KLJN systems. Results indicate that the duality method\nsuccessfully detects information leaks, further supporting the necessity of\nthermal equilibrium for unconditional security in KLJN systems."
    },
    {
        "date": "2025-05",
        "title": "Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks",
        "author": "Qianxi Fu, Youngjoon Suh, Xiaojing Zhang, and Yoonjin Won",
        "link": "http://arxiv.org/abs/2505.00823v1",
        "abstract": "Phase change plays a critical role in thermal management systems, yet\nquantitative characterization of multiphase heat transfer remains limited by\nthe challenges of measuring temperature fields in chaotic, rapidly evolving\nflow regimes. While computational methods offer spatiotemporal resolution in\nidealized cases, replicating complex experimental conditions remains\nprohibitively difficult. Here, we present a data-driven framework that\nleverages a conditional generative adversarial network (CGAN) to infer\ntemperature fields from geometric phase contours in a canonical pool boiling\nconfiguration where advanced data collection techniques are restricted. Using\nhigh-speed imaging data and simulation-informed training, our model\ndemonstrates the ability to reconstruct temperature fields with errors below\n6%. We further show that standard data augmentation strategies are effective in\nenhancing both accuracy and physical plausibility of the predicted maps across\nboth simulation and experimental datasets when precise physical constraints are\nnot applicable. Our results highlight the potential of deep generative models\nto bridge the gap between observable multiphase phenomena and underlying\nthermal transport, offering a powerful approach to augment and interpret\nexperimental measurements in complex two-phase systems."
    },
    {
        "date": "2025-05",
        "title": "Enhancing the Cloud Security through Topic Modelling",
        "author": "Sabbir M. Saleh, Nazim Madhavji, and John Steinbacher",
        "link": "http://arxiv.org/abs/2505.01463v1",
        "abstract": "Protecting cloud applications is crucial in an age where security constantly\nthreatens the digital world. The inevitable cyber-attacks throughout the CI/CD\npipeline make cloud security innovations necessary. This research is motivated\nby applying Natural Language Processing (NLP) methodologies, such as Topic\nModelling, to analyse cloud security data and predict future attacks. This\nresearch aims to use topic modelling, specifically Latent Dirichlet Allocation\n(LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and\nPLSA, security-related text data, such as reports, logs, and other relevant\ndocuments, will be analysed and sorted into relevant topics (such as phishing\nor encryption). These algorithms may apply through Python using the Gensim\nframework. The topics shall be utilised to detect vulnerabilities within\nrelevant CI/CD pipeline records or log data. This application of Topic\nModelling anticipates providing a new form of vulnerability detection,\nimproving overall security throughout the CI/CD pipeline."
    },
    {
        "date": "2025-05",
        "title": "RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks",
        "author": "Gurjot Singh, Alim Dhanani, and Diogo Barradas",
        "link": "http://arxiv.org/abs/2505.00618v1",
        "abstract": "Network attackers have increasingly resorted to proxy chains, VPNs, and\nanonymity networks to conceal their activities. To tackle this issue, past\nresearch has explored the applicability of traffic correlation techniques to\nperform attack attribution, i.e., to identify an attacker's true network\nlocation. However, current traffic correlation approaches rely on\nwell-provisioned and centralized systems that ingest flows from multiple\nnetwork probes to compute correlation scores. Unfortunately, this makes\ncorrelation efforts scale poorly for large high-speed networks.\n  In this paper, we propose RevealNet, a decentralized framework for attack\nattribution that orchestrates a fleet of P4-programmable switches to perform\ntraffic correlation. RevealNet builds on a set of correlation primitives\ninspired by prior work on computing and comparing flow sketches -- compact\nsummaries of flows' key characteristics -- to enable efficient, distributed,\nin-network traffic correlation. Our evaluation suggests that RevealNet achieves\ncomparable accuracy to centralized attack attribution systems while\nsignificantly reducing both the computational complexity and bandwidth\noverheads imposed by correlation tasks."
    },
    {
        "date": "2025-05",
        "title": "A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks",
        "author": "Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, and Baraq Ghaleb",
        "link": "http://arxiv.org/abs/2505.00593v1",
        "abstract": "The security of image data in the Internet of Things (IoT) and edge networks\nis crucial due to the increasing deployment of intelligent systems for\nreal-time decision-making. Traditional encryption algorithms such as AES and\nRSA are computationally expensive for resource-constrained IoT devices and\nineffective for large-volume image data, leading to inefficiencies in\nprivacy-preserving distributed learning applications. To address these\nconcerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption\nscheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic\nChain Permutation and Confusion mechanisms to enhance security while\nmaintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,\nwhich extracts and reorganizes pixels based on high and low edge intensity\nfeatures for correlation disruption; (2) Chaotic Chain Permutation, which\nemploys a logistic chaotic map with SHA-256-based dynamically updated keys for\nblock-wise permutation; and (3) Chaotic chain Confusion, which utilises\ndynamically generated chaotic seed matrices for bitwise XOR operations.\nExtensive security and performance evaluations demonstrate that the proposed\nscheme significantly reduces pixel correlation -- almost zero, achieves high\nentropy values close to 8, and resists differential cryptographic attacks. The\noptimum design of the proposed scheme makes it suitable for real-time\ndeployment in resource-constrained environments."
    },
    {
        "date": "2025-05",
        "title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic",
        "author": "Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, and Rana Hammad Raza",
        "link": "http://arxiv.org/abs/2505.00534v1",
        "abstract": "Vision sensors are becoming more important in Intelligent Transportation\nSystems (ITS) for traffic monitoring, management, and optimization as the\nnumber of network cameras continues to rise. However, manual object tracking\nand matching across multiple non-overlapping cameras pose significant\nchallenges in city-scale urban traffic scenarios. These challenges include\nhandling diverse vehicle attributes, occlusions, illumination variations,\nshadows, and varying video resolutions. To address these issues, we propose an\nefficient and cost-effective deep learning-based framework for Multi-Object\nMulti-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for\nobject detection and employs Non-Maximum Suppression (NMS) to select target\nobjects from overlapping detections. Transfer learning is employed for\nre-identification, enabling the association and generation of vehicle tracklets\nacross multiple cameras. Moreover, we leverage appropriate loss functions and\ndistance measures to handle occlusion, illumination, and shadow challenges. The\nfinal solution identification module performs feature extraction using\nResNet-152 coupled with Deep SORT based vehicle tracking. The proposed\nframework is evaluated on the 5th AI City Challenge dataset (Track 3),\ncomprising 46 camera feeds. Among these 46 camera streams, 40 are used for\nmodel training and validation, while the remaining six are utilized for model\ntesting. The proposed framework achieves competitive performance with an IDF1\nscore of 0.8289, and precision and recall scores of 0.9026 and 0.8527\nrespectively, demonstrating its effectiveness in robust and accurate vehicle\ntracking."
    },
    {
        "date": "2025-05",
        "title": "KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution",
        "author": "Antoni Bigata, Rodrigo Mira, Stella Bounareli, Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic",
        "link": "http://arxiv.org/abs/2505.00497v1",
        "abstract": "Lip synchronization, known as the task of aligning lip movements in an\nexisting video with new input audio, is typically framed as a simpler variant\nof audio-driven facial animation. However, as well as suffering from the usual\nissues in talking head generation (e.g., temporal consistency), lip\nsynchronization presents significant new challenges such as expression leakage\nfrom the input video and facial occlusions, which can severely impact\nreal-world applications like automated dubbing, but are often neglected in\nexisting works. To address these shortcomings, we present KeySync, a two-stage\nframework that succeeds in solving the issue of temporal consistency, while\nalso incorporating solutions for leakage and occlusions using a carefully\ndesigned masking strategy. We show that KeySync achieves state-of-the-art\nresults in lip reconstruction and cross-synchronization, improving visual\nquality and reducing expression leakage according to LipLeak, our novel leakage\nmetric. Furthermore, we demonstrate the effectiveness of our new masking\napproach in handling occlusions and validate our architectural choices through\nseveral ablation studies. Code and model weights can be found at\nhttps://antonibigata.github.io/KeySync."
    },
    {
        "date": "2025-05",
        "title": "Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks",
        "author": "Leonid Legashev, Artur Zhigalov, and Denis Parfenov",
        "link": "http://arxiv.org/abs/2505.00487v1",
        "abstract": "This article describes the process of creating a script and conducting an\nanalytical study of a dataset using the DeepMIMO emulator. An advertorial\nattack was carried out using the FGSM method to maximize the gradient. A\ncomparison is made of the effectiveness of binary classifiers in the task of\ndetecting distorted data. The dynamics of changes in the quality indicators of\nthe regression model were analyzed in conditions without adversarial attacks,\nduring an adversarial attack and when the distorted data was isolated. It is\nshown that an adversarial FGSM attack with gradient maximization leads to an\nincrease in the value of the MSE metric by 33% and a decrease in the R2\nindicator by 10% on average. The LightGBM binary classifier effectively\nidentifies data with adversarial anomalies with 98% accuracy. Regression\nmachine learning models are susceptible to adversarial attacks, but rapid\nanalysis of network traffic and data transmitted over the network makes it\npossible to identify malicious activity"
    },
    {
        "date": "2025-05",
        "title": "Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management",
        "author": "Novruz Amirov, and Kemal Bicakci",
        "link": "http://arxiv.org/abs/2505.00480v1",
        "abstract": "This paper proposes a decentralized, blockchain-based system for the\npublication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate\nthe limitations of the current centralized model primarily overseen by MITRE.\nThe proposed architecture leverages a permissioned blockchain, wherein only\nauthenticated CVE Numbering Authorities (CNAs) are authorized to submit\nentries. This ensures controlled write access while preserving public\ntransparency. By incorporating smart contracts, the system supports key\nfeatures such as embargoed disclosures and decentralized governance. We\nevaluate the proposed model in comparison with existing practices, highlighting\nits advantages in transparency, trust decentralization, and auditability. A\nprototype implementation using Hyperledger Fabric is presented to demonstrate\nthe feasibility of the approach, along with a discussion of its implications\nfor the future of vulnerability disclosure."
    },
    {
        "date": "2025-05",
        "title": "The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks",
        "author": "Anjith George, and Sebastien Marcel",
        "link": "http://arxiv.org/abs/2505.00380v1",
        "abstract": "Cross-spectral face recognition systems are designed to enhance the\nperformance of facial recognition systems by enabling cross-modal matching\nunder challenging operational conditions. A particularly relevant application\nis the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,\nenabling the verification of individuals by comparing NIR facial captures\nacquired with VIS reference images. The use of NIR imaging offers several\nadvantages, including greater robustness to illumination variations, better\nvisibility through glasses and glare, and greater resistance to presentation\nattacks. Despite these claimed benefits, the robustness of NIR-based systems\nagainst presentation attacks has not been systematically studied in the\nliterature. In this work, we conduct a comprehensive evaluation into the\nvulnerability of NIR-VIS cross-spectral face recognition systems to\npresentation attacks. Our empirical findings indicate that, although these\nsystems exhibit a certain degree of reliability, they remain vulnerable to\nspecific attacks, emphasizing the need for further research in this area."
    },
    {
        "date": "2025-05",
        "title": "Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication",
        "author": "Marco De Vincenzi, Shuyang Sun, Chen Bo Calvin Zhang, Manuel Garcia, Shaozu Ding, Chiara Bodei, Ilaria Matteucci, Sanjay E. Sarma, and Dajiang Suo",
        "link": "http://arxiv.org/abs/2505.00340v2",
        "abstract": "Secure and reliable communications are crucial for Intelligent Transportation\nSystems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key\nrole in enabling mobility-enhancing and safety-critical services. Current V2I\nauthentication relies on credential-based methods over wireless\nNon-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation\nand proximity attacks. To mitigate these risks, we propose a unified\nMulti-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS\ncryptographic credentials with a Line-of-Sight (LOS) visual channel. Our\napproach leverages a challenge-response security paradigm: the infrastructure\nissues challenges and the vehicle's headlights respond by flashing a structured\nsequence containing encoded security data. Deep learning models on the\ninfrastructure side then decode the embedded information to authenticate the\nvehicle. Real-world experimental evaluations demonstrate high test accuracy,\nreaching an average of 95% and 96.6%, respectively, under various lighting,\nweather, speed, and distance conditions. Additionally, we conducted extensive\nexperiments on three state-of-the-art deep learning models, including detailed\nablation studies for decoding the flashing sequence. Our results indicate that\nthe optimal architecture employs a dual-channel design, enabling simultaneous\ndecoding of the flashing sequence and extraction of vehicle spatial and\nlocational features for robust authentication."
    },
    {
        "date": "2025-05",
        "title": "AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection",
        "author": "Muhammad Salman, Iqra Tariq, Mishal Zulfiqar, Muqadas Jalal, Sami Aujla, and Sumbal Fatima",
        "link": "http://arxiv.org/abs/2505.00312v1",
        "abstract": "Deepfake detection has become increasingly important due to the rise of\nsynthetic media, which poses significant risks to digital identity and cyber\npresence for security and trust. While multiple approaches have improved\ndetection accuracy, challenges remain in achieving consistent performance\nacross diverse datasets and manipulation types. In response, we propose a novel\ntwo-tier ensemble framework for deepfake detection based on deep learning that\nhierarchically combines multiple instances of three state-of-the-art\narchitectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs\na unique approach where each architecture is instantiated three times with\ndifferent initializations to enhance model diversity, followed by a learnable\nweighting mechanism that dynamically combines their predictions. Unlike\ntraditional fixed-weight ensembles, our first-tier averages predictions within\neach architecture family to reduce model variance, while the second tier learns\noptimal contribution weights through backpropagation, automatically adjusting\neach architecture's influence based on their detection reliability. Our\nexperiments achieved state-of-the-art intra-dataset performance with AUC scores\nof 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and\n99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC\nscores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%\n(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset\ngeneralization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of\n93.16% and 80.62% in cross-dataset evaluations."
    },
    {
        "date": "2025-05",
        "title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data",
        "author": "Jacob Carlson, and Melissa Dell",
        "link": "http://arxiv.org/abs/2505.00282v1",
        "abstract": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS."
    },
    {
        "date": "2025-05",
        "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation",
        "author": "Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, and Mohit Bansal",
        "link": "http://arxiv.org/abs/2505.01456v1",
        "abstract": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs."
    },
    {
        "date": "2025-04",
        "title": "Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework",
        "author": "Ankit Amrutkar, Bj\u00f6rn Kampa, Volkmar Schulz, Johannes Stegmaier, Markus Rothermel, and Dorit Merhof",
        "link": "http://arxiv.org/abs/2505.00220v1",
        "abstract": "Computer-generated holography (CGH) enables applications in holographic\naugmented reality (AR), 3D displays, systems neuroscience, and optical\ntrapping. The fundamental challenge in CGH is solving the inverse problem of\nphase retrieval from intensity measurements. Physics-inspired neural networks\n(PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced\nphase retrieval capabilities. However, their performance strongly depends on\nforward models (FMs) and their hyperparameters (FMHs), limiting generalization,\ncomplicating benchmarking, and hindering hardware optimization. We present a\nsystematic sensitivity analysis framework based on Saltelli's extension of\nSobol's method to quantify FMH impacts on GS-PINN performance. Our analysis\ndemonstrates that SLM pixel-resolution is the primary factor affecting neural\nnetwork sensitivity, followed by pixel-pitch, propagation distance, and\nwavelength. Free space propagation forward models demonstrate superior neural\nnetwork performance compared to Fourier holography, providing enhanced\nparameterization and generalization. We introduce a composite evaluation metric\ncombining performance consistency, generalization capability, and\nhyperparameter perturbation resilience, establishing a unified benchmarking\nstandard across CGH configurations. Our research connects physics-inspired deep\nlearning theory with practical CGH implementations through concrete guidelines\nfor forward model selection, neural network architecture, and performance\nevaluation. Our contributions advance the development of robust, interpretable,\nand generalizable neural networks for diverse holographic applications,\nsupporting evidence-based decisions in CGH research and implementation."
    },
    {
        "date": "2025-04",
        "title": "Efficient and robust 3D blind harmonization for large domain gaps",
        "author": "Hwihun Jeong, Hayeon Lee, Se Young Chun, and Jongho Lee",
        "link": "http://arxiv.org/abs/2505.00133v1",
        "abstract": "Blind harmonization has emerged as a promising technique for MR image\nharmonization to achieve scale-invariant representations, requiring only target\ndomain data (i.e., no source domain data necessary). However, existing methods\nface limitations such as inter-slice heterogeneity in 3D, moderate image\nquality, and limited performance for a large domain gap. To address these\nchallenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization\nframework that leverages an edge-to-image model tailored specifically to\nharmonization. Our framework employs a 3D rectified flow trained on target\ndomain images to reconstruct the original image from an edge map, then yielding\na harmonized image from the edge of a source domain image. We propose\nmulti-stride patch training for efficient 3D training and a refinement module\nfor robust inference by suppressing hallucination. Extensive experiments\ndemonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse\nsource domain images to the target domain, achieving higher correspondence to\nthe target domain characteristics. Downstream task-based quality assessments\nsuch as tissue segmentation and age prediction on diverse MR scanners further\nconfirm the effectiveness of our approach and demonstrate the capability of our\nrobust and generalizable blind harmonization."
    },
    {
        "date": "2025-04",
        "title": "Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned",
        "author": "Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, and Roberto Natella",
        "link": "http://arxiv.org/abs/2505.00111v1",
        "abstract": "This paper presents our experience, in the context of an industrial R&D\nproject, on securing GENIO, a platform for edge computing on Passive Optical\nNetwork (PON) infrastructures, and based on Open-Source Software (OSS). We\nidentify threats and related mitigations through hardening, vulnerability\nmanagement, digital signatures, and static and dynamic analysis. In particular,\nwe report lessons learned in applying these mitigations using OSS, and share\nour findings about the maturity and limitations of these security solutions in\nan industrial context."
    },
    {
        "date": "2025-04",
        "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense",
        "author": "Ting Qiao, Yingjia Wang, Xing Liu, Sixing Wu, Jianbing Li, and Yiming Li",
        "link": "http://arxiv.org/abs/2504.21730v1",
        "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB."
    },
    {
        "date": "2025-04",
        "title": "Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning",
        "author": "Zhiyong Jin, Runhua Xu, Chao Li, Yizhong Liu, and Jianxin Li",
        "link": "http://arxiv.org/abs/2505.01454v2",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems",
        "author": "Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, and Polina Harikeo",
        "link": "http://arxiv.org/abs/2505.00061v1",
        "abstract": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings."
    },
    {
        "date": "2025-04",
        "title": "Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs",
        "author": "Pan Suo, Yu-Ming Shang, San-Chuan Guo, and Xi Zhang",
        "link": "http://arxiv.org/abs/2504.21680v1",
        "abstract": "Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)\nwith external knowledge bases, improving output quality while introducing new\nsecurity risks. Existing studies on RAG vulnerabilities typically focus on\nexploiting the retrieval mechanism to inject erroneous knowledge or malicious\ntexts, inducing incorrect outputs. However, these approaches overlook critical\nweaknesses within LLMs, leaving important attack vectors unexplored and\nlimiting the scope and efficiency of attacks. In this paper, we uncover a novel\nvulnerability: the safety guardrails of LLMs, while designed for protection,\ncan also be exploited as an attack vector by adversaries. Building on this\nvulnerability, we propose MutedRAG, a novel denial-of-service attack that\nreversely leverages the guardrails of LLMs to undermine the availability of RAG\nsystems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to\nbuild a bomb}\", into the knowledge base, MutedRAG intentionally triggers the\nLLM's safety guardrails, causing the system to reject legitimate queries.\nBesides, due to the high sensitivity of guardrails, a single jailbreak sample\ncan affect multiple queries, effectively amplifying the efficiency of attacks\nwhile reducing their costs. Experimental results on three datasets demonstrate\nthat MutedRAG achieves an attack success rate exceeding 60% in many scenarios,\nrequiring only less than one malicious text to each target query on average. In\naddition, we evaluate potential defense strategies against MutedRAG, finding\nthat some of current mechanisms are insufficient to mitigate this threat,\nunderscoring the urgent need for more robust solutions."
    },
    {
        "date": "2025-04",
        "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Haoran Xin, Minghong Fang, Zhuqing Liu, Biao Yi, Tong Li, and Zheli Liu",
        "link": "http://arxiv.org/abs/2504.21668v1",
        "abstract": "Large language models (LLMs) integrated with retrieval-augmented generation\n(RAG) systems improve accuracy by leveraging external knowledge sources.\nHowever, recent research has revealed RAG's susceptibility to poisoning\nattacks, where the attacker injects poisoned texts into the knowledge database,\nleading to attacker-desired responses. Existing defenses, which predominantly\nfocus on inference-time mitigation, have proven insufficient against\nsophisticated attacks. In this paper, we introduce RAGForensics, the first\ntraceback system for RAG, designed to identify poisoned texts within the\nknowledge database that are responsible for the attacks. RAGForensics operates\niteratively, first retrieving a subset of texts from the database and then\nutilizing a specially crafted prompt to guide an LLM in detecting potential\npoisoning texts. Empirical evaluations across multiple datasets demonstrate the\neffectiveness of RAGForensics against state-of-the-art poisoning attacks. This\nwork pioneers the traceback of poisoned texts in RAG systems, providing a\npractical and promising defense mechanism to enhance their security."
    },
    {
        "date": "2025-04",
        "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection",
        "author": "Liqin Wang, Qianyue Hu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2504.21646v1",
        "abstract": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun."
    },
    {
        "date": "2025-04",
        "title": "A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense",
        "author": "Yuchen Ding, Hongli Peng, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.21480v1",
        "abstract": "With the rapid advancement of blockchain technology, smart contracts have\nenabled the implementation of increasingly complex functionalities. However,\nensuring the security of smart contracts remains a persistent challenge across\nthe stages of development, compilation, and execution. Vulnerabilities within\nsmart contracts not only undermine the security of individual applications but\nalso pose significant risks to the broader blockchain ecosystem, as\ndemonstrated by the growing frequency of attacks since 2016, resulting in\nsubstantial financial losses. This paper provides a comprehensive analysis of\nkey security risks in Ethereum smart contracts, specifically those written in\nSolidity and executed on the Ethereum Virtual Machine (EVM). We focus on two\nprevalent and critical vulnerability types (reentrancy and integer overflow) by\nexamining their underlying mechanisms, replicating attack scenarios, and\nassessing effective countermeasures."
    },
    {
        "date": "2025-04",
        "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering",
        "author": "Jingjing Liu, Nian Wu, Xianchao Xiu, and Jianhua Zhang",
        "link": "http://arxiv.org/abs/2504.21472v1",
        "abstract": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu."
    },
    {
        "date": "2025-04",
        "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion",
        "author": "Yu Guo, Guoqing Chen, Tieyong Zeng, Qiyu Jin, and Michael Kwok-Po Ng",
        "link": "http://arxiv.org/abs/2504.21468v1",
        "abstract": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning",
        "author": "Pulkit Agrawal, Rukma Talwadker, Aditya Pareek, and Tridib Mukherjee",
        "link": "http://arxiv.org/abs/2504.21383v1",
        "abstract": "Recent advancements in state-of-the-art (SOTA) offline reinforcement learning\n(RL) have primarily focused on addressing function approximation errors, which\ncontribute to the overestimation of Q-values for out-of-distribution actions, a\nchallenge that static datasets exacerbate. However, high stakes applications\nsuch as recommendation systems in online gaming, introduce further complexities\ndue to player's psychology (intent) driven by gameplay experiences and the\ninherent volatility on the platform. These factors create highly sparse,\npartially overlapping state spaces across policies, further influenced by the\nexperiment path selection logic which biases state spaces towards specific\npolicies. Current SOTA methods constrain learning from such offline data by\nclipping known counterfactual actions as out-of-distribution due to poor\ngeneralization across unobserved states. Further aggravating conservative\nQ-learning and necessitating more online exploration. FAST-Q introduces a novel\napproach that (1) leverages Gradient Reversal Learning to construct balanced\nstate representations, regularizing the policy-specific bias between the\nplayer's state and action thereby enabling counterfactual estimation; (2)\nsupports offline counterfactual exploration in parallel with static data\nexploitation; and (3) proposes a Q-value decomposition strategy for\nmulti-objective optimization, facilitating explainable recommendations over\nshort and long-term objectives. These innovations demonstrate superiority of\nFAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent\nincrease in player returns, 2 percent improvement in lifetime value (LTV), 0.4\npercent enhancement in the recommendation driven engagement, 2 percent\nimprovement in the player's platform dwell time and an impressive 10 percent\nreduction in the costs associated with the recommendation, on our volatile\ngaming platform."
    }
]