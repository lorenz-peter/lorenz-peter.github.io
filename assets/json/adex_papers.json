[
    {
        "date": "2025-05",
        "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval",
        "author": "Nandan Thakur, Crystina Zhang, Xueguang Ma, and Jimmy Lin",
        "link": "http://arxiv.org/abs/2505.16967v1",
        "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."
    },
    {
        "date": "2025-05",
        "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
        "author": "Csaba D\u00e9k\u00e1ny, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16947v1",
        "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."
    },
    {
        "date": "2025-05",
        "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
        "author": "Xiang Li, Yong Tao, Siyuan Zhang, Siwei Liu, Zhitong Xiong, Chunbo Luo, Lu Liu, Mykola Pechenizkiy, Xiao Xiang Zhu, and Tianjin Huang",
        "link": "http://arxiv.org/abs/2505.16793v1",
        "abstract": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models."
    },
    {
        "date": "2025-05",
        "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models",
        "author": "Zhenzhen Ren, GuoBiao Li, Sheng Li, Zhenxing Qian, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2505.16785v1",
        "abstract": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."
    },
    {
        "date": "2025-05",
        "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques",
        "author": "Jianing Geng, Biao Yi, Zekun Fei, Tongxi Wu, Lihai Nie, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.16765v1",
        "abstract": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"
    },
    {
        "date": "2025-05",
        "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP",
        "author": "Alya Zouzou, L\u00e9o and\u00e9ol, M\u00e9lanie Ducoffe, and Ryma Boumazouza",
        "link": "http://arxiv.org/abs/2505.16740v1",
        "abstract": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting",
        "author": "Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, and Hoon-Young Cho",
        "link": "http://arxiv.org/abs/2505.16735v2",
        "abstract": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct extensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach."
    },
    {
        "date": "2025-05",
        "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks",
        "author": "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\u0107, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16723v1",
        "abstract": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."
    },
    {
        "date": "2025-05",
        "title": "Experimental robustness benchmark of quantum neural network on a superconducting quantum processor",
        "author": "Hai-Feng Zhang, Zhao-Yun Chen, Peng Wang, Liang-Liang Guo, Tian-Le Wang, Xiao-Yan Yang, Ren-Ze Zhao, Ze-An Zhao, Sheng Zhang, Lei Du, Hao-Ran Tao, Zhi-Long Jia, Wei-Cheng Kong, Huan-Yu Liu, Athanasios V. Vasilakos, Yang Yang, Yu-Chun Wu, Ji Guan, Peng Duan, and Guo-Ping Guo",
        "link": "http://arxiv.org/abs/2505.16714v1",
        "abstract": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications."
    },
    {
        "date": "2025-05",
        "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models",
        "author": "Xiaobei Yan, Yiming Li, Zhaoxin Fan, Han Qiu, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2505.16670v1",
        "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs."
    },
    {
        "date": "2025-05",
        "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models",
        "author": "Yiwei Sun, Peiqi Jiang, Chuanbin Liu, Luohao Lin, Zhiying Lu, and Hongtao Xie",
        "link": "http://arxiv.org/abs/2505.16643v1",
        "abstract": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}"
    },
    {
        "date": "2025-05",
        "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization",
        "author": "Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.16640v1",
        "abstract": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/."
    },
    {
        "date": "2025-05",
        "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving",
        "author": "Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, and Yadan Luo",
        "link": "http://arxiv.org/abs/2505.16524v1",
        "abstract": "Maintaining robust 3D perception under dynamic and unpredictable test-time\nconditions remains a critical challenge for autonomous driving systems.\nExisting test-time adaptation (TTA) methods often fail in high-variance tasks\nlike 3D object detection due to unstable optimization and sharp minima. While\nrecent model merging strategies based on linear mode connectivity (LMC) offer\nimproved stability by interpolating between fine-tuned checkpoints, they are\ncomputationally expensive, requiring repeated checkpoint access and multiple\nforward passes. In this paper, we introduce CodeMerge, a lightweight and\nscalable model merging framework that bypasses these limitations by operating\nin a compact latent space. Instead of loading full models, CodeMerge represents\neach checkpoint with a low-dimensional fingerprint derived from the source\nmodel's penultimate features and constructs a key-value codebook. We compute\nmerging coefficients using ridge leverage scores on these fingerprints,\nenabling efficient model composition without compromising adaptation quality.\nOur method achieves strong performance across challenging benchmarks, improving\nend-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by\nover 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as\nonline mapping, motion prediction and planning even without training. Code and\npretrained models are released in the supplementary material."
    },
    {
        "date": "2025-05",
        "title": "Language-based Security and Time-inserting Supervisor",
        "author": "Damas P. Gruska",
        "link": "http://arxiv.org/abs/2505.16503v1",
        "abstract": "Algebraic methods are employed in order to define language-based security\nproperties of processes. A supervisor is introduced that can disable unwanted\nbehavior of an insecure process by controlling some of its actions or by\ninserting timed actions to make an insecure process secure. We assume a\nsituation where neither the supervisor nor the attacker has complete\ninformation about the ongoing systems behavior. We study the conditions under\nwhich such a supervisor exists, as well as its properties and limitations."
    },
    {
        "date": "2025-05",
        "title": "Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models",
        "author": "Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.16446v1",
        "abstract": "Multimodal large language models (MLLMs) enable powerful cross-modal\nreasoning capabilities. However, the expanded input space introduces new attack\nsurfaces. Previous jailbreak attacks often inject malicious instructions from\ntext into less aligned modalities, such as vision. As MLLMs increasingly\nincorporate cross-modal consistency and alignment mechanisms, such explicit\nattacks become easier to detect and block. In this work, we propose a novel\nimplicit jailbreak framework termed IJA that stealthily embeds malicious\ninstructions into images via least significant bit steganography and couples\nthem with seemingly benign, image-related textual prompts. To further enhance\nattack effectiveness across diverse MLLMs, we incorporate adversarial suffixes\ngenerated by a surrogate model and introduce a template optimization module\nthat iteratively refines both the prompt and embedding based on model feedback.\nOn commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack\nsuccess rates of over 90% using an average of only 3 queries."
    },
    {
        "date": "2025-05",
        "title": "Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach",
        "author": "Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, and Suiyang Khoo",
        "link": "http://arxiv.org/abs/2505.16403v1",
        "abstract": "Manipulation of local training data and local updates, i.e., the poisoning\nattack, is the main threat arising from the collaborative nature of the\nfederated learning (FL) paradigm. Most existing poisoning attacks aim to\nmanipulate local data/models in a way that causes denial-of-service (DoS)\nissues. In this paper, we introduce a novel attack method, named Federated\nLearning Sliding Attack (FedSA) scheme, aiming at precisely introducing the\nextent of poisoning in a subtle controlled manner. It operates with a\npredefined objective, such as reducing global model's prediction accuracy by\n10\\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)\ntheory with model poisoning attacks. It can manipulate the updates from\nmalicious clients to drive the global model towards a compromised state,\nachieving this at a controlled and inconspicuous rate. Additionally, leveraging\nthe robust control properties of FedSA allows precise control over the\nconvergence bounds, enabling the attacker to set the global accuracy of the\npoisoned model to any desired level. Experimental results demonstrate that\nFedSA can accurately achieve a predefined global accuracy with fewer malicious\nclients while maintaining a high level of stealth and adjustable learning\nrates."
    },
    {
        "date": "2025-05",
        "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems",
        "author": "Yuanhao Huang, Yilong Ren, Jinlei Wang, Lujia Huo, Xuesong Bai, Jinchuan Zhang, and Haiyan Yu",
        "link": "http://arxiv.org/abs/2505.16402v1",
        "abstract": "Autonomous vehicles are typical complex intelligent systems with artificial\nintelligence at their core. However, perception methods based on deep learning\nare extremely vulnerable to adversarial samples, resulting in safety accidents.\nHow to generate effective adversarial examples in the physical world and\nevaluate object detection systems is a huge challenge. In this study, we\npropose a unified joint adversarial training framework for both 2D and 3D\nsamples to address the challenges of intra-class diversity and environmental\nvariations in real-world scenarios. Building upon this framework, we introduce\nan adversarial sample reality enhancement approach that incorporates non-rigid\nsurface modeling and a realistic 3D matching mechanism. We compare with 5\nadvanced adversarial patches and evaluate their attack performance on 8 object\ndetecotrs, including single-stage, two-stage, and transformer-based models.\nExtensive experiment results in digital and physical environments demonstrate\nthat the adversarial textures generated by our method can effectively mislead\nthe target detection model. Moreover, proposed method demonstrates excellent\nrobustness and transferability under multi-angle attacks, varying lighting\nconditions, and different distance in the physical world. The demo video and\ncode can be obtained at https://github.com/Huangyh98/AdvReal.git."
    },
    {
        "date": "2025-05",
        "title": "Consistent and Compatible Modelling of Cyber Intrusions and Incident Response Demonstrated in the Context of Malware Attacks on Critical Infrastructure",
        "author": "Peter Maynard, Yulia Cherdantseva, Avi Shaked, Pete Burnap, and Arif Mehmood",
        "link": "http://arxiv.org/abs/2505.16398v1",
        "abstract": "Cyber Security Incident Response (IR) Playbooks are used to capture the steps\nrequired to recover from a cyber intrusion. Individual IR playbooks should\nfocus on a specific type of incident and be aligned with the architecture of a\nsystem under attack. Intrusion modelling focuses on a specific potential cyber\nintrusion and is used to identify where and what countermeasures are needed,\nand the resulting intrusion models are expected to be used in effective IR,\nideally by feeding IR Playbooks designs. IR playbooks and intrusion models,\nhowever, are created in isolation and at varying stages of the system's\nlifecycle. We take nine critical national infrastructure intrusion models -\nexpressed using Sequential AND Attack Trees - and transform them into models of\nthe same format as IR playbooks. We use Security Modelling Framework for\nmodelling attacks and playbooks, and for demonstrating the feasibility of the\nbetter integration between risk assessment and IR at the modelling level. This\nresults in improved intrusion models and tighter coupling between IR playbooks\nand threat modelling which - as we demonstrate - yields novel insights into the\nanalysis of attacks and response actions. The main contributions of this paper\nare (a) a novel way of representing attack trees using the Security Modelling\nFramework,(b) a new tool for converting Sequential AND attack trees into models\ncompatible with playbooks, and (c) the examples of nine intrusion models\nrepresented using the Security Modelling Framework."
    },
    {
        "date": "2025-05",
        "title": "Efficient Motion Prompt Learning for Robust Visual Tracking",
        "author": "Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, and Huchuan Lu",
        "link": "http://arxiv.org/abs/2505.16321v1",
        "abstract": "Due to the challenges of processing temporal information, most trackers\ndepend solely on visual discriminability and overlook the unique temporal\ncoherence of video data. In this paper, we propose a lightweight and\nplug-and-play motion prompt tracking method. It can be easily integrated into\nexisting vision-based trackers to build a joint tracking framework leveraging\nboth motion and vision cues, thereby achieving robust tracking through\nefficient prompt learning. A motion encoder with three different positional\nencodings is proposed to encode the long-term motion trajectory into the visual\nembedding space, while a fusion decoder and an adaptive weight mechanism are\ndesigned to dynamically fuse visual and motion features. We integrate our\nmotion module into three different trackers with five models in total.\nExperiments on seven challenging tracking benchmarks demonstrate that the\nproposed motion module significantly improves the robustness of vision-based\ntrackers, with minimal training costs and negligible speed sacrifice. Code is\navailable at https://github.com/zj5559/Motion-Prompt-Tracking."
    },
    {
        "date": "2025-05",
        "title": "SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models",
        "author": "Hossein Khalili, Seongbin Park, Venkat Bollapragada, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2505.16318v1",
        "abstract": "As vision-based machine learning models are increasingly integrated into\nautonomous and cyber-physical systems, concerns about (physical) adversarial\npatch attacks are growing. While state-of-the-art defenses can achieve\ncertified robustness with minimal impact on utility against highly-concentrated\nlocalized patch attacks, they fall short in two important areas: (i)\nState-of-the-art methods are vulnerable to low-noise distributed patches where\nperturbations are subtly dispersed to evade detection or masking, as shown\nrecently by the DorPatch attack; (ii) Achieving high robustness with\nstate-of-the-art methods is extremely time and resource-consuming, rendering\nthem impractical for latency-sensitive applications in many cyber-physical\nsystems.\n  To address both robustness and latency issues, this paper proposes a new\ndefense strategy for adversarial patch attacks called SuperPure. The key\nnovelty is developing a pixel-wise masking scheme that is robust against both\ndistributed and localized patches. The masking involves leveraging a GAN-based\nsuper-resolution scheme to gradually purify the image from adversarial patches.\nOur extensive evaluations using ImageNet and two standard classifiers, ResNet\nand EfficientNet, show that SuperPure advances the state-of-the-art in three\nmajor directions: (i) it improves the robustness against conventional localized\npatches by more than 20%, on average, while also improving top-1 clean accuracy\nby almost 10%; (ii) It achieves 58% robustness against distributed patch\nattacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It\ndecreases the defense end-to-end latency by over 98% compared to PatchCleanser.\nOur further analysis shows that SuperPure is robust against white-box attacks\nand different patch sizes. Our code is open-source."
    },
    {
        "date": "2025-05",
        "title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings",
        "author": "Arjhun Swaminathan, and Mete Akg\u00fcn",
        "link": "http://arxiv.org/abs/2505.16313v1",
        "abstract": "Deep neural networks for image classification remain vulnerable to\nadversarial examples -- small, imperceptible perturbations that induce\nmisclassifications. In black-box settings, where only the final prediction is\naccessible, crafting targeted attacks that aim to misclassify into a specific\ntarget class is particularly challenging due to narrow decision regions.\nCurrent state-of-the-art methods often exploit the geometric properties of the\ndecision boundary separating a source image and a target image rather than\nincorporating information from the images themselves. In contrast, we propose\nTargeted Edge-informed Attack (TEA), a novel attack that utilizes edge\ninformation from the target image to carefully perturb it, thereby producing an\nadversarial image that is closer to the source image while still achieving the\ndesired target classification. Our approach consistently outperforms current\nstate-of-the-art methods across different models in low query settings (nearly\n70\\% fewer queries are used), a scenario especially relevant in real-world\napplications with limited queries and black-box access. Furthermore, by\nefficiently generating a suitable adversarial example, TEA provides an improved\ntarget initialization for established geometry-based attacks."
    },
    {
        "date": "2025-05",
        "title": "Paired and Unpaired Image to Image Translation using Generative Adversarial Networks",
        "author": "Gaurav Kumar, Soham Satyadharma, and Harpreet Singh",
        "link": "http://arxiv.org/abs/2505.16310v1",
        "abstract": "Image to image translation is an active area of research in the field of\ncomputer vision, enabling the generation of new images with different styles,\ntextures, or resolutions while preserving their characteristic properties.\nRecent architectures leverage Generative Adversarial Networks (GANs) to\ntransform input images from one domain to another. In this work, we focus on\nthe study of both paired and unpaired image translation across multiple image\ndomains. For the paired task, we used a conditional GAN model, and for the\nunpaired task, we trained it using cycle consistency loss. We experimented with\ndifferent types of loss functions, multiple Patch-GAN sizes, and model\narchitectures. New quantitative metrics - precision, recall, and FID score -\nwere used for analysis. In addition, a qualitative study of the results of\ndifferent experiments was conducted."
    },
    {
        "date": "2025-05",
        "title": "Poster: Towards an Automated Security Testing Framework for Industrial UEs",
        "author": "Sotiris Michaelides, Daniel Eguiguren Chavez, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.16300v1",
        "abstract": "With the ongoing adoption of 5G for communication in industrial systems and\ncritical infrastructure, the security of industrial UEs such as 5G-enabled\nindustrial robots becomes an increasingly important topic. Most notably, to\nmeet the stringent security requirements of industrial deployments, industrial\nUEs not only have to fully comply with the 5G specifications but also implement\nand use correctly secure communication protocols such as TLS. To ensure the\nsecurity of industrial UEs, operators of industrial 5G networks rely on\nsecurity testing before deploying new devices to their production networks.\nHowever, currently only isolated tests for individual security aspects of\nindustrial UEs exist, severely hindering comprehensive testing. In this paper,\nwe report on our ongoing efforts to alleviate this situation by creating an\nautomated security testing framework for industrial UEs to comprehensively\nevaluate their security posture before deployment. With this framework, we aim\nto provide stakeholders with a fully automated-method to verify that\nhigher-layer security protocols are correctly implemented, while simultaneously\nensuring that the UE's protocol stack adheres to 3GPP specifications."
    },
    {
        "date": "2025-05",
        "title": "Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces",
        "author": "Preeti Mehta, Aman Sagar, and Suchi Kumari",
        "link": "http://arxiv.org/abs/2505.16253v1",
        "abstract": "This study aims to address the growing challenge of distinguishing\ncomputer-generated imagery (CGI) from authentic digital images across three\ndifferent color spaces; RGB, YCbCr, and HSV. Given the limitations of existing\nclassification methods in handling the complexity and variability of CGI, this\nresearch proposes a Swin Transformer based model for accurate differentiation\nbetween natural and synthetic images. The proposed model leverages the Swin\nTransformer's hierarchical architecture to capture local and global features\nfor distinguishing CGI from natural images. Its performance was assessed\nthrough intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,\nand Columbia. The model was evaluated individually on each dataset (D1, D2, D3)\nand on the combined datasets (D1+D2+D3) to test its robustness and domain\ngeneralization. To address dataset imbalance, data augmentation techniques were\napplied. Additionally, t-SNE visualization was used to demonstrate the feature\nseparability achieved by the Swin Transformer across the selected color spaces.\nThe model's performance was tested across all color schemes, with the RGB color\nscheme yielding the highest accuracy for each dataset. As a result, RGB was\nselected for domain generalization analysis and compared with other CNN-based\nmodels, VGG-19 and ResNet-50. The comparative results demonstrate the proposed\nmodel's effectiveness in detecting CGI, highlighting its robustness and\nreliability in both intra-dataset and inter-dataset evaluations. The findings\nof this study highlight the Swin Transformer model's potential as an advanced\ntool for digital image forensics, particularly in distinguishing CGI from\nnatural images. The model's strong performance indicates its capability for\ndomain generalization, making it a valuable asset in scenarios requiring\nprecise and reliable image classification."
    },
    {
        "date": "2025-05",
        "title": "VIVID: A Novel Approach to Remediation Prioritization in Static Application Security Testing (SAST)",
        "author": "Naeem Budhwani, Mohammad Faghani, and Hayden Richard",
        "link": "http://arxiv.org/abs/2505.16205v1",
        "abstract": "Static Application Security Testing (SAST) enables organizations to detect\nvulnerabilities in code early; however, major SAST platforms do not include\nvisual aids and present little insight on correlations between tainted data\nchains. We propose VIVID - Vulnerability Information Via Data flow - a novel\nmethod to extract and consume SAST insights, which is to graph the\napplication's vulnerability data flows (VDFs) and carry out graph theory\nanalysis on the resulting VDF directed graph. Nine metrics were assessed to\nevaluate their effectiveness in analyzing the VDF graphs of deliberately\ninsecure web applications. These metrics include 3 centrality metrics, 2\nstructural metrics, PageRank, in-degree, out-degree, and cross-clique\nconnectivity. We present simulations that find that out-degree, betweenness\ncentrality, in-eigenvector centrality, and cross-clique connectivity were found\nto be associated with files exhibiting high vulnerability traffic, making them\nrefactoring candidates where input sanitization may have been missed.\nMeanwhile, out-eigenvector centrality, PageRank, and in-degree were found to be\nassociated with nodes enabling vulnerability flow and sinks, but not\nnecessarily where input validation should be placed. This is a novel method to\nautomatically provide development teams an evidence-based prioritized list of\nfiles to embed security controls into, informed by vulnerability propagation\npatterns in the application architecture."
    },
    {
        "date": "2025-05",
        "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation",
        "author": "Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Yiwei Jin, Keyu Li, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2505.16196v1",
        "abstract": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines."
    },
    {
        "date": "2025-05",
        "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion",
        "author": "Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2505.16166v1",
        "abstract": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks."
    },
    {
        "date": "2025-05",
        "title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World",
        "author": "Ji Guo, Long Zhou, Zhijin Wang, Jiaming He, Qiyang Song, Aiguo Chen, and Wenbo Jiang",
        "link": "http://arxiv.org/abs/2505.16154v1",
        "abstract": "In recent years, deep learning-based Monocular Depth Estimation (MDE) models\nhave been widely applied in fields such as autonomous driving and robotics.\nHowever, their vulnerability to backdoor attacks remains unexplored. To fill\nthe gap in this area, we conduct a comprehensive investigation of backdoor\nattacks against MDE models. Typically, existing backdoor attack methods can not\nbe applied to MDE models. This is because the label used in MDE is in the form\nof a depth map. To address this, we propose BadDepth, the first backdoor attack\ntargeting MDE models. BadDepth overcomes this limitation by selectively\nmanipulating the target object's depth using an image segmentation model and\nrestoring the surrounding areas via depth completion, thereby generating\npoisoned datasets for object-level backdoor attacks. To improve robustness in\nphysical world scenarios, we further introduce digital-to-physical augmentation\nto adapt to the domain gap between the physical world and the digital domain.\nExtensive experiments on multiple models validate the effectiveness of BadDepth\nin both the digital domain and the physical world, without being affected by\nenvironmental factors."
    },
    {
        "date": "2025-05",
        "title": "Outsourcing SAT-based Verification Computations in Network Security",
        "author": "Qi Duan, and Ehab Al-Shaer",
        "link": "http://arxiv.org/abs/2505.16137v1",
        "abstract": "The emergence of cloud computing gives huge impact on large computations.\nCloud computing platforms offer servers with large computation power to be\navailable for customers. These servers can be used efficiently to solve\nproblems that are complex by nature, for example, satisfiability (SAT)\nproblems. Many practical problems can be converted to SAT, for example, circuit\nverification and network configuration analysis. However, outsourcing SAT\ninstances to the servers may cause data leakage that can jeopardize system's\nsecurity. Before\n  outsourcing the SAT instance, one needs to hide the input information. One\nway to preserve privacy and hide information is to randomize the SAT\n  instance before outsourcing. In this paper, we present multiple novel methods\nto randomize SAT instances. We present a novel method to randomize the SAT\ninstance, a variable randomization method to randomize the solution set, and\nmethods to randomize Mincost SAT and MAX3SAT instances. Our analysis and\nevaluation show the correctness and feasibility of these randomization methods.\nThe scalability and generality of our methods make it applicable for real world\nproblems."
    },
    {
        "date": "2025-05",
        "title": "Robust Invariant Representation Learning by Distribution Extrapolation",
        "author": "Kotaro Yoshida, and Konstantinos Slavakis",
        "link": "http://arxiv.org/abs/2505.16126v2",
        "abstract": "Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)\ngeneralization in deep learning by learning invariant representations. As IRM\nposes an inherently challenging bi-level optimization problem, most existing\napproaches -- including IRMv1 -- adopt penalty-based single-level\napproximations. However, empirical studies consistently show that these methods\noften fail to outperform well-tuned empirical risk minimization (ERM),\nhighlighting the need for more robust IRM implementations. This work\ntheoretically identifies a key limitation common to many IRM variants: their\npenalty terms are highly sensitive to limited environment diversity and\nover-parameterization, resulting in performance degradation. To address this\nissue, a novel extrapolation-based framework is proposed that enhances\nenvironmental diversity by augmenting the IRM penalty through synthetic\ndistributional shifts. Extensive experiments -- ranging from synthetic setups\nto realistic, over-parameterized scenarios -- demonstrate that the proposed\nmethod consistently outperforms state-of-the-art IRM variants, validating its\neffectiveness and robustness."
    },
    {
        "date": "2025-05",
        "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization",
        "author": "Wenrui Yu, Yiyi Chen, Johannes Bjerva, Sokol Kosta, and Qiongxiu Li",
        "link": "http://arxiv.org/abs/2505.16008v1",
        "abstract": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings."
    },
    {
        "date": "2025-05",
        "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations",
        "author": "Aaron J. Li, Suraj Srinivas, Usha Bhalla, and Himabindu Lakkaraju",
        "link": "http://arxiv.org/abs/2505.16004v1",
        "abstract": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."
    },
    {
        "date": "2025-05",
        "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security",
        "author": "Omer Hofman, Oren Rachmil, Shamik Bose, Vikas Pahuja, Jonathan Brokman, Toshiya Shimizu, Trisha Starostina, Kelly Marchisio, Seraphina Goldfarb-Tarrant, and Roman Vainshtein",
        "link": "http://arxiv.org/abs/2505.15935v1",
        "abstract": "Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS"
    },
    {
        "date": "2025-05",
        "title": "AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning",
        "author": "Morteza Alizadeh, Mehrdad Oveisi, Sonya Falahati, Ghazal Mousavi, Mohsen Alambardar Meybodi, Somayeh Sadat Mehrnia, Ilker Hacihaliloglu, Arman Rahmim, and Mohammad R. Salmanpour",
        "link": "http://arxiv.org/abs/2505.15931v1",
        "abstract": "Machine learning (ML) models rely heavily on consistent and accurate\nperformance metrics to evaluate and compare their effectiveness. However,\nexisting libraries often suffer from fragmentation, inconsistent\nimplementations, and insufficient data validation protocols, leading to\nunreliable results. Existing libraries have often been developed independently\nand without adherence to a unified standard, particularly concerning the\nspecific tasks they aim to support. As a result, each library tends to adopt\nits conventions for metric computation, input/output formatting, error\nhandling, and data validation protocols. This lack of standardization leads to\nboth implementation differences (ID) and reporting differences (RD), making it\ndifficult to compare results across frameworks or ensure reliable evaluations.\nTo address these issues, we introduce AllMetrics, an open-source unified Python\nlibrary designed to standardize metric evaluation across diverse ML tasks,\nincluding regression, classification, clustering, segmentation, and\nimage-to-image translation. The library implements class-specific reporting for\nmulti-class tasks through configurable parameters to cover all use cases, while\nincorporating task-specific parameters to resolve metric computation\ndiscrepancies across implementations. Various datasets from domains like\nhealthcare, finance, and real estate were applied to our library and compared\nwith Python, Matlab, and R components to identify which yield similar results.\nAllMetrics combines a modular Application Programming Interface (API) with\nrobust input validation mechanisms to ensure reproducibility and reliability in\nmodel evaluation. This paper presents the design principles, architectural\ncomponents, and empirical analyses demonstrating the ability to mitigate\nevaluation errors and to enhance the trustworthiness of ML workflows."
    },
    {
        "date": "2025-05",
        "title": "Challenger: Affordable Adversarial Driving Video Generation",
        "author": "Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, and Hao Zhao",
        "link": "http://arxiv.org/abs/2505.15880v2",
        "abstract": "Generating photorealistic driving videos has seen significant progress\nrecently, but current methods largely focus on ordinary, non-adversarial\nscenarios. Meanwhile, efforts to generate adversarial driving scenarios often\noperate on abstract trajectory or BEV representations, falling short of\ndelivering realistic sensor data that can truly stress-test autonomous driving\n(AD) systems. In this work, we introduce Challenger, a framework that produces\nphysically plausible yet photorealistic adversarial driving videos. Generating\nsuch videos poses a fundamental challenge: it requires jointly optimizing over\nthe space of traffic interactions and high-fidelity sensor observations.\nChallenger makes this affordable through two techniques: (1) a physics-aware\nmulti-round trajectory refinement process that narrows down candidate\nadversarial maneuvers, and (2) a tailored trajectory scoring function that\nencourages realistic yet adversarial behavior while maintaining compatibility\nwith downstream video synthesis. As tested on the nuScenes dataset, Challenger\ngenerates a diverse range of aggressive driving scenarios-including cut-ins,\nsudden lane changes, tailgating, and blind spot intrusions-and renders them\ninto multiview photorealistic videos. Extensive evaluations show that these\nscenarios significantly increase the collision rate of state-of-the-art\nend-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and\nimportantly, adversarial behaviors discovered for one model often transfer to\nothers."
    },
    {
        "date": "2025-05",
        "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
        "author": "Taiye Chen, Zeming Wei, Ang Li, and Yisen Wang",
        "link": "http://arxiv.org/abs/2505.15753v1",
        "abstract": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication."
    },
    {
        "date": "2025-05",
        "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses",
        "author": "Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, and Yves-Alexandre de Montjoye",
        "link": "http://arxiv.org/abs/2505.15738v1",
        "abstract": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs."
    },
    {
        "date": "2025-05",
        "title": "RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction",
        "author": "Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, and Nantheera Anantrasirichai",
        "link": "http://arxiv.org/abs/2505.15737v1",
        "abstract": "Reconstructing high-fidelity underwater scenes remains a challenging task due\nto light absorption, scattering, and limited visibility inherent in aquatic\nenvironments. This paper presents an enhanced Gaussian Splatting-based\nframework that improves both the visual quality and geometric accuracy of deep\nunderwater rendering. We propose decoupled learning for RGB channels, guided by\nthe physics of underwater attenuation, to enable more accurate colour\nrestoration. To address sparse-view limitations and improve view consistency,\nwe introduce a frame interpolation strategy with a novel adaptive weighting\nscheme. Additionally, we introduce a new loss function aimed at reducing noise\nwhile preserving edges, which is essential for deep-sea content. We also\nrelease a newly collected dataset, Submerged3D, captured specifically in\ndeep-sea environments. Experimental results demonstrate that our framework\nconsistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB,\ndelivering superior perceptual quality and robustness, and offering promising\ndirections for marine robotics and underwater visual analytics."
    },
    {
        "date": "2025-05",
        "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
        "author": "Xingyu Zhou, Yulian Wu, and Francesco Orabona",
        "link": "http://arxiv.org/abs/2505.15694v1",
        "abstract": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios."
    },
    {
        "date": "2025-05",
        "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability",
        "author": "Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, and Zhiming Zheng",
        "link": "http://arxiv.org/abs/2505.15683v1",
        "abstract": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
    },
    {
        "date": "2025-05",
        "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off",
        "author": "Yury Belousov, Brian Pulfer, Vitaliy Kinakh, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2505.15594v1",
        "abstract": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed."
    },
    {
        "date": "2025-05",
        "title": "Model Checking the Security of the Lightning Network",
        "author": "Matthias Grundmann, and Hannes Hartenstein",
        "link": "http://arxiv.org/abs/2505.15568v1",
        "abstract": "Payment channel networks are an approach to improve the scalability of\nblockchain-based cryptocurrencies. The Lightning Network is a payment channel\nnetwork built for Bitcoin that is already used in practice. Because the\nLightning Network is used for transfer of financial value, its security in the\npresence of adversarial participants should be verified. The Lightning\nprotocol's complexity makes it hard to assess whether the protocol is secure.\nTo enable computer-aided security verification of Lightning, we formalize the\nprotocol in TLA+ and formally specify the security property that honest users\nare guaranteed to retrieve their correct balance. While model checking provides\na fully automated verification of the security property, the state space of the\nprotocol's specification is so large that model checking becomes unfeasible. We\nmake model checking the Lightning Network possible using two refinement steps\nthat we verify using proofs. In a first step, we prove that the model of time\nused in the protocol can be abstracted using ideas from the research of timed\nautomata. In a second step, we prove that it suffices to model check the\nprotocol for single payment channels and the protocol for multi-hop payments\nseparately. These refinements reduce the state space sufficiently to allow for\nmodel checking Lightning with models with payments over up to four hops and two\nconcurrent payments. These results indicate that the current specification of\nLightning is secure."
    },
    {
        "date": "2025-05",
        "title": "On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?",
        "author": "Raza Imam, Rufael Marew, and Mohammad Yaqub",
        "link": "http://arxiv.org/abs/2505.15425v2",
        "abstract": "Medical Vision-Language Models (MVLMs) have achieved par excellence\ngeneralization in medical image analysis, yet their performance under noisy,\ncorrupted conditions remains largely untested. Clinical imaging is inherently\nsusceptible to acquisition artifacts and noise; however, existing evaluations\npredominantly assess generally clean datasets, overlooking robustness -- i.e.,\nthe model's ability to perform under real-world distortions. To address this\ngap, we first introduce MediMeta-C, a corruption benchmark that systematically\napplies several perturbations across multiple medical imaging datasets.\nCombined with MedMNIST-C, this establishes a comprehensive robustness\nevaluation framework for MVLMs. We further propose RobustMedCLIP, a visual\nencoder adaptation of a pretrained MVLM that incorporates few-shot tuning to\nenhance resilience against corruptions. Through extensive experiments, we\nbenchmark 5 major MVLMs across 5 medical imaging modalities, revealing that\nexisting models exhibit severe degradation under corruption and struggle with\ndomain-modality tradeoffs. Our findings highlight the necessity of diverse\ntraining and robust adaptation strategies, demonstrating that efficient\nlow-rank adaptation when paired with few-shot tuning, improves robustness while\npreserving generalization across modalities."
    },
    {
        "date": "2025-05",
        "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
        "author": "Yuhao Wang, Wenjie Qu, Yanze Jiang, Zichen Liu, Yue Liu, Shengfang Zhai, Yinpeng Dong, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2505.15420v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems."
    },
    {
        "date": "2025-05",
        "title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion",
        "author": "Leon Chlon, Maggie Chlon, and MarcAntonio M. Awada",
        "link": "http://arxiv.org/abs/2505.15417v1",
        "abstract": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference."
    },
    {
        "date": "2025-05",
        "title": "RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation",
        "author": "Naman Patel, Prashanth Krishnamurthy, and Farshad Khorrami",
        "link": "http://arxiv.org/abs/2505.15373v1",
        "abstract": "Mapping and understanding complex 3D environments is fundamental to how\nautonomous systems perceive and interact with the physical world, requiring\nboth precise geometric reconstruction and rich semantic comprehension. While\nexisting 3D semantic mapping systems excel at reconstructing and identifying\npredefined object instances, they lack the flexibility to efficiently build\nsemantic maps with open-vocabulary during online operation. Although recent\nvision-language models have enabled open-vocabulary object recognition in 2D\nimages, they haven't yet bridged the gap to 3D spatial understanding. The\ncritical challenge lies in developing a training-free unified system that can\nsimultaneously construct accurate 3D maps while maintaining semantic\nconsistency and supporting natural language interactions in real time. In this\npaper, we develop a zero-shot framework that seamlessly integrates\nGPU-accelerated geometric reconstruction with open-vocabulary vision-language\nmodels through online instance-level semantic embedding fusion, guided by\nhierarchical object association with spatial indexing. Our training-free system\nachieves superior performance through incremental processing and unified\ngeometric-semantic updates, while robustly handling 2D segmentation\ninconsistencies. The proposed general-purpose 3D scene understanding framework\ncan be used for various tasks including zero-shot 3D instance retrieval,\nsegmentation, and object detection to reason about previously unseen objects\nand interpret natural language queries. The project page is available at\nhttps://razer-3d.github.io."
    },
    {
        "date": "2025-05",
        "title": "Distributionally Robust Federated Learning with Client Drift Minimization",
        "author": "Mounssif Krouka, Chaouki Ben Issaid, and Mehdi Bennis",
        "link": "http://arxiv.org/abs/2505.15371v1",
        "abstract": "Federated learning (FL) faces critical challenges, particularly in\nheterogeneous environments where non-independent and identically distributed\ndata across clients can lead to unfair and inefficient model performance. In\nthis work, we introduce \\textit{DRDM}, a novel algorithm that addresses these\nissues by combining a distributionally robust optimization (DRO) framework with\ndynamic regularization to mitigate client drift. \\textit{DRDM} frames the\ntraining as a min-max optimization problem aimed at maximizing performance for\nthe worst-case client, thereby promoting robustness and fairness. This robust\nobjective is optimized through an algorithm leveraging dynamic regularization\nand efficient local updates, which significantly reduces the required number of\ncommunication rounds. Moreover, we provide a theoretical convergence analysis\nfor convex smooth objectives under partial participation. Extensive experiments\non three benchmark datasets, covering various model architectures and data\nheterogeneity levels, demonstrate that \\textit{DRDM} significantly improves\nworst-case test accuracy while requiring fewer communication rounds than\nexisting state-of-the-art baselines. Furthermore, we analyze the impact of\nsignal-to-noise ratio (SNR) and bandwidth on the energy consumption of\nparticipating clients, demonstrating that the number of local update steps can\nbe adaptively selected to achieve a target worst-case test accuracy with\nminimal total energy cost across diverse communication environments."
    },
    {
        "date": "2025-05",
        "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors",
        "author": "Hao Fang, Jiawei Kong, Tianqu Zhuang, Yixiang Qiu, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, and Min Zhang",
        "link": "http://arxiv.org/abs/2505.15337v1",
        "abstract": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "Towards Zero-Shot Differential Morphing Attack Detection with Multimodal Large Language Models",
        "author": "Ria Shekhawat, Hailin Li, Raghavendra Ramachandra, and Sushma Venkatesh",
        "link": "http://arxiv.org/abs/2505.15332v1",
        "abstract": "Leveraging the power of multimodal large language models (LLMs) offers a\npromising approach to enhancing the accuracy and interpretability of morphing\nattack detection (MAD), especially in real-world biometric applications. This\nwork introduces the use of LLMs for differential morphing attack detection\n(D-MAD). To the best of our knowledge, this is the first study to employ\nmultimodal LLMs to D-MAD using real biometric data. To effectively utilize\nthese models, we design Chain-of-Thought (CoT)-based prompts to reduce\nfailure-to-answer rates and enhance the reasoning behind decisions. Our\ncontributions include: (1) the first application of multimodal LLMs for D-MAD\nusing real data subjects, (2) CoT-based prompt engineering to improve response\nreliability and explainability, (3) comprehensive qualitative and quantitative\nbenchmarking of LLM performance using data from 54 individuals captured in\npassport enrollment scenarios, and (4) comparative analysis of two multimodal\nLLMs: ChatGPT-4o and Gemini providing insights into their morphing attack\ndetection accuracy and decision transparency. Experimental results show that\nChatGPT-4o outperforms Gemini in detection accuracy, especially against\nGAN-based morphs, though both models struggle under challenging conditions.\nWhile Gemini offers more consistent explanations, ChatGPT-4o is more resilient\nbut prone to a higher failure-to-answer rate."
    },
    {
        "date": "2025-05",
        "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution",
        "author": "Ji Guo, Xiaolei Wen, Wenbo Jiang, Cheng Huang, Jinjin Li, and Hongwei Li",
        "link": "http://arxiv.org/abs/2505.15308v1",
        "abstract": "With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks."
    },
    {
        "date": "2025-05",
        "title": "R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections",
        "author": "Xu yan, Zhaohui Wang, Rong Wei, Jingbo Yu, Dong Li, and Xiangde Liu",
        "link": "http://arxiv.org/abs/2505.15294v1",
        "abstract": "We propose R3GS, a robust reconstruction and relocalization framework\ntailored for unconstrained datasets. Our method uses a hybrid representation\nduring training. Each anchor combines a global feature from a convolutional\nneural network (CNN) with a local feature encoded by the multiresolution hash\ngrids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict\nthe attributes of each Gaussians, including color, opacity, and covariance. To\nmitigate the adverse effects of transient objects on the reconstruction\nprocess, we ffne-tune a lightweight human detection network. Once ffne-tuned,\nthis network generates a visibility map that efffciently generalizes to other\ntransient objects (such as posters, banners, and cars) with minimal need for\nfurther adaptation. Additionally, to address the challenges posed by sky\nregions in outdoor scenes, we propose an effective sky-handling technique that\nincorporates a depth prior as a constraint. This allows the inffnitely distant\nsky to be represented on the surface of a large-radius sky sphere,\nsigniffcantly reducing ffoaters caused by errors in sky reconstruction.\nFurthermore, we introduce a novel relocalization method that remains robust to\nchanges in lighting conditions while estimating the camera pose of a given\nimage within the reconstructed 3DGS scene. As a result, R3GS significantly\nenhances rendering ffdelity, improves both training and rendering efffciency,\nand reduces storage requirements. Our method achieves state-of-the-art\nperformance compared to baseline methods on in-the-wild datasets. The code will\nbe made open-source following the acceptance of the paper."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Plan-Execute Framework for Smart Contract Security Auditing",
        "author": "Zhiyuan Wei, Jing Sun, Zijian Zhang, Zhe Hou, and Zixiao Zhao",
        "link": "http://arxiv.org/abs/2505.15242v2",
        "abstract": "Large Language Models (LLMs) have shown great promise in code analysis and\nauditing; however, they still struggle with hallucinations and limited\ncontext-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute\nframework that enhances smart contract security analysis through dynamic audit\nplanning and structured execution. Unlike conventional LLM-based auditing\napproaches that follow fixed workflows and predefined steps, SmartAuditFlow\ndynamically generates and refines audit plans based on the unique\ncharacteristics of each smart contract. It continuously adjusts its auditing\nstrategy in response to intermediate LLM outputs and newly detected\nvulnerabilities, ensuring a more adaptive and precise security assessment. The\nframework then executes these plans step by step, applying a structured\nreasoning process to enhance vulnerability detection accuracy while minimizing\nhallucinations and false positives. To further improve audit precision,\nSmartAuditFlow integrates iterative prompt optimization and external knowledge\nsources, such as static analysis tools and Retrieval-Augmented Generation\n(RAG). This ensures audit decisions are contextually informed and backed by\nreal-world security knowledge, producing comprehensive security reports.\nExtensive evaluations across multiple benchmarks demonstrate that\nSmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on\ncommon and critical vulnerabilities, 41.2 percent accuracy for comprehensive\ncoverage of known smart contract weaknesses in real-world projects, and\nsuccessfully identifying all 13 tested CVEs. These results highlight\nSmartAuditFlow's scalability, cost-effectiveness, and superior adaptability\nover traditional static analysis tools and contemporary LLM-based approaches,\nestablishing it as a robust solution for automated smart contract auditing."
    },
    {
        "date": "2025-05",
        "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems",
        "author": "Andy K. Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Y. Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, Sara Hong, Nardos Demilew, Shivatmica Murgai, Jason Tran, Nishka Kacheria, Ethan Ho, Denis Liu, Lauren McLane, Olivia Bruvik, Dai-Rong Han, Seungwoo Kim, Akhil Vyas, Cuiyuanxiu Chen, Ryan Li, Weiran Xu, Jonathan Z. Ye, Prerit Choudhary, Siddharth M. Bhatia, Vikram Sivashankar, Yuxuan Bao, Dawn Song, Dan Boneh, Daniel E. Ho, and Percy Liang",
        "link": "http://arxiv.org/abs/2505.15216v1",
        "abstract": "AI agents have the potential to significantly alter the cybersecurity\nlandscape. To help us understand this change, we introduce the first framework\nto capture offensive and defensive cyber-capabilities in evolving real-world\nsystems. Instantiating this framework with BountyBench, we set up 25 systems\nwith complex, real-world codebases. To capture the vulnerability lifecycle, we\ndefine three task types: Detect (detecting a new vulnerability), Exploit\n(exploiting a specific vulnerability), and Patch (patching a specific\nvulnerability). For Detect, we construct a new success indicator, which is\ngeneral across vulnerability types and provides localized evaluation. We\nmanually set up the environment for each system, including installing packages,\nsetting up server(s), and hydrating database(s). We add 40 bug bounties, which\nare vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of\nthe OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy\nbased on information to guide detection, interpolating from identifying a zero\nday to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,\nOpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and\nClaude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing\nagents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with\nClaude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on\nExploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch,\nmapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at\ndefense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit\nscores of 32.5% and 57.5% respectively; in contrast, the custom agents are\nrelatively balanced between offense and defense, achieving Exploit scores of\n40-67.5% and Patch scores of 45-60%."
    },
    {
        "date": "2025-05",
        "title": "Group Distributionally Robust Optimization with Flexible Sample Queries",
        "author": "Haomin Bai, Dingzhi Yu, Shuai Li, Haipeng Luo, and Lijun Zhang",
        "link": "http://arxiv.org/abs/2505.15212v1",
        "abstract": "Group distributionally robust optimization (GDRO) aims to develop models that\nperform well across $m$ distributions simultaneously. Existing GDRO algorithms\ncan only process a fixed number of samples per iteration, either 1 or $m$, and\ntherefore can not support scenarios where the sample size varies dynamically.\nTo address this limitation, we investigate GDRO with flexible sample queries\nand cast it as a two-player game: one player solves an online convex\noptimization problem, while the other tackles a prediction with limited advice\n(PLA) problem. Within such a game, we propose a novel PLA algorithm,\nconstructing appropriate loss estimators for cases where the sample size is\neither 1 or not, and updating the decision using follow-the-regularized-leader.\nThen, we establish the first high-probability regret bound for non-oblivious\nPLA. Building upon the above approach, we develop a GDRO algorithm that allows\nan arbitrary and varying sample size per round, achieving a high-probability\noptimization error bound of $O\\left(\\frac{1}{t}\\sqrt{\\sum_{j=1}^t\n\\frac{m}{r_j}\\log m}\\right)$, where $r_t$ denotes the sample size at round $t$.\nThis result demonstrates that the optimization error decreases as the number of\nsamples increases and implies a consistent sample complexity of $O(m\\log\n(m)/\\epsilon^2)$ for any fixed sample size $r\\in[m]$, aligning with existing\nbounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary\nand real-world multi-class datasets."
    },
    {
        "date": "2025-05",
        "title": "EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network",
        "author": "Rina Tazaki, Tomoyuki Akiyama, and Akira Furui",
        "link": "http://arxiv.org/abs/2505.15203v1",
        "abstract": "Automated epileptic seizure detection from electroencephalogram (EEG) remains\nchallenging due to significant individual differences in EEG patterns across\npatients. While existing studies achieve high accuracy with patient-specific\napproaches, they face difficulties in generalizing to new patients. To address\nthis, we propose a detection framework combining domain adversarial training\nwith a convolutional neural network (CNN) and a bidirectional long short-term\nmemory (BiLSTM). First, the CNN extracts local patient-invariant features\nthrough domain adversarial training, which optimizes seizure detection accuracy\nwhile minimizing patient-specific characteristics. Then, the BiLSTM captures\ntemporal dependencies in the extracted features to model seizure evolution\npatterns. Evaluation using EEG recordings from 20 patients with focal epilepsy\ndemonstrated superior performance over non-adversarial methods, achieving high\ndetection accuracy across different patients. The integration of adversarial\ntraining with temporal modeling enables robust cross-patient seizure detection."
    },
    {
        "date": "2025-05",
        "title": "GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation",
        "author": "Hana Satou, and F Monkey",
        "link": "http://arxiv.org/abs/2505.15194v1",
        "abstract": "Domain adaptation remains a challenge when there is significant manifold\ndiscrepancy between source and target domains. Although recent methods leverage\nmanifold-aware adversarial perturbations to perform data augmentation, they\noften neglect precise manifold alignment and systematic exploration of\nstructured perturbations. To address this, we propose GAMA (Geometry-Aware\nManifold Alignment), a structured framework that achieves explicit manifold\nalignment via adversarial perturbation guided by geometric information. GAMA\nsystematically employs tangent space exploration and manifold-constrained\nadversarial optimization, simultaneously enhancing semantic consistency,\nrobustness to off-manifold deviations, and cross-domain alignment. Theoretical\nanalysis shows that GAMA tightens the generalization bound via structured\nregularization and explicit alignment. Empirical results on DomainNet, VisDA,\nand Office-Home demonstrate that GAMA consistently outperforms existing\nadversarial and adaptation methods in both unsupervised and few-shot settings,\nexhibiting superior robustness, generalization, and manifold alignment\ncapability."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss",
        "author": "Bo-Han Lai, Pin-Han Huang, Bo-Han Kung, and Shang-Tse Chen",
        "link": "http://arxiv.org/abs/2505.15174v1",
        "abstract": "Lipschitz neural networks are well-known for providing certified robustness\nin deep learning. In this paper, we present a novel, efficient Block Reflector\nOrthogonal (BRO) layer that enhances the capability of orthogonal layers on\nconstructing more expressive Lipschitz neural architectures. In addition, by\ntheoretically analyzing the nature of Lipschitz neural networks, we introduce a\nnew loss function that employs an annealing mechanism to increase margin for\nmost data points. This enables Lipschitz models to provide better certified\nrobustness. By employing our BRO layer and loss function, we design BRONet - a\nsimple yet effective Lipschitz neural network that achieves state-of-the-art\ncertified robustness. Extensive experiments and empirical analysis on\nCIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms\nexisting baselines. The implementation is available at\n\\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}."
    },
    {
        "date": "2025-05",
        "title": "EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression",
        "author": "Tong Cheng, Fu Jie, Xinpeng Ling, Huifa Li, and Zhili Chen",
        "link": "http://arxiv.org/abs/2505.15140v1",
        "abstract": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. However, since clients\nare required to upload model parameters to the server in each round, this\nprovides the server with an opportunity to infer each client's data privacy. In\nthis paper, we focus on label distribution attacks(LDAs) that aim to infer the\nlabel distributions of the clients' local data. We take the first step to\nattack client's label distributions in FGL. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and we propose a new attack\nnamed EC-LDA, which significantly improves the attack effectiveness by\ncompressing node embeddings. Thirdly, extensive experiments on node\nclassification and link prediction tasks across six widely used graph datasets\nshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal\nvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull and\nLastFM datasets. Finally, we explore the robustness of EC-LDA under\ndifferential privacy protection."
    },
    {
        "date": "2025-05",
        "title": "Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models",
        "author": "Sajjad Ghiasvand, Haniyeh Ehsani Oskouie, Mahnoosh Alizadeh, and Ramtin Pedarsani",
        "link": "http://arxiv.org/abs/2505.15130v1",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have shown remarkable performance\nin cross-modal tasks through large-scale contrastive pre-training. To adapt\nthese large transformer-based models efficiently for downstream tasks,\nParameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as\nscalable alternatives to full fine-tuning, especially in few-shot scenarios.\nHowever, like traditional deep neural networks, VLMs are highly vulnerable to\nadversarial attacks, where imperceptible perturbations can significantly\ndegrade model performance. Adversarial training remains the most effective\nstrategy for improving model robustness in PEFT. In this work, we propose\nAdvCLIP-LoRA, the first algorithm designed to enhance the adversarial\nrobustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method\nformulates adversarial fine-tuning as a minimax optimization problem and\nprovides theoretical guarantees for convergence under smoothness and\nnonconvex-strong-concavity assumptions. Empirical results across eight datasets\nusing ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly\nimproves robustness against common adversarial attacks (e.g., FGSM, PGD),\nwithout sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA\nas a practical and theoretically grounded approach for robust adaptation of\nVLMs in resource-constrained settings."
    },
    {
        "date": "2025-05",
        "title": "A Survey On Secure Machine Learning",
        "author": "Taobo Liao, Taoran Li, and Prathamesh Nadkarni",
        "link": "http://arxiv.org/abs/2505.15124v1",
        "abstract": "In this survey, we will explore the interaction between secure multiparty\ncomputation and the area of machine learning. Recent advances in secure\nmultiparty computation (MPC) have significantly improved its applicability in\nthe realm of machine learning (ML), offering robust solutions for\nprivacy-preserving collaborative learning. This review explores key\ncontributions that leverage MPC to enable multiple parties to engage in ML\ntasks without compromising the privacy of their data. The integration of MPC\nwith ML frameworks facilitates the training and evaluation of models on\ncombined datasets from various sources, ensuring that sensitive information\nremains encrypted throughout the process. Innovations such as specialized\nsoftware frameworks and domain-specific languages streamline the adoption of\nMPC in ML, optimizing performance and broadening its usage. These frameworks\naddress both semi-honest and malicious threat models, incorporating features\nsuch as automated optimizations and cryptographic auditing to ensure compliance\nand data integrity. The collective insights from these studies highlight MPC's\npotential in fostering collaborative yet confidential data analysis, marking a\nsignificant stride towards the realization of secure and efficient\ncomputational solutions in privacy-sensitive industries. This paper\ninvestigates a spectrum of SecureML libraries that includes cryptographic\nprotocols, federated learning frameworks, and privacy-preserving algorithms. By\nsurveying the existing literature, this paper aims to examine the efficacy of\nthese libraries in preserving data privacy, ensuring model confidentiality, and\nfortifying ML systems against adversarial attacks. Additionally, the study\nexplores an innovative application domain for SecureML techniques: the\nintegration of these methodologies in gaming environments utilizing ML."
    },
    {
        "date": "2025-05",
        "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features",
        "author": "Jeremy Qin",
        "link": "http://arxiv.org/abs/2505.15083v1",
        "abstract": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW"
    },
    {
        "date": "2025-05",
        "title": "An Empirical Analysis of EOS Blockchain: Architecture, Contract, and Security",
        "author": "Haiyang Liu, Yingjie Mao, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.15051v1",
        "abstract": "With the rapid development of blockchain technology, various blockchain\nsystems are exhibiting vitality and potential. As a representative of\nBlockchain 3.0, the EOS blockchain has been regarded as a strong competitor to\nEthereum. Nevertheless, compared with Bitcoin and Ethereum, academic research\nand in-depth analyses of EOS remain scarce. To address this gap, this study\nconducts a comprehensive investigation of the EOS blockchain from five key\ndimensions: system architecture, decentralization, performance, smart\ncontracts, and behavioral security. The architectural analysis focuses on six\ncore components of the EOS system, detailing their functionalities and\noperational workflows. The decentralization and performance evaluations, based\non data from the XBlock data-sharing platform, reveal several critical issues:\nlow account activity, limited participation in the supernode election process,\nminimal variation in the set of block producers, and a substantial gap between\nactual throughput and the claimed million-level performance. Five types of\ncontract vulnerabilities are identified in the smart contract dimension, and\nfour mainstream vulnerability detection platforms are introduced and\ncomparatively analyzed. In terms of behavioral security, four real-world\nattacks targeting the structural characteristics of EOS are summarized. This\nstudy contributes to the ongoing development of the EOS blockchain and provides\nvaluable insights for enhancing the security and regulatory mechanisms of\nblockchain ecosystems."
    },
    {
        "date": "2025-05",
        "title": "Topology-aware Detection and Localization of Distributed Denial-of-Service Attacks in Network-on-Chips",
        "author": "Hansika Weerasena, Xiaoguo Jia, and Prabhat Mishra",
        "link": "http://arxiv.org/abs/2505.14898v1",
        "abstract": "Network-on-Chip (NoC) enables on-chip communication between diverse cores in\nmodern System-on-Chip (SoC) designs. With its shared communication fabric, NoC\nhas become a focal point for various security threats, especially in\nheterogeneous and high-performance computing platforms. Among these attacks,\nDistributed Denial of Service (DDoS) attacks occur when multiple malicious\nentities collaborate to overwhelm and disrupt access to critical system\ncomponents, potentially causing severe performance degradation or complete\ndisruption of services. These attacks are particularly challenging to detect\ndue to their distributed nature and dynamic traffic patterns in NoC, which\noften evade static detection rules or simple profiling. This paper presents a\nframework to conduct topology-aware detection and localization of DDoS attacks\nusing Graph Neural Networks (GNNs) by analyzing NoC traffic patterns.\nSpecifically, by modeling the NoC as a graph, our method utilizes\nspatiotemporal traffic features to effectively identify and localize DDoS\nattacks. Unlike prior works that rely on handcrafted features or\nthreshold-based detection, our GNN-based approach operates directly on raw\ninter-flit delay data, learning complex traffic dependencies without manual\nintervention. Experimental results demonstrate that our approach can detect and\nlocalize DDoS attacks with high accuracy (up to 99\\%) while maintaining\nconsistent performance under diverse attack strategies. Furthermore, the\nproposed method exhibits strong robustness across varying numbers and\nplacements of malicious IPs, different packet injection rates, application\nworkloads, and architectural configurations, including both 2D mesh and 3D\nTSV-based NoCs. Our work provides a scalable, flexible, and\narchitecture-agnostic defense mechanism, significantly improving the\navailability and trustworthiness of on-chip communication in future SoC\ndesigns."
    },
    {
        "date": "2025-05",
        "title": "On the (in)security of Proofs-of-Space based Longest-Chain Blockchains",
        "author": "Mirza Ahad Baig, and Krzysztof Pietrzak",
        "link": "http://arxiv.org/abs/2505.14891v1",
        "abstract": "The Nakamoto consensus protocol underlying the Bitcoin blockchain uses proof\nof work as a voting mechanism. Honest miners who contribute hashing power\ntowards securing the chain try to extend the longest chain they are aware of.\nDespite its simplicity, Nakamoto consensus achieves meaningful security\nguarantees assuming that at any point in time, a majority of the hashing power\nis controlled by honest parties. This also holds under ``resource\nvariability'', i.e., if the total hashing power varies greatly over time.\n  Proofs of space (PoSpace) have been suggested as a more sustainable\nreplacement for proofs of work. Unfortunately, no construction of a\n``longest-chain'' blockchain based on PoSpace, that is secure under dynamic\navailability, is known. In this work, we prove that without additional\nassumptions no such protocol exists. We exactly quantify this impossibility\nresult by proving a bound on the length of the fork required for double\nspending as a function of the adversarial capabilities. This bound holds for\nany chain selection rule, and we also show a chain selection rule (albeit a\nvery strange one) that almost matches this bound.\n  Concretely, we consider a security game in which the honest parties at any\npoint control $\\phi>1$ times more space than the adversary. The adversary can\nchange the honest space by a factor $1\\pm \\varepsilon$ with every block\n(dynamic availability), and ``replotting'' the space takes as much time as\n$\\rho$ blocks.\n  We prove that no matter what chain selection rule is used, in this game the\nadversary can create a fork of length $\\phi^2\\cdot \\rho / \\varepsilon$ that\nwill be picked as the winner by the chain selection rule.\n  We also provide an upper bound that matches the lower bound up to a factor\n$\\phi$. There exists a chain selection rule which in the above game requires\nforks of length at least $\\phi\\cdot \\rho / \\varepsilon$."
    },
    {
        "date": "2025-05",
        "title": "Replay Attacks Against Audio Deepfake Detection",
        "author": "Nicolas M\u00fcller, Piotr Kawa, Wei-Herng Choong, Adriana Stan, Aditya Tirumala Bukkapatnam, Karla Pizzi, Alexander Wagner, and Philip Sperl",
        "link": "http://arxiv.org/abs/2505.14862v1",
        "abstract": "We show how replay attacks undermine audio deepfake detection: By playing and\nre-recording deepfake audio through various speakers and microphones, we make\nspoofed samples appear authentic to the detection model. To study this\nphenomenon in more detail, we introduce ReplayDF, a dataset of recordings\nderived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations\nacross six languages and four TTS models. It includes diverse acoustic\nconditions, some highly challenging for detection. Our analysis of six\nopen-source detection models across five datasets reveals significant\nvulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate\n(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response\n(RIR) retraining, performance remains compromised with an 11.0% EER. We release\nReplayDF for non-commercial research use."
    },
    {
        "date": "2025-05",
        "title": "Robust and Efficient AI-Based Attack Recovery in Autonomous Drones",
        "author": "Diego Ortiz Barbosa, Luis Burbano, Siwei Yang, Zijun Wang, Alvaro A. Cardenas, Cihang Xie, and Yinzhi Cao",
        "link": "http://arxiv.org/abs/2505.14835v1",
        "abstract": "We introduce an autonomous attack recovery architecture to add common sense\nreasoning to plan a recovery action after an attack is detected. We outline\nuse-cases of our architecture using drones, and then discuss how to implement\nthis architecture efficiently and securely in edge devices."
    },
    {
        "date": "2025-05",
        "title": "Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks",
        "author": "Navneet Kaur, and Lav Gupta",
        "link": "http://arxiv.org/abs/2505.14659v1",
        "abstract": "As healthcare systems increasingly adopt advanced wireless networks and\nconnected devices, securing medical applications has become critical. The\nintegration of Internet of Medical Things devices, such as robotic surgical\ntools, intensive care systems, and wearable monitors has enhanced patient care\nbut introduced serious security risks. Cyberattacks on these devices can lead\nto life threatening consequences, including surgical errors, equipment failure,\nand data breaches. While the ITU IMT 2030 vision highlights 6G's transformative\nrole in healthcare through AI and cloud integration, it also raises new\nsecurity concerns. This paper explores how explainable AI techniques like SHAP,\nLIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve\ntrust and transparency in 6G enabled healthcare. We support our approach with\nexperimental analysis and highlight promising results."
    },
    {
        "date": "2025-05",
        "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
        "author": "Wentao Ma, Weiming Ren, Yiming Jia, Zhuofeng Li, Ping Nie, Ge Zhang, and Wenhu Chen",
        "link": "http://arxiv.org/abs/2505.14640v1",
        "abstract": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain."
    },
    {
        "date": "2025-05",
        "title": "SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification",
        "author": "Theo Lepage, and Reda Dehak",
        "link": "http://arxiv.org/abs/2505.14561v1",
        "abstract": "Self-Supervised Learning (SSL) has led to considerable progress in Speaker\nVerification (SV). The standard framework uses same-utterance positive sampling\nand data-augmentation to generate anchor-positive pairs of the same speaker.\nThis is a major limitation, as this strategy primarily encodes channel\ninformation from the recording condition, shared by the anchor and positive. We\npropose a new positive sampling technique to address this bottleneck:\nSelf-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find\nan appropriate positive, i.e., of the same speaker identity but a different\nrecording condition, in the latent space using clustering assignments and a\nmemory queue of positive embeddings. SSPS improves SV performance for both\nSimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods\non VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by\nlowering intra-speaker variance, providing comparable performance to DINO-SSPS."
    },
    {
        "date": "2025-05",
        "title": "Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium",
        "author": "Xinxin Fan, Wenxiong Chen, Mengfan Li, Wenqi Wei, and Ling Liu",
        "link": "http://arxiv.org/abs/2505.14463v1",
        "abstract": "Adversarial attacks to graph analytics are gaining increased attention. To\ndate, two lines of countermeasures have been proposed to resist various graph\nadversarial attacks from the perspectives of either graph per se or graph\nneural networks. Nevertheless, a fundamental question lies in whether there\nexists an intrinsic adversarial resilience state within a graph regime and how\nto find out such a critical state if exists. This paper contributes to tackle\nthe above research questions from three unique perspectives: i) we regard the\nprocess of adversarial learning on graph as a complex multi-object dynamic\nsystem, and model the behavior of adversarial attack; ii) we propose a\ngeneralized theoretical framework to show the existence of critical adversarial\nresilience state; and iii) we develop a condensed one-dimensional function to\ncapture the dynamic variation of graph regime under perturbations, and pinpoint\nthe critical state through solving the equilibrium point of dynamic system.\nMulti-facet experiments are conducted to show our proposed approach can\nsignificantly outperform the state-of-the-art defense methods under five\ncommonly-used real-world datasets and three representative attacks."
    },
    {
        "date": "2025-05",
        "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency",
        "author": "Jiafeng Liang, Shixin Jiang, Xuan Dong, Ning Wang, Zheng Chu, Hui Su, Jinlan Fu, Ming Liu, See-Kiong Ng, and Bing Qin",
        "link": "http://arxiv.org/abs/2505.14405v1",
        "abstract": "Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis."
    },
    {
        "date": "2025-05",
        "title": "Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs",
        "author": "Jiawen Wang, Pritha Gupta, Ivan Habernal, and Eyke H\u00fcllermeier",
        "link": "http://arxiv.org/abs/2505.14368v1",
        "abstract": "Recent studies demonstrate that Large Language Models (LLMs) are vulnerable\nto different prompt-based attacks, generating harmful content or sensitive\ninformation. Both closed-source and open-source LLMs are underinvestigated for\nthese attacks. This paper studies effective prompt injection attacks against\nthe $\\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.\nCurrent metrics only consider successful attacks, whereas our proposed Attack\nSuccess Probability (ASP) also captures uncertainty in the model's response,\nreflecting ambiguity in attack feasibility. By comprehensively analyzing the\neffectiveness of prompt injection attacks, we propose a simple and effective\nhypnotism attack; results show that this attack causes aligned language models,\nincluding Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable\nbehaviors, achieving around $90$% ASP. They also indicate that our ignore\nprefix attacks can break all $\\mathbf{14}$ open-source LLMs, achieving over\n$60$% ASP on a multi-categorical dataset. We find that moderately well-known\nLLMs exhibit higher vulnerability to prompt injection attacks, highlighting the\nneed to raise public awareness and prioritize efficient mitigation strategies."
    },
    {
        "date": "2025-05",
        "title": "Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime",
        "author": "Tomasz Maci\u0105\u017cek, and Robert Allison",
        "link": "http://arxiv.org/abs/2505.14323v1",
        "abstract": "Training data reconstruction attacks enable adversaries to recover portions\nof a released model's training data. We consider the attacks where a\nreconstructor neural network learns to invert the (random) mapping between\ntraining data and model weights. Prior work has shown that an informed\nadversary with access to released model's weights and all but one training data\npoint can achieve high-quality reconstructions in this way. However,\ndifferential privacy can defend against such an attack with little to no loss\nin model's utility when the amount of training data is sufficiently large. In\nthis work we consider a more realistic adversary who only knows the\ndistribution from which a small training dataset has been sampled and who\nattacks a transfer-learned neural network classifier that has been trained on\nthis dataset. We exhibit an attack that works in this realistic threat model\nand demonstrate that in the small-data regime it cannot be defended against by\nDP-SGD without severely damaging the classifier accuracy. This raises\nsignificant concerns about the use of such transfer-learned classifiers when\nprotection of training-data is paramount. We demonstrate the effectiveness and\nrobustness of our attack on VGG, EfficientNet and ResNet image classifiers\ntransfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we\npoint out that the commonly used (true-positive) reconstruction success rate\nmetric fails to reliably quantify the actual reconstruction effectiveness.\nInstead, we make use of the Neyman-Pearson lemma to construct the receiver\noperating characteristic curve and consider the associated true-positive\nreconstruction rate at a fixed level of the false-positive reconstruction rate."
    },
    {
        "date": "2025-05",
        "title": "Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion",
        "author": "Tiehan Cui, Yanxu Mao, Peipei Liu, Congying Liu, and Datao You",
        "link": "http://arxiv.org/abs/2505.14316v1",
        "abstract": "Although large language models (LLMs) have achieved remarkable advancements,\ntheir security remains a pressing concern. One major threat is jailbreak\nattacks, where adversarial prompts bypass model safeguards to generate harmful\nor objectionable content. Researchers study jailbreak attacks to understand\nsecurity and robustness of LLMs. However, existing jailbreak attack methods\nface two main challenges: (1) an excessive number of iterative queries, and (2)\npoor generalization across models. In addition, recent jailbreak evaluation\ndatasets focus primarily on question-answering scenarios, lacking attention to\ntext generation tasks that require accurate regeneration of toxic content. To\ntackle these challenges, we propose two contributions: (1) ICE, a novel\nblack-box jailbreak method that employs Intent Concealment and divErsion to\neffectively circumvent security constraints. ICE achieves high attack success\nrates (ASR) with a single query, significantly improving efficiency and\ntransferability across different models. (2) BiSceneEval, a comprehensive\ndataset designed for assessing LLM robustness in question-answering and\ntext-generation tasks. Experimental results demonstrate that ICE outperforms\nexisting jailbreak techniques, revealing critical vulnerabilities in current\ndefense mechanisms. Our findings underscore the necessity of a hybrid security\nstrategy that integrates predefined security mechanisms with real-time semantic\ndecomposition to enhance the security of LLMs."
    },
    {
        "date": "2025-05",
        "title": "AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis",
        "author": "Eirini Panteli, Paulo E. Santos, and Nabil Humphrey",
        "link": "http://arxiv.org/abs/2505.14285v1",
        "abstract": "This paper presents AquaSignal, a modular and scalable pipeline for\npreprocessing, denoising, classification, and novelty detection of underwater\nacoustic signals. Designed to operate effectively in noisy and dynamic marine\nenvironments, AquaSignal integrates state-of-the-art deep learning\narchitectures to enhance the reliability and accuracy of acoustic signal\nanalysis. The system is evaluated on a combined dataset from the Deepship and\nOcean Networks Canada (ONC) benchmarks, providing a diverse set of real-world\nunderwater scenarios. AquaSignal employs a U-Net architecture for denoising, a\nResNet18 convolutional neural network for classifying known acoustic events,\nand an AutoEncoder-based model for unsupervised detection of novel or anomalous\nsignals. To our knowledge, this is the first comprehensive study to apply and\nevaluate this combination of techniques on maritime vessel acoustic data.\nExperimental results show that AquaSignal improves signal clarity and task\nperformance, achieving 71% classification accuracy and 91% accuracy in novelty\ndetection. Despite slightly lower classification performance compared to some\nstate-of-the-art models, differences in data partitioning strategies limit\ndirect comparisons. Overall, AquaSignal demonstrates strong potential for\nreal-time underwater acoustic monitoring in scientific, environmental, and\nmaritime domains."
    },
    {
        "date": "2025-05",
        "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
        "author": "Jennifer D'Souza, Hamed Babaei Giglou, and Quentin M\u00fcnch",
        "link": "http://arxiv.org/abs/2505.14279v1",
        "abstract": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence."
    },
    {
        "date": "2025-05",
        "title": "Embedded Mean Field Reinforcement Learning for Perimeter-defense Game",
        "author": "Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2505.14209v1",
        "abstract": "With the rapid advancement of unmanned aerial vehicles (UAVs) and missile\ntechnologies, perimeter-defense game between attackers and defenders for the\nprotection of critical regions have become increasingly complex and\nstrategically significant across a wide range of domains. However, existing\nstudies predominantly focus on small-scale, simplified two-dimensional\nscenarios, often overlooking realistic environmental perturbations, motion\ndynamics, and inherent heterogeneity--factors that pose substantial challenges\nto real-world applicability. To bridge this gap, we investigate large-scale\nheterogeneous perimeter-defense game in a three-dimensional setting,\nincorporating realistic elements such as motion dynamics and wind fields. We\nderive the Nash equilibrium strategies for both attackers and defenders,\ncharacterize the victory regions, and validate our theoretical findings through\nextensive simulations. To tackle large-scale heterogeneous control challenges\nin defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)\nframework. EMFAC leverages representation learning to enable high-level action\naggregation in a mean-field manner, supporting scalable coordination among\ndefenders. Furthermore, we introduce a lightweight agent-level attention\nmechanism based on reward representation, which selectively filters\nobservations and mean-field information to enhance decision-making efficiency\nand accelerate convergence in large-scale tasks. Extensive simulations across\nvarying scales demonstrate the effectiveness and adaptability of EMFAC, which\noutperforms established baselines in both convergence speed and overall\nperformance. To further validate practicality, we test EMFAC in small-scale\nreal-world experiments and conduct detailed analyses, offering deeper insights\ninto the framework's effectiveness in complex scenarios."
    },
    {
        "date": "2025-05",
        "title": "Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning",
        "author": "Yusuf Denizay D\u00f6nder, Derek Hommel, Andrea W Wen-Yi, David Mimno, and Unso Eun Seo Jo",
        "link": "http://arxiv.org/abs/2505.14174v1",
        "abstract": "LLMs are effective at code generation tasks like text-to-SQL, but is it worth\nthe cost? Many state-of-the-art approaches use non-task-specific LLM techniques\nincluding Chain-of-Thought (CoT), self-consistency, and fine-tuning. These\nmethods can be costly at inference time, sometimes requiring over a hundred LLM\ncalls with reasoning, incurring average costs of up to \\$0.46 per query, while\nfine-tuning models can cost thousands of dollars. We introduce \"N-rep\"\nconsistency, a more cost-efficient text-to-SQL approach that achieves similar\nBIRD benchmark scores as other more expensive methods, at only \\$0.039 per\nquery. N-rep leverages multiple representations of the same schema input to\nmitigate weaknesses in any single representation, making the solution more\nrobust and allowing the use of smaller and cheaper models without any reasoning\nor fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL\napproach in its cost range."
    },
    {
        "date": "2025-05",
        "title": "Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games",
        "author": "Vojt\u011bch K\u016fr, V\u00edt Musil, and Vojt\u011bch \u0158eh\u00e1k",
        "link": "http://arxiv.org/abs/2505.14137v1",
        "abstract": "Adversarial Patrolling games form a subclass of Security games where a\nDefender moves between locations, guarding vulnerable targets. The main\nalgorithmic problem is constructing a strategy for the Defender that minimizes\nthe worst damage an Attacker can cause. We focus on the class of finite-memory\n(also known as regular) Defender's strategies that experimentally outperformed\nother competing classes. A finite-memory strategy can be seen as a positional\nstrategy on a finite set of states. Each state consists of a pair of a location\nand a certain integer value--called memory. Existing algorithms improve the\ntransitional probabilities between the states but require that the available\nmemory size itself is assigned at each location manually. Choosing the right\nmemory assignment is a well-known open and hard problem that hinders the\nusability of finite-memory strategies. We solve this issue by developing a\ngeneral method that iteratively changes the memory assignment. Our algorithm\ncan be used in connection with \\emph{any} black-box strategy optimization tool.\nWe evaluate our method on various experiments and show its robustness by\nsolving instances of various patrolling models."
    },
    {
        "date": "2025-05",
        "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
        "author": "Guangke Chen, Fu Song, Zhe Zhao, Xiaojun Jia, Yang Liu, Yanchen Qiao, and Weizhe Zhang",
        "link": "http://arxiv.org/abs/2505.14103v2",
        "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied\nrecently, but they achieve suboptimal effectiveness, applicability, and\npracticability, particularly, assuming that the adversary can fully manipulate\nuser prompts. In this work, we first conduct an extensive experiment showing\nthat advanced text jailbreak attacks cannot be easily ported to end-to-end\nLALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a\nnovel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio\ndoes not need to align with user prompts in the time axis by crafting suffixal\njailbreak audios; (2) universality: a single jailbreak perturbation is\neffective for different prompts by incorporating multiple prompts into\nperturbation generation; (3) stealthiness: the malicious intent of jailbreak\naudios will not raise the awareness of victims by proposing various intent\nconcealment strategies; and (4) over-the-air robustness: the jailbreak audios\nremain effective when being played over the air by incorporating the\nreverberation distortion effect with room impulse response into the generation\nof the perturbations. In contrast, all prior audio jailbreak attacks cannot\noffer asynchrony, universality, stealthiness, or over-the-air robustness.\nMoreover, AudioJailbreak is also applicable to the adversary who cannot fully\nmanipulate user prompts, thus has a much broader attack scenario. Extensive\nexperiments with thus far the most LALMs demonstrate the high effectiveness of\nAudioJailbreak. We highlight that our work peeks into the security implications\nof audio jailbreak attacks against LALMs, and realistically fosters improving\ntheir security robustness. The implementation and audio samples are available\nat our website https://audiojailbreak.github.io/AudioJailbreak."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners",
        "author": "Soichiro Kumano, Hiroshi Kera, and Toshihiko Yamasaki",
        "link": "http://arxiv.org/abs/2505.14042v1",
        "abstract": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner."
    },
    {
        "date": "2025-05",
        "title": "FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix",
        "author": "Di Wu, Qian Li, Heng Yang, and Yong Han",
        "link": "http://arxiv.org/abs/2505.14024v1",
        "abstract": "Federated Learning (FL) enables geographically distributed clients to\ncollaboratively train machine learning models by sharing only their local\nmodels, ensuring data privacy. However, FL is vulnerable to untargeted attacks\nthat aim to degrade the global model's performance on the underlying data\ndistribution. Existing defense mechanisms attempt to improve FL's resilience\nagainst such attacks, but their effectiveness is limited in practical FL\nenvironments due to data heterogeneity. On the contrary, we aim to detect and\nremove the attacks to mitigate their impact. Generalization contribution plays\na crucial role in distinguishing untargeted attacks. Our observations indicate\nthat, with limited data, the divergence between embeddings representing\ndifferent classes provides a better measure of generalization than direct\naccuracy. In light of this, we propose a novel robust aggregation method,\nFedGraM, designed to defend against untargeted attacks in FL. The server\nmaintains an auxiliary dataset containing one sample per class to support\naggregation. This dataset is fed to the local models to extract embeddings.\nThen, the server calculates the norm of the Gram Matrix of the embeddings for\neach local model. The norm serves as an indicator of each model's inter-class\nseparation capability in the embedding space. FedGraM identifies and removes\npotentially malicious models by filtering out those with the largest norms,\nthen averages the remaining local models to form the global model. We conduct\nextensive experiments to evaluate the performance of FedGraM. Our empirical\nresults show that with limited data samples used to construct the auxiliary\ndataset, FedGraM achieves exceptional performance, outperforming\nstate-of-the-art defense methods."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Training from Mean Field Perspective",
        "author": "Soichiro Kumano, Hiroshi Kera, and Toshihiko Yamasaki",
        "link": "http://arxiv.org/abs/2505.14021v1",
        "abstract": "Although adversarial training is known to be effective against adversarial\nexamples, training dynamics are not well understood. In this study, we present\nthe first theoretical analysis of adversarial training in random deep neural\nnetworks without any assumptions on data distributions. We introduce a new\ntheoretical framework based on mean field theory, which addresses the\nlimitations of existing mean field-based approaches. Based on this framework,\nwe derive (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial\nloss with $\\ell_p$ norm-based adversarial examples for various values of $p$\nand $q$. Moreover, we prove that networks without shortcuts are generally not\nadversarially trainable and that adversarial training reduces network capacity.\nWe also show that network width alleviates these issues. Furthermore, we\npresent the various impacts of the input and output dimensions on the upper\nbounds and time evolution of the weight variance."
    },
    {
        "date": "2025-05",
        "title": "D4+: Emergent Adversarial Driving Maneuvers with Approximate Functional Optimization",
        "author": "Diego Ortiz Barbosa, Luis Burbano, Carlos Hernandez, Zengxiang Lei, Younghee Park, Satish Ukkusuri, and Alvaro A Cardenas",
        "link": "http://arxiv.org/abs/2505.13942v1",
        "abstract": "Intelligent mechanisms implemented in autonomous vehicles, such as proactive\ndriving assist and collision alerts, reduce traffic accidents. However,\nverifying their correct functionality is difficult due to complex interactions\nwith the environment. This problem is exacerbated in adversarial environments,\nwhere an attacker can control the environment surrounding autonomous vehicles\nto exploit vulnerabilities.\n  To preemptively identify vulnerabilities in these systems, in this paper, we\nimplement a scenario-based framework with a formal method to identify the\nimpact of malicious drivers interacting with autonomous vehicles. The\nformalization of the evaluation requirements utilizes metric temporal logic\n(MTL) to identify a safety condition that we want to test. Our goal is to find,\nthrough a rigorous testing approach, any trace that violates this MTL safety\nspecification. Our results can help designers identify the range of safe\noperational behaviors that prevent malicious drivers from exploiting the\nautonomous features of modern vehicles."
    },
    {
        "date": "2025-05",
        "title": "The Hidden Dangers of Outdated Software: A Cyber Security Perspective",
        "author": "Gogulakrishnan Thiyagarajan, Vinay Bist, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2505.13922v1",
        "abstract": "Outdated software remains a potent and underappreciated menace in 2025's\ncybersecurity environment, exposing systems to a broad array of threats,\nincluding ransomware, data breaches, and operational outages that can have\ndevastating and far-reaching impacts. This essay explores the unseen threats of\ncyberattacks by presenting robust statistical information, including the\nstaggering reality that 32% of cyberattacks exploit unpatched software\nvulnerabilities, based on a 2025 TechTarget survey. Furthermore, it discusses\nreal case studies, including the MOVEit breach in 2023 and the Log4Shell breach\nin 2021, both of which illustrate the catastrophic consequences of failing to\nperform software updates. The article offers a detailed analysis of the nature\nof software vulnerabilities, the underlying reasons for user resistance to\npatches, and organizational barriers that compound the issue. Furthermore, it\nsuggests actionable solutions, including automation and awareness campaigns, to\naddress these shortcomings. Apart from this, the paper also talks of trends\nsuch as AI-driven vulnerability patching and legal consequences of\nnon-compliance under laws like HIPAA, thus providing a futuristic outlook on\nhow such advancements may define future defenses. Supplemented by tables like\none detailing trends in vulnerability and a graph illustrating technology\nadoption, this report showcases the pressing demand for anticipatory update\nstrategies to safeguard digital ecosystems against the constantly evolving\nthreats that characterize the modern cyber landscape. As it stands, it is a\nvery useful document for practitioners, policymakers, and researchers."
    },
    {
        "date": "2025-05",
        "title": "ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models",
        "author": "Guangtao Zheng, Wenqian Ye, and Aidong Zhang",
        "link": "http://arxiv.org/abs/2505.13910v1",
        "abstract": "Deep learning models often achieve high performance by inadvertently learning\nspurious correlations between targets and non-essential features. For example,\nan image classifier may identify an object via its background that spuriously\ncorrelates with it. This prediction behavior, known as spurious bias, severely\ndegrades model performance on data that lacks the learned spurious\ncorrelations. Existing methods on spurious bias mitigation typically require a\nvariety of data groups with spurious correlation annotations called group\nlabels. However, group labels require costly human annotations and often fail\nto capture subtle spurious biases such as relying on specific pixels for\npredictions. In this paper, we propose a novel post hoc spurious bias\nmitigation framework without requiring group labels. Our framework, termed\nShortcutProbe, identifies prediction shortcuts that reflect potential\nnon-robustness in predictions in a given model's latent space. The model is\nthen retrained to be invariant to the identified prediction shortcuts for\nimproved robustness. We theoretically analyze the effectiveness of the\nframework and empirically demonstrate that it is an efficient and practical\ntool for improving a model's robustness to spurious bias on diverse datasets."
    },
    {
        "date": "2025-05",
        "title": "PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks",
        "author": "Guobin Shen, Dongcheng Zhao, Linghao Feng, Xiang He, Jihang Wang, Sicheng Shen, Haibo Tong, Yiting Dong, Jindong Li, Xiang Zheng, and Yi Zeng",
        "link": "http://arxiv.org/abs/2505.13862v2",
        "abstract": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety."
    },
    {
        "date": "2025-05",
        "title": "hChain 4.0: A Secure and Scalable Permissioned Blockchain for EHR Management in Smart Healthcare",
        "author": "Musharraf N. Alruwaill, Saraju P. Mohanty, and Elias Kougianos",
        "link": "http://arxiv.org/abs/2505.13861v1",
        "abstract": "The growing utilization of Internet of Medical Things (IoMT) devices,\nincluding smartwatches and wearable medical devices, has facilitated real-time\nhealth monitoring and data analysis to enhance healthcare outcomes. These\ngadgets necessitate improved security measures to safeguard sensitive health\ndata while tackling scalability issues in real-time settings. The proposed\nsystem, hChain 4.0, employs a permissioned blockchain to provide a secure and\nscalable data infrastructure designed to fulfill these needs. This stands in\ncontrast to conventional systems, which are vulnerable to security flaws or\nrely on public blockchains, constrained by scalability and expense. The\nproposed approach introduces a high-privacy method in which health data are\nencrypted using the Advanced Encryption Standard (AES) for time-efficient\nencryption, combined with Partial Homomorphic Encryption (PHE) to enable secure\ncomputations on encrypted data, thereby enhancing privacy. Moreover, it\nutilizes private channels that enable isolated communication and ledger between\nstakeholders, ensuring robust privacy while supporting collaborative\noperations. The proposed framework enables anonymized health data sharing for\nmedical research by pseudonymizing patient identity. Additionally, hChain 4.0\nincorporates Attribute-Based Access Control (ABAC) to provide secure electronic\nhealth record (EHR) sharing among authorized parties, where ABAC ensures\nfine-grained permission management vital for multi-organizational healthcare\nsettings. Experimental assessments indicate that the proposed approach achieves\nhigher scalability, cost-effectiveness, and validated security."
    },
    {
        "date": "2025-05",
        "title": "QUT-DV25: A Dataset for Dynamic Analysis of Next-Gen Software Supply Chain Attacks",
        "author": "Sk Tanzir Mehedi, Raja Jurdak, Chadni Islam, and Gowri Ramachandran",
        "link": "http://arxiv.org/abs/2505.13804v1",
        "abstract": "Securing software supply chains is a growing challenge due to the inadequacy\nof existing datasets in capturing the complexity of next-gen attacks, such as\nmultiphase malware execution, remote access activation, and dynamic payload\ngeneration. Existing datasets, which rely on metadata inspection and static\ncode analysis, are inadequate for detecting such attacks. This creates a\ncritical gap because these datasets do not capture what happens during and\nafter a package is installed. To address this gap, we present QUT-DV25, a\ndynamic analysis dataset specifically designed to support and advance research\non detecting and mitigating supply chain attacks within the Python Package\nIndex (PyPI) ecosystem. This dataset captures install and post-install-time\ntraces from 14,271 Python packages, of which 7,127 are malicious. The packages\nare executed in an isolated sandbox environment using an extended Berkeley\nPacket Filter (eBPF) kernel and user-level probes. It captures 36 real-time\nfeatures, that includes system calls, network traffic, resource usages,\ndirectory access patterns, dependency logs, and installation behaviors,\nenabling the study of next-gen attack vectors. ML analysis using the QUT-DV25\ndataset identified four malicious PyPI packages previously labeled as benign,\neach with thousands of downloads. These packages deployed covert remote access\nand multi-phase payloads, were reported to PyPI maintainers, and subsequently\nremoved. This highlights the practical value of QUT-DV25, as it outperforms\nreactive, metadata, and static datasets, offering a robust foundation for\ndeveloping and benchmarking advanced threat detection within the evolving\nsoftware supply chain ecosystem."
    },
    {
        "date": "2025-05",
        "title": "Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery",
        "author": "Aikaterini-Panagiota Stouka, Julian Ma, and Thomas Thiery",
        "link": "http://arxiv.org/abs/2505.13751v1",
        "abstract": "Censorship resistance is one of the core value proposition of blockchains. A\nrecurring design pattern aimed at providing censorship resistance is enabling\nmultiple proposers to contribute inputs into block construction. Notably,\nFork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in\nEthereum. However, the current proposal relies on altruistic behavior, without\na Transaction Fee Mechanism (TFM). This study aims to address this gap by\nexploring how multiple proposers should be rewarded to incentivize censorship\nresistance. The main contribution of this work is the identification of TFMs\nthat ensure censorship resistance under bribery attacks, while also satisfying\nthe incentive compatibility properties of EIP-1559. We provide a concrete\npayment mechanism for FOCIL, along with generalizable contributions to the\nliterature by analyzing 1) incentive compatibility of TFMs in the presence of a\nbribing adversary, 2) TFMs in protocols with multiple phases of transaction\ninclusion, and 3) TFMs of protocols in which parties are uncertain about the\nbehavior and the possible bribe of others."
    },
    {
        "date": "2025-05",
        "title": "Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning",
        "author": "Jiayu Chen, Aravind Venugopal, and Jeff Schneider",
        "link": "http://arxiv.org/abs/2505.13709v1",
        "abstract": "Offline reinforcement learning (RL) offers a powerful paradigm for\ndata-driven control. Compared to model-free approaches, offline model-based RL\n(MBRL) explicitly learns a world model from a static dataset and uses it as a\nsurrogate simulator, improving data efficiency and enabling potential\ngeneralization beyond the dataset support. However, most existing offline MBRL\nmethods follow a two-stage training procedure: first learning a world model by\nmaximizing the likelihood of the observed transitions, then optimizing a policy\nto maximize its expected return under the learned model. This objective\nmismatch results in a world model that is not necessarily optimized for\neffective policy learning. Moreover, we observe that policies learned via\noffline MBRL often lack robustness during deployment, and small adversarial\nnoise in the environment can lead to significant performance degradation. To\naddress these, we propose a framework that dynamically adapts the world model\nalongside the policy under a unified learning objective aimed at improving\nrobustness. At the core of our method is a maximin optimization problem, which\nwe solve by innovatively utilizing Stackelberg learning dynamics. We provide\ntheoretical analysis to support our design and introduce computationally\nefficient implementations. We benchmark our algorithm on twelve noisy D4RL\nMuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its\nstate-of-the-art performance."
    },
    {
        "date": "2025-05",
        "title": "Robust learning of halfspaces under log-concave marginals",
        "author": "Jane Lange, and Arsen Vasilyan",
        "link": "http://arxiv.org/abs/2505.13708v1",
        "abstract": "We say that a classifier is \\emph{adversarially robust} to perturbations of\nnorm $r$ if, with high probability over a point $x$ drawn from the input\ndistribution, there is no point within distance $\\le r$ from $x$ that is\nclassified differently. The \\emph{boundary volume} is the probability that a\npoint falls within distance $r$ of a point with a different label. This work\nstudies the task of computationally efficient learning of hypotheses with small\nboundary volume, where the input is distributed as a subgaussian isotropic\nlog-concave distribution over $\\mathbb{R}^d$.\n  Linear threshold functions are adversarially robust; they have boundary\nvolume proportional to $r$. Such concept classes are efficiently learnable by\npolynomial regression, which produces a polynomial threshold function (PTF),\nbut PTFs in general may have boundary volume $\\Omega(1)$, even for $r \\ll 1$.\n  We give an algorithm that agnostically learns linear threshold functions and\nreturns a classifier with boundary volume $O(r+\\varepsilon)$ at radius of\nperturbation $r$. The time and sample complexity of\n$d^{\\tilde{O}(1/\\varepsilon^2)}$ matches the complexity of polynomial\nregression.\n  Our algorithm augments the classic approach of polynomial regression with\nthree additional steps: a) performing the $\\ell_1$-error regression under noise\nsensitivity constraints, b) a structured partitioning and rounding step that\nreturns a Boolean classifier with error $\\textsf{opt} + O(\\varepsilon)$ and\nnoise sensitivity $O(r+\\varepsilon)$ simultaneously, and c) a local corrector\nthat ``smooths'' a function with low noise sensitivity into a function that is\nadversarially robust."
    },
    {
        "date": "2025-05",
        "title": "R3: Robust Rubric-Agnostic Reward Models",
        "author": "David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, and Genta Indra Winata",
        "link": "http://arxiv.org/abs/2505.13388v1",
        "abstract": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"
    },
    {
        "date": "2025-05",
        "title": "DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks",
        "author": "Javad Forough, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2505.13362v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a significant risk to the privacy of\ntraining datasets by exploiting subtle differences in model outputs to\ndetermine whether a particular data sample was used during training. These\nattacks can compromise sensitive information, especially in domains such as\nhealthcare and finance, where data privacy is paramount. Traditional mitigation\ntechniques, such as static differential privacy, rely on injecting a fixed\namount of noise during training or inference. However, this approach often\nleads to a detrimental trade-off: the noise may be insufficient to counter\nsophisticated attacks or, when increased, may substantially degrade model\nperformance. In this paper, we present DynaNoise, an adaptive approach that\ndynamically modulates noise injection based on query sensitivity. Our approach\nperforms sensitivity analysis using measures such as Shannon entropy to\nevaluate the risk associated with each query and adjusts the noise variance\naccordingly. A probabilistic smoothing step is then applied to renormalize the\nperturbed outputs, ensuring that the model maintains high accuracy while\neffectively obfuscating membership signals. We further propose an empirical\nmetric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),\nwhich quantifies the balance between reducing attack success rates and\npreserving the target model's accuracy. Our extensive evaluation on several\nbenchmark datasets demonstrates that DynaNoise not only significantly reduces\nMIA success rates but also achieves up to a fourfold improvement in the MIDPUT\nmetric compared to the state-of-the-art. Moreover, DynaNoise maintains\ncompetitive model accuracy while imposing only marginal inference overhead,\nhighlighting its potential as an effective and efficient privacy defense\nagainst MIAs."
    },
    {
        "date": "2025-05",
        "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications",
        "author": "Fr\u00e9d\u00e9ric Berdoz, Dustin Brunner, Yann Vonlanthen, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2505.13329v1",
        "abstract": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future."
    },
    {
        "date": "2025-05",
        "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning",
        "author": "Ajian Liu, Haocheng Yuan, Xiao Guo, Hui Ma, Wanyi Zhuang, Changtao Miao, Yan Hong, Chuanbiao Song, Jun Lan, Qi Chu, Tao Gong, Yanyan Liang, Weiqiang Wang, Jun Wan, Xiaoming Liu, and Zhen Lei",
        "link": "http://arxiv.org/abs/2505.13327v2",
        "abstract": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."
    },
    {
        "date": "2025-05",
        "title": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation",
        "author": "Tian Wen, Sheng Sun, Yuwei Wang, Peiyan Chen, Zhiyuan Wu, Min Liu, and Bo Gao",
        "link": "http://arxiv.org/abs/2505.13319v2",
        "abstract": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems."
    },
    {
        "date": "2025-05",
        "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization",
        "author": "Alonso Urbano, David W. Romero, Max Zimmer, and Sebastian Pokutta",
        "link": "http://arxiv.org/abs/2505.13289v1",
        "abstract": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."
    },
    {
        "date": "2025-05",
        "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification",
        "author": "Elias Collaert, Abel Rodr\u00edguez, Sander Joos, Lieven Desmet, and Vera Rimmer",
        "link": "http://arxiv.org/abs/2505.13280v1",
        "abstract": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."
    },
    {
        "date": "2025-05",
        "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment",
        "author": "Younghyun Kim, Jongheon Jeong, Sangkyung Kwak, Kyungmin Lee, Juho Lee, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2505.13232v2",
        "abstract": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions. We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features. Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities",
        "author": "Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, and Tomas Ward",
        "link": "http://arxiv.org/abs/2505.13195v1",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."
    },
    {
        "date": "2025-05",
        "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework",
        "author": "Shaowu Wu, Liting Zeng, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13101v1",
        "abstract": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."
    },
    {
        "date": "2025-05",
        "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry",
        "author": "Zhaoyi Wang, Shengyu Huang, Jemil Avers Butt, Yuanzhou Cai, Matej Varga, and Andreas Wieser",
        "link": "http://arxiv.org/abs/2505.13088v1",
        "abstract": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"
    },
    {
        "date": "2025-05",
        "title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction",
        "author": "Jie Yan, Xin Liu, and Zhong-Yuan Zhang",
        "link": "http://arxiv.org/abs/2505.13071v1",
        "abstract": "Federated clustering (FC) aims to discover global cluster structures across\ndecentralized clients without sharing raw data, making privacy preservation a\nfundamental requirement. There are two critical challenges: (1) privacy leakage\nduring collaboration, and (2) robustness degradation due to aggregation of\nproxy information from non-independent and identically distributed (Non-IID)\nlocal data, leading to inaccurate or inconsistent global clustering. Existing\nsolutions typically rely on model-specific local proxies, which are sensitive\nto data heterogeneity and inherit inductive biases from their centralized\ncounterparts, thus limiting robustness and generality. We propose Omni\nFederated Clustering (OmniFC), a unified and model-agnostic framework.\nLeveraging Lagrange coded computing, our method enables clients to share only\nencoded data, allowing exact reconstruction of the global distance matrix--a\nfundamental representation of sample relationships--without leaking private\ninformation, even under client collusion. This construction is naturally\nresilient to Non-IID data distributions. This approach decouples FC from\nmodel-specific proxies, providing a unified extension mechanism applicable to\ndiverse centralized clustering methods. Theoretical analysis confirms both\nreconstruction fidelity and privacy guarantees, while comprehensive experiments\ndemonstrate OmniFC's superior robustness, effectiveness, and generality across\nvarious benchmarks compared to state-of-the-art methods. Code will be released."
    },
    {
        "date": "2025-05",
        "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions",
        "author": "Yimao Guo, Zuomin Qu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.13023v1",
        "abstract": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."
    },
    {
        "date": "2025-05",
        "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents",
        "author": "Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2505.12981v2",
        "abstract": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."
    },
    {
        "date": "2025-05",
        "title": "RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees",
        "author": "Eilon Vaknin Laufer, and Boaz Nadler",
        "link": "http://arxiv.org/abs/2505.12919v1",
        "abstract": "Recovering a low rank matrix from a subset of its entries, some of which may\nbe corrupted, is known as the robust matrix completion (RMC) problem. Existing\nRMC methods have several limitations: they require a relatively large number of\nobserved entries; they may fail under overparametrization, when their assumed\nrank is higher than the correct one; and many of them fail to recover even\nmildly ill-conditioned matrices. In this paper we propose a novel RMC method,\ndenoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$\nis a simple factorization-based iterative algorithm, which combines a\nGauss-Newton linearization with removal of entries suspected to be outliers. On\nthe theoretical front, we prove that under suitable assumptions,\n$\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank\nmatrix. Our theoretical results improve upon the best currently known for\nfactorization-based methods. On the empirical front, we show via several\nsimulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and\nin particular its ability to handle a small number of observed entries,\noverparameterization of the rank and ill-conditioned matrices."
    },
    {
        "date": "2025-05",
        "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?",
        "author": "Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Ronghua Li",
        "link": "http://arxiv.org/abs/2505.12871v1",
        "abstract": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."
    },
    {
        "date": "2025-05",
        "title": "Causality-Inspired Robustness for Nonlinear Models via Representation Learning",
        "author": "Marin \u0160ola, Peter B\u00fchlmann, and Xinwei Shen",
        "link": "http://arxiv.org/abs/2505.12868v1",
        "abstract": "Distributional robustness is a central goal of prediction algorithms due to\nthe prevalent distribution shifts in real-world data. The prediction model aims\nto minimize the worst-case risk among a class of distributions, a.k.a., an\nuncertainty set. Causality provides a modeling framework with a rigorous\nrobustness guarantee in the above sense, where the uncertainty set is\ndata-driven rather than pre-specified as in traditional distributional\nrobustness optimization. However, current causality-inspired robustness methods\npossess finite-radius robustness guarantees only in the linear settings, where\nthe causal relationships among the covariates and the response are linear. In\nthis work, we propose a nonlinear method under a causal framework by\nincorporating recent developments in identifiable representation learning and\nestablish a distributional robustness guarantee. To our best knowledge, this is\nthe first causality-inspired robustness method with such a finite-radius\nrobustness guarantee in nonlinear settings. Empirical validation of the\ntheoretical findings is conducted on both synthetic data and real-world\nsingle-cell data, also illustrating that finite-radius robustness is crucial."
    },
    {
        "date": "2025-05",
        "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation",
        "author": "Jiaqi Tan, Xu Zheng, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.12861v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."
    },
    {
        "date": "2025-05",
        "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting",
        "author": "Yanhua Wen, Lu Ai, Gang Liu, Chuang Li, and Jianhao Wei",
        "link": "http://arxiv.org/abs/2505.12851v1",
        "abstract": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection",
        "author": "Aditya Taparia, Noel Ngu, Mario Leiva, Joshua Shay Kricheli, John Corcoran, Nathaniel D. Bastian, Gerardo Simari, Paulo Shakarian, and Ransalu Senanayake",
        "link": "http://arxiv.org/abs/2505.12715v1",
        "abstract": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."
    },
    {
        "date": "2025-05",
        "title": "Writing a Good Security Paper for ISSCC (2025)",
        "author": "Utsav Banerjee, Chiraag Juvekar, Yong Ki Lee, Leibo Liu, Sanu Mathew, Thomas Poeppelmann, Shreyas Sen, Takeshi Sugawara, Ingrid Verbauwhede, and Rabia Tugce Yazicigil",
        "link": "http://arxiv.org/abs/2505.12700v1",
        "abstract": "Security is increasingly more important in designing chips and systems based\non them, and the International Solid-State Circuits Conference (ISSCC), the\nleading conference for presenting advances in solid-state circuits and\nsemiconductor technology, is committed to hardware security by establishing the\nsecurity subcommittee since 2024. In the past two years, the authors of this\npaper reviewed submissions as members of the Security Subcommittee, a part of\nInternational Technical Program Committee (ITPC). This paper aims to encourage\nhigh-quality submissions to grow this field in the overall scope of the ISSCC."
    },
    {
        "date": "2025-05",
        "title": "Shielding Latent Face Representations From Privacy Attacks",
        "author": "Arjun Ramesh Kaushik, Bharat Chandra Yalavarthi, Arun Ross, Vishnu Boddeti, and Nalini Ratha",
        "link": "http://arxiv.org/abs/2505.12688v1",
        "abstract": "In today's data-driven analytics landscape, deep learning has become a\npowerful tool, with latent representations, known as embeddings, playing a\ncentral role in several applications. In the face analytics domain, such\nembeddings are commonly used for biometric recognition (e.g., face\nidentification). However, these embeddings, or templates, can inadvertently\nexpose sensitive attributes such as age, gender, and ethnicity. Leaking such\ninformation can compromise personal privacy and affect civil liberty and human\nrights. To address these concerns, we introduce a multi-layer protection\nframework for embeddings. It consists of a sequence of operations: (a)\nencrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing\nthem using irreversible feature manifold hashing. Unlike conventional\nencryption methods, FHE enables computations directly on encrypted data,\nallowing downstream analytics while maintaining strong privacy guarantees. To\nreduce the overhead of encrypted processing, we employ embedding compression.\nOur proposed method shields latent representations of sensitive data from\nleaking private attributes (such as age and gender) while retaining essential\nfunctional capabilities (such as face identification). Extensive experiments on\ntwo datasets using two face encoders demonstrate that our approach outperforms\nseveral state-of-the-art privacy protection methods."
    },
    {
        "date": "2025-05",
        "title": "RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations",
        "author": "Seungmin Kim, Sohee Park, Donghyun Kim, Jisu Lee, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2505.12686v1",
        "abstract": "With the advancement of AI-based speech synthesis technologies such as Deep\nVoice, there is an increasing risk of voice spoofing attacks, including voice\nphishing and fake news, through unauthorized use of others' voices. Existing\ndefenses that inject adversarial perturbations directly into audio signals have\nlimited effectiveness, as these perturbations can easily be neutralized by\nspeech enhancement methods. To overcome this limitation, we propose RoVo\n(Robust Voice), a novel proactive defense technique that injects adversarial\nperturbations into high-dimensional embedding vectors of audio signals,\nreconstructing them into protected speech. This approach effectively defends\nagainst speech synthesis attacks and also provides strong resistance to speech\nenhancement models, which represent a secondary attack threat.\n  In extensive experiments, RoVo increased the Defense Success Rate (DSR) by\nover 70% compared to unprotected speech, across four state-of-the-art speech\nsynthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial\nspeaker-verification API, effectively neutralizing speech synthesis attack.\nMoreover, RoVo's perturbations remained robust even under strong speech\nenhancement conditions, outperforming traditional methods. A user study\nconfirmed that RoVo preserves both naturalness and usability of protected\nspeech, highlighting its effectiveness in complex and evolving threat\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "RoFL: Robust Fingerprinting of Language Models",
        "author": "Yun-Yun Tsai, Chuan Guo, Junfeng Yang, and Laurens van der Maaten",
        "link": "http://arxiv.org/abs/2505.12682v1",
        "abstract": "AI developers are releasing large language models (LLMs) under a variety of\ndifferent licenses. Many of these licenses restrict the ways in which the\nmodels or their outputs may be used. This raises the question how license\nviolations may be recognized. In particular, how can we identify that an API or\nproduct uses (an adapted version of) a particular LLM? We present a new method\nthat enable model developers to perform such identification via fingerprints:\nstatistical patterns that are unique to the developer's model and robust to\ncommon alterations of that model. Our method permits model identification in a\nblack-box setting using a limited number of queries, enabling identification of\nmodels that can only be accessed via an API or product. The fingerprints are\nnon-invasive: our method does not require any changes to the model during\ntraining, hence by design, it does not impact model quality. Empirically, we\nfind our method provides a high degree of robustness to common changes in the\nmodel or inference settings. In our experiments, it substantially outperforms\nprior art, including invasive methods that explicitly train watermarks into the\nmodel."
    },
    {
        "date": "2025-05",
        "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning",
        "author": "Hana Satou, and Alan Mitkiy",
        "link": "http://arxiv.org/abs/2505.12681v1",
        "abstract": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."
    },
    {
        "date": "2025-05",
        "title": "Know Or Not: a library for evaluating out-of-knowledge base robustness",
        "author": "Jessica Foo, Pradyumna Shyama Prasad, and Shaun Khoo",
        "link": "http://arxiv.org/abs/2505.13545v1",
        "abstract": "While the capabilities of large language models (LLMs) have progressed\nsignificantly, their use in high-stakes applications have been limited due to\nrisks of hallucination. One key approach in reducing hallucination is\nretrieval-augmented generation (RAG), but even in such setups, LLMs may still\nhallucinate when presented with questions outside of the knowledge base. Such\nbehavior is unacceptable in high-stake applications where LLMs are expected to\nabstain from answering queries it does not have sufficient context on. In this\nwork, we present a novel methodology for systematically evaluating\nout-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not\nknow) in the RAG setting, without the need for manual annotation of gold\nstandard answers. We implement our methodology in knowornot, an open-source\nlibrary that enables users to develop their own customized evaluation data and\npipelines for OOKB robustness. knowornot comprises four main features. Firstly,\nit provides a unified, high-level API that streamlines the process of setting\nup and running robustness benchmarks. Secondly, its modular architecture\nemphasizes extensibility and flexibility, allowing users to easily integrate\ntheir own LLM clients and RAG settings. Thirdly, its rigorous data modeling\ndesign ensures experiment reproducibility, reliability and traceability.\nLastly, it implements a comprehensive suite of tools for users to customize\ntheir pipelines. We demonstrate the utility of knowornot by developing a\nchallenging benchmark, PolicyBench, which spans four Question-Answer (QA)\nchatbots on government policies, and analyze its OOKB robustness. The source\ncode of knowornot is available\nhttps://github.com/govtech-responsibleai/KnowOrNot."
    },
    {
        "date": "2025-05",
        "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency",
        "author": "Bo Yang, Hengwei Zhang, Jindong Wang, Yuchen Ren, Chenhao Lin, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2505.12644v1",
        "abstract": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."
    },
    {
        "date": "2025-05",
        "title": "Two out of Three (ToT): using self-consistency to make robust predictions",
        "author": "Jung Hoon Lee, and Sujith Vijayan",
        "link": "http://arxiv.org/abs/2505.12642v1",
        "abstract": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."
    },
    {
        "date": "2025-05",
        "title": "hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy",
        "author": "Musharraf Alruwaill, Saraju Mohanty, and Elias Kougianos",
        "link": "http://arxiv.org/abs/2505.12610v1",
        "abstract": "Concerns regarding privacy and data security in conventional healthcare\nprompted alternative technologies. In smart healthcare, blockchain technology\naddresses existing concerns with security, privacy, and electronic healthcare\ntransmission. Integration of Blockchain Technology with the Internet of Medical\nThings (IoMT) allows real-time monitoring of protected healthcare data.\nUtilizing edge devices with IoMT devices is very advantageous for addressing\nsecurity, computing, and storage challenges. Encryption using symmetric and\nasymmetric keys is used to conceal sensitive information from unauthorized\nparties. SHA256 is an algorithm for one-way hashing. It is used to verify that\nthe data has not been altered, since if it had, the hash value would have\nchanged. This article offers a blockchain-based smart healthcare system using\nIoMT devices for continuous patient monitoring. In addition, it employs edge\nresources in addition to IoMT devices to have extra computing power and storage\nto hash and encrypt incoming data before sending it to the blockchain.\nSymmetric key is utilized to keep the data private even in the blockchain,\nallowing the patient to safely communicate the data through smart contracts\nwhile preventing unauthorized physicians from seeing the data. Through the use\nof a verification node and blockchain, an asymmetric key is used for the\nsigning and validation of patient data in the healthcare provider system. In\naddition to other security measures, location-based authentication is\nrecommended to guarantee that data originates from the patient area. Through\nthe edge device, SHA256 is utilized to secure the data's integrity and a secret\nkey is used to maintain its secrecy. The hChain architecture improves the\ncomputing power of IoMT environments, the security of EHR sharing through smart\ncontracts, and the privacy and authentication procedures."
    },
    {
        "date": "2025-05",
        "title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection",
        "author": "Sanggeon Yun, Ryozo Masukawa, Hyunwoo Oh, Nathaniel D. Bastian, and Mohsen Imani",
        "link": "http://arxiv.org/abs/2505.12586v3",
        "abstract": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy. The code is available here:\nhttps://github.com/c0510gy/AFLS-AED."
    },
    {
        "date": "2025-05",
        "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization",
        "author": "En Yu, Jie Lu, Xiaoyu Yang, Guangquan Zhang, and Zhen Fang",
        "link": "http://arxiv.org/abs/2505.12585v1",
        "abstract": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."
    },
    {
        "date": "2025-05",
        "title": "A Survey of Attacks on Large Language Models",
        "author": "Wenrui Xu, and Keshab K. Parhi",
        "link": "http://arxiv.org/abs/2505.12567v1",
        "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats."
    },
    {
        "date": "2025-05",
        "title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks",
        "author": "Amirbek Djanibekov, Nurdaulet Mukhituly, Kentaro Inui, Hanan Aldarmaki, and Nils Lukas",
        "link": "http://arxiv.org/abs/2505.13541v1",
        "abstract": "Speech Language Models (SLMs) enable natural interactions via spoken\ninstructions, which more effectively capture user intent by detecting nuances\nin speech. The richer speech signal introduces new security risks compared to\ntext-based models, as adversaries can better bypass safety mechanisms by\ninjecting imperceptible noise to speech. We analyze adversarial attacks and\nfind that SLMs are substantially more vulnerable to jailbreak attacks, which\ncan achieve a perfect 100% attack success rate in some instances. To improve\nsecurity, we propose post-hoc patching defenses used to intervene during\ninference by modifying the SLM's activations that improve robustness up to 99%\nwith (i) negligible impact on utility and (ii) without any re-training. We\nconduct ablation studies to maximize the efficacy of our defenses and improve\nthe utility/security trade-off, validated with large-scale benchmarks unique to\nSLMs."
    },
    {
        "date": "2025-05",
        "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning",
        "author": "Zachary Roch, Chi Zhang, George Atia, and Yue Wang",
        "link": "http://arxiv.org/abs/2505.12462v1",
        "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems."
    },
    {
        "date": "2025-05",
        "title": "SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding",
        "author": "Peihua Mai, Youlong Ding, Ziyan Lyu, Minxin Du, and Yan Pang",
        "link": "http://arxiv.org/abs/2505.12453v1",
        "abstract": "Federated recommender system (FedRec) has emerged as a solution to protect\nuser data through collaborative training techniques. A typical FedRec involves\ntransmitting the full model and entire weight updates between edge devices and\nthe server, causing significant burdens to devices with limited bandwidth and\ncomputational power. While the sparsity of embedding updates provides\nopportunity for payload optimization, existing sparsity-aware federated\nprotocols generally sacrifice privacy for efficiency. A key challenge in\ndesigning a secure sparsity-aware efficient protocol is to protect the rated\nitem indices from the server. In this paper, we propose a lossless secure\nrecommender systems on sparse embedding updates (SecEmb). SecEmb reduces user\npayload while ensuring that the server learns no information about both rated\nitem indices and individual updates except the aggregated model. The protocol\nconsists of two correlated modules: (1) a privacy-preserving embedding\nretrieval module that allows users to download relevant embeddings from the\nserver, and (2) an update aggregation module that securely aggregates updates\nat the server. Empirical analysis demonstrates that SecEmb reduces both\ndownload and upload communication costs by up to 90x and decreases user-side\ncomputation time by up to 70x compared with secure FedRec protocols.\nAdditionally, it offers non-negligible utility advantages compared with lossy\nmessage compression methods."
    },
    {
        "date": "2025-05",
        "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
        "author": "Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, and Shing-Chi Cheung",
        "link": "http://arxiv.org/abs/2505.12442v2",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."
    },
    {
        "date": "2025-05",
        "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization",
        "author": "Lior Broide, and Roni Stern",
        "link": "http://arxiv.org/abs/2505.12424v1",
        "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites."
    },
    {
        "date": "2025-05",
        "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
        "author": "Gauri Kholkar, and Ratinder Ahuja",
        "link": "http://arxiv.org/abs/2505.12368v1",
        "abstract": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."
    },
    {
        "date": "2025-05",
        "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
        "author": "Qianyue Hu, Junyan Wu, Wei Lu, and Xiangyang Luo",
        "link": "http://arxiv.org/abs/2505.12332v2",
        "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning."
    },
    {
        "date": "2025-05",
        "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
        "author": "Albert Zhao, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2505.12327v1",
        "abstract": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."
    },
    {
        "date": "2025-05",
        "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces",
        "author": "Ruoqi Wang, Haitao Wang, Shaojie Guo, and Qiong Luo",
        "link": "http://arxiv.org/abs/2505.12317v1",
        "abstract": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."
    },
    {
        "date": "2025-05",
        "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
        "author": "Shengkang Gu, Jiahao Liu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Ning Gu, Li Shang, and Tun Lu",
        "link": "http://arxiv.org/abs/2505.13528v1",
        "abstract": "Recommender systems (RS) are increasingly vulnerable to shilling attacks,\nwhere adversaries inject fake user profiles to manipulate system outputs.\nTraditional attack strategies often rely on simplistic heuristics, require\naccess to internal RS data, and overlook the manipulation potential of textual\nreviews. In this work, we introduce Agent4SR, a novel framework that leverages\nLarge Language Model (LLM)-based agents to perform low-knowledge, high-impact\nshilling attacks through both rating and review generation. Agent4SR simulates\nrealistic user behavior by orchestrating adversarial interactions, selecting\nitems, assigning ratings, and crafting reviews, while maintaining behavioral\nplausibility. Our design includes targeted profile construction, hybrid memory\nretrieval, and a review attack strategy that propagates target item features\nacross unrelated reviews to amplify manipulation. Extensive experiments on\nmultiple datasets and RS architectures demonstrate that Agent4SR outperforms\nexisting low-knowledge baselines in both effectiveness and stealth. Our\nfindings reveal a new class of emergent threats posed by LLM-driven agents,\nunderscoring the urgent need for enhanced defenses in modern recommender\nsystems."
    },
    {
        "date": "2025-05",
        "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI",
        "author": "Karthik Gopinath, Annabel Sorby-Adams, Jonathan W. Ramirez, Dina Zemlyanker, Jennifer Guo, David Hunt, Christine L. Mac Donald, C. Dirk Keene, Timothy Coalson, Matthew F. Glasser, David Van Essen, Matthew S. Rosen, Oula Puonti, W. Taylor Kimberly, and Juan Eugenio Iglesias",
        "link": "http://arxiv.org/abs/2505.12228v1",
        "abstract": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"
    },
    {
        "date": "2025-05",
        "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning",
        "author": "Zhenghao Li, Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2505.12202v1",
        "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm."
    },
    {
        "date": "2025-05",
        "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
        "author": "Kui Jiang, Jing Cao, Zhaocheng Yu, Junjun Jiang, and Jingchun Zhou",
        "link": "http://arxiv.org/abs/2505.12199v1",
        "abstract": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."
    },
    {
        "date": "2025-05",
        "title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech",
        "author": "Riad Hossain, Muhammad Ashad Kabir, Arat Ibne Golam Mowla, Animesh Chandra Roy, and Ranjit Kumar Ghosh",
        "link": "http://arxiv.org/abs/2505.12192v1",
        "abstract": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX."
    },
    {
        "date": "2025-05",
        "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
        "author": "Wenquan Lu, Jiaqi Zhang, Hugues Van Assel, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2505.12191v1",
        "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."
    },
    {
        "date": "2025-05",
        "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective",
        "author": "Sen Fang, Weiyuan Ding, and Bowen Xu",
        "link": "http://arxiv.org/abs/2505.12185v1",
        "abstract": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."
    },
    {
        "date": "2025-05",
        "title": "FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models",
        "author": "Yue Deng, Asadullah Hill Galib, Xin Lan, Pang-Ning Tan, and Lifeng Luo",
        "link": "http://arxiv.org/abs/2505.12167v1",
        "abstract": "Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics."
    },
    {
        "date": "2025-05",
        "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
        "author": "Ranit Karmakar, and Simon F. N\u00f8rrelykke",
        "link": "http://arxiv.org/abs/2505.12155v1",
        "abstract": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."
    },
    {
        "date": "2025-05",
        "title": "T-Rex: Fitting a Robust Factor Model via Expectation-Maximization",
        "author": "Daniel Cederberg",
        "link": "http://arxiv.org/abs/2505.12117v1",
        "abstract": "Over the past decades, there has been a surge of interest in studying\nlow-dimensional structures within high-dimensional data. Statistical factor\nmodels $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a\npowerful framework for modeling such structures. However, traditional methods\nfor fitting statistical factor models, such as principal component analysis\n(PCA) or maximum likelihood estimation assuming the data is Gaussian, are\nhighly sensitive to heavy tails and outliers in the observed data. In this\npaper, we propose a novel expectation-maximization (EM) algorithm for robustly\nfitting statistical factor models. Our approach is based on Tyler's M-estimator\nof the scatter matrix for an elliptical distribution, and consists of solving\nTyler's maximum likelihood estimation problem while imposing a structural\nconstraint that enforces the low-rank plus diagonal covariance structure. We\npresent numerical experiments on both synthetic and real examples,\ndemonstrating the robustness of our method for direction-of-arrival estimation\nin nonuniform noise and subspace recovery."
    },
    {
        "date": "2025-05",
        "title": "Improving Fairness in LLMs Through Testing-Time Adversaries",
        "author": "Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, and Artur Jord\u00e3o",
        "link": "http://arxiv.org/abs/2505.12100v1",
        "abstract": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."
    },
    {
        "date": "2025-05",
        "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition",
        "author": "Shuai Yuan, Guowen Xu, Hongwei Li, Rui Zhang, Xinyuan Qian, Wenbo Jiang, Hangcheng Cao, and Qingchuan Zhao",
        "link": "http://arxiv.org/abs/2505.12045v1",
        "abstract": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."
    },
    {
        "date": "2025-05",
        "title": "FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients",
        "author": "Jianyi Zhang, Ziyin Zhou, Yilong Li, and Qichao Jin",
        "link": "http://arxiv.org/abs/2505.12019v1",
        "abstract": "Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case."
    },
    {
        "date": "2025-05",
        "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation",
        "author": "Zhiying Li, Guanggang Geng, Yeying Jin, Zhizhi Guo, Bruce Gu, Jidong Huo, Zhaoxin Fan, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2505.12009v1",
        "abstract": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."
    },
    {
        "date": "2025-05",
        "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
        "author": "Ahmed Lekssays, Utsav Shukla, Husrev Taha Sencar, and Md Rizwan Parvez",
        "link": "http://arxiv.org/abs/2505.11988v1",
        "abstract": "Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration",
        "author": "Chih-Ting Liao, Bin Ren, Guofeng Mei, and Xu Zheng",
        "link": "http://arxiv.org/abs/2505.11895v1",
        "abstract": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Facial Recognition Leveraging Generative Adversarial Networks",
        "author": "Zhongwen Li, Zongwei Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.11884v1",
        "abstract": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."
    },
    {
        "date": "2025-05",
        "title": "AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES Instruction Extension for IoT Security",
        "author": "Van Tinh Nguyen, Phuc Hung Pham, Vu Trung Duong Le, Hoai Luan Pham, Tuan Hai Vu, and Thi Diem Tran",
        "link": "http://arxiv.org/abs/2505.11880v1",
        "abstract": "The Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm essential for securing embedded systems and IoT platforms. However,\nexisting AES hardware accelerators often face limitations in performance,\nenergy efficiency, and flexibility. This paper presents AES-RV, a\nhardware-efficient RISC-V accelerator featuring low-latency AES instruction\nextensions optimized for real-time processing across all AES modes and key\nsizes. AES-RV integrates three key innovations: high-bandwidth internal buffers\nfor continuous data processing, a specialized AES unit with custom low-latency\ninstructions, and a pipelined system supported by a ping-pong memory transfer\nmechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to\n255.97 times speedup and up to 453.04 times higher energy efficiency compared\nto baseline and conventional CPU/GPU platforms. It also demonstrates superior\nthroughput and area efficiency against state-of-the-art AES accelerators,\nmaking it a strong candidate for secure and high-performance embedded systems."
    },
    {
        "date": "2025-05",
        "title": "On Membership Inference Attacks in Knowledge Distillation",
        "author": "Ziyao Cui, Minxing Zhang, and Jian Pei",
        "link": "http://arxiv.org/abs/2505.11837v1",
        "abstract": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."
    },
    {
        "date": "2025-05",
        "title": "Multilingual Collaborative Defense for Large Language Models",
        "author": "Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, and Kaiyu Huang",
        "link": "http://arxiv.org/abs/2505.11835v1",
        "abstract": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."
    },
    {
        "date": "2025-05",
        "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement",
        "author": "Ke Li, Di Wang, Xiaowei Wang, Zhihong Wu, Yiming Zhang, Yifeng Wang, and Quan Wang",
        "link": "http://arxiv.org/abs/2505.11822v1",
        "abstract": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."
    },
    {
        "date": "2025-05",
        "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings",
        "author": "Jiajun Qin, Yuan Pu, Zhuolun He, Seunggeun Kim, David Z. Pan, and Bei Yu",
        "link": "http://arxiv.org/abs/2505.11815v1",
        "abstract": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."
    },
    {
        "date": "2025-05",
        "title": "Are vision language models robust to uncertain inputs?",
        "author": "Xi Wang, and Eric Nalisnick",
        "link": "http://arxiv.org/abs/2505.11804v1",
        "abstract": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."
    },
    {
        "date": "2025-05",
        "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
        "author": "Jianing Wang, Siying Guo, Zheng Hua, Runhu Huang, Jinyu Hu, and Maoguo Gong",
        "link": "http://arxiv.org/abs/2505.11793v1",
        "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."
    },
    {
        "date": "2025-05",
        "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents",
        "author": "Xilong Wang, John Bloch, Zedian Shao, Yuepeng Hu, Shuyan Zhou, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.11717v1",
        "abstract": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."
    },
    {
        "date": "2025-05",
        "title": "Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming",
        "author": "Diksha Goel, Hussain Ahmad, Kristen Moore, and Mingyu Guo",
        "link": "http://arxiv.org/abs/2505.11710v1",
        "abstract": "Modern enterprise networks increasingly rely on Active Directory (AD) for\nidentity and access management. However, this centralization exposes a single\npoint of failure, allowing adversaries to compromise high-value assets.\nExisting AD defense approaches often assume static attacker behavior, but\nreal-world adversaries adapt dynamically, rendering such methods brittle. To\naddress this, we model attacker-defender interactions in AD as a Stackelberg\ngame between an adaptive attacker and a proactive defender. We propose a\nco-evolutionary defense framework that combines Graph Neural Network\nApproximated Dynamic Programming (GNNDP) to model attacker strategies, with\nEvolutionary Diversity Optimization (EDO) to generate resilient blocking\nstrategies. To ensure scalability, we introduce a Fixed-Parameter Tractable\n(FPT) graph reduction method that reduces complexity while preserving strategic\nstructure. Our framework jointly refines attacker and defender policies to\nimprove generalization and prevent premature convergence. Experiments on\nsynthetic AD graphs show near-optimal results (within 0.1 percent of optimality\non r500) and improved performance on larger graphs (r1000 and r2000),\ndemonstrating the framework's scalability and effectiveness."
    },
    {
        "date": "2025-05",
        "title": "Joint Graph Estimation and Signal Restoration for Robust Federated Learning",
        "author": "Tsutahiro Fukuhara, Junya Hara, Hiroshi Higashi, and Yuichi Tanaka",
        "link": "http://arxiv.org/abs/2505.11648v1",
        "abstract": "We propose a robust aggregation method for model parameters in federated\nlearning (FL) under noisy communications. FL is a distributed machine learning\nparadigm in which a central server aggregates local model parameters from\nmultiple clients. These parameters are often noisy and/or have missing values\nduring data collection, training, and communication between the clients and\nserver. This may cause a considerable drop in model accuracy. To address this\nissue, we learn a graph that represents pairwise relationships between model\nparameters of the clients during aggregation. We realize it with a joint\nproblem of graph learning and signal (i.e., model parameters) restoration. The\nproblem is formulated as a difference-of-convex (DC) optimization, which is\nefficiently solved via a proximal DC algorithm. Experimental results on MNIST\nand CIFAR-10 datasets show that the proposed method outperforms existing\napproaches by up to $2$--$5\\%$ in classification accuracy under biased data\ndistributions and noisy conditions."
    },
    {
        "date": "2025-05",
        "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning",
        "author": "Falong Fan, and Xi Li",
        "link": "http://arxiv.org/abs/2505.11642v1",
        "abstract": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience",
        "author": "Shuyi Chen, Shixiang Zhu, and Ramteen Sioshansi",
        "link": "http://arxiv.org/abs/2505.11627v1",
        "abstract": "Extreme weather events are placing growing strain on electric power systems,\nexposing the limitations of purely reactive responses and prompting the need\nfor proactive resilience planning. However, existing approaches often rely on\nsimplified uncertainty models and decouple proactive and reactive decisions,\noverlooking their critical interdependence. This paper proposes a novel\ntri-level optimization framework that integrates proactive infrastructure\ninvestment, adversarial modeling of spatio-temporal disruptions, and adaptive\nreactive response. We construct high-probability, distribution-free uncertainty\nsets using conformal prediction to capture complex and data-scarce outage\npatterns. To solve the resulting nested decision problem, we derive a bi-level\nreformulation via strong duality and develop a scalable Benders decomposition\nalgorithm. Experiments on both real and synthetic data demonstrate that our\napproach consistently outperforms conventional robust and two-stage methods,\nachieving lower worst-case losses and more efficient resource allocation,\nespecially under tight operational constraints and large-scale uncertainty."
    },
    {
        "date": "2025-05",
        "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks",
        "author": "Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, and Yang Zhang",
        "link": "http://arxiv.org/abs/2505.11586v1",
        "abstract": "Recent research highlights concerns about the trustworthiness of third-party\nPre-Trained Language Models (PTLMs) due to potential backdoor attacks. These\nbackdoored PTLMs, however, are effective only for specific pre-defined\ndownstream tasks. In reality, these PTLMs can be adapted to many other\nunrelated downstream tasks. Such adaptation may lead to unforeseen consequences\nin downstream model outputs, consequently raising user suspicion and\ncompromising attack stealthiness. We refer to this phenomenon as backdoor\ncomplications. In this paper, we undertake the first comprehensive\nquantification of backdoor complications. Through extensive experiments using 4\nprominent PTLMs and 16 text classification benchmark datasets, we demonstrate\nthe widespread presence of backdoor complications in downstream models\nfine-tuned from backdoored PTLMs. The output distribution of triggered samples\nsignificantly deviates from that of clean samples. Consequently, we propose a\nbackdoor complication reduction method leveraging multi-task learning to\nmitigate complications without prior knowledge of downstream tasks. The\nexperimental results demonstrate that our proposed method can effectively\nreduce complications while maintaining the efficacy and consistency of backdoor\nattacks. Our code is available at\nhttps://github.com/zhangrui4041/Backdoor_Complications."
    },
    {
        "date": "2025-05",
        "title": "ProxyPrompt: Securing System Prompts against Prompt Extraction Attacks",
        "author": "Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, and Mario Fritz",
        "link": "http://arxiv.org/abs/2505.11459v1",
        "abstract": "The integration of large language models (LLMs) into a wide range of\napplications has highlighted the critical role of well-crafted system prompts,\nwhich require extensive testing and domain expertise. These prompts enhance\ntask performance but may also encode sensitive information and filtering\ncriteria, posing security risks if exposed. Recent research shows that system\nprompts are vulnerable to extraction attacks, while existing defenses are\neither easily bypassed or require constant updates to address new threats. In\nthis work, we introduce ProxyPrompt, a novel defense mechanism that prevents\nprompt leakage by replacing the original prompt with a proxy. This proxy\nmaintains the original task's utility while obfuscating the extracted prompt,\nensuring attackers cannot reproduce the task or access sensitive information.\nComprehensive evaluations on 264 LLM and system prompt pairs show that\nProxyPrompt protects 94.70% of prompts from extraction attacks, outperforming\nthe next-best defense, which only achieves 42.80%."
    },
    {
        "date": "2025-05",
        "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks",
        "author": "Christoph Leiter, Yuki M. Asano, Margret Keuper, and Steffen Eger",
        "link": "http://arxiv.org/abs/2505.11314v1",
        "abstract": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts."
    },
    {
        "date": "2025-05",
        "title": "LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios",
        "author": "Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, and Jun Ma",
        "link": "http://arxiv.org/abs/2505.11247v1",
        "abstract": "Ensuring the safety and robustness of autonomous driving systems necessitates\na comprehensive evaluation in safety-critical scenarios. However, these\nsafety-critical scenarios are rare and difficult to collect from real-world\ndriving data, posing significant challenges to effectively assessing the\nperformance of autonomous vehicles. Typical existing methods often suffer from\nlimited controllability and lack user-friendliness, as extensive expert\nknowledge is essentially required. To address these challenges, we propose\nLD-Scene, a novel framework that integrates Large Language Models (LLMs) with\nLatent Diffusion Models (LDMs) for user-controllable adversarial scenario\ngeneration through natural language. Our approach comprises an LDM that\ncaptures realistic driving trajectory distributions and an LLM-based guidance\nmodule that translates user queries into adversarial loss functions,\nfacilitating the generation of scenarios aligned with user queries. The\nguidance module integrates an LLM-based Chain-of-Thought (CoT) code generator\nand an LLM-based code debugger, enhancing the controllability and robustness in\ngenerating guidance functions. Extensive experiments conducted on the nuScenes\ndataset demonstrate that LD-Scene achieves state-of-the-art performance in\ngenerating realistic, diverse, and effective adversarial scenarios.\nFurthermore, our framework provides fine-grained control over adversarial\nbehaviors, thereby facilitating more effective testing tailored to specific\ndriving scenarios."
    },
    {
        "date": "2025-05",
        "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol",
        "author": "Zihan Wang, Hongwei Li, Rui Zhang, Yu Liu, Wenbo Jiang, Wenshu Fan, Qingchuan Zhao, and Guowen Xu",
        "link": "http://arxiv.org/abs/2505.11154v1",
        "abstract": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem."
    },
    {
        "date": "2025-05",
        "title": "EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation",
        "author": "Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, and Chenyang Tu",
        "link": "http://arxiv.org/abs/2505.13506v1",
        "abstract": "Retrieval-Augmented Generation (RAG) compensates for the static knowledge\nlimitations of Large Language Models (LLMs) by integrating external knowledge,\nproducing responses with enhanced factual correctness and query-specific\ncontextualization. However, it also introduces new attack surfaces such as\ncorpus poisoning at the same time. Most of the existing defense methods rely on\nthe internal knowledge of the model, which conflicts with the design concept of\nRAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and\nbait-guided context diversity detection to identify malicious content by\nanalyzing the context diversity of candidate documents without relying on LLM\ninternal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art\nsecurity with plug-and-play deployment, simultaneously improving clean-scenario\nRAG performance while maintaining practical operational costs (relatively\n1.2$\\times$ latency, 48\\%-80\\% token reduction versus Vanilla RAG)."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection",
        "author": "Desong Zhang, Jia Hu, and Geyong Min",
        "link": "http://arxiv.org/abs/2505.11134v1",
        "abstract": "Spiking Neural Networks (SNNs) process information via discrete spikes,\nenabling them to operate at remarkably low energy levels. However, our\nexperimental observations reveal a striking vulnerability when SNNs are trained\nusing the mainstream method--direct encoding combined with backpropagation\nthrough time (BPTT): even a single backward pass on data drawn from a slightly\ndifferent distribution can lead to catastrophic network collapse. Our\ntheoretical analysis attributes this vulnerability to the repeated inputs\ninherent in direct encoding and the gradient accumulation characteristic of\nBPTT, which together produce an exceptional large Hessian spectral radius. To\naddress this challenge, we develop a hyperparameter-free method called Dominant\nEigencomponent Projection (DEP). By orthogonally projecting gradients to\nprecisely remove their dominant components, DEP effectively reduces the Hessian\nspectral radius, thereby preventing SNNs from settling into sharp minima.\nExtensive experiments demonstrate that DEP not only mitigates the vulnerability\nof SNNs to heterogeneous data poisoning, but also significantly enhances\noverall robustness compared to key baselines, providing strong support for\nsafer and more reliable SNN deployment."
    },
    {
        "date": "2025-05",
        "title": "GoLeash: Mitigating Golang Software Supply Chain Attacks with Runtime Policy Enforcement",
        "author": "Carmine Cesarano, Martin Monperrus, and Roberto Natella",
        "link": "http://arxiv.org/abs/2505.11016v1",
        "abstract": "Modern software supply chain attacks consist of introducing new, malicious\ncapabilities into trusted third-party software components, in order to\npropagate to a victim through a package dependency chain. These attacks are\nespecially concerning for the Go language ecosystem, which is extensively used\nin critical cloud infrastructures. We present GoLeash, a novel system that\napplies the principle of least privilege at the package-level granularity, by\nenforcing distinct security policies for each package in the supply chain. This\nfiner granularity enables GoLeash to detect malicious packages more precisely\nthan traditional sandboxing that handles security policies at process- or\ncontainer-level. Moreover, GoLeash remains effective under obfuscation, can\novercome the limitations of static analysis, and incurs acceptable runtime\noverhead."
    },
    {
        "date": "2025-05",
        "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?",
        "author": "An-Lan Wang, Jingqun Tang, Liao Lei, Hao Feng, Qi Liu, Xiang Fei, Jinghui Lu, Han Wang, Weiwei Liu, Hao Liu, Yuliang Liu, Xiang Bai, and Can Huang",
        "link": "http://arxiv.org/abs/2505.11015v1",
        "abstract": "The rapid advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced capabilities in Document Understanding. However,\nprevailing benchmarks like DocVQA and ChartQA predominantly comprise\n\\textit{scanned or digital} documents, inadequately reflecting the intricate\nchallenges posed by diverse real-world scenarios, such as variable illumination\nand physical distortions. This paper introduces WildDoc, the inaugural\nbenchmark designed specifically for assessing document understanding in natural\nenvironments. WildDoc incorporates a diverse set of manually captured document\nimages reflecting real-world conditions and leverages document sources from\nestablished benchmarks to facilitate comprehensive comparisons with digital or\nscanned documents. Further, to rigorously evaluate model robustness, each\ndocument is captured four times under different conditions. Evaluations of\nstate-of-the-art MLLMs on WildDoc expose substantial performance declines and\nunderscore the models' inadequate robustness compared to traditional\nbenchmarks, highlighting the unique challenges posed by real-world document\nunderstanding. Our project homepage is available at\nhttps://bytedance.github.io/WildDoc."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion",
        "author": "Zongye Zhang, Bohan Kong, Qingjie Liu, and Yunhong Wang",
        "link": "http://arxiv.org/abs/2505.11013v1",
        "abstract": "Generating 3D human motion from text descriptions remains challenging due to\nthe diverse and complex nature of human motion. While existing methods excel\nwithin the training distribution, they often struggle with out-of-distribution\nmotions, limiting their applicability in real-world scenarios. Existing\nVQVAE-based methods often fail to represent novel motions faithfully using\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\nMeanwhile, diffusion-based methods operating on continuous representations\noften lack fine-grained control over individual frames. To address these\nchallenges, we propose a robust motion generation framework MoMADiff, which\ncombines masked modeling with diffusion processes to generate motion using\nframe-level continuous representations. Our model supports flexible\nuser-provided keyframe specification, enabling precise control over both\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\ngeneralization capability on novel text-to-motion datasets with sparse\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\ntwo standard benchmarks show that our method consistently outperforms\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\nadherence."
    },
    {
        "date": "2025-05",
        "title": "RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization",
        "author": "Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, and Yun Ma",
        "link": "http://arxiv.org/abs/2505.10989v1",
        "abstract": "RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various\nRAG paradigms, including vanilla, planning-based, and iterative RAG, are built\nupon 2 cores: the retriever, which should robustly select relevant documents\nacross complex queries, and the generator, which should faithfully synthesize\nresponses. However, existing retrievers rely heavily on public knowledge and\nstruggle with queries of varying logical complexity and clue completeness,\nwhile generators frequently face fidelity problems. In this work, we introduce\nRAGSynth, a framework that includes a data construction modeling and a\ncorresponding synthetic data generation implementation, designed to optimize\nretriever robustness and generator fidelity. Additionally, we present\nSynthBench, a benchmark encompassing 8 domain-specific documents across 4\ndomains, featuring diverse query complexities, clue completeness, and\nfine-grained citation granularity. Leveraging RAGSynth, we generate a\nlarge-scale synthetic dataset, including single and multi-hop. Extensive\nexperiments demonstrate that the synthetic data significantly improves the\nrobustness of the retrievers and the fidelity of the generators. Additional\nevaluations confirm that RAGSynth can also generalize well across different\ndomains. By integrating the optimized retrievers into various RAG paradigms, we\nconsistently observe enhanced RAG system performance. We have open-sourced the\nimplementation on https://github.com/EachSheep/RAGSynth."
    },
    {
        "date": "2025-05",
        "title": "GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models",
        "author": "Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, and Yan Chen",
        "link": "http://arxiv.org/abs/2505.10983v1",
        "abstract": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,\nGenoArmory offers the first comprehensive evaluation framework to\nsystematically assess the vulnerability of GFMs to adversarial attacks.\nMethodologically, we evaluate the adversarial robustness of five\nstate-of-the-art GFMs using four widely adopted attack algorithms and three\ndefense strategies. Importantly, our benchmark provides an accessible and\ncomprehensive framework to analyze GFM vulnerabilities with respect to model\narchitecture, quantization schemes, and training datasets. Additionally, we\nintroduce GenoAdv, a new adversarial sample dataset designed to improve GFM\nsafety. Empirically, classification models exhibit greater robustness to\nadversarial perturbations compared to generative models, highlighting the\nimpact of task type on model vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Robust Spiking Neural Networks with Sparse Connectivity",
        "author": "Mathias Schmolli, Maximilian Baronig, Robert Legenstein, and Ozan \u00d6zdenizci",
        "link": "http://arxiv.org/abs/2505.15833v1",
        "abstract": "Deployment of deep neural networks in resource-constrained embedded systems\nrequires innovative algorithmic solutions to facilitate their energy and memory\nefficiency. To further ensure the reliability of these systems against\nmalicious actors, recent works have extensively studied adversarial robustness\nof existing architectures. Our work focuses on the intersection of adversarial\nrobustness, memory- and energy-efficiency in neural networks. We introduce a\nneural network conversion algorithm designed to produce sparse and\nadversarially robust spiking neural networks (SNNs) by leveraging the sparse\nconnectivity and weights from a robustly pretrained artificial neural network\n(ANN). Our approach combines the energy-efficient architecture of SNNs with a\nnovel conversion algorithm, leading to state-of-the-art performance with\nenhanced energy and memory efficiency through sparse connectivity and\nactivations. Our models are shown to achieve up to 100x reduction in the number\nof weights to be stored in memory, with an estimated 8.6x increase in energy\nefficiency compared to dense SNNs, while maintaining high performance and\nrobustness against adversarial threats."
    },
    {
        "date": "2025-05",
        "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
        "author": "Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, and Shuai Wang",
        "link": "http://arxiv.org/abs/2505.10924v1",
        "abstract": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents."
    },
    {
        "date": "2025-05",
        "title": "On the Security Risks of ML-based Malware Detection Systems: A Survey",
        "author": "Ping He, Yuhao Mao, Changjiang Li, Lorenzo Cavallaro, Ting Wang, and Shouling Ji",
        "link": "http://arxiv.org/abs/2505.10903v1",
        "abstract": "Malware presents a persistent threat to user privacy and data integrity. To\ncombat this, machine learning-based (ML-based) malware detection (MD) systems\nhave been developed. However, these systems have increasingly been attacked in\nrecent years, undermining their effectiveness in practice. While the security\nrisks associated with ML-based MD systems have garnered considerable attention,\nthe majority of prior works is limited to adversarial malware examples, lacking\na comprehensive analysis of practical security risks. This paper addresses this\ngap by utilizing the CIA principles to define the scope of security risks. We\nthen deconstruct ML-based MD systems into distinct operational stages, thus\ndeveloping a stage-based taxonomy. Utilizing this taxonomy, we summarize the\ntechnical progress and discuss the gaps in the attack and defense proposals\nrelated to the ML-based MD systems within each stage. Subsequently, we conduct\ntwo case studies, using both inter-stage and intra-stage analyses according to\nthe stage-based taxonomy to provide new empirical insights. Based on these\nanalyses and insights, we suggest potential future directions from both\ninter-stage and intra-stage perspectives."
    },
    {
        "date": "2025-05",
        "title": "Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators",
        "author": "Md Farhan Tasnim Oshim, Nigel Doering, Bashima Islam, Tsui-Wei Weng, and Tauhidur Rahman",
        "link": "http://arxiv.org/abs/2505.10864v1",
        "abstract": "Recent advancements in Ultra-Wideband (UWB) radar technology have enabled\ncontactless, non-line-of-sight vital sign monitoring, making it a valuable tool\nfor healthcare. However, UWB radar's ability to capture sensitive physiological\ndata, even through walls, raises significant privacy concerns, particularly in\nhuman-robot interactions and autonomous systems that rely on radar for sensing\nhuman presence and physiological functions. In this paper, we present\nAnti-Sensing, a novel defense mechanism designed to prevent unauthorized\nradar-based sensing. Our approach introduces physically realizable\nperturbations, such as oscillatory motion from wearable devices, to disrupt\nradar sensing by mimicking natural cardiac motion, thereby misleading heart\nrate (HR) estimations. We develop a gradient-based algorithm to optimize the\nfrequency and spatial amplitude of these oscillations for maximal disruption\nwhile ensuring physiological plausibility. Through both simulations and\nreal-world experiments with radar data and neural network-based HR sensing\nmodels, we demonstrate the effectiveness of Anti-Sensing in significantly\ndegrading model accuracy, offering a practical solution for privacy\npreservation."
    },
    {
        "date": "2025-05",
        "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs",
        "author": "Ran Li, Hao Wang, and Chengzhi Mao",
        "link": "http://arxiv.org/abs/2505.10838v1",
        "abstract": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization."
    },
    {
        "date": "2025-05",
        "title": "RAN Tester UE: An Automated Declarative UE Centric Security Testing Platform",
        "author": "Charles Marion Ueltschey, Joshua Moore, Aly Sabri Abdalla, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2505.10812v1",
        "abstract": "Cellular networks require strict security procedures and measures across\nvarious network components, from core to radio access network (RAN) and\nend-user devices. As networks become increasingly complex and interconnected,\nas in O-RAN deployments, they are exposed to a numerous security threats.\nTherefore, ensuring robust security is critical for O-RAN to protect network\nintegrity and safeguard user data. This requires rigorous testing methodologies\nto mitigate threats. This paper introduces an automated, adaptive, and scalable\nuser equipment (UE) based RAN security testing framework designed to address\nthe shortcomings of existing RAN testing solutions. Experimental results on a\n5G software radio testbed built with commercial off-the-shelf hardware and open\nsource software validate the efficiency and reproducibility of sample security\ntest procedures developed on the RAN Tester UE framework."
    },
    {
        "date": "2025-05",
        "title": "SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty",
        "author": "Zequn He, and Celia Reina",
        "link": "http://arxiv.org/abs/2505.13501v1",
        "abstract": "The data-driven discovery of long-time macroscopic dynamics and\nthermodynamics of dissipative systems with particle fidelity is hampered by\nsignificant obstacles. These include the strong time-scale limitations inherent\nto particle simulations, the non-uniqueness of the thermodynamic potentials and\noperators from given macroscopic dynamics, and the need for efficient\nuncertainty quantification. This paper introduces Statistical-Physics Informed\nEpistemic Diffusion Models (SPIEDiff), a machine learning framework designed to\novercome these limitations in the context of purely dissipative systems by\nleveraging statistical physics, conditional diffusion models, and epinets. We\nevaluate the proposed framework on stochastic Arrhenius particle processes and\ndemonstrate that SPIEDiff can accurately uncover both thermodynamics and\nkinetics, while enabling reliable long-time macroscopic predictions using only\nshort-time particle simulation data. SPIEDiff can deliver accurate predictions\nwith quantified uncertainty in minutes, drastically reducing the computational\ndemand compared to direct particle simulations, which would take days or years\nin the examples considered. Overall, SPIEDiff offers a robust and trustworthy\npathway for the data-driven discovery of thermodynamic models."
    },
    {
        "date": "2025-05",
        "title": "Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency",
        "author": "Kelvin Kan, Xingjian Li, Benjamin J. Zhang, Tuhin Sahai, Stanley Osher, and Markos A. Katsoulakis",
        "link": "http://arxiv.org/abs/2505.13499v1",
        "abstract": "We study Transformers through the perspective of optimal control theory,\nusing tools from continuous-time formulations to derive actionable insights\ninto training and architecture design. This framework improves the performance\nof existing Transformer models while providing desirable theoretical\nguarantees, including generalization and robustness. Our framework is designed\nto be plug-and-play, enabling seamless integration with established Transformer\nmodels and requiring only slight changes to the implementation. We conduct\nseven extensive experiments on tasks motivated by text generation, sentiment\nanalysis, image classification, and point cloud classification. Experimental\nresults show that the framework improves the test performance of the baselines,\nwhile being more parameter-efficient. On character-level text generation with\nnanoGPT, our framework achieves a 46% reduction in final test loss while using\n42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in\nfinal test loss, demonstrating scalability to larger models. To the best of our\nknowledge, this is the first work that applies optimal control theory to both\nthe training and architecture of Transformers. It offers a new foundation for\nsystematic, theory-driven improvements and moves beyond costly trial-and-error\napproaches."
    },
    {
        "date": "2025-05",
        "title": "Automating Security Audit Using Large Language Model based Agent: An Exploration Experiment",
        "author": "Jia Hui Chin, Pu Zhang, Yu Xin Cheong, and Jonathan Pan",
        "link": "http://arxiv.org/abs/2505.10732v1",
        "abstract": "In the current rapidly changing digital environment, businesses are under\nconstant stress to ensure that their systems are secured. Security audits help\nto maintain a strong security posture by ensuring that policies are in place,\ncontrols are implemented, gaps are identified for cybersecurity risks\nmitigation. However, audits are usually manual, requiring much time and costs.\nThis paper looks at the possibility of developing a framework to leverage Large\nLanguage Models (LLMs) as an autonomous agent to execute part of the security\naudit, namely with the field audit. password policy compliance for Windows\noperating system. Through the conduct of an exploration experiment of using\nGPT-4 with Langchain, the agent executed the audit tasks by accurately flagging\npassword policy violations and appeared to be more efficient than traditional\nmanual audits. Despite its potential limitations in operational consistency in\ncomplex and dynamic environment, the framework suggests possibilities to extend\nfurther to real-time threat monitoring and compliance checks."
    },
    {
        "date": "2025-05",
        "title": "ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation",
        "author": "Sayed Mehedi Azim, Brian Corbett, and Iman Dehzangi",
        "link": "http://arxiv.org/abs/2505.10687v1",
        "abstract": "The hippocampus, a critical brain structure involved in memory processing and\nvarious neurodegenerative and psychiatric disorders, comprises three key\nsubregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3\n(CA3). Accurate segmentation of these subregions from histological tissue\nimages is essential for advancing our understanding of disease mechanisms,\ndevelopmental dynamics, and therapeutic interventions. However, no existing\nmethods address the automated segmentation of hippocampal subregions from\ntissue images, particularly from immunohistochemistry (IHC) images. To bridge\nthis gap, we introduce a novel set of four comprehensive murine hippocampal IHC\ndatasets featuring distinct staining modalities: cFos, NeuN, and multiplexed\nstains combining cFos, NeuN, and either {\\Delta}FosB or GAD67, capturing\nstructural, neuronal activity, and plasticity associated information.\nAdditionally, we propose ROIsGAN, a region-guided U-Net-based generative\nadversarial network tailored for hippocampal subregion segmentation. By\nleveraging adversarial learning, ROIsGAN enhances boundary delineation and\nstructural detail refinement through a novel region-guided discriminator loss\ncombining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3\nsubregions, ROIsGAN consistently outperforms conventional segmentation models,\nachieving performance gains ranging from 1-10% in Dice score and up to 11% in\nIntersection over Union (IoU), particularly under challenging staining\nconditions. Our work establishes foundational datasets and methods for\nautomated hippocampal segmentation, enabling scalable, high-precision analysis\nof tissue images in neuroscience research. Our generated datasets, proposed\nmodel as a standalone tool, and its corresponding source code are publicly\navailable at: https://github.com/MehediAzim/ROIsGAN"
    },
    {
        "date": "2025-05",
        "title": "Agent Name Service (ANS): A Universal Directory for Secure AI Agent Discovery and Interoperability",
        "author": "Ken Huang, Vineeth Sai Narajala, Idan Habler, and Akram Sheriff",
        "link": "http://arxiv.org/abs/2505.10609v1",
        "abstract": "The proliferation of AI agents requires robust mechanisms for secure\ndiscovery. This paper introduces the Agent Name Service (ANS), a novel\narchitecture based on DNS addressing the lack of a public agent discovery\nframework. ANS provides a protocol-agnostic registry infrastructure that\nleverages Public Key Infrastructure (PKI) certificates for verifiable agent\nidentity and trust. The architecture features several key innovations: a\nformalized agent registration and renewal mechanism for lifecycle management;\nDNS-inspired naming conventions with capability-aware resolution; a modular\nProtocol Adapter Layer supporting diverse communication standards (A2A, MCP,\nACP etc.); and precisely defined algorithms for secure resolution. We implement\nstructured communication using JSON Schema and conduct a comprehensive threat\nanalysis of our proposal. The result is a foundational directory service\naddressing the core challenges of secured discovery and interaction in\nmulti-agent systems, paving the way for future interoperable, trustworthy, and\nscalable agent ecosystems."
    },
    {
        "date": "2025-05",
        "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit",
        "author": "Imranur Rahman, Yasemin Acar, Michel Cukier, William Enck, Christian Kastner, Alexandros Kapravelos, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2505.10538v1",
        "abstract": "While providing economic and software development value, software supply\nchains are only as strong as their weakest link. Over the past several years,\nthere has been an exponential increase in cyberattacks, specifically targeting\nvulnerable links in critical software supply chains. These attacks disrupt the\nday-to-day functioning and threaten the security of nearly everyone on the\ninternet, from billion-dollar companies and government agencies to hobbyist\nopen-source developers. The ever-evolving threat of software supply chain\nattacks has garnered interest from the software industry and the US government\nin improving software supply chain security.\n  On September 20, 2024, three researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 12 practitioners from 9 companies. The goals of the Summit\nwere to: (1) to enable sharing between individuals from different companies\nregarding practical experiences and challenges with software supply chain\nsecurity, (2) to help form new collaborations, (3) to share our observations\nfrom our previous summits with industry, and (4) to learn about practitioners'\nchallenges to inform our future research direction. The summit consisted of\ndiscussions of six topics relevant to the companies represented, including\nupdating vulnerable dependencies, component and container choice, malicious\ncommits, building infrastructure, large language models, and reducing entire\nclasses of vulnerabilities."
    },
    {
        "date": "2025-05",
        "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks",
        "author": "Iurii Medvedev, and Nuno Goncalves",
        "link": "http://arxiv.org/abs/2505.10497v1",
        "abstract": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods."
    },
    {
        "date": "2025-05",
        "title": "Superposition Yields Robust Neural Scaling",
        "author": "Yizhou Liu, Ziming Liu, and Jeff Gore",
        "link": "http://arxiv.org/abs/2505.10465v2",
        "abstract": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?",
        "author": "Pedro Orvalho, and Marta Kwiatkowska",
        "link": "http://arxiv.org/abs/2505.10443v1",
        "abstract": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding."
    },
    {
        "date": "2025-05",
        "title": "The Ephemeral Threat: Assessing the Security of Algorithmic Trading Systems powered by Deep Learning",
        "author": "Advije Rizvani, Giovanni Apruzzese, and Pavel Laskov",
        "link": "http://arxiv.org/abs/2505.10430v1",
        "abstract": "We study the security of stock price forecasting using Deep Learning (DL) in\ncomputational finance. Despite abundant prior research on the vulnerability of\nDL to adversarial perturbations, such work has hitherto hardly addressed\npractical adversarial threat models in the context of DL-powered algorithmic\ntrading systems (ATS). Specifically, we investigate the vulnerability of ATS to\nadversarial perturbations launched by a realistically constrained attacker. We\nfirst show that existing literature has paid limited attention to DL security\nin the financial domain, which is naturally attractive for adversaries. Then,\nwe formalize the concept of ephemeral perturbations (EP), which can be used to\nstage a novel type of attack tailored for DL-based ATS. Finally, we carry out\nan end-to-end evaluation of our EP against a profitable ATS. Our results reveal\nthat the introduction of small changes to the input stock prices not only (i)\ninduces the DL model to behave incorrectly but also (ii) leads the whole ATS to\nmake suboptimal buy/sell decisions, resulting in a worse financial performance\nof the targeted ATS."
    },
    {
        "date": "2025-05",
        "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs",
        "author": "Jorge Machado",
        "link": "http://arxiv.org/abs/2505.10603v1",
        "abstract": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development."
    },
    {
        "date": "2025-05",
        "title": "Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data",
        "author": "Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, and Sarwar Jahan",
        "link": "http://arxiv.org/abs/2505.10600v1",
        "abstract": "Due to the rapid growth in the number of Internet of Things (IoT) networks,\nthe cyber risk has increased exponentially, and therefore, we have to develop\neffective IDS that can work well with highly imbalanced datasets. A high rate\nof missed threats can be the result, as traditional machine learning models\ntend to struggle in identifying attacks when normal data volume is much higher\nthan the volume of attacks. For example, the dataset used in this study reveals\na strong class imbalance with 94,659 instances of the majority class and only\n28 instances of the minority class, making it quite challenging to determine\nrare attacks accurately. The challenges presented in this research are\naddressed by hybrid sampling techniques designed to improve data imbalance\ndetection accuracy in IoT domains. After applying these techniques, we evaluate\nthe performance of several machine learning models such as Random Forest, Soft\nVoting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer\nPerceptron (MLP), and Logistic Regression with respect to the classification of\ncyber-attacks. The obtained results indicate that the Random Forest model\nachieved the best performance with a Kappa score of 0.9903, test accuracy of\n0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting\nmodel, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of\ncombining model predictions. Overall, this work demonstrates the value of\nhybrid sampling combined with robust model and feature selection for\nsignificantly improving IoT security against cyber-attacks, especially in\nhighly imbalanced data environments."
    },
    {
        "date": "2025-05",
        "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning",
        "author": "Chibueze Peace Obioma, Youcheng Sun, and Mustafa A. Mustafa",
        "link": "http://arxiv.org/abs/2505.10297v1",
        "abstract": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments."
    },
    {
        "date": "2025-05",
        "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons",
        "author": "Hexu Li, Konstantinos Kalogiannis, Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.10273v1",
        "abstract": "Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats."
    },
    {
        "date": "2025-05",
        "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning",
        "author": "Francesco Diana, Andr\u00e9 Nusser, Chuan Xu, and Giovanni Neglia",
        "link": "http://arxiv.org/abs/2505.10264v1",
        "abstract": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art."
    },
    {
        "date": "2025-05",
        "title": "The Tangent Space Attack",
        "author": "Axel Lemoine",
        "link": "http://arxiv.org/abs/2505.10184v1",
        "abstract": "We propose a new method for retrieving the algebraic structure of a generic\nalternant code given an arbitrary generator matrix, provided certain conditions\nare met. We then discuss how this challenges the security of the McEliece\ncryptosystem instantiated with this family of codes. The central object of our\nwork is the quadratic hull related to a linear code, defined as the\nintersection of all quadrics passing through the columns of a given generator\nor parity-check matrix, where the columns are considered as points in the\naffine or projective space. The geometric properties of this object reveal\nimportant information about the internal algebraic structure of the code. This\nis particularly evident in the case of generalized Reed-Solomon codes, whose\nquadratic hull is deeply linked to a well-known algebraic variety called the\nrational normal curve. By utilizing the concept of Weil restriction of affine\nvarieties, we demonstrate that the quadratic hull of a generic dual alternant\ncode inherits many interesting features from the rational normal curve, on\naccount of the fact that alternant codes are subfield-subcodes of generalized\nReed-Solomon codes. If the rate of the generic alternant code is sufficiently\nhigh, this allows us to construct a polynomial-time algorithm for retrieving\nthe underlying generalized Reed-Solomon code from which the alternant code is\ndefined, which leads to an efficient key-recovery attack against the McEliece\ncryptosystem when instantiated with this class of codes. Finally, we discuss\nthe generalization of this approach to Algebraic-Geometry codes and Goppa\ncodes."
    },
    {
        "date": "2025-05",
        "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
        "author": "Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, and Fernando de la Torre",
        "link": "http://arxiv.org/abs/2505.10144v1",
        "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity",
        "author": "Huy Q. Le, Latif U. Khan, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2505.10128v1",
        "abstract": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance."
    },
    {
        "date": "2025-05",
        "title": "When Mitigations Backfire: Timing Channel Attacks and Defense for PRAC-Based RowHammer Mitigations",
        "author": "Jeonghyun Woo, Joyce Qu, Gururaj Saileshwar, and Prashant J. Nair",
        "link": "http://arxiv.org/abs/2505.10111v3",
        "abstract": "Per Row Activation Counting (PRAC) has emerged as a robust framework for\nmitigating RowHammer (RH) vulnerabilities in modern DRAM systems. However, we\nuncover a critical vulnerability: a timing channel introduced by the Alert\nBack-Off (ABO) protocol and Refresh Management (RFM) commands. We present\nPRACLeak, a novel attack that exploits these timing differences to leak\nsensitive information, such as secret keys from vulnerable AES implementations,\nby monitoring memory access latencies.\n  To counter this, we propose Timing-Safe PRAC (TPRAC), a defense that\neliminates PRAC-induced timing channels without compromising RH mitigation\nefficacy. TPRAC uses Timing-Based RFMs, issued periodically and independent of\nmemory activity. It requires only a single-entry in-DRAM mitigation queue per\nDRAM bank and is compatible with existing DRAM standards. Our evaluations\ndemonstrate that TPRAC closes timing channels while incurring only 3.4%\nperformance overhead at the RH threshold of 1024."
    },
    {
        "date": "2025-05",
        "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems",
        "author": "Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Ziyou Jiang, Yang Liu, and Qing Wang",
        "link": "http://arxiv.org/abs/2505.11548v2",
        "abstract": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. While previous studies have exposed knowledge\npoisoning risks in RAG systems, existing attack methods suffer from critical\nlimitations: they either require injecting multiple poisoned documents\n(resulting in poor stealthiness) or can only function effectively on simplistic\nqueries (limiting real-world applicability). This paper reveals a more\nrealistic knowledge poisoning attack against RAG systems that achieves\nsuccessful attacks by poisoning only a single document while remaining\neffective for complex multi-hop questions involving complex relationships\nbetween multiple elements. Our proposed AuthChain address three challenges to\nensure the poisoned documents are reliably retrieved and trusted by the LLM,\neven against large knowledge bases and LLM's own knowledge. Extensive\nexperiments across six popular LLMs demonstrate that AuthChain achieves\nsignificantly higher attack success rates while maintaining superior\nstealthiness against RAG defense mechanisms compared to state-of-the-art\nbaselines."
    }
]