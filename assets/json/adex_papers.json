[
    {
        "date": "2025-05",
        "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit",
        "author": "Imranur Rahman, Yasemin Acar, Michel Cukier, William Enck, Christian Kastner, Alexandros Kapravelos, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2505.10538v1",
        "abstract": "While providing economic and software development value, software supply\nchains are only as strong as their weakest link. Over the past several years,\nthere has been an exponential increase in cyberattacks, specifically targeting\nvulnerable links in critical software supply chains. These attacks disrupt the\nday-to-day functioning and threaten the security of nearly everyone on the\ninternet, from billion-dollar companies and government agencies to hobbyist\nopen-source developers. The ever-evolving threat of software supply chain\nattacks has garnered interest from the software industry and the US government\nin improving software supply chain security.\n  On September 20, 2024, three researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 12 practitioners from 9 companies. The goals of the Summit\nwere to: (1) to enable sharing between individuals from different companies\nregarding practical experiences and challenges with software supply chain\nsecurity, (2) to help form new collaborations, (3) to share our observations\nfrom our previous summits with industry, and (4) to learn about practitioners'\nchallenges to inform our future research direction. The summit consisted of\ndiscussions of six topics relevant to the companies represented, including\nupdating vulnerable dependencies, component and container choice, malicious\ncommits, building infrastructure, large language models, and reducing entire\nclasses of vulnerabilities."
    },
    {
        "date": "2025-05",
        "title": "MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks",
        "author": "Iurii Medvedev, and Nuno Goncalves",
        "link": "http://arxiv.org/abs/2505.10497v1",
        "abstract": "Face recognition has evolved significantly with the advancement of deep\nlearning techniques, enabling its widespread adoption in various applications\nrequiring secure authentication. However, this progress has also increased its\nexposure to presentation attacks, including face morphing, which poses a\nserious security threat by allowing one identity to impersonate another.\nTherefore, modern face recognition systems must be robust against such attacks.\n  In this work, we propose a novel approach for training deep networks for face\nrecognition with enhanced robustness to face morphing attacks. Our method\nmodifies the classification task by introducing a dual-branch classification\nstrategy that effectively handles the ambiguity in the labeling of face morphs.\nThis adaptation allows the model to incorporate morph images into the training\nprocess, improving its ability to distinguish them from bona fide samples.\n  Our strategy has been validated on public benchmarks, demonstrating its\neffectiveness in enhancing robustness against face morphing attacks.\nFurthermore, our approach is universally applicable and can be integrated into\nexisting face recognition training pipelines to improve classification-based\nrecognition methods."
    },
    {
        "date": "2025-05",
        "title": "Superposition Yields Robust Neural Scaling",
        "author": "Yizhou liu, Ziming Liu, and Jeff Gore",
        "link": "http://arxiv.org/abs/2505.10465v1",
        "abstract": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."
    },
    {
        "date": "2025-05",
        "title": "Are Large Language Models Robust in Understanding Code Against Semantics-Preserving Mutations?",
        "author": "Pedro Orvalho, and Marta Kwiatkowska",
        "link": "http://arxiv.org/abs/2505.10443v1",
        "abstract": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding."
    },
    {
        "date": "2025-05",
        "title": "The Ephemeral Threat: Assessing the Security of Algorithmic Trading Systems powered by Deep Learning",
        "author": "Advije Rizvani, Giovanni Apruzzese, and Pavel Laskov",
        "link": "http://arxiv.org/abs/2505.10430v1",
        "abstract": "We study the security of stock price forecasting using Deep Learning (DL) in\ncomputational finance. Despite abundant prior research on the vulnerability of\nDL to adversarial perturbations, such work has hitherto hardly addressed\npractical adversarial threat models in the context of DL-powered algorithmic\ntrading systems (ATS). Specifically, we investigate the vulnerability of ATS to\nadversarial perturbations launched by a realistically constrained attacker. We\nfirst show that existing literature has paid limited attention to DL security\nin the financial domain, which is naturally attractive for adversaries. Then,\nwe formalize the concept of ephemeral perturbations (EP), which can be used to\nstage a novel type of attack tailored for DL-based ATS. Finally, we carry out\nan end-to-end evaluation of our EP against a profitable ATS. Our results reveal\nthat the introduction of small changes to the input stock prices not only (i)\ninduces the DL model to behave incorrectly but also (ii) leads the whole ATS to\nmake suboptimal buy/sell decisions, resulting in a worse financial performance\nof the targeted ATS."
    },
    {
        "date": "2025-05",
        "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning",
        "author": "Chibueze Peace Obioma, Youcheng Sun, and Mustafa A. Mustafa",
        "link": "http://arxiv.org/abs/2505.10297v1",
        "abstract": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments."
    },
    {
        "date": "2025-05",
        "title": "AttentionGuard: Transformer-based Misbehavior Detection for Secure Vehicular Platoons",
        "author": "Hexu Li, Konstantinos Kalogiannis, Ahmed Mohamed Hussain, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.10273v1",
        "abstract": "Vehicle platooning, with vehicles traveling in close formation coordinated\nthrough Vehicle-to-Everything (V2X) communications, offers significant benefits\nin fuel efficiency and road utilization. However, it is vulnerable to\nsophisticated falsification attacks by authenticated insiders that can\ndestabilize the formation and potentially cause catastrophic collisions. This\npaper addresses this challenge: misbehavior detection in vehicle platooning\nsystems. We present AttentionGuard, a transformer-based framework for\nmisbehavior detection that leverages the self-attention mechanism to identify\nanomalous patterns in mobility data. Our proposal employs a multi-head\ntransformer-encoder to process sequential kinematic information, enabling\neffective differentiation between normal mobility patterns and falsification\nattacks across diverse platooning scenarios, including steady-state\n(no-maneuver) operation, join, and exit maneuvers. Our evaluation uses an\nextensive simulation dataset featuring various attack vectors (constant,\ngradual, and combined falsifications) and operational parameters (controller\ntypes, vehicle speeds, and attacker positions). Experimental results\ndemonstrate that AttentionGuard achieves up to 0.95 F1-score in attack\ndetection, with robust performance maintained during complex maneuvers.\nNotably, our system performs effectively with minimal latency (100ms decision\nintervals), making it suitable for real-time transportation safety\napplications. Comparative analysis reveals superior detection capabilities and\nestablishes the transformer-encoder as a promising approach for securing\nCooperative Intelligent Transport Systems (C-ITS) against sophisticated insider\nthreats."
    },
    {
        "date": "2025-05",
        "title": "Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning",
        "author": "Francesco Diana, Andr\u00e9 Nusser, Chuan Xu, and Giovanni Neglia",
        "link": "http://arxiv.org/abs/2505.10264v1",
        "abstract": "Federated Learning (FL) enables collaborative training of machine learning\nmodels across distributed clients without sharing raw data, ostensibly\npreserving data privacy. Nevertheless, recent studies have revealed critical\nvulnerabilities in FL, showing that a malicious central server can manipulate\nmodel updates to reconstruct clients' private training data. Existing data\nreconstruction attacks have important limitations: they often rely on\nassumptions about the clients' data distribution or their efficiency\nsignificantly degrades when batch sizes exceed just a few tens of samples.\n  In this work, we introduce a novel data reconstruction attack that overcomes\nthese limitations. Our method leverages a new geometric perspective on fully\nconnected layers to craft malicious model parameters, enabling the perfect\nrecovery of arbitrarily large data batches in classification tasks without any\nprior knowledge of clients' data. Through extensive experiments on both image\nand tabular datasets, we demonstrate that our attack outperforms existing\nmethods and achieves perfect reconstruction of data batches two orders of\nmagnitude larger than the state of the art."
    },
    {
        "date": "2025-05",
        "title": "The Tangent Space Attack",
        "author": "Axel Lemoine",
        "link": "http://arxiv.org/abs/2505.10184v1",
        "abstract": "We propose a new method for retrieving the algebraic structure of a generic\nalternant code given an arbitrary generator matrix, provided certain conditions\nare met. We then discuss how this challenges the security of the McEliece\ncryptosystem instantiated with this family of codes. The central object of our\nwork is the quadratic hull related to a linear code, defined as the\nintersection of all quadrics passing through the columns of a given generator\nor parity-check matrix, where the columns are considered as points in the\naffine or projective space. The geometric properties of this object reveal\nimportant information about the internal algebraic structure of the code. This\nis particularly evident in the case of generalized Reed-Solomon codes, whose\nquadratic hull is deeply linked to a well-known algebraic variety called the\nrational normal curve. By utilizing the concept of Weil restriction of affine\nvarieties, we demonstrate that the quadratic hull of a generic dual alternant\ncode inherits many interesting features from the rational normal curve, on\naccount of the fact that alternant codes are subfield-subcodes of generalized\nReed-Solomon codes. If the rate of the generic alternant code is sufficiently\nhigh, this allows us to construct a polynomial-time algorithm for retrieving\nthe underlying generalized Reed-Solomon code from which the alternant code is\ndefined, which leads to an efficient key-recovery attack against the McEliece\ncryptosystem when instantiated with this class of codes. Finally, we discuss\nthe generalization of this approach to Algebraic-Geometry codes and Goppa\ncodes."
    },
    {
        "date": "2025-05",
        "title": "VRSplat: Fast and Robust Gaussian Splatting for Virtual Reality",
        "author": "Xuechang Tu, Lukas Radl, Michael Steiner, Markus Steinberger, Bernhard Kerbl, and Fernando de la Torre",
        "link": "http://arxiv.org/abs/2505.10144v1",
        "abstract": "3D Gaussian Splatting (3DGS) has rapidly become a leading technique for\nnovel-view synthesis, providing exceptional performance through efficient\nsoftware-based GPU rasterization. Its versatility enables real-time\napplications, including on mobile and lower-powered devices. However, 3DGS\nfaces key challenges in virtual reality (VR): (1) temporal artifacts, such as\npopping during head movements, (2) projection-based distortions that result in\ndisturbing and view-inconsistent floaters, and (3) reduced framerates when\nrendering large numbers of Gaussians, falling below the critical threshold for\nVR. Compared to desktop environments, these issues are drastically amplified by\nlarge field-of-view, constant head movements, and high resolution of\nhead-mounted displays (HMDs). In this work, we introduce VRSplat: we combine\nand extend several recent advancements in 3DGS to address challenges of VR\nholistically. We show how the ideas of Mini-Splatting, StopThePop, and Optimal\nProjection can complement each other, by modifying the individual techniques\nand core 3DGS rasterizer. Additionally, we propose an efficient foveated\nrasterizer that handles focus and peripheral areas in a single GPU launch,\navoiding redundant computations and improving GPU utilization. Our method also\nincorporates a fine-tuning step that optimizes Gaussian parameters based on\nStopThePop depth evaluations and Optimal Projection. We validate our method\nthrough a controlled user study with 25 participants, showing a strong\npreference for VRSplat over other configurations of Mini-Splatting. VRSplat is\nthe first, systematically evaluated 3DGS approach capable of supporting modern\nVR applications, achieving 72+ FPS while eliminating popping and\nstereo-disrupting floaters."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning on Edge Devices with Domain Heterogeneity",
        "author": "Huy Q. Le, Latif U. Khan, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2505.10128v1",
        "abstract": "Federated Learning (FL) allows collaborative training while ensuring data\nprivacy across distributed edge devices, making it a popular solution for\nprivacy-sensitive applications. However, FL faces significant challenges due to\nstatistical heterogeneity, particularly domain heterogeneity, which impedes the\nglobal mode's convergence. In this study, we introduce a new framework to\naddress this challenge by improving the generalization ability of the FL global\nmodel under domain heterogeneity, using prototype augmentation. Specifically,\nwe introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a\nprototype-based FL framework designed to enhance feature diversity and model\nrobustness. FedAPC leverages prototypes derived from the mean features of\naugmented data to capture richer representations. By aligning local features\nwith global prototypes, we enable the model to learn meaningful semantic\nfeatures while reducing overfitting to any specific domain. Experimental\nresults on the Office-10 and Digits datasets illustrate that our framework\noutperforms SOTA baselines, demonstrating superior performance."
    },
    {
        "date": "2025-05",
        "title": "When Mitigations Backfire: Timing Channel Attacks and Defense for PRAC-Based RowHammer Mitigations",
        "author": "Jeonghyun Woo, Joyce Qu, Gururaj Saileshwar, and Prashant J. Nair",
        "link": "http://arxiv.org/abs/2505.10111v1",
        "abstract": "Per Row Activation Counting (PRAC) has emerged as a robust framework for\nmitigating RowHammer (RH) vulnerabilities in modern DRAM systems. However, we\nuncover a critical vulnerability: a timing channel introduced by the Alert\nBack-Off (ABO) protocol and Refresh Management (RFM) commands. We present\nPRACLeak, a novel attack that exploits these timing differences to leak\nsensitive information, such as secret keys from vulnerable AES implementations,\nby monitoring memory access latencies.\n  To counter this, we propose Timing-Safe PRAC (TPRAC), a defense that\neliminates PRAC-induced timing channels without compromising RH mitigation\nefficacy. TPRAC uses Timing-Based RFMs, issued periodically and independent of\nmemory activity. It requires only a single-entry in-DRAM mitigation queue per\nDRAM bank and is compatible with existing DRAM standards. Our evaluations\ndemonstrate that TPRAC closes timing channels while incurring only 3.4%\nperformance overhead at the RH threshold of 1024."
    },
    {
        "date": "2025-05",
        "title": "Evaluating Robustness of Deep Reinforcement Learning for Autonomous Surface Vehicle Control in Field Tests",
        "author": "Luis F. W. Batista, St\u00e9phanie Aravecchia, Seth Hutchinson, and C\u00e9dric Pradalier",
        "link": "http://arxiv.org/abs/2505.10033v1",
        "abstract": "Despite significant advancements in Deep Reinforcement Learning (DRL) for\nAutonomous Surface Vehicles (ASVs), their robustness in real-world conditions,\nparticularly under external disturbances, remains insufficiently explored. In\nthis paper, we evaluate the resilience of a DRL-based agent designed to capture\nfloating waste under various perturbations. We train the agent using domain\nrandomization and evaluate its performance in real-world field tests, assessing\nits ability to handle unexpected disturbances such as asymmetric drag and an\noff-center payload. We assess the agent's performance under these perturbations\nin both simulation and real-world experiments, quantifying performance\ndegradation and benchmarking it against an MPC baseline. Results indicate that\nthe DRL agent performs reliably despite significant disturbances. Along with\nthe open-source release of our implementation, we provide insights into\neffective training strategies, real-world challenges, and practical\nconsiderations for deploying DRLbased ASV controllers."
    },
    {
        "date": "2025-05",
        "title": "DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera",
        "author": "Miit Daga, Dhriti Parikh, and Swarna Priya Ramu",
        "link": "http://arxiv.org/abs/2505.10030v1",
        "abstract": "Coconut tree diseases are a serious risk to agricultural yield, particularly\nin developing countries where conventional farming practices restrict early\ndiagnosis and intervention. Current disease identification methods are manual,\nlabor-intensive, and non-scalable. In response to these limitations, we come up\nwith DeepSeqCoco, a deep learning based model for accurate and automatic\ndisease identification from coconut tree images. The model was tested under\nvarious optimizer settings, such as SGD, Adam, and hybrid configurations, to\nidentify the optimal balance between accuracy, minimization of loss, and\ncomputational cost. Results from experiments indicate that DeepSeqCoco can\nachieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than\nexisting models) with the hybrid SGD-Adam showing the lowest validation loss of\n2.81%. It also shows a drop of up to 18% in training time and up to 85% in\nprediction time for input images. The results point out the promise of the\nmodel to improve precision agriculture through an AI-based, scalable, and\nefficient disease monitoring system."
    },
    {
        "date": "2025-05",
        "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning",
        "author": "Zijun Chen, Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2505.10007v1",
        "abstract": "Motivated by practical applications where stable long-term performance is\ncritical-such as robotics, operations research, and healthcare-we study the\nproblem of distributionally robust (DR) average-reward reinforcement learning.\nWe propose two algorithms that achieve near-optimal sample complexity. The\nfirst reduces the problem to a DR discounted Markov decision process (MDP),\nwhile the second, Anchored DR Average-Reward MDP, introduces an anchoring state\nto stabilize the controlled transition kernels within the uncertainty set.\nAssuming the nominal MDP is uniformly ergodic, we prove that both algorithms\nattain a sample complexity of $\\widetilde{O}\\left(|\\mathbf{S}||\\mathbf{A}|\nt_{\\mathrm{mix}}^2\\varepsilon^{-2}\\right)$ for estimating the optimal policy as\nwell as the robust average reward under KL and $f_k$-divergence-based\nuncertainty sets, provided the uncertainty radius is sufficiently small. Here,\n$\\varepsilon$ is the target accuracy, $|\\mathbf{S}|$ and $|\\mathbf{A}|$ denote\nthe sizes of the state and action spaces, and $t_{\\mathrm{mix}}$ is the mixing\ntime of the nominal MDP. This represents the first finite-sample convergence\nguarantee for DR average-reward reinforcement learning. We further validate the\nconvergence rates of our algorithms through numerical experiments."
    },
    {
        "date": "2025-05",
        "title": "Sybil-based Virtual Data Poisoning Attacks in Federated Learning",
        "author": "Changxun Zhu, Qilong Wu, Lingjuan Lyu, and Shibei Xue",
        "link": "http://arxiv.org/abs/2505.09983v1",
        "abstract": "Federated learning is vulnerable to poisoning attacks by malicious\nadversaries. Existing methods often involve high costs to achieve effective\nattacks. To address this challenge, we propose a sybil-based virtual data\npoisoning attack, where a malicious client generates sybil nodes to amplify the\npoisoning model's impact. To reduce neural network computational complexity, we\ndevelop a virtual data generation method based on gradient matching. We also\ndesign three schemes for target model acquisition, applicable to online local,\nonline global, and offline scenarios. In simulation, our method outperforms\nother attack algorithms since our method can obtain a global target model under\nnon-independent uniformly distributed data."
    },
    {
        "date": "2025-05",
        "title": "Analysing Safety Risks in LLMs Fine-Tuned with Pseudo-Malicious Cyber Security Data",
        "author": "Adel ElZemity, Budi Arief, and Shujun Li",
        "link": "http://arxiv.org/abs/2505.09974v1",
        "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. We present a systematic evaluation of safety risks in fine-tuned\nLLMs for cyber security applications. Using the OWASP Top 10 for LLM\nApplications framework, we assessed seven open-source LLMs: Phi 3 Mini 3.8B,\nMistral 7B, Qwen 2.5 7B, Llama 3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B.\nOur evaluation shows that fine-tuning reduces safety resilience across all\ntested LLMs (e.g., the safety score of Llama 3.1 8B against prompt injection\ndrops from 0.95 to 0.15). We propose and evaluate a safety alignment approach\nthat carefully rewords instruction-response pairs to include explicit safety\nprecautions and ethical considerations. This approach demonstrates that it is\npossible to maintain or even improve model safety while preserving technical\nutility, offering a practical path forward for developing safer fine-tuning\nmethodologies. This work offers a systematic evaluation for safety risks in\nLLMs, enabling safer adoption of generative AI in sensitive domains, and\ncontributing towards the development of secure, trustworthy, and ethically\naligned LLMs."
    },
    {
        "date": "2025-05",
        "title": "Security and Privacy Measurement on Chinese Consumer IoT Traffic based on Device Lifecycle",
        "author": "Chenghua Jin, Yan Jia, Yuxin Song, Qingyin Tan, Rui Yang, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.09929v1",
        "abstract": "In recent years, consumer Internet of Things (IoT) devices have become widely\nused in daily life. With the popularity of devices, related security and\nprivacy risks arise at the same time as they collect user-related data and\ntransmit it to various service providers. Although China accounts for a larger\nshare of the consumer IoT industry, current analyses on consumer IoT device\ntraffic primarily focus on regions such as Europe, the United States, and\nAustralia. Research on China, however, is currently rather rare. This study\nconstructs the first large-scale dataset about consumer IoT device traffic in\nChina. Specifically, we propose a fine-grained traffic collection guidance\ncovering the entire lifecycle of consumer IoT devices, gathering traffic from\n70 devices spanning 36 brands and 8 device categories. Based on this dataset,\nwe analyze traffic destinations and encryption practices across different\ndevice types during the entire lifecycle and compare the findings with the\nresults of other regions. Compared to other regions, our results show that\nconsumer IoT devices in China rely more on domestic services and overally\nperform better in terms of encryption practices. However, there are still 20/35\ndevices improperly conduct certificate validation, and 5/70 devices use\ninsecure encryption protocols. To facilitate future research, we open-source\nour traffic collection guidance and make our dataset publicly available."
    },
    {
        "date": "2025-05",
        "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for Connected Autonomous Vehicles",
        "author": "Xingchen Sun, Runhua Xu, Wei Ni, Li Duan, and Chao Li",
        "link": "http://arxiv.org/abs/2505.09928v1",
        "abstract": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
    },
    {
        "date": "2025-05",
        "title": "PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization",
        "author": "Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, and Binxing Fang",
        "link": "http://arxiv.org/abs/2505.09921v1",
        "abstract": "Large Language Models (LLMs) excel in various domains but pose inherent\nprivacy risks. Existing methods to evaluate privacy leakage in LLMs often use\nmemorized prefixes or simple instructions to extract data, both of which\nwell-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM\nsafety mechanisms to generate harmful content, but their role in privacy\nscenarios remains underexplored. In this paper, we examine the effectiveness of\njailbreak attacks in extracting sensitive information, bridging privacy leakage\nand jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework\ntargeting Personally Identifiable Information (PII) and addressing the\nlimitations of current jailbreak methods. Specifically, PIG identifies PII\nentities and their types in privacy queries, uses in-context learning to build\na privacy context, and iteratively updates it with three gradient-based\nstrategies to elicit target PII. We evaluate PIG and existing jailbreak methods\nusing two privacy-related datasets. Experiments on four white-box and two\nblack-box LLMs show that PIG outperforms baseline methods and achieves\nstate-of-the-art (SoTA) results. The results underscore significant privacy\nrisks in LLMs, emphasizing the need for stronger safeguards. Our code is\navailble at\n\\href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attack on Large Language Models using Exponentiated Gradient Descent",
        "author": "Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, and Xiuwen Liu",
        "link": "http://arxiv.org/abs/2505.09820v1",
        "abstract": "As Large Language Models (LLMs) are widely used, understanding them\nsystematically is key to improving their safety and realizing their full\npotential. Although many models are aligned using techniques such as\nreinforcement learning from human feedback (RLHF), they are still vulnerable to\njailbreaking attacks. Some of the existing adversarial attack methods search\nfor discrete tokens that may jailbreak a target model while others try to\noptimize the continuous space represented by the tokens of the model's\nvocabulary. While techniques based on the discrete space may prove to be\ninefficient, optimization of continuous token embeddings requires projections\nto produce discrete tokens, which might render them ineffective. To fully\nutilize the constraints and the structures of the space, we develop an\nintrinsic optimization technique using exponentiated gradient descent with the\nBregman projection method to ensure that the optimized one-hot encoding always\nstays within the probability simplex. We prove the convergence of the technique\nand implement an efficient algorithm that is effective in jailbreaking several\nwidely used LLMs. We demonstrate the efficacy of the proposed technique using\nfive open-source LLMs on four openly available datasets. The results show that\nthe technique achieves a higher success rate with great efficiency compared to\nthree other state-of-the-art jailbreaking techniques. The source code for our\nimplementation is available at:\nhttps://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack"
    },
    {
        "date": "2025-05",
        "title": "Self-Consuming Generative Models with Adversarially Curated Data",
        "author": "Xiukun Wei, and Xueru Zhang",
        "link": "http://arxiv.org/abs/2505.09768v1",
        "abstract": "Recent advances in generative models have made it increasingly difficult to\ndistinguish real data from model-generated synthetic data. Using synthetic data\nfor successive training of future model generations creates \"self-consuming\nloops\", which may lead to model collapse or training instability. Furthermore,\nsynthetic data is often subject to human feedback and curated by users based on\ntheir preferences. Ferbach et al. (2024) recently showed that when data is\ncurated according to user preferences, the self-consuming retraining loop\ndrives the model to converge toward a distribution that optimizes those\npreferences. However, in practice, data curation is often noisy or\nadversarially manipulated. For example, competing platforms may recruit\nmalicious users to adversarially curate data and disrupt rival models. In this\npaper, we study how generative models evolve under self-consuming retraining\nloops with noisy and adversarially curated data. We theoretically analyze the\nimpact of such noisy data curation on generative models and identify conditions\nfor the robustness of the retraining process. Building on this analysis, we\ndesign attack algorithms for competitive adversarial scenarios, where a\nplatform with a limited budget employs malicious users to misalign a rival's\nmodel from actual user preferences. Experiments on both synthetic and\nreal-world datasets demonstrate the effectiveness of the proposed algorithms."
    },
    {
        "date": "2025-05",
        "title": "Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data",
        "author": "Alpaslan Gokcen, and Ali Boyaci",
        "link": "http://arxiv.org/abs/2505.09733v1",
        "abstract": "Federated learning (FL) presents an effective solution for collaborative\nmodel training while maintaining data privacy across decentralized client\ndatasets. However, data quality issues such as noisy labels, missing classes,\nand imbalanced distributions significantly challenge its effectiveness. This\nstudy proposes a federated learning methodology that systematically addresses\ndata quality issues, including noise, class imbalance, and missing labels. The\nproposed approach systematically enhances data integrity through adaptive noise\ncleaning, collaborative conditional GAN-based synthetic data generation, and\nrobust federated model training. Experimental evaluations conducted on\nbenchmark datasets (MNIST and Fashion-MNIST) demonstrate significant\nimprovements in federated model performance, particularly macro-F1 Score, under\nvarying noise and class imbalance conditions. Additionally, the proposed\nframework carefully balances computational feasibility and substantial\nperformance gains, ensuring practicality for resource constrained edge devices\nwhile rigorously maintaining data privacy. Our results indicate that this\nmethod effectively mitigates common data quality challenges, providing a\nrobust, scalable, and privacy compliant solution suitable for diverse\nreal-world federated learning scenarios."
    },
    {
        "date": "2025-05",
        "title": "Forests for Differences: Robust Causal Inference Beyond Parametric DiD",
        "author": "Hugo Gobato Souto, and Francisco Louzada Neto",
        "link": "http://arxiv.org/abs/2505.09706v1",
        "abstract": "This paper introduces the Difference-in-Differences Bayesian Causal Forest\n(DiD-BCF), a novel non-parametric model addressing key challenges in DiD\nestimation, such as staggered adoption and heterogeneous treatment effects.\nDiD-BCF provides a unified framework for estimating Average (ATE),\nGroup-Average (GATE), and Conditional Average Treatment Effects (CATE). A core\ninnovation, its Parallel Trends Assumption (PTA)-based reparameterization,\nenhances estimation accuracy and stability in complex panel data settings.\nExtensive simulations demonstrate DiD-BCF's superior performance over\nestablished benchmarks, particularly under non-linearity, selection biases, and\neffect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers\nsignificant conditional treatment effect heterogeneity related to county\npopulation, insights obscured by traditional methods. DiD-BCF offers a robust\nand versatile tool for more nuanced causal inference in modern DiD\napplications."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
        "author": "David Khachaturov, and Robert Mullins",
        "link": "http://arxiv.org/abs/2505.09602v1",
        "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios."
    },
    {
        "date": "2025-05",
        "title": "\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs",
        "author": "Maris F. L. Galesloot, Roman Andriushchenko, Milan \u010ce\u0161ka, Sebastian Junges, and Nils Jansen",
        "link": "http://arxiv.org/abs/2505.09518v1",
        "abstract": "Partially observable Markov decision processes (POMDPs) model specific\nenvironments in sequential decision-making under uncertainty. Critically,\noptimal policies for POMDPs may not be robust against perturbations in the\nenvironment. Hidden-model POMDPs (HM-POMDPs) capture sets of different\nenvironment models, that is, POMDPs with a shared action and observation space.\nThe intuition is that the true model is hidden among a set of potential models,\nand it is unknown which model will be the environment at execution time. A\npolicy is robust for a given HM-POMDP if it achieves sufficient performance for\neach of its POMDPs. We compute such robust policies by combining two orthogonal\ntechniques: (1) a deductive formal verification technique that supports\ntractable robust policy evaluation by computing a worst-case POMDP within the\nHM-POMDP and (2) subgradient ascent to optimize the candidate policy for a\nworst-case POMDP. The empirical evaluation shows that, compared to various\nbaselines, our approach (1) produces policies that are more robust and\ngeneralize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of\nover a hundred thousand environments."
    },
    {
        "date": "2025-05",
        "title": "Layered Unlearning for Adversarial Relearning",
        "author": "Timothy Qian, Vinith Suriyakumar, Ashia Wilson, and Dylan Hadfield-Menell",
        "link": "http://arxiv.org/abs/2505.09500v1",
        "abstract": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates."
    },
    {
        "date": "2025-05",
        "title": "Independent Component Analysis by Robust Distance Correlation",
        "author": "Sarah Leyder, Jakob Raymaekers, Peter J. Rousseeuw, Tom Van Deuren, and Tim Verdonck",
        "link": "http://arxiv.org/abs/2505.09425v1",
        "abstract": "Independent component analysis (ICA) is a powerful tool for decomposing a\nmultivariate signal or distribution into fully independent sources, not just\nuncorrelated ones. Unfortunately, most approaches to ICA are not robust against\noutliers. Here we propose a robust ICA method called RICA, which estimates the\ncomponents by minimizing a robust measure of dependence between multivariate\nrandom variables. The dependence measure used is the distance correlation\n(dCor). In order to make it more robust we first apply a new transformation\ncalled the bowl transform, which is bounded, one-to-one, continuous, and maps\nfar outliers to points close to the origin. This preserves the crucial property\nthat a zero dCor implies independence. RICA estimates the independent sources\nsequentially, by looking for the component that has the smallest dCor with the\nremainder. RICA is strongly consistent and has the usual parametric rate of\nconvergence. Its robustness is investigated by a simulation study, in which it\ngenerally outperforms its competitors. The method is illustrated on three\napplications, including the well-known cocktail party problem."
    },
    {
        "date": "2025-05",
        "title": "MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection",
        "author": "Xiangyuan Peng, Yu Wang, Miao Tang, Bierzynski Kay, Lorenzo Servadei, and Robert Wille",
        "link": "http://arxiv.org/abs/2505.09422v1",
        "abstract": "Reliable autonomous driving systems require accurate detection of traffic\nparticipants. To this end, multi-modal fusion has emerged as an effective\nstrategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame\nradar point clouds have demonstrated the effectiveness in bridging the point\ndensity gap. However, they often neglect radar point clouds' inter-frame\nmisalignment caused by object movement during accumulation and do not fully\nexploit the object dynamic information from 4D radar. In this paper, we propose\nMoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for\nrobust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is\ndesigned to compensate for inter-frame radar misalignment from moving objects.\nLater, a Motion Attention Gated Fusion (MAGF) module integrate radar motion\nfeatures to guide LiDAR features to focus on dynamic foreground objects.\nExtensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL\noutperforms existing methods, achieving the highest mAP of 73.30% in the entire\narea and 88.68% in the driving corridor. Notably, our method also achieves the\nbest AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in\nthe driving corridor."
    },
    {
        "date": "2025-05",
        "title": "FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization",
        "author": "Xiaoyang Yu, Xiaoming Wu, Xin Wang, Dongrun Li, Ming Yang, and Peng Cheng",
        "link": "http://arxiv.org/abs/2505.09385v1",
        "abstract": "Federated semantic segmentation enables pixel-level classification in images\nthrough collaborative learning while maintaining data privacy. However,\nexisting research commonly overlooks the fine-grained class relationships\nwithin the semantic space when addressing heterogeneous problems, particularly\ndomain shift. This oversight results in ambiguities between class\nrepresentation. To overcome this challenge, we propose a novel federated\nsegmentation framework that strikes class consistency, termed FedSaaS.\nSpecifically, we introduce class exemplars as a criterion for both local- and\nglobal-level class representations. On the server side, the uploaded class\nexemplars are leveraged to model class prototypes, which supervise global\nbranch of clients, ensuring alignment with global-level representation. On the\nclient side, we incorporate an adversarial mechanism to harmonize contributions\nof global and local branches, leading to consistent output. Moreover,\nmultilevel contrastive losses are employed on both sides to enforce consistency\nbetween two-level representations in the same semantic space. Extensive\nexperiments on several driving scene segmentation datasets demonstrate that our\nframework outperforms state-of-the-art methods, significantly improving average\nsegmentation accuracy and effectively addressing the class-consistency\nrepresentation problem."
    },
    {
        "date": "2025-05",
        "title": "DNS Query Forgery: A Client-Side Defense Against Mobile App Traffic Profiling",
        "author": "Andrea Jimenez-Berenguel, C\u00e9sar Gil, Carlos Garcia-Rubio, Jordi Forn\u00e9, and Celeste Campo",
        "link": "http://arxiv.org/abs/2505.09374v1",
        "abstract": "Mobile applications continuously generate DNS queries that can reveal\nsensitive user behavioral patterns even when communications are encrypted. This\npaper presents a privacy enhancement framework based on query forgery to\nprotect users against profiling attempts that leverage these background\ncommunications. We first mathematically model user profiles as probability\ndistributions over interest categories derived from mobile application traffic.\nWe then evaluate three query forgery strategies -- uniform sampling,\nTrackMeNot-based generation, and an optimized approach that minimizes\nKullback-Leibler divergence -- to quantify their effectiveness in obfuscating\nuser profiles. Then we create a synthetic dataset comprising 1,000 user traces\nconstructed from real mobile application traffic and we extract the user\nprofiles based on DNS traffic. Our evaluation reveals that a 50\\% privacy\nimprovement is achievable with less than 20\\% traffic overhead when using our\napproach, while achieving 100\\% privacy protection requires approximately\n40-60\\% additional traffic. We further propose a modular system architecture\nfor practical implementation of our protection mechanisms on mobile devices.\nThis work offers a client-side privacy solution that operates without\nthird-party trust requirements, empowering individual users to defend against\ntraffic analysis without compromising application functionality."
    },
    {
        "date": "2025-05",
        "title": "RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo",
        "author": "Jenny Schmalfuss, Victor Oei, Lukas Mehl, Madlen Bartsch, Shashank Agnihotri, Margret Keuper, and Andr\u00e9s Bruhn",
        "link": "http://arxiv.org/abs/2505.09368v1",
        "abstract": "Standard benchmarks for optical flow, scene flow, and stereo vision\nalgorithms generally focus on model accuracy rather than robustness to image\ncorruptions like noise or rain. Hence, the resilience of models to such\nreal-world perturbations is largely unquantified. To address this, we present\nRobustSpring, a comprehensive dataset and benchmark for evaluating robustness\nto image corruptions for optical flow, scene flow, and stereo models.\nRobustSpring applies 20 different image corruptions, including noise, blur,\ncolor changes, quality degradations, and weather distortions, in a time-,\nstereo-, and depth-consistent manner to the high-resolution Spring dataset,\ncreating a suite of 20,000 corrupted images that reflect challenging\nconditions. RobustSpring enables comparisons of model robustness via a new\ncorruption robustness metric. Integration with the Spring benchmark enables\npublic two-axis evaluations of both accuracy and robustness. We benchmark a\ncurated selection of initial models, observing that accurate models are not\nnecessarily robust and that robustness varies widely by corruption type.\nRobustSpring is a new computer vision benchmark that treats robustness as a\nfirst-class citizen to foster models that combine accuracy with resilience. It\nwill be available at https://spring-benchmark.org."
    },
    {
        "date": "2025-05",
        "title": "Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems",
        "author": "Mostafa Jafari, and Alireza Shameli-Sendi",
        "link": "http://arxiv.org/abs/2505.09342v1",
        "abstract": "Machine learning is a key tool for Android malware detection, effectively\nidentifying malicious patterns in apps. However, ML-based detectors are\nvulnerable to evasion attacks, where small, crafted changes bypass detection.\nDespite progress in adversarial defenses, the lack of comprehensive evaluation\nframeworks in binary-constrained domains limits understanding of their\nrobustness. We introduce two key contributions. First, Prioritized Binary\nRounding, a technique to convert continuous perturbations into binary feature\nspaces while preserving high attack success and low perturbation size. Second,\nthe sigma-binary attack, a novel adversarial method for binary domains,\ndesigned to achieve attack goals with minimal feature changes. Experiments on\nthe Malscan dataset show that sigma-binary outperforms existing attacks and\nexposes key vulnerabilities in state-of-the-art defenses. Defenses equipped\nwith adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant\nbrittleness, with attack success rates exceeding 90% using fewer than 10\nfeature modifications and reaching 100% with just 20. Adversarially trained\ndefenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small\nbudgets but remains vulnerable to unrestricted perturbations, with attack\nsuccess rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates\nstrong robustness against state-of-the-art gradient-based adversarial attacks\nby maintaining an attack success rate below 16.55%, the sigma-binary attack\nsignificantly outperforms these methods, achieving a 94.56% success rate under\nunrestricted perturbations. These findings highlight the critical need for\nprecise method like sigma-binary to expose hidden vulnerabilities in existing\ndefenses and support the development of more resilient malware detection\nsystems."
    },
    {
        "date": "2025-05",
        "title": "Securing P4 Programs by Information Flow Control",
        "author": "Anoud Alshnakat, Amir M. Ahmadian, Musard Balliu, Roberto Guanciale, and Mads Dam",
        "link": "http://arxiv.org/abs/2505.09221v1",
        "abstract": "Software-Defined Networking (SDN) has transformed network architectures by\ndecoupling the control and data-planes, enabling fine-grained control over\npacket processing and forwarding. P4, a language designed for programming\ndata-plane devices, allows developers to define custom packet processing\nbehaviors directly on programmable network devices. This provides greater\ncontrol over packet forwarding, inspection, and modification. However, the\nincreased flexibility provided by P4 also brings significant security\nchallenges, particularly in managing sensitive data and preventing information\nleakage within the data-plane.\n  This paper presents a novel security type system for analyzing information\nflow in P4 programs that combines security types with interval analysis. The\nproposed type system allows the specification of security policies in terms of\ninput and output packet bit fields rather than program variables. We formalize\nthis type system and prove it sound, guaranteeing that well-typed programs\nsatisfy noninterference. Our prototype implementation, Tap4s, is evaluated on\nseveral use cases, demonstrating its effectiveness in detecting security\nviolations and information leakages."
    },
    {
        "date": "2025-05",
        "title": "DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis",
        "author": "Zeeshan Ahmad, Shudi Bao, and Meng Chen",
        "link": "http://arxiv.org/abs/2505.09091v1",
        "abstract": "In recent years, generative adversarial networks (GANs) have made significant\nprogress in generating audio sequences. However, these models typically rely on\nbandwidth-limited mel-spectrograms, which constrain the resolution of generated\naudio sequences, and lead to mode collapse during conditional generation. To\naddress this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),\na novel GAN architecture that incorporates a kernel-based periodic ReLU\nactivation function to induce periodic bias in audio generation. This\ninnovative approach enhances the model's ability to capture and reproduce\nintricate audio patterns. In particular, our proposed model features a DPN\nmodule for multi-resolution generation utilizing deformable convolution\noperations, allowing for adaptive receptive fields that improve the quality and\nfidelity of the synthetic audio. Additionally, we enhance the discriminator\nnetwork using deformable convolution to better distinguish between real and\ngenerated samples, further refining the audio quality. We trained two versions\nof the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M\nparameters). For evaluation, we use five different datasets, covering both\nspeech synthesis and music generation tasks, to demonstrate the efficiency of\nthe DPN-GAN. The experimental results demonstrate that DPN-GAN delivers\nsuperior performance on both out-of-distribution and noisy data, showcasing its\nrobustness and adaptability. Trained across various datasets, DPN-GAN\noutperforms state-of-the-art GAN architectures on standard evaluation metrics,\nand exhibits increased robustness in synthesized audio."
    },
    {
        "date": "2025-05",
        "title": "AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation",
        "author": "Berkay Guler, and Hamid Jafarkhani",
        "link": "http://arxiv.org/abs/2505.09076v1",
        "abstract": "Deep learning models for channel estimation in Orthogonal Frequency Division\nMultiplexing (OFDM) systems often suffer from performance degradation under\nfast-fading channels and low-SNR scenarios. To address these limitations, we\nintroduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model\nspecifically designed to enhance channel estimation in challenging\nenvironments. Our approach employs convolutional layers that exploit locality\nbias to capture strong correlations between neighboring channel elements,\ncombined with a transformer encoder that applies the global Attention mechanism\nto channel patches. This approach effectively models both long-range\ndependencies and spectro-temporal interactions within single OFDM frames. We\nfurther augment the model's adaptability by integrating nonlinear\nrepresentations of available channel statistics SNR, delay spread, and Doppler\nshift as priors. A residual connection is employed to merge global features\nfrom the transformer with local features from early convolutional processing,\nfollowed by final convolutional layers to refine the hierarchical channel\nrepresentation. Despite its compact architecture, AdaFortiTran achieves up to 6\ndB reduction in mean squared error (MSE) compared to state-of-the-art models.\nTested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),\nand delay spreads (50-300 ns), it demonstrates superior robustness in\nhigh-mobility environments."
    },
    {
        "date": "2025-05",
        "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition",
        "author": "J. Brennan Peace, Shuowen Hu, and Benjamin S. Riggan",
        "link": "http://arxiv.org/abs/2505.09073v1",
        "abstract": "Despite recent advances in facial recognition, there remains a fundamental\nissue concerning degradations in performance due to substantial perspective\n(pose) differences between enrollment and query (probe) imagery. Therefore, we\npropose a novel domain adaptive framework to facilitate improved performances\nacross large discrepancies in pose by enabling image-based (2D) representations\nto infer properties of inherently pose invariant point cloud (3D)\nrepresentations. Specifically, our proposed framework achieves better pose\ninvariance by using (1) a shared (joint) attention mapping to emphasize common\npatterns that are most correlated between 2D facial images and 3D facial data\nand (2) a joint entropy regularizing loss to promote better\nconsistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D\nand 3D representations$\\unicode{x2014}$by leveraging both attention maps. This\nframework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms\ncompetitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$)\nTAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and\n1.57$\\unicode{x0025}$, respectively."
    },
    {
        "date": "2025-05",
        "title": "Unencrypted Flying Objects: Security Lessons from University Small Satellite Developers and Their Code",
        "author": "Rachel McAmis, Gregor Haas, Mattea Sim, David Kohlbrenner, and Tadayoshi Kohno",
        "link": "http://arxiv.org/abs/2505.09038v1",
        "abstract": "Satellites face a multitude of security risks that set them apart from\nhardware on Earth. Small satellites may face additional challenges, as they are\noften developed on a budget and by amateur organizations or universities that\ndo not consider security. We explore the security practices and preferences of\nsmall satellite teams, particularly university satellite teams, to understand\nwhat barriers exist to building satellites securely. We interviewed 8\nuniversity satellite club leaders across 4 clubs in the U.S. and perform a code\naudit of 3 of these clubs' code repositories. We find that security practices\nvary widely across teams, but all teams studied had vulnerabilities available\nto an unprivileged, ground-based attacker. Participants foresee many risks of\nunsecured small satellites and indicate security shortcomings in industry and\ngovernment. Lastly, we identify a set of considerations for how to build future\nsmall satellites securely, in amateur organizations and beyond."
    },
    {
        "date": "2025-05",
        "title": "Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features",
        "author": "Monica Welfert, Nathan Stromberg, Mario Diaz, and Lalitha Sankar",
        "link": "http://arxiv.org/abs/2505.09004v1",
        "abstract": "We propose an adversarial evaluation framework for sensitive feature\ninference based on minimum mean-squared error (MMSE) estimation with a finite\nsample size and linear predictive models. Our approach establishes theoretical\nlower bounds on the true MMSE of inferring sensitive features from noisy\nobservations of other correlated features. These bounds are expressed in terms\nof the empirical MMSE under a restricted hypothesis class and a non-negative\nerror term. The error term captures both the estimation error due to finite\nnumber of samples and the approximation error from using a restricted\nhypothesis class. For linear predictive models, we derive closed-form bounds,\nwhich are order optimal in terms of the noise variance, on the approximation\nerror for several classes of relationships between the sensitive and\nnon-sensitive features, including linear mappings, binary symmetric channels,\nand class-conditional multi-variate Gaussian distributions. We also present a\nnew lower bound that relies on the MSE computed on a hold-out validation\ndataset of the MMSE estimator learned on finite-samples and a restricted\nhypothesis class. Through empirical evaluation, we demonstrate that our\nframework serves as an effective tool for MMSE-based adversarial evaluation of\nsensitive feature inference that balances theoretical guarantees with practical\nefficiency."
    },
    {
        "date": "2025-05",
        "title": "SAFE-SiP: Secure Authentication Framework for System-in-Package Using Multi-party Computation",
        "author": "Ishraq Tashdid, Tasnuva Farheen, and Sazadur Rahman",
        "link": "http://arxiv.org/abs/2505.09002v1",
        "abstract": "The emergence of chiplet-based heterogeneous integration is transforming the\nsemiconductor, AI, and high-performance computing industries by enabling\nmodular designs and improved scalability. However, assembling chiplets from\nmultiple vendors after fabrication introduces a complex supply chain that\nraises serious security concerns, including counterfeiting, overproduction, and\nunauthorized access. Current solutions often depend on dedicated security\nchiplets or changes to the timing flow, which assume a trusted SiP integrator.\nThis assumption can expose chiplet signatures to other vendors and create new\nattack surfaces. This work addresses those vulnerabilities using Multi-party\nComputation (MPC), which enables zero-trust authentication without disclosing\nsensitive information to any party. We present SAFE-SiP, a scalable\nauthentication framework that garbles chiplet signatures and uses MPC for\nverifying integrity, effectively blocking unauthorized access and adversarial\ninference. SAFE-SiP removes the need for a dedicated security chiplet and\nensures secure authentication, even in untrusted integration scenarios. We\nevaluated SAFE-SiP on five RISC-V-based System-in-Package (SiP) designs.\nExperimental results show that SAFE-SiP incurs minimal power overhead, an\naverage area overhead of only 3.05%, and maintains a computational complexity\nof 2^192, offering a highly efficient and scalable security solution."
    },
    {
        "date": "2025-05",
        "title": "Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking",
        "author": "Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, and Mao-Li Wang",
        "link": "http://arxiv.org/abs/2505.08999v1",
        "abstract": "In recent years, visual tracking methods based on convolutional neural\nnetworks and Transformers have achieved remarkable performance and have been\nsuccessfully applied in fields such as autonomous driving. However, the\nnumerous security issues exposed by deep learning models have gradually\naffected the reliable application of visual tracking methods in real-world\nscenarios. Therefore, how to reveal the security vulnerabilities of existing\nvisual trackers through effective adversarial attacks has become a critical\nproblem that needs to be addressed. To this end, we propose an adaptive\nmeta-gradient adversarial attack (AMGA) method for visual tracking. This method\nintegrates multi-model ensembles and meta-learning strategies, combining\nmomentum mechanisms and Gaussian smoothing, which can significantly enhance the\ntransferability and attack effectiveness of adversarial examples. AMGA randomly\nselects models from a large model repository, constructs diverse tracking\nscenarios, and iteratively performs both white- and black-box adversarial\nattacks in each scenario, optimizing the gradient directions of each model.\nThis paradigm minimizes the gap between white- and black-box adversarial\nattacks, thus achieving excellent attack performance in black-box scenarios.\nExtensive experimental results on large-scale datasets such as OTB2015, LaSOT,\nand GOT-10k demonstrate that AMGA significantly improves the attack\nperformance, transferability, and deception of adversarial examples. Codes and\ndata are available at https://github.com/pgao-lab/AMGA."
    },
    {
        "date": "2025-05",
        "title": "Inference Attacks for X-Vector Speaker Anonymization",
        "author": "Luke Bauer, Wenxuan Bao, Malvika Jadhav, and Vincent Bindschaedler",
        "link": "http://arxiv.org/abs/2505.08978v1",
        "abstract": "We revisit the privacy-utility tradeoff of x-vector speaker anonymization.\nExisting approaches quantify privacy through training complex speaker\nverification or identification models that are later used as attacks. Instead,\nwe propose a novel inference attack for de-anonymization. Our attack is simple\nand ML-free yet we show experimentally that it outperforms existing approaches."
    },
    {
        "date": "2025-05",
        "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
        "author": "Lukas Ammann, Sara Ott, Christoph R. Landolt, and Marco P. Lehmann",
        "link": "http://arxiv.org/abs/2505.08728v1",
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry\nstandard for user-facing NLP applications, offering the ability to integrate\ndata without re-training or fine-tuning Large Language Models (LLMs). This\ncapability enhances the quality and accuracy of responses but also introduces\nnovel security and privacy challenges, particularly when sensitive data is\nintegrated. With the rapid adoption of RAG, securing data and services has\nbecome a critical priority. This paper first reviews the vulnerabilities of RAG\npipelines, and outlines the attack surface from data pre-processing and data\nstorage management to integration with LLMs. The identified risks are then\npaired with corresponding mitigations in a structured overview. In a second\nstep, the paper develops a framework that combines RAG-specific security\nconsiderations, with existing general security guidelines, industry standards,\nand best practices. The proposed framework aims to guide the implementation of\nrobust, compliant, secure, and trustworthy RAG systems."
    },
    {
        "date": "2025-05",
        "title": "Cryptologic Techniques and Associated Risks in Public and Private Security. An Italian and European Union Perspective with an Overview of the Current Legal Framework",
        "author": "Zana Kudriasova",
        "link": "http://arxiv.org/abs/2505.08650v1",
        "abstract": "This article examines the evolution of cryptologic techniques and their\nimplications for public and private security, focusing on the Italian and EU\nlegal frameworks. It explores the roles of cryptography, steganography, and\nquantum technologies in countering cybersecurity threats, emphasising the need\nfor robust legislation to address emerging challenges. Special attention is\ngiven to Italy's legislative reforms, including Law No. 90 of 2024, which\nstrengthens penalties for cybercrimes and establishes the National Cryptography\nCentre within the Italian National Cybersecurity Agency. Additionally, the\narticle highlights international initiatives, such as the UN's draft convention\non cybercrime, emphasising the balance between security, privacy, and\nfundamental human rights in a post-quantum era."
    },
    {
        "date": "2025-05",
        "title": "WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks",
        "author": "Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, and Dan Ma",
        "link": "http://arxiv.org/abs/2505.08614v2",
        "abstract": "Deepfake technology poses increasing risks such as privacy invasion and\nidentity theft. To address these threats, we propose WaveGuard, a proactive\nwatermarking framework that enhances robustness and imperceptibility via\nfrequency-domain embedding and graph-based structural consistency.\nSpecifically, we embed watermarks into high-frequency sub-bands using Dual-Tree\nComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph\nNeural Network (SC-GNN) to preserve visual quality. We also design an attention\nmodule to refine embedding precision. Experimental results on face swap and\nreenactment tasks demonstrate that WaveGuard outperforms state-of-the-art\nmethods in both robustness and visual quality. Code is available at\nhttps://github.com/vpsg-research/WaveGuard."
    },
    {
        "date": "2025-05",
        "title": "GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning",
        "author": "Minsu Kim, Seong-Hyeon Hwang, and Steven Euijong Whang",
        "link": "http://arxiv.org/abs/2505.08528v1",
        "abstract": "In the context of continual learning, acquiring new knowledge while\nmaintaining previous knowledge presents a significant challenge. Existing\nmethods often use experience replay techniques that store a small portion of\nprevious task data for training. In experience replay approaches, data\naugmentation has emerged as a promising strategy to further improve the model\nperformance by mixing limited previous task data with sufficient current task\ndata. However, we theoretically and empirically analyze that training with\nmixed samples from random sample pairs may harm the knowledge of previous tasks\nand cause greater catastrophic forgetting. We then propose GradMix, a robust\ndata augmentation method specifically designed for mitigating catastrophic\nforgetting in class-incremental learning. GradMix performs gradient-based\nselective mixup using a class-based criterion that mixes only samples from\nhelpful class pairs and not from detrimental class pairs for reducing\ncatastrophic forgetting. Our experiments on various real datasets show that\nGradMix outperforms data augmentation baselines in accuracy by minimizing the\nforgetting of previous knowledge."
    },
    {
        "date": "2025-05",
        "title": "DArFace: Deformation Aware Robustness for Low Quality Face Recognition",
        "author": "Sadaf Gulshad, and Abdullah Aldahlawi Thakaa",
        "link": "http://arxiv.org/abs/2505.08423v1",
        "abstract": "Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce DArFace, a\nDeformation-Aware robust Face recognition framework that enhances robustness to\nsuch degradations without requiring paired high- and low-quality training\nsamples. Our method adversarially integrates both global transformations (e.g.,\nrotation, translation) and local elastic deformations during training to\nsimulate realistic low-quality conditions. Moreover, we introduce a contrastive\nobjective to enforce identity consistency across different deformed views.\nExtensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and\nIJB-C demonstrate that DArFace surpasses state-of-the-art methods, with\nsignificant gains attributed to the inclusion of local deformation modeling."
    },
    {
        "date": "2025-05",
        "title": "SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness",
        "author": "Yoonhyuk Choi, and Chong-Kwon Kim",
        "link": "http://arxiv.org/abs/2505.08320v2",
        "abstract": "We introduce SpecSphere, the first dual-pass spectral-spatial GNN that\ncertifies every prediction against both $\\ell\\_{0}$ edge flips and\n$\\ell\\_{\\infty}$ feature perturbations, adapts to the full\nhomophily-heterophily spectrum, and surpasses the expressive power of\n1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a\nChebyshev-polynomial spectral branch with an attention-gated spatial branch and\nfuses their representations through a lightweight MLP trained in a\ncooperative-adversarial min-max game. We further establish (i) a uniform\nChebyshev approximation theorem, (ii) minimax-optimal risk across the\nhomophily-heterophily spectrum, (iii) closed-form robustness certificates, and\n(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves\nstate-of-the-art node-classification accuracy and delivers tighter certified\nrobustness guarantees on real-world benchmarks. These results demonstrate that\nhigh expressivity, heterophily adaptation, and provable robustness can coexist\nwithin a single, scalable architecture."
    },
    {
        "date": "2025-05",
        "title": "On the Account Security Risks Posed by Password Strength Meters",
        "author": "Ming Xu, Weili Han, Jitao Yu, Jing Liu, Xinyi Zhang, Yun Lin, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2505.08292v1",
        "abstract": "Password strength meters (PSMs) have been widely used by websites to gauge\npassword strength, encouraging users to create stronger passwords. Popular\ndata-driven PSMs, e.g., based on Markov, Probabilistic Context-free Grammar\n(PCFG) and neural networks, alarm strength based on a model learned from real\npasswords. Despite their proven effectiveness, the secure utility that arises\nfrom the leakage of trained passwords remains largely overlooked. To address\nthis gap, we analyze 11 PSMs and find that 5 data-driven meters are vulnerable\nto membership inference attacks that expose their trained passwords, and\nseriously, 3 rule-based meters openly disclose their blocked passwords. We\nspecifically design a PSM privacy leakage evaluation approach, and uncover that\na series of general data-driven meters are vulnerable to leaking between 10^4\nto 10^5 trained passwords, with the PCFG-based models being more vulnerable\nthan other counterparts; furthermore, we aid in deriving insights that the\ninherent utility-privacy tradeoff is not as severe as previously thought. To\nfurther exploit the risks, we develop novel meter-aware attacks when a clever\nattacker can filter the used passwords during compromising accounts on websites\nusing the meter, and experimentally show that attackers targeting websites that\ndeployed the popular Zxcvbn meter can compromise an additional 5.84% user\naccounts within 10 attempts, demonstrating the urgent need for\nprivacy-preserving PSMs that protect the confidentiality of the meter's used\npasswords. Finally, we sketch some counter-measures to mitigate these threats."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning",
        "author": "Muhammad Saqib, Dipkumar Mehta, Fnu Yashu, and Shubham Malhotra",
        "link": "http://arxiv.org/abs/2505.08837v1",
        "abstract": "The security of cloud environments, such as Amazon Web Services (AWS), is\ncomplex and dynamic. Static security policies have become inadequate as threats\nevolve and cloud resources exhibit elasticity [1]. This paper addresses the\nlimitations of static policies by proposing a security policy management\nframework that uses reinforcement learning (RL) to adapt dynamically.\nSpecifically, we employ deep reinforcement learning algorithms, including deep\nQ Networks and proximal policy optimization, enabling the learning and\ncontinuous adjustment of controls such as firewall rules and Identity and\nAccess Management (IAM) policies. The proposed RL based solution leverages\ncloud telemetry data (AWS Cloud Trail logs, network traffic data, threat\nintelligence feeds) to continuously refine security policies, maximizing threat\nmitigation, and compliance while minimizing resource impact. Experimental\nresults demonstrate that our adaptive RL based framework significantly\noutperforms static policies, achieving higher intrusion detection rates (92%\ncompared to 82% for static policies) and substantially reducing incident\ndetection and response times by 58%. In addition, it maintains high conformity\nwith security requirements and efficient resource usage. These findings\nvalidate the effectiveness of adaptive reinforcement learning approaches in\nimproving cloud security policy management."
    },
    {
        "date": "2025-05",
        "title": "Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning",
        "author": "Ahmed Abouelazm, Tim Weinstein, Tim Joseph, Philip Sch\u00f6rner, and J. Marius Z\u00f6llner",
        "link": "http://arxiv.org/abs/2505.08264v1",
        "abstract": "This paper addresses the challenges of training end-to-end autonomous driving\nagents using Reinforcement Learning (RL). RL agents are typically trained in a\nfixed set of scenarios and nominal behavior of surrounding road users in\nsimulations, limiting their generalization and real-life deployment. While\ndomain randomization offers a potential solution by randomly sampling driving\nscenarios, it frequently results in inefficient training and sub-optimal\npolicies due to the high variance among training scenarios. To address these\nlimitations, we propose an automatic curriculum learning framework that\ndynamically generates driving scenarios with adaptive complexity based on the\nagent's evolving capabilities. Unlike manually designed curricula that\nintroduce expert bias and lack scalability, our framework incorporates a\n``teacher'' that automatically generates and mutates driving scenarios based on\ntheir learning potential -- an agent-centric metric derived from the agent's\ncurrent policy -- eliminating the need for expert design. The framework\nenhances training efficiency by excluding scenarios the agent has mastered or\nfinds too challenging. We evaluate our framework in a reinforcement learning\nsetting where the agent learns a driving policy from camera images. Comparative\nresults against baseline methods, including fixed scenario training and domain\nrandomization, demonstrate that our approach leads to enhanced generalization,\nachieving higher success rates: +9\\% in low traffic density, +21\\% in high\ntraffic density, and faster convergence with fewer training steps. Our findings\nhighlight the potential of ACL in improving the robustness and efficiency of\nRL-based autonomous driving agents."
    },
    {
        "date": "2025-05",
        "title": "Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores",
        "author": "Hyunsik Na, Wonho Lee, Seungdeok Roh, Sohee Park, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2505.08835v1",
        "abstract": "The advent of convenient and efficient fully unmanned stores equipped with\nartificial intelligence-based automated checkout systems marks a new era in\nretail. However, these systems have inherent artificial intelligence security\nvulnerabilities, which are exploited via adversarial patch attacks,\nparticularly in physical environments. This study demonstrated that adversarial\npatches can severely disrupt object detection models used in unmanned stores,\nleading to issues such as theft, inventory discrepancies, and interference. We\ninvestigated three types of adversarial patch attacks -- Hiding, Creating, and\nAltering attacks -- and highlighted their effectiveness. We also introduce the\nnovel color histogram similarity loss function by leveraging attacker knowledge\nof the color information of a target class object. Besides the traditional\nconfusion-matrix-based attack success rate, we introduce a new\nbounding-boxes-based metric to analyze the practical impact of these attacks.\nStarting with attacks on object detection models trained on snack and fruit\ndatasets in a digital environment, we evaluated the effectiveness of\nadversarial patches in a physical testbed that mimicked a real unmanned store\nwith RGB cameras and realistic conditions. Furthermore, we assessed the\nrobustness of these attacks in black-box scenarios, demonstrating that shadow\nattacks can enhance success rates of attacks even without direct access to\nmodel parameters. Our study underscores the necessity for robust defense\nstrategies to protect unmanned stores from adversarial threats. Highlighting\nthe limitations of the current defense mechanisms in real-time detection\nsystems and discussing various proactive measures, we provide insights into\nimproving the robustness of object detection models and fortifying unmanned\nretail environments against these attacks."
    },
    {
        "date": "2025-05",
        "title": "LM-Scout: Analyzing the Security of Language Model Integration in Android Apps",
        "author": "Muhammad Ibrahim, G\u0171liz Seray Tuncay, Z. Berkay Celik, Aravind Machiry, and Antonio Bianchi",
        "link": "http://arxiv.org/abs/2505.08204v1",
        "abstract": "Developers are increasingly integrating Language Models (LMs) into their\nmobile apps to provide features such as chat-based assistants. To prevent LM\nmisuse, they impose various restrictions, including limits on the number of\nqueries, input length, and allowed topics. However, if the LM integration is\ninsecure, attackers can bypass these restrictions and gain unrestricted access\nto the LM, potentially harming developers' reputations and leading to\nsignificant financial losses.\n  This paper presents the first systematic study of insecure usage of LMs by\nAndroid apps. We first manually analyze a preliminary dataset of apps to\ninvestigate LM integration methods, construct a taxonomy that categorizes the\nLM usage restrictions implemented by the apps, and determine how to bypass\nthem. Alarmingly, we can bypass restrictions in 127 out of 181 apps. Then, we\ndevelop LM-Scout, a fully automated tool to detect on a large-scale vulnerable\nusage of LMs in 2,950 mobile apps. LM-Scout shows that, in many cases (i.e.,\n120 apps), it is possible to find and exploit such security issues\nautomatically. Finally, we identify the root causes for the identified issues\nand offer recommendations for secure LM integration."
    },
    {
        "date": "2025-05",
        "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions",
        "author": "Wenhao Jiang, Yuchuan Luo, Guilin Deng, Silong Chen, Xu Yang, Shihong Wu, Xinwen Gao, Lin Liu, and Shaojing Fu",
        "link": "http://arxiv.org/abs/2505.08830v1",
        "abstract": "The integration of Large Language Models (LLMs) and Federated Learning (FL)\npresents a promising solution for joint training on distributed data while\npreserving privacy and addressing data silo issues. However, this emerging\nfield, known as Federated Large Language Models (FLLM), faces significant\nchallenges, including communication and computation overheads, heterogeneity,\nprivacy and security concerns. Current research has primarily focused on the\nfeasibility of FLLM, but future trends are expected to emphasize enhancing\nsystem robustness and security. This paper provides a comprehensive review of\nthe latest advancements in FLLM, examining challenges from four critical\nperspectives: feasibility, robustness, security, and future directions. We\npresent an exhaustive survey of existing studies on FLLM feasibility, introduce\nmethods to enhance robustness in the face of resource, data, and task\nheterogeneity, and analyze novel risks associated with this integration,\nincluding privacy threats and security challenges. We also review the latest\ndevelopments in defense mechanisms and explore promising future research\ndirections, such as few-shot learning, machine unlearning, and IP protection.\nThis survey highlights the pressing need for further research to enhance system\nrobustness and security while addressing the unique challenges posed by the\nintegration of FL and LLM."
    },
    {
        "date": "2025-05",
        "title": "Fast Text-to-Audio Generation with Adversarial Post-Training",
        "author": "Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, and Jordi Pons",
        "link": "http://arxiv.org/abs/2505.08175v2",
        "abstract": "Text-to-audio systems, while increasingly performant, are slow at inference\ntime, thus making their latency unpractical for many creative applications. We\npresent Adversarial Relativistic-Contrastive (ARC) post-training, the first\nadversarial acceleration algorithm for diffusion/flow models not based on\ndistillation. While past adversarial post-training methods have struggled to\ncompare against their expensive distillation counterparts, ARC post-training is\na simple procedure that (1) extends a recent relativistic adversarial\nformulation to diffusion/flow post-training and (2) combines it with a novel\ncontrastive discriminator objective to encourage better prompt adherence. We\npair ARC post-training with a number optimizations to Stable Audio Open and\nbuild a model capable of generating $\\approx$12s of 44.1kHz stereo audio in\n$\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest\ntext-to-audio model to our knowledge."
    },
    {
        "date": "2025-05",
        "title": "A Federated Random Forest Solution for Secure Distributed Machine Learning",
        "author": "Alexandre Cotorobai, Jorge Miguel Silva, and Jose Luis Oliveira",
        "link": "http://arxiv.org/abs/2505.08085v1",
        "abstract": "Privacy and regulatory barriers often hinder centralized machine learning\nsolutions, particularly in sectors like healthcare where data cannot be freely\nshared. Federated learning has emerged as a powerful paradigm to address these\nconcerns; however, existing frameworks primarily support gradient-based models,\nleaving a gap for more interpretable, tree-based approaches. This paper\nintroduces a federated learning framework for Random Forest classifiers that\npreserves data privacy and provides robust performance in distributed settings.\nBy leveraging PySyft for secure, privacy-aware computation, our method enables\nmultiple institutions to collaboratively train Random Forest models on locally\nstored data without exposing sensitive information. The framework supports\nweighted model averaging to account for varying data distributions, incremental\nlearning to progressively refine models, and local evaluation to assess\nperformance across heterogeneous datasets. Experiments on two real-world\nhealthcare benchmarks demonstrate that the federated approach maintains\ncompetitive predictive accuracy - within a maximum 9\\% margin of centralized\nmethods - while satisfying stringent privacy requirements. These findings\nunderscore the viability of tree-based federated learning for scenarios where\ndata cannot be centralized due to regulatory, competitive, or technical\nconstraints. The proposed solution addresses a notable gap in existing\nfederated learning libraries, offering an adaptable tool for secure distributed\nmachine learning tasks that demand both transparency and reliable performance.\nThe tool is available at https://github.com/ieeta-pt/fed_rf."
    },
    {
        "date": "2025-05",
        "title": "Browser Security Posture Analysis: A Client-Side Security Assessment Framework",
        "author": "Avihay Cohen",
        "link": "http://arxiv.org/abs/2505.08050v1",
        "abstract": "Modern web browsers have effectively become the new operating system for\nbusiness applications, yet their security posture is often under-scrutinized.\nThis paper presents a novel, comprehensive Browser Security Posture Analysis\nFramework[1], a browser-based client-side security assessment toolkit that runs\nentirely in JavaScript and WebAssembly within the browser. It performs a\nbattery of over 120 in-browser security tests in situ, providing fine-grained\ndiagnostics of security policies and features that network-level or os-level\ntools cannot observe. This yields insights into how well a browser enforces\ncritical client-side security invariants. We detail the motivation for such a\nframework, describe its architecture and implementation, and dive into the\ntechnical design of numerous test modules (covering the same-origin policy,\ncross-origin resource sharing, content security policy, sandboxing, XSS\nprotection, extension interference via WeakRefs, permissions audits, garbage\ncollection behavior, cryptographic APIs, SSL certificate validation, advanced\nweb platform security features like SharedArrayBuffer, Content filtering\ncontrols ,and internal network accessibility). We then present an experimental\nevaluation across different browsers and enterprise scenarios, highlighting\ngaps in legacy browsers and common misconfigurations. Finally, we discuss the\nsecurity and privacy implications of our findings, compare with related work in\nbrowser security and enterprise endpoint solutions, and outline future\nenhancements such as real-time posture monitoring and SIEM integration."
    },
    {
        "date": "2025-05",
        "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
        "author": "Steffen Schotth\u00f6fer, H. Lexie Yang, and Stefan Schnake",
        "link": "http://arxiv.org/abs/2505.08022v1",
        "abstract": "Deployment of neural networks on resource-constrained devices demands models\nthat are both compact and robust to adversarial inputs. However, compression\nand adversarial robustness often conflict. In this work, we introduce a\ndynamical low-rank training scheme enhanced with a novel spectral regularizer\nthat controls the condition number of the low-rank core in each layer. This\napproach mitigates the sensitivity of compressed models to adversarial\nperturbations without sacrificing clean accuracy. The method is model- and\ndata-agnostic, computationally efficient, and supports rank adaptivity to\nautomatically compress the network at hand. Extensive experiments across\nstandard architectures, datasets, and adversarial attacks show the regularized\nnetworks can achieve over 94% compression while recovering or improving\nadversarial accuracy relative to uncompressed baselines."
    },
    {
        "date": "2025-05",
        "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer",
        "author": "Gonglin Chen, Tianwen Fu, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, and Yajie Zhao",
        "link": "http://arxiv.org/abs/2505.08013v1",
        "abstract": "As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes."
    },
    {
        "date": "2025-05",
        "title": "Wasserstein Distributionally Robust Nonparametric Regression",
        "author": "Changyu Liu, Yuling Jiao, Junhui Wang, and Jian Huang",
        "link": "http://arxiv.org/abs/2505.07967v1",
        "abstract": "Distributionally robust optimization has become a powerful tool for\nprediction and decision-making under model uncertainty. By focusing on the\nlocal worst-case risk, it enhances robustness by identifying the most\nunfavorable distribution within a predefined ambiguity set. While extensive\nresearch has been conducted in parametric settings, studies on nonparametric\nframeworks remain limited. This paper studies the generalization properties of\nWasserstein distributionally robust nonparametric estimators, with particular\nattention to the impact of model misspecification, where non-negligible\ndiscrepancies between the estimation function space and target function can\nimpair generalization performance. We establish non-asymptotic error bounds for\nthe excess local worst-case risk by analyzing the regularization effects\ninduced by distributional perturbations and employing feedforward neural\nnetworks with Lipschitz constraints. These bounds illustrate how uncertainty\nlevels and neural network structures influence generalization performance and\nare applicable to both Lipschitz and quadratic loss functions. Furthermore, we\ninvestigate the Lagrangian relaxation of the local worst-case risk and derive\ncorresponding non-asymptotic error bounds for these estimators. The robustness\nof the proposed estimator is evaluated through simulation studies and\nillustrated with an application to the MNIST dataset."
    },
    {
        "date": "2025-05",
        "title": "Securing WiFi Fingerprint-based Indoor Localization Systems from Malicious Access Points",
        "author": "Fariha Tanjim Shifat, Sayma Sarwar Ela, and Mosarrat Jahan",
        "link": "http://arxiv.org/abs/2505.07724v1",
        "abstract": "WiFi fingerprint-based indoor localization schemes deliver highly accurate\nlocation data by matching the received signal strength indicator (RSSI) with an\noffline database using machine learning (ML) or deep learning (DL) models.\nHowever, over time, RSSI values degrade due to the malicious behavior of access\npoints (APs), causing low positional accuracy due to RSSI value mismatch with\nthe offline database. Existing literature lacks detection of malicious APs in\nthe online phase and mitigating their effects. This research addresses these\nlimitations and proposes a long-term reliable indoor localization scheme by\nincorporating malicious AP detection and their effect mitigation techniques.\nThe proposed scheme uses a Light Gradient-Boosting Machine (LGBM) classifier to\nestimate locations and integrates simple yet efficient techniques to detect\nmalicious APs based on online query data. Subsequently, a mitigation technique\nis incorporated that updates the offline database and online queries by\nimputing stable values for malicious APs using LGBM Regressors. Additionally,\nwe introduce a noise addition mechanism in the offline database to capture the\ndynamic environmental effects. Extensive experimental evaluation shows that the\nproposed scheme attains a detection accuracy above 95% for each attack type.\nThe mitigation strategy effectively restores the system's performance nearly to\nits original state when no malicious AP is present. The noise addition module\nreduces localization errors by nearly 16%. Furthermore, the proposed solution\nis lightweight, reducing the execution time by approximately 94% compared to\nthe existing methods."
    },
    {
        "date": "2025-05",
        "title": "Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy",
        "author": "Gleb Molodtsov, Daniil Medyakov, Sergey Skorik, Nikolas Khachaturov, Shahane Tigranyan, Vladimir Aletov, Aram Avetisyan, Martin Tak\u00e1\u010d, and Aleksandr Beznosikov",
        "link": "http://arxiv.org/abs/2505.07614v1",
        "abstract": "Recent advancements in machine learning have improved performance while also\nincreasing computational demands. While federated and distributed setups\naddress these issues, their structure is vulnerable to malicious influences. In\nthis paper, we address a specific threat, Byzantine attacks, where compromised\nclients inject adversarial updates to derail global convergence. We combine the\ntrust scores concept with trial function methodology to dynamically filter\noutliers. Our methods address the critical limitations of previous approaches,\nallowing functionality even when Byzantine nodes are in the majority. Moreover,\nour algorithms adapt to widely used scaled methods like Adam and RMSProp, as\nwell as practical scenarios, including local training and partial\nparticipation. We validate the robustness of our methods by conducting\nextensive experiments on both synthetic and real ECG data collected from\nmedical institutions. Furthermore, we provide a broad theoretical analysis of\nour algorithms and their extensions to aforementioned practical setups. The\nconvergence guarantees of our methods are comparable to those of classical\nalgorithms developed without Byzantine interference."
    },
    {
        "date": "2025-05",
        "title": "SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models",
        "author": "Huining Cui, and Wei Liu",
        "link": "http://arxiv.org/abs/2505.07584v1",
        "abstract": "The increasing deployment of large language models in security-sensitive\ndomains necessitates rigorous evaluation of their resilience against\nadversarial prompt-based attacks. While previous benchmarks have focused on\nsecurity evaluations with limited and predefined attack domains, such as\ncybersecurity attacks, they often lack a comprehensive assessment of\nintent-driven adversarial prompts and the consideration of real-life\nscenario-based multi-turn attacks. To address this gap, we present\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\nmodel assessment: one-off attack, successive attack, successive reverse attack,\nalternative attack, sequential ascending attack with escalating threat levels\nand sequential descending attack with diminishing threat levels. In addition,\nwe introduce a dataset customized for the benchmark, which incorporates both\nneutral and malicious prompts, categorised across seven security domains and\nsixteen attack techniques. In applying this benchmark, we systematically\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\ninsights into the strengths and weaknesses of modern large language models in\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\npublicly available at\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\nwhich provides a groundwork for advancing research in large language model\nsecurity."
    },
    {
        "date": "2025-05",
        "title": "Security through the Eyes of AI: How Visualization is Shaping Malware Detection",
        "author": "Matteo Brosolo, Asmitha K. A., Rafidha Rehiman K. A., Muhammed Shafi K. P., Serena Nicolazzo, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2505.07574v2",
        "abstract": "Malware, a persistent cybersecurity threat, increasingly targets\ninterconnected digital systems such as desktop, mobile, and IoT platforms\nthrough sophisticated attack vectors. By exploiting these vulnerabilities,\nattackers compromise the integrity and resilience of modern digital ecosystems.\nTo address this risk, security experts actively employ Machine Learning or Deep\nLearning-based strategies, integrating static, dynamic, or hybrid approaches to\ncategorize malware instances. Despite their advantages, these methods have\ninherent drawbacks and malware variants persistently evolve with increased\nsophistication, necessitating advancements in detection strategies.\nVisualization-based techniques are emerging as scalable and interpretable\nsolutions for detecting and understanding malicious behaviors across diverse\nplatforms including desktop, mobile, IoT, and distributed systems as well as\nthrough analysis of network packet capture files. In this comprehensive survey\nof more than 100 high-quality research articles, we evaluate existing\nvisualization-based approaches applied to malware detection and classification.\nAs a first contribution, we propose a new all-encompassing framework to study\nthe landscape of visualization-based malware detection techniques. Within this\nframework, we systematically analyze state-of-the-art approaches across the\ncritical stages of the malware detection pipeline. By analyzing not only the\nsingle techniques but also how they are combined to produce the final solution,\nwe shed light on the main challenges in visualization-based approaches and\nprovide insights into the advancements and potential future directions in this\ncritical field."
    },
    {
        "date": "2025-05",
        "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework",
        "author": "Sarah de Boer, Hartmut H\u00e4ntze, Kiran Vaidhya Venkadesh, Myrthe A. D. Buser, Gabriel E. Humpire Mamani, Lina Xu, Lisa C. Adams, Jawed Nawabi, Keno K. Bressem, Bram van Ginneken, Mathias Prokop, and Alessa Hering",
        "link": "http://arxiv.org/abs/2505.07573v1",
        "abstract": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation."
    },
    {
        "date": "2025-05",
        "title": "GRADA: Graph-based Reranker against Adversarial Documents Attack",
        "author": "Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2505.07546v1",
        "abstract": "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large\nlanguage models (LLMs) by integrating external knowledge from retrieved\ndocuments, thereby overcoming the limitations of models' static intrinsic\nknowledge. However, these systems are susceptible to adversarial attacks that\nmanipulate the retrieval process by introducing documents that are adversarial\nyet semantically similar to the query. Notably, while these adversarial\ndocuments resemble the query, they exhibit weak similarity to benign documents\nin the retrieval set. Thus, we propose a simple yet effective Graph-based\nReranking against Adversarial Document Attacks (GRADA) framework aiming at\npreserving retrieval quality while significantly reducing the success of\nadversaries. Our study evaluates the effectiveness of our approach through\nexperiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,\nLlama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with\nresults from the Natural Questions dataset demonstrating up to an 80% reduction\nin attack success rates while maintaining minimal loss in accuracy."
    },
    {
        "date": "2025-05",
        "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection",
        "author": "Juan E. Tapia, Fabian Stockhardt, L\u00e1zaro Janier Gonz\u00e1lez-Soler, and Christoph Busch",
        "link": "http://arxiv.org/abs/2505.07540v1",
        "abstract": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images."
    },
    {
        "date": "2025-05",
        "title": "Post-Quantum Secure Decentralized Random Number Generation Protocol with Two Rounds of Communication in the Standard Model",
        "author": "Pham Nhat Minh, and Khuong Nguyen-An",
        "link": "http://arxiv.org/abs/2505.07536v1",
        "abstract": "Randomness plays a vital role in numerous applications, including simulation,\ncryptography, distributed systems, and gaming. Consequently, extensive research\nhas been conducted to generate randomness. One such method is to design a\ndecentralized random number generator (DRNG), a protocol that enables multiple\nparticipants to collaboratively generate random outputs that must be publicly\nverifiable. However, existing DRNGs are either not secure against quantum\ncomputers or depend on the random oracle model (ROM) to achieve security. In\nthis paper, we design a DRNG based on lattice-based publicly verifiable secret\nsharing (PVSS) that is post-quantum secure and proven secure in the standard\nmodel. Additionally, our DRNG requires only two rounds of communication to\ngenerate a single (pseudo)random value and can tolerate up to any t < n/2\ndishonest participants. To our knowledge, the proposed DRNG construction is the\nfirst to achieve all these properties."
    },
    {
        "date": "2025-05",
        "title": "From Search To Sampling: Generative Models For Robust Algorithmic Recourse",
        "author": "Prateek Garg, Lokesh Nagalapatti, and Sunita Sarawagi",
        "link": "http://arxiv.org/abs/2505.07351v1",
        "abstract": "Algorithmic Recourse provides recommendations to individuals who are\nadversely impacted by automated model decisions, on how to alter their profiles\nto achieve a favorable outcome. Effective recourse methods must balance three\nconflicting goals: proximity to the original profile to minimize cost,\nplausibility for realistic recourse, and validity to ensure the desired\noutcome. We show that existing methods train for these objectives separately\nand then search for recourse through a joint optimization over the recourse\ngoals during inference, leading to poor recourse recommendations. We introduce\nGenRe, a generative recourse model designed to train the three recourse\nobjectives jointly. Training such generative models is non-trivial due to lack\nof direct recourse supervision. We propose efficient ways to synthesize such\nsupervision and further show that GenRe's training leads to a consistent\nestimator. Unlike most prior methods, that employ non-robust gradient descent\nbased search during inference, GenRe simply performs a forward sampling over\nthe generative model to produce minimum cost recourse, leading to superior\nperformance across multiple metrics. We also demonstrate GenRe provides the\nbest trade-off between cost, plausibility and validity, compared to\nstate-of-art baselines. Our code is available at:\nhttps://github.com/prateekgargx/genre."
    },
    {
        "date": "2025-05",
        "title": "Assessing the Latency of Network Layer Security in 5G Networks",
        "author": "Sotiris Michaelides, Jonathan Mucke, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.07328v1",
        "abstract": "In contrast to its predecessors, 5G supports a wide range of commercial,\nindustrial, and critical infrastructure scenarios. One key feature of 5G,\nultra-reliable low latency communication, is particularly appealing to such\nscenarios for its real-time capabilities. However, 5G's enhanced security,\nmostly realized through optional security controls, imposes additional overhead\non the network performance, potentially hindering its real-time capabilities.\nTo better assess this impact and guide operators in choosing between different\noptions, we measure the latency overhead of IPsec when applied over the N3 and\nthe service-based interfaces to protect user and control plane data,\nrespectively. Furthermore, we evaluate whether WireGuard constitutes an\nalternative to reduce this overhead. Our findings show that IPsec, if\nconfigured correctly, has minimal latency impact and thus is a prime candidate\nto secure real-time critical scenarios."
    },
    {
        "date": "2025-05",
        "title": "Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication",
        "author": "Bappa Muktar, Vincent Fono, and Adama Nouboukpo",
        "link": "http://arxiv.org/abs/2505.08810v1",
        "abstract": "Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent\nTransportation Systems (ITS), particularly in enabling real-time communication\nfor emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,\nwhich interfere with safety-critical communication channels, can severely\nimpair their reliability. This study introduces a robust and scalable framework\nto detect DDoS attacks in highway-based VANET environments. A synthetic dataset\nwas constructed using Network Simulator 3 (NS-3) in conjunction with the\nSimulation of Urban Mobility (SUMO) and further enriched with real-world\nmobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).\nThree traffic categories were simulated: DDoS, VoIP, and TCP-based video\nstreaming (VideoTCP). The data preprocessing pipeline included normalization,\nsignal-to-noise ratio (SNR) feature engineering, missing value imputation, and\nclass balancing using the Synthetic Minority Over-sampling Technique (SMOTE).\nFeature importance was assessed using SHapley Additive exPlanations (SHAP).\nEleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),\nAdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).\nXGB and CB achieved the best performance, each attaining an F1-score of 96%.\nThese results highlight the robustness of the proposed framework and its\npotential for real-time deployment in VANETs to secure critical emergency\ncommunications."
    },
    {
        "date": "2025-05",
        "title": "On the Robustness of Reward Models for Language Model Alignment",
        "author": "Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, and James Thorne",
        "link": "http://arxiv.org/abs/2505.07271v1",
        "abstract": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness."
    },
    {
        "date": "2025-05",
        "title": "Adaptive, Robust and Scalable Bayesian Filtering for Online Learning",
        "author": "Gerardo Duran-Martin",
        "link": "http://arxiv.org/abs/2505.07267v1",
        "abstract": "In this thesis, we introduce Bayesian filtering as a principled framework for\ntackling diverse sequential machine learning problems, including online\n(continual) learning, prequential (one-step-ahead) forecasting, and contextual\nbandits. To this end, this thesis addresses key challenges in applying Bayesian\nfiltering to these problems: adaptivity to non-stationary environments,\nrobustness to model misspecification and outliers, and scalability to the\nhigh-dimensional parameter space of deep neural networks. We develop novel\ntools within the Bayesian filtering framework to address each of these\nchallenges, including: (i) a modular framework that enables the development\nadaptive approaches for online learning; (ii) a novel, provably robust filter\nwith similar computational cost to standard filters, that employs Generalised\nBayes; and (iii) a set of tools for sequentially updating model parameters\nusing approximate second-order optimisation methods that exploit the\noverparametrisation of high-dimensional parametric models such as neural\nnetworks. Theoretical analysis and empirical results demonstrate the improved\nperformance of our methods in dynamic, high-dimensional, and misspecified\nmodels."
    },
    {
        "date": "2025-05",
        "title": "MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\u00f6dinger Bridges",
        "author": "Shixi Qin, Zhiyong Yang, Shilong Bao, Shi Wang, Qianqian Xu, and Qingming Huang",
        "link": "http://arxiv.org/abs/2505.08809v1",
        "abstract": "This paper focuses on implanting multiple heterogeneous backdoor triggers in\nbridge-based diffusion models designed for complex and arbitrary input\ndistributions. Existing backdoor formulations mainly address single-attack\nscenarios and are limited to Gaussian noise input models. To fill this gap, we\npropose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to\ncater to arbitrary input distributions (taking I2I tasks as special cases).\nBeyond this trait, we demonstrate that backdoor triggers can be injected into\nMixBridge by directly training with poisoned image pairs. This eliminates the\nneed for the cumbersome modifications to stochastic differential equations\nrequired in previous studies, providing a flexible tool to study backdoor\nbehavior for bridge models. However, a key question arises: can a single DSB\nmodel train multiple backdoor triggers? Unfortunately, our theory shows that\nwhen attempting this, the model ends up following the geometric mean of benign\nand backdoored distributions, leading to performance conflict across backdoor\ntasks. To overcome this, we propose a Divide-and-Merge strategy to mix\ndifferent bridges, where models are independently pre-trained for each specific\nobjective (Divide) and then integrated into a unified model (Merge). In\naddition, a Weight Reallocation Scheme (WRS) is also designed to enhance the\nstealthiness of MixBridge. Empirical studies across diverse generation tasks\nspeak to the efficacy of MixBridge."
    },
    {
        "date": "2025-05",
        "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments",
        "author": "Chetan Pathade, and Shubham Patil",
        "link": "http://arxiv.org/abs/2505.07188v1",
        "abstract": "Federated Learning (FL) offers a promising framework for collaboratively\ntraining machine learning models across decentralized genomic datasets without\ndirect data sharing. While this approach preserves data locality, it remains\nsusceptible to sophisticated inference attacks that can compromise individual\nprivacy. In this study, we simulate a federated learning setup using synthetic\ngenomic data and assess its vulnerability to three key attack vectors:\nMembership Inference Attack (MIA), Gradient-Based Membership Inference Attack,\nand Label Inference Attack (LIA). Our experiments reveal that Gradient-Based\nMIA achieves the highest effectiveness, with a precision of 0.79 and F1-score\nof 0.87, underscoring the risk posed by gradient exposure in federated updates.\nAdditionally, we visualize comparative attack performance through radar plots\nand quantify model leakage across clients. The findings emphasize the\ninadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate\nthe development of more robust privacy-preserving mechanisms tailored to the\nunique sensitivity of genomic data."
    },
    {
        "date": "2025-05",
        "title": "Security of Internet of Agents: Attacks and Countermeasures",
        "author": "Yuntao Wang, Yanghe Pan, Shaolong Guo, and Zhou Su",
        "link": "http://arxiv.org/abs/2505.08807v1",
        "abstract": "With the rise of large language and vision-language models, AI agents have\nevolved into autonomous, interactive systems capable of perception, reasoning,\nand decision-making. As they proliferate across virtual and physical domains,\nthe Internet of Agents (IoA) has emerged as a key infrastructure for enabling\nscalable and secure coordination among heterogeneous agents. This survey offers\na comprehensive examination of the security and privacy landscape in IoA\nsystems. We begin by outlining the IoA architecture and its distinct\nvulnerabilities compared to traditional networks, focusing on four critical\naspects: identity authentication threats, cross-agent trust issues, embodied\nsecurity, and privacy risks. We then review existing and emerging defense\nmechanisms and highlight persistent challenges. Finally, we identify open\nresearch directions to advance the development of resilient and\nprivacy-preserving IoA ecosystems."
    },
    {
        "date": "2025-05",
        "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models",
        "author": "Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.07167v1",
        "abstract": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods."
    },
    {
        "date": "2025-05",
        "title": "AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation",
        "author": "Heqing Ren, Chao Feng, Alberto Huertas, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2505.07149v1",
        "abstract": "Traditional machine learning (ML) raises serious privacy concerns, while\nfederated learning (FL) mitigates the risk of data leakage by keeping data on\nlocal devices. However, the training process of FL can still leak sensitive\ninformation, which adversaries may exploit to infer private data. One of the\nmost prominent threats is the membership inference attack (MIA), where the\nadversary aims to determine whether a particular data record was part of the\ntraining set.\n  This paper addresses this problem through a two-stage defense called\nAugMixCloak. The core idea is to apply data augmentation and principal\ncomponent analysis (PCA)-based information fusion to query images, which are\ndetected by perceptual hashing (pHash) as either identical to or highly similar\nto images in the training set. Experimental results show that AugMixCloak\nsuccessfully defends against both binary classifier-based MIA and metric-based\nMIA across five datasets and various decentralized FL (DFL) topologies.\nCompared with regularization-based defenses, AugMixCloak demonstrates stronger\nprotection. Compared with confidence score masking, AugMixCloak exhibits better\ngeneralization."
    },
    {
        "date": "2025-05",
        "title": "Standing Firm in 5G: A Single-Round, Dropout-Resilient Secure Aggregation for Federated Learning",
        "author": "Yiwei Zhang, Rouzbeh Behnia, Imtiaz Karim, Attila A. Yavuz, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2505.07148v1",
        "abstract": "Federated learning (FL) is well-suited to 5G networks, where many mobile\ndevices generate sensitive edge data. Secure aggregation protocols enhance\nprivacy in FL by ensuring that individual user updates reveal no information\nabout the underlying client data. However, the dynamic and large-scale nature\nof 5G-marked by high mobility and frequent dropouts-poses significant\nchallenges to the effective adoption of these protocols. Existing protocols\noften require multi-round communication or rely on fixed infrastructure,\nlimiting their practicality. We propose a lightweight, single-round secure\naggregation protocol designed for 5G environments. By leveraging base stations\nfor assisted computation and incorporating precomputation, key-homomorphic\npseudorandom functions, and t-out-of-k secret sharing, our protocol ensures\nefficiency, robustness, and privacy. Experiments show strong security\nguarantees and significant gains in communication and computation efficiency,\nmaking the approach well-suited for real-world 5G FL deployments."
    },
    {
        "date": "2025-05",
        "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization",
        "author": "Jitesh Joshi, and Youngjun Cho",
        "link": "http://arxiv.org/abs/2505.07013v1",
        "abstract": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live"
    },
    {
        "date": "2025-05",
        "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding",
        "author": "Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, and Yu-Jou Hsiao",
        "link": "http://arxiv.org/abs/2505.06991v1",
        "abstract": "This report presents our semantic segmentation framework developed by team\nACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which\nfocuses on parsing outdoor scenes into nine semantic categories under\nreal-world conditions. Our method integrates a Swin Transformer backbone\nenhanced with Rotary Position Embedding (RoPE) for improved spatial\ngeneralization, alongside a Color Shift Estimation-and-Correction module\ndesigned to compensate for illumination inconsistencies in natural\nenvironments. To further improve training stability, we adopt a quantile-based\ndenoising strategy that downweights the top 2.5\\% of highest-error pixels,\ntreating them as noise and suppressing their influence during optimization.\nEvaluated on the official GOOSE test set, our approach achieved a mean\nIntersection over Union (mIoU) of 0.848, demonstrating the effectiveness of\ncombining color correction, positional encoding, and error-aware denoising in\nrobust semantic segmentation."
    },
    {
        "date": "2025-05",
        "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition",
        "author": "Md. Naimur Asif Borno, Md Sakib Hossain Shovon, MD Hanif Sikder, Iffat Firozy Rimi, Tahani Jaser Alahmadi, and Mohammad Ali Moni",
        "link": "http://arxiv.org/abs/2505.06982v1",
        "abstract": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection."
    },
    {
        "date": "2025-05",
        "title": "A Formally Verified Robustness Certifier for Neural Networks (Extended Version)",
        "author": "James Tobler, Hira Taqdees Syeda, and Toby Murray",
        "link": "http://arxiv.org/abs/2505.06958v1",
        "abstract": "Neural networks are often susceptible to minor perturbations in input that\ncause them to misclassify. A recent solution to this problem is the use of\nglobally-robust neural networks, which employ a function to certify that the\nclassification of an input cannot be altered by such a perturbation. Outputs\nthat pass this test are called certified robust. However, to the authors'\nknowledge, these certification functions have not yet been verified at the\nimplementation level. We demonstrate how previous unverified implementations\nare exploitably unsound in certain circumstances. Moreover, they often rely on\napproximation-based algorithms, such as power iteration, that (perhaps\nsurprisingly) do not guarantee soundness. To provide assurance that a given\noutput is robust, we implemented and formally verified a certification function\nfor globally-robust neural networks in Dafny. We describe the program, its\nspecifications, and the important design decisions taken for its implementation\nand verification, as well as our experience applying it in practice."
    },
    {
        "date": "2025-05",
        "title": "RedTeamLLM: an Agentic AI framework for offensive security",
        "author": "Brian Challita, and Pierre Parrend",
        "link": "http://arxiv.org/abs/2505.06913v1",
        "abstract": "From automated intrusion testing to discovery of zero-day attacks before\nsoftware launch, agentic AI calls for great promises in security engineering.\nThis strong capability is bound with a similar threat: the security and\nresearch community must build up its models before the approach is leveraged by\nmalicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,\nan integrated architecture with a comprehensive security model for\nautomatization of pentest tasks. RedTeamLLM follows three key steps:\nsummarizing, reasoning and act, which embed its operational capacity. This\nnovel framework addresses four open challenges: plan correction, memory\nmanagement, context window constraint, and generality vs. specialization.\nEvaluation is performed through the automated resolution of a range of\nentry-level, but not trivial, CTF challenges. The contribution of the reasoning\ncapability of our agentic AI framework is specifically evaluated."
    },
    {
        "date": "2025-05",
        "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method",
        "author": "Mihyeon Kim, Juhyoung Park, and Youngbin Kim",
        "link": "http://arxiv.org/abs/2505.06889v1",
        "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy."
    },
    {
        "date": "2025-05",
        "title": "NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks",
        "author": "Shunyao Wang, Ming Cheng, and Christina Dan Wang",
        "link": "http://arxiv.org/abs/2505.06864v1",
        "abstract": "Stochastic Discount Factor (SDF) models provide a unified framework for asset\npricing and risk assessment, yet traditional formulations struggle to\nincorporate unstructured textual information. We introduce NewsNet-SDF, a novel\ndeep learning framework that seamlessly integrates pretrained language model\nembeddings with financial time series through adversarial networks. Our\nmultimodal architecture processes financial news using GTE-multilingual models,\nextracts temporal patterns from macroeconomic data via LSTM networks, and\nnormalizes firm characteristics, fusing these heterogeneous information sources\nthrough an innovative adversarial training mechanism. Our dataset encompasses\napproximately 2.5 million news articles and 10,000 unique securities,\naddressing the computational challenges of processing and aligning text data\nwith financial time series. Empirical evaluations on U.S. equity data\n(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with\na Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%\nimprovement versus traditional SDF implementations, and a 74% reduction in\npricing errors compared to the Fama-French five-factor model. In comprehensive\ncomparisons, our deep learning approach consistently outperforms traditional,\nmodern, and other neural asset pricing models across all key metrics. Ablation\nstudies confirm that text embeddings contribute significantly more to model\nperformance than macroeconomic features, with news-derived principal components\nranking among the most influential determinants of SDF dynamics. These results\nvalidate the effectiveness of our multimodal deep learning approach in\nintegrating unstructured text with traditional financial data for more accurate\nasset pricing, providing new insights for digital intelligent decision-making\nin financial technology."
    },
    {
        "date": "2025-05",
        "title": "DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection",
        "author": "Xia Du, Jiajie Zhu, Jizhe Zhou, Chi-man Pun, Zheng Lin, Cong Wu, Zhe Chen, and Jun Luo",
        "link": "http://arxiv.org/abs/2505.06860v1",
        "abstract": "In the field of digital security, Reversible Adversarial Examples (RAE)\ncombine adversarial attacks with reversible data hiding techniques to\neffectively protect sensitive data and prevent unauthorized analysis by\nmalicious Deep Neural Networks (DNNs). However, existing RAE techniques\nprimarily focus on white-box attacks, lacking a comprehensive evaluation of\ntheir effectiveness in black-box scenarios. This limitation impedes their\nbroader deployment in complex, dynamic environments. Further more, traditional\nblack-box attacks are often characterized by poor transferability and high\nquery costs, significantly limiting their practical applicability. To address\nthese challenges, we propose the Dual-Phase Merging Transferable Reversible\nAttack method, which generates highly transferable initial adversarial\nperturbations in a white-box model and employs a memory augmented black-box\nstrategy to effectively mislead target mod els. Experimental results\ndemonstrate the superiority of our approach, achieving a 99.0% attack success\nrate and 100% recovery rate in black-box scenarios, highlighting its robustness\nin privacy protection. Moreover, we successfully implemented a black-box attack\non a commercial model, further substantiating the potential of this approach\nfor practical use."
    },
    {
        "date": "2025-05",
        "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification",
        "author": "Miaoyun Zhao, Qiang Zhang, and Chenrong Li",
        "link": "http://arxiv.org/abs/2505.06831v1",
        "abstract": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios."
    },
    {
        "date": "2025-05",
        "title": "ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification",
        "author": "Dipayan Saha, Hasan Al Shaikh, Shams Tarek, and Farimah Farahmandi",
        "link": "http://arxiv.org/abs/2505.06821v1",
        "abstract": "Current hardware security verification processes predominantly rely on manual\nthreat modeling and test plan generation, which are labor-intensive,\nerror-prone, and struggle to scale with increasing design complexity and\nevolving attack methodologies. To address these challenges, we propose\nThreatLens, an LLM-driven multi-agent framework that automates security threat\nmodeling and test plan generation for hardware security verification.\nThreatLens integrates retrieval-augmented generation (RAG) to extract relevant\nsecurity knowledge, LLM-powered reasoning for threat assessment, and\ninteractive user feedback to ensure the generation of practical test plans. By\nautomating these processes, the framework reduces the manual verification\neffort, enhances coverage, and ensures a structured, adaptable approach to\nsecurity verification. We evaluated our framework on the NEORV32 SoC,\ndemonstrating its capability to automate security verification through\nstructured test plans and validating its effectiveness in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints",
        "author": "Abhishek Sinha, and Rahul Vaze",
        "link": "http://arxiv.org/abs/2505.06709v1",
        "abstract": "We revisit the Online Convex Optimization problem with adversarial\nconstraints (COCO) where, in each round, a learner is presented with a convex\ncost function and a convex constraint function, both of which may be chosen\nadversarially. The learner selects actions from a convex decision set in an\nonline fashion, with the goal of minimizing both regret and the cumulative\nconstraint violation (CCV) over a horizon of $T$ rounds. The best-known policy\nfor this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.\nIn this paper, we present a surprising improvement that achieves a\nsignificantly smaller CCV by trading it off with regret. Specifically, for any\nbounded convex cost and constraint functions, we propose an online policy that\nachieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$\nCCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a\ntunable parameter. We achieve this result by first considering the special case\nof $\\textsf{Constrained Expert}$ problem where the decision set is a\nprobability simplex and the cost and constraint functions are linear.\nLeveraging a new adaptive small-loss regret bound, we propose an efficient\npolicy for the $\\textsf{Constrained Expert}$ problem, that attains\n$O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV,\nwhere $N$ is the number of experts. The original problem is then reduced to the\n$\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an\nadditional smoothness assumption, we propose an efficient gradient-based policy\nattaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$\nCCV."
    },
    {
        "date": "2025-05",
        "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels",
        "author": "Xuefeng Jiang, Jia Li, Nannan Wu, Zhiyuan Wu, Xujing Li, Sheng Sun, Gang Xu, Yuwei Wang, Qi Li, and Min Liu",
        "link": "http://arxiv.org/abs/2505.06684v1",
        "abstract": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench."
    },
    {
        "date": "2025-05",
        "title": "Practical Reasoning Interruption Attacks on Reasoning Large Language Models",
        "author": "Yu Cui, and Cong Zuo",
        "link": "http://arxiv.org/abs/2505.06643v1",
        "abstract": "Reasoning large language models (RLLMs) have demonstrated outstanding\nperformance across a variety of tasks, yet they also expose numerous security\nvulnerabilities. Most of these vulnerabilities have centered on the generation\nof unsafe content. However, recent work has identified a distinct\n\"thinking-stopped\" vulnerability in DeepSeek-R1: under adversarial prompts, the\nmodel's reasoning process ceases at the system level and produces an empty\nfinal answer. Building upon this vulnerability, researchers developed a novel\nprompt injection attack, termed reasoning interruption attack, and also offered\nan initial analysis of its root cause. Through extensive experiments, we verify\nthe previous analyses, correct key errors based on three experimental findings,\nand present a more rigorous explanation of the fundamental causes driving the\nvulnerability. Moreover, existing attacks typically require over 2,000 tokens,\nimpose significant overhead, reduce practicality, and are easily detected. To\novercome these limitations, we propose the first practical reasoning\ninterruption attack. It succeeds with just 109 tokens by exploiting our newly\nuncovered \"reasoning token overflow\" (RTO) effect to overwrite the model's\nfinal answer, forcing it to return an invalid response. Experimental results\ndemonstrate that our proposed attack is highly effective. Furthermore, we\ndiscover that the method for triggering RTO differs between the official\nDeepSeek-R1 release and common unofficial deployments. As a broadened\napplication of RTO, we also construct a novel jailbreak attack that enables the\ntransfer of unsafe content within the reasoning tokens into final answer,\nthereby exposing it to the user. Our work carries significant implications for\nenhancing the security of RLLMs."
    },
    {
        "date": "2025-05",
        "title": "AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles",
        "author": "Rathin Chandra Shit, and Sharmila Subudhi",
        "link": "http://arxiv.org/abs/2505.06632v1",
        "abstract": "Autonomous Vehicles (AV) proliferation brings important and pressing security\nand reliability issues that must be dealt with to guarantee public safety and\nhelp their widespread adoption. The contribution of the proposed research is\ntowards achieving more secure, reliable, and trustworthy autonomous\ntransportation system by providing more capabilities for anomaly detection,\ndata provenance, and real-time response in safety critical AV deployments. In\nthis research, we develop a new framework that combines the power of Artificial\nIntelligence (AI) for real-time anomaly detection with blockchain technology to\ndetect and prevent any malicious activity including sensor failures in AVs.\nThrough Long Short-Term Memory (LSTM) networks, our approach continually\nmonitors associated multi-sensor data streams to detect anomalous patterns that\nmay represent cyberattacks as well as hardware malfunctions. Further, this\nframework employs a decentralized platform for securely storing sensor data and\nanomaly alerts in a blockchain ledger for data incorruptibility and\nauthenticity, while offering transparent forensic features. Moreover, immediate\nautomated response mechanisms are deployed using smart contracts when anomalies\nare found. This makes the AV system more resilient to attacks from both\ncyberspace and hardware component failure. Besides, we identify potential\nchallenges of scalability in handling high frequency sensor data, computational\nconstraint in resource constrained environment, and of distributed data storage\nin terms of privacy."
    },
    {
        "date": "2025-05",
        "title": "Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation",
        "author": "Yuqin Lan",
        "link": "http://arxiv.org/abs/2505.06612v1",
        "abstract": "In the era of rapid development of social media, social recommendation\nsystems as hybrid recommendation systems have been widely applied. Existing\nmethods capture interest similarity between users to filter out\ninterest-irrelevant relations in social networks that inevitably decrease\nrecommendation accuracy, however, limited research has a focus on the mutual\ninfluence of semantic information between the social network and the user-item\ninteraction network for further improving social recommendation. To address\nthese issues, we introduce a social \\underline{r}ecommendation model with\nro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion\nand multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly\npropose to construct a social tensor in order to smooth the training process of\nthe model. Then, a graph convolutional network and a tensor convolutional\nnetwork are employed to capture user's item preference and social preference,\nrespectively. Considering the different semantic information in the user-item\ninteraction network and the social network, a bi-semantic coordination loss is\nproposed to model the mutual influence of semantic information. To alleviate\nthe interference of interest-irrelevant relations on multi-semantic modeling,\nwe further use Bayesian posterior probability to mine potential social\nrelations to replace social noise. Finally, the sliding window mechanism is\nutilized to update the social tensor as the input for the next iteration.\nExtensive experiments on three real datasets show Burger has a superior\nperformance compared with the state-of-the-art models."
    },
    {
        "date": "2025-05",
        "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
        "author": "Dongyoon Yang, Jihu Lee, and Yongdai Kim",
        "link": "http://arxiv.org/abs/2505.06580v1",
        "abstract": "Robust domain adaptation against adversarial attacks is a critical research\narea that aims to develop models capable of maintaining consistent performance\nacross diverse and challenging domains. In this paper, we derive a new\ngeneralization bound for robust risk on the target domain using a novel\ndivergence measure specifically designed for robust domain adaptation. Building\nupon this, we propose a new algorithm named TAROT, which is designed to enhance\nboth domain adaptability and robustness. Through extensive experiments, TAROT\nnot only surpasses state-of-the-art methods in accuracy and robustness but also\nsignificantly enhances domain generalization and scalability by effectively\nlearning domain-invariant features. In particular, TAROT achieves superior\nperformance on the challenging DomainNet dataset, demonstrating its ability to\nlearn domain-invariant representations that generalize well across different\ndomains, including unseen ones. These results highlight the broader\napplicability of our approach in real-world domain adaptation scenarios."
    },
    {
        "date": "2025-05",
        "title": "dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data",
        "author": "Ad\u00e8le H. Ribeiro, and Dominik Heider",
        "link": "http://arxiv.org/abs/2505.06542v1",
        "abstract": "Causal discovery is central to inferring causal relationships from\nobservational data. In the presence of latent confounding, algorithms such as\nFast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing\nthe true model's Markov Equivalence Class. However, their correctness\ncritically depends on empirical faithfulness, the assumption that observed\n(in)dependencies perfectly reflect those of the underlying causal model, which\noften fails in practice due to limited sample sizes. To address this, we\nintroduce the first nonparametric score to assess a PAG's compatibility with\nobserved data, even with mixed variable types. This score is both necessary and\nsufficient to characterize structural uncertainty and distinguish between\ndistinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid\ncausal discovery algorithm to jointly address latent confounding, empirical\nunfaithfulness, and mixed data types. dcFCI integrates our score into an\n(Anytime)FCI-guided search that systematically explores, ranks, and validates\ncandidate PAGs. Experiments on synthetic and real-world scenarios demonstrate\nthat dcFCI significantly outperforms state-of-the-art methods, often recovering\nthe true PAG even in small and heterogeneous datasets. Examining top-ranked\nPAGs further provides valuable insights into structural uncertainty, supporting\nmore robust and informed causal reasoning and decision-making."
    },
    {
        "date": "2025-05",
        "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations",
        "author": "Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, and Thomas Poulet",
        "link": "http://arxiv.org/abs/2505.06502v1",
        "abstract": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper."
    },
    {
        "date": "2025-05",
        "title": "An In-kernel Forensics Engine for Investigating Evasive Attacks",
        "author": "Javad Zhandi, Lalchandra Rampersaud, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2505.06498v1",
        "abstract": "Over the years, adversarial attempts against critical services have become\nmore effective and sophisticated in launching low-profile attacks. This trend\nhas always been concerning. However, an even more alarming trend is the\nincreasing difficulty of collecting relevant evidence about these attacks and\nthe involved threat actors in the early stages before significant damage is\ndone. This issue puts defenders at a significant disadvantage, as it becomes\nexceedingly difficult to understand the attack details and formulate an\nappropriate response. Developing robust forensics tools to collect evidence\nabout modern threats has never been easy. One main challenge is to provide a\nrobust trade-off between achieving sufficient visibility while leaving minimal\ndetectable artifacts. This paper will introduce LASE, an open-source\nLow-Artifact Forensics Engine to perform threat analysis and forensics in\nWindows operating system. LASE augments current analysis tools by providing\ndetailed, system-wide monitoring capabilities while minimizing detectable\nartifacts. We designed multiple deployment scenarios, showing LASE's potential\nin evidence gathering and threat reasoning in a real-world setting. By making\nLASE and its execution trace data available to the broader research community,\nthis work encourages further exploration in the field by reducing the\nengineering costs for threat analysis and building a longitudinal behavioral\nanalysis catalog for diverse security domains."
    },
    {
        "date": "2025-05",
        "title": "System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection",
        "author": "Jiawei Guo, and Haipeng Cai",
        "link": "http://arxiv.org/abs/2505.06493v1",
        "abstract": "Large language models (LLMs) have gained widespread adoption across diverse\napplications due to their impressive generative capabilities. Their\nplug-and-play nature enables both developers and end users to interact with\nthese models through simple prompts. However, as LLMs become more integrated\ninto various systems in diverse domains, concerns around their security are\ngrowing. Existing studies mainly focus on threats arising from user prompts\n(e.g. prompt injection attack) and model output (e.g. model inversion attack),\nwhile the security of system prompts remains largely overlooked. This work\nbridges the critical gap. We introduce system prompt poisoning, a new attack\nvector against LLMs that, unlike traditional user prompt injection, poisons\nsystem prompts hence persistently impacts all subsequent user interactions and\nmodel responses. We systematically investigate four practical attack strategies\nin various poisoning scenarios. Through demonstration on both generative and\nreasoning LLMs, we show that system prompt poisoning is highly feasible without\nrequiring jailbreak techniques, and effective across a wide range of tasks,\nincluding those in mathematics, coding, logical reasoning, and natural language\nprocessing. Importantly, our findings reveal that the attack remains effective\neven when user prompts employ advanced prompting techniques like\nchain-of-thought (CoT). We also show that such techniques, including CoT and\nretrieval-augmentation-generation (RAG), which are proven to be effective for\nimproving LLM performance in a wide range of tasks, are significantly weakened\nin their effectiveness by system prompt poisoning."
    },
    {
        "date": "2025-05",
        "title": "Learning from the Good Ones: Risk Profiling-Based Defenses Against Evasion Attacks on DNNs",
        "author": "Mohammed Elnawawy, Gargi Mitra, Shahrear Iqbal, and Karthik Pattabiraman",
        "link": "http://arxiv.org/abs/2505.06477v1",
        "abstract": "Safety-critical applications such as healthcare and autonomous vehicles use\ndeep neural networks (DNN) to make predictions and infer decisions. DNNs are\nsusceptible to evasion attacks, where an adversary crafts a malicious data\ninstance to trick the DNN into making wrong decisions at inference time.\nExisting defenses that protect DNNs against evasion attacks are either static\nor dynamic. Static defenses are computationally efficient but do not adapt to\nthe evolving threat landscape, while dynamic defenses are adaptable but suffer\nfrom an increased computational overhead. To combine the best of both worlds,\nin this paper, we propose a novel risk profiling framework that uses a\nrisk-aware strategy to selectively train static defenses using victim instances\nthat exhibit the most resilient features and are hence more resilient against\nan evasion attack. We hypothesize that training existing defenses on instances\nthat are less vulnerable to the attack enhances the adversarial detection rate\nby reducing false negatives. We evaluate the efficacy of our risk-aware\nselective training strategy on a blood glucose management system that\ndemonstrates how training static anomaly detectors indiscriminately may result\nin an increased false negative rate, which could be life-threatening in\nsafety-critical applications. Our experiments show that selective training on\nthe less vulnerable patients achieves a recall increase of up to 27.5\\% with\nminimal impact on precision compared to indiscriminate training."
    },
    {
        "date": "2025-05",
        "title": "\"vcd2df\" -- Leveraging Data Science Insights for Hardware Security Research",
        "author": "Calvin Deutschbein, Jimmy Ostler, and Hriday Raj",
        "link": "http://arxiv.org/abs/2505.06470v2",
        "abstract": "In this work, we hope to expand the universe of security practitioners of\nopen-source hardware by creating a bridge from hardware design languages (HDLs)\nto data science languages like Python and R through libraries that convert VCD\n(value change dump) files into data frames, the expected input type of the\nmodern data science tools. We show how insights can be derived in high-level\nlanguages from register transfer level (RTL) trace data. Additional, we show a\npromising future direction in hardware security research leveraging the\nparallelism of the Spark DataFrame."
    },
    {
        "date": "2025-05",
        "title": "Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning",
        "author": "Syed Mhamudul Hasan, Hussein Zangoti, Iraklis Anagnostopoulos, and Abdur R. Shahid",
        "link": "http://arxiv.org/abs/2505.06454v1",
        "abstract": "Recent studies have shown that sponge attacks can significantly increase the\nenergy consumption and inference latency of deep neural networks (DNNs).\nHowever, prior work has focused primarily on computer vision and natural\nlanguage processing tasks, overlooking the growing use of lightweight AI models\nin sensing-based applications on resource-constrained devices, such as those in\nInternet of Things (IoT) environments. These attacks pose serious threats of\nenergy depletion and latency degradation in systems where limited battery\ncapacity and real-time responsiveness are critical for reliable operation. This\npaper makes two key contributions. First, we present the first systematic\nexploration of energy-latency sponge attacks targeting sensing-based AI models.\nUsing wearable sensing-based AI as a case study, we demonstrate that sponge\nattacks can substantially degrade performance by increasing energy consumption,\nleading to faster battery drain, and by prolonging inference latency. Second,\nto mitigate such attacks, we investigate model pruning, a widely adopted\ncompression technique for resource-constrained AI, as a potential defense. Our\nexperiments show that pruning-induced sparsity significantly improves model\nresilience against sponge poisoning. We also quantify the trade-offs between\nmodel efficiency and attack resilience, offering insights into the security\nimplications of model compression in sensing-based AI systems deployed in IoT\nenvironments."
    },
    {
        "date": "2025-05",
        "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving",
        "author": "Ming Liu, Siyuan Liang, Koushik Howlader, Liwen Wang, Dacheng Tao, and Wensheng Zhang",
        "link": "http://arxiv.org/abs/2505.06413v1",
        "abstract": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems."
    },
    {
        "date": "2025-05",
        "title": "Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2505.06409v1",
        "abstract": "As AI models scale to billions of parameters and operate with increasing\nautonomy, ensuring their safe, reliable operation demands engineering-grade\nsecurity and assurance frameworks. This paper presents an enterprise-level,\nrisk-aware, security-by-design approach for large-scale autonomous AI systems,\nintegrating standardized threat metrics, adversarial hardening techniques, and\nreal-time anomaly detection into every phase of the development lifecycle. We\ndetail a unified pipeline - from design-time risk assessments and secure\ntraining protocols to continuous monitoring and automated audit logging - that\ndelivers provable guarantees of model behavior under adversarial and\noperational stress. Case studies in national security, open-source model\ngovernance, and industrial automation demonstrate measurable reductions in\nvulnerability and compliance overhead. Finally, we advocate cross-sector\ncollaboration - uniting engineering teams, standards bodies, and regulatory\nagencies - to institutionalize these technical safeguards within a resilient,\nend-to-end assurance ecosystem for the next generation of AI."
    },
    {
        "date": "2025-05",
        "title": "Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers",
        "author": "Massimiliano Albanese, Xinming Ou, Kevin Lybarger, Daniel Lende, and Dmitry Goldgof",
        "link": "http://arxiv.org/abs/2505.06394v1",
        "abstract": "Security Operations Centers (SOCs) face growing challenges in managing\ncybersecurity threats due to an overwhelming volume of alerts, a shortage of\nskilled analysts, and poorly integrated tools. Human-AI collaboration offers a\npromising path to augment the capabilities of SOC analysts while reducing their\ncognitive overload. To this end, we introduce an AI-driven human-machine\nco-teaming paradigm that leverages large language models (LLMs) to enhance\nthreat intelligence, alert triage, and incident response workflows. We present\na vision in which LLM-based AI agents learn from human analysts the tacit\nknowledge embedded in SOC operations, enabling the AI agents to improve their\nperformance on SOC tasks through this co-teaming. We invite SOCs to collaborate\nwith us to further develop this process and uncover replicable patterns where\nhuman-AI co-teaming yields measurable improvements in SOC productivity."
    },
    {
        "date": "2025-05",
        "title": "Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms",
        "author": "Adrien Chan-Hon-Tong, Aur\u00e9lien Plyer, Baptiste Cadalen, and Laurent Serre",
        "link": "http://arxiv.org/abs/2505.06389v1",
        "abstract": "Sensor-based guidance is required for long-range platforms. To bypass the\nstructural limitation of classical registration on reference image framework,\nwe offer in this paper to encode a stack of images of the scene into a deep\nnetwork. Relying on a stack is showed to be relevant on bimodal scene (e.g.\nwhen the scene can or can not be snowy)."
    },
    {
        "date": "2025-05",
        "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal",
        "author": "Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, and Andreas Dengel",
        "link": "http://arxiv.org/abs/2505.06381v1",
        "abstract": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet)."
    },
    {
        "date": "2025-05",
        "title": "Offensive Security for AI Systems: Concepts, Practices, and Applications",
        "author": "Josh Harguess, and Chris M. Ward",
        "link": "http://arxiv.org/abs/2505.06380v1",
        "abstract": "As artificial intelligence (AI) systems become increasingly adopted across\nsectors, the need for robust, proactive security strategies is paramount.\nTraditional defensive measures often fall short against the unique and evolving\nthreats facing AI-driven technologies, making offensive security an essential\napproach for identifying and mitigating risks. This paper presents a\ncomprehensive framework for offensive security in AI systems, emphasizing\nproactive threat simulation and adversarial testing to uncover vulnerabilities\nthroughout the AI lifecycle. We examine key offensive security techniques,\nincluding weakness and vulnerability assessment, penetration testing, and red\nteaming, tailored specifically to address AI's unique susceptibilities. By\nsimulating real-world attack scenarios, these methodologies reveal critical\ninsights, informing stronger defensive strategies and advancing resilience\nagainst emerging threats. This framework advances offensive AI security from\ntheoretical concepts to practical, actionable methodologies that organizations\ncan implement to strengthen their AI systems against emerging threats."
    },
    {
        "date": "2025-05",
        "title": "Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment",
        "author": "Muhy Eddin Za'ter, Amir Sajad, and Bri-Mathias Hodge",
        "link": "http://arxiv.org/abs/2505.06207v1",
        "abstract": "This paper introduces a novel approach to the power system security\nassessment using Multi-Task Learning (MTL), and reformulating the problem as a\nmulti-label classification task. The proposed MTL framework simultaneously\nassesses static, voltage, transient, and small-signal stability, improving both\naccuracy and interpretability with respect to the most state of the art machine\nlearning methods. It consists of a shared encoder and multiple decoders,\nenabling knowledge transfer between stability tasks. Experiments on the IEEE\n68-bus system demonstrate a measurable superior performance of the proposed\nmethod compared to the extant state-of-the-art approaches."
    },
    {
        "date": "2025-05",
        "title": "Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients",
        "author": "Jinsheng Yuan, Yuhang Hao, Weisi Guo, Yun Wu, and Chongyan Gu",
        "link": "http://arxiv.org/abs/2505.06335v1",
        "abstract": "Federated Learning (FL) has the potential for simultaneous global learning\namongst a large number of parallel agents, enabling emerging AI such as LLMs to\nbe trained across demographically diverse data. Central to this being efficient\nis the ability for FL to perform sparse gradient updates and remote direct\nmemory access at the central server. Most of the research in FL security\nfocuses on protecting data privacy at the edge client or in the communication\nchannels between the client and server. Client-facing attacks on the server are\nless well investigated as the assumption is that a large collective of clients\noffer resilience.\n  Here, we show that by attacking certain clients that lead to a high frequency\nrepetitive memory update in the server, we can remote initiate a rowhammer\nattack on the server memory. For the first time, we do not need backdoor access\nto the server, and a reinforcement learning (RL) attacker can learn how to\nmaximize server repetitive memory updates by manipulating the client's sensor\nobservation. The consequence of the remote rowhammer attack is that we are able\nto achieve bit flips, which can corrupt the server memory. We demonstrate the\nfeasibility of our attack using a large-scale FL automatic speech recognition\n(ASR) systems with sparse updates, our adversarial attacking agent can achieve\naround 70\\% repeated update rate (RUR) in the targeted server model,\neffectively inducing bit flips on server DRAM. The security implications are\nthat can cause disruptions to learning or may inadvertently cause elevated\nprivilege. This paves the way for further research on practical mitigation\nstrategies in FL and hardware design."
    },
    {
        "date": "2025-05",
        "title": "NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines",
        "author": "Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, and Amit Sheth",
        "link": "http://arxiv.org/abs/2505.06333v1",
        "abstract": "In modern assembly pipelines, identifying anomalies is crucial in ensuring\nproduct quality and operational efficiency. Conventional single-modality\nmethods fail to capture the intricate relationships required for precise\nanomaly prediction in complex predictive environments with abundant data and\nmultiple modalities. This paper proposes a neurosymbolic AI and fusion-based\napproach for multimodal anomaly prediction in assembly pipelines. We introduce\na time series and image-based fusion model that leverages decision-level fusion\ntechniques. Our research builds upon three primary novel approaches in\nmultimodal learning: time series and image-based decision-level fusion\nmodeling, transfer learning for fusion, and knowledge-infused learning. We\nevaluate the novel method using our derived and publicly available multimodal\ndataset and conduct comprehensive ablation studies to assess the impact of our\npreprocessing techniques and fusion model compared to traditional baselines.\nThe results demonstrate that a neurosymbolic AI-based fusion approach that uses\ntransfer learning can effectively harness the complementary strengths of time\nseries and image data, offering a robust and interpretable approach for anomaly\nprediction in assembly pipelines with enhanced performance. \\noindent The\ndatasets, codes to reproduce the results, supplementary materials, and demo are\navailable at https://github.com/ChathurangiShyalika/NSF-MAP."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies",
        "author": "Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, and Junliang Du",
        "link": "http://arxiv.org/abs/2505.06145v1",
        "abstract": "Few-shot text classification has important application value in low-resource\nenvironments. This paper proposes a strategy that combines adaptive\nfine-tuning, contrastive learning, and regularization optimization to improve\nthe classification performance of Transformer-based models. Experiments on the\nFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform\nwell in few-shot tasks, especially in the 5-shot setting, which can more\neffectively capture text features and improve classification accuracy. The\nexperiment also found that there are significant differences in the\nclassification difficulty of different relationship categories. Some categories\nhave fuzzy semantic boundaries or complex feature distributions, making it\ndifficult for the standard cross entropy loss to learn the discriminative\ninformation required to distinguish categories. By introducing contrastive loss\nand regularization loss, the generalization ability of the model is enhanced,\neffectively alleviating the overfitting problem in few-shot environments. In\naddition, the research results show that the use of Transformer models or\ngenerative architectures with stronger self-attention mechanisms can help\nimprove the stability and accuracy of few-shot classification."
    },
    {
        "date": "2025-05",
        "title": "Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation",
        "author": "Julian F. Schumann, Jeroen Hagenus, Frederik Baymler Mathiesen, and Arkady Zgonnikov",
        "link": "http://arxiv.org/abs/2505.06134v1",
        "abstract": "Trajectory prediction is a key element of autonomous vehicle systems,\nenabling them to anticipate and react to the movements of other road users.\nEvaluating the robustness of prediction models against adversarial attacks is\nessential to ensure their reliability in real-world traffic. However, current\napproaches tend to focus on perturbing the past positions of surrounding\nagents, which can generate unrealistic scenarios and overlook critical\nvulnerabilities. This limitation may result in overly optimistic assessments of\nmodel performance in real-world conditions.\n  In this work, we demonstrate that perturbing not just past but also future\nstates of adversarial agents can uncover previously undetected weaknesses and\nthereby provide a more rigorous evaluation of model robustness. Our novel\napproach incorporates dynamic constraints and preserves tactical behaviors,\nenabling more effective and realistic adversarial attacks. We introduce new\nperformance measures to assess the realism and impact of these adversarial\ntrajectories. Testing our method on a state-of-the-art prediction model\nrevealed significant increases in prediction errors and collision rates under\nadversarial conditions. Qualitative analysis further showed that our attacks\ncan expose critical weaknesses, such as the inability of the model to detect\npotential collisions in what appear to be safe predictions. These results\nunderscore the need for more comprehensive adversarial testing to better\nevaluate and improve the reliability of trajectory prediction models for\nautonomous vehicles."
    },
    {
        "date": "2025-05",
        "title": "TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations",
        "author": "Shuaiyi Huang, Mara Levy, Anubhav Gupta, Daniel Ekpo, Ruijie Zheng, and Abhinav Shrivastava",
        "link": "http://arxiv.org/abs/2505.06079v1",
        "abstract": "Preference feedback collected by human or VLM annotators is often noisy,\npresenting a significant challenge for preference-based reinforcement learning\nthat relies on accurate preference labels. To address this challenge, we\npropose TREND, a novel framework that integrates few-shot expert demonstrations\nwith a tri-teaching strategy for effective noise mitigation. Our method trains\nthree reward models simultaneously, where each model views its small-loss\npreference pairs as useful knowledge and teaches such useful pairs to its peer\nnetwork for updating the parameters. Remarkably, our approach requires as few\nas one to three expert demonstrations to achieve high performance. We evaluate\nTREND on various robotic manipulation tasks, achieving up to 90% success rates\neven with noise levels as high as 40%, highlighting its effective robustness in\nhandling noisy preference feedback. Project page:\nhttps://shuaiyihuang.github.io/publications/TREND."
    },
    {
        "date": "2025-05",
        "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
        "author": "Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, and Christof Monz",
        "link": "http://arxiv.org/abs/2505.06027v1",
        "abstract": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning."
    },
    {
        "date": "2025-05",
        "title": "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward Secure Inference in FinTech Applications",
        "author": "Faneela, Baraq Ghaleb, Jawad Ahmad, William J. Buchanan, and Sana Ullah Jan",
        "link": "http://arxiv.org/abs/2505.05920v1",
        "abstract": "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems."
    },
    {
        "date": "2025-05",
        "title": "A Taxonomy of Attacks and Defenses in Split Learning",
        "author": "Aqsa Shabbir, Halil \u0130brahim Kanpak, Alptekin K\u00fcp\u00e7\u00fc, and Sinem Sav",
        "link": "http://arxiv.org/abs/2505.05872v1",
        "abstract": "Split Learning (SL) has emerged as a promising paradigm for distributed deep\nlearning, allowing resource-constrained clients to offload portions of their\nmodel computation to servers while maintaining collaborative learning. However,\nrecent research has demonstrated that SL remains vulnerable to a range of\nprivacy and security threats, including information leakage, model inversion,\nand adversarial attacks. While various defense mechanisms have been proposed, a\nsystematic understanding of the attack landscape and corresponding\ncountermeasures is still lacking. In this study, we present a comprehensive\ntaxonomy of attacks and defenses in SL, categorizing them along three key\ndimensions: employed strategies, constraints, and effectiveness. Furthermore,\nwe identify key open challenges and research gaps in SL based on our\nsystematization, highlighting potential future directions."
    },
    {
        "date": "2025-05",
        "title": "Intrusion Detection System Using Deep Learning for Network Security",
        "author": "Soham Chatterjee, Satvik Chaudhary, and Aswani Kumar Cherukuri",
        "link": "http://arxiv.org/abs/2505.05810v1",
        "abstract": "As the number of cyberattacks and their particualr nature escalate, the need\nfor effective intrusion detection systems (IDS) has become indispensable for\nensuring the security of contemporary networks. Adaptive and more sophisticated\nthreats are often beyond the reach of traditional approaches to intrusion\ndetection and access control. This paper proposes an experimental evaluation of\nIDS models based on deep learning techniques, focusing on the classification of\nnetwork traffic into malicious and benign categories. We analyze and retrain an\nassortment of architectures, such as Convolutional Neural Networks (CNN),\nArtificial Neural Networks (ANN), and LSTM models. Each model was tested based\non a real dataset simulated in a multi-faceted and everchanging network traffic\nenvironment. Among the tested models, the best achieved an accuracy of 96\npercent, underscoring the potential of deep learning models in improving\nefficiency and rapid response in IDS systems. The goal of the research is to\ndemonstrate the effectiveness of distinct architectures and their corresponding\ntrade-offs to enhance framework development for adaptive IDS solutions and\nimprove overall network security."
    },
    {
        "date": "2025-05",
        "title": "Measuring Security in 5G and Future Networks",
        "author": "Loay Abdelrazek, Rim ElMalki, Filippo Rebecchi, and Daniel Cho",
        "link": "http://arxiv.org/abs/2505.08799v1",
        "abstract": "In today's increasingly interconnected and fast-paced digital ecosystem,\nmobile networks, such as 5G and future generations such as 6G, play a pivotal\nrole and must be considered as critical infrastructures. Ensuring their\nsecurity is paramount to safeguard both individual users and the industries\nthat depend on these networks. An essential condition for maintaining and\nimproving the security posture of a system is the ability to effectively\nmeasure and monitor its security state. In this work we address the need for an\nobjective measurement of the security state of 5G and future networks. We\nintroduce a state machine model designed to capture the security life cycle of\nnetwork functions and the transitions between different states within the life\ncycle. Such a model can be computed locally at each node, or hierarchically, by\naggregating measurements into security domains or the whole network. We\nidentify three essential security metrics -- attack surface exposure, impact of\nsystem vulnerabilities, and effectiveness of applied security controls -- that\ncollectively form the basis for calculating the overall security score. With\nthis approach, it is possible to provide a holistic understanding of the\nsecurity posture, laying the foundation for effective security management in\nthe expected dynamic threat landscape of 6G networks. Through practical\nexamples, we illustrate the real-world application of our proposed methodology,\noffering valuable insights for developing risk management and informed\ndecision-making strategies in 5G and 6G security operations and laying the\nfoundation for effective security management in the expected dynamic threat\nlandscape of 6G networks."
    },
    {
        "date": "2025-05",
        "title": "Efficient Full-Stack Private Federated Deep Learning with Post-Quantum Security",
        "author": "Yiwei Zhang, Rouzbeh Behnia, Attila A. Yavuz, Reza Ebrahimi, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2505.05751v1",
        "abstract": "Federated learning (FL) enables collaborative model training while preserving\nuser data privacy by keeping data local. Despite these advantages, FL remains\nvulnerable to privacy attacks on user updates and model parameters during\ntraining and deployment. Secure aggregation protocols have been proposed to\nprotect user updates by encrypting them, but these methods often incur high\ncomputational costs and are not resistant to quantum computers. Additionally,\ndifferential privacy (DP) has been used to mitigate privacy leakages, but\nexisting methods focus on secure aggregation or DP, neglecting their potential\nsynergies. To address these gaps, we introduce Beskar, a novel framework that\nprovides post-quantum secure aggregation, optimizes computational overhead for\nFL settings, and defines a comprehensive threat model that accounts for a wide\nspectrum of adversaries. We also integrate DP into different stages of FL\ntraining to enhance privacy protection in diverse scenarios. Our framework\nprovides a detailed analysis of the trade-offs between security, performance,\nand model accuracy, representing the first thorough examination of secure\naggregation protocols combined with various DP approaches for post-quantum\nsecure FL. Beskar aims to address the pressing privacy and security issues FL\nwhile ensuring quantum-safety and robust performance."
    },
    {
        "date": "2025-05",
        "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
        "author": "Julia Shuieh, Prasann Singhal, Apaar Shanker, John Heyer, George Pu, and Samuel Denton",
        "link": "http://arxiv.org/abs/2505.05704v1",
        "abstract": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."
    },
    {
        "date": "2025-05",
        "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology",
        "author": "Fuyao Chen, Yuexi Du, Tal Zeevi, Nicha C. Dvornek, and John A. Onofrey",
        "link": "http://arxiv.org/abs/2505.05689v1",
        "abstract": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging."
    },
    {
        "date": "2025-05",
        "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval",
        "author": "Alexander Most, Joseph Winjum, Ayan Biswas, Shawn Jones, Nishath Rajiv Ranasinghe, Dan O'Malley, and Manish Bhattarai",
        "link": "http://arxiv.org/abs/2505.05666v1",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments."
    },
    {
        "date": "2025-05",
        "title": "On Corruption-Robustness in Performative Reinforcement Learning",
        "author": "Vasilis Pollatos, Debmalya Mandal, and Goran Radanovic",
        "link": "http://arxiv.org/abs/2505.05609v1",
        "abstract": "In performative Reinforcement Learning (RL), an agent faces a\npolicy-dependent environment: the reward and transition functions depend on the\nagent's policy. Prior work on performative RL has studied the convergence of\nrepeated retraining approaches to a performatively stable policy. In the finite\nsample regime, these approaches repeatedly solve for a saddle point of a\nconvex-concave objective, which estimates the Lagrangian of a regularized\nversion of the reinforcement learning problem. In this paper, we aim to extend\nsuch repeated retraining approaches, enabling them to operate under corrupted\ndata. More specifically, we consider Huber's $\\epsilon$-contamination model,\nwhere an $\\epsilon$ fraction of data points is corrupted by arbitrary\nadversarial noise. We propose a repeated retraining approach based on\nconvex-concave optimization under corrupted gradients and a novel\nproblem-specific robust mean estimator for the gradients. We prove that our\napproach exhibits last-iterate convergence to an approximately stable policy,\nwith the approximation error linear in $\\sqrt{\\epsilon}$. We experimentally\ndemonstrate the importance of accounting for corruption in performative RL."
    },
    {
        "date": "2025-05",
        "title": "QUIC-Exfil: Exploiting QUIC's Server Preferred Address Feature to Perform Data Exfiltration Attacks",
        "author": "Thomas Gr\u00fcbl, Weijie Niu, Jan von der Assen, and Burkhard Stiller",
        "link": "http://arxiv.org/abs/2505.05292v1",
        "abstract": "The QUIC protocol is now widely adopted by major tech companies and accounts\nfor a significant fraction of today's Internet traffic. QUIC's multiplexing\ncapabilities, encrypted headers, dynamic IP address changes, and encrypted\nparameter negotiations make the protocol not only more efficient, secure, and\ncensorship-resistant, but also practically unmanageable by firewalls. This\nopens doors for attackers who may exploit certain traits of the QUIC protocol\nto perform targeted attacks, such as data exfiltration attacks. Whereas\nexisting data exfiltration techniques, such as TLS and DNS-based exfiltration,\ncan be detected on a firewall level, QUIC-based data exfiltration is more\ndifficult to detect, since changes in IP addresses and ports are inherent to\nthe protocol's normal behavior. To show the feasibility of a QUIC-based data\nexfiltration attack, we introduce a novel method leveraging the server\npreferred address feature of the QUIC protocol and, thus, allows an attacker to\nexfiltrate sensitive data from an infected machine to a malicious server,\ndisguised as a server-side connection migration. The attack is implemented as a\nproof of concept tool in Rust. We evaluated the performance of five anomaly\ndetection classifiers - Random Forest, Multi-Layer Perceptron, Support Vector\nMachine, Autoencoder, and Isolation Forest - trained on datasets collected from\nthree network traffic scenarios. The classifiers were trained on over 700K\nbenign and malicious QUIC packets and 786 connection migration events, but were\nunable to detect the data exfiltration attempts. Furthermore, post-analysis of\nthe traffic captures did not reveal any identifiable fingerprint. As part of\nour evaluation, we also interviewed five leading firewall vendors and found\nthat, as of today, no major firewall vendor implements functionality capable of\ndistinguishing between benign and malicious QUIC connection migrations."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration",
        "author": "Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, and George Vouros",
        "link": "http://arxiv.org/abs/2505.05262v1",
        "abstract": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks."
    },
    {
        "date": "2025-05",
        "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality",
        "author": "Chara Podimata",
        "link": "http://arxiv.org/abs/2505.05211v1",
        "abstract": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems."
    },
    {
        "date": "2025-05",
        "title": "Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks",
        "author": "Yixin Cheng, Hongcheng Guo, Yangming Li, and Leonid Sigal",
        "link": "http://arxiv.org/abs/2505.05190v2",
        "abstract": "Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking."
    },
    {
        "date": "2025-05",
        "title": "PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting",
        "author": "Elad Feldman, Jacob Shams, Dudi Biton, Alfred Chen, Shaoyuan Xie, Satoru Koda, Yisroel Mirsky, Asaf Shabtai, Yuval Elovici, and Ben Nassi",
        "link": "http://arxiv.org/abs/2505.05183v1",
        "abstract": "The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection."
    },
    {
        "date": "2025-05",
        "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP",
        "author": "Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, and James Bailey",
        "link": "http://arxiv.org/abs/2505.05528v1",
        "abstract": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}."
    },
    {
        "date": "2025-05",
        "title": "Integrating Communication, Sensing, and Security: Progress and Prospects of PLS in ISAC Systems",
        "author": "Waqas Aman, El-Mehdi Illi, Marwa Qaraqe, and Saif Al-Kuwari",
        "link": "http://arxiv.org/abs/2505.05090v1",
        "abstract": "The sixth generation of wireless networks defined several key performance\nindicators (KPIs) for assessing its networks, mainly in terms of reliability,\ncoverage, and sensing. In this regard, remarkable attention has been paid\nrecently to the integrated sensing and communication (ISAC) paradigm as an\nenabler for efficiently and jointly performing communication and sensing using\nthe same spectrum and hardware resources. On the other hand, ensuring\ncommunication and data security has been an imperative requirement for wireless\nnetworks throughout their evolution. The physical-layer security (PLS) concept\npaved the way to catering to the security needs in wireless networks in a\nsustainable way while guaranteeing theoretically secure transmissions,\nindependently of the computational capacity of adversaries. Therefore, it is of\nparamount importance to consider a balanced trade-off between communication\nreliability, sensing, and security in future networks, such as the 5G and\nbeyond, and the 6G. In this paper, we provide a comprehensive and system-wise\nreview of designed secure ISAC systems from a PLS point of view. In particular,\nthe impact of various physical-layer techniques, schemes, and wireless\ntechnologies to ensure the sensing-security trade-off is studied from the\nsurveyed work. Furthermore, the amalgamation of PLS and ISAC is analyzed in a\nbroader impact by considering attacks targeting data confidentiality,\ncommunication covertness, and sensing spoofing. The paper also serves as a\ntutorial by presenting several theoretical foundations on ISAC and PLS, which\nrepresent a practical guide for readers to develop novel secure ISAC network\ndesigns."
    },
    {
        "date": "2025-05",
        "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
        "author": "Jaehyun Jeon, Jang Han Yoon, Min Soo Kim, Sumin Shim, Yejin Choi, Hanbin Kim, and Youngjae Yu",
        "link": "http://arxiv.org/abs/2505.05026v2",
        "abstract": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly."
    },
    {
        "date": "2025-05",
        "title": "Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights",
        "author": "Pawe\u0142 Walkowiak, Marek Klonowski, Marcin Oleksy, and Arkadiusz Janz",
        "link": "http://arxiv.org/abs/2505.07856v1",
        "abstract": "Various techniques are used in the generation of adversarial examples,\nincluding methods such as TextBugger which introduce minor, hardly visible\nperturbations to words leading to changes in model behaviour. Another class of\ntechniques involves substituting words with their synonyms in a way that\npreserves the text's meaning but alters its predicted class, with TextFooler\nbeing a prominent example of such attacks. Most adversarial example generation\nmethods are developed and evaluated primarily on non-inflectional languages,\ntypically English. In this work, we evaluate and explain how adversarial\nattacks perform in inflectional languages. To explain the impact of inflection\non model behaviour and its robustness under attack, we designed a novel\nprotocol inspired by mechanistic interpretability, based on Edge Attribution\nPatching (EAP) method. The proposed evaluation protocol relies on parallel\ntask-specific corpora that include both inflected and syncretic variants of\ntexts in two languages -- Polish and English. To analyse the models and explain\nthe relationship between inflection and adversarial robustness, we create a new\nbenchmark based on task-oriented dataset MultiEmo, enabling the identification\nof mechanistic inflection-related elements of circuits within the model and\nanalyse their behaviour under attack."
    },
    {
        "date": "2025-05",
        "title": "Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought",
        "author": "Mingfei Zeng, Ming Xie, Xixi Zheng, Chunhai Li, Chuan Zhang, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2505.06307v1",
        "abstract": "The rapid development of Internet of Things (IoT) technology has transformed\npeople's way of life and has a profound impact on both production and daily\nactivities. However, with the rapid advancement of IoT technology, the security\nof IoT devices has become an unavoidable issue in both research and\napplications. Although some efforts have been made to detect or mitigate IoT\nsecurity vulnerabilities, they often struggle to adapt to the complexity of IoT\nenvironments, especially when dealing with dynamic security scenarios. How to\nautomatically, efficiently, and accurately understand these vulnerabilities\nremains a challenge. To address this, we propose an IoT security assistant\ndriven by Large Language Model (LLM), which enhances the LLM's understanding of\nIoT security vulnerabilities and related threats. The aim of the ICoT method we\npropose is to enable the LLM to understand security issues by breaking down the\nvarious dimensions of security vulnerabilities and generating responses\ntailored to the user's specific needs and expertise level. By incorporating\nICoT, LLM can gradually analyze and reason through complex security scenarios,\nresulting in more accurate, in-depth, and personalized security recommendations\nand solutions. Experimental results show that, compared to methods relying\nsolely on LLM, our proposed LLM-driven IoT security assistant significantly\nimproves the understanding of IoT security issues through the ICoT approach and\nprovides personalized solutions based on the user's identity, demonstrating\nhigher accuracy and reliability."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Contextual Embedding for Robust Far-View Borehole Detection",
        "author": "Xuesong Liu, Tianyu Hao, and Emmett J. Ientilucci",
        "link": "http://arxiv.org/abs/2505.05008v1",
        "abstract": "In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios."
    },
    {
        "date": "2025-05",
        "title": "ChainMarks: Securing DNN Watermark with Cryptographic Chain",
        "author": "Brian Choi, Shu Wang, Isabelle Choi, and Kun Sun",
        "link": "http://arxiv.org/abs/2505.04977v1",
        "abstract": "With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy."
    },
    {
        "date": "2025-05",
        "title": "ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators",
        "author": "Ziyu Zhang, Sergey Bashkirov, Dun Yang, Michael Taylor, and Xue Bin Peng",
        "link": "http://arxiv.org/abs/2505.04961v1",
        "abstract": "Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w."
    },
    {
        "date": "2025-05",
        "title": "RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models",
        "author": "Zhenhua Xu, Zhebo Wang, Maike Li, Wenpeng Xing, Chunqiang Hu, Chen Zhi, and Meng Han",
        "link": "http://arxiv.org/abs/2505.06304v1",
        "abstract": "Recent advances in large language models (LLMs) have underscored the\nimportance of safeguarding intellectual property rights through robust\nfingerprinting techniques. Traditional fingerprint verification approaches\ntypically focus on a single model, seeking to improve the robustness of its\nfingerprint.However, these single-model methods often struggle to capture\nintrinsic commonalities across multiple related models. In this paper, we\npropose RAP-SM (Robust Adversarial Prompt via Shadow Models), a novel framework\nthat extracts a public fingerprint for an entire series of LLMs. Experimental\nresults demonstrate that RAP-SM effectively captures the intrinsic\ncommonalities among different models while exhibiting strong adversarial\nrobustness. Our findings suggest that RAP-SM presents a valuable avenue for\nscalable fingerprint verification, offering enhanced protection against\npotential model breaches in the era of increasingly prevalent LLMs."
    },
    {
        "date": "2025-05",
        "title": "Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition",
        "author": "Xiaozhou Ye, and Kevin I-Kai Wang",
        "link": "http://arxiv.org/abs/2505.06301v1",
        "abstract": "Cross-user variability in Human Activity Recognition (HAR) remains a critical\nchallenge due to differences in sensor placement, body dynamics, and behavioral\npatterns. Traditional methods often fail to capture biomechanical invariants\nthat persist across users, limiting their generalization capability. We propose\nan Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)\nframework that integrates anatomical correlation knowledge into a unified graph\nneural network (GNN) architecture. By modeling three biomechanically motivated\nrelationships together-Interconnected Units, Analogous Units, and Lateral\nUnits-our method encodes domain-invariant features while addressing\nuser-specific variability through Variational Edge Feature Extractor. A\nGradient Reversal Layer (GRL) enforces adversarial domain generalization,\nensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and\nDSADS datasets demonstrate state-of-the-art performance. Our work bridges\nbiomechanical principles with graph-based adversarial learning by integrating\ninformation fusion techniques. This fusion of information underpins our unified\nand generalized model for cross-user HAR."
    },
    {
        "date": "2025-05",
        "title": "CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability",
        "author": "Taisuke Kobayashi",
        "link": "http://arxiv.org/abs/2505.04897v1",
        "abstract": "Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction."
    },
    {
        "date": "2025-05",
        "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on Memory",
        "author": "MD Mahady Hassan, Shanto Roy, and Reza Rahaeimehr",
        "link": "http://arxiv.org/abs/2505.04896v1",
        "abstract": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
    },
    {
        "date": "2025-05",
        "title": "FedRE: Robust and Effective Federated Learning with Privacy Preference",
        "author": "Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, and Ruixuan Li",
        "link": "http://arxiv.org/abs/2505.04889v1",
        "abstract": "Despite Federated Learning (FL) employing gradient aggregation at the server\nfor distributed training to prevent the privacy leakage of raw data, private\ninformation can still be divulged through the analysis of uploaded gradients\nfrom clients. Substantial efforts have been made to integrate local\ndifferential privacy (LDP) into the system to achieve a strict privacy\nguarantee. However, existing methods fail to take practical issues into account\nby merely perturbing each sample with the same mechanism while each client may\nhave their own privacy preferences on privacy-sensitive information (PSI),\nwhich is not uniformly distributed across the raw data. In such a case,\nexcessive privacy protection from private-insensitive information can\nadditionally introduce unnecessary noise, which may degrade the model\nperformance. In this work, we study the PSI within data and develop FedRE, that\ncan simultaneously achieve robustness and effectiveness benefits with LDP\nprotection. More specifically, we first define PSI with regard to the privacy\npreferences of each client. Then, we optimize the LDP by allocating less\nprivacy budget to gradients with higher PSI in a layer-wise manner, thus\nproviding a stricter privacy guarantee for PSI. Furthermore, to mitigate the\nperformance degradation caused by LDP, we design a parameter aggregation\nmechanism based on the distribution of the perturbed information. We conducted\nexperiments with text tamper detection on T-SROIE and DocTamper datasets, and\nFedRE achieves competitive performance compared to state-of-the-art methods."
    },
    {
        "date": "2025-05",
        "title": "Robust ML Auditing using Prior Knowledge",
        "author": "Jade Garcia Bourr\u00e9e, Augustin Godinot, Martijn De Vos, Milos Vujasinovic, Sayan Biswas, Gilles Tredan, Erwan Le Merrer, and Anne-Marie Kermarrec",
        "link": "http://arxiv.org/abs/2505.04796v1",
        "abstract": "The rapid adoption of ML decision-making systems across products and services\nhas led to a set of regulations on how such systems should behave and be built.\nAmong all the technical challenges to enforcing these regulations, one crucial,\nyet under-explored problem is the risk of manipulation while these systems are\nbeing audited for fairness. This manipulation occurs when a platform\ndeliberately alters its answers to a regulator to pass an audit without\nmodifying its answers to other users. In this paper, we introduce a novel\napproach to manipulation-proof auditing by taking into account the auditor's\nprior knowledge of the task solved by the platform. We first demonstrate that\nregulators must not rely on public priors (e.g. a public dataset), as platforms\ncould easily fool the auditor in such cases. We then formally establish the\nconditions under which an auditor can prevent audit manipulations using prior\nknowledge about the ground truth. Finally, our experiments with two standard\ndatasets exemplify the maximum level of unfairness a platform can hide before\nbeing detected as malicious. Our formalization and generalization of\nmanipulation-proof auditing with a prior opens up new research directions for\nmore robust fairness audits."
    },
    {
        "date": "2025-05",
        "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World",
        "author": "Bangyan Liao, Zhenjun Zhao, Haoang Li, Yi Zhou, Yingping Zeng, Hao Li, and Peidong Liu",
        "link": "http://arxiv.org/abs/2505.04788v1",
        "abstract": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental\ntask in many 3D vision applications, consists of jointly inferring the line-VP\nassociation and locating each VP. Existing methods are, however, either\nsub-optimal solvers or pursuing global optimality at a significant cost of\ncomputing time. In contrast to prior works, we introduce convex relaxation\ntechniques to solve this task for the first time. Specifically, we employ a\n``soft'' association scheme, realized via a truncated multi-selection error,\nthat allows for joint estimation of VPs' locations and line-VP associations.\nThis approach leads to a primal problem that can be reformulated into a\nquadratically constrained quadratic programming (QCQP) problem, which is then\nrelaxed into a convex semidefinite programming (SDP) problem. To solve this SDP\nproblem efficiently, we present a globally optimal outlier-robust iterative\nsolver (called \\textbf{GlobustVP}), which independently searches for one VP and\nits associated lines in each iteration, treating other lines as outliers. After\neach independent update of all VPs, the mutual orthogonality between the three\nVPs in a Manhattan world is reinforced via local refinement. Extensive\nexperiments on both synthetic and real-world data demonstrate that\n\\textbf{GlobustVP} achieves a favorable balance between efficiency, robustness,\nand global optimality compared to previous works. The code is publicly\navailable at https://github.com/WU-CVGL/GlobustVP."
    },
    {
        "date": "2025-05",
        "title": "Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain",
        "author": "Spyridon Raptis, and Haralampos-G. Stratigopoulos",
        "link": "http://arxiv.org/abs/2505.06299v1",
        "abstract": "As Spiking Neural Networks (SNNs) gain traction across various applications,\nunderstanding their security vulnerabilities becomes increasingly important. In\nthis work, we focus on the adversarial attacks, which is perhaps the most\nconcerning threat. An adversarial attack aims at finding a subtle input\nperturbation to fool the network's decision-making. We propose two novel\nadversarial attack algorithms for SNNs: an input-specific attack that crafts\nadversarial samples from specific dataset inputs and a universal attack that\ngenerates a reusable patch capable of inducing misclassification across most\ninputs, thus offering practical feasibility for real-time deployment. The\nalgorithms are gradient-based operating in the spiking domain proving to be\neffective across different evaluation metrics, such as adversarial accuracy,\nstealthiness, and generation time. Experimental results on two widely used\nneuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our\nproposed attacks surpass in all metrics all existing state-of-the-art methods.\nAdditionally, we present the first demonstration of adversarial attack\ngeneration in the sound domain using the SHD dataset."
    },
    {
        "date": "2025-05",
        "title": "Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting",
        "author": "Shai Feldman, Stephen Bates, and Yaniv Romano",
        "link": "http://arxiv.org/abs/2505.04733v1",
        "abstract": "We introduce a framework for robust uncertainty quantification in situations\nwhere labeled training data are corrupted, through noisy or missing labels. We\nbuild on conformal prediction, a statistical tool for generating prediction\nsets that cover the test label with a pre-specified probability. The validity\nof conformal prediction, however, holds under the i.i.d assumption, which does\nnot hold in our setting due to the corruptions in the data. To account for this\ndistribution shift, the privileged conformal prediction (PCP) method proposed\nleveraging privileged information (PI) -- additional features available only\nduring training -- to re-weight the data distribution, yielding valid\nprediction sets under the assumption that the weights are accurate. In this\nwork, we analyze the robustness of PCP to inaccuracies in the weights. Our\nanalysis indicates that PCP can still yield valid uncertainty estimates even\nwhen the weights are poorly estimated. Furthermore, we introduce uncertain\nimputation (UI), a new conformal method that does not rely on weight\nestimation. Instead, we impute corrupted labels in a way that preserves their\nuncertainty. Our approach is supported by theoretical guarantees and validated\nempirically on both synthetic and real benchmarks. Finally, we show that these\ntechniques can be integrated into a triply robust framework, ensuring\nstatistically valid predictions as long as at least one underlying method is\nvalid."
    },
    {
        "date": "2025-05",
        "title": "Qualitative Analysis of $\u03c9$-Regular Objectives on Robust MDPs",
        "author": "Ali Asadi, Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, and Ali Shafiee",
        "link": "http://arxiv.org/abs/2505.04539v1",
        "abstract": "Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states."
    },
    {
        "date": "2025-05",
        "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation",
        "author": "Edward Humes, Xiaomin Lin, Uttej Kallakuri, and Tinoosh Mohsenin",
        "link": "http://arxiv.org/abs/2505.04529v1",
        "abstract": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU."
    },
    {
        "date": "2025-05",
        "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective Encryption",
        "author": "Mohammad Waquas Usmani, Susmit Shannigrahi, and Michael Zink",
        "link": "http://arxiv.org/abs/2505.04466v1",
        "abstract": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
    },
    {
        "date": "2025-05",
        "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
        "author": "Xiaoyu Xu, Minxin Du, Qingqing Ye, and Haibo Hu",
        "link": "http://arxiv.org/abs/2505.04416v1",
        "abstract": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."
    },
    {
        "date": "2025-05",
        "title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning",
        "author": "Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, and Philip S. Yu",
        "link": "http://arxiv.org/abs/2505.04339v1",
        "abstract": "DBSCAN, a well-known density-based clustering algorithm, has gained\nwidespread popularity and usage due to its effectiveness in identifying\nclusters of arbitrary shapes and handling noisy data. However, it encounters\nchallenges in producing satisfactory cluster results when confronted with\ndatasets of varying density scales, a common scenario in real-world\napplications. In this paper, we propose a novel Adaptive and Robust DBSCAN with\nMulti-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,\nwe model the initial dataset as a two-level encoding tree and categorize the\ndata vertices into distinct density partitions according to the information\nuncertainty determined in the encoding tree. Each partition is then assigned to\nan agent to find the best clustering parameters without manual assistance. The\nallocation is density-adaptive, enabling AR-DBSCAN to effectively handle\ndiverse density distributions within the dataset by utilizing distinct agents\nfor different partitions. Second, a multi-agent deep reinforcement learning\nguided automatic parameter searching process is designed. The process of\nadjusting the parameter search direction by perceiving the clustering\nenvironment is modeled as a Markov decision process. Using a weakly-supervised\nreward training policy network, each agent adaptively learns the optimal\nclustering parameters by interacting with the clusters. Third, a recursive\nsearch mechanism adaptable to the data's scale is presented, enabling efficient\nand controlled exploration of large parameter spaces. Extensive experiments are\nconducted on nine artificial datasets and a real-world dataset. The results of\noffline and online tasks show that AR-DBSCAN not only improves clustering\naccuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,\nbut also is capable of robustly finding dominant parameters."
    },
    {
        "date": "2025-05",
        "title": "Guardians of the Web: The Evolution and Future of Website Information Security",
        "author": "Md Saiful Islam, and Li Xiangdong",
        "link": "http://arxiv.org/abs/2505.04308v1",
        "abstract": "Website information security has become a critical concern in the digital\nage. This article explores the evolution of website information security,\nexamining its historical development, current practices, and future directions.\nThe early beginnings from the 1960s to the 1980s laid the groundwork for modern\ncybersecurity, with the development of ARPANET, TCP/IP, public-key\ncryptography, and the first antivirus programs. The 1990s marked a\ntransformative era, driven by the commercialization of the Internet and the\nemergence of web-based services. As the Internet grew, so did the range and\nsophistication of cyber threats, leading to advancements in security\ntechnologies such as the Secure Sockets Layer (SSL) protocol, password\nprotection, and firewalls. Current practices in website information security\ninvolve a multi-layered approach, including encryption, secure coding\npractices, regular security audits, and user education. The future of website\ninformation security is expected to be shaped by emerging technologies such as\nartificial intelligence, blockchain, and quantum computing, as well as the\nincreasing importance of international cooperation and standardization efforts.\nAs cyber threats continue to evolve, ongoing research and innovation in website\ninformation security will be essential to protect sensitive information and\nmaintain trust in the digital world."
    },
    {
        "date": "2025-05",
        "title": "Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets",
        "author": "Mateo Lopez-Ledezma, and Gissel Velarde",
        "link": "http://arxiv.org/abs/2505.04204v1",
        "abstract": "Cybersecurity has become essential worldwide and at all levels, concerning\nindividuals, institutions, and governments. A basic principle in cybersecurity\nis to be always alert. Therefore, automation is imperative in processes where\nthe volume of daily operations is large. Several cybersecurity applications can\nbe addressed as binary classification problems, including anomaly detection,\nfraud detection, intrusion detection, spam detection, or malware detection. We\npresent three experiments. In the first experiment, we evaluate single\nclassifiers including Random Forests, Light Gradient Boosting Machine, eXtreme\nGradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting\nDecision Tree. In the second experiment, we test different sampling techniques\nincluding over-sampling, under-sampling, Synthetic Minority Over-sampling\nTechnique, and Self-Paced Ensembling. In the last experiment, we evaluate\nSelf-Paced Ensembling and its number of base classifiers. We found that\nimbalance learning techniques had positive and negative effects, as reported in\nrelated studies. Thus, these techniques should be applied with caution.\nBesides, we found different best performers for each dataset. Therefore, we\nrecommend testing single classifiers and imbalance learning techniques for each\nnew dataset and application involving imbalanced datasets as is the case in\nseveral cyber security applications."
    },
    {
        "date": "2025-05",
        "title": "Trajectory Entropy Reinforcement Learning for Predictable and Robust Control",
        "author": "Bang You, Chenxu Wang, and Huaping Liu",
        "link": "http://arxiv.org/abs/2505.04193v1",
        "abstract": "Simplicity is a critical inductive bias for designing data-driven\ncontrollers, especially when robustness is important. Despite the impressive\nresults of deep reinforcement learning in complex control tasks, it is prone to\ncapturing intricate and spurious correlations between observations and actions,\nleading to failure under slight perturbations to the environment. To tackle\nthis problem, in this work we introduce a novel inductive bias towards simple\npolicies in reinforcement learning. The simplicity inductive bias is introduced\nby minimizing the entropy of entire action trajectories, corresponding to the\nnumber of bits required to describe information in action trajectories after\nthe agent observes state trajectories. Our reinforcement learning agent,\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\ntrajectory entropy while maximizing rewards. We show that the trajectory\nentropy can be effectively estimated by learning a variational parameterized\naction prediction model, and use the prediction model to construct an\ninformation-regularized reward function. Furthermore, we construct a practical\nalgorithm that enables the joint optimization of models, including the policy\nand the prediction model. Experimental evaluations on several high-dimensional\nlocomotion tasks show that our learned policies produce more cyclical and\nconsistent action trajectories, and achieve superior performance, and\nrobustness to noise and dynamic changes than the state-of-the-art."
    },
    {
        "date": "2025-05",
        "title": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling",
        "author": "AbdulAziz AbdulGhaffar, and Ashraf Matrawy",
        "link": "http://arxiv.org/abs/2505.04101v1",
        "abstract": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases."
    },
    {
        "date": "2025-05",
        "title": "Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks",
        "author": "Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, and Dezhong Peng",
        "link": "http://arxiv.org/abs/2505.04046v1",
        "abstract": "Recently, trustworthy multi-view learning has attracted extensive attention\nbecause evidence learning can provide reliable uncertainty estimation to\nenhance the credibility of multi-view predictions. Existing trusted multi-view\nlearning methods implicitly assume that multi-view data is secure. In practice,\nhowever, in safety-sensitive applications such as autonomous driving and\nsecurity monitoring, multi-view data often faces threats from adversarial\nperturbations, thereby deceiving or disrupting multi-view learning models. This\ninevitably leads to the adversarial unreliability problem (AUP) in trusted\nmulti-view learning. To overcome this tricky problem, we propose a novel\nmulti-view learning framework, namely Reliable Disentanglement Multi-view\nLearning (RDML). Specifically, we first propose evidential disentanglement\nlearning to decompose each view into clean and adversarial parts under the\nguidance of corresponding evidences, which is extracted by a pretrained\nevidence extractor. Then, we employ the feature recalibration module to\nmitigate the negative impact of adversarial perturbations and extract potential\ninformative features from them. Finally, to further ignore the irreparable\nadversarial interferences, a view-level evidential attention mechanism is\ndesigned. Extensive experiments on multi-view classification tasks with\nadversarial attacks show that our RDML outperforms the state-of-the-art\nmulti-view learning methods by a relatively large margin."
    },
    {
        "date": "2025-05",
        "title": "MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models",
        "author": "Soheil Zibakhsh Shabgahi, Yaman Jandali, and Farinaz Koushanfar",
        "link": "http://arxiv.org/abs/2505.04015v1",
        "abstract": "This paper proposes MergeGuard, a novel methodology for mitigation of AI\nTrojan attacks. Trojan attacks on AI models cause inputs embedded with triggers\nto be misclassified to an adversary's target class, posing a significant threat\nto model usability trained by an untrusted third party. The core of MergeGuard\nis a new post-training methodology for linearizing and merging fully connected\nlayers which we show simultaneously improves model generalizability and\nperformance. Our Proof of Concept evaluation on Transformer models demonstrates\nthat MergeGuard maintains model accuracy while decreasing trojan attack success\nrate, outperforming commonly used (post-training) Trojan mitigation by\nfine-tuning methodologies."
    },
    {
        "date": "2025-05",
        "title": "Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]",
        "author": "David Chu, Aditya Balasubramanian, Dee Bao, Natacha Crooks, Heidi Howard, Lucky E. Katahanas, and Soujanya Ponnapalli",
        "link": "http://arxiv.org/abs/2505.04014v1",
        "abstract": "Today, users can \"lift-and-shift\" unmodified applications into modern,\nVM-based Trusted Execution Environments (TEEs) in order to gain hardware-based\nsecurity guarantees. However, TEEs do not protect applications against disk\nrollback attacks, where persistent storage can be reverted to an earlier state\nafter a crash; existing rollback resistance solutions either only support a\nsubset of applications or require code modification. Our key insight is that\nrestoring disk consistency after a rollback attack guarantees rollback\nresistance for any application. We present Rollbaccine, a device mapper that\nprovides automatic rollback resistance for all applications by provably\npreserving disk consistency. Rollbaccine intercepts and replicates writes to\ndisk, restores lost state from backups during recovery, and minimizes overheads\nby taking advantage of the weak, multi-threaded semantics of disk operations.\nAcross benchmarks over two real applications (PostgreSQL and HDFS) and two file\nsystems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the\nfsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the\nstate-of-the-art, non-automatic rollback resistant solution by $208\\times$."
    },
    {
        "date": "2025-05",
        "title": "AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience",
        "author": "Shamnad Mohamed Shaffi, Sunish Vengathattil, Jezeena Nikarthil Sidhick, and Resmi Vijayan",
        "link": "http://arxiv.org/abs/2505.03945v1",
        "abstract": "Cloud security concerns have been greatly realized in recent years due to the\nincrease of complicated threats in the computing world. Many traditional\nsolutions do not work well in real-time to detect or prevent more complex\nthreats. Artificial intelligence is today regarded as a revolution in\ndetermining a protection plan for cloud data architecture through machine\nlearning, statistical visualization of computing infrastructure, and detection\nof security breaches followed by counteraction. These AI-enabled systems make\nwork easier as more network activities are scrutinized, and any anomalous\nbehavior that might be a precursor to a more serious breach is prevented. This\npaper examines ways AI can enhance cloud security by applying predictive\nanalytics, behavior-based security threat detection, and AI-stirring\nencryption. It also outlines the problems of the previous security models and\nhow AI overcomes them. For a similar reason, issues like data privacy, biases\nin the AI model, and regulatory compliance are also covered. So, AI improves\nthe protection of cloud computing contexts; however, more efforts are needed in\nthe subsequent phases to extend the technology's reliability, modularity, and\nethical aspects. This means that AI can be blended with other new computing\ntechnologies, including blockchain, to improve security frameworks further. The\npaper discusses the current trends in securing cloud data architecture using AI\nand presents further research and application directions."
    },
    {
        "date": "2025-05",
        "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders",
        "author": "Chethan Krishnamurthy Ramanaik, Arjun Roy, and Eirini Ntoutsi",
        "link": "http://arxiv.org/abs/2505.03646v1",
        "abstract": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples."
    },
    {
        "date": "2025-05",
        "title": "Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection",
        "author": "Fangling Jiang, Qi Li, Bing Liu, Weining Wang, Caifeng Shan, Zhenan Sun, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2505.03610v1",
        "abstract": "3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets."
    },
    {
        "date": "2025-05",
        "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets",
        "author": "Charita Dellaporta, Patrick O'Hara, and Theodoros Damoulas",
        "link": "http://arxiv.org/abs/2505.03585v1",
        "abstract": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification."
    },
    {
        "date": "2025-05",
        "title": "LlamaFirewall: An open source guardrail system for building secure AI agents",
        "author": "Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, Alekhya Gampa, Beto de Paola, Dominik Gabi, James Crnkovich, Jean-Christophe Testud, Kat He, Rashnil Chaturvedi, Wu Zhou, and Joshua Saxe",
        "link": "http://arxiv.org/abs/2505.03574v1",
        "abstract": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails."
    },
    {
        "date": "2025-05",
        "title": "Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication",
        "author": "Chenguang Liu, Jianjun Chen, Yunfei Chen, Yubei He, Zhuangkun Wei, Hongjian Sun, Haiyan Lu, and Qi Hao",
        "link": "http://arxiv.org/abs/2505.03528v1",
        "abstract": "Cooperative perception, leveraging shared information from multiple vehicles\nvia vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous\ndriving to alleviate the limitation of single-vehicle perception. Existing\nworks have explored the effects of V2V communication impairments on perception\nprecision, but they lack generalization to different levels of impairments. In\nthis work, we propose a joint weighting and denoising framework, Coop-WD, to\nenhance cooperative perception subject to V2V channel impairments. In this\nframework, the self-supervised contrastive model and the conditional diffusion\nprobabilistic model are adopted hierarchically for vehicle-level and\npixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is\nproposed to selectively deactivate denoising to reduce processing overhead.\nRician fading, non-stationarity, and time-varying distortion are considered.\nSimulation results demonstrate that the proposed Coop-WD outperforms\nconventional benchmarks in all types of channels. Qualitative analysis with\nvisual examples further proves the superiority of our proposed method. The\nproposed Coop-WD-eco achieves up to 50% reduction in computational cost under\nsevere distortion while maintaining comparable accuracy as channel conditions\nimprove."
    },
    {
        "date": "2025-05",
        "title": "Uncovering the Limitations of Model Inversion Evaluation -- Benchmarks and Connection to Type-I Adversarial Attacks",
        "author": "Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, and Ngai-Man Cheung",
        "link": "http://arxiv.org/abs/2505.03519v2",
        "abstract": "Model Inversion (MI) attacks aim to reconstruct information of private\ntraining data by exploiting access to machine learning models. The most common\nevaluation framework for MI attacks/defenses relies on an evaluation model that\nhas been utilized to assess progress across almost all MI attacks and defenses\nproposed in recent years. In this paper, for the first time, we present an\nin-depth study of MI evaluation. Firstly, we construct the first comprehensive\nhuman-annotated dataset of MI attack samples, based on 28 setups of different\nMI attacks, defenses, private and public datasets. Secondly, using our dataset,\nwe examine the accuracy of the MI evaluation framework and reveal that it\nsuffers from a significant number of false positives. These findings raise\nquestions about the previously reported success rates of SOTA MI attacks.\nThirdly, we analyze the causes of these false positives, design controlled\nexperiments, and discover the surprising effect of Type I adversarial features\non MI evaluation, as well as adversarial transferability, highlighting a\nrelationship between two previously distinct research areas. Our findings\nsuggest that the performance of SOTA MI attacks has been overestimated, with\nthe actual privacy leakage being significantly less than previously reported.\nIn conclusion, we highlight critical limitations in the widely used MI\nevaluation framework and present our methods to mitigate false positive rates.\nWe remark that prior research has shown that Type I adversarial attacks are\nvery challenging, with no existing solution. Therefore, we urge to consider\nhuman evaluation as a primary MI evaluation framework rather than merely a\nsupplement as in previous MI research. We also encourage further work on\ndeveloping more robust and reliable automatic evaluation frameworks."
    },
    {
        "date": "2025-05",
        "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models",
        "author": "Zihan Wang, Hongwei Li, Rui Zhang, Wenbo Jiang, Kangjie Chen, Tianwei Zhang, Qingchuan Zhao, and Guowen Xu",
        "link": "http://arxiv.org/abs/2505.03501v1",
        "abstract": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness"
    },
    {
        "date": "2025-05",
        "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)",
        "author": "Faiz Taleb, Ivan Gazeau, and Maryline Laurent",
        "link": "http://arxiv.org/abs/2505.03490v1",
        "abstract": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models."
    },
    {
        "date": "2025-05",
        "title": "Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in Voice Authentication Systems",
        "author": "Alireza Mohammadi, Keshav Sood, Dhananjay Thiruvady, and Asef Nazari",
        "link": "http://arxiv.org/abs/2505.03455v1",
        "abstract": "Voice authentication systems remain susceptible to two major threats:\nbackdoor triggered attacks and targeted data poisoning attacks. This dual\nvulnerability is critical because conventional solutions typically address each\nthreat type separately, leaving systems exposed to adversaries who can exploit\nboth attacks simultaneously. We propose a unified defense framework that\neffectively addresses both BTA and TDPA. Our framework integrates a frequency\nfocused detection mechanism that flags covert pitch boosting and sound masking\nbackdoor attacks in near real time, followed by a convolutional neural network\nthat addresses TDPA. This dual layered defense approach utilizes\nmultidimensional acoustic features to isolate anomalous signals without\nrequiring costly model retraining. In particular, our PBSM detection mechanism\ncan seamlessly integrate into existing voice authentication pipelines and scale\neffectively for large scale deployments. Experimental results on benchmark\ndatasets and their compression with the state of the art algorithm demonstrate\nthat our PBSM detection mechanism outperforms the state of the art. Our\nframework reduces attack success rates to as low as five to fifteen percent\nwhile maintaining a recall rate of up to ninety five percent in recognizing\nTDPA."
    },
    {
        "date": "2025-05",
        "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis",
        "author": "Fouad Trad, and Ali Chehab",
        "link": "http://arxiv.org/abs/2505.03451v1",
        "abstract": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses."
    },
    {
        "date": "2025-05",
        "title": "Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks",
        "author": "Sun Haoxuan, Hong Yan, Zhan Jiahui, Chen Haoxing, Lan Jun, Zhu Huijia, Wang Weiqiang, Zhang Liqing, and Zhang Jianfu",
        "link": "http://arxiv.org/abs/2505.03435v1",
        "abstract": "The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification."
    },
    {
        "date": "2025-05",
        "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense",
        "author": "Kirill Lukyanov, Mikhail Drobyshevskiy, Georgii Sazonov, Mikhail Soloviov, and Ilya Makarov",
        "link": "http://arxiv.org/abs/2505.03424v1",
        "abstract": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}"
    },
    {
        "date": "2025-05",
        "title": "Automatic Calibration for Membership Inference Attack on Large Language Models",
        "author": "Saleh Zare Zade, Yao Qiang, Xiangyu Zhou, Hui Zhu, Mohammad Amin Roshani, Prashant Khanduri, and Dongxiao Zhu",
        "link": "http://arxiv.org/abs/2505.03392v1",
        "abstract": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}."
    },
    {
        "date": "2025-05",
        "title": "Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples",
        "author": "Jian-Wei Li, and Wen-Ze Shao",
        "link": "http://arxiv.org/abs/2505.03383v1",
        "abstract": "Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods."
    },
    {
        "date": "2025-05",
        "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection",
        "author": "June-Woo Kim, Haram Yoon, Wonkyo Oh, Dawoon Jung, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, and Chan-Mo Yang",
        "link": "http://arxiv.org/abs/2505.03359v1",
        "abstract": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment."
    },
    {
        "date": "2025-05",
        "title": "A Chaos Driven Metric for Backdoor Attack Detection",
        "author": "Hema Karnam Surendrababu, and Nithin Nagaraj",
        "link": "http://arxiv.org/abs/2505.03208v1",
        "abstract": "The advancement and adoption of Artificial Intelligence (AI) models across\ndiverse domains have transformed the way we interact with technology. However,\nit is essential to recognize that while AI models have introduced remarkable\nadvancements, they also present inherent challenges such as their vulnerability\nto adversarial attacks. The current work proposes a novel defense mechanism\nagainst one of the most significant attack vectors of AI models - the backdoor\nattack via data poisoning of training datasets. In this defense technique, an\nintegrated approach that combines chaos theory with manifold learning is\nproposed. A novel metric - Precision Matrix Dependency Score (PDS) that is\nbased on the conditional variance of Neurochaos features is formulated. The PDS\nmetric has been successfully evaluated to distinguish poisoned samples from\nnon-poisoned samples across diverse datasets."
    },
    {
        "date": "2025-05",
        "title": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks",
        "author": "Qi Qin, Xinye Cao, Guoshun Nan, Sihan Chen, Rushan Li, Li Su, Haitao Du, Qimei Cui, Pengxuan Mao, Xiaofeng Tao, and Tony Q. S. Quek",
        "link": "http://arxiv.org/abs/2505.03161v2",
        "abstract": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community."
    },
    {
        "date": "2025-05",
        "title": "Robust Fairness Vision-Language Learning for Medical Image Analysis",
        "author": "Sparsh Bansal, Mingyang Wu, Xin Wang, and Shu Hu",
        "link": "http://arxiv.org/abs/2505.03153v1",
        "abstract": "The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC."
    },
    {
        "date": "2025-05",
        "title": "Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models",
        "author": "Hoang Cuong Nguyen, Shahroz Tariq, Mohan Baruwal Chhetri, and Bao Quoc Vo",
        "link": "http://arxiv.org/abs/2505.03147v1",
        "abstract": "This work evaluates the performance of Cyber Threat Intelligence (CTI)\nextraction methods in identifying attack techniques from threat reports\navailable on the web using the MITRE ATT&CK framework. We analyse four\nconfigurations utilising state-of-the-art tools, including the Threat Report\nATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as\nLlama2. Our findings reveal significant challenges, including class imbalance,\noverfitting, and domain-specific complexity, which impede accurate technique\nextraction. To mitigate these issues, we propose a novel two-step pipeline:\nfirst, an LLM summarises the reports, and second, a retrained SciBERT model\nprocesses a rebalanced dataset augmented with LLM-generated data. This approach\nachieves an improvement in F1-scores compared to baseline models, with several\nattack techniques surpassing an F1-score of 0.90. Our contributions enhance the\nefficiency of web-based CTI systems and support collaborative cybersecurity\noperations in an interconnected digital landscape, paving the way for future\nresearch on integrating human-AI collaboration platforms."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems",
        "author": "Abdul Mustafa, Muhammad Talha Khan, Muhammad Azmi Umer, Zaki Masood, and Chuadhry Mujeeb Ahmed",
        "link": "http://arxiv.org/abs/2505.03120v1",
        "abstract": "Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable\nto adversarial attacks. It is crucial for an IDS to learn to recognize\nadversarial examples before malicious entities exploit them. In this paper, we\ngenerated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We\nvalidate the generalization and scalability of the adversarial samples to\ntackle a broad range of real attacks on Industrial Control Systems (ICS). We\nevaluated the impact by assessing multiple attacks generated using the proposed\nmethod. The model trained with adversarial samples detected attacks with 95%\naccuracy on real-world attack data not used during training. The study was\nconducted using an operational secure water treatment (SWaT) testbed."
    },
    {
        "date": "2025-05",
        "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering",
        "author": "Joshua Owotogbe",
        "link": "http://arxiv.org/abs/2505.03096v1",
        "abstract": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey",
        "author": "Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, and Aman Raj",
        "link": "http://arxiv.org/abs/2505.03084v1",
        "abstract": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world."
    },
    {
        "date": "2025-05",
        "title": "Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles",
        "author": "Hanlin Chen, Simin Chen, Wenyu Li, Wei Yang, and Yiheng Feng",
        "link": "http://arxiv.org/abs/2505.03850v1",
        "abstract": "As a safety-critical cyber-physical system, cybersecurity and related safety\nissues for Autonomous Vehicles (AVs) have been important research topics for a\nwhile. Among all the modules on AVs, perception is one of the most accessible\nattack surfaces, as drivers and AVs have no control over the outside\nenvironment. Most current work targeting perception security for AVs focuses on\nperception correctness. In this work, we propose an impact analysis based on\ninference time attacks for autonomous vehicles. We demonstrate in a simulation\nsystem that such inference time attacks can also threaten the safety of both\nthe ego vehicle and other traffic participants."
    },
    {
        "date": "2025-05",
        "title": "AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks",
        "author": "Ilyas Oulkadda, and Julien Perez",
        "link": "http://arxiv.org/abs/2505.06267v1",
        "abstract": "The widespread adoption of Large Language Models (LLMs) for code generation,\nexemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM\nto assist in code completion tasks} surpassing a million users, highlights the\ntransformative potential of these tools in improving developer productivity.\nHowever, this rapid growth also underscores critical concerns regarding the\nquality, safety, and reliability of the code they generate. As Code-LLMs\nevolve, they face significant challenges, including the diminishing returns of\nmodel scaling and the scarcity of new, high-quality training data. To address\nthese issues, this paper introduces Adversarial Knowledge Distillation (AKD), a\nnovel approach that leverages adversarially generated synthetic datasets to\ndistill the capabilities of larger models into smaller, more efficient ones. By\nsystematically stress-testing and refining the reasoning capabilities of\nCode-LLMs, AKD provides a framework for enhancing model robustness,\nreliability, and security while improving their parameter-efficiency. We\nbelieve this work represents a critical step toward ensuring dependable\nautomated code generation within the constraints of existing data and the\ncost-efficiency of model execution."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation",
        "author": "Anjila Budathoki, and Manish Dhakal",
        "link": "http://arxiv.org/abs/2505.02971v1",
        "abstract": "Adversarial attacks have been fairly explored for computer vision and\nvision-language models. However, the avenue of adversarial attack for the\nvision language segmentation models (VLSMs) is still under-explored, especially\nfor medical image analysis.\n  Thus, we have investigated the robustness of VLSMs against adversarial\nattacks for 2D medical images with different modalities with radiology,\nphotography, and endoscopy. The main idea of this project was to assess the\nrobustness of the fine-tuned VLSMs specially in the medical domain setting to\naddress the high risk scenario.\n  First, we have fine-tuned pre-trained VLSMs for medical image segmentation\nwith adapters.\n  Then, we have employed adversarial attacks -- projected gradient descent\n(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to\ndetermine its robustness against adversaries.\n  We have reported models' performance decline to analyze the adversaries'\nimpact.\n  The results exhibit significant drops in the DSC and IoU scores after the\nintroduction of these adversaries. Furthermore, we also explored universal\nperturbation but were not able to find for the medical images.\n  \\footnote{https://github.com/anjilab/secure-private-ai}"
    },
    {
        "date": "2025-05",
        "title": "Single-Sample and Robust Online Resource Allocation",
        "author": "Rohan Ghuge, Sahil Singla, and Yifan Wang",
        "link": "http://arxiv.org/abs/2505.02963v1",
        "abstract": "Online Resource Allocation problem is a central problem in many areas of\nComputer Science, Operations Research, and Economics. In this problem, we\nsequentially receive $n$ stochastic requests for $m$ kinds of shared resources,\nwhere each request can be satisfied in multiple ways, consuming different\namounts of resources and generating different values. The goal is to achieve a\n$(1-\\epsilon)$-approximation to the hindsight optimum, where $\\epsilon>0$ is a\nsmall constant, assuming each resource has a large budget.\n  In this paper, we investigate the learnability and robustness of online\nresource allocation. Our primary contribution is a novel Exponential Pricing\nalgorithm with the following properties: 1. It requires only a \\emph{single\nsample} from each of the $n$ request distributions to achieve a\n$(1-\\epsilon)$-approximation for online resource allocation with large budgets.\nSuch an algorithm was previously unknown, even with access to polynomially many\nsamples, as prior work either assumed full distributional knowledge or was\nlimited to i.i.d.\\,or random-order arrivals. 2. It is robust to corruptions in\nthe outliers model and the value augmentation model. Specifically, it maintains\nits $(1 - \\epsilon)$-approximation guarantee under both these robustness\nmodels, resolving the open question posed in Argue, Gupta, Molinaro, and Singla\n(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures\nincentive compatibility.\n  The intuition behind our Exponential Pricing algorithm is that the price of a\nresource should adjust exponentially as it is overused or underused. It differs\nfrom conventional approaches that use an online learning algorithm for item\npricing. This departure guarantees that the algorithm will never run out of any\nresource, but loses the usual no-regret properties of online learning\nalgorithms, necessitating a new analytical approach."
    },
    {
        "date": "2025-05",
        "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models",
        "author": "Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2505.02824v1",
        "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance."
    },
    {
        "date": "2025-05",
        "title": "Acoustic Side-Channel Attacks on a Computer Mouse",
        "author": "Mauro Conti, Marin Duroyon, Gabriele Orazi, and Gene Tsudik",
        "link": "http://arxiv.org/abs/2505.02725v1",
        "abstract": "Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using\naudio emitted from a computing devices and their peripherals. Attacks targeting\nkeyboards are popular and have been explored in the literature. However,\nsimilar attacks targeting other human interface peripherals, such as computer\nmice, are under-explored. To this end, this paper considers security leakage\nvia acoustic signals emanating from normal mouse usage. We first confirm\nfeasibility of such attacks by showing a proof-of-concept attack that\nclassifies four mouse movements with 97% accuracy in a controlled environment.\nWe then evolve the attack towards discerning twelve unique mouse movements\nusing a smartphone to record the experiment. Using Machine Learning (ML)\ntechniques, the model is trained on an experiment with six participants to be\ngeneralizable and discern among twelve movements with 94% accuracy. In\naddition, we experiment with an attack that detects a user action of closing a\nfull-screen window on a laptop. Achieving an accuracy of 91%, this experiment\nhighlights exploiting audio leakage from computer mouse movements in a\nrealistic scenario."
    },
    {
        "date": "2025-05",
        "title": "Robustness questions the interpretability of graph neural networks: what to do?",
        "author": "Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, and Ilya Makarov",
        "link": "http://arxiv.org/abs/2505.02566v1",
        "abstract": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications."
    },
    {
        "date": "2025-05",
        "title": "Antifragility of RIS-assisted Communication Systems under Jamming Attacks",
        "author": "Mounir Bensalem, Thomas R\u00f6thig, and Admela Jukan",
        "link": "http://arxiv.org/abs/2505.02565v1",
        "abstract": "Antifragility of communication systems is defined as measure of benefits\ngained from the adverse events and variability of its environment. In this\npaper, we introduce the notion of antifragility in Reconfigurable Intelligent\nSurface (RIS) assisted communication systems affected by a jamming attack. We\nanalyzed the antifragility of the two hop systems, where the wireless path\ncontains source node, RIS, destination node, and a eavesdropping/jamming node.\nWe propose and analyze the antifragility performance for several jamming\nmodels, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude\nshifting. Our paper shows that antifragility throughput can indeed be achieved\nunder certain power thresholds and for various jamming models. In particular,\nhigh jamming power combined with low baseline data rates yields an antifragile\ngain factor of approximately five times. The results confirm that\nreconfigurable intelligent surfaces, when coupled with an antifragile design\nphilosophy, can convert hostile interference from a liability into a throughput\ngain."
    },
    {
        "date": "2025-05",
        "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification",
        "author": "Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, and Peng Hu",
        "link": "http://arxiv.org/abs/2505.02549v2",
        "abstract": "Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE."
    },
    {
        "date": "2025-05",
        "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction",
        "author": "Aiman Farooq, Azad Singh, Deepak Mishra, and Santanu Chaudhury",
        "link": "http://arxiv.org/abs/2505.02529v1",
        "abstract": "Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care."
    },
    {
        "date": "2025-05",
        "title": "Bayesian Robust Aggregation for Federated Learning",
        "author": "Aleksandr Karakulev, Usama Zafar, Salman Toor, and Prashant Singh",
        "link": "http://arxiv.org/abs/2505.02490v1",
        "abstract": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients."
    },
    {
        "date": "2025-05",
        "title": "Economic Security of Multiple Shared Security Protocols",
        "author": "Abhimanyu Nag, Dhruv Bodani, and Abhishek Kumar",
        "link": "http://arxiv.org/abs/2505.03843v2",
        "abstract": "As restaking protocols gain adoption across blockchain ecosystems, there is a\nneed for Actively Validated Services (AVSs) to span multiple Shared Security\nProviders (SSPs). This leads to stake fragmentation which introduces new\ncomplications where an adversary may compromise an AVS by targeting its weakest\nSSP. In this paper, we formalize the Multiple SSP Problem and analyze two\narchitectures : an isolated fragmented model called Model $\\mathbb{M}$ and a\nshared unified model called Model $\\mathbb{S}$, through a convex optimization\nand game-theoretic lens. We derive utility bounds, attack cost conditions, and\nmarket equilibrium that describes protocol security for both models. Our\nresults show that while Model $\\mathbb{M}$ offers deployment flexibility, it\ninherits lowest-cost attack vulnerabilities, whereas Model $\\mathbb{S}$\nachieves tighter security guarantees through single validator sets and\naggregated slashing logic. We conclude with future directions of work including\nan incentive-compatible stake rebalancing allocation in restaking ecosystems."
    },
    {
        "date": "2025-05",
        "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning",
        "author": "Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, and Ganesh Ramakrishnan",
        "link": "http://arxiv.org/abs/2505.02433v1",
        "abstract": "We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs."
    },
    {
        "date": "2025-05",
        "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training",
        "author": "Fares B. Mehouachi, and Saif Eddin Jabari",
        "link": "http://arxiv.org/abs/2505.02360v1",
        "abstract": "Adversarial training is a cornerstone of robust deep learning, but fast\nmethods like the Fast Gradient Sign Method (FGSM) often suffer from\nCatastrophic Overfitting (CO), where models become robust to single-step\nattacks but fail against multi-step variants. While existing solutions rely on\nnoise injection, regularization, or gradient clipping, we propose a novel\nsolution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent\nunder the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we\ndevelop a framework for generalized $l^p$ attack as a fixed point problem and\ncraft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to\n$l^{\\infty}$. This leads to our core insight: CO emerges when highly\nconcentrated gradients where information localizes in few dimensions interact\nwith aggressive norm constraints. By quantifying gradient concentration through\nParticipation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM\nthat automatically tunes the training norm based on gradient information.\nExtensive experiments demonstrate that this approach achieves strong robustness\nwithout requiring additional regularization or noise injection, providing a\nnovel and theoretically-principled pathway to mitigate the CO problem."
    },
    {
        "date": "2025-05",
        "title": "Temporal Robustness in Discrete Time Linear Dynamical Systems",
        "author": "Nilava Metya, and Arunesh Sinha",
        "link": "http://arxiv.org/abs/2505.02347v1",
        "abstract": "Discrete time linear dynamical systems, including Markov chains, have found\nmany applications. However, in some problems, there is uncertainty about the\ntime horizon for which the system runs. This creates uncertainty about the cost\n(or reward) incurred based on the state distribution when the system stops.\nGiven past data samples of how long a system ran, we propose to theoretically\nanalyze a distributional robust cost estimation task in a Wasserstein ambiguity\nset, instead of learning a probability distribution from a few samples. Towards\nthis, we show an equivalence between a discrete time Markov Chain on a\nprobability simplex and a global asymptotic stable (GAS) discrete time linear\ndynamical system, allowing us to base our study on a GAS system only. Then, we\nprovide various polynomial time algorithms and hardness results for different\ncases in our theoretical study, including a fundamental result about\nWasserstein distance based polytope."
    },
    {
        "date": "2025-05",
        "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection",
        "author": "Daisuke Yamada, Harit Vishwakarma, and Ramya Korlakai Vinayak",
        "link": "http://arxiv.org/abs/2505.02299v1",
        "abstract": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control."
    },
    {
        "date": "2025-05",
        "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots",
        "author": "Dyuman Aditya, Junning Huang, Nico Bohlinger, Piotr Kicki, Krzysztof Walas, Jan Peters, Matteo Luperto, and Davide Tateo",
        "link": "http://arxiv.org/abs/2505.02272v1",
        "abstract": "Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam"
    },
    {
        "date": "2025-05",
        "title": "Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees",
        "author": "Andrew Quijano, Spyros T. Halkidis, Kevin Gallagher, Kemal Akkaya, and Nikolaos Samaras",
        "link": "http://arxiv.org/abs/2505.02224v1",
        "abstract": "A decision tree is an easy-to-understand tool that has been widely used for\nclassification tasks. On the one hand, due to privacy concerns, there has been\nan urgent need to create privacy-preserving classifiers that conceal the user's\ninput from the classifier. On the other hand, with the rise of cloud computing,\ndata owners are keen to reduce risk by outsourcing their model, but want\nsecurity guarantees that third parties cannot steal their decision tree model.\nTo address these issues, Joye and Salehi introduced a theoretical protocol that\nefficiently evaluates decision trees while maintaining privacy by leveraging\ntheir comparison protocol that is resistant to timing attacks. However, their\napproach was not only inefficient but also prone to side-channel attacks.\nTherefore, in this paper, we propose a new decision tree inference protocol in\nwhich the model is shared and evaluated among multiple entities. We partition\nour decision tree model by each level to be stored in a new entity we refer to\nas a \"level-site.\" Utilizing this approach, we were able to gain improved\naverage run time for classifier evaluation for a non-complete tree, while also\nhaving strong mitigations against side-channel attacks."
    },
    {
        "date": "2025-05",
        "title": "Robust AI-Generated Face Detection with Imbalanced Data",
        "author": "Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, and Shu Hu",
        "link": "http://arxiv.org/abs/2505.02182v1",
        "abstract": "Deepfakes, created using advanced AI techniques such as Variational\nAutoencoder and Generative Adversarial Networks, have evolved from research and\nentertainment applications into tools for malicious activities, posing\nsignificant threats to digital trust. Current deepfake detection techniques\nhave evolved from CNN-based methods focused on local artifacts to more advanced\napproaches using vision transformers and multimodal models like CLIP, which\ncapture global anomalies and improve cross-domain generalization. Despite\nrecent progress, state-of-the-art deepfake detectors still face major\nchallenges in handling distribution shifts from emerging generative models and\naddressing severe class imbalance between authentic and fake samples in\ndeepfake datasets, which limits their robustness and detection accuracy. To\naddress these challenges, we propose a framework that combines dynamic loss\nreweighting and ranking-based optimization, which achieves superior\ngeneralization and performance under imbalanced dataset conditions. The code is\navailable at https://github.com/Purdue-M2/SP_CUP."
    }
]