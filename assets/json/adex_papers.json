[
    {
        "date": "2025-04",
        "title": "Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "author": "Cristian Padron-Manrique, Juan Jos\u00e9 Oropeza Valdez, and Osbaldo Resendis-Antonio",
        "link": "http://arxiv.org/abs/2504.10343v1",
        "abstract": "Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring\nmolecular features linked to patient survival. This hampers the discovery of\ngeneralizable biomarkers, as models tend to overfit tissue-specific patterns\nrather than capture survival-relevant signals. To address this, we propose a\nDomain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn\nrepresentations less biased by tissue and more focused on survival. Identifying\ntissue-independent genetic profiles is key to revealing core cancer programs.\nWe assess the DANN using: (1) Standard SHAP, based on the original input space\nand DANN's mortality classifier; (2) A layer-aware strategy applied to hidden\nactivations, including an unsupervised manifold from raw activations and a\nsupervised manifold from mortality-specific SHAP values. Standard SHAP remains\nconfounded by tissue signals due to biases inherent in its computation. The raw\nactivation manifold was dominated by high-magnitude activations, which masked\nsubtle tissue and mortality-related signals. In contrast, the layer-aware SHAP\nmanifold offers improved low-dimensional representations of both tissue and\nmortality signals, independent of activation strength, enabling subpopulation\nstratification and pan-cancer identification of survival-associated genes."
    },
    {
        "date": "2025-04",
        "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing Obfuscation",
        "author": "Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, and Pen Chung Yew",
        "link": "http://arxiv.org/abs/2504.10318v1",
        "abstract": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
    },
    {
        "date": "2025-04",
        "title": "ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "author": "Vivek Yelleti",
        "link": "http://arxiv.org/abs/2504.10229v1",
        "abstract": "Continuous generation of streaming data from diverse sources, such as online\ntransactions and digital interactions, necessitates timely fraud detection.\nTraditional batch processing methods often struggle to capture the rapidly\nevolving patterns of fraudulent activities. This paper highlights the critical\nimportance of processing streaming data for effective fraud detection. To\naddress the inherent challenges of latency, scalability, and concept drift in\nstreaming environments, we propose a robust online streaming fraud detection\n(ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage\nOne: Offline Model Initialization. In this initial stage, a model is built in\noffline settings using incremental learning principles to overcome the\n\"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this\ndynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are\nemployed to identify concept drift in the incoming data stream and\nincrementally train the model accordingly. This \"train-only-when-required\"\nstrategy drastically reduces the number of retrains needed without\nsignificantly impacting the area under the receiver operating characteristic\ncurve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method\ndemonstrated the best performance among the employed methods. In terms of model\nefficacy, Adaptive Random Forest consistently outperformed other models,\nachieving the highest AUC in four out of five datasets."
    },
    {
        "date": "2025-04",
        "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
        "author": "Anwesha Mohanty, Venkatesh Balavadhani Parthasarathy, and Arsalan Shahid",
        "link": "http://arxiv.org/abs/2504.10179v1",
        "abstract": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
        "author": "Andreas Happe, and J\u00fcrgen Cito",
        "link": "http://arxiv.org/abs/2504.10112v1",
        "abstract": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
    },
    {
        "date": "2025-04",
        "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling",
        "author": "Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, and Tram T. Doan",
        "link": "http://arxiv.org/abs/2504.09960v1",
        "abstract": "Event-based eye tracking has become a pivotal technology for augmented\nreality and human-computer interaction. Yet, existing methods struggle with\nreal-world challenges such as abrupt eye movements and environmental noise.\nBuilding on the efficiency of the Lightweight Spatiotemporal Network-a causal\narchitecture optimized for edge devices-we introduce two key advancements.\nFirst, a robust data augmentation pipeline incorporating temporal shift,\nspatial flip, and event deletion improves model resilience, reducing Euclidean\ndistance error by 12% (1.61 vs. 1.70 baseline) on challenging samples. Second,\nwe propose KnightPupil, a hybrid architecture combining an EfficientNet-B3\nbackbone for spatial feature extraction, a bidirectional GRU for contextual\ntemporal modeling, and a Linear Time-Varying State-Space Module to adapt to\nsparse inputs and noise dynamically. Evaluated on the 3ET+ benchmark, our\nframework achieved 1.61 Euclidean distance on the private test set of the\nEvent-based Eye Tracking Challenge at CVPR 2025, demonstrating its\neffectiveness for practical deployment in AR/VR systems while providing a\nfoundation for future innovations in neuromorphic vision."
    },
    {
        "date": "2025-04",
        "title": "LangPert: Detecting and Handling Task-level Perturbations for Robust Object Rearrangement",
        "author": "Xu Yin, Min-Sung Yoon, Yuchi Huo, Kang Zhang, and Sung-Eui Yoon",
        "link": "http://arxiv.org/abs/2504.09893v1",
        "abstract": "Task execution for object rearrangement could be challenged by Task-Level\nPerturbations (TLP), i.e., unexpected object additions, removals, and\ndisplacements that can disrupt underlying visual policies and fundamentally\ncompromise task feasibility and progress. To address these challenges, we\npresent LangPert, a language-based framework designed to detect and mitigate\nTLP situations in tabletop rearrangement tasks. LangPert integrates a Visual\nLanguage Model (VLM) to comprehensively monitor policy's skill execution and\nenvironmental TLP, while leveraging the Hierarchical Chain-of-Thought (HCoT)\nreasoning mechanism to enhance the Large Language Model (LLM)'s contextual\nunderstanding and generate adaptive, corrective skill-execution plans. Our\nexperimental results demonstrate that LangPert handles diverse TLP situations\nmore effectively than baseline methods, achieving higher task completion rates,\nimproved execution efficiency, and potential generalization to unseen\nscenarios."
    },
    {
        "date": "2025-04",
        "title": "Revisiting the attacker's knowledge in inference attacks against Searchable Symmetric Encryption",
        "author": "Marc Damie, Jean-Benoist Leger, Florian Hahn, and Andreas Peter",
        "link": "http://arxiv.org/abs/2504.09879v1",
        "abstract": "Encrypted search schemes have been proposed to address growing privacy\nconcerns. However, several leakage-abuse attacks have highlighted some security\nvulnerabilities. Recent attacks assumed an attacker's knowledge containing data\n``similar'' to the indexed data. However, this vague assumption is barely\ndiscussed in literature: how likely is it for an attacker to obtain a \"similar\nenough\" data?\n  Our paper provides novel statistical tools usable on any attack in this\nsetting to analyze its sensitivity to data similarity. First, we introduce a\nmathematical model based on statistical estimators to analytically understand\nthe attackers' knowledge and the notion of similarity. Second, we conceive\nstatistical tools to model the influence of the similarity on the attack\naccuracy. We apply our tools on three existing attacks to answer questions such\nas: is similarity the only factor influencing accuracy of a given attack?\nThird, we show that the enforcement of a maximum index size can make the\n``similar-data'' assumption harder to satisfy. In particular, we propose a\nstatistical method to estimate an appropriate maximum size for a given attack\nand dataset. For the best known attack on the Enron dataset, a maximum index\nsize of 200 guarantees (with high probability) the attack accuracy to be below\n5%."
    },
    {
        "date": "2025-04",
        "title": "StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models",
        "author": "Yang Feng, and Xudong Pan",
        "link": "http://arxiv.org/abs/2504.09841v1",
        "abstract": "The proliferation of autonomous agents powered by large language models\n(LLMs) has revolutionized popular business applications dealing with tabular\ndata, i.e., tabular agents. Although LLMs are observed to be vulnerable against\nprompt injection attacks from external data sources, tabular agents impose\nstrict data formats and predefined rules on the attacker's payload, which are\nineffective unless the agent navigates multiple layers of structural data to\nincorporate the payload. To address the challenge, we present a novel attack\ntermed StruPhantom which specifically targets black-box LLM-powered tabular\nagents. Our attack designs an evolutionary optimization procedure which\ncontinually refines attack payloads via the proposed constrained Monte Carlo\nTree Search augmented by an off-topic evaluator. StruPhantom helps\nsystematically explore and exploit the weaknesses of target applications to\nachieve goal hijacking. Our evaluation validates the effectiveness of\nStruPhantom across various LLM-based agents, including those on real-world\nplatforms, and attack scenarios. Our attack achieves over 50% higher success\nrates than baselines in enforcing the application's response to contain\nphishing links or malicious codes."
    },
    {
        "date": "2025-04",
        "title": "SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "author": "Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, and Yixian Yang",
        "link": "http://arxiv.org/abs/2504.09839v1",
        "abstract": "Speech synthesis technology has brought great convenience, while the\nwidespread usage of realistic deepfake audio has triggered hazards. Malicious\nadversaries may unauthorizedly collect victims' speeches and clone a similar\nvoice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the\nexisting defense methods cannot effectively prevent deepfake exploitation and\nare vulnerable to robust training techniques. Therefore, a more effective and\nrobust data protection method is urgently needed. In response, we propose a\ndefensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users'\naudio before uploading by embedding imperceptible perturbations on original\nspeeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a\nrobust and universal proactive protection technique, \\textbf{S}peech\n\\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a\nsurrogate model to generate universally applicable perturbation for generative\nsynthetic models. Moreover, we optimize the human perception of embedded\nperturbation in terms of time and frequency domains. To evaluate our method\ncomprehensively, we conduct extensive experiments across advanced models and\ndatasets, both subjectively and objectively. Our experimental results\ndemonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection\neffectiveness and transferability and is highly robust against advanced\nadaptive adversaries. Moreover, SafeSpeech has real-time capability in\nreal-world tests. The source code is available at\n\\href{https://github.com/wxzyd123/SafeSpeech}{https://github.com/wxzyd123/SafeSpeech}."
    },
    {
        "date": "2025-04",
        "title": "Socratic Chart: Cooperating Multiple Agents for Robust SVG Chart Understanding",
        "author": "Yuyang Ji, and Haohan Wang",
        "link": "http://arxiv.org/abs/2504.09764v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable versatility\nbut face challenges in demonstrating true visual understanding, particularly in\nchart reasoning tasks. Existing benchmarks like ChartQA reveal significant\nreliance on text-based shortcuts and probabilistic pattern-matching rather than\ngenuine visual reasoning. To rigorously evaluate visual reasoning, we introduce\na more challenging test scenario by removing textual labels and introducing\nchart perturbations in the ChartQA dataset. Under these conditions, models like\nGPT-4o and Gemini-2.0 Pro experience up to a 30% performance drop, underscoring\ntheir limitations. To address these challenges, we propose Socratic Chart, a\nnew framework that transforms chart images into Scalable Vector Graphics (SVG)\nrepresentations, enabling MLLMs to integrate textual and visual modalities for\nenhanced chart understanding. Socratic Chart employs a multi-agent pipeline\nwith specialized agent-generators to extract primitive chart attributes (e.g.,\nbar heights, line coordinates) and an agent-critic to validate results,\nensuring high-fidelity symbolic representations. Our framework surpasses\nstate-of-the-art models in accurately capturing chart primitives and improving\nreasoning performance, establishing a robust pathway for advancing MLLM visual\nunderstanding."
    },
    {
        "date": "2025-04",
        "title": "Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "author": "Lucas Cardoso, Vitor Santos, Jos\u00e9 Ribeiro, Regiane Kawasaki, Ricardo Prud\u00eancio, and Ronnie Alves",
        "link": "http://arxiv.org/abs/2504.09759v1",
        "abstract": "Benchmarking is a fundamental practice in machine learning (ML) for comparing\nthe performance of classification algorithms. However, traditional evaluation\nmethods often overlook a critical aspect: the joint consideration of dataset\ncomplexity and an algorithm's ability to generalize. Without this dual\nperspective, assessments may favor models that perform well on easy instances\nwhile failing to capture their true robustness. To address this limitation,\nthis study introduces a novel evaluation methodology that combines Item\nResponse Theory (IRT) with the Glicko-2 rating system, originally developed to\nmeasure player strength in competitive games. IRT assesses classifier ability\nbased on performance over difficult instances, while Glicko-2 updates\nperformance metrics - such as rating, deviation, and volatility - via simulated\ntournaments between classifiers. This combined approach provides a fairer and\nmore nuanced measure of algorithm capability. A case study using the\nOpenML-CC18 benchmark showed that only 15% of the datasets are truly\nchallenging and that a reduced subset with 50% of the original datasets offers\ncomparable evaluation power. Among the algorithms tested, Random Forest\nachieved the highest ability score. The results highlight the importance of\nimproving benchmark design by focusing on dataset quality and adopting\nevaluation strategies that reflect both difficulty and classifier proficiency."
    },
    {
        "date": "2025-04",
        "title": "Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "author": "Shuai Jiang, and Saeed Hassanpour",
        "link": "http://arxiv.org/abs/2504.09704v1",
        "abstract": "Transformer-based models have achieved remarkable success in natural language\nand vision tasks, but their application to gene expression analysis remains\nlimited due to data sparsity, high dimensionality, and missing values. We\npresent GexBERT, a transformer-based autoencoder framework for robust\nrepresentation learning of gene expression data. GexBERT learns context-aware\ngene embeddings by pretraining on large-scale transcriptomic profiles with a\nmasking and restoration objective that captures co-expression relationships\namong thousands of genes. We evaluate GexBERT across three critical tasks in\ncancer research: pan-cancer classification, cancer-specific survival\nprediction, and missing value imputation. GexBERT achieves state-of-the-art\nclassification accuracy from limited gene subsets, improves survival prediction\nby restoring expression of prognostic anchor genes, and outperforms\nconventional imputation methods under high missingness. Furthermore, its\nattention-based interpretability reveals biologically meaningful gene patterns\nacross cancer types. These findings demonstrate the utility of GexBERT as a\nscalable and effective tool for gene expression modeling, with translational\npotential in settings where gene coverage is limited or incomplete."
    },
    {
        "date": "2025-04",
        "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "author": "Anxian Liu, Junying Ma, and Guang Zhang",
        "link": "http://arxiv.org/abs/2504.09664v1",
        "abstract": "Financial time series forecasting in the zero-shot setting is essential for\nrisk management and investment decision-making, particularly during abrupt\nmarket regime shifts or in emerging markets with limited historical data. While\nModel-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this\ndomain, existing meta task construction strategies often lead to suboptimal\nperformance, especially when dealing with highly turbulent financial time\nseries. To address this challenge, we propose a novel task construction method\nthat leverages learned embeddings for more effective meta-learning in the\nzero-shot setting. Specifically, we construct two complementary types of\nmeta-tasks based on the learned embeddings: intra-cluster tasks and\ninter-cluster tasks. To capture diverse fine-grained patterns, we apply\nstochastic projection matrices to the learned embeddings and use clustering\nalgorithm to form the tasks. Additionally, to improve generalization\ncapabilities, we employ hard task mining strategies and leverage inter-cluster\ntasks to identify invariant patterns across different time series. Extensive\nexperiments on the real world financial dataset demonstrate that our method\nsignificantly outperforms existing approaches, showing better generalization\nability in the zero-shot scenario."
    },
    {
        "date": "2025-04",
        "title": "Bridging Immutability with Flexibility: A Scheme for Secure and Efficient Smart Contract Upgrades",
        "author": "Tahrim Hossain, Sakib Hassan, Faisal Haque Bappy, Muhammad Nur Yanhaona, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09652v1",
        "abstract": "The emergence of blockchain technology has revolutionized contract execution\nthrough the introduction of smart contracts. Ethereum, the leading blockchain\nplatform, leverages smart contracts to power decentralized applications\n(DApps), enabling transparent and self-executing systems across various\ndomains. While the immutability of smart contracts enhances security and trust,\nit also poses significant challenges for updates, defect resolution, and\nadaptation to changing requirements. Existing upgrade mechanisms are complex,\nresource-intensive, and costly in terms of gas consumption, often compromising\nsecurity and limiting practical adoption. To address these challenges, we\npropose FlexiContracts+, a novel scheme that reimagines smart contracts by\nenabling secure, in-place upgrades on Ethereum while preserving historical data\nwithout relying on multiple contracts or extensive pre-deployment planning.\nFlexiContracts+ enhances security, simplifies development, reduces engineering\noverhead, and supports adaptable, expandable smart contracts. Comprehensive\ntesting demonstrates that FlexiContracts+ achieves a practical balance between\nimmutability and flexibility, advancing the capabilities of smart contract\nsystems."
    },
    {
        "date": "2025-04",
        "title": "RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "author": "Guixian Chen, Jianhao Ma, and Salar Fattahi",
        "link": "http://arxiv.org/abs/2504.09648v1",
        "abstract": "In this paper, we study the problem of robust subspace recovery (RSR) in the\npresence of both strong adversarial corruptions and Gaussian noise.\nSpecifically, given a limited number of noisy samples -- some of which are\ntampered by an adaptive and strong adversary -- we aim to recover a\nlow-dimensional subspace that approximately contains a significant fraction of\nthe uncorrupted samples, up to an error that scales with the Gaussian noise.\nExisting approaches to this problem often suffer from high computational costs\nor rely on restrictive distributional assumptions, limiting their applicability\nin truly adversarial settings. To address these challenges, we revisit the\nclassical random sample consensus (RANSAC) algorithm, which offers strong\nrobustness to adversarial outliers, but sacrifices efficiency and robustness\nagainst Gaussian noise and model misspecification in the process. We propose a\ntwo-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure\nmodes of standard RANSAC. Our method is provably robust to both Gaussian and\nadversarial corruptions, achieves near-optimal sample complexity without\nrequiring prior knowledge of the subspace dimension, and is more efficient than\nexisting RANSAC-type methods."
    },
    {
        "date": "2025-04",
        "title": "A Secure Communication Protocol for Remote Keyless Entry System with Adaptive Adjustment of Transmission Parameters",
        "author": "Jingjing Guo, Bo Tang, Jiayuan Xu, Qingyi Li, Yuyuan Qin, and Xinghua Li",
        "link": "http://arxiv.org/abs/2504.09527v1",
        "abstract": "Remote Keyless Entry (RKE) systems have become a standard feature in modern\nvehicles, yet their unidirectional fixed-frequency radio communication renders\nthem vulnerable to replay attacks, impersonation attacks, cryptanalysis, and\nintentional interference. Existing cryptographic authentication methods enhance\nsecurity but often fail to address real-world constraints such as computational\nefficiency and radio interference. To mitigate these threats, we designed the\nAdaptive Frequency-Hopping Algorithm and the Adaptive TXP and PHY Mode Control\nAlgorithm that can dynamically optimize channel selection, transmission power,\nand PHY modes based on real-time channel quality assessment. To enhance the\nsecurity and reliability of RKE systems, we propose the Lightweight Vehicle-Key\nAuthentication Protocol. In addition, a prototype of the proposed scheme was\nimplemented to verify its effectiveness in mitigating interference and\npreventing unauthorized access.Experimental results show that our scheme\nsignificantly enhances communication security and reliability while maintaining\nlow computational overhead. Under mild interference conditions, the packet\ndelivery rate (PDR) of the adaptive scheme increases from 93% to 99.23%, and\nunder strong interference, it improves from 85% to 99.01%. Additionally, the\nscheme effectively prevents replay and impersonation attacks, ensuring secure\nvehicle access control by dynamically optimizing communication parameters to\nmaintain stable and reliable transmission."
    },
    {
        "date": "2025-04",
        "title": "PLS-Assisted Offloading for Edge Computing-Enabled Post-Quantum Security in Resource-Constrained Devices",
        "author": "Hamid Amiriara, Mahtab Mirmohseni, and Rahim Tafazolli",
        "link": "http://arxiv.org/abs/2504.09437v1",
        "abstract": "With the advent of post-quantum cryptography (PQC) standards, it has become\nimperative for resource-constrained devices (RCDs) in the Internet of Things\n(IoT) to adopt these quantum-resistant protocols. However, the high\ncomputational overhead and the large key sizes associated with PQC make direct\ndeployment on such devices impractical. To address this challenge, we propose\nan edge computing-enabled PQC framework that leverages a physical-layer\nsecurity (PLS)-assisted offloading strategy, allowing devices to either offload\nintensive cryptographic tasks to a post-quantum edge server (PQES) or perform\nthem locally. Furthermore, to ensure data confidentiality within the edge\ndomain, our framework integrates two PLS techniques: offloading RCDs employ\nwiretap coding to secure data transmission, while non-offloading RCDs serve as\nfriendly jammers by broadcasting artificial noise to disrupt potential\neavesdroppers. Accordingly, we co-design the computation offloading and PLS\nstrategy by jointly optimizing the device transmit power, PQES computation\nresource allocation, and offloading decisions to minimize overall latency under\nresource constraints. Numerical results demonstrate significant latency\nreductions compared to baseline schemes, confirming the scalability and\nefficiency of our approach for secure PQC operations in IoT networks."
    },
    {
        "date": "2025-04",
        "title": "Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "author": "Moirangthem Tiken Singh",
        "link": "http://arxiv.org/abs/2504.09427v1",
        "abstract": "Fault classification in industrial machinery is vital for enhancing\nreliability and reducing downtime, yet it remains challenging due to the\nvariability of vibration patterns across diverse operating conditions. This\nstudy introduces a novel graph-based framework for fault classification,\nconverting time-series vibration data from machinery operating at varying\nhorsepower levels into a graph representation. We utilize Shannon's entropy to\ndetermine the optimal window size for data segmentation, ensuring each segment\ncaptures significant temporal patterns, and employ Dynamic Time Warping (DTW)\nto define graph edges based on segment similarity. A Graph Auto Encoder (GAE)\nwith a deep graph transformer encoder, decoder, and ensemble classifier is\ndeveloped to learn latent graph representations and classify faults across\nvarious categories. The GAE's performance is evaluated on the Case Western\nReserve University (CWRU) dataset, with cross-dataset generalization assessed\non the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on\nthe CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN,\nGRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for\nBi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99\nvs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that\ndatasets with amplified vibration patterns and diverse fault dynamics enhance\ngeneralization. This framework provides a robust solution for fault diagnosis\nunder varying conditions, offering insights into dataset impacts on model\nperformance."
    },
    {
        "date": "2025-04",
        "title": "Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "author": "Gengcan Chen, Donghong Cai, Zahid Khan, Jawad Ahmad, and Wadii Boulila",
        "link": "http://arxiv.org/abs/2504.09415v1",
        "abstract": "In electronic consumer Internet of Things (IoT), consumer electronic devices\nas edge devices require less computational overhead and the remote state\nestimation (RSE) of consumer electronic devices is always at risk of\ndenial-of-service (DoS) attacks. Therefore, the adversarial strategy between\nconsumer electronic devices and DoS attackers is critical. This paper focuses\non the adversarial strategy between consumer electronic devices and DoS\nattackers in IoT-enabled RSE Systems. We first propose a remote joint\nestimation model for distributed measurements to effectively reduce consumer\nelectronic device workload and minimize data leakage risks. The Kalman filter\nis deployed on the remote estimator, and the DoS attacks with open-loop as well\nas closed-loop are considered. We further introduce advanced reinforcement\nlearning techniques, including centralized and distributed Minimax-DQN, to\naddress high-dimensional decision-making challenges in both open-loop and\nclosed-loop scenarios. Especially, the Q-network instead of the Q-table is used\nin the proposed approaches, which effectively solves the challenge of\nQ-learning. Moreover, the proposed distributed Minimax-DQN reduces the action\nspace to expedite the search for Nash Equilibrium (NE). The experimental\nresults validate that the proposed model can expeditiously restore the RSE\nerror covariance to a stable state in the presence of DoS attacks, exhibiting\nnotable attack robustness. The proposed centralized and distributed Minimax-DQN\neffectively resolves the NE in both open and closed-loop case, showcasing\nremarkable performance in terms of convergence. It reveals that substantial\nadvantages in both efficiency and stability are achieved compared with the\nstate-of-the-art methods."
    },
    {
        "date": "2025-04",
        "title": "PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking",
        "author": "Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.09361v1",
        "abstract": "Tracking multiple objects in a continuous video stream is crucial for many\ncomputer vision tasks. It involves detecting and associating objects with their\nrespective identities across successive frames. Despite significant progress\nmade in multiple object tracking (MOT), recent studies have revealed the\nvulnerability of existing MOT methods to adversarial attacks. Nevertheless, all\nof these attacks belong to digital attacks that inject pixel-level noise into\ninput images, and are therefore ineffective in physical scenarios. To fill this\ngap, we propose PapMOT, which can generate physical adversarial patches against\nMOT for both digital and physical scenarios. Besides attacking the detection\nmechanism, PapMOT also optimizes a printable patch that can be detected as new\ntargets to mislead the identity association process. Moreover, we introduce a\npatch enhancement strategy to further degrade the temporal consistency of\ntracking results across video frames, resulting in more aggressive attacks. We\nfurther develop new evaluation metrics to assess the robustness of MOT against\nsuch attacks. Extensive evaluations on multiple datasets demonstrate that our\nPapMOT can successfully attack various architectures of MOT trackers in digital\nscenarios. We also validate the effectiveness of PapMOT for physical attacks by\ndeploying printed adversarial patches in the real world."
    },
    {
        "date": "2025-04",
        "title": "Explorer: Robust Collection of Interactable GUI Elements",
        "author": "Iason Chaimalas, Arnas Vy\u0161niauskas, and Gabriel Brostow",
        "link": "http://arxiv.org/abs/2504.09352v1",
        "abstract": "Automation of existing Graphical User Interfaces (GUIs) is important but hard\nto achieve. Upstream of making the GUI user-accessible or somehow scriptable,\neven the data-collection to understand the original interface poses significant\nchallenges. For example, large quantities of general UI data seem helpful for\ntraining general machine learning (ML) models, but accessibility for each\nperson can hinge on the ML's precision on a specific app. We therefore take the\nperspective that a given user needs confidence, that the relevant UI elements\nare being detected correctly throughout one app or digital environment. We\nmostly assume that the target application is known in advance, so that data\ncollection and ML-training can be personalized for the test-time target domain.\nThe proposed Explorer system focuses on detecting on-screen buttons and\ntext-entry fields, i.e. interactables, where the training process has access to\na live version of the application. The live application can run on almost any\npopular platform except iOS phones, and the collection is especially\nstreamlined for Android phones or for desktop Chrome browsers. Explorer also\nenables the recording of interactive user sessions, and subsequent mapping of\nhow such sessions overlap and sometimes loop back to similar states. We show\nhow having such a map enables a kind of path planning through the GUI, letting\na user issue audio commands to get to their destination. Critically, we are\nreleasing our code for Explorer openly at https://github.com/varnelis/Explorer."
    },
    {
        "date": "2025-04",
        "title": "CrossLink: A Decentralized Framework for Secure Cross-Chain Smart Contract Execution",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09319v1",
        "abstract": "This paper introduces CrossLink, a decentralized framework for secure\ncross-chain smart contract execution that effectively addresses the inherent\nlimitations of contemporary solutions, which primarily focus on asset transfers\nand rely on potentially vulnerable centralized intermediaries. Recognizing the\nescalating demand for seamless interoperability among decentralized\napplications, CrossLink provides a trustless mechanism for smart contracts\nacross disparate blockchain networks to communicate and interact. At its core,\nCrossLink utilizes a compact chain for selectively storing authorized contract\nstates and employs a secure inter-chain messaging mechanism to ensure atomic\nexecution and data consistency. By implementing a deposit/collateral fee system\nand efficient state synchronization, CrossLink enhances security and mitigates\nvulnerabilities, offering a novel approach to seamless, secure, and\ndecentralized cross-chain interoperability. A formal security analysis further\nvalidates CrossLink's robustness against unauthorized modifications and\ndenial-of-service attacks."
    },
    {
        "date": "2025-04",
        "title": "SmartShift: A Secure and Efficient Approach to Smart Contract Migration",
        "author": "Tahrim Hossain, Faisal Haque Bappy, Tarannum Shaila Zaman, Raiful Hasan, and Tariqul Islam",
        "link": "http://arxiv.org/abs/2504.09315v1",
        "abstract": "Blockchain and smart contracts have emerged as revolutionary technologies\ntransforming distributed computing. While platform evolution and smart\ncontracts' inherent immutability necessitate migrations both across and within\nchains, migrating the vast amounts of critical data in these contracts while\nmaintaining data integrity and minimizing operational disruption presents a\nsignificant challenge. To address these challenges, we present SmartShift, a\nframework that enables secure and efficient smart contract migrations through\nintelligent state partitioning and progressive function activation, preserving\noperational continuity during transitions. Our comprehensive evaluation\ndemonstrates that SmartShift significantly reduces migration downtime while\nensuring robust security, establishing a foundation for efficient and secure\nsmart contract migration systems."
    },
    {
        "date": "2025-04",
        "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search",
        "author": "Tinh-Anh Nguyen-Nhu, Huu-Loc Tran, Nguyen-Khang Le, Minh-Nhat Nguyen, Tien-Huy Nguyen, Hoang-Long Nguyen-Huu, Huu-Phong Phan-Nguyen, Huy-Thach Pham, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.09298v1",
        "abstract": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories."
    },
    {
        "date": "2025-04",
        "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
        "author": "You Wu, Xucheng Wang, Xiangyang Yang, Mengyuan Liu, Dan Zeng, Hengzhou Ye, and Shuiwang Li",
        "link": "http://arxiv.org/abs/2504.09228v1",
        "abstract": "Single-stream architectures using Vision Transformer (ViT) backbones show\ngreat potential for real-time UAV tracking recently. However, frequent\nocclusions from obstacles like buildings and trees expose a major drawback:\nthese models often lack strategies to handle occlusions effectively. New\nmethods are needed to enhance the occlusion resilience of single-stream ViT\nmodels in aerial tracking. In this work, we propose to learn Occlusion-Robust\nRepresentations (ORR) based on ViTs for UAV tracking by enforcing an invariance\nof the feature representation of a target with respect to random masking\noperations modeled by a spatial Cox process. Hopefully, this random masking\napproximately simulates target occlusions, thereby enabling us to learn ViTs\nthat are robust to target occlusion for UAV tracking. This framework is termed\nORTrack. Additionally, to facilitate real-time applications, we propose an\nAdaptive Feature-Based Knowledge Distillation (AFKD) method to create a more\ncompact tracker, which adaptively mimics the behavior of the teacher model\nORTrack according to the task's difficulty. This student model, dubbed\nORTrack-D, retains much of ORTrack's performance while offering higher\nefficiency. Extensive experiments on multiple benchmarks validate the\neffectiveness of our method, demonstrating its state-of-the-art performance.\nCodes is available at https://github.com/wuyou3474/ORTrack."
    },
    {
        "date": "2025-04",
        "title": "FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "author": "Jiaxin Liu, Xiaoqian Jiang, Xiang Li, Bohan Zhang, and Jing Zhang",
        "link": "http://arxiv.org/abs/2504.09210v2",
        "abstract": "Fairness has been a significant challenge in graph neural networks (GNNs)\nsince degree biases often result in un-equal prediction performance among nodes\nwith varying degrees. Existing GNN models focus on prediction accuracy,\nfrequently overlooking fairness across different degree groups. To addressthis\nissue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric\nContrastive Ensemble (FairACE), which inte-grates asymmetric contrastive\nlearning with adversarial training to improve degree fairness. FairACE captures\none-hop local neighborhood information and two-hop monophily similarity to\ncreate fairer node representations and employs a degree fairness regulator to\nbalance performance between high-degree and low-degree nodes. During model\ntraining, a novel group-balanced fairness loss is proposed to minimize\nclassification disparities across degree groups. In addition, we also propose a\nnovel fairness metric, the Accuracy Distribution Gap (ADG), which can\nquantitatively assess and ensure equitable performance across different\ndegree-based node groups. Experimental results on both synthetic and real-world\ndatasets demonstrate that FairACE significantly improves degree fairness\nmetrics while maintaining competitive accuracy in comparison to the\nstate-of-the-art GNN models."
    },
    {
        "date": "2025-04",
        "title": "Illusion Worlds: Deceptive UI Attacks in Social VR",
        "author": "Junhee Lee, Hwanjo Heo, Seungwon Woo, Minseok Kim, Jongseop Kim, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.09199v1",
        "abstract": "Social Virtual Reality (VR) platforms have surged in popularity, yet their\nsecurity risks remain underexplored. This paper presents four novel UI attacks\nthat covertly manipulate users into performing harmful actions through\ndeceptive virtual content. Implemented on VRChat and validated in an\nIRB-approved study with 30 participants, these attacks demonstrate how\ndeceptive elements can mislead users into malicious actions without their\nawareness. To address these vulnerabilities, we propose MetaScanner, a\nproactive countermeasure that rapidly analyzes objects and scripts in virtual\nworlds, detecting suspicious elements within seconds."
    },
    {
        "date": "2025-04",
        "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning",
        "author": "Feng Lv, Chunlong Xia, Shuo Wang, and Huo Cao",
        "link": "http://arxiv.org/abs/2504.09196v1",
        "abstract": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon."
    },
    {
        "date": "2025-04",
        "title": "Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "author": "Zhiyong Wang",
        "link": "http://arxiv.org/abs/2504.09192v1",
        "abstract": "The primary goal of my Ph.D. study is to develop provably efficient and\npractical algorithms for data-driven online sequential decision-making under\nuncertainty. My work focuses on reinforcement learning (RL), multi-armed\nbandits, and their applications, including recommendation systems, computer\nnetworks, video analytics, and large language models (LLMs). Online learning\nmethods, such as bandits and RL, have demonstrated remarkable success - ranging\nfrom outperforming human players in complex games like Atari and Go to\nadvancing robotics, recommendation systems, and fine-tuning LLMs. Despite these\nsuccesses, many established algorithms rely on idealized models that can fail\nunder model misspecifications or adversarial perturbations, particularly in\nsettings where accurate prior knowledge of the underlying model class is\nunavailable or where malicious users operate within dynamic systems. These\nchallenges are pervasive in real-world applications, where robust and adaptive\nsolutions are critical. Furthermore, while worst-case guarantees provide\ntheoretical reliability, they often fail to capture instance-dependent\nperformance, which can lead to more efficient and practical solutions. Another\nkey challenge lies in generalizing to new, unseen environments, a crucial\nrequirement for deploying these methods in dynamic and unpredictable settings.\nTo address these limitations, my research aims to develop more efficient,\nrobust, instance-adaptive, and generalizable online learning algorithms for\nboth reinforcement learning and bandits. Towards this end, I focus on\ndeveloping more efficient, robust, instance-adaptive, and generalizable for\nboth general reinforcement learning (RL) and bandits."
    },
    {
        "date": "2025-04",
        "title": "A Multi-Layered Security Analysis of Blockchain Systems: From Attack Vectors to Defense and System Hardening",
        "author": "Yuhuan Yang, Shipeng Ye, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.09181v1",
        "abstract": "The application of Bitcoin enables people to understand blockchain technology\ngradually. Bitcoin is a decentralized currency that does not rely on\nthird-party credit institutions, and the core of Bitcoin's underlying\ntechnology is blockchain. With the increasing value of Bitcoin and the vigorous\ndevelopment of decentralization, people's research on blockchain is also\nincreasing day by day. Today's blockchain technology has not only made great\nachievements in the application of Bitcoin, but has also been preliminarily\napplied in other fields, such as finance, medical treatment, the Internet of\nThings, and so on. However, with the initial application of blockchain\ntechnology on the Internet, the security of blockchain technology has also been\nwidely concerned by people in the industry. For example, whether currency\ntrading platforms, smart contracts, blockchain consensus mechanisms, and other\ntechnologies are vulnerable to attacks, and how we can defend against these\nattacks digitally and optimize the blockchain system is exactly the subject we\nwant to study. For the security of appeal blockchain, this paper first analyzes\nthe security threats faced by the application digital currency trading platform\nof the blockchain system, then analyzes the security problems of smart contract\nclosely related to blockchain 2.0, and then analyzes and studies the security\nthreats of blockchain public chain, consensus mechanism, and P2P. Finally,\ncombined with the security problems at all levels of the blockchain system we\nanalyze and study how to optimize the security of the blockchain system."
    },
    {
        "date": "2025-04",
        "title": "A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "author": "Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, and Jie Xiang",
        "link": "http://arxiv.org/abs/2504.09179v1",
        "abstract": "In open data sets of functional magnetic resonance imaging (fMRI), the\nheterogeneity of the data is typically attributed to a combination of factors,\nincluding differences in scanning procedures, the presence of confounding\neffects, and population diversities between multiple sites. These factors\ncontribute to the diminished effectiveness of representation learning, which in\nturn affects the overall efficacy of subsequent classification procedures. To\naddress these limitations, we propose a novel multi-site adversarial learning\nnetwork (MSalNET) for fMRI-based mental disorder detection. Firstly, a\nrepresentation learning module is introduced with a node information assembly\n(NIA) mechanism to better extract features from functional connectivity (FC).\nThis mechanism aggregates edge information from both horizontal and vertical\ndirections, effectively assembling node information. Secondly, to generalize\nthe feature across sites, we proposed a site-level feature extraction module\nthat can learn from individual FC data, which circumvents additional prior\ninformation. Lastly, an adversarial learning network is proposed as a means of\nbalancing the trade-off between individual classification and site regression\ntasks, with the introduction of a novel loss function. The proposed method was\nevaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data\nExchange (ABIDE) and ADHD-200. The results indicate that the proposed method\nachieves a better performance than other related algorithms with the accuracy\nof 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore,\nthe result of the site regression indicates that the proposed method reduces\nsite variability from a data-driven perspective. The most discriminative brain\nregions revealed by NIA are consistent with statistical findings, uncovering\nthe \"black box\" of deep learning to a certain extent."
    },
    {
        "date": "2025-04",
        "title": "Secure Physical Layer Communications for Low-Altitude Economy Networking: A Survey",
        "author": "Lingyi Cai, Jiacheng Wang, Ruichen Zhang, Yu Zhang, Tao Jiang, Dusit Niyato, Xianbin Wang, Abbas Jamalipour, and Xuemin Shen",
        "link": "http://arxiv.org/abs/2504.09153v1",
        "abstract": "The Low-Altitude Economy Networking (LAENet) is emerging as a transformative\nparadigm that enables an integrated and sophisticated communication\ninfrastructure to support aerial vehicles in carrying out a wide range of\neconomic activities within low-altitude airspace. However, the physical layer\ncommunications in the LAENet face growing security threats due to inherent\ncharacteristics of aerial communication environments, such as signal broadcast\nnature and channel openness. These challenges highlight the urgent need for\nsafeguarding communication confidentiality, availability, and integrity. In\nview of the above, this survey comprehensively reviews existing secure\ncountermeasures for physical layer communication in the LAENet. We explore core\nmethods focusing on anti-eavesdropping and authentication for ensuring\ncommunication confidentiality. Subsequently, availability-enhancing techniques\nare thoroughly discussed for anti-jamming and spoofing defense. Then, we review\napproaches for safeguarding integrity through anomaly detection and injection\nprotection. Furthermore, we discuss future research directions, emphasizing\nenergy-efficient physical layer security, multi-drone collaboration for secure\ncommunication, AI-driven security defense strategy, space-air-ground integrated\nsecurity architecture, and 6G-enabled secure UAV communication. This survey may\nprovide valuable references and new insights for researchers in the field of\nsecure physical layer communication for the LAENet."
    },
    {
        "date": "2025-04",
        "title": "Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "author": "Matthew B. Webster, Dongheon Lee, and Joonnyong Lee",
        "link": "http://arxiv.org/abs/2504.09132v1",
        "abstract": "Biosignals can be viewed as mixtures measuring particular physiological\nevents, and blind source separation (BSS) aims to extract underlying source\nsignals from mixtures. This paper proposes a self-supervised multi-encoder\nautoencoder (MEAE) to separate heartbeat-related source signals from\nphotoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG\ndata. The MEAE is trained on PPG signals from a large open polysomnography\ndatabase without any pre-processing or data selection. The trained network is\nthen applied to a noisy PPG dataset collected during the daily activities of\nnine subjects. The extracted heartbeat-related source signal significantly\nimproves HR detection as compared to the original PPG. The absence of\npre-processing and the self-supervised nature of the proposed method, combined\nwith its strong performance, highlight the potential of BSS in biosignal\nanalysis."
    },
    {
        "date": "2025-04",
        "title": "CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift",
        "author": "Jiongchi Yu, Xiaofei Xie, Qiang Hu, Bowen Zhang, Ziming Zhao, Yun Lin, Lei Ma, Ruitao Feng, and Frank Liau",
        "link": "http://arxiv.org/abs/2504.09115v1",
        "abstract": "With the rapid advancement of cloud-native computing, securing cloud\nenvironments has become an important task. Log-based Anomaly Detection (LAD) is\nthe most representative technique used in different systems for attack\ndetection and safety guarantee, where multiple LAD methods and relevant\ndatasets have been proposed. However, even though some of these datasets are\nspecifically prepared for cloud systems, they only cover limited cloud\nbehaviors and lack information from a whole-system perspective. Besides,\nanother critical issue to consider is normality shift, which implies the test\ndistribution could differ from the training distribution and highly affects the\nperformance of LAD. Unfortunately, existing works only focus on simple shift\ntypes such as chronological changes, while other important and cloud-specific\nshift types are ignored, e.g., the distribution shift introduced by different\ndeployed cloud architectures. Therefore, creating a new dataset that covers\ndiverse behaviors of cloud systems and normality shift types is necessary.\n  To fill the gap in evaluating LAD under real-world conditions, we present\nCAShift, the first normality shift-aware dataset for cloud systems. CAShift\ncaptures three shift types, including application, version, and cloud\narchitecture shifts, and includes 20 diverse attack scenarios across various\ncloud components. Using CAShift, we conduct an empirical study showing that (1)\nall LAD methods are significantly affected by normality shifts, with\nperformance drops of up to 34%, and (2) continuous learning techniques can\nimprove F1-scores by up to 27%, depending on data usage and algorithm choice.\nBased on our findings, we offer valuable implications for future research in\ndesigning more robust LAD models and methods for LAD shift adaptation."
    },
    {
        "date": "2025-04",
        "title": "Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "author": "Jiawei Li",
        "link": "http://arxiv.org/abs/2504.09026v1",
        "abstract": "Instruction fine-tuning attacks pose a significant threat to large language\nmodels (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which\ncan trigger harmful or unintended responses across a range of tasks. This\nundermines model alignment and poses security risks in real-world deployment.\nIn this work, we present a simple and effective approach to detect and mitigate\nsuch attacks using influence functions, a classical statistical tool adapted\nfor machine learning interpretation. Traditionally, the high computational\ncosts of influence functions have limited their application to large models and\ndatasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate\nCurvature (EK-FAC) approximation method enables efficient influence score\ncomputation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model\ninstruction fine-tuning attacks on large-scale datasets, as both the\ninstruction fine-tuning attack on language models and the influence calculation\napproximation technique are relatively new. Our large-scale empirical\nevaluation of influence functions on 50,000 fine-tuning examples and 32 tasks\nreveals a strong association between influence scores and sentiment. Building\non this, we introduce a novel sentiment transformation combined with influence\nfunctions to detect and remove critical poisons -- poisoned data points that\nskew model predictions. Removing these poisons (only 1% of total data) recovers\nmodel performance to near-clean levels, demonstrating the effectiveness and\nefficiency of our approach. Artifact is available at\nhttps://github.com/lijiawei20161002/Poison-Detection.\n  WARNING: This paper contains offensive data examples."
    },
    {
        "date": "2025-04",
        "title": "Robust Steganography from Large Language Models",
        "author": "Neil Perry, Sanket Gupte, Nishant Pitta, and Lior Rotem",
        "link": "http://arxiv.org/abs/2504.08977v1",
        "abstract": "Recent steganographic schemes, starting with Meteor (CCS'21), rely on\nleveraging large language models (LLMs) to resolve a historically-challenging\ntask of disguising covert communication as ``innocent-looking''\nnatural-language communication. However, existing methods are vulnerable to\n``re-randomization attacks,'' where slight changes to the communicated text,\nthat might go unnoticed, completely destroy any hidden message. This is also a\nvulnerability in more traditional encryption-based stegosystems, where\nadversaries can modify the randomness of an encryption scheme to destroy the\nhidden message while preserving an acceptable covertext to ordinary users. In\nthis work, we study the problem of robust steganography. We introduce formal\ndefinitions of weak and strong robust LLM-based steganography, corresponding to\ntwo threat models in which natural language serves as a covertext channel\nresistant to realistic re-randomization attacks. We then propose two\nconstructions satisfying these notions. We design and implement our\nsteganographic schemes that embed arbitrary secret messages into natural\nlanguage text generated by LLMs, ensuring recoverability even under adversarial\nparaphrasing and rewording attacks. To support further research and real-world\ndeployment, we release our implementation and datasets for public use."
    },
    {
        "date": "2025-04",
        "title": "Exploring the Effects of Load Altering Attacks on Load Frequency Control through Python and RTDS",
        "author": "Micha\u0142 Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, and Charalambos Konstantinou",
        "link": "http://arxiv.org/abs/2504.08951v1",
        "abstract": "The modern power grid increasingly depends on advanced information and\ncommunication technology (ICT) systems to enhance performance and reliability\nthrough real-time monitoring, intelligent control, and bidirectional\ncommunication. However, ICT integration also exposes the grid to cyber-threats.\nLoad altering attacks (LAAs), which use botnets of high-wattage devices to\nmanipulate load profiles, are a notable threat to grid stability. While\nprevious research has examined LAAs, their specific impact on load frequency\ncontrol (LFC), critical for maintaining nominal frequency during load\nfluctuations, still needs to be explored. Even minor frequency deviations can\njeopardize grid operations. This study bridges the gap by analyzing LAA effects\non LFC through simulations of static and dynamic scenarios using Python and\nRTDS. The results highlight LAA impacts on frequency stability and present an\neigenvalue-based stability assessment for dynamic LAAs (DLAAs), identifying key\nparameters influencing grid resilience."
    },
    {
        "date": "2025-04",
        "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models",
        "author": "Jiahuan Long, Zhengqin Xu, Tingsong Jiang, Wen Yao, Shuai Jia, Chao Ma, and Xiaoqian Chen",
        "link": "http://arxiv.org/abs/2504.08906v1",
        "abstract": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance."
    },
    {
        "date": "2025-04",
        "title": "Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "author": "Jiaqi Lin, and Abhronil Sengupta",
        "link": "http://arxiv.org/abs/2504.08897v1",
        "abstract": "Recent research has shown the vulnerability of Spiking Neural Networks (SNNs)\nunder adversarial examples that are nearly indistinguishable from clean data in\nthe context of frame-based and event-based information. The majority of these\nstudies are constrained in generating adversarial examples using\nBackpropagation Through Time (BPTT), a gradient-based method which lacks\nbiological plausibility. In contrast, local learning methods, which relax many\nof BPTT's constraints, remain under-explored in the context of adversarial\nattacks. To address this problem, we examine adversarial robustness in SNNs\nthrough the framework of four types of training algorithms. We provide an\nin-depth analysis of the ineffectiveness of gradient-based adversarial attacks\nto generate adversarial instances in this scenario. To overcome these\nlimitations, we introduce a hybrid adversarial attack paradigm that leverages\nthe transferability of adversarial instances. The proposed hybrid approach\ndemonstrates superior performance, outperforming existing adversarial attack\nmethods. Furthermore, the generalizability of the method is assessed under\nmulti-step adversarial attacks, adversarial attacks in black-box FGSM\nscenarios, and within the non-spiking domain."
    },
    {
        "date": "2025-04",
        "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
        "author": "Vineeth Sai Narajala, and Idan Habler",
        "link": "http://arxiv.org/abs/2504.08623v1",
        "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a\nstandardized framework for artificial intelligence (AI) systems to interact\nwith external data sources and tools in real-time. While MCP offers significant\nadvantages for AI integration and capability extension, it introduces novel\nsecurity challenges that demand rigorous analysis and mitigation. This paper\nbuilds upon foundational research into MCP architecture and preliminary\nsecurity assessments to deliver enterprise-grade mitigation frameworks and\ndetailed technical implementation strategies. Through systematic threat\nmodeling and analysis of MCP implementations and analysis of potential attack\nvectors, including sophisticated threats like tool poisoning, we present\nactionable security patterns tailored for MCP implementers and adopters. The\nprimary contribution of this research lies in translating theoretical security\nconcerns into a practical, implementable framework with actionable controls,\nthereby providing essential guidance for the secure enterprise adoption and\ngovernance of integrated AI systems."
    },
    {
        "date": "2025-04",
        "title": "A Hybrid Chaos-Based Cryptographic Framework for Post-Quantum Secure Communications",
        "author": "Kevin Song, Noorullah Imran, Jake Y. Chen, and Allan C. Dobbins",
        "link": "http://arxiv.org/abs/2504.08618v1",
        "abstract": "We present CryptoChaos, a novel hybrid cryptographic framework that\nsynergizes deterministic chaos theory with cutting-edge cryptographic\nprimitives to achieve robust, post-quantum resilient encryption. CryptoChaos\nharnesses the intrinsic unpredictability of four discrete chaotic maps\n(Logistic, Chebyshev, Tent, and Henon) to generate a high-entropy,\nmultidimensional key from a unified entropy pool. This key is derived through a\nlayered process that combines SHA3-256 hashing with an ephemeral X25519\nDiffie-Hellman key exchange and is refined using an HMAC-based key derivation\nfunction (HKDF). The resulting encryption key powers AES-GCM, providing both\nconfidentiality and integrity. Comprehensive benchmarking against established\nsymmetric ciphers confirms that CryptoChaos attains near-maximal Shannon\nentropy (approximately 8 bits per byte) and exhibits negligible adjacent-byte\ncorrelations, while robust performance on the NIST SP 800-22 test suite\nunderscores its statistical rigor. Moreover, quantum simulations demonstrate\nthat the additional complexity inherent in chaotic key generation dramatically\nelevates the resource requirements for Grover-based quantum attacks, with an\nestimated T gate count of approximately 2.1 x 10^9. The modular and\ninteroperable design of CryptoChaos positions it as a promising candidate for\nhigh-assurance applications, ranging from secure communications and financial\ntransactions to IoT systems, paving the way for next-generation post-quantum\nencryption standards."
    },
    {
        "date": "2025-04",
        "title": "Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities",
        "author": "Maria Santos-Villafranca, Dustin Carri\u00f3n-Ojeda, Alejandro Perez-Yus, Jesus Bermudez-Cameo, Jose J. Guerrero, and Simone Schaub-Meyer",
        "link": "http://arxiv.org/abs/2504.08578v1",
        "abstract": "Action recognition is an essential task in egocentric vision due to its wide\nrange of applications across many fields. While deep learning methods have been\nproposed to address this task, most rely on a single modality, typically video.\nHowever, including additional modalities may improve the robustness of the\napproaches to common issues in egocentric videos, such as blurriness and\nocclusions. Recent efforts in multimodal egocentric action recognition often\nassume the availability of all modalities, leading to failures or performance\ndrops when any modality is missing. To address this, we introduce an efficient\nmultimodal knowledge distillation approach for egocentric action recognition\nthat is robust to missing modalities (KARMMA) while still benefiting when\nmultiple modalities are available. Our method focuses on resource-efficient\ndevelopment by leveraging pre-trained models as unimodal feature extractors in\nour teacher model, which distills knowledge into a much smaller and faster\nstudent model. Experiments on the Epic-Kitchens and Something-Something\ndatasets demonstrate that our student model effectively handles missing\nmodalities while reducing its accuracy drop in this scenario."
    },
    {
        "date": "2025-04",
        "title": "Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability",
        "author": "Sabrine Ennaji, Elhadj Benkhelifa, and Luigi Vincenzo Mancini",
        "link": "http://arxiv.org/abs/2504.08480v1",
        "abstract": "Transferability-based adversarial attacks exploit the ability of adversarial\nexamples, crafted to deceive a specific source Intrusion Detection System (IDS)\nmodel, to also mislead a target IDS model without requiring access to the\ntraining data or any internal model parameters. These attacks exploit common\nvulnerabilities in machine learning models to bypass security measures and\ncompromise systems. Although the transferability concept has been widely\nstudied, its practical feasibility remains limited due to assumptions of high\nsimilarity between source and target models. This paper analyzes the core\nfactors that contribute to transferability, including feature alignment, model\narchitectural similarity, and overlap in the data distributions that each IDS\nexamines. We propose a novel metric, the Transferability Feasibility Score\n(TFS), to assess the feasibility and reliability of such attacks based on these\nfactors. Through experimental evidence, we demonstrate that TFS and actual\nattack success rates are highly correlated, addressing the gap between\ntheoretical understanding and real-world impact. Our findings provide needed\nguidance for designing more realistic transferable adversarial attacks,\ndeveloping robust defenses, and ultimately improving the security of machine\nlearning-based IDS in critical systems."
    },
    {
        "date": "2025-04",
        "title": "On Transfer-based Universal Attacks in Pure Black-box Setting",
        "author": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Ajmal Mian, Nazanin Rahnavard, and Mubarak Shah",
        "link": "http://arxiv.org/abs/2504.08866v1",
        "abstract": "Despite their impressive performance, deep visual models are susceptible to\ntransferable black-box adversarial attacks. Principally, these attacks craft\nperturbations in a target model-agnostic manner. However, surprisingly, we find\nthat existing methods in this domain inadvertently take help from various\npriors that violate the black-box assumption such as the availability of the\ndataset used to train the target model, and the knowledge of the number of\nclasses in the target model. Consequently, the literature fails to articulate\nthe true potency of transferable black-box attacks. We provide an empirical\nstudy of these biases and propose a framework that aids in a prior-free\ntransparent study of this paradigm. Using our framework, we analyze the role of\nprior knowledge of the target model data and number of classes in attack\nperformance. We also provide several interesting insights based on our\nanalysis, and demonstrate that priors cause overestimation in transferability\nscores. Finally, we extend our framework to query-based attacks. This extension\ninspires a novel image-blending technique to prepare data for effective\nsurrogate model training."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Examples in Environment Perception for Automated Driving (Review)",
        "author": "Jun Yan, and Huilin Yin",
        "link": "http://arxiv.org/abs/2504.08414v1",
        "abstract": "The renaissance of deep learning has led to the massive development of\nautomated driving. However, deep neural networks are vulnerable to adversarial\nexamples. The perturbations of adversarial examples are imperceptible to human\neyes but can lead to the false predictions of neural networks. It poses a huge\nrisk to artificial intelligence (AI) applications for automated driving. This\nsurvey systematically reviews the development of adversarial robustness\nresearch over the past decade, including the attack and defense methods and\ntheir applications in automated driving. The growth of automated driving pushes\nforward the realization of trustworthy AI applications. This review lists\nsignificant references in the research history of adversarial examples."
    },
    {
        "date": "2025-04",
        "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation",
        "author": "Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2504.08411v1",
        "abstract": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability."
    },
    {
        "date": "2025-04",
        "title": "Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking",
        "author": "Huu-Loc Tran, Tinh-Anh Nguyen-Nhu, Huu-Phong Phan-Nguyen, Tien-Huy Nguyen, Nhat-Minh Nguyen-Dich, Anh Dao, Huy-Duc Do, Quan Nguyen, Hoang M. Le, and Quang-Vinh Dinh",
        "link": "http://arxiv.org/abs/2504.08384v1",
        "abstract": "Long-form video understanding presents significant challenges for interactive\nretrieval systems, as conventional methods struggle to process extensive video\ncontent efficiently. Existing approaches often rely on single models,\ninefficient storage, unstable temporal search, and context-agnostic reranking,\nlimiting their effectiveness. This paper presents a novel framework to enhance\ninteractive video retrieval through four key innovations: (1) an ensemble\nsearch strategy that integrates coarse-grained (CLIP) and fine-grained (BEIT3)\nmodels to improve retrieval accuracy, (2) a storage optimization technique that\nreduces redundancy by selecting representative keyframes via TransNetV2 and\ndeduplication, (3) a temporal search mechanism that localizes video segments\nusing dual queries for start and end points, and (4) a temporal reranking\napproach that leverages neighboring frame context to stabilize rankings.\nEvaluated on known-item search and question-answering tasks, our framework\ndemonstrates substantial improvements in retrieval precision, efficiency, and\nuser interpretability, offering a robust solution for real-world interactive\nvideo retrieval applications."
    },
    {
        "date": "2025-04",
        "title": "Practical Secure Aggregation by Combining Cryptography and Trusted Execution Environments",
        "author": "Romain de Laage, Peterson Yuhala, Fran\u00e7ois-Xavier Wicht, Pascal Felber, Christian Cachin, and Valerio Schiavoni",
        "link": "http://arxiv.org/abs/2504.08325v1",
        "abstract": "Secure aggregation enables a group of mutually distrustful parties, each\nholding private inputs, to collaboratively compute an aggregate value while\npreserving the privacy of their individual inputs. However, a major challenge\nin adopting secure aggregation approaches for practical applications is the\nsignificant computational overhead of the underlying cryptographic protocols,\ne.g. fully homomorphic encryption. This overhead makes secure aggregation\nprotocols impractical, especially for large datasets. In contrast,\nhardware-based security techniques such as trusted execution environments\n(TEEs) enable computation at near-native speeds, making them a promising\nalternative for reducing the computational burden typically associated with\npurely cryptographic techniques. Yet, in many scenarios, parties may opt for\neither cryptographic or hardware-based security mechanisms, highlighting the\nneed for hybrid approaches. In this work, we introduce several secure\naggregation architectures that integrate both cryptographic and TEE-based\ntechniques, analyzing the trade-offs between security and performance."
    },
    {
        "date": "2025-04",
        "title": "To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning",
        "author": "Justin Feng, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2504.08264v1",
        "abstract": "The increasing use of the Internet of Things raises security concerns. To\naddress this, device fingerprinting is often employed to authenticate devices,\ndetect adversaries, and identify eavesdroppers in an environment. This requires\nthe ability to discern between legitimate and malicious devices which is\nachieved by analyzing the unique physical and/or operational characteristics of\nIoT devices. In the era of the latest progress in machine learning,\nparticularly generative models, it is crucial to methodically examine the\ncurrent studies in device fingerprinting. This involves explaining their\napproaches and underscoring their limitations when faced with adversaries armed\nwith these ML tools. To systematically analyze existing methods, we propose a\ngeneric, yet simplified, model for device fingerprinting. Additionally, we\nthoroughly investigate existing methods to authenticate devices and detect\neavesdropping, using our proposed model. We further study trends and\nsimilarities between works in authentication and eavesdropping detection and\npresent the existing threats and attacks in these domains. Finally, we discuss\nfuture directions in fingerprinting based on these trends to develop more\nsecure IoT fingerprinting schemes."
    },
    {
        "date": "2025-04",
        "title": "Hardware Design and Security Needs Attention: From Survey to Path Forward",
        "author": "Sujan Ghimire, Muhtasim Alam Chowdhury, Banafsheh Saber Latibari, Muntasir Mamun, Jaeden Wolf Carpenter, Benjamin Tan, Hammond Pearce, Pratik Satam, and Soheil Salehi",
        "link": "http://arxiv.org/abs/2504.08854v1",
        "abstract": "Recent advances in attention-based artificial intelligence (AI) models have\nunlocked vast potential to automate digital hardware design while enhancing and\nstrengthening security measures against various threats. This rapidly emerging\nfield leverages Large Language Models (LLMs) to generate HDL code, identify\nvulnerabilities, and sometimes mitigate them. The state of the art in this\ndesign automation space utilizes optimized LLMs with HDL datasets, creating\nautomated systems for register-transfer level (RTL) generation, verification,\nand debugging, and establishing LLM-driven design environments for streamlined\nlogic designs. Additionally, attention-based models like graph attention have\nshown promise in chip design applications, including floorplanning. This survey\ninvestigates the integration of these models into hardware-related domains,\nemphasizing logic design and hardware security, with or without the use of IP\nlibraries. This study explores the commercial and academic landscape,\nhighlighting technical hurdles and future prospects for automating hardware\ndesign and security. Moreover, it provides new insights into the study of\nLLM-driven design systems, advances in hardware security mechanisms, and the\nimpact of influential works on industry practices. Through the examination of\n30 representative approaches and illustrative case studies, this paper\nunderscores the transformative potential of attention-based models in\nrevolutionizing hardware design while addressing the challenges that lie ahead\nin this interdisciplinary domain."
    },
    {
        "date": "2025-04",
        "title": "DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments",
        "author": "Sheikh Muhammad Farjad",
        "link": "http://arxiv.org/abs/2504.08227v1",
        "abstract": "DaemonSec is an early-stage startup exploring machine learning (ML)-based\nsecurity for Linux daemons, a critical yet often overlooked attack surface.\nWhile daemon security remains underexplored, conventional defenses struggle\nagainst adaptive threats and zero-day exploits. To assess the perspectives of\nIT professionals on ML-driven daemon protection, a systematic interview study\nbased on semi-structured interviews was conducted with 22 professionals from\nindustry and academia. The study evaluates adoption, feasibility, and trust in\nML-based security solutions. While participants recognized the potential of ML\nfor real-time anomaly detection, findings reveal skepticism toward full\nautomation, limited security awareness among non-security roles, and concerns\nabout patching delays creating attack windows. This paper presents the methods,\nkey findings, and implications for advancing ML-driven daemon security in\nindustry."
    },
    {
        "date": "2025-04",
        "title": "EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models",
        "author": "Minjae Seo, Myoungsung You, Junhee Lee, Jaehan Kim, Hwanjo Heo, Jintae Oh, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.08205v1",
        "abstract": "Vision models are increasingly deployed in critical applications such as\nautonomous driving and CCTV monitoring, yet they remain susceptible to\nresource-consuming attacks. In this paper, we introduce a novel\nenergy-overloading attack that leverages vision language model (VLM) prompts to\ngenerate adversarial images targeting vision models. These images, though\nimperceptible to the human eye, significantly increase GPU energy consumption\nacross various vision models, threatening the availability of these systems.\nOur framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it\nis not limited by the architecture or type of the target vision model. By\nexploiting the lack of safety filters in VLMs like DALL-E 3, we create\nadversarial noise images without requiring prior knowledge or internal\nstructure of the target vision models. Our experiments demonstrate up to a 50%\nincrease in energy consumption, revealing a critical vulnerability in current\nvision models."
    },
    {
        "date": "2025-04",
        "title": "A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "author": "Yixuan Zhang, Dongyan Huo, Yudong Chen, and Qiaomin Xie",
        "link": "http://arxiv.org/abs/2504.08178v3",
        "abstract": "Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds."
    },
    {
        "date": "2025-04",
        "title": "GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in WAFs",
        "author": "Vahid Babaey, and Arun Ravindran",
        "link": "http://arxiv.org/abs/2504.08176v1",
        "abstract": "The increasing reliance on web services has led to a rise in cybersecurity\nthreats, particularly Cross-Site Scripting (XSS) attacks, which target\nclient-side layers of web applications by injecting malicious scripts.\nTraditional Web Application Firewalls (WAFs) struggle to detect highly\nobfuscated and complex attacks, as their rules require manual updates. This\npaper presents a novel generative AI framework that leverages Large Language\nModels (LLMs) to enhance XSS mitigation. The framework achieves two primary\nobjectives: (1) generating sophisticated and syntactically validated XSS\npayloads using in-context learning, and (2) automating defense mechanisms by\ntesting these attacks against a vulnerable application secured by a WAF,\nclassifying bypassing attacks, and generating effective WAF security rules.\nExperimental results using GPT-4o demonstrate the framework's effectiveness\ngenerating 264 XSS payloads, 83% of which were validated, with 80% bypassing\nModSecurity WAF equipped with an industry standard security rule set developed\nby the Open Web Application Security Project (OWASP) to protect against web\nvulnerabilities. Through rule generation, 86% of previously successful attacks\nwere blocked using only 15 new rules. In comparison, Google Gemini Pro achieved\na lower bypass rate of 63%, highlighting performance differences across LLMs."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
        "author": "Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, and Domenico Talia",
        "link": "http://arxiv.org/abs/2504.07887v1",
        "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndriving advancements in machine translation, summarization, and conversational\nagents. However, their increasing integration into critical societal domains\nhas raised concerns about embedded biases, which can perpetuate stereotypes and\ncompromise fairness. These biases stem from various sources, including\nhistorical inequalities in training data, linguistic imbalances, and\nadversarial manipulation. Despite mitigation efforts, recent studies indicate\nthat LLMs remain vulnerable to adversarial attacks designed to elicit biased\nresponses. This work proposes a scalable benchmarking framework to evaluate LLM\nrobustness against adversarial bias elicitation. Our methodology involves (i)\nsystematically probing models with a multi-task approach targeting biases\nacross various sociocultural dimensions, (ii) quantifying robustness through\nsafety scores using an LLM-as-a-Judge approach for automated assessment of\nmodel responses, and (iii) employing jailbreak techniques to investigate\nvulnerabilities in safety mechanisms. Our analysis examines prevalent biases in\nboth small and large state-of-the-art models and their impact on model safety.\nAdditionally, we assess the safety of domain-specific models fine-tuned for\ncritical fields, such as medicine. Finally, we release a curated dataset of\nbias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability\nbenchmarking. Our findings reveal critical trade-offs between model size and\nsafety, aiding the development of fairer and more robust future language\nmodels."
    },
    {
        "date": "2025-04",
        "title": "QubitHammer Attacks: Qubit Flipping Attacks in Multi-tenant Superconducting Quantum Computers",
        "author": "Yizhuo Tan, Navnil Choudhury, Kanad Basu, and Jakub Szefer",
        "link": "http://arxiv.org/abs/2504.07875v1",
        "abstract": "Quantum computing is rapidly evolving its capabilities, with a corresponding\nsurge in its deployment within cloud-based environments. Various quantum\ncomputers are accessible today via pay-as-you-go cloud computing models,\noffering unprecedented convenience. Due to its rapidly growing demand, quantum\ncomputers are shifting from a single-tenant to a multi-tenant model to enhance\nresource utilization. However, this widespread accessibility to shared\nmulti-tenant systems also introduces potential security vulnerabilities. In\nthis work, we present for the first time a set of novel attacks, named together\nas the QubitHammer attacks, which target state-of-the-art superconducting\nquantum computers. We show that in a multi-tenant cloud-based quantum system,\nan adversary with the basic capability to deploy custom pulses, similar to any\nstandard user today, can utilize the QubitHammer attacks to significantly\ndegrade the fidelity of victim circuits located on the same quantum computer.\nUpon extensive evaluation, the QubitHammer attacks achieve a very high\nvariational distance of up to 0.938 from the expected outcome, thus\ndemonstrating their potential to degrade victim computation. Our findings\nexhibit the effectiveness of these attacks across various superconducting\nquantum computers from a leading vendor, suggesting that QubitHammer represents\na new class of security attacks. Further, the attacks are demonstrated to\nbypass all existing defenses proposed so far for ensuring the reliability in\nmulti-tenant superconducting quantum computers."
    },
    {
        "date": "2025-04",
        "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
        "author": "Mengjia Niu, Hamed Haddadi, and Guansong Pang",
        "link": "http://arxiv.org/abs/2504.07863v1",
        "abstract": "Hallucinations in large language models (LLMs) pose significant safety\nconcerns that impede their broader deployment. Recent research in hallucination\ndetection has demonstrated that LLMs' internal representations contain\ntruthfulness hints, which can be harnessed for detector training. However, the\nperformance of these detectors is heavily dependent on the internal\nrepresentations of predetermined tokens, fluctuating considerably when working\non free-form generations with varying lengths and sparse distributions of\nhallucinated entities. To address this, we propose HaMI, a novel approach that\nenables robust detection of hallucinations through adaptive selection and\nlearning of critical tokens that are most indicative of hallucinations. We\nachieve this robustness by an innovative formulation of the Hallucination\ndetection task as Multiple Instance (HaMI) learning over token-level\nrepresentations within a sequence, thereby facilitating a joint optimisation of\ntoken selection and hallucination detection on generation sequences of diverse\nforms. Comprehensive experimental results on four hallucination benchmarks show\nthat HaMI significantly outperforms existing state-of-the-art approaches."
    },
    {
        "date": "2025-04",
        "title": "Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security",
        "author": "Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, and Suresh Venkatasubramanian",
        "link": "http://arxiv.org/abs/2504.07719v1",
        "abstract": "Financial instability has become a significant issue in today's society.\nWhile research typically focuses on financial aspects, there is a tendency to\noverlook time-related aspects of unstable work schedules. The inability to rely\non consistent work schedules leads to burnout, work-family conflicts, and\nfinancial shocks that directly impact workers' income and assets. Unforeseen\nfluctuations in earnings pose challenges in financial planning, affecting\ndecisions on savings and spending and ultimately undermining individuals'\nlong-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience\nfrequently changing schedules without sufficient notice, including those in the\nfood service and retail sectors, part-time and hourly workers, and individuals\nwith lower incomes. These groups are already more financially vulnerable, and\nthe unpredictable nature of their schedules exacerbates their financial\nfragility.\n  Our objective is to understand how unforeseen fluctuations in earnings\nexacerbate financial fragility by investigating the extent to which\nindividuals' financial management depends on their ability to anticipate and\nplan for the future. To address this question, we develop a simulation\nframework that models how individuals optimize utility amidst financial\nuncertainty and the imperative to avoid financial ruin. We employ online\nlearning techniques, specifically adapting workers' consumption policies based\non evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a\nworker's capacity to anticipate schedule changes enhances their long-term\nutility. Conversely, the inability to predict future events can worsen workers'\ninstability. Moreover, our framework enables us to explore interventions to\nmitigate the problem of schedule uncertainty and evaluate their effectiveness."
    },
    {
        "date": "2025-04",
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
        "author": "Yang Jiao, Xiaodong Wang, and Kai Yang",
        "link": "http://arxiv.org/abs/2504.07717v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
    },
    {
        "date": "2025-04",
        "title": "RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions",
        "author": "Youngwan Jin, Michal Kovac, Yagiz Nalcakan, Hyeongjin Ju, Hanbin Song, Sanghyeop Yeo, and Shiho Kim",
        "link": "http://arxiv.org/abs/2504.07603v1",
        "abstract": "Current autonomous driving algorithms heavily rely on the visible spectrum,\nwhich is prone to performance degradation in adverse conditions like fog, rain,\nsnow, glare, and high contrast. Although other spectral bands like\nnear-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception\nin such situations, they have limitations and lack large-scale datasets and\nbenchmarks. Short-wave infrared (SWIR) imaging offers several advantages over\nNIR and LWIR. However, no publicly available large-scale datasets currently\nincorporate SWIR data for autonomous driving. To address this gap, we introduce\nthe RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000\nsynchronized and spatially aligned RGB-SWIR image pairs collected across\ndiverse locations, lighting, and weather conditions. In addition, we provide a\nsubset for RGB-SWIR translation and object detection annotations for a subset\nof challenging traffic scenarios to demonstrate the utility of SWIR imaging\nthrough experiments on both object detection and RGB-to-SWIR image translation.\nOur experiments show that combining RGB and SWIR data in an ensemble framework\nsignificantly improves detection accuracy compared to RGB-only approaches,\nparticularly in conditions where visible-spectrum sensors struggle. We\nanticipate that the RASMD dataset will advance research in multispectral\nimaging for autonomous driving and robust perception systems."
    },
    {
        "date": "2025-04",
        "title": "DWFS-Obfuscation: Dynamic Weighted Feature Selection for Robust Malware Familial Classification under Obfuscation",
        "author": "Xingyuan Wei, Zijun Cheng, Ning Li, Qiujian Lv, Ziyang Yu, and Degang Sun",
        "link": "http://arxiv.org/abs/2504.07590v1",
        "abstract": "Due to its open-source nature, the Android operating system has consistently\nbeen a primary target for attackers. Learning-based methods have made\nsignificant progress in the field of Android malware detection. However,\ntraditional detection methods based on static features struggle to identify\nobfuscated malicious code, while methods relying on dynamic analysis suffer\nfrom low efficiency. To address this, we propose a dynamic weighted feature\nselection method that analyzes the importance and stability of features,\ncalculates scores to filter out the most robust features, and combines these\nselected features with the program's structural information. We then utilize\ngraph neural networks for classification, thereby improving the robustness and\naccuracy of the detection system. We analyzed 8,664 malware samples from eight\nmalware families and tested a total of 44,940 malware variants generated using\nseven obfuscation strategies. Experiments demonstrate that our proposed method\nachieves an F1-score of 95.56% on the unobfuscated dataset and 92.28% on the\nobfuscated dataset, indicating that the model can effectively detect obfuscated\nmalware."
    },
    {
        "date": "2025-04",
        "title": "MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting",
        "author": "Minjae Seo, Myoungsung You, Jaehan Kim, Taejune Park, Seungwon Shin, and Jinwoo Kim",
        "link": "http://arxiv.org/abs/2504.07543v2",
        "abstract": "Tor, a widely utilized privacy network, enables anonymous communication but\nis vulnerable to flow correlation attacks that deanonymize users by correlating\ntraffic patterns from Tor's ingress and egress segments. Various defenses have\nbeen developed to mitigate these attacks; however, they have two critical\nlimitations: (i) significant network overhead during obfuscation and (ii) a\nlack of dynamic obfuscation for egress segments, exposing traffic patterns to\nadversaries. In response, we introduce MUFFLER, a novel connection-level\ntraffic obfuscation system designed to secure Tor egress traffic. It\ndynamically maps real connections to a distinct set of virtual connections\nbetween the final Tor nodes and targeted services, either public or hidden.\nThis approach creates egress traffic patterns fundamentally different from\nthose at ingress segments without adding intentional padding bytes or timing\ndelays. The mapping of real and virtual connections is adjusted in real-time\nbased on ongoing network conditions, thwarting adversaries' efforts to detect\negress traffic patterns. Extensive evaluations show that MUFFLER mitigates\npowerful correlation attacks with a TPR of 1% at an FPR of 10^-2 while imposing\nonly a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency\noverhead than existing solutions and seamlessly integrates with the current Tor\narchitecture."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
        "author": "Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, and Klemens B\u00f6hm",
        "link": "http://arxiv.org/abs/2504.07522v1",
        "abstract": "Outlier detection in high-dimensional tabular data is challenging since data\nis often distributed across multiple lower-dimensional subspaces -- a\nphenomenon known as the Multiple Views effect (MV). This effect led to a large\nbody of research focused on mining such subspaces, known as subspace selection.\nHowever, as the precise nature of the MV effect was not well understood,\ntraditional methods had to rely on heuristic-driven search schemes that\nstruggle to accurately capture the true structure of the data. Properly\nidentifying these subspaces is critical for unsupervised tasks such as outlier\ndetection or clustering, where misrepresenting the underlying data structure\ncan hinder the performance. We introduce Myopic Subspace Theory (MST), a new\ntheoretical framework that mathematically formulates the Multiple Views effect\nand writes subspace selection as a stochastic optimization problem. Based on\nMST, we introduce V-GAN, a generative method trained to solve such an\noptimization problem. This approach avoids any exhaustive search over the\nfeature space while ensuring that the intrinsic data structure is preserved.\nExperiments on 42 real-world datasets show that using V-GAN subspaces to build\nensemble methods leads to a significant increase in one-class classification\nperformance -- compared to existing subspace selection, feature selection, and\nembedding methods. Further experiments on synthetic data show that V-GAN\nidentifies subspaces more accurately while scaling better than other relevant\nsubspace selection methods. These results confirm the theoretical guarantees of\nour approach and also highlight its practical viability in high-dimensional\nsettings."
    },
    {
        "date": "2025-04",
        "title": "Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
        "author": "Caroline Panggabean, Chandrasekar Venkatachalam, Priyanka Shah, Sincy John, Renuka Devi P, and Shanmugavalli Venkatachalam",
        "link": "http://arxiv.org/abs/2504.07478v1",
        "abstract": "Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS)\nattacks remains a critical challenge in cybersecurity. This research introduces\na hybrid deep learning model combining Gated Recurrent Units (GRUs) and a\nNeural Turing Machine (NTM) for enhanced intrusion detection. Trained on the\nUNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential\ndata processing and an NTM for long-term pattern recognition. The proposed\napproach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS\ntraffic. These findings offer promising advancements in real-time threat\ndetection and contribute to improved network security across various domains."
    },
    {
        "date": "2025-04",
        "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer",
        "author": "Huilin Yin, Pengyu Wang, Senmao Li, Jun Yan, and Daniel Watzenig",
        "link": "http://arxiv.org/abs/2504.07441v1",
        "abstract": "Robust object detection for Unmanned Surface Vehicles (USVs) in complex water\nenvironments is essential for reliable navigation and operation. Specifically,\nwater surface object detection faces challenges from blurred edges and diverse\nobject scales. Although vision-radar fusion offers a feasible solution,\nexisting approaches suffer from cross-modal feature conflicts, which negatively\naffect model robustness. To address this problem, we propose a robust\nvision-radar fusion model WS-DETR. In particular, we first introduce a\nMulti-Scale Edge Information Integration (MSEII) module to enhance edge\nperception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale\nobject detection in the encoder. Then, we adopt self-moving point\nrepresentations for continuous convolution and residual connection to\nefficiently extract irregular features under the scenarios of irregular point\ncloud data. To further mitigate cross-modal conflicts, an Adaptive Feature\nInteractive Fusion (AFIF) module is introduced to integrate visual and radar\nfeatures through geometric alignment and semantic fusion. Extensive experiments\non the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art\n(SOTA) performance, maintaining its superiority even under adverse weather and\nlighting conditions."
    },
    {
        "date": "2025-04",
        "title": "Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy",
        "author": "Takao Murakami, Yuichi Sei, and Reo Eriguchi",
        "link": "http://arxiv.org/abs/2504.07362v1",
        "abstract": "The shuffle model of DP (Differential Privacy) provides high utility by\nintroducing a shuffler that randomly shuffles noisy data sent from users.\nHowever, recent studies show that existing shuffle protocols suffer from the\nfollowing two major drawbacks. First, they are vulnerable to local data\npoisoning attacks, which manipulate the statistics about input data by sending\ncrafted data, especially when the privacy budget epsilon is small. Second, the\nactual value of epsilon is increased by collusion attacks by the data collector\nand users.\n  In this paper, we address these two issues by thoroughly exploring the\npotential of the augmented shuffle model, which allows the shuffler to perform\nadditional operations, such as random sampling and dummy data addition.\nSpecifically, we propose a generalized framework for local-noise-free protocols\nin which users send (encrypted) input data to the shuffler without adding\nnoise. We show that this generalized protocol provides DP and is robust to the\nabove two attacks if a simpler mechanism that performs the same process on\nbinary input data provides DP. Based on this framework, we propose three\nconcrete protocols providing DP and robustness against the two attacks. Our\nfirst protocol generates the number of dummy values for each item from a\nbinomial distribution and provides higher utility than several state-of-the-art\nexisting shuffle protocols. Our second protocol significantly improves the\nutility of our first protocol by introducing a novel dummy-count distribution:\nasymmetric two-sided geometric distribution. Our third protocol is a special\ncase of our second protocol and provides pure epsilon-DP. We show the\neffectiveness of our protocols through theoretical analysis and comprehensive\nexperiments."
    },
    {
        "date": "2025-04",
        "title": "Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey",
        "author": "Aaron Yu, Iuliia Kolotylo, Hashim A. Hashim, and A. E. E. Eltoukhy",
        "link": "http://arxiv.org/abs/2504.07358v1",
        "abstract": "Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air\nmobility, and the reliability of UAV avionics systems is critical to ensuring\nmission success, sustainability practices, and public safety. The success of\nUAV missions depends on effectively mitigating various aspects of electronic\nwarfare, including non-destructive and destructive cyberattacks, transponder\nvulnerabilities, and jamming threats, while rigorously implementing\ncountermeasures and defensive aids. This paper provides a comprehensive review\nof UAV cyberattacks, countermeasures, and defensive strategies. It explores\nUAV-to-UAV coordination attacks and their associated features, such as dispatch\nsystem attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks,\nTraffic Alert and Collision Avoidance System (TCAS)-induced collisions, and\nTCAS attacks. Additionally, the paper examines UAV-to-command center\ncoordination attacks, as well as UAV functionality attacks. The review also\ncovers various countermeasures and defensive aids designed for UAVs. Lastly, a\ncomparison of common cyberattacks and countermeasure approaches is conducted,\nalong with a discussion of future trends in the field. Keywords: Electronic\nwarfare, UAVs, Avionics Systems, cyberattacks, coordination attacks,\nfunctionality attacks, countermeasure, defensive-aids."
    },
    {
        "date": "2025-04",
        "title": "Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2504.07345v1",
        "abstract": "The cacophony of urban sounds presents a significant challenge for smart city\napplications that rely on accurate acoustic scene analysis. Effectively\nanalyzing these complex soundscapes, often characterized by overlapping sound\nsources, diverse acoustic events, and unpredictable noise levels, requires\nprecise source separation. This task becomes more complicated when only limited\ntraining data is available. This paper introduces a novel Quantum-Inspired\nGenetic Algorithm (p-QIGA) for source separation, drawing inspiration from\nquantum information theory to enhance acoustic scene analysis in smart cities.\nBy leveraging quantum superposition for efficient solution space exploration\nand entanglement to handle correlated sources, p-QIGA achieves robust\nseparation even with limited data. These quantum-inspired concepts are\nintegrated into a genetic algorithm framework to optimize source separation\nparameters. The effectiveness of our approach is demonstrated on two datasets:\nthe TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban\nsoundscapes, and the Silent Cities dataset, capturing quieter urban\nenvironments during the COVID-19 pandemic. Experimental results show that the\np-QIGA achieves accuracy comparable to state-of-the-art methods while\nexhibiting superior resilience to noise and limited training data, achieving up\nto 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and\noutperforming baseline methods by up to 2 dB with only 10% of the training\ndata. This research highlights the potential of p-QIGA to advance acoustic\nsignal processing in smart cities, particularly for noise pollution monitoring\nand acoustic surveillance."
    },
    {
        "date": "2025-04",
        "title": "Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism",
        "author": "Gabriel K. Gegenhuber, Philipp \u00c9. Frenzel, Maximilian G\u00fcnther, and Aljosha Judmayer",
        "link": "http://arxiv.org/abs/2504.07323v1",
        "abstract": "WhatsApp, the world's largest messaging application, uses a version of the\nSignal protocol to provide end-to-end encryption (E2EE) with strong security\nguarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from\nthe start of a new conversation -- even when the recipient is offline -- a\nstash of ephemeral (one-time) prekeys must be stored on a server. While the\ncritical role of these one-time prekeys in achieving PFS has been outlined in\nthe Signal specification, we are the first to demonstrate a targeted depletion\nattack against them on individual WhatsApp user devices. Our findings not only\nreveal an attack that can degrade PFS for certain messages, but also expose\ninherent privacy risks and serious availability implications arising from the\nrefilling and distribution procedure essential for this security mechanism."
    },
    {
        "date": "2025-04",
        "title": "Context Switching for Secure Multi-programming of Near-Term Quantum Computers",
        "author": "Avinash Kumar, Meng Wang, Chenxu Liu, Ang Li, Prashant J. Nair, and Poulami Das",
        "link": "http://arxiv.org/abs/2504.07048v2",
        "abstract": "Multi-programming quantum computers improve device utilization and\nthroughput. However, crosstalk from concurrent two-qubit CNOT gates poses\nsecurity risks, compromising the fidelity and output of co-running victim\nprograms. We design Zero Knowledge Tampering Attacks (ZKTAs), using which\nattackers can exploit crosstalk without knowledge of the hardware error\nprofile. ZKTAs can alter victim program outputs in 40% of cases on commercial\nsystems.\n  We identify that ZKTAs succeed because the attacker's program consistently\nruns with the same victim program in a fixed context. To mitigate this, we\npropose QONTEXTS: a context-switching technique that defends against ZKTAs by\nrunning programs across multiple contexts, each handling only a subset of\ntrials. QONTEXTS uses multi-programming with frequent context switching while\nidentifying a unique set of programs for each context. This helps limit only a\nfraction of execution to ZKTAs. We enhance QONTEXTS with attack detection\ncapabilities that compare the distributions from different contexts against\neach other to identify noisy contexts executed with ZKTAs. Our evaluations on\nreal IBMQ systems show that QONTEXTS increases program resilience by three\norders of magnitude and fidelity by 1.33$\\times$ on average. Moreover, QONTEXTS\nimproves throughput by 2$\\times$, advancing security in multi-programmed\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Efficient Storage Integrity in Adversarial Settings",
        "author": "Quinn Burke, Ryan Sheatsley, Yohan Beugin, Eric Pauley, Owen Hines, Michael Swift, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2504.07041v1",
        "abstract": "Storage integrity is essential to systems and applications that use untrusted\nstorage (e.g., public clouds, end-user devices). However, known methods for\nachieving storage integrity either suffer from high (and often prohibitive)\noverheads or provide weak integrity guarantees. In this work, we demonstrate a\nhybrid approach to storage integrity that simultaneously reduces overhead while\nproviding strong integrity guarantees. Our system, partially asynchronous\nintegrity checking (PAC), allows disk write commitments to be deferred while\nstill providing guarantees around read integrity. PAC delivers a 5.5X\nthroughput and latency improvement over the state of the art, and 85% of the\nthroughput achieved by non-integrity-assuring approaches. In this way, we show\nthat untrusted storage can be used for integrity-critical workloads without\nmeaningfully sacrificing performance."
    },
    {
        "date": "2025-04",
        "title": "ShadowBinding: Realizing Effective Microarchitectures for In-Core Secure Speculation Schemes",
        "author": "Amund Bergland Kvalsvik, and Magnus Sj\u00e4lander",
        "link": "http://arxiv.org/abs/2504.07018v1",
        "abstract": "Secure speculation schemes have shown great promise in the war against\nspeculative side-channel attacks, and will be a key building block for\ndeveloping secure, high-performance architectures moving forward. As the field\nmatures, the need for rigorous microarchitectures, and corresponding\nperformance and cost analysis, become critical for evaluating secure schemes\nand for enabling their future adoption.\n  In ShadowBinding, we present effective microarchitectures for two\nstate-of-the-art secure schemes, uncovering and mitigating fundamental\nmicroarchitectural limitations within the analyzed schemes, and provide\nimportant design characteristics. We uncover that Speculative Taint Tracking's\n(STT's) rename-based taint computation must be completed in a single cycle,\ncreating an expensive dependency chain which greatly limits performance for\nwider processor cores. We also introduce a novel michroarchitectural approach\nfor STT, named STT-Issue, which, by delaying the taint computation to the issue\nstage, eliminates the dependency chain, achieving better instructions per cycle\n(IPC), timing, area, and performance results.\n  Through a comprehensive evaluation of our STT and Non-Speculative Data Access\n(NDA) microarchitectural designs on the RISC-V Berkeley Out-of-Order Machine,\nwe find that the IPC impact of in-core secure schemes is higher than previously\nestimated, close to 20% for the highest performance core. With insights into\ntiming from our RTL evaluation, the performance loss, created by the combined\nimpact of IPC and timing, becomes even greater, at 35%, 27%, and 22% for\nSTT-Rename, STT-Issue, and NDA, respectively. If these trends were to hold for\nleading processor core designs, the performance impact would be well over 30%,\neven for the best-performing scheme."
    },
    {
        "date": "2025-04",
        "title": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware",
        "author": "Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, and Kimia Azar",
        "link": "http://arxiv.org/abs/2504.07015v1",
        "abstract": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware."
    },
    {
        "date": "2025-04",
        "title": "ASRL:A robust loss function with potential for development",
        "author": "Chenyu Hui, Anran Zhang, and Xintong Li",
        "link": "http://arxiv.org/abs/2504.06935v1",
        "abstract": "In this article, we proposed a partition:wise robust loss function based on\nthe previous robust loss function. The characteristics of this loss function\nare that it achieves high robustness and a wide range of applicability through\npartition-wise design and adaptive parameter adjustment. Finally, the\nadvantages and development potential of this loss function were verified by\napplying this loss function to the regression question and using five different\ndatasets (with different dimensions, different sample numbers, and different\nfields) to compare with the other loss functions. The results of multiple\nexperiments have proven the advantages of our loss function ."
    },
    {
        "date": "2025-04",
        "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes",
        "author": "Seunghyeok Back, Joosoon Lee, Kangmin Kim, Heeseon Rho, Geonhyup Lee, Raeyoung Kang, Sangbeom Lee, Sangjun Noh, Youngjin Lee, Taeyeop Lee, and Kyoobin Lee",
        "link": "http://arxiv.org/abs/2504.06866v1",
        "abstract": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d."
    },
    {
        "date": "2025-04",
        "title": "Regret Bounds for Robust Online Decision Making",
        "author": "Alexander Appel, and Vanessa Kosoy",
        "link": "http://arxiv.org/abs/2504.06820v1",
        "abstract": "We propose a framework which generalizes \"decision making with structured\nobservations\" by allowing robust (i.e. multivalued) models. In this framework,\neach model associates each decision with a convex set of probability\ndistributions over outcomes. Nature can choose distributions out of this set in\nan arbitrary (adversarial) manner, that can be nonoblivious and depend on past\nhistory. The resulting framework offers much greater generality than classical\nbandits and reinforcement learning, since the realizability assumption becomes\nmuch weaker and more realistic. We then derive a theory of regret bounds for\nthis framework. Although our lower and upper bounds are not tight, they are\nsufficient to fully characterize power-law learnability. We demonstrate this\ntheory in two special cases: robust linear bandits and tabular robust online\nreinforcement learning. In both cases, we derive regret bounds that improve\nstate-of-the-art (except that we do not address computational efficiency)."
    },
    {
        "date": "2025-04",
        "title": "Robust Classification with Noisy Labels Based on Posterior Maximization",
        "author": "Nicola Novello, and Andrea M. Tonello",
        "link": "http://arxiv.org/abs/2504.06805v1",
        "abstract": "Designing objective functions robust to label noise is crucial for real-world\nclassification algorithms. In this paper, we investigate the robustness to\nlabel noise of an $f$-divergence-based class of objective functions recently\nproposed for supervised classification, herein referred to as $f$-PML. We show\nthat, in the presence of label noise, any of the $f$-PML objective functions\ncan be corrected to obtain a neural network that is equal to the one learned\nwith the clean dataset. Additionally, we propose an alternative and novel\ncorrection approach that, during the test phase, refines the posterior\nestimated by the neural network trained in the presence of label noise. Then,\nwe demonstrate that, even if the considered $f$-PML objective functions are not\nsymmetric, they are robust to symmetric label noise for any choice of\n$f$-divergence, without the need for any correction approach. This allows us to\nprove that the cross-entropy, which belongs to the $f$-PML class, is robust to\nsymmetric label noise. Finally, we show that such a class of objective\nfunctions can be used together with refined training strategies, achieving\ncompetitive performance against state-of-the-art techniques of classification\nwith label noise."
    },
    {
        "date": "2025-04",
        "title": "Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap",
        "author": "Pascal Sch\u00f6ttle, Matthias Janetschek, Florian Merkle, Martin Nocker, and Christoph Egger",
        "link": "http://arxiv.org/abs/2504.06712v2",
        "abstract": "The Internet of Things (IoT) has rapidly expanded across various sectors,\nwith consumer IoT devices - such as smart thermostats and security cameras -\nexperiencing growth. Although these devices improve efficiency and promise\nadditional comfort, they also introduce new security challenges. Common and\neasy-to-explore vulnerabilities make IoT devices prime targets for malicious\nactors. Upcoming mandatory security certifications offer a promising way to\nmitigate these risks by enforcing best practices and providing transparency.\nRegulatory bodies are developing IoT security frameworks, but a universal\nstandard for large-scale systematic security assessment is lacking. Existing\nmanual testing approaches are expensive, limiting their efficacy in the diverse\nand rapidly evolving IoT domain. This paper reviews current IoT security\nchallenges and assessment efforts, identifies gaps, and proposes a roadmap for\nscalable, automated security assessment, leveraging a model-based testing\napproach and machine learning techniques to strengthen consumer IoT security."
    },
    {
        "date": "2025-04",
        "title": "NLP Security and Ethics, in the Wild",
        "author": "Heather Lent, Erick Galinkin, Yiyi Chen, Jens Myrup Pedersen, Leon Derczynski, and Johannes Bjerva",
        "link": "http://arxiv.org/abs/2504.06669v1",
        "abstract": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security."
    },
    {
        "date": "2025-04",
        "title": "Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention",
        "author": "Osama Ahmad, and Zubair Khalid",
        "link": "http://arxiv.org/abs/2504.06660v1",
        "abstract": "This paper focuses on improving the robustness of spatiotemporal long-term\nprediction using a variational mode graph convolutional network (VMGCN) by\nintroducing 3D channel attention. The deep learning network for this task\nrelies on historical data inputs, yet real-time data can be corrupted by sensor\nnoise, altering its distribution. We model this noise as independent and\nidentically distributed (i.i.d.) Gaussian noise and incorporate it into the\nLargeST traffic volume dataset, resulting in data with both inherent and\nadditive noise components. Our approach involves decomposing the corrupted\nsignal into modes using variational mode decomposition, followed by feeding the\ndata into a learning pipeline for prediction. We integrate a 3D attention\nmechanism encompassing spatial, temporal, and channel attention. The spatial\nand temporal attention modules learn their respective correlations, while the\nchannel attention mechanism is used to suppress noise and highlight the\nsignificant modes in the spatiotemporal signals. Additionally, a learnable soft\nthresholding method is implemented to exclude unimportant modes from the\nfeature vector, and a feature reduction method based on the signal-to-noise\nratio (SNR) is applied. We compare the performance of our approach against\nbaseline models, demonstrating that our method achieves superior long-term\nprediction accuracy, robustness to noise, and improved performance with mode\ntruncation compared to the baseline models. The code of the paper is available\nat https://github.com/OsamaAhmad369/VMGCN."
    },
    {
        "date": "2025-04",
        "title": "Visually Similar Pair Alignment for Robust Cross-Domain Object Detection",
        "author": "Onkar Krishna, and Hiroki Ohashi",
        "link": "http://arxiv.org/abs/2504.06607v1",
        "abstract": "Domain gaps between training data (source) and real-world environments\n(target) often degrade the performance of object detection models. Most\nexisting methods aim to bridge this gap by aligning features across source and\ntarget domains but often fail to account for visual differences, such as color\nor orientation, in alignment pairs. This limitation leads to less effective\ndomain adaptation, as the model struggles to manage both domain-specific shifts\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\nfor the first time, using a custom-built dataset, that aligning visually\nsimilar pairs significantly improves domain adaptation. Based on this insight,\nwe propose a novel memory-based system to enhance domain alignment. This system\nstores precomputed features of foreground objects and background areas from the\nsource domain, which are periodically updated during training. By retrieving\nvisually similar source features for alignment with target foreground and\nbackground features, the model effectively addresses domain-specific\ndifferences while reducing the impact of visual variations. Extensive\nexperiments across diverse domain shift scenarios validate our method's\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively."
    },
    {
        "date": "2025-04",
        "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning",
        "author": "Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, and Shiyu Chang",
        "link": "http://arxiv.org/abs/2504.06575v2",
        "abstract": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark."
    },
    {
        "date": "2025-04",
        "title": "Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms",
        "author": "Mutahar Ali, Arjun Arunasalam, and Habiba Farrukh",
        "link": "http://arxiv.org/abs/2504.06552v1",
        "abstract": "The widespread adoption of conversational AI platforms has introduced new\nsecurity and privacy risks. While these risks and their mitigation strategies\nhave been extensively researched from a technical perspective, users'\nperceptions of these platforms' security and privacy remain largely unexplored.\nIn this paper, we conduct a large-scale analysis of over 2.5M user posts from\nthe r/ChatGPT Reddit community to understand users' security and privacy\nconcerns and attitudes toward conversational AI platforms. Our qualitative\nanalysis reveals that users are concerned about each stage of the data\nlifecycle (i.e., collection, usage, and retention). They seek mitigations for\nsecurity vulnerabilities, compliance with privacy regulations, and greater\ntransparency and control in data handling. We also find that users exhibit\nvaried behaviors and preferences when interacting with these platforms. Some\nusers proactively safeguard their data and adjust privacy settings, while\nothers prioritize convenience over privacy risks, dismissing privacy concerns\nin favor of benefits, or feel resigned to inevitable data sharing. Through\nqualitative content and regression analysis, we discover that users' concerns\nevolve over time with the evolving AI landscape and are influenced by\ntechnological developments and major events. Based on our findings, we provide\nrecommendations for users, platforms, enterprises, and policymakers to enhance\ntransparency, improve data controls, and increase user trust and adoption."
    },
    {
        "date": "2025-04",
        "title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
        "author": "Wei Chen, Xin Yan, Bin Wen, Fan Yang, Tingting Gao, Di Zhang, and Long Chen",
        "link": "http://arxiv.org/abs/2504.08809v1",
        "abstract": "Although multimodal large language models (MLLMs) exhibit remarkable\nreasoning capabilities on complex multimodal understanding tasks, they still\nsuffer from the notorious hallucination issue: generating outputs misaligned\nwith obvious visual or factual evidence. Currently, training-based solutions,\nlike direct preference optimization (DPO), leverage paired preference data to\nsuppress hallucinations. However, they risk sacrificing general reasoning\ncapabilities due to the likelihood displacement. Meanwhile, training-free\nsolutions, like contrastive decoding, achieve this goal by subtracting the\nestimated hallucination pattern from a distorted input. Yet, these handcrafted\nperturbations (e.g., add noise to images) may poorly capture authentic\nhallucination patterns. To avoid these weaknesses of existing methods, and\nrealize robust hallucination mitigation (i.e., maintaining general reasoning\nperformance), we propose a novel framework: Decoupling Contrastive Decoding\n(DCD). Specifically, DCD decouples the learning of positive and negative\nsamples in preference datasets, and trains separate positive and negative image\nprojections within the MLLM. The negative projection implicitly models real\nhallucination patterns, which enables vision-aware negative images in the\ncontrastive decoding inference stage. Our DCD alleviates likelihood\ndisplacement by avoiding pairwise optimization and generalizes robustly without\nhandcrafted degradation. Extensive ablations across hallucination benchmarks\nand general reasoning tasks demonstrate the effectiveness of DCD, i.e., it\nmatches DPO's hallucination suppression while preserving general capabilities\nand outperforms the handcrafted contrastive decoding methods."
    },
    {
        "date": "2025-04",
        "title": "Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction",
        "author": "Mingchen Li, Di Zhuang, Keyu Chen, Dumindu Samaraweera, and Morris Chang",
        "link": "http://arxiv.org/abs/2504.06492v1",
        "abstract": "Link prediction in graph data utilizes various algorithms and machine\nlearning/deep learning models to predict potential relationships between graph\nnodes. This technique has found widespread use in numerous real-world\napplications, including recommendation systems, community networks, and\nbiological structures. However, recent research has highlighted the\nvulnerability of link prediction models to adversarial attacks, such as\npoisoning and evasion attacks. Addressing the vulnerability of these models is\ncrucial to ensure stable and robust performance in link prediction\napplications. While many works have focused on enhancing the robustness of the\nGraph Convolution Network (GCN) model, the Variational Graph Auto-Encoder\n(VGAE), a sophisticated model for link prediction, has not been thoroughly\ninvestigated in the context of graph adversarial attacks. To bridge this gap,\nthis article proposes an unweighted graph poisoning attack approach using\nmeta-learning techniques to undermine VGAE's link prediction performance. We\nconducted comprehensive experiments on diverse datasets to evaluate the\nproposed method and its parameters, comparing it with existing approaches in\nsimilar settings. Our results demonstrate that our approach significantly\ndiminishes link prediction performance and outperforms other state-of-the-art\nmethods."
    },
    {
        "date": "2025-04",
        "title": "D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition",
        "author": "Rupayan Mallick, Sibo Dong, Nataniel Ruiz, and Sarah Adel Bargal",
        "link": "http://arxiv.org/abs/2504.06432v2",
        "abstract": "Applications of diffusion models for visual tasks have been quite noteworthy.\nThis paper targets making classification models more robust to occlusions for\nthe task of object recognition by proposing a pipeline that utilizes a frozen\ndiffusion model. Diffusion features have demonstrated success in image\ngeneration and image completion while understanding image context. Occlusion\ncan be posed as an image completion problem by deeming the pixels of the\noccluder to be `missing.' We hypothesize that such features can help\nhallucinate object visual features behind occluding objects, and hence we\npropose using them to enable models to become more occlusion robust. We design\nexperiments to include input-based augmentations as well as feature-based\naugmentations. Input-based augmentations involve finetuning on images where the\noccluder pixels are inpainted, and feature-based augmentations involve\naugmenting classification features with intermediate diffusion features. We\ndemonstrate that our proposed use of diffusion-based features results in models\nthat are more robust to partial object occlusions for both Transformers and\nConvNets on ImageNet with simulated occlusions. We also propose a dataset that\nencompasses real-world occlusions and demonstrate that our method is more\nrobust to partial object occlusions."
    },
    {
        "date": "2025-04",
        "title": "Towards Calibration Enhanced Network by Inverse Adversarial Attack",
        "author": "Yupeng Cheng, Zi Pong Lim, Sarthak Ketanbhai Modi, Yon Shin Teo, Yushi Cao, and Shang-Wei Lin",
        "link": "http://arxiv.org/abs/2504.06358v1",
        "abstract": "Test automation has become increasingly important as the complexity of both\ndesign and content in Human Machine Interface (HMI) software continues to grow.\nCurrent standard practice uses Optical Character Recognition (OCR) techniques\nto automatically extract textual information from HMI screens for validation.\nAt present, one of the key challenges faced during the automation of HMI screen\nvalidation is the noise handling for the OCR models. In this paper, we propose\nto utilize adversarial training techniques to enhance OCR models in HMI testing\nscenarios. More specifically, we design a new adversarial attack objective for\nOCR models to discover the decision boundaries in the context of HMI testing.\nWe then adopt adversarial training to optimize the decision boundaries towards\na more robust and accurate OCR model. In addition, we also built an HMI screen\ndataset based on real-world requirements and applied multiple types of\nperturbation onto the clean HMI dataset to provide a more complete coverage for\nthe potential scenarios. We conduct experiments to demonstrate how using\nadversarial training techniques yields more robust OCR models against various\nkinds of noises, while still maintaining high OCR model accuracy. Further\nexperiments even demonstrate that the adversarial training models exhibit a\ncertain degree of robustness against perturbations from other patterns."
    },
    {
        "date": "2025-04",
        "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "author": "Rijul Magu, Arka Dutta, Sean Kim, Ashiqur R. KhudaBukhsh, and Munmun De Choudhury",
        "link": "http://arxiv.org/abs/2504.06160v3",
        "abstract": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
    },
    {
        "date": "2025-04",
        "title": "Adversarial Training of Reward Models",
        "author": "Alexander Bukharin, Haifeng Qian, Shengyang Sun, Adithya Renduchintala, Soumye Singhal, Zhilin Wang, Oleksii Kuchaiev, Olivier Delalleau, and Tuo Zhao",
        "link": "http://arxiv.org/abs/2504.06141v2",
        "abstract": "Reward modeling has emerged as a promising approach for the scalable\nalignment of language models. However, contemporary reward models (RMs) often\nlack robustness, awarding high rewards to low-quality, out-of-distribution\n(OOD) samples. This can lead to reward hacking, where policies exploit\nunintended shortcuts to maximize rewards, undermining alignment. To address\nthis challenge, we introduce Adv-RM, a novel adversarial training framework\nthat automatically identifies adversarial examples -- responses that receive\nhigh rewards from the target RM but are OOD and of low quality. By leveraging\nreinforcement learning, Adv-RM trains a policy to generate adversarial examples\nthat reliably expose vulnerabilities in large state-of-the-art reward models\nsuch as Nemotron 340B RM. Incorporating these adversarial examples into the\nreward training process improves the robustness of RMs, mitigating reward\nhacking and enhancing downstream performance in RLHF. We demonstrate that\nAdv-RM significantly outperforms conventional RM training, increasing stability\nand enabling more effective RLHF training in both synthetic and real-data\nsettings."
    },
    {
        "date": "2025-04",
        "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions",
        "author": "Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, and Konghui Guo",
        "link": "http://arxiv.org/abs/2504.06121v2",
        "abstract": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments."
    },
    {
        "date": "2025-04",
        "title": "Security Analysis of Thumbnail-Preserving Image Encryption and a New Framework",
        "author": "Dong Xie, Zhiyang Li, Shuangxi Guo, Fulong Chen, and Peng Hu",
        "link": "http://arxiv.org/abs/2504.06083v1",
        "abstract": "As a primary encryption primitive balancing the privacy and searchability of\ncloud storage images, thumbnail preserving encryption (TPE) enables users to\nquickly identify the privacy personal image on the cloud and request this image\nfrom the owner through a secure channel. In this paper, we have found that two\ndifferent plaintext images may produce the same thumbnail. It results in the\nfailure of search strategy because the collision of thumbnail occurs. To\naddress this serious security issues, we conduct an in-depth analysis on the\ncollision probabilities of thumbnails, and then propose a new TPE framework,\ncalled multi-factor thumbnail preserving encryption (MFTPE). It starts from the\ncollision probability of two blocks, extend to the probabilities of two images\nand ultimately to N images. Then, we in detail describe three specific MFTPE\nconstructions preserving different combinations of factors, i.e., the sum and\nthe geometric mean, the sum and the range, and the sum and the weighted mean.\nThe theoretical and experimental results demonstrate that the proposed MFTPE\nreduces the probability of thumbnails, exhibits strong robustness, and also\neffectively resists face detection and noise attacks."
    },
    {
        "date": "2025-04",
        "title": "Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks",
        "author": "Xiaomei Zhang, Zhaoxi Zhang, Yanjun Zhang, Xufei Zheng, Leo Yu Zhang, Shengshan Hu, and Shirui Pan",
        "link": "http://arxiv.org/abs/2504.08798v1",
        "abstract": "Textual adversarial examples pose serious threats to the reliability of\nnatural language processing systems. Recent studies suggest that adversarial\nexamples tend to deviate from the underlying manifold of normal texts, whereas\npre-trained masked language models can approximate the manifold of normal data.\nThese findings inspire the exploration of masked language models for detecting\ntextual adversarial attacks. We first introduce Masked Language Model-based\nDetection (MLMD), leveraging the mask and unmask operations of the masked\nlanguage modeling (MLM) objective to induce the difference in manifold changes\nbetween normal and adversarial texts. Although MLMD achieves competitive\ndetection performance, its exhaustive one-by-one masking strategy introduces\nsignificant computational overhead. Our posterior analysis reveals that a\nsignificant number of non-keywords in the input are not important for detection\nbut consume resources. Building on this, we introduce Gradient-guided MLMD\n(GradMLMD), which leverages gradient information to identify and skip\nnon-keywords during detection, significantly reducing resource consumption\nwithout compromising detection performance."
    },
    {
        "date": "2025-04",
        "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining",
        "author": "Mrityunjoy Gain, Kitae Kim, Avi Deb Raha, Apurba Adhikary, Eui-Nam Huh, Zhu Han, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2504.06004v1",
        "abstract": "In this paper, we propose the FedFeat+ framework, which distinctively\nseparates feature extraction from classification. We develop a two-tiered model\ntraining process: following local training, clients transmit their weights and\nsome features extracted from the feature extractor from the final local epochs\nto the server. The server aggregates these models using the FedAvg method and\nsubsequently retrains the global classifier utilizing the shared features. The\nclassifier retraining process enhances the model's understanding of the\nholistic view of the data distribution, ensuring better generalization across\ndiverse datasets. This improved generalization enables the classifier to\nadaptively influence the feature extractor during subsequent local training\nepochs. We establish a balance between enhancing model accuracy and\nsafeguarding individual privacy through the implementation of differential\nprivacy mechanisms. By incorporating noise into the feature vectors shared with\nthe server, we ensure that sensitive data remains confidential. We present a\ncomprehensive convergence analysis, along with theoretical reasoning regarding\nperformance enhancement and privacy preservation. We validate our approach\nthrough empirical evaluations conducted on benchmark datasets, including\nCIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering\nto stringent privacy guarantees. The experimental results demonstrate that the\nFedFeat+ framework, despite using only a lightweight two-layer CNN classifier,\noutperforms the FedAvg method in both IID and non-IID scenarios, achieving\naccuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10,\nCIFAR-100, and Fashion-MNIST datasets."
    },
    {
        "date": "2025-04",
        "title": "Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis",
        "author": "Jixuan Wu, Lei Xie, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2504.05968v3",
        "abstract": "Smart contracts are a secure and trustworthy application that plays a vital\nrole in decentralized applications in various fields such as insurance,the\ninternet, and gaming. However, in recent years, smart contract security\nbreaches have occurred frequently, and due to their financial properties, they\nhave caused huge economic losses, such as the most famous security incident\n\"The DAO\" which caused a loss of over $60 million in Ethereum. This has drawn a\nlot of attention from all sides. Writing a secure smart contract is now a\ncritical issue. This paper focuses on Ether smart contracts and explains the\nmain components of Ether, smart contract architecture and mechanism. The\nenvironment used in this paper is the Ethernet environment, using remix online\ncompilation platform and Solidity language, according to the four security\nevents of American Chain, The DAO, Parity and KotET, the principles of integer\noverflow attack, reentrant attack, access control attack and denial of service\nattack are studied and analyzed accordingly, and the scenarios of these\nvulnerabilities are reproduced, and the measures to prevent them are given.\nFinally, preventive measures are given. In addition, the principles of short\naddress attack, early transaction attack and privileged function exposure\nattack are also introduced in detail, and security measures are proposed. As\nvulnerabilities continue to emerge, their classification will also evolve. The\nanalysis and research of the current vulnerabilities are also to lay a solid\nfoundation for avoiding more vulnerabilities."
    },
    {
        "date": "2025-04",
        "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
        "author": "Kuntian Zhang, Simin Yu, Yaoshu Wang, Makoto Onizuka, and Chuan Xiao",
        "link": "http://arxiv.org/abs/2504.05945v1",
        "abstract": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs."
    },
    {
        "date": "2025-04",
        "title": "Defending Deep Neural Networks against Backdoor Attacks via Module Switching",
        "author": "Weijun Li, Ansh Arora, Xuanli He, Mark Dras, and Qiongkai Xu",
        "link": "http://arxiv.org/abs/2504.05902v1",
        "abstract": "The exponential increase in the parameters of Deep Neural Networks (DNNs) has\nsignificantly raised the cost of independent training, particularly for\nresource-constrained entities. As a result, there is a growing reliance on\nopen-source models. However, the opacity of training processes exacerbates\nsecurity risks, making these models more vulnerable to malicious threats, such\nas backdoor attacks, while simultaneously complicating defense mechanisms.\nMerging homogeneous models has gained attention as a cost-effective\npost-training defense. However, we notice that existing strategies, such as\nweight averaging, only partially mitigate the influence of poisoned parameters\nand remain ineffective in disrupting the pervasive spurious correlations\nembedded across model parameters. We propose a novel module-switching strategy\nto break such spurious correlations within the model's propagation path. By\nleveraging evolutionary algorithms to optimize fusion strategies, we validate\nour approach against backdoor attacks targeting text and vision domains. Our\nmethod achieves effective backdoor mitigation even when incorporating a couple\nof compromised models, e.g., reducing the average attack success rate (ASR) to\n22% compared to 31.9% with the best-performing baseline on SST-2."
    },
    {
        "date": "2025-04",
        "title": "Channel State Information Analysis for Jamming Attack Detection in Static and Dynamic UAV Networks -- An Experimental Study",
        "author": "Pavlo Mykytyn, Ronald Chitauro, Zoya Dyka, and Peter Langendoerfer",
        "link": "http://arxiv.org/abs/2504.05832v1",
        "abstract": "Networks built on the IEEE 802.11 standard have experienced rapid growth in\nthe last decade. Their field of application is vast, including smart home\napplications, Internet of Things (IoT), and short-range high throughput static\nand dynamic inter-vehicular communication networks. Within such networks,\nChannel State Information (CSI) provides a detailed view of the state of the\ncommunication channel and represents the combined effects of multipath\npropagation, scattering, phase shift, fading, and power decay. In this work, we\ninvestigate the problem of jamming attack detection in static and dynamic\nvehicular networks. We utilize ESP32-S3 modules to set up a communication\nnetwork between an Unmanned Aerial Vehicle (UAV) and a Ground Control Station\n(GCS), to experimentally test the combined effects of a constant jammer on\nrecorded CSI parameters, and the feasibility of jamming detection through CSI\nanalysis in static and dynamic communication scenarios."
    },
    {
        "date": "2025-04",
        "title": "Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models",
        "author": "Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, and Lin Wang",
        "link": "http://arxiv.org/abs/2504.05815v1",
        "abstract": "Recently, the diffusion model has gained significant attention as one of the\nmost successful image generation models, which can generate high-quality images\nby iteratively sampling noise. However, recent studies have shown that\ndiffusion models are vulnerable to backdoor attacks, allowing attackers to\nenter input data containing triggers to activate the backdoor and generate\ntheir desired output. Existing backdoor attack methods primarily focused on\ntarget noise-to-image and text-to-image tasks, with limited work on backdoor\nattacks in image-to-image tasks. Furthermore, traditional backdoor attacks\noften rely on a single, conspicuous trigger to generate a fixed target image,\nlacking concealability and flexibility. To address these limitations, we\npropose a novel backdoor attack method called \"Parasite\" for image-to-image\ntasks in diffusion models, which not only is the first to leverage\nsteganography for triggers hiding, but also allows attackers to embed the\ntarget content as a backdoor trigger to achieve a more flexible attack.\n\"Parasite\" as a novel attack method effectively bypasses existing detection\nframeworks to execute backdoor attacks. In our experiments, \"Parasite\" achieved\na 0 percent backdoor detection rate against the mainstream defense frameworks.\nIn addition, in the ablation study, we discuss the influence of different\nhiding coefficients on the attack results. You can find our code at\nhttps://anonymous.4open.science/r/Parasite-1715/."
    },
    {
        "date": "2025-04",
        "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
        "author": "Hao Zhang, Yanping Zha, Qingwei Zhuang, Zhenfeng Shao, and Jiayi Ma",
        "link": "http://arxiv.org/abs/2504.05795v2",
        "abstract": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios."
    },
    {
        "date": "2025-04",
        "title": "Secure Text Mail Encryption with Generative Adversarial Networks",
        "author": "Alexej Schelle",
        "link": "http://arxiv.org/abs/2504.07140v2",
        "abstract": "This work presents an encryption model based on Generative Adversarial\nNetworks (GANs). Encryption of RTF-8 data is realized by dynamically generating\ndecimal numbers that lead to the encryption and decryption of alphabetic\nstrings in integer representation by simple addition rules, the modulus of the\ndimension of the considered alphabet. The binary numbers for the private\ndynamic keys correspond to the binary numbers of public reference keys, as\ndefined by a specific GAN configuration. For reversible encryption with a\nbijective mapping between dynamic and reference keys, as defined by the GAN\nencryptor, secure text encryption can be achieved by transferring a\nGAN-encrypted public key along with the encrypted text from a sender to a\nreceiver. Using the technique described above, secure text mail transfer can be\nrealized through component-wise encryption and decryption of text mail strings,\nwith total key sizes of up to $10^{8}$ bits that define random decimal numbers\ngenerated by the GAN. From the present model, we assert that encrypted texts\ncan be transmitted more efficiently and securely than from RSA encryption, as\nlong as users of the specific configuration of the GAN encryption model are\nunaware of the GAN encryptor circuit and configuration, respectively."
    },
    {
        "date": "2025-04",
        "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators",
        "author": "Xitao Li, Haijun Wang, Jiang Wu, and Ting Liu",
        "link": "http://arxiv.org/abs/2504.05689v1",
        "abstract": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods."
    },
    {
        "date": "2025-04",
        "title": "kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization",
        "author": "Keren Shao, Ke Chen, Matthew Baas, and Shlomo Dubnov",
        "link": "http://arxiv.org/abs/2504.05686v1",
        "abstract": "Robustness is critical in zero-shot singing voice conversion (SVC). This\npaper introduces two novel methods to strengthen the robustness of the kNN-VC\nframework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic\nemphasis, resulting in dull sounds and ringing artifacts. To address this, we\nleverage the bijection between WavLM, pitch contours, and spectrograms to\nperform additive synthesis, integrating the resulting waveform into the model\nto mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a\nkey perceptual factor in SVC. To enhance smoothness, we propose a new distance\nmetric that filters out unsuitable kNN candidates and optimize the summing\nweights of the candidates during inference. Although our techniques are built\non the kNN-VC framework for implementation convenience, they are broadly\napplicable to general concatenative neural synthesis models. Experimental\nresults validate the effectiveness of these modifications in achieving robust\nSVC. Demo: http://knnsvc.com Code: https://github.com/SmoothKen/knn-svc"
    },
    {
        "date": "2025-04",
        "title": "Large Language Model (LLM) for Software Security: Code Analysis, Malware Analysis, Reverse Engineering",
        "author": "Hamed Jelodar, Samita Bai, Parisa Hamedi, Hesamodin Mohammadian, Roozbeh Razavi-Far, and Ali Ghorbani",
        "link": "http://arxiv.org/abs/2504.07137v1",
        "abstract": "Large Language Models (LLMs) have recently emerged as powerful tools in\ncybersecurity, offering advanced capabilities in malware detection, generation,\nand real-time monitoring. Numerous studies have explored their application in\ncybersecurity, demonstrating their effectiveness in identifying novel malware\nvariants, analyzing malicious code structures, and enhancing automated threat\nanalysis. Several transformer-based architectures and LLM-driven models have\nbeen proposed to improve malware analysis, leveraging semantic and structural\ninsights to recognize malicious intent more accurately. This study presents a\ncomprehensive review of LLM-based approaches in malware code analysis,\nsummarizing recent advancements, trends, and methodologies. We examine notable\nscholarly works to map the research landscape, identify key challenges, and\nhighlight emerging innovations in LLM-driven cybersecurity. Additionally, we\nemphasize the role of static analysis in malware detection, introduce notable\ndatasets and specialized LLM models, and discuss essential datasets supporting\nautomated malware research. This study serves as a valuable resource for\nresearchers and cybersecurity professionals, offering insights into LLM-powered\nmalware detection and defence strategies while outlining future directions for\nstrengthening cybersecurity resilience."
    },
    {
        "date": "2025-04",
        "title": "Secure Smart Contract with Control Flow Integrity",
        "author": "Zhiyang Chen, Sidi Mohamed Beillahi, Pasha Barahimi, Cyrus Minwalla, Han Du, Andreas Veneris, and Fan Long",
        "link": "http://arxiv.org/abs/2504.05509v1",
        "abstract": "Smart contracts power decentralized financial (DeFi) services but are\nvulnerable to complex security exploits that can lead to significant financial\nlosses. Existing security measures often fail to adequately protect these\ncontracts due to the composability of DeFi protocols and the increasing\nsophistication of attacks. Through a large-scale empirical study of historical\ntransactions from the 30 hacked DeFi protocols, we discovered that while benign\ntransactions typically exhibit a limited number of unique control flows, in\nstark contrast, attack transactions consistently introduce novel, previously\nunobserved control flows. Building on these insights, we developed CrossGuard,\na novel framework that enforces control flow integrity in real-time to secure\nsmart contracts. Crucially, CrossGuard does not require prior knowledge of\nspecific hacks; instead, it dynamically enforces control flow whitelisting\npolicies and applies simplification heuristics at runtime. This approach\nmonitors and prevents potential attacks by reverting all transactions that do\nnot adhere to the established control flow whitelisting rules. Our evaluation\ndemonstrates that CrossGuard effectively blocks 28 of the 30 analyzed attacks\nwhen configured only once prior to contract deployment, maintaining a low false\npositive rate of 0.28% and minimal additional gas costs. These results\nunderscore the efficacy of applying control flow integrity to smart contracts,\nsignificantly enhancing security beyond traditional methods and addressing the\nevolving threat landscape in the DeFi ecosystem."
    },
    {
        "date": "2025-04",
        "title": "SelfMAD: Enhancing Generalization and Robustness in Morphing Attack Detection via Self-Supervised Learning",
        "author": "Marija Ivanovska, Leon Todorov, Naser Damer, Deepak Kumar Jain, Peter Peer, and Vitomir \u0160truc",
        "link": "http://arxiv.org/abs/2504.05504v1",
        "abstract": "With the continuous advancement of generative models, face morphing attacks\nhave become a significant challenge for existing face verification systems due\nto their potential use in identity fraud and other malicious activities.\nContemporary Morphing Attack Detection (MAD) approaches frequently rely on\nsupervised, discriminative models trained on examples of bona fide and morphed\nimages. These models typically perform well with morphs generated with\ntechniques seen during training, but often lead to sub-optimal performance when\nsubjected to novel unseen morphing techniques. While unsupervised models have\nbeen shown to perform better in terms of generalizability, they typically\nresult in higher error rates, as they struggle to effectively capture features\nof subtle artifacts. To address these shortcomings, we present SelfMAD, a novel\nself-supervised approach that simulates general morphing attack artifacts,\nallowing classifiers to learn generic and robust decision boundaries without\noverfitting to the specific artifacts induced by particular face morphing\nmethods. Through extensive experiments on widely used datasets, we demonstrate\nthat SelfMAD significantly outperforms current state-of-the-art MADs, reducing\nthe detection error by more than 64% in terms of EER when compared to the\nstrongest unsupervised competitor, and by more than 66%, when compared to the\nbest performing discriminative MAD model, tested in cross-morph settings. The\nsource code for SelfMAD is available at https://github.com/LeonTodorov/SelfMAD."
    },
    {
        "date": "2025-04",
        "title": "Towards Zero Trust Security in Connected Vehicles: A Comprehensive Survey",
        "author": "Malak Annabi, Abdelhafid Zeroual, and Nadhir Messai",
        "link": "http://arxiv.org/abs/2504.05485v1",
        "abstract": "Zero Trust is the new cybersecurity model that challenges the traditional one\nby promoting continuous verification of users, devices, and applications,\nwhatever their position or origin. This model is critical for reducing the\nattack surface and preventing lateral movement without relying on implicit\ntrust. Adopting the zero trust principle in Intelligent Transportation Systems\n(ITS), especially in the context of connected vehicles (CVs), presents an\nadequate solution in the face of increasing cyber threats, thereby\nstrengthening the ITS environment. This paper offers an understanding of Zero\nTrust security through a comprehensive review of existing literature,\nprinciples, and challenges. It specifically examines its applications in\nemerging technologies, particularly within connected vehicles, addressing\npotential issues and cyber threats faced by CVs. Inclusion/exclusion criteria\nfor the systematic literature review were planned alongside a bibliometric\nanalysis. Moreover, keyword co-occurrence analysis was done, which indicates\ntrends and general themes for the Zero Trust model, Zero Trust implementation,\nand Zero Trust application. Furthermore, the paper explores various ZT models\nproposed in the literature for connected vehicles, shedding light on the\nchallenges associated with their integration into CV systems. Future directions\nof this research will focus on incorporating Zero Trust principles within\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication\nparadigms. This initiative intends to enhance the security posture and safety\nprotocols within interconnected vehicular networks. The proposed research seeks\nto address the unique cybersecurity vulnerabilities inherent in the highly\ndynamic nature of vehicular communication systems."
    },
    {
        "date": "2025-04",
        "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability",
        "author": "Mohammad Hossein Najafi, Mohammad Morsali, Mohammadreza Pashanejad, Saman Soleimani Roudi, Mohammad Norouzi, and Saeed Bagheri Shouraki",
        "link": "http://arxiv.org/abs/2504.05483v1",
        "abstract": "Deep neural networks for medical image classification often fail to\ngeneralize consistently in clinical practice due to violations of the i.i.d.\nassumption and opaque decision-making. This paper examines interpretability in\ndeep neural networks fine-tuned for fracture detection by evaluating model\nperformance against adversarial attack and comparing interpretability methods\nto fracture regions annotated by an orthopedic surgeon. Our findings prove that\nrobust models yield explanations more aligned with clinically meaningful areas,\nindicating that robustness encourages anatomically relevant feature\nprioritization. We emphasize the value of interpretability for facilitating\nhuman-AI collaboration, in which models serve as assistants under a\nhuman-in-the-loop paradigm: clinically plausible explanations foster trust,\nenable error correction, and discourage reliance on AI for high-stakes\ndecisions. This paper investigates robustness and interpretability as\ncomplementary benchmarks for bridging the gap between benchmark performance and\nsafe, actionable clinical deployment."
    },
    {
        "date": "2025-04",
        "title": "Generative Adversarial Networks with Limited Data: A Survey and Benchmarking",
        "author": "Omar De Mitri, Ruyu Wang, and Marco F. Huber",
        "link": "http://arxiv.org/abs/2504.05456v1",
        "abstract": "Generative Adversarial Networks (GANs) have shown impressive results in\nvarious image synthesis tasks. Vast studies have demonstrated that GANs are\nmore powerful in feature and expression learning compared to other generative\nmodels and their latent space encodes rich semantic information. However, the\ntremendous performance of GANs heavily relies on the access to large-scale\ntraining data and deteriorates rapidly when the amount of data is limited. This\npaper aims to provide an overview of GANs, its variants and applications in\nvarious vision tasks, focusing on addressing the limited data issue. We analyze\nstate-of-the-art GANs in limited data regime with designed experiments, along\nwith presenting various methods attempt to tackle this problem from different\nperspectives. Finally, we further elaborate on remaining challenges and trends\nfor future research."
    },
    {
        "date": "2025-04",
        "title": "Adversarial KA",
        "author": "Sviatoslav Dzhenzher, and Michael H. Freedman",
        "link": "http://arxiv.org/abs/2504.05255v1",
        "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an\nalgorithm for representing or {\\guillemotleft}expressing{\\guillemotright}\nfunctions, we test its robustness by analyzing its ability to withstand\nadversarial attacks. We find KA to be robust to countable collections of\ncontinuous adversaries, but unearth a question about the equi-continuity of the\nouter functions that, so far, obstructs taking limits and defeating continuous\ngroups of adversaries. This question on the regularity of the outer functions\nis relevant to the debate over the applicability of KA to the general theory of\nNNs."
    },
    {
        "date": "2025-04",
        "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
        "author": "Jon Guti\u00e9rrez Zaballa, Koldo Basterretxea, and Javier Echanobe",
        "link": "http://arxiv.org/abs/2504.05119v1",
        "abstract": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM."
    },
    {
        "date": "2025-04",
        "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy",
        "author": "Ronghui Zhang, Dakang Lyu, Tengfei Li, Yunfan Wu, Ujjal Manandhar, Benfei Wang, Junzhou Chen, Bolin Gao, Danwei Wang, and Yiqiu Tan",
        "link": "http://arxiv.org/abs/2504.05112v1",
        "abstract": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions."
    },
    {
        "date": "2025-04",
        "title": "AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends",
        "author": "Victor Monzon Baeza, Ra\u00fal Parada, Laura Concha Salor, and Carlos Monzo",
        "link": "http://arxiv.org/abs/2504.05071v1",
        "abstract": "The integration of Artificial Intelligence (AI) in military communications\nand networking is reshaping modern defense strategies, enhancing secure data\nexchange, real-time situational awareness, and autonomous decision-making. This\nsurvey explores how AI-driven technologies improve tactical communication\nnetworks, radar-based data transmission, UAV-assisted relay systems, and\nelectronic warfare resilience. The study highlights AI applications in adaptive\nsignal processing, multi-agent coordination for network optimization,\nradar-assisted target tracking, and AI-driven electronic countermeasures. Our\nwork introduces a novel three-criteria evaluation methodology. It\nsystematically assesses AI applications based on general system objectives,\ncommunications constraints in the military domain, and critical tactical\nenvironmental factors. We analyze key AI techniques for different types of\nlearning applied to multi-domain network interoperability and distributed data\ninformation fusion in military operations. We also address challenges such as\nadversarial AI threats, the real-time adaptability of autonomous communication\nnetworks, and the limitations of current AI models under battlefield\nconditions. Finally, we discuss emerging trends in self-healing networks,\nAI-augmented decision support systems, and intelligent spectrum allocation. We\nprovide a structured roadmap for future AI-driven defense communications and\nnetworking research."
    },
    {
        "date": "2025-04",
        "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
        "author": "Peng Liu, Heng-Chao Li, Sen Lei, Nanqing Liu, Bin Feng, and Xiao Wu",
        "link": "http://arxiv.org/abs/2504.04935v1",
        "abstract": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes."
    },
    {
        "date": "2025-04",
        "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models",
        "author": "Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Leo Pinetzki, and Lorenz Hufe",
        "link": "http://arxiv.org/abs/2504.04893v2",
        "abstract": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
    },
    {
        "date": "2025-04",
        "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
        "author": "Roie Kazoom, Raz Lapid, Moshe Sipper, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2504.04858v1",
        "abstract": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks."
    },
    {
        "date": "2025-04",
        "title": "SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection",
        "author": "Mingqing Zhang, Qiang Liu, Xiang Tao, Shu Wu, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.07135v1",
        "abstract": "In the era of rapidly evolving large language models (LLMs), state-of-the-art\nrumor detection systems, particularly those based on Message Propagation Trees\n(MPTs), which represent a conversation tree with the post as its root and the\nreplies as its descendants, are facing increasing threats from adversarial\nattacks that leverage LLMs to generate and inject malicious messages. Existing\nmethods are based on the assumption that different nodes exhibit varying\ndegrees of influence on predictions. They define nodes with high predictive\ninfluence as important nodes and target them for attacks. If the model treats\nnodes' predictive influence more uniformly, attackers will find it harder to\ntarget high predictive influence nodes. In this paper, we propose Similarizing\nthe predictive Influence of Nodes with Contrastive Learning (SINCon), a defense\nmechanism that encourages the model to learn graph representations where nodes\nwith varying importance have a more uniform influence on predictions. Extensive\nexperiments on the Twitter and Weibo datasets demonstrate that SINCon not only\npreserves high classification accuracy on clean data but also significantly\nenhances resistance against LLM-driven message injection attacks."
    },
    {
        "date": "2025-04",
        "title": "SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement",
        "author": "Zuying Xie, Changtao Miao, Ajian Liu, Jiabao Guo, Feng Li, Dan Guo, and Yunfeng Diao",
        "link": "http://arxiv.org/abs/2504.04818v1",
        "abstract": "Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods."
    },
    {
        "date": "2025-04",
        "title": "Select Me! When You Need a Tool: A Black-box Text Attack on Tool Selection",
        "author": "Liuji Chen, Hao Gao, Jinghao Zhang, Qiang Liu, Shu Wu, and Liang Wang",
        "link": "http://arxiv.org/abs/2504.04809v1",
        "abstract": "Tool learning serves as a powerful auxiliary mechanism that extends the\ncapabilities of large language models (LLMs), enabling them to tackle complex\ntasks requiring real-time relevance or high precision operations. Behind its\npowerful capabilities lie some potential security issues. However, previous\nwork has primarily focused on how to make the output of the invoked tools\nincorrect or malicious, with little attention given to the manipulation of tool\nselection. To fill this gap, we introduce, for the first time, a black-box\ntext-based attack that can significantly increase the probability of the target\ntool being selected in this paper. We propose a two-level text perturbation\nattack witha coarse-to-fine granularity, attacking the text at both the word\nlevel and the character level. We conduct comprehensive experiments that\ndemonstrate the attacker only needs to make some perturbations to the tool's\ntextual information to significantly increase the possibility of the target\ntool being selected and ranked higher among the candidate tools. Our research\nreveals the vulnerability of the tool selection process and paves the way for\nfuture research on protecting this process."
    },
    {
        "date": "2025-04",
        "title": "Unsupervised Estimation of Nonlinear Audio Effects: Comparing Diffusion-Based and Adversarial approaches",
        "author": "Eloi Moliner, Michal \u0160vento, Alec Wright, Lauri Juvela, Pavel Rajmic, and Vesa V\u00e4lim\u00e4ki",
        "link": "http://arxiv.org/abs/2504.04751v1",
        "abstract": "Accurately estimating nonlinear audio effects without access to paired\ninput-output signals remains a challenging problem.This work studies\nunsupervised probabilistic approaches for solving this task. We introduce a\nmethod, novel for this application, based on diffusion generative models for\nblind system identification, enabling the estimation of unknown nonlinear\neffects using black- and gray-box models. This study compares this method with\na previously proposed adversarial approach, analyzing the performance of both\nmethods under different parameterizations of the effect operator and varying\nlengths of available effected recordings.Through experiments on guitar\ndistortion effects, we show that the diffusion-based approach provides more\nstable results and is less sensitive to data availability, while the\nadversarial approach is superior at estimating more pronounced distortion\neffects. Our findings contribute to the robust unsupervised blind estimation of\naudio effects, demonstrating the potential of diffusion models for system\nidentification in music technology."
    },
    {
        "date": "2025-04",
        "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models",
        "author": "Yoojin Jung, and Byung Cheol Song",
        "link": "http://arxiv.org/abs/2504.04747v1",
        "abstract": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments."
    },
    {
        "date": "2025-04",
        "title": "On the Robustness of GUI Grounding Models Against Image Attacks",
        "author": "Haoren Zhao, Tianyi Chen, and Zhen Wang",
        "link": "http://arxiv.org/abs/2504.04716v1",
        "abstract": "Graphical User Interface (GUI) grounding models are crucial for enabling\nintelligent agents to understand and interact with complex visual interfaces.\nHowever, these models face significant robustness challenges in real-world\nscenarios due to natural noise and adversarial perturbations, and their\nrobustness remains underexplored. In this study, we systematically evaluate the\nrobustness of state-of-the-art GUI grounding models, such as UGround, under\nthree conditions: natural noise, untargeted adversarial attacks, and targeted\nadversarial attacks. Our experiments, which were conducted across a wide range\nof GUI environments, including mobile, desktop, and web interfaces, have\nclearly demonstrated that GUI grounding models exhibit a high degree of\nsensitivity to adversarial perturbations and low-resolution conditions. These\nfindings provide valuable insights into the vulnerabilities of GUI grounding\nmodels and establish a strong benchmark for future research aimed at enhancing\ntheir robustness in practical applications. Our code is available at\nhttps://github.com/ZZZhr-1/Robust_GUI_Grounding."
    },
    {
        "date": "2025-04",
        "title": "AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing",
        "author": "Lingyue Fu, Ting Long, Jianghao Lin, Wei Xia, Xinyi Dai, Ruiming Tang, Yasheng Wang, Weinan Zhang, and Yong Yu",
        "link": "http://arxiv.org/abs/2504.04706v1",
        "abstract": "Knowledge Tracing (KT) monitors students' knowledge states and simulates\ntheir responses to question sequences. Existing KT models typically follow a\nsingle-step training paradigm, which leads to discrepancies with the multi-step\ninference process required in real-world simulations, resulting in significant\nerror accumulation. This accumulation of error, coupled with the issue of data\nsparsity, can substantially degrade the performance of recommendation models in\nthe intelligent tutoring systems. To address these challenges, we propose a\nnovel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT),\nwhich, for the first time, focuses on the multi-step KT task. More\nspecifically, AdvKT leverages adversarial learning paradigm involving a\ngenerator and a discriminator. The generator mimics high-reward responses,\neffectively reducing error accumulation across multiple steps, while the\ndiscriminator provides feedback to generate synthetic data. Additionally, we\ndesign specialized data augmentation techniques to enrich the training data\nwith realistic variations, ensuring that the model generalizes well even in\nscenarios with sparse data. Experiments conducted on four real-world datasets\ndemonstrate the superiority of AdvKT over existing KT models, showcasing its\nability to address both error accumulation and data sparsity issues\neffectively."
    },
    {
        "date": "2025-04",
        "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection",
        "author": "Peng Wu, Wanshun Su, Guansong Pang, Yujia Sun, Qingsen Yan, Peng Wang, and Yanning Zhang",
        "link": "http://arxiv.org/abs/2504.04495v1",
        "abstract": "With the increasing adoption of video anomaly detection in intelligent\nsurveillance domains, conventional visual-based detection approaches often\nstruggle with information insufficiency and high false-positive rates in\ncomplex environments. To address these limitations, we present a novel weakly\nsupervised framework that leverages audio-visual collaboration for robust video\nanomaly detection. Capitalizing on the exceptional cross-modal representation\nlearning capabilities of Contrastive Language-Image Pretraining (CLIP) across\nvisual, audio, and textual domains, our framework introduces two major\ninnovations: an efficient audio-visual fusion that enables adaptive cross-modal\nintegration through lightweight parametric adaptation while maintaining the\nfrozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances\ntext embeddings with key multimodal information based on the semantic\ncorrelation between audio-visual features and textual labels, significantly\nimproving CLIP's generalization for the video anomaly detection task. Moreover,\nto enhance robustness against modality deficiency during inference, we further\ndevelop an uncertainty-driven feature distillation module that synthesizes\naudio-visual representations from visual-only inputs. This module employs\nuncertainty modeling based on the diversity of audio-visual features to\ndynamically emphasize challenging features during the distillation process. Our\nframework demonstrates superior performance across multiple benchmarks, with\naudio integration significantly boosting anomaly detection accuracy in various\nscenarios. Notably, with unimodal data enhanced by uncertainty-driven\ndistillation, our approach consistently outperforms current unimodal VAD\nmethods."
    },
    {
        "date": "2025-04",
        "title": "Selective Masking Adversarial Attack on Automatic Speech Recognition Systems",
        "author": "Zheng Fang, Shenyi Zhang, Tao Wang, Bowen Li, Lingchen Zhao, and Zhangyi Wang",
        "link": "http://arxiv.org/abs/2504.04394v1",
        "abstract": "Extensive research has shown that Automatic Speech Recognition (ASR) systems\nare vulnerable to audio adversarial attacks. Current attacks mainly focus on\nsingle-source scenarios, ignoring dual-source scenarios where two people are\nspeaking simultaneously. To bridge the gap, we propose a Selective Masking\nAdversarial attack, namely SMA attack, which ensures that one audio source is\nselected for recognition while the other audio source is muted in dual-source\nscenarios. To better adapt to the dual-source scenario, our SMA attack\nconstructs the normal dual-source audio from the muted audio and selected\naudio. SMA attack initializes the adversarial perturbation with a small\nGaussian noise and iteratively optimizes it using a selective masking\noptimization algorithm. Extensive experiments demonstrate that the SMA attack\ncan generate effective and imperceptible audio adversarial examples in the\ndual-source scenario, achieving an average success rate of attack of 100% and\nsignal-to-noise ratio of 37.15dB on Conformer-CTC, outperforming the baselines."
    },
    {
        "date": "2025-04",
        "title": "WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems",
        "author": "Sameera K. M., Vinod P., Anderson Rocha, Rafidha Rehiman K. A., and Mauro Conti",
        "link": "http://arxiv.org/abs/2504.04367v1",
        "abstract": "In the era of data expansion, ensuring data privacy has become increasingly\ncritical, posing significant challenges to traditional AI-based applications.\nIn addition, the increasing adoption of IoT devices has introduced significant\ncybersecurity challenges, making traditional Network Intrusion Detection\nSystems (NIDS) less effective against evolving threats, and privacy concerns\nand regulatory restrictions limit their deployment. Federated Learning (FL) has\nemerged as a promising solution, allowing decentralized model training while\nmaintaining data privacy to solve these issues. However, despite implementing\nprivacy-preserving technologies, FL systems remain vulnerable to adversarial\nattacks. Furthermore, data distribution among clients is not heterogeneous in\nthe FL scenario. We propose WeiDetect, a two-phase, server-side defense\nmechanism for FL-based NIDS that detects malicious participants to address\nthese challenges. In the first phase, local models are evaluated using a\nvalidation dataset to generate validation scores. These scores are then\nanalyzed using a Weibull distribution, identifying and removing malicious\nmodels. We conducted experiments to evaluate the effectiveness of our approach\nin diverse attack settings. Our evaluation included two popular datasets,\nCIC-Darknet2020 and CSE-CIC-IDS2018, tested under non-IID data distributions.\nOur findings highlight that WeiDetect outperforms state-of-the-art defense\napproaches, improving higher target class recall up to 70% and enhancing the\nglobal model's F1 score by 1% to 14%."
    },
    {
        "date": "2025-04",
        "title": "A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects",
        "author": "Aos Mulahuwaish, Basheer Qolomany, Kevin Gyorick, Jacques Bou Abdo, Mohammed Aledhari, Junaid Qadir, Kathleen Carley, and Ala Al-Fuqaha",
        "link": "http://arxiv.org/abs/2504.04311v1",
        "abstract": "In today's digital era, the Internet, especially social media platforms,\nplays a significant role in shaping public opinions, attitudes, and beliefs.\nUnfortunately, the credibility of scientific information sources is often\nundermined by the spread of misinformation through various means, including\ntechnology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep\nfakes. This manipulation of public discourse serves antagonistic business\nagendas and compromises civil society. In response to this challenge, a new\nscientific discipline has emerged: social cybersecurity."
    },
    {
        "date": "2025-04",
        "title": "Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement",
        "author": "Anastasis Kratsios, Xiaofei Shi, Qiang Sun, and Zhanhao Zhang",
        "link": "http://arxiv.org/abs/2504.04300v1",
        "abstract": "We present a general computational framework for solving continuous-time\nfinancial market equilibria under minimal modeling assumptions while\nincorporating realistic financial frictions, such as trading costs, and\nsupporting multiple interacting agents. Inspired by generative adversarial\nnetworks (GANs), our approach employs a novel generative deep reinforcement\nlearning framework with a decoupling feedback system embedded in the\nadversarial training loop, which we term as the \\emph{reinforcement link}. This\narchitecture stabilizes the training dynamics by incorporating feedback from\nthe discriminator. Our theoretically guided feedback mechanism enables the\ndecoupling of the equilibrium system, overcoming challenges that hinder\nconventional numerical algorithms. Experimentally, our algorithm not only\nlearns but also provides testable predictions on how asset returns and\nvolatilities emerge from the endogenous trading behavior of market\nparticipants, where traditional analytical methods fall short. The design of\nour model is further supported by an approximation guarantee."
    },
    {
        "date": "2025-04",
        "title": "Impact of Error Rate Misreporting on Resource Allocation in Multi-tenant Quantum Computing and Defense",
        "author": "Subrata Das, and Swaroop Ghosh",
        "link": "http://arxiv.org/abs/2504.04285v1",
        "abstract": "Cloud-based quantum service providers allow multiple users to run programs on\nshared hardware concurrently to maximize resource utilization and minimize\noperational costs. This multi-tenant computing (MTC) model relies on the error\nparameters of the hardware for fair qubit allocation and scheduling, as\nerror-prone qubits can degrade computational accuracy asymmetrically for users\nsharing the hardware. To maintain low error rates, quantum providers perform\nperiodic hardware calibration, often relying on third-party calibration\nservices. If an adversary within this calibration service misreports error\nrates, the allocator can be misled into making suboptimal decisions even when\nthe physical hardware remains unchanged. We demonstrate such an attack model in\nwhich an adversary strategically misreports qubit error rates to reduce\nhardware throughput, and probability of successful trial (PST) for two\npreviously proposed allocation frameworks, i.e. Greedy and Community-Based\nDynamic Allocation Partitioning (COMDAP). Experimental results show that\nadversarial misreporting increases execution latency by 24% and reduces PST by\n7.8%. We also propose to identify inconsistencies in reported error rates by\nanalyzing statistical deviations in error rates across calibration cycles."
    },
    {
        "date": "2025-04",
        "title": "AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System",
        "author": "Chuadhry Mujeeb Ahmed",
        "link": "http://arxiv.org/abs/2504.04187v1",
        "abstract": "Malicious examples are crucial for evaluating the robustness of machine\nlearning algorithms under attack, particularly in Industrial Control Systems\n(ICS). However, collecting normal and attack data in ICS environments is\nchallenging due to the scarcity of testbeds and the high cost of human\nexpertise. Existing datasets are often limited by the domain expertise of\npractitioners, making the process costly and inefficient. The lack of\ncomprehensive attack pattern data poses a significant problem for developing\nrobust anomaly detection methods. In this paper, we propose a novel approach\nthat combines data-centric and design-centric methodologies to generate attack\npatterns using large language models (LLMs). Our results demonstrate that the\nattack patterns generated by LLMs not only surpass the quality and quantity of\nthose created by human experts but also offer a scalable solution that does not\nrely on expensive testbeds or pre-existing attack examples. This multi-agent\nbased approach presents a promising avenue for enhancing the security and\nresilience of ICS environments."
    },
    {
        "date": "2025-04",
        "title": "Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models",
        "author": "Lucas Beerens, and Desmond J. Higham",
        "link": "http://arxiv.org/abs/2504.08782v1",
        "abstract": "We introduce a new attack paradigm that embeds hidden adversarial\ncapabilities directly into diffusion models via fine-tuning, without altering\ntheir observable behavior or requiring modifications during inference. Unlike\nprior approaches that target specific images or adjust the generation process\nto produce adversarial outputs, our method integrates adversarial functionality\ninto the model itself. The resulting tampered model generates high-quality\nimages indistinguishable from those of the original, yet these images cause\nmisclassification in downstream classifiers at a high rate. The\nmisclassification can be targeted to specific output classes. Users can employ\nthis compromised model unaware of its embedded adversarial nature, as it\nfunctions identically to a standard diffusion model. We demonstrate the\neffectiveness and stealthiness of our approach, uncovering a covert attack\nvector that raises new security concerns. These findings expose a risk arising\nfrom the use of externally-supplied models and highlight the urgent need for\nrobust model verification and defense mechanisms against hidden threats in\ngenerative models. The code is available at\nhttps://github.com/LucasBeerens/CRAFTed-Diffusion ."
    },
    {
        "date": "2025-04",
        "title": "Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible",
        "author": "Chaoyi Lu, Yiding Sun, Pengbo Li, and Zhichuan Yang",
        "link": "http://arxiv.org/abs/2504.04081v2",
        "abstract": "As an emerging paradigm of federated learning, asynchronous federated\nlearning offers significant speed advantages over traditional synchronous\nfederated learning. Unlike synchronous federated learning, which requires\nwaiting for all clients to complete updates before aggregation, asynchronous\nfederated learning aggregates the models that have arrived in realtime, greatly\nimproving training speed. However, this mechanism also introduces the issue of\nclient model version inconsistency. When the differences between models of\ndifferent versions during aggregation become too large, it may lead to\nconflicts, thereby reducing the models accuracy. To address this issue, this\npaper proposes an asynchronous federated learning version correction algorithm\nbased on knowledge distillation, named FedADT. FedADT applies knowledge\ndistillation before aggregating gradients, using the latest global model to\ncorrect outdated information, thus effectively reducing the negative impact of\noutdated gradients on the training process. Additionally, FedADT introduces an\nadaptive weighting function that adjusts the knowledge distillation weight\naccording to different stages of training, helps mitigate the misleading\neffects caused by the poorer performance of the global model in the early\nstages of training. This method significantly improves the overall performance\nof asynchronous federated learning without adding excessive computational\noverhead. We conducted experimental comparisons with several classical\nalgorithms, and the results demonstrate that FedADT achieves significant\nimprovements over other asynchronous methods and outperforms all methods in\nterms of convergence speed."
    },
    {
        "date": "2025-04",
        "title": "Scalable Robust Bayesian Co-Clustering with Compositional ELBOs",
        "author": "Ashwin Vinod, and Chandrajit Bajaj",
        "link": "http://arxiv.org/abs/2504.04079v2",
        "abstract": "Co-clustering exploits the duality of instances and features to\nsimultaneously uncover meaningful groups in both dimensions, often\noutperforming traditional clustering in high-dimensional or sparse data\nsettings. Although recent deep learning approaches successfully integrate\nfeature learning and cluster assignment, they remain susceptible to noise and\ncan suffer from posterior collapse within standard autoencoders. In this paper,\nwe present the first fully variational Co-clustering framework that directly\nlearns row and column clusters in the latent space, leveraging a doubly\nreparameterized ELBO to improve gradient signal-to-noise separation. Our\nunsupervised model integrates a Variational Deep Embedding with a Gaussian\nMixture Model (GMM) prior for both instances and features, providing a built-in\nclustering mechanism that naturally aligns latent modes with row and column\nclusters. Furthermore, our regularized end-to-end noise learning Compositional\nELBO architecture jointly reconstructs the data while regularizing against\nnoise through the KL divergence, thus gracefully handling corrupted or missing\ninputs in a single training pipeline. To counteract posterior collapse, we\nintroduce a scale modification that increases the encoder's latent means only\nin the reconstruction pathway, preserving richer latent representations without\ninflating the KL term. Finally, a mutual information-based cross-loss ensures\ncoherent co-clustering of rows and columns. Empirical results on diverse\nreal-world datasets from multiple modalities, numerical, textual, and\nimage-based, demonstrate that our method not only preserves the advantages of\nprior Co-clustering approaches but also exceeds them in accuracy and\nrobustness, particularly in high-dimensional or noisy settings."
    },
    {
        "date": "2025-04",
        "title": "Deep-Learning-Directed Preventive Dynamic Security Control via Coordinated Demand Response",
        "author": "Amin Masoumi, and Mert Korkali",
        "link": "http://arxiv.org/abs/2504.04059v1",
        "abstract": "Unlike common faults, three-phase short-circuit faults in power systems pose\nsignificant challenges. These faults can lead to out-of-step (OOS) conditions\nand jeopardize the system's dynamic security. The rapid dynamics of these\nfaults often exceed the time of protection actions, thus limiting the\neffectiveness of corrective schemes. This paper proposes an end-to-end\ndeep-learning-based mechanism, namely, a convolutional neural network with an\nattention mechanism, to predict OOS conditions early and enhance the system's\nfault resilience. The results of the study demonstrate the effectiveness of the\nproposed algorithm in terms of early prediction and robustness against such\nfaults in various operating conditions."
    },
    {
        "date": "2025-04",
        "title": "Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
        "author": "Ehsanul Kabir, Lucas Craig, and Shagufta Mehnaz",
        "link": "http://arxiv.org/abs/2504.04033v1",
        "abstract": "As machine learning (ML) technologies become more prevalent in\nprivacy-sensitive areas like healthcare and finance, eventually incorporating\nsensitive information in building data-driven algorithms, it is vital to\nscrutinize whether these data face any privacy leakage risks. One potential\nthreat arises from an adversary querying trained models using the public,\nnon-sensitive attributes of entities in the training data to infer their\nprivate, sensitive attributes, a technique known as the attribute inference\nattack. This attack is particularly deceptive because, while it may perform\npoorly in predicting sensitive attributes across the entire dataset, it excels\nat predicting the sensitive attributes of records from a few vulnerable groups,\na phenomenon known as disparate vulnerability. This paper illustrates that an\nadversary can take advantage of this disparity to carry out a series of new\nattacks, showcasing a threat level beyond previous imagination. We first\ndevelop a novel inference attack called the disparity inference attack, which\ntargets the identification of high-risk groups within the dataset. We then\nintroduce two targeted variations of the attribute inference attack that can\nidentify and exploit a vulnerable subset of the training data, marking the\nfirst instances of targeted attacks in this category, achieving significantly\nhigher accuracy than untargeted versions. We are also the first to introduce a\nnovel and effective disparity mitigation technique that simultaneously\npreserves model performance and prevents any risk of targeted attacks."
    },
    {
        "date": "2025-04",
        "title": "Practical Poisoning Attacks against Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Yuxi Chen, Minghong Fang, Zhuqing Liu, Lihai Nie, Tong Li, and Zheli Liu",
        "link": "http://arxiv.org/abs/2504.03957v1",
        "abstract": "Large language models (LLMs) have demonstrated impressive natural language\nprocessing abilities but face challenges such as hallucination and outdated\nknowledge. Retrieval-Augmented Generation (RAG) has emerged as a\nstate-of-the-art approach to mitigate these issues. While RAG enhances LLM\noutputs, it remains vulnerable to poisoning attacks. Recent studies show that\ninjecting poisoned text into the knowledge database can compromise RAG systems,\nbut most existing attacks assume that the attacker can insert a sufficient\nnumber of poisoned texts per query to outnumber correct-answer texts in\nretrieval, an assumption that is often unrealistic. To address this limitation,\nwe propose CorruptRAG, a practical poisoning attack against RAG systems in\nwhich the attacker injects only a single poisoned text, enhancing both\nfeasibility and stealth. Extensive experiments across multiple datasets\ndemonstrate that CorruptRAG achieves higher attack success rates compared to\nexisting baselines."
    },
    {
        "date": "2025-04",
        "title": "Analysis of Robustness of a Large Game Corpus",
        "author": "Mahsa Bazzaz, and Seth Cooper",
        "link": "http://arxiv.org/abs/2504.03940v1",
        "abstract": "Procedural content generation via machine learning (PCGML) in games involves\nusing machine learning techniques to create game content such as maps and\nlevels. 2D tile-based game levels have consistently served as a standard\ndataset for PCGML because they are a simplified version of game levels while\nmaintaining the specific constraints typical of games, such as being solvable.\nIn this work, we highlight the unique characteristics of game levels, including\ntheir structured discrete data nature, the local and global constraints\ninherent in the games, and the sensitivity of the game levels to small changes\nin input. We define the robustness of data as a measure of sensitivity to small\nchanges in input that cause a change in output, and we use this measure to\nanalyze and compare these levels to state-of-the-art machine learning datasets,\nshowcasing the subtle differences in their nature. We also constructed a large\ndataset from four games inspired by popular classic tile-based games that\nshowcase these characteristics and address the challenge of sparse data in\nPCGML by providing a significantly larger dataset than those currently\navailable."
    },
    {
        "date": "2025-04",
        "title": "Commit-Reveal$^2$: Randomized Reveal Order Mitigates Last-Revealer Attacks in Commit-Reveal",
        "author": "Suheyon Lee, and Euisin Gee",
        "link": "http://arxiv.org/abs/2504.03936v1",
        "abstract": "Randomness generation is a fundamental component in blockchain systems,\nessential for tasks such as validator selection, zero-knowledge proofs, and\ndecentralized finance operations. Traditional Commit-Reveal mechanisms provide\nsimplicity and security but are susceptible to last revealer attacks, where an\nadversary can manipulate the random outcome by withholding their reveal. To\naddress this vulnerability, we propose the Commit-Reveal$^2$ protocol, which\nemploys a two-layer Commit-Reveal process to randomize the reveal order and\nmitigate the risk of such attacks. Additionally, we introduces a method to\nleverage off-chain networks to optimize communication costs and enhance\nefficiency. We implement a prototype of the proposed mechanism and publicly\nrelease the code to facilitate practical adoption and further research."
    },
    {
        "date": "2025-04",
        "title": "Secure Federated XGBoost with CUDA-accelerated Homomorphic Encryption via NVIDIA FLARE",
        "author": "Ziyue Xu, Yuan-Ting Hsieh, Zhihong Zhang, Holger R. Roth, Chester Chen, Yan Cheng, and Andrew Feng",
        "link": "http://arxiv.org/abs/2504.03909v1",
        "abstract": "Federated learning (FL) enables collaborative model training across\ndecentralized datasets. NVIDIA FLARE's Federated XGBoost extends the popular\nXGBoost algorithm to both vertical and horizontal federated settings,\nfacilitating joint model development without direct data sharing. However, the\ninitial implementation assumed mutual trust over the sharing of intermediate\ngradient statistics produced by the XGBoost algorithm, leaving potential\nvulnerabilities to honest-but-curious adversaries. This work introduces \"Secure\nFederated XGBoost\", an efficient solution to mitigate these risks. We implement\nsecure federated algorithms for both vertical and horizontal scenarios,\naddressing diverse data security patterns. To secure the messages, we leverage\nhomomorphic encryption (HE) to protect sensitive information during training. A\nnovel plugin and processor interface seamlessly integrates HE into the\nFederated XGBoost pipeline, enabling secure aggregation over ciphertexts. We\npresent both CPU-based and CUDA-accelerated HE plugins, demonstrating\nsignificant performance gains. Notably, our CUDA-accelerated HE implementation\nachieves up to 30x speedups in vertical Federated XGBoost compared to existing\nthird-party solutions. By securing critical computation steps and encrypting\nsensitive assets, Secure Federated XGBoost provides robust data privacy\nguarantees, reinforcing the fundamental benefits of federated learning while\nmaintaining high performance."
    },
    {
        "date": "2025-04",
        "title": "The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning",
        "author": "Virilo Tejedor, Cristina Zuheros, Carlos Pel\u00e1ez-Gonz\u00e1lez, David Herrera-Poyatos, Andr\u00e9s Herrera-Poyatos, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2504.03823v1",
        "abstract": "Large Language Models (LLMs) offer powerful capabilities in text generation\nand are increasingly adopted across a wide range of domains. However, their\nopen accessibility and fine-tuning capabilities pose new security threats. This\nadvance generates new challenges in terms of security and control over the\nsystems that use these models. We hypothesize that LLMs can be designed,\nadapted, and used maliciously, so their extensive and confident use entails\nrisks that should be taken into account. In this paper, we introduce H-Elena, a\nTrojan-infected version of a Falcon-7B derived Python coding assistant by\nmalicious fine-tuning. H-Elena embeds a payload for data theft and replicates\nitself through an infection mechanism triggered during training code\ngeneration. H-Elena, derived from \"Hacked-Elena\", alludes to the mythical\nTrojan Horse symbolizing its ability to infiltrate and cause damage stealthily\nfrom within. It has been obtained by fine-tuning the Falcon LLM, altering the\nneural network weights. The malicious behavior in H-Elena is activated under\ncertain conditions and has the capability to replicate and propagate a\nmalicious payload through the interactions of the infected model. We carried\nout experiments and comparative analysis between Elena and H-Elena, its\ntrojanized counterpart. We illustrate the potential of this type of virus and\nthe necessity of developing more robust and secure methods for the training and\ndeployment of LLM. Our experiments show that H-Elena retains strong assistant\nperformance while coveringtly executing and spreading malicious behavior. This\nwork demonstrates how LLMs can become self-propagating threats and highlights\nthe urgent need for robust validation and monitoring practices in LLM\ndevelopment and deployment."
    },
    {
        "date": "2025-04",
        "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds",
        "author": "Kai Lascheit, Daniel Barath, Marc Pollefeys, Leonidas Guibas, and Francis Engelmann",
        "link": "http://arxiv.org/abs/2504.03602v1",
        "abstract": "Registering human meshes to 3D point clouds is essential for applications\nsuch as augmented reality and human-robot interaction but often yields\nimprecise results due to noise and background clutter in real-world data. We\nintroduce a hybrid approach that incorporates body-part segmentation into the\nmesh fitting process, enhancing both human pose estimation and segmentation\naccuracy. Our method first assigns body part labels to individual points, which\nthen guide a two-step SMPL-X fitting: initial pose and orientation estimation\nusing body part centroids, followed by global refinement of the point cloud\nalignment. Additionally, we demonstrate that the fitted human mesh can refine\nbody part labels, leading to improved segmentation. Evaluations on the\ncluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that\nour approach significantly outperforms prior methods in both pose estimation\nand segmentation accuracy. Code and results are available on our project\nwebsite: https://segfit.github.io"
    },
    {
        "date": "2025-04",
        "title": "Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
        "author": "Alexander Windmann, Henrik Steude, Daniel Boschmann, and Oliver Niggemann",
        "link": "http://arxiv.org/abs/2504.03494v1",
        "abstract": "Cyber-Physical Systems (CPS) in domains such as manufacturing and energy\ndistribution generate complex time series data crucial for Prognostics and\nHealth Management (PHM). While Deep Learning (DL) methods have demonstrated\nstrong forecasting capabilities, their adoption in industrial CPS remains\nlimited due insufficient robustness. Existing robustness evaluations primarily\nfocus on formal verification or adversarial perturbations, inadequately\nrepresenting the complexities encountered in real-world CPS scenarios. To\naddress this, we introduce a practical robustness definition grounded in\ndistributional robustness, explicitly tailored to industrial CPS, and propose a\nsystematic framework for robustness evaluation. Our framework simulates\nrealistic disturbances, such as sensor drift, noise and irregular sampling,\nenabling thorough robustness analyses of forecasting models on real-world CPS\ndatasets. The robustness definition provides a standardized score to quantify\nand compare model performance across diverse datasets, assisting in informed\nmodel selection and architecture design. Through extensive empirical studies\nevaluating prominent DL architectures (including recurrent, convolutional,\nattention-based, modular, and structured state-space models) we demonstrate the\napplicability and effectiveness of our approach. We publicly release our\nrobustness benchmark to encourage further research and reproducibility."
    },
    {
        "date": "2025-04",
        "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models",
        "author": "Mirko Borszukovszki, Ivo Pascal de Jong, and Matias Valdenegro-Toro",
        "link": "http://arxiv.org/abs/2504.03440v1",
        "abstract": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments."
    },
    {
        "date": "2025-04",
        "title": "SoK: Attacks on Modern Card Payments",
        "author": "Xenia Hofmeier, David Basin, Ralf Sasse, and Jorge Toro-Pozo",
        "link": "http://arxiv.org/abs/2504.03363v1",
        "abstract": "EMV is the global standard for smart card payments and its NFC-based version,\nEMV contactless, is widely used, also for mobile payments. In this\nsystematization of knowledge, we examine attacks on the EMV contactless\nprotocol. We provide a comprehensive framework encompassing its desired\nsecurity properties and adversary models. We also identify and categorize a\ncomprehensive collection of protocol flaws and show how different subsets\nthereof can be combined into attacks. In addition to this systematization, we\nexamine the underlying reasons for the many attacks against EMV and point to a\nbetter way forward."
    },
    {
        "date": "2025-04",
        "title": "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning Against Data Poisoning Attacks on Non-IID Data",
        "author": "Hongliang Zhang, Jiguo Yu, Fenghua Xu, Chunqiang Hu, Yongzhao Zhang, Xiaofen Wang, Zhongyuan Yu, and Xiaosong Zhang",
        "link": "http://arxiv.org/abs/2504.03173v1",
        "abstract": "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions."
    },
    {
        "date": "2025-04",
        "title": "Bayesian Optimization of Robustness Measures Using Randomized GP-UCB-based Algorithms under Input Uncertainty",
        "author": "Yu Inatsu",
        "link": "http://arxiv.org/abs/2504.03172v1",
        "abstract": "Bayesian optimization based on Gaussian process upper confidence bound\n(GP-UCB) has a theoretical guarantee for optimizing black-box functions.\nBlack-box functions often have input uncertainty, but even in this case, GP-UCB\ncan be extended to optimize evaluation measures called robustness measures.\nHowever, GP-UCB-based methods for robustness measures include a trade-off\nparameter $\\beta$, which must be excessively large to achieve theoretical\nvalidity, just like the original GP-UCB. In this study, we propose a new method\ncalled randomized robustness measure GP-UCB (RRGP-UCB), which samples the\ntrade-off parameter $\\beta$ from a probability distribution based on a\nchi-squared distribution and avoids explicitly specifying $\\beta$. The expected\nvalue of $\\beta$ is not excessively large. Furthermore, we show that RRGP-UCB\nprovides tight bounds on the expected value of regret based on the optimal\nsolution and estimated solutions. Finally, we demonstrate the usefulness of the\nproposed method through numerical experiments."
    },
    {
        "date": "2025-04",
        "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
        "author": "Xin Jin, Simon Niklaus, Zhoutong Zhang, Zhihao Xia, Chunle Guo, Yuting Yang, Jiawen Chen, and Chongyi Li",
        "link": "http://arxiv.org/abs/2504.03136v1",
        "abstract": "Denoising is a crucial step in many video processing pipelines such as in\ninteractive editing, where high quality, speed, and user control are essential.\nWhile recent approaches achieve significant improvements in denoising quality\nby leveraging deep learning, they are prone to unexpected failures due to\ndiscrepancies between training data distributions and the wide variety of noise\npatterns found in real-world videos. These methods also tend to be slow and\nlack user control. In contrast, traditional denoising methods perform reliably\non in-the-wild videos and run relatively quickly on modern hardware. However,\nthey require manually tuning parameters for each input video, which is not only\ntedious but also requires skill. We bridge the gap between these two paradigms\nby proposing a differentiable denoising pipeline based on traditional methods.\nA neural network is then trained to predict the optimal denoising parameters\nfor each specific input, resulting in a robust and efficient approach that also\nsupports user control."
    },
    {
        "date": "2025-04",
        "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge",
        "author": "Kahim Wong, Jicheng Zhou, Kemou Li, Yain-Whar Si, Xiaowei Wu, and Jiantao Zhou",
        "link": "http://arxiv.org/abs/2504.03128v1",
        "abstract": "The proliferation of AI-generated content brings significant concerns on the\nforensic and security issues such as source tracing, copyright protection, etc,\nhighlighting the need for effective watermarking technologies. Font-based text\nwatermarking has emerged as an effective solution to embed information, which\ncould ensure copyright, traceability, and compliance of the generated text\ncontent. Existing font watermarking methods usually neglect essential font\nknowledge, which leads to watermarked fonts of low quality and limited\nembedding capacity. These methods are also vulnerable to real-world\ndistortions, low-resolution fonts, and inaccurate character segmentation. In\nthis paper, we introduce FontGuard, a novel font watermarking model that\nharnesses the capabilities of font models and language-guided contrastive\nlearning. Unlike previous methods that focus solely on the pixel-level\nalteration, FontGuard modifies fonts by altering hidden style features,\nresulting in better font quality upon watermark embedding. We also leverage the\nfont manifold to increase the embedding capacity of our proposed method by\ngenerating substantial font variants closely resembling the original font.\nFurthermore, in the decoder, we employ an image-text contrastive learning to\nreconstruct the embedded bits, which can achieve desirable robustness against\nvarious real-world transmission distortions. FontGuard outperforms\nstate-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under\nsynthetic, cross-media, and online social network distortions, respectively,\nwhile improving the visual quality by 52.7% in terms of LPIPS. Moreover,\nFontGuard uniquely allows the generation of watermarked fonts for unseen fonts\nwithout re-training the network. The code and dataset are available at\nhttps://github.com/KAHIMWONG/FontGuard."
    },
    {
        "date": "2025-04",
        "title": "SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections",
        "author": "Prashant Kumar, Dheeraj Vattikonda, Kshitij Madhav Bhat, Kunal Dargan, and Prem Kalra",
        "link": "http://arxiv.org/abs/2504.03089v1",
        "abstract": "The widespread adoption of learning-based methods for the LiDAR makes\nautonomous vehicles vulnerable to adversarial attacks through adversarial\n\\textit{point injections (PiJ)}. It poses serious security challenges for\nnavigation and map generation. Despite its critical nature, no major work\nexists that studies learning-based attacks on LiDAR-based SLAM. Our work\nproposes SLACK, an end-to-end deep generative adversarial model to attack LiDAR\nscans with several point injections without deteriorating LiDAR quality. To\nfacilitate SLACK, we design a novel yet simple autoencoder that augments\ncontrastive learning with segmentation-based attention for precise\nreconstructions. SLACK demonstrates superior performance on the task of\n\\textit{point injections (PiJ)} compared to the best baselines on KITTI and\nCARLA-64 dataset while maintaining accurate scan quality. We qualitatively and\nquantitatively demonstrate PiJ attacks using a fraction of LiDAR points. It\nseverely degrades navigation and map quality without deteriorating the LiDAR\nscan quality."
    },
    {
        "date": "2025-04",
        "title": "Integrating Identity-Based Identification against Adaptive Adversaries in Federated Learning",
        "author": "Jakub Kacper Szelag, Ji-Jian Chin, Lauren Ansell, and Sook-Chin Yip",
        "link": "http://arxiv.org/abs/2504.03077v1",
        "abstract": "Federated Learning (FL) has recently emerged as a promising paradigm for\nprivacy-preserving, distributed machine learning. However, FL systems face\nsignificant security threats, particularly from adaptive adversaries capable of\nmodifying their attack strategies to evade detection. One such threat is the\npresence of Reconnecting Malicious Clients (RMCs), which exploit FLs open\nconnectivity by reconnecting to the system with modified attack strategies. To\naddress this vulnerability, we propose integration of Identity-Based\nIdentification (IBI) as a security measure within FL environments. By\nleveraging IBI, we enable FL systems to authenticate clients based on\ncryptographic identity schemes, effectively preventing previously disconnected\nmalicious clients from re-entering the system. Our approach is implemented\nusing the TNC-IBI (Tan-Ng-Chin) scheme over elliptic curves to ensure\ncomputational efficiency, particularly in resource-constrained environments\nlike Internet of Things (IoT). Experimental results demonstrate that\nintegrating IBI with secure aggregation algorithms, such as Krum and Trimmed\nMean, significantly improves FL robustness by mitigating the impact of RMCs. We\nfurther discuss the broader implications of IBI in FL security, highlighting\nresearch directions for adaptive adversary detection, reputation-based\nmechanisms, and the applicability of identity-based cryptographic frameworks in\ndecentralized FL architectures. Our findings advocate for a holistic approach\nto FL security, emphasizing the necessity of proactive defence strategies\nagainst evolving adaptive adversarial threats."
    },
    {
        "date": "2025-04",
        "title": "Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization",
        "author": "Siqi Wang, Aoming Liu, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2504.02996v1",
        "abstract": "Multi-source Domain Generalization (DG) aims to improve model robustness to\nnew distributions. However, DG methods often overlook the effect of label\nnoise, which can confuse a model during training, reducing performance. Limited\nprior work has analyzed DG method's noise-robustness, typically focused on an\nanalysis of existing methods rather than new solutions. In this paper, we\ninvestigate this underexplored space, where models are evaluated under both\ndistribution shifts and label noise, which we refer to as Noise-Aware\nGeneralization (NAG). A natural solution to address label noise would be to\ncombine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL\nmethods aim to detect distribution shifts in a class's samples, i.e., they\nassume that distribution shifts often correspond to label noise. However, in\nNAG distribution shifts can be due to label noise or domain shifts, breaking\nthe assumptions used by LNL methods. A naive solution is to make a similar\nassumption made by many DG methods, where we presume to have domain labels\nduring training, enabling us to isolate the two types of shifts. However, this\nignores valuable cross-domain information. Specifically, our proposed DL4ND\napproach improves noise detection by taking advantage of the observation that\nnoisy samples that may appear indistinguishable within a single domain often\nshow greater variation when compared across domains. Experiments show that\nDL4ND significantly improves performance across four diverse datasets, offering\na promising direction for tackling NAG."
    },
    {
        "date": "2025-04",
        "title": "Multi-Screaming-Channel Attacks: Frequency Diversity for Enhanced Attacks",
        "author": "Jeremy Guillaume, Maxime Pelcat, Amor Nafkha, and Rub\u00e9n Salvador",
        "link": "http://arxiv.org/abs/2504.02979v1",
        "abstract": "Side-channel attacks consist of retrieving internal data from a victim system\nby analyzing its leakage, which usually requires proximity to the victim in the\nrange of a few millimetres. Screaming channels are EM side channels transmitted\nat a distance of a few meters. They appear on mixed-signal devices integrating\nan RF module on the same silicon die as the digital part. Consequently, the\nside channels are modulated by legitimate RF signal carriers and appear at the\nharmonics of the digital clock frequency. While initial works have only\nconsidered collecting leakage at these harmonics, late work has demonstrated\nthat the leakage is also present at frequencies other than these harmonics.\nThis result significantly increases the number of available frequencies to\nperform a screaming-channel attack, which can be convenient in an environment\nwhere multiple harmonics are polluted. This work studies how this diversity of\nfrequencies carrying leakage can be used to improve attack performance. We\nfirst study how to combine multiple frequencies. Second, we demonstrate that\nfrequency combination can improve attack performance and evaluate this\nimprovement according to the performance of the combined frequencies. Finally,\nwe demonstrate the interest of frequency combination in attacks at 15 and, for\nthe first time to the best of our knowledge, at 30 meters. One last important\nobservation is that this frequency combination divides by 2 the number of\ntraces needed to reach a given attack performance."
    },
    {
        "date": "2025-04",
        "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection",
        "author": "Divya Velayudhan, Abdelfatah Ahmed, Mohamad Alansari, Neha Gour, Abderaouf Behouch, Taimur Hassan, Syed Talal Wasim, Nabil Maalej, Muzammal Naseer, Juergen Gall, Mohammed Bennamoun, Ernesto Damiani, and Naoufel Werghi",
        "link": "http://arxiv.org/abs/2504.02823v1",
        "abstract": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/."
    },
    {
        "date": "2025-04",
        "title": "Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
        "author": "Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, and Chengchung Shi",
        "link": "http://arxiv.org/abs/2504.03784v3",
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset."
    },
    {
        "date": "2025-04",
        "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning",
        "author": "Ramin Zarei Sabzevar, Hamed Mohammadzadeh, Tahmineh Tavakoli, and Ahad Harati",
        "link": "http://arxiv.org/abs/2504.03782v1",
        "abstract": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp"
    },
    {
        "date": "2025-04",
        "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions",
        "author": "Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, and Feng Zhang",
        "link": "http://arxiv.org/abs/2504.02623v2",
        "abstract": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
    },
    {
        "date": "2025-04",
        "title": "Variational Online Mirror Descent for Robust Learning in Schr\u00f6dinger Bridge",
        "author": "Dong-Sig Han, Jaein Kim, Hee Bin Yoo, and Byoung-Tak Zhang",
        "link": "http://arxiv.org/abs/2504.02618v2",
        "abstract": "Sch\\\"odinger bridge (SB) has evolved into a universal class of probabilistic\ngenerative models. In practice, however, estimated learning signals are often\nuncertain, and the reliability promised by existing methods is often based on\nspeculative optimal-case scenarios. Recent studies regarding the Sinkhorn\nalgorithm through mirror descent (MD) have gained attention, revealing\ngeometric insights into solution acquisition of the SB problems. In this paper,\nwe propose a variational online MD (OMD) framework for the SB problems, which\nprovides further stability to SB solvers. We formally prove convergence and a\nregret bound for the novel OMD formulation of SB acquisition. As a result, we\npropose a simulation-free SB algorithm called Variational Mirrored\nSchr\\\"odinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of\nthe Gaussian mixture parameterization for Schr\\\"odinger potentials. Based on\nthe Wasserstein gradient flow theory, the algorithm offers tractable learning\ndynamics that precisely approximate each OMD step. In experiments, we validate\nthe performance of the proposed VMSB algorithm across an extensive suite of\nbenchmarks. VMSB consistently outperforms contemporary SB solvers on a range of\nSB problems, demonstrating the robustness predicted by our theory."
    },
    {
        "date": "2025-04",
        "title": "Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation",
        "author": "Liangbo Ning, Wenqi Fan, and Qing Li",
        "link": "http://arxiv.org/abs/2504.02458v1",
        "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems have\nrevolutionized personalized recommendation frameworks and attracted extensive\nattention. Despite the remarkable success, existing LLM-empowered RecSys have\nbeen demonstrated to be highly vulnerable to minor perturbations. To mitigate\nthe negative impact of such vulnerabilities, one potential solution is to\nemploy collaborative signals based on item-item co-occurrence to purify the\nmalicious collaborative knowledge from the user's historical interactions\ninserted by attackers. On the other hand, due to the capabilities to expand\ninsufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)\ntechniques provide unprecedented opportunities to enhance the robustness of\nLLM-empowered recommender systems by introducing external collaborative\nknowledge. Therefore, in this paper, we propose a novel framework (RETURN) by\nretrieving external collaborative signals to purify the poisoned user profiles\nand enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.\nSpecifically, retrieval-augmented perturbation positioning is proposed to\nidentify potential perturbations within the users' historical sequences by\nretrieving external knowledge from collaborative item graphs. After that, we\nfurther retrieve the collaborative knowledge to cleanse the perturbations by\nusing either deletion or replacement strategies and introduce a robust ensemble\nrecommendation strategy to generate final robust predictions. Extensive\nexperiments on three real-world datasets demonstrate the effectiveness of the\nproposed RETURN."
    },
    {
        "date": "2025-04",
        "title": "Robust Randomized Low-Rank Approximation with Row-Wise Outlier Detection",
        "author": "Aidan Tiruvan",
        "link": "http://arxiv.org/abs/2504.02432v1",
        "abstract": "Robust low-rank approximation under row-wise adversarial corruption can be\nachieved with a single pass, randomized procedure that detects and removes\noutlier rows by thresholding their projected norms. We propose a scalable,\nnon-iterative algorithm that efficiently recovers the underlying low-rank\nstructure in the presence of row-wise adversarial corruption. By first\ncompressing the data with a Johnson Lindenstrauss projection, our approach\npreserves the geometry of clean rows while dramatically reducing\ndimensionality. Robust statistical techniques based on the median and median\nabsolute deviation then enable precise identification and removal of outlier\nrows with abnormally high norms. The subsequent rank-k approximation achieves\nnear-optimal error bounds with a one pass procedure that scales linearly with\nthe number of observations. Empirical results confirm that combining random\nsketches with robust statistics yields efficient, accurate decompositions even\nin the presence of large fractions of corrupted rows."
    },
    {
        "date": "2025-04",
        "title": "Toward General and Robust LLM-enhanced Text-attributed Graph Learning",
        "author": "Zihao Zhang, Xunkai Li, Rong-Hua Li, Bing Zhou, Zhenjun Li, and Guoren Wang",
        "link": "http://arxiv.org/abs/2504.02343v1",
        "abstract": "Recent advancements in Large Language Models (LLMs) and the proliferation of\nText-Attributed Graphs (TAGs) across various domains have positioned\nLLM-enhanced TAG learning as a critical research area. By utilizing rich graph\ndescriptions, this paradigm leverages LLMs to generate high-quality embeddings,\nthereby enhancing the representational capacity of Graph Neural Networks\n(GNNs). However, the field faces significant challenges: (1) the absence of a\nunified framework to systematize the diverse optimization perspectives arising\nfrom the complex interactions between LLMs and GNNs, and (2) the lack of a\nrobust method capable of handling real-world TAGs, which often suffer from\ntexts and edge sparsity, leading to suboptimal performance.\n  To address these challenges, we propose UltraTAG, a unified pipeline for\nLLM-enhanced TAG learning. UltraTAG provides a unified comprehensive and\ndomain-adaptive framework that not only organizes existing methodologies but\nalso paves the way for future advancements in the field. Building on this\nframework, we propose UltraTAG-S, a robust instantiation of UltraTAG designed\nto tackle the inherent sparsity issues in real-world TAGs. UltraTAG-S employs\nLLM-based text propagation and text augmentation to mitigate text sparsity,\nwhile leveraging LLM-augmented node selection techniques based on PageRank and\nedge reconfiguration strategies to address edge sparsity. Our extensive\nexperiments demonstrate that UltraTAG-S significantly outperforms existing\nbaselines, achieving improvements of 2.12\\% and 17.47\\% in ideal and sparse\nsettings, respectively. Moreover, as the data sparsity ratio increases, the\nperformance improvement of UltraTAG-S also rises, which underscores the\neffectiveness and robustness of UltraTAG-S."
    },
    {
        "date": "2025-04",
        "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing",
        "author": "Seif Mzoughi, Mohamed Elshafeia, and Foutse Khomh",
        "link": "http://arxiv.org/abs/2504.02335v1",
        "abstract": "Image segmentation is critical for applications such as medical imaging,\naugmented reality, and video surveillance. However, segmentation models often\nlack robustness, making them vulnerable to adversarial perturbations from\nsubtle image distortions. In this work, we propose SegRMT, a metamorphic\ntesting approach that leverages genetic algorithms (GA) to optimize sequences\nof spatial and spectral transformations while preserving image fidelity via a\npredefined PSNR threshold. Using the Cityscapes dataset, our method generates\nadversarial examples that effectively challenge the DeepLabV3 segmentation\nmodel. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection\nover Union (mIoU) to 6.4%, outperforming other adversarial baselines that\ndecrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial\ntraining, SegRMT boosts model performance, achieving mIoU improvements up to\n73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to\n53.8%, compared to only 2%-10% for other methods. These findings demonstrate\nthat SegRMT not only simulates realistic image distortions but also enhances\nthe robustness of segmentation models, making it a valuable tool for ensuring\nreliable performance in safety-critical applications."
    },
    {
        "date": "2025-04",
        "title": "Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance",
        "author": "Bo Yuan, Yulin Chen, Yin Zhang, and Wei Jiang",
        "link": "http://arxiv.org/abs/2504.02901v1",
        "abstract": "Learning from noisy labels (LNL) is a challenge that arises in many\nreal-world scenarios where collected training data can contain incorrect or\ncorrupted labels. Most existing solutions identify noisy labels and adopt\nactive learning to query human experts on them for denoising. In the era of\nlarge language models (LLMs), although we can reduce the human effort to\nimprove these methods, their performances are still subject to accurately\nseparating the clean and noisy samples from noisy data. In this paper, we\npropose an innovative collaborative learning framework NoiseAL based on active\nlearning to combine LLMs and small models (SMs) for learning from noisy labels.\nDuring collaborative training, we first adopt two SMs to form a co-prediction\nnetwork and propose a dynamic-enhanced threshold strategy to divide the noisy\ndata into different subsets, then select the clean and noisy samples from these\nsubsets to feed the active annotator LLMs to rectify noisy samples. Finally, we\nemploy different optimization objectives to conquer subsets with different\ndegrees of label noises. Extensive experiments on synthetic and real-world\nnoise datasets further demonstrate the superiority of our framework over\nstate-of-the-art baselines."
    },
    {
        "date": "2025-04",
        "title": "Secure Generalization through Stochastic Bidirectional Parameter Updates Using Dual-Gradient Mechanism",
        "author": "Shourya Goel, Himanshi Tibrewal, Anant Jain, Anshul Pundhir, and Pravendra Singh",
        "link": "http://arxiv.org/abs/2504.02213v1",
        "abstract": "Federated learning (FL) has gained increasing attention due to\nprivacy-preserving collaborative training on decentralized clients, mitigating\nthe need to upload sensitive data to a central server directly. Nonetheless,\nrecent research has underscored the risk of exposing private data to\nadversaries, even within FL frameworks. In general, existing methods sacrifice\nperformance while ensuring resistance to privacy leakage in FL. We overcome\nthese issues and generate diverse models at a global server through the\nproposed stochastic bidirectional parameter update mechanism. Using diverse\nmodels, we improved the generalization and feature representation in the FL\nsetup, which also helped to improve the robustness of the model against privacy\nleakage without hurting the model's utility. We use global models from past FL\nrounds to follow systematic perturbation in parameter space at the server to\nensure model generalization and resistance against privacy attacks. We generate\ndiverse models (in close neighborhoods) for each client by using systematic\nperturbations in model parameters at a fine-grained level (i.e., altering each\nconvolutional filter across the layers of the model) to improve the\ngeneralization and security perspective. We evaluated our proposed approach on\nfour benchmark datasets to validate its superiority. We surpassed the\nstate-of-the-art methods in terms of model utility and robustness towards\nprivacy leakage. We have proven the effectiveness of our method by evaluating\nperformance using several quantitative and qualitative results."
    },
    {
        "date": "2025-04",
        "title": "FastFlow: Early Yet Robust Network Flow Classification using the Minimal Number of Time-Series Packets",
        "author": "Rushi Jayeshkumar Babaria, Minzhao Lyu, Gustavo Batista, and Vijay Sivaraman",
        "link": "http://arxiv.org/abs/2504.02174v1",
        "abstract": "Network traffic classification is of great importance for network operators\nin their daily routines, such as analyzing the usage patterns of multimedia\napplications and optimizing network configurations. Internet service providers\n(ISPs) that operate high-speed links expect network flow classifiers to\naccurately classify flows early, using the minimal number of necessary initial\npackets per flow. These classifiers must also be robust to packet sequence\ndisorders in candidate flows and capable of detecting unseen flow types that\nare not within the existing classification scope, which are not well achieved\nby existing methods. In this paper, we develop FastFlow, a time-series flow\nclassification method that accurately classifies network flows as one of the\nknown types or the unknown type, which dynamically selects the minimal number\nof packets to balance accuracy and efficiency. Toward the objectives, we first\ndevelop a flow representation process that converts packet streams at both\nper-packet and per-slot granularity for precise packet statistics with\nrobustness to packet sequence disorders. Second, we develop a sequential\ndecision-based classification model that leverages LSTM architecture trained\nwith reinforcement learning. Our model makes dynamic decisions on the minimal\nnumber of time-series data points per flow for the confident classification as\none of the known flow types or an unknown one. We evaluated our method on\npublic datasets and demonstrated its superior performance in early and accurate\nflow classification. Deployment insights on the classification of over 22.9\nmillion flows across seven application types and 33 content providers in a\ncampus network over one week are discussed, showing that FastFlow requires an\naverage of only 8.37 packets and 0.5 seconds to classify the application type\nof a flow with over 91% accuracy and over 96% accuracy for the content\nproviders."
    },
    {
        "date": "2025-04",
        "title": "Exploring the Privacy and Security Challenges Faced by Migrant Domestic Workers in Chinese Smart Homes",
        "author": "Shijing He, Xiao Zhan, Yaxiong Lei, Yueyan Liu, Ruba Abu-Salma, and Jose Such",
        "link": "http://arxiv.org/abs/2504.02149v1",
        "abstract": "The growing use of smart home devices poses considerable privacy and security\nchallenges, especially for individuals like migrant domestic workers (MDWs) who\nmay be surveilled by their employers. This paper explores the privacy and\nsecurity challenges experienced by MDWs in multi-user smart homes through\nin-depth semi-structured interviews with 26 MDWs and 5 staff members of\nagencies that recruit and/or train domestic workers in China. Our findings\nreveal that the relationships between MDWs, their employers, and agencies are\ncharacterized by significant power imbalances, influenced by Chinese cultural\nand social factors (such as Confucianism and collectivism), as well as legal\nones. Furthermore, the widespread and normalized use of surveillance\ntechnologies in China, particularly in public spaces, exacerbates these power\nimbalances, reinforcing a sense of constant monitoring and control. Drawing on\nour findings, we provide recommendations to domestic worker agencies and\npolicymakers to address the privacy and security challenges facing MDWs in\nChinese smart homes."
    },
    {
        "date": "2025-04",
        "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits",
        "author": "Brandon Radosevich, and John Halloran",
        "link": "http://arxiv.org/abs/2504.03767v2",
        "abstract": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner"
    },
    {
        "date": "2025-04",
        "title": "Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds",
        "author": "Michael-Andrei Panaitescu-Liess, Yigitcan Kaya, Sicheng Zhu, Furong Huang, and Tudor Dumitras",
        "link": "http://arxiv.org/abs/2504.02142v1",
        "abstract": "Group robustness has become a major concern in machine learning (ML) as\nconventional training paradigms were found to produce high error on minority\ngroups. Without explicit group annotations, proposed solutions rely on\nheuristics that aim to identify and then amplify the minority samples during\ntraining. In our work, we first uncover a critical shortcoming of these\nmethods: an inability to distinguish legitimate minority samples from poison\nsamples in the training set. By amplifying poison samples as well, group\nrobustness methods inadvertently boost the success rate of an adversary --\ne.g., from $0\\%$ without amplification to over $97\\%$ with it. Notably, we\nsupplement our empirical evidence with an impossibility result proving this\ninability of a standard heuristic under some assumptions. Moreover,\nscrutinizing recent poisoning defenses both in centralized and federated\nlearning, we observe that they rely on similar heuristics to identify which\nsamples should be eliminated as poisons. In consequence, minority samples are\neliminated along with poisons, which damages group robustness -- e.g., from\n$55\\%$ without the removal of the minority samples to $41\\%$ with it. Finally,\nas they pursue opposing goals using similar heuristics, our attempt to\nalleviate the trade-off by combining group robustness methods and poisoning\ndefenses falls short. By exposing this tension, we also hope to highlight how\nbenchmark-driven ML scholarship can obscure the trade-offs among different\nmetrics with potentially detrimental consequences."
    },
    {
        "date": "2025-04",
        "title": "Robust Channel Estimation for Optical Wireless Communications Using Neural Network",
        "author": "Dianxin Luan, and John Thompson",
        "link": "http://arxiv.org/abs/2504.02134v1",
        "abstract": "Optical Wireless Communication (OWC) has gained significant attention due to\nits high-speed data transmission and throughput. Optical wireless channels are\noften assumed to be flat, but we evaluate frequency selective channels to\nconsider high data rate optical wireless or very dispersive environments. To\naddress this for optical scenarios, this paper presents a robust channel\nestimation framework with low-complexity to mitigate frequency-selective\neffects, then to improve system reliability and performance. This channel\nestimation framework contains a neural network that can estimate general\noptical wireless channels without prior channel information about the\nenvironment. Based on this estimate and the corresponding delay spread, one of\nseveral candidate offline-trained neural networks will be activated to predict\nthis channel. Simulation results demonstrate that the proposed method has\nimproved and robust normalized mean square error (NMSE) and bit error rate\n(BER) performance compared to conventional estimation methods while maintaining\ncomputational efficiency. These findings highlight the potential of neural\nnetwork solutions in enhancing the performance of OWC systems under indoor\nchannel conditions."
    },
    {
        "date": "2025-04",
        "title": "Base Station Certificate and Multi-Factor Authentication for Cellular Radio Control Communication Security",
        "author": "Sourav Purification, Simeon Wuthier, Jinoh Kim, Ikkyun Kim, and Sang-Yoon Chang",
        "link": "http://arxiv.org/abs/2504.02133v1",
        "abstract": "Current cellular networking remains vulnerable to malicious fake base\nstations due to the lack of base station authentication mechanism or even a key\nto enable authentication. We design and build a base station certificate\n(certifying the base station's public key and location) and a multi-factor\nauthentication (making use of the certificate and the information transmitted\nin the online radio control communications) to secure the authenticity and\nmessage integrity of the base station control communications. We advance beyond\nthe state-of-the-art research by introducing greater authentication factors\n(and analyzing their individual security properties and benefits), and by using\nblockchain to deliver the base station digital certificate offline (enabling\ngreater key length or security strength and computational or networking\nefficiency). We design the certificate construction, delivery, and the\nmulti-factor authentication use on the user equipment. The user verification\ninvolves multiple factors verified through the ledger database, the location\nsensing (GPS in our implementation), and the cryptographic signature\nverification of the cellular control communication (SIB1 broadcasting). We\nanalyze our scheme's security, performance, and the fit to the existing\nstandardized networking protocols. Our work involves the implementation of\nbuilding on X.509 certificate (adapted), smart contract-based blockchain,\n5G-standardized RRC control communications, and software-defined radios. Our\nanalyses show that our scheme effectively defends against more security threats\nand can enable stronger security, i.e., ECDSA with greater key lengths.\nFurthermore, our scheme enables computing and energy to be more than three\ntimes efficient than the previous research on the mobile user equipment."
    },
    {
        "date": "2025-04",
        "title": "On Model Protection in Federated Learning against Eavesdropping Attacks",
        "author": "Dipankar Maity, and Kushal Chakrabarti",
        "link": "http://arxiv.org/abs/2504.02114v1",
        "abstract": "In this study, we investigate the protection offered by federated learning\nalgorithms against eavesdropping adversaries. In our model, the adversary is\ncapable of intercepting model updates transmitted from clients to the server,\nenabling it to create its own estimate of the model. Unlike previous research,\nwhich predominantly focuses on safeguarding client data, our work shifts\nattention protecting the client model itself. Through a theoretical analysis,\nwe examine how various factors, such as the probability of client selection,\nthe structure of local objective functions, global aggregation at the server,\nand the eavesdropper's capabilities, impact the overall level of protection. We\nfurther validate our findings through numerical experiments, assessing the\nprotection by evaluating the model accuracy achieved by the adversary. Finally,\nwe compare our results with methods based on differential privacy, underscoring\ntheir limitations in this specific context."
    },
    {
        "date": "2025-04",
        "title": "A Systematic Review of Security Communication Strategies: Guidelines and Open Challenges",
        "author": "Carolina Carreira, Alexandra Mendes, Jo\u00e3o F. Ferreira, and Nicolas Christin",
        "link": "http://arxiv.org/abs/2504.02109v1",
        "abstract": "Cybersecurity incidents such as data breaches have become increasingly\ncommon, affecting millions of users and organizations worldwide. The complexity\nof cybersecurity threats challenges the effectiveness of existing security\ncommunication strategies. Through a systematic review of over 3,400 papers, we\nidentify specific user difficulties including information overload, technical\njargon comprehension, and balancing security awareness with comfort. Our\nfindings reveal consistent communication paradoxes: users require technical\ndetails for credibility yet struggle with jargon and need risk awareness\nwithout experiencing anxiety. We propose seven evidence-based guidelines to\nimprove security communication and identify critical research gaps including\nlimited studies with older adults, children, and non-US populations,\ninsufficient longitudinal research, and limited protocol sharing for\nreproducibility. Our guidelines emphasize user-centric communication adapted to\ncultural and demographic differences while ensuring security advice remains\nactionable. This work contributes to more effective security communication\npractices that enable users to recognize and respond to cybersecurity threats\nappropriately."
    },
    {
        "date": "2025-04",
        "title": "Chunking Attacks on File Backup Services using Content-Defined Chunking",
        "author": "Boris Alexeev, Colin Percival, and Yan X Zhang",
        "link": "http://arxiv.org/abs/2504.02095v1",
        "abstract": "Systems such as file backup services often use content-defined chunking (CDC)\nalgorithms, especially those based on rolling hash techniques, to split files\ninto chunks in a way that allows for data deduplication. These chunking\nalgorithms often depend on per-user parameters in an attempt to avoid leaking\ninformation about the data being stored. We present attacks to extract these\nchunking parameters and discuss protocol-agnostic attacks and loss of security\nonce the parameters are breached (including when these parameters are not setup\nat all, which is often available as an option). Our parameter-extraction\nattacks themselves are protocol-specific but their ideas are generalizable to\nmany potential CDC schemes."
    },
    {
        "date": "2025-04",
        "title": "Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses",
        "author": "Zhengchun Shang, and Wenlan Wei",
        "link": "http://arxiv.org/abs/2504.02080v1",
        "abstract": "Large Language Models (LLMs) are increasingly popular, powering a wide range\nof applications. Their widespread use has sparked concerns, especially through\njailbreak attacks that bypass safety measures to produce harmful content.\n  In this paper, we present a comprehensive security analysis of large language\nmodels (LLMs), addressing critical research questions on the evolution and\ndeterminants of model safety.\n  Specifically, we begin by identifying the most effective techniques for\ndetecting jailbreak attacks. Next, we investigate whether newer versions of\nLLMs offer improved security compared to their predecessors. We also assess the\nimpact of model size on overall security and explore the potential benefits of\nintegrating multiple defense strategies to enhance model robustness.\n  Our study evaluates both open-source models (e.g., LLaMA and Mistral) and\nclosed-source systems (e.g., GPT-4) by employing four state-of-the-art attack\ntechniques and assessing the efficacy of three new defensive approaches."
    },
    {
        "date": "2025-04",
        "title": "CO-DEFEND: Continuous Decentralized Federated Learning for Secure DoH-Based Threat Detection",
        "author": "Diego Cajaraville-Aboy, Marta Moure-Garrido, Carlos Beis-Penedo, Carlos Garcia-Rubio, Rebeca P. D\u00edaz-Redondo, Celeste Campo, Ana Fern\u00e1ndez-Vilas, and Manuel Fern\u00e1ndez-Veiga",
        "link": "http://arxiv.org/abs/2504.01882v1",
        "abstract": "The use of DNS over HTTPS (DoH) tunneling by an attacker to hide malicious\nactivity within encrypted DNS traffic poses a serious threat to network\nsecurity, as it allows malicious actors to bypass traditional monitoring and\nintrusion detection systems while evading detection by conventional traffic\nanalysis techniques. Machine Learning (ML) techniques can be used to detect DoH\ntunnels; however, their effectiveness relies on large datasets containing both\nbenign and malicious traffic. Sharing such datasets across entities is\nchallenging due to privacy concerns. In this work, we propose CO-DEFEND\n(Continuous Decentralized Federated Learning for Secure DoH-Based Threat\nDetection), a Decentralized Federated Learning (DFL) framework that enables\nmultiple entities to collaboratively train a classification machine learning\nmodel while preserving data privacy and enhancing resilience against single\npoints of failure. The proposed DFL framework, which is scalable and\nprivacy-preserving, is based on a federation process that allows multiple\nentities to train online their local models using incoming DoH flows in real\ntime as they are processed by the entity. In addition, we adapt four classical\nmachine learning algorithms, Support Vector Machines (SVM), Logistic Regression\n(LR), Decision Trees (DT), and Random Forest (RF), for federated scenarios,\ncomparing their results with more computationally complex alternatives such as\nneural networks. We compare our proposed method by using the dataset\nCIRA-CIC-DoHBrw-2020 with existing machine learning approaches to demonstrate\nits effectiveness in detecting malicious DoH tunnels and the benefits it\nbrings."
    },
    {
        "date": "2025-04",
        "title": "An Approach to Technical AGI Safety and Security",
        "author": "Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, S\u00e9bastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, and Anca Dragan",
        "link": "http://arxiv.org/abs/2504.01849v1",
        "abstract": "Artificial General Intelligence (AGI) promises transformative benefits but\nalso presents significant risks. We develop an approach to address the risk of\nharms consequential enough to significantly harm humanity. We identify four\nareas of risk: misuse, misalignment, mistakes, and structural risks. Of these,\nwe focus on technical approaches to misuse and misalignment. For misuse, our\nstrategy aims to prevent threat actors from accessing dangerous capabilities,\nby proactively identifying dangerous capabilities, and implementing robust\nsecurity, access restrictions, monitoring, and model safety mitigations. To\naddress misalignment, we outline two lines of defense. First, model-level\nmitigations such as amplified oversight and robust training can help to build\nan aligned model. Second, system-level security measures such as monitoring and\naccess control can mitigate harm even if the model is misaligned. Techniques\nfrom interpretability, uncertainty estimation, and safer design patterns can\nenhance the effectiveness of these mitigations. Finally, we briefly outline how\nthese ingredients could be combined to produce safety cases for AGI systems."
    },
    {
        "date": "2025-04",
        "title": "Autonomous optical navigation for DESTINY+: Enhancing misalignment robustness in flyby observations with a rotating telescope",
        "author": "Takayuki Hosonuma, Takeshi Miyabara, Naoya Ozaki, Ko Ishibashi, Yuta Suzaki, Peng Hong, Masayuki Ohta, and Takeshi Takashima",
        "link": "http://arxiv.org/abs/2504.01835v1",
        "abstract": "DESTINY+ is an upcoming JAXA Epsilon medium-class mission to flyby multiple\nasteroids including Phaethon. As an asteroid flyby observation instrument, a\ntelescope mechanically capable of single-axis rotation, named TCAP, is mounted\non the spacecraft to track and observe the target asteroids during flyby. As in\npast flyby missions utilizing rotating telescopes, TCAP is also used as a\nnavigation camera for autonomous optical navigation during the closest-approach\nphase. To mitigate the degradation of the navigation accuracy, past missions\nperformed calibration of the navigation camera's alignment before starting\noptical navigation. However, such calibration requires significant operational\ntime to complete and imposes constraints on the operation sequence. From the\nabove background, the DESTINY+ team has studied the possibility of reducing\noperational costs by allowing TCAP alignment errors to remain. This paper\ndescribes an autonomous optical navigation algorithm robust to the misalignment\nof rotating telescopes, proposed in this context. In the proposed method, the\nmisalignment of the telescope is estimated simultaneously with the spacecraft's\norbit relative to the flyby target. To deal with the nonlinearity between the\nmisalignment and the observation value, the proposed method utilizes the\nunscented Kalman filter, instead of the extended Kalman filter widely used in\npast studies. The proposed method was evaluated with numerical simulations on a\nPC and with hardware-in-the-loop simulation, taking the Phaethon flyby in the\nDESTINY+ mission as an example. The validation results suggest that the\nproposed method can mitigate the misalignment-induced degradation of the\noptical navigation accuracy with reasonable computational costs suited for\nonboard computers."
    },
    {
        "date": "2025-04",
        "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
        "author": "Huayang Huang, Xiangye Jin, Jiaxu Miao, and Yu Wu",
        "link": "http://arxiv.org/abs/2504.01819v1",
        "abstract": "The proliferation of text-to-image diffusion models (T2I DMs) has led to an\nincreased presence of AI-generated images in daily life. However, biased T2I\nmodels can generate content with specific tendencies, potentially influencing\npeople's perceptions. Intentional exploitation of these biases risks conveying\nmisleading information to the public. Current research on bias primarily\naddresses explicit biases with recognizable visual patterns, such as skin color\nand gender. This paper introduces a novel form of implicit bias that lacks\nexplicit visual features but can manifest in diverse ways across various\nsemantic contexts. This subtle and versatile nature makes this bias challenging\nto detect, easy to propagate, and adaptable to a wide range of scenarios. We\nfurther propose an implicit bias injection attack framework (IBI-Attacks)\nagainst T2I diffusion models by precomputing a general bias direction in the\nprompt embedding space and adaptively adjusting it based on different inputs.\nOur attack module can be seamlessly integrated into pre-trained diffusion\nmodels in a plug-and-play manner without direct manipulation of user input or\nmodel retraining. Extensive experiments validate the effectiveness of our\nscheme in introducing bias through subtle and diverse modifications while\npreserving the original semantics. The strong concealment and transferability\nof our attack across various scenarios further underscore the significance of\nour approach. Code is available at https://github.com/Hannah1102/IBI-attacks."
    },
    {
        "date": "2025-04",
        "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization",
        "author": "Chaohu Liu, Tianyi Gui, Yu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2504.01735v1",
        "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research."
    },
    {
        "date": "2025-04",
        "title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
        "author": "Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu",
        "link": "http://arxiv.org/abs/2504.01724v2",
        "abstract": "While recent image-based human animation methods achieve realistic body and\nfacial motion synthesis, critical gaps remain in fine-grained holistic\ncontrollability, multi-scale adaptability, and long-term temporal coherence,\nwhich leads to their lower expressiveness and robustness. We propose a\ndiffusion transformer (DiT) based framework, DreamActor-M1, with hybrid\nguidance to overcome these limitations. For motion guidance, our hybrid control\nsignals that integrate implicit facial representations, 3D head spheres, and 3D\nbody skeletons achieve robust control of facial expressions and body movements,\nwhile producing expressive and identity-preserving animations. For scale\nadaptation, to handle various body poses and image scales ranging from\nportraits to full-body views, we employ a progressive training strategy using\ndata with varying resolutions and scales. For appearance guidance, we integrate\nmotion patterns from sequential frames with complementary visual references,\nensuring long-term temporal coherence for unseen regions during complex\nmovements. Experiments demonstrate that our method outperforms the\nstate-of-the-art works, delivering expressive results for portraits,\nupper-body, and full-body generation with robust long-term consistency. Project\nPage: https://grisoon.github.io/DreamActor-M1/."
    },
    {
        "date": "2025-04",
        "title": "TransforMerger: Transformer-based Voice-Gesture Fusion for Robust Human-Robot Communication",
        "author": "Petr Vanc, and Karla Stepanova",
        "link": "http://arxiv.org/abs/2504.01708v1",
        "abstract": "As human-robot collaboration advances, natural and flexible communication\nmethods are essential for effective robot control. Traditional methods relying\non a single modality or rigid rules struggle with noisy or misaligned data as\nwell as with object descriptions that do not perfectly fit the predefined\nobject names (e.g. 'Pick that red object'). We introduce TransforMerger, a\ntransformer-based reasoning model that infers a structured action command for\nrobotic manipulation based on fused voice and gesture inputs. Our approach\nmerges multimodal data into a single unified sentence, which is then processed\nby the language model. We employ probabilistic embeddings to handle uncertainty\nand we integrate contextual scene understanding to resolve ambiguous references\n(e.g., gestures pointing to multiple objects or vague verbal cues like \"this\").\nWe evaluate TransforMerger in simulated and real-world experiments,\ndemonstrating its robustness to noise, misalignment, and missing information.\nOur results show that TransforMerger outperforms deterministic baselines,\nespecially in scenarios requiring more contextual knowledge, enabling more\nrobust and flexible human-robot communication. Code and datasets are available\nat: http://imitrob.ciirc.cvut.cz/publications/transformerger."
    },
    {
        "date": "2025-04",
        "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation",
        "author": "Junjie Chen, Yuecong Xu, Haosheng Li, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01668v2",
        "abstract": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack."
    },
    {
        "date": "2025-04",
        "title": "Robust Unsupervised Domain Adaptation for 3D Point Cloud Segmentation Under Source Adversarial Attacks",
        "author": "Haosheng Li, Junjie Chen, Yuecong Xu, and Kemi Ding",
        "link": "http://arxiv.org/abs/2504.01659v3",
        "abstract": "Unsupervised domain adaptation (UDA) frameworks have shown good\ngeneralization capabilities for 3D point cloud semantic segmentation models on\nclean data. However, existing works overlook adversarial robustness when the\nsource domain itself is compromised. To comprehensively explore the robustness\nof the UDA frameworks, we first design a stealthy adversarial point cloud\ngeneration attack that can significantly contaminate datasets with only minor\nperturbations to the point cloud surface. Based on that, we propose a novel\ndataset, AdvSynLiDAR, comprising synthesized contaminated LiDAR point clouds.\nWith the generated corrupted data, we further develop the Adversarial\nAdaptation Framework (AAF) as the countermeasure. Specifically, by extending\nthe key point sensitive (KPS) loss towards the Robust Long-Tail loss (RLT loss)\nand utilizing a decoder branch, our approach enables the model to focus on\nlong-tail classes during the pre-training phase and leverages high-confidence\ndecoded point cloud information to restore point cloud structures during the\nadaptation phase. We evaluated our AAF method on the AdvSynLiDAR dataset, where\nthe results demonstrate that our AAF method can mitigate performance\ndegradation under source adversarial perturbations for UDA in the 3D point\ncloud segmentation application."
    },
    {
        "date": "2025-04",
        "title": "Benchmarking the Spatial Robustness of DNNs via Natural and Adversarial Localized Corruptions",
        "author": "Giulia Marchiori Pietrosanti, Giulio Rossolini, Alessandro Biondi, and Giorgio Buttazzo",
        "link": "http://arxiv.org/abs/2504.01632v1",
        "abstract": "The robustness of DNNs is a crucial factor in safety-critical applications,\nparticularly in complex and dynamic environments where localized corruptions\ncan arise. While previous studies have evaluated the robustness of semantic\nsegmentation (SS) models under whole-image natural or adversarial corruptions,\na comprehensive investigation into the spatial robustness of dense vision\nmodels under localized corruptions remained underexplored. This paper fills\nthis gap by introducing specialized metrics for benchmarking the spatial\nrobustness of segmentation models, alongside with an evaluation framework to\nassess the impact of localized corruptions. Furthermore, we uncover the\ninherent complexity of characterizing worst-case robustness using a single\nlocalized adversarial perturbation. To address this, we propose region-aware\nmulti-attack adversarial analysis, a method that enables a deeper understanding\nof model robustness against adversarial perturbations applied to specific\nregions. The proposed metrics and analysis were evaluated on 15 segmentation\nmodels in driving scenarios, uncovering key insights into the effects of\nlocalized corruption in both natural and adversarial forms. The results reveal\nthat models respond to these two types of threats differently; for instance,\ntransformer-based segmentation models demonstrate notable robustness to\nlocalized natural corruptions but are highly vulnerable to adversarial ones and\nvice-versa for CNN-based models. Consequently, we also address the challenge of\nbalancing robustness to both natural and adversarial localized corruptions by\nmeans of ensemble models, thereby achieving a broader threat coverage and\nimproved reliability for dense vision tasks."
    },
    {
        "date": "2025-04",
        "title": "LightDefense: A Lightweight Uncertainty-Driven Defense against Jailbreaks via Shifted Token Distribution",
        "author": "Zhuoran Yang, Jie Peng, Zhen Tan, Tianlong Chen, and Yanyong Zhang",
        "link": "http://arxiv.org/abs/2504.01533v1",
        "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for defending against jailbreak attacks are primarily based on\nauxiliary models. These strategies, however, often require extensive data\ncollection or training. We propose LightDefense, a lightweight defense\nmechanism targeted at white-box models, which utilizes a safety-oriented\ndirection to adjust the probabilities of tokens in the vocabulary, making\nsafety disclaimers appear among the top tokens after sorting tokens by\nprobability in descending order. We further innovatively leverage LLM's\nuncertainty about prompts to measure their harmfulness and adaptively adjust\ndefense strength, effectively balancing safety and helpfulness. The\neffectiveness of LightDefense in defending against 5 attack methods across 2\ntarget LLMs, without compromising helpfulness to benign user queries,\nhighlights its potential as a novel and lightweight defense mechanism,\nenhancing security of LLMs."
    },
    {
        "date": "2025-04",
        "title": "A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown L\u00e9vy Process Dynamics",
        "author": "Qihao Ye, Xiaochuan Tian, and Yuhua Zhu",
        "link": "http://arxiv.org/abs/2504.01482v1",
        "abstract": "This paper develops a model-based framework for continuous-time policy\nevaluation (CTPE) in reinforcement learning, incorporating both Brownian and\nL\\'evy noise to model stochastic dynamics influenced by rare and extreme\nevents. Our approach formulates the policy evaluation problem as solving a\npartial integro-differential equation (PIDE) for the value function with\nunknown coefficients. A key challenge in this setting is accurately recovering\nthe unknown coefficients in the stochastic dynamics, particularly when driven\nby L\\'evy processes with heavy tail effects. To address this, we propose a\nrobust numerical approach that effectively handles both unbiased and censored\ntrajectory datasets. This method combines maximum likelihood estimation with an\niterative tail correction mechanism, improving the stability and accuracy of\ncoefficient recovery. Additionally, we establish a theoretical bound for the\npolicy evaluation error based on coefficient recovery error. Through numerical\nexperiments, we demonstrate the effectiveness and robustness of our method in\nrecovering heavy-tailed L\\'evy dynamics and verify the theoretical error\nanalysis in policy evaluation."
    },
    {
        "date": "2025-04",
        "title": "Emerging Cyber Attack Risks of Medical AI Agents",
        "author": "Jianing Qiu, Lin Li, Jiankai Sun, Hao Wei, Zhe Xu, Kyle Lam, and Wu Yuan",
        "link": "http://arxiv.org/abs/2504.03759v1",
        "abstract": "Large language models (LLMs)-powered AI agents exhibit a high level of\nautonomy in addressing medical and healthcare challenges. With the ability to\naccess various tools, they can operate within an open-ended action space.\nHowever, with the increase in autonomy and ability, unforeseen risks also\narise. In this work, we investigated one particular risk, i.e., cyber attack\nvulnerability of medical AI agents, as agents have access to the Internet\nthrough web browsing tools. We revealed that through adversarial prompts\nembedded on webpages, cyberattackers can: i) inject false information into the\nagent's response; ii) they can force the agent to manipulate recommendation\n(e.g., healthcare products and services); iii) the attacker can also steal\nhistorical conversations between the user and agent, resulting in the leak of\nsensitive/private medical information; iv) furthermore, the targeted agent can\nalso cause a computer system hijack by returning a malicious URL in its\nresponse. Different backbone LLMs were examined, and we found such cyber\nattacks can succeed in agents powered by most mainstream LLMs, with the\nreasoning models such as DeepSeek-R1 being the most vulnerable."
    },
    {
        "date": "2025-04",
        "title": "Leveraging Generalizability of Image-to-Image Translation for Enhanced Adversarial Defense",
        "author": "Haibo Zhang, Zhihua Yao, Kouichi Sakurai, and Takeshi Saitoh",
        "link": "http://arxiv.org/abs/2504.01399v1",
        "abstract": "In the rapidly evolving field of artificial intelligence, machine learning\nemerges as a key technology characterized by its vast potential and inherent\nrisks. The stability and reliability of these models are important, as they are\nfrequent targets of security threats. Adversarial attacks, first rigorously\ndefined by Ian Goodfellow et al. in 2013, highlight a critical vulnerability:\nthey can trick machine learning models into making incorrect predictions by\napplying nearly invisible perturbations to images. Although many studies have\nfocused on constructing sophisticated defensive mechanisms to mitigate such\nattacks, they often overlook the substantial time and computational costs of\ntraining and maintaining these models. Ideally, a defense method should be able\nto generalize across various, even unseen, adversarial attacks with minimal\noverhead. Building on our previous work on image-to-image translation-based\ndefenses, this study introduces an improved model that incorporates residual\nblocks to enhance generalizability. The proposed method requires training only\na single model, effectively defends against diverse attack types, and is\nwell-transferable between different target models. Experiments show that our\nmodel can restore the classification accuracy from near zero to an average of\n72\\% while maintaining competitive performance compared to state-of-the-art\nmethods."
    },
    {
        "date": "2025-04",
        "title": "FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores",
        "author": "Zhe Jiang, Sam Ainsworth, and Timothy Jones",
        "link": "http://arxiv.org/abs/2504.01380v1",
        "abstract": "High-performance security guarantees rely on hardware support. Generic\nprogrammable support for fine-grained instruction analysis has gained broad\ninterest in the literature as a fundamental building block for the security of\nfuture processors. Yet, implementation in real out-of-order (OoO) superscalar\nprocessors presents tough challenges that cannot be explored in highly abstract\nsimulators. We detail the challenges of implementing complex programmable\npathways without critical paths or contention. We then introduce FireGuard, the\nfirst implementation of fine-grained instruction analysis on a real OoO\nsuperscalar processor. We establish an end-to-end system, including\nmicroarchitecture, SoC, ISA and programming model. Experiments show that our\nsolution simultaneously ensures both security and performance of the system,\nwith parallel scalability. We examine the feasibility of building FireGuard\ninto modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F,\nwhere less than 1% silicon area is introduced. The Repo. of FireGuard's source\ncode: https://github.com/SEU-ACAL/reproduce-FireGuard-DAC-25."
    },
    {
        "date": "2025-04",
        "title": "Breaking BERT: Gradient Attack on Twitter Sentiment Analysis for Targeted Misclassification",
        "author": "Akil Raj Subedi, Taniya Shah, Aswani Kumar Cherukuri, and Thanos Vasilakos",
        "link": "http://arxiv.org/abs/2504.01345v1",
        "abstract": "Social media platforms like Twitter have increasingly relied on Natural\nLanguage Processing NLP techniques to analyze and understand the sentiments\nexpressed in the user generated content. One such state of the art NLP model is\nBidirectional Encoder Representations from Transformers BERT which has been\nwidely adapted in sentiment analysis. BERT is susceptible to adversarial\nattacks. This paper aims to scrutinize the inherent vulnerabilities of such\nmodels in Twitter sentiment analysis. It aims to formulate a framework for\nconstructing targeted adversarial texts capable of deceiving these models,\nwhile maintaining stealth. In contrast to conventional methodologies, such as\nImportance Reweighting, this framework core idea resides in its reliance on\ngradients to prioritize the importance of individual words within the text. It\nuses a whitebox approach to attain fine grained sensitivity, pinpointing words\nthat exert maximal influence on the classification outcome. This paper is\norganized into three interdependent phases. It starts with fine-tuning a\npre-trained BERT model on Twitter data. It then analyzes gradients of the model\nto rank words on their importance, and iteratively replaces those with feasible\ncandidates until an acceptable solution is found. Finally, it evaluates the\neffectiveness of the adversarial text against the custom trained sentiment\nclassification model. This assessment would help in gauging the capacity of the\nadversarial text to successfully subvert classification without raising any\nalarm."
    },
    {
        "date": "2025-04",
        "title": "Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks",
        "author": "Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, and Kin-Man Lam",
        "link": "http://arxiv.org/abs/2504.01308v2",
        "abstract": "Vision-Language Models (VLMs) extend the capabilities of Large Language\nModels (LLMs) by incorporating visual information, yet they remain vulnerable\nto jailbreak attacks, especially when processing noisy or corrupted images.\nAlthough existing VLMs adopt security measures during training to mitigate such\nattacks, vulnerabilities associated with noise-augmented visual inputs are\noverlooked. In this work, we identify that missing noise-augmented training\ncauses critical security gaps: many VLMs are susceptible to even simple\nperturbations such as Gaussian noise. To address this challenge, we propose\nRobust-VLGuard, a multimodal safety dataset with aligned / misaligned\nimage-text pairs, combined with noise-augmented fine-tuning that reduces attack\nsuccess rates while preserving functionality of VLM. For stronger\noptimization-based visual perturbation attacks, we propose DiffPure-VLM,\nleveraging diffusion models to convert adversarial perturbations into\nGaussian-like noise, which can be defended by VLMs with noise-augmented safety\nfine-tuning. Experimental results demonstrate that the distribution-shifting\nproperty of diffusion model aligns well with our fine-tuned VLMs, significantly\nmitigating adversarial perturbations across varying intensities. The dataset\nand code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM."
    },
    {
        "date": "2025-04",
        "title": "TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification",
        "author": "Kimia haghjooei, and Mansoor Rezghi",
        "link": "http://arxiv.org/abs/2504.01228v1",
        "abstract": "Deep learning models have achieved remarkable success in computer vision but\nremain vulnerable to adversarial attacks, particularly in black-box settings\nwhere model details are unknown. Existing adversarial attack methods(even those\nworks with key frames) often treat video data as simple vectors, ignoring their\ninherent multi-dimensional structure, and require a large number of queries,\nmaking them inefficient and detectable. In this paper, we propose\n\\textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages\nthe multi-dimensional properties of video data by representing videos as\nfourth-order tensors. By exploiting low-rank attack, our method significantly\nreduces the search space and the number of queries needed to generate\nadversarial examples in black-box settings. Experimental results on standard\nvideo classification datasets demonstrate that \\textbf{TenAd} effectively\ngenerates imperceptible adversarial perturbations while achieving higher attack\nsuccess rates and query efficiency compared to state-of-the-art methods. Our\napproach outperforms existing black-box adversarial attacks in terms of success\nrate, query efficiency, and perturbation imperceptibility, highlighting the\npotential of tensor-based methods for adversarial attacks on video models."
    },
    {
        "date": "2025-04",
        "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01220v1",
        "abstract": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal."
    },
    {
        "date": "2025-04",
        "title": "GRU-AUNet: A Domain Adaptation Framework for Contactless Fingerprint Presentation Attack Detection",
        "author": "Banafsheh Adami, and Nima Karimian",
        "link": "http://arxiv.org/abs/2504.01213v1",
        "abstract": "Although contactless fingerprints offer user comfort, they are more\nvulnerable to spoofing. The current solution for anti-spoofing in the area of\ncontactless fingerprints relies on domain adaptation learning, limiting their\ngeneralization and scalability. To address these limitations, we introduce\nGRU-AUNet, a domain adaptation approach that integrates a Swin\nTransformer-based UNet architecture with GRU-enhanced attention mechanisms, a\nDynamic Filter Network in the bottleneck, and a combined Focal and Contrastive\nLoss function. Trained in both genuine and spoof fingerprint images, GRU-AUNet\ndemonstrates robust resilience against presentation attacks, achieving an\naverage BPCER of 0.09\\% and APCER of 1.2\\% in the CLARKSON, COLFISPOOF, and\nIIITD datasets, outperforming state-of-the-art domain adaptation methods."
    },
    {
        "date": "2025-04",
        "title": "Performative Drift Resistant Classification Using Generative Domain Adversarial Networks",
        "author": "Maciej Makowski, Brandon Gower-Winter, and Georg Krempl",
        "link": "http://arxiv.org/abs/2504.01135v1",
        "abstract": "Performative Drift is a special type of Concept Drift that occurs when a\nmodel's predictions influence the future instances the model will encounter. In\nthese settings, retraining is not always feasible. In this work, we instead\nfocus on drift understanding as a method for creating drift-resistant\nclassifiers. To achieve this, we introduce the Generative Domain Adversarial\nNetwork (GDAN) which combines both Domain and Generative Adversarial Networks.\nUsing GDAN, domain-invariant representations of incoming data are created and a\ngenerative network is used to reverse the effects of performative drift. Using\nsemi-real and synthetic data generators, we empirically evaluate GDAN's ability\nto provide drift-resistant classification. Initial results are promising with\nGDAN limiting performance degradation over several timesteps. Additionally,\nGDAN's generative network can be used in tandem with other models to limit\ntheir performance degradation in the presence of performative drift. Lastly, we\nhighlight the relationship between model retraining and the unpredictability of\nperformative drift, providing deeper insights into the challenges faced when\nusing traditional Concept Drift mitigation strategies in the performative\nsetting."
    },
    {
        "date": "2025-04",
        "title": "ShieldGemma 2: Robust and Tractable Image Content Moderation",
        "author": "Wenjun Zeng, Dana Kurniawan, Ryan Mullins, Yuchi Liu, Tamoghna Saha, Dirichi Ike-Njoku, Jindong Gu, Yiwen Song, Cai Xu, Jingjing Zhou, Aparna Joshi, Shravan Dheep, Mani Malek, Hamid Palangi, Joon Baek, Rick Pereira, and Karthik Narasimhan",
        "link": "http://arxiv.org/abs/2504.01081v2",
        "abstract": "We introduce ShieldGemma 2, a 4B parameter image content moderation model\nbuilt on Gemma 3. This model provides robust safety risk predictions across the\nfollowing key harm categories: Sexually Explicit, Violence \\& Gore, and\nDangerous Content for synthetic images (e.g. output of any image generation\nmodel) and natural images (e.g. any image input to a Vision-Language Model). We\nevaluated on both internal and external benchmarks to demonstrate\nstate-of-the-art performance compared to LlavaGuard\n\\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base\nGemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we\npresent a novel adversarial data generation pipeline which enables a\ncontrolled, diverse, and robust image generation. ShieldGemma 2 provides an\nopen image moderation tool to advance multimodal safety and responsible AI\ndevelopment."
    },
    {
        "date": "2025-04",
        "title": "Safety and Security Risk Mitigation in Satellite Missions via Attack-Fault-Defense Trees",
        "author": "Reza Soltani, Pablo Diale, Milan Lopuha\u00e4-Zwakenberg, and Mari\u00eblle Stoelinga",
        "link": "http://arxiv.org/abs/2504.00988v1",
        "abstract": "Cyber-physical systems, such as self-driving cars or digitized electrical\ngrids, often involve complex interactions between security, safety, and\ndefense. Proper risk management strategies must account for these three\ncritical domains and their interaction because the failure to address one\ndomain can exacerbate risks in the others, leading to cascading effects that\ncompromise the overall system resilience. This work presents a case study from\nAscentio Technologies, a mission-critical system company in Argentina\nspecializing in aerospace, where the interplay between safety, security, and\ndefenses is critical for ensuring the resilience and reliability of their\nsystems. The main focus will be on the Ground Segment for the satellite project\ncurrently developed by the company. Analyzing safety, security, and defense\nmechanisms together in the Ground Segment of a satellite project is crucial\nbecause these domains are deeply interconnected--for instance, a security\nbreach could disable critical safety functions, or a safety failure could\ncreate opportunities for attackers to exploit vulnerabilities, amplifying the\nrisks to the entire system. This paper showcases the application of the\nAttack-Fault-Defense Tree (AFDT) framework, which integrates attack trees,\nfault trees, and defense mechanisms into a unified model. AFDT provides an\nintuitive visual language that facilitates interdisciplinary collaboration,\nenabling experts from various fields to better assess system vulnerabilities\nand defenses. By applying AFDT to the Ground Segment of the satellite project,\nwe demonstrate how qualitative analyses can be performed to identify weaknesses\nand enhance the overall system's security and safety. This case highlights the\nimportance of jointly analyzing attacks, faults, and defenses to improve\nresilience in complex cyber-physical environments."
    },
    {
        "date": "2025-04",
        "title": "S3C2 Summit 2024-08: Government Secure Supply Chain Summit",
        "author": "Courtney Miller, William Enck, Yasemin Acar, Michel Cukier, Alexandros Kapravelos, Christian Kastner, Dominik Wermke, and Laurie Williams",
        "link": "http://arxiv.org/abs/2504.00924v1",
        "abstract": "Supply chain security has become a very important vector to consider when\ndefending against adversary attacks. Due to this, more and more developers are\nkeen on improving their supply chains to make them more robust against future\nthreats. On August 29, 2024 researchers from the Secure Software Supply Chain\nCenter (S3C2) gathered 14 practitioners from 10 government agencies to discuss\nthe state of supply chain security. The goal of the summit is to share insights\nbetween companies and developers alike to foster new collaborations and ideas\nmoving forward. Through this meeting, participants were questions on best\npractices and thoughts how to improve things for the future. In this paper we\nsummarize the responses and discussions of the summit."
    },
    {
        "date": "2025-04",
        "title": "TAMIS: Tailored Membership Inference Attacks on Synthetic Data",
        "author": "Paul Andrey, Batiste Le Bars, and Marc Tommasi",
        "link": "http://arxiv.org/abs/2504.00758v1",
        "abstract": "Membership Inference Attacks (MIA) enable to empirically assess the privacy\nof a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA\nagainst differentially-private synthetic data generation methods that rely on\ngraphical models. This attack builds upon MAMA-MIA, a recently-published\nstate-of-the-art method. It lowers its computational cost and requires less\nattacker knowledge. Our attack is the product of a two-fold improvement. First,\nwe recover the graphical model having generated a synthetic dataset by using\nsolely that dataset, rather than shadow-modeling over an auxiliary one. This\nproves less costly and more performant. Second, we introduce a more\nmathematically-grounded attack score, that provides a natural threshold for\nbinary predictions. In our experiments, TAMIS achieves better or similar\nperformance as MAMA-MIA on replicas of the SNAKE challenge."
    },
    {
        "date": "2025-04",
        "title": "Alleviating Performance Disparity in Adversarial Spatiotemporal Graph Learning Under Zero-Inflated Distribution",
        "author": "Songran Bai, Yuheng Ji, Yue Liu, Xingwei Zhang, Xiaolong Zheng, and Daniel Dajun Zeng",
        "link": "http://arxiv.org/abs/2504.00721v1",
        "abstract": "Spatiotemporal Graph Learning (SGL) under Zero-Inflated Distribution (ZID) is\ncrucial for urban risk management tasks, including crime prediction and traffic\naccident profiling. However, SGL models are vulnerable to adversarial attacks,\ncompromising their practical utility. While adversarial training (AT) has been\nwidely used to bolster model robustness, our study finds that traditional AT\nexacerbates performance disparities between majority and minority classes under\nZID, potentially leading to irreparable losses due to underreporting critical\nrisk events. In this paper, we first demonstrate the smaller top-k gradients\nand lower separability of minority class are key factors contributing to this\ndisparity. To address these issues, we propose MinGRE, a framework for Minority\nClass Gradients and Representations Enhancement. MinGRE employs a\nmulti-dimensional attention mechanism to reweight spatiotemporal gradients,\nminimizing the gradient distribution discrepancies across classes.\nAdditionally, we introduce an uncertainty-guided contrastive loss to improve\nthe inter-class separability and intra-class compactness of minority\nrepresentations with higher uncertainty. Extensive experiments demonstrate that\nthe MinGRE framework not only significantly reduces the performance disparity\nacross classes but also achieves enhanced robustness compared to existing\nbaselines. These findings underscore the potential of our method in fostering\nthe development of more equitable and robust models."
    },
    {
        "date": "2025-04",
        "title": "Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models",
        "author": "Alireza Aghabagherloo, Aydin Abadi, Sumanta Sarkar, Vishnu Asutosh Dasu, and Bart Preneel",
        "link": "http://arxiv.org/abs/2504.00638v1",
        "abstract": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy."
    }
]