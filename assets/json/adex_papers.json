[
    {
        "date": "2025-07",
        "title": "BURN: Backdoor Unlearning via Adversarial Boundary Analysis",
        "author": "Yanghao Su, Jie Zhang, Yiming Li, Tianwei Zhang, Qing Guo, Weiming Zhang, Nenghai Yu, Nils Lukas, and Wenbo Zhou",
        "link": "http://arxiv.org/abs/2507.10491v1",
        "abstract": "Backdoor unlearning aims to remove backdoor-related information while\npreserving the model's original functionality. However, existing unlearning\nmethods mainly focus on recovering trigger patterns but fail to restore the\ncorrect semantic labels of poison samples. This limitation prevents them from\nfully eliminating the false correlation between the trigger pattern and the\ntarget label. To address this, we leverage boundary adversarial attack\ntechniques, revealing two key observations. First, poison samples exhibit\nsignificantly greater distances from decision boundaries compared to clean\nsamples, indicating they require larger adversarial perturbations to change\ntheir predictions. Second, while adversarial predicted labels for clean samples\nare uniformly distributed, those for poison samples tend to revert to their\noriginal correct labels. Moreover, the features of poison samples restore to\nclosely resemble those of corresponding clean samples after adding adversarial\nperturbations. Building upon these insights, we propose Backdoor Unlearning via\nadversaRial bouNdary analysis (BURN), a novel defense framework that integrates\nfalse correlation decoupling, progressive data refinement, and model\npurification. In the first phase, BURN employs adversarial boundary analysis to\ndetect poisoned samples based on their abnormal adversarial boundary distances,\nthen restores their correct semantic labels for fine-tuning. In the second\nphase, it employs a feedback mechanism that tracks prediction discrepancies\nbetween the original backdoored model and progressively sanitized models,\nguiding both dataset refinement and model purification. Extensive evaluations\nacross multiple datasets, architectures, and seven diverse backdoor attack\ntypes confirm that BURN effectively removes backdoor threats while maintaining\nthe model's original performance."
    },
    {
        "date": "2025-07",
        "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems",
        "author": "Hammad Atta, Ken Huang, Manish Bhatt, Kamal Ahmed, Muhammad Aziz Ul Haq, and Yasir Mehmood",
        "link": "http://arxiv.org/abs/2507.10457v1",
        "abstract": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions."
    },
    {
        "date": "2025-07",
        "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
        "author": "Utkarsh Singhal, Ryan Feng, Stella X. Yu, and Atul Prakash",
        "link": "http://arxiv.org/abs/2507.10375v1",
        "abstract": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal."
    },
    {
        "date": "2025-07",
        "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach",
        "author": "Mohammed Bouri, and Adnane Saoud",
        "link": "http://arxiv.org/abs/2507.10330v1",
        "abstract": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM"
    },
    {
        "date": "2025-07",
        "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
        "author": "Xinlong Ding, Hongwei Yu, Jiawei Li, Feifan Li, Yu Shang, Bochao Zou, Huimin Ma, and Jiansheng Chen",
        "link": "http://arxiv.org/abs/2507.10265v1",
        "abstract": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels."
    },
    {
        "date": "2025-07",
        "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks",
        "author": "Ben Hamscher, Edgar Heinert, Annika M\u00fctze, Kira Maag, and Matthias Rottmann",
        "link": "http://arxiv.org/abs/2507.10239v1",
        "abstract": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method."
    },
    {
        "date": "2025-07",
        "title": "Secure and Efficient Quantum Signature Scheme Based on the Controlled Unitary Operations Encryption",
        "author": "Debnath Ghosh, Soumit Roy, Prithwi Bagchi, Indranil Chakrabarty, and Ashok Kumar Das",
        "link": "http://arxiv.org/abs/2507.10233v1",
        "abstract": "Quantum digital signatures ensure unforgeable message authenticity and\nintegrity using quantum principles, offering unconditional security against\nboth classical and quantum attacks. They are crucial for secure communication\nin high-stakes environments, ensuring trust and long-term protection in the\nquantum era. Nowadays, the majority of arbitrated quantum signature (AQS)\nprotocols encrypt data qubit by qubit using the quantum one-time pad (QOTP).\nDespite providing robust data encryption, QOTP is not a good fit for AQS\nbecause of its susceptibility to many types of attacks. In this work, we\npresent an efficient AQS protocol to encrypt quantum message ensembles using a\ndistinct encryption technique, the chained controlled unitary operations. In\ncontrast to existing protocols, our approach successfully prevents disavowal\nand forgery attacks. We hope this contributes to advancing future\ninvestigations into the development of AQS protocols."
    },
    {
        "date": "2025-07",
        "title": "Learning Private Representations through Entropy-based Adversarial Training",
        "author": "Tassilo Klein, and Moin Nabi",
        "link": "http://arxiv.org/abs/2507.10194v1",
        "abstract": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage."
    },
    {
        "date": "2025-07",
        "title": "HASSLE: A Self-Supervised Learning Enhanced Hijacking Attack on Vertical Federated Learning",
        "author": "Weiyang He, and Chip-Hong Chang",
        "link": "http://arxiv.org/abs/2507.10162v1",
        "abstract": "Vertical Federated Learning (VFL) enables an orchestrating active party to\nperform a machine learning task by cooperating with passive parties that\nprovide additional task-related features for the same training data entities.\nWhile prior research has leveraged the privacy vulnerability of VFL to\ncompromise its integrity through a combination of label inference and backdoor\nattacks, their effectiveness is constrained by the low label inference\nprecision and suboptimal backdoor injection conditions. To facilitate a more\nrigorous security evaluation on VFL without these limitations, we propose\nHASSLE, a hijacking attack framework composed of a gradient-direction-based\nlabel inference module and an adversarial embedding generation algorithm\nenhanced by self-supervised learning. HASSLE accurately identifies private\nsamples associated with a targeted label using only a single known instance of\nthat label. In the two-party scenario, it demonstrates strong performance with\nan attack success rate (ASR) of over 99% across four datasets, including both\nimage and tabular modalities, and achieves 85% ASR on the more complex\nCIFAR-100 dataset. Evaluation of HASSLE against 8 potential defenses further\nhighlights its significant threat while providing new insights into building a\ntrustworthy VFL system."
    },
    {
        "date": "2025-07",
        "title": "On the Efficiency of Training Robust Decision Trees",
        "author": "Benedict Gerlach, Marie Anastacio, and Holger H. Hoos",
        "link": "http://arxiv.org/abs/2507.10048v1",
        "abstract": "As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time."
    },
    {
        "date": "2025-07",
        "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving",
        "author": "Yixun Zhang, Lizhi Wang, Junjun Zhao, Wending Zhao, Feng Zhou, Yonghao Dang, and Jianqin Yin",
        "link": "http://arxiv.org/abs/2507.09993v1",
        "abstract": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving."
    },
    {
        "date": "2025-07",
        "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering",
        "author": "Yiran Qiao, Disheng Liu, Yiren Lu, Yu Yin, Mengnan Du, and Jing Ma",
        "link": "http://arxiv.org/abs/2507.09881v1",
        "abstract": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism."
    },
    {
        "date": "2025-07",
        "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing",
        "author": "Nguyen Van Duc, Bui Duc Manh, Quang-Trung Luu, Dinh Thai Hoang, Van-Linh Nguyen, and Diep N. Nguyen",
        "link": "http://arxiv.org/abs/2507.09860v1",
        "abstract": "This paper aims to propose a novel machine learning (ML) approach\nincorporating Homomorphic Encryption (HE) to address privacy limitations in\nUnmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related\nto distance, altitude, and face orientation, high-resolution imagery and\nsophisticated neural networks enable accurate face recognition in dynamic\nenvironments. However, privacy concerns arise from the extensive surveillance\ncapabilities of UAVs. To resolve this issue, we propose a novel framework that\nintegrates HE with advanced neural networks to secure facial data throughout\nthe inference phase. This method ensures that facial data remains secure with\nminimal impact on detection accuracy. Specifically, the proposed system\nleverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly\non encrypted data, optimizing computational efficiency and security.\nFurthermore, we develop an effective data encoding method specifically designed\nto preprocess the raw facial data into CKKS form in a\nSingle-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a\nsecure inference algorithm to compute on ciphertext without needing decryption.\nThis approach not only protects data privacy during the processing of facial\ndata but also enhances the efficiency of UAV-based face detection systems.\nExperimental results demonstrate that our method effectively balances privacy\nprotection and detection performance, making it a viable solution for UAV-based\nsecure face detection. Significantly, our approach (while maintaining data\nconfidentially with HE encryption) can still achieve an accuracy of less than\n1% compared to the benchmark without using encryption."
    },
    {
        "date": "2025-07",
        "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective",
        "author": "Xiaofei Wang, Mingliang Han, Tianyu Hao, Cegang Li, Yunbo Zhao, and Keke Tang",
        "link": "http://arxiv.org/abs/2507.09857v1",
        "abstract": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability"
    },
    {
        "date": "2025-07",
        "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions",
        "author": "Yasir Ech-Chammakhy, Anas Motii, Anass Rabii, and Jaafar Chbili",
        "link": "http://arxiv.org/abs/2507.09762v1",
        "abstract": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis."
    },
    {
        "date": "2025-07",
        "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)",
        "author": "Abdul Manaf, and Nimra Mughal",
        "link": "http://arxiv.org/abs/2507.09759v1",
        "abstract": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification"
    },
    {
        "date": "2025-07",
        "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging",
        "author": "Robby Hoover, Nelly Elsayed, Zag ElSayed, and Chengcheng Li",
        "link": "http://arxiv.org/abs/2507.09731v1",
        "abstract": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts."
    },
    {
        "date": "2025-07",
        "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness",
        "author": "Md Mushfiqur Rahaman, Elliot Chang, Tasmiah Haque, and Srinjoy Das",
        "link": "http://arxiv.org/abs/2507.09687v1",
        "abstract": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments."
    },
    {
        "date": "2025-07",
        "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories",
        "author": "Xiaojie Lin, Baihe Ma, Xu Wang, Guangsheng Yu, Ying He, Wei Ni, and Ren Ping Liu",
        "link": "http://arxiv.org/abs/2507.09624v1",
        "abstract": "Driving trajectory data remains vulnerable to privacy breaches despite\nexisting mitigation measures. Traditional methods for detecting driving\ntrajectories typically rely on map-matching the path using Global Positioning\nSystem (GPS) data, which is susceptible to GPS data outage. This paper\nintroduces CAN-Trace, a novel privacy attack mechanism that leverages\nController Area Network (CAN) messages to uncover driving trajectories, posing\na significant risk to drivers' long-term privacy. A new trajectory\nreconstruction algorithm is proposed to transform the CAN messages,\nspecifically vehicle speed and accelerator pedal position, into weighted graphs\naccommodating various driving statuses. CAN-Trace identifies driving\ntrajectories using graph-matching algorithms applied to the created graphs in\ncomparison to road networks. We also design a new metric to evaluate matched\ncandidates, which allows for potential data gaps and matching inaccuracies.\nEmpirical validation under various real-world conditions, encompassing\ndifferent vehicles and driving regions, demonstrates the efficacy of CAN-Trace:\nit achieves an attack success rate of up to 90.59% in the urban region, and\n99.41% in the suburban region."
    },
    {
        "date": "2025-07",
        "title": "Efficient Private Inference Based on Helper-Assisted Malicious Security Dishonest Majority MPC",
        "author": "Kaiwen Wang, Yuehan Dong, Junchao Fan, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2507.09607v2",
        "abstract": "Private inference based on Secure Multi-Party Computation (MPC) addresses\ndata privacy risks in Machine Learning as a Service (MLaaS). However, existing\nMPC-based private inference frameworks focuses on semi-honest or honest\nmajority models, whose threat models are overly idealistic, while malicious\nsecurity dishonest majority models face the challenge of low efficiency. To\nbalance security and efficiency, we propose a private inference framework using\nHelper-Assisted Malicious Security Dishonest Majority Model (HA-MSDM). This\nframework includes our designed five MPC protocols and a co-optimized strategy.\nThese protocols achieve efficient fixed-round multiplication, exponentiation,\nand polynomial operations, providing foundational primitives for private\ninference. The co-optimized strategy balances inference efficiency and\naccuracy. To enhance efficiency, we employ polynomial approximation for\nnonlinear layers. For improved accuracy, we construct sixth-order polynomial\napproximation within a fixed interval to achieve high-precision activation\nfunction fitting and introduce parameter-adjusted batch normalization layers to\nconstrain the activation escape problem. Benchmark results on LeNet and AlexNet\nshow our framework achieves 2.4-25.7x speedup in LAN and 1.3-9.5x acceleration\nin WAN compared to state-of-the-art frameworks (IEEE S&P'25), maintaining high\naccuracy with only 0.04%-1.08% relative errors."
    },
    {
        "date": "2025-07",
        "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences",
        "author": "Bocheng Ju, Junchao Fan, Jiaqi Liu, and Xiaolin Chang",
        "link": "http://arxiv.org/abs/2507.09602v1",
        "abstract": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications."
    },
    {
        "date": "2025-07",
        "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation",
        "author": "Isaac Shi, Zeyuan Li, Fan Liu, Wenli Wang, Lewei He, Yang Yang, and Tianyu Shi",
        "link": "http://arxiv.org/abs/2507.09588v1",
        "abstract": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance."
    },
    {
        "date": "2025-07",
        "title": "A Login Page Transparency and Visual Similarity Based Zero Day Phishing Defense Protocol",
        "author": "Gaurav Varshney, Akanksha Raj, Divya Sangwan, Sharif Abuadbba, Rina Mishra, and Yansong Gao",
        "link": "http://arxiv.org/abs/2507.09564v1",
        "abstract": "Phishing is a prevalent cyberattack that uses look-alike websites to deceive\nusers into revealing sensitive information. Numerous efforts have been made by\nthe Internet community and security organizations to detect, prevent, or train\nusers to avoid falling victim to phishing attacks. Most of this research over\nthe years has been highly diverse and application-oriented, often serving as\nstandalone solutions for HTTP clients, servers, or third parties. However,\nlimited work has been done to develop a comprehensive or proactive\nprotocol-oriented solution to effectively counter phishing attacks. Inspired by\nthe concept of certificate transparency, which allows certificates issued by\nCertificate Authorities (CAs) to be publicly verified by clients, thereby\nenhancing transparency, we propose a concept called Page Transparency (PT) for\nthe web. The proposed PT requires login pages that capture users' sensitive\ninformation to be publicly logged via PLS and made available to web clients for\nverification. The pages are verified to be logged using cryptographic proofs.\nSince all pages are logged on a PLS and visually compared with existing pages\nthrough a comprehensive visual page-matching algorithm, it becomes impossible\nfor an attacker to register a deceptive look-alike page on the PLS and receive\nthe cryptographic proof required for client verification. All implementations\noccur on the client side, facilitated by the introduction of a new HTTP PT\nheader, eliminating the need for platform-specific changes or the installation\nof third-party solutions for phishing prevention."
    },
    {
        "date": "2025-07",
        "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
        "author": "Zihao Xiong, Fei Zhou, Fengyi Wu, Shuai Yuan, Maixia Fu, Zhenming Peng, Jian Yang, and Yimian Dai",
        "link": "http://arxiv.org/abs/2507.09541v1",
        "abstract": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net."
    },
    {
        "date": "2025-07",
        "title": "A Mixture of Linear Corrections Generates Secure Code",
        "author": "Weichen Yu, Ravi Mangal, Terry Zhuo, Matt Fredrikson, and Corina S. Pasareanu",
        "link": "http://arxiv.org/abs/2507.09508v1",
        "abstract": "Large language models (LLMs) have become proficient at sophisticated\ncode-generation tasks, yet remain ineffective at reliably detecting or avoiding\ncode vulnerabilities. Does this deficiency stem from insufficient learning\nabout code vulnerabilities, or is it merely a result of ineffective prompting?\nUsing representation engineering techniques, we investigate whether LLMs\ninternally encode the concepts necessary to identify code vulnerabilities. We\nfind that current LLMs encode precise internal representations that distinguish\nvulnerable from secure code--achieving greater accuracy than standard prompting\napproaches. Leveraging these vulnerability-sensitive representations, we\ndevelop an inference-time steering technique that subtly modulates the model's\ntoken-generation probabilities through a mixture of corrections (MoC). Our\nmethod effectively guides LLMs to produce less vulnerable code without\ncompromising functionality, demonstrating a practical approach to controlled\nvulnerability management in generated code. Notably, MoC enhances the security\nratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving\nfunctionality on HumanEval pass@1 by 2.1\\%."
    },
    {
        "date": "2025-07",
        "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning",
        "author": "Lingfeng He, De Cheng, Zhiheng Ma, Huaijie Wang, Dingwen Zhang, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2507.09471v1",
        "abstract": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers",
        "author": "Santhosh Kumar Ravindran",
        "link": "http://arxiv.org/abs/2507.09406v1",
        "abstract": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models."
    },
    {
        "date": "2025-07",
        "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents",
        "author": "Matous Kozak, Roshanak Zilouchian Moghaddam, and Siva Sivaraman",
        "link": "http://arxiv.org/abs/2507.09329v1",
        "abstract": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents."
    },
    {
        "date": "2025-07",
        "title": "Hybrid Quantum Security for IPsec",
        "author": "Javier Blanco-Romero, Pedro Otero Garc\u00eda, Daniel Sobral-Blanco, Florina Almenares Mendoza, Ana Fern\u00e1ndez Vilas, and Manuel Fern\u00e1ndez-Veiga",
        "link": "http://arxiv.org/abs/2507.09288v1",
        "abstract": "Quantum Key Distribution (QKD) offers information-theoretic security against\nquantum computing threats, but integrating QKD into existing security protocols\nremains an unsolved challenge due to fundamental mismatches between\npre-distributed quantum keys and computational key exchange paradigms. This\npaper presents the first systematic comparison of sequential versus parallel\nhybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental\nprotocol design principles that extend beyond specific implementations. We\nintroduce two novel approaches for incorporating QKD into Internet Key Exchange\nversion 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS\nQKD 014 stateless API specifications: (1) a pure QKD approach that replaces\ncomputational key derivation with identifier-based quantum key coordination,\nand (2) a unified QKD-KEM abstraction that enables parallel composition of\nquantum and post-quantum cryptographic methods within existing protocol\nframeworks. Our key insight is that parallel hybrid approaches eliminate the\nmultiplicative latency penalties inherent in sequential methods mandated by RFC\n9370, achieving significant performance improvements under realistic network\nconditions. Performance evaluation using a Docker-based testing framework with\nIDQuantique QKD hardware demonstrates that the parallel hybrid approach\nsignificantly outperforms sequential methods under network latency conditions,\nwhile pure QKD achieves minimal bandwidth overhead through identifier-based key\ncoordination. Our implementations provide practical quantum-enhanced IPsec\nsolutions suitable for critical infrastructure deployments requiring\ndefense-in-depth security."
    },
    {
        "date": "2025-07",
        "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift",
        "author": "Behraj Khan, and Tahir Syed",
        "link": "http://arxiv.org/abs/2507.09222v1",
        "abstract": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md"
    },
    {
        "date": "2025-07",
        "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
        "author": "Louise Largeau, Erwan Koch, David Leutwyler, Gregoire Mariethoz, Valerie Chavez-Demoulin, and Tom Beucler",
        "link": "http://arxiv.org/abs/2507.09166v1",
        "abstract": "The coarse spatial resolution of gridded climate models, such as general\ncirculation models, limits their direct use in projecting socially relevant\nvariables like extreme precipitation. Most downscaling methods estimate the\nconditional distributions of extremes by generating large ensembles,\ncomplicating the assessment of robustness under distributional shifts, such as\nthose induced by climate change. To better understand and potentially improve\nrobustness, we propose super-resolving the parameters of the target variable's\nprobability distribution directly using analytically tractable mappings. Within\na perfect-model framework over Switzerland, we demonstrate that vector\ngeneralized linear and additive models can super-resolve the generalized\nextreme value distribution of summer hourly precipitation extremes from coarse\nprecipitation fields and topography. We introduce the notion of a \"robustness\ngap\", defined as the difference in predictive error between present-trained and\nfuture-trained models, and use it to diagnose how model structure affects the\ngeneralization of each quantile to a pseudo-global warming scenario. By\nevaluating multiple model configurations, we also identify an upper limit on\nthe super-resolution factor based on the spatial auto- and cross-correlation of\nprecipitation and elevation, beyond which coarse precipitation loses predictive\nvalue. Our framework is broadly applicable to variables governed by parametric\ndistributions and offers a model-agnostic diagnostic for understanding when and\nwhy empirical downscaling generalizes to climate change and extremes."
    },
    {
        "date": "2025-07",
        "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
        "author": "Di Wen, Kunyu Peng, Kailun Yang, Yufan Chen, Ruiping Liu, Junwei Zheng, Alina Roitberg, and Rainer Stiefelhagen",
        "link": "http://arxiv.org/abs/2507.09111v1",
        "abstract": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI."
    },
    {
        "date": "2025-07",
        "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models",
        "author": "Hanene F. Z. Brachemi Meftah, Wassim Hamidouche, Sid Ahmed Fezza, and Olivier D\u00e9forges",
        "link": "http://arxiv.org/abs/2507.08982v1",
        "abstract": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack."
    },
    {
        "date": "2025-07",
        "title": "Characterizing Security and Privacy Teaching Standards for Schools in the United States",
        "author": "Katherine Limes, Nathan Malkin, and Kelsey R. Fulton",
        "link": "http://arxiv.org/abs/2507.08978v1",
        "abstract": "Increasingly, students begin learning aspects of security and privacy during\ntheir primary and secondary education (grades K-12 in the United States).\nIndividual U.S. states and some national organizations publish teaching\nstandards -- guidance that outlines expectations for what students should learn\n-- which often form the basis for course curricula. However, research has not\nyet examined what is covered by these standards and whether the topics align\nwith what the broader security and privacy community thinks students should\nknow. To shed light on these questions, we started by collecting computer\nscience teaching standards from all U.S. states and eight national\norganizations. After manually examining a total of 11,954 standards, we labeled\n3,778 of them as being related to security and privacy, further classifying\nthese into 103 topics. Topics ranged from technical subjects like encryption,\nnetwork security, and embedded systems to social subjects such as laws, ethics,\nand appropriate online behavior. Subsequently, we interviewed 11 security and\nprivacy professionals to examine how the teaching standards align with their\nexpectations. We found that, while the specific topics they mentioned mostly\noverlapped with those of existing standards, professionals placed a greater\nemphasis on threat modeling and security mindset."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series",
        "author": "Azimov Sherkhon, Susana Lopez-Moreno, Eric Dolores-Cuenca, Sieun Lee, and Sangil Kim",
        "link": "http://arxiv.org/abs/2507.08738v1",
        "abstract": "Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency."
    },
    {
        "date": "2025-07",
        "title": "SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations",
        "author": "Peter Crowley, Zachary Serlin, Tyler Paine, Makai Mann, Michael Benjamin, and Calin Belta",
        "link": "http://arxiv.org/abs/2507.08707v1",
        "abstract": "Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations."
    },
    {
        "date": "2025-07",
        "title": "Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security",
        "author": "Pascal Debus, Maximilian Wendlinger, Kilian Tscharke, Daniel Herr, Cedric Br\u00fcgmann, Daniel Ohl de Mello, Juris Ulmanis, Alexander Erhard, Arthur Schmidt, and Fabian Petsch",
        "link": "http://arxiv.org/abs/2507.08623v1",
        "abstract": "Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning."
    },
    {
        "date": "2025-07",
        "title": "Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation",
        "author": "Junxue Yang, Xin Liao, Weixuan Tang, Jianhua Yang, and Zheng Qin",
        "link": "http://arxiv.org/abs/2507.08343v1",
        "abstract": "Deep hiding has been exploring the hiding capability of deep learning-based\nmodels, aiming to conceal image-level messages into cover images and reveal\nthem from generated stego images. Existing schemes are easily detected by\nsteganalyzers due to their large payloads and their limitation to feature\nextraction based solely on either pure convolution or pure transformer\noperators within a single range, as well as pixel-level loss constraints. To\naddress the issue, in this paper, we introduce generation-based adversarial\nattacks into color JPEG image deep hiding and propose a multi-range\nrepresentations-driven adversarial stego generation framework called MRAG from\na steganalysis perspective. Specifically, we integrate the local-range neighbor\nreception characteristic of the convolution and the global-range dependency\nmodeling of the transformer to construct MRAG. Meanwhile, we use the\ntransformed images obtained through coarse-grained and fine-grained frequency\ndecomposition as inputs, introducing multi-grained information. Furthermore, a\nfeatures angle-norm disentanglement loss is designed to constrain the generated\nstegos closer to covers in the angle and norm space of the steganalyzer's\nclassified features. Consequently, small yet effective adversarial\nperturbations can be injected into the process of generating stegos, ensuring\nthat stegos maintain favorable secret restorability and imperceptibility.\nExtensive experiments demonstrate that MRAG can achieve state-of-the-art\nperformance."
    },
    {
        "date": "2025-07",
        "title": "Invariant-based Robust Weights Watermark for Large Language Models",
        "author": "Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, and Xiaobing Guo",
        "link": "http://arxiv.org/abs/2507.08288v1",
        "abstract": "Watermarking technology has gained significant attention due to the\nincreasing importance of intellectual property (IP) rights, particularly with\nthe growing deployment of large language models (LLMs) on billions\nresource-constrained edge devices. To counter the potential threats of IP theft\nby malicious users, this paper introduces a robust watermarking scheme without\nretraining or fine-tuning for transformer models. The scheme generates a unique\nkey for each user and derives a stable watermark value by solving linear\nconstraints constructed from model invariants. Moreover, this technology\nutilizes noise mechanism to hide watermark locations in multi-user scenarios\nagainst collusion attack. This paper evaluates the approach on three popular\nmodels (Llama3, Phi3, Gemma), and the experimental results confirm the strong\nrobustness across a range of attack methods (fine-tuning, pruning,\nquantization, permutation, scaling, reversible matrix and collusion attacks)."
    },
    {
        "date": "2025-07",
        "title": "Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training",
        "author": "Aleksei Ilin, Gor Matevosyan, Xueying Ma, Vladimir Eremin, Suhaa Dada, Muqun Li, Riyaaz Shaik, and Haluk Noyan Tokgozoglu",
        "link": "http://arxiv.org/abs/2507.08284v1",
        "abstract": "We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems."
    },
    {
        "date": "2025-07",
        "title": "MIRRAMS: Towards Training Models Robust to Missingness Distribution Shifts",
        "author": "Jihye Lee, Minseo Kang, and Dongha Kim",
        "link": "http://arxiv.org/abs/2507.08280v1",
        "abstract": "In real-world data analysis, missingness distributional shifts between\ntraining and test input datasets frequently occur, posing a significant\nchallenge to achieving robust prediction performance. In this study, we propose\na novel deep learning framework designed to address such shifts in missingness\ndistributions. We begin by introducing a set of mutual information-based\nconditions, called MI robustness conditions, which guide a prediction model to\nextract label-relevant information while remaining invariant to diverse\nmissingness patterns, thereby enhancing robustness to unseen missingness\nscenarios at test-time. To make these conditions practical, we propose simple\nyet effective techniques to derive loss terms corresponding to each and\nformulate a final objective function, termed MIRRAMS(Mutual Information\nRegularization for Robustness Against Missingness Shifts). As a by-product, our\nanalysis provides a theoretical interpretation of the principles underlying\nconsistency regularization-based semi-supervised learning methods, such as\nFixMatch. Extensive experiments across various benchmark datasets show that\nMIRRAMS consistently outperforms existing baselines and maintains stable\nperformance across diverse missingness scenarios. Moreover, our approach\nachieves state-of-the-art performance even without missing data and can be\nnaturally extended to address semi-supervised learning tasks, highlighting\nMIRRAMS as a powerful, off-the-shelf framework for general-purpose learning."
    },
    {
        "date": "2025-07",
        "title": "Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks",
        "author": "Sofia Ivolgina, P. Thomas Fletcher, and Baba C. Vemuri",
        "link": "http://arxiv.org/abs/2507.08261v1",
        "abstract": "Batch normalization (BN) is a ubiquitous operation in deep neural networks\nused primarily to achieve stability and regularization during network training.\nBN involves feature map centering and scaling using sample means and variances,\nrespectively. Since these statistics are being estimated across the feature\nmaps within a batch, this problem is ideally suited for the application of\nStein's shrinkage estimation, which leads to a better, in the\nmean-squared-error sense, estimate of the mean and variance of the batch. In\nthis paper, we prove that the Stein shrinkage estimator for the mean and\nvariance dominates over the sample mean and variance estimators in the presence\nof adversarial attacks when modeling these attacks using sub-Gaussian\ndistributions. This facilitates and justifies the application of Stein\nshrinkage to estimate the mean and variance parameters in BN and use it in\nimage classification (segmentation) tasks with and without adversarial attacks.\nWe present SOTA performance results using this Stein corrected batch norm in a\nstandard ResNet architecture applied to the task of image classification using\nCIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using\nHRNet on Cityscape data with and without adversarial attacks."
    },
    {
        "date": "2025-07",
        "title": "EvA: Evolutionary Attacks on Graphs",
        "author": "Mohammad Sadegh Akhondzadeh, Soroush H. Zargarbashi, Jimin Cao, and Aleksandar Bojchevski",
        "link": "http://arxiv.org/abs/2507.08212v1",
        "abstract": "Even a slight perturbation in the graph structure can cause a significant\ndrop in the accuracy of graph neural networks (GNNs). Most existing attacks\nleverage gradient information to perturb edges. This relaxes the attack's\noptimization problem from a discrete to a continuous space, resulting in\nsolutions far from optimal. It also restricts the adaptability of the attack to\nnon-differentiable objectives. Instead, we introduce a few simple yet effective\nenhancements of an evolutionary-based algorithm to solve the discrete\noptimization problem directly. Our Evolutionary Attack (EvA) works with any\nblack-box model and objective, eliminating the need for a differentiable proxy\nloss. This allows us to design two novel attacks that reduce the effectiveness\nof robustness certificates and break conformal sets. The memory complexity of\nour attack is linear in the attack budget. Among our experiments, EvA shows\n$\\sim$11\\% additional drop in accuracy on average compared to the best previous\nattack, revealing significant untapped potential in designing attacks."
    },
    {
        "date": "2025-07",
        "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking",
        "author": "Zhengye Han, and Quanyan Zhu",
        "link": "http://arxiv.org/abs/2507.08207v1",
        "abstract": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking."
    },
    {
        "date": "2025-07",
        "title": "HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation",
        "author": "Ken C. L. Wong, Hongzhi Wang, and Tanveer Syeda-Mahmood",
        "link": "http://arxiv.org/abs/2507.08205v1",
        "abstract": "In medical image segmentation, convolutional neural networks (CNNs) and\ntransformers are dominant. For CNNs, given the local receptive fields of\nconvolutional layers, long-range spatial correlations are captured through\nconsecutive convolutions and pooling. However, as the computational cost and\nmemory footprint can be prohibitively large, 3D models can only afford fewer\nlayers than 2D models with reduced receptive fields and abstract levels. For\ntransformers, although long-range correlations can be captured by multi-head\nattention, its quadratic complexity with respect to input size is\ncomputationally demanding. Therefore, either model may require input size\nreduction to allow more filters and layers for better segmentation.\nNevertheless, given their discrete nature, models trained with patch-wise\ntraining or image downsampling may produce suboptimal results when applied on\nhigher resolutions. To address this issue, here we propose the\nresolution-robust HNOSeg-XS architecture. We model image segmentation by\nlearnable partial differential equations through the Fourier neural operator\nwhich has the zero-shot super-resolution property. By replacing the Fourier\ntransform by the Hartley transform and reformulating the problem in the\nfrequency domain, we created the HNOSeg-XS model, which is resolution robust,\nfast, memory efficient, and extremely parameter efficient. When tested on the\nBraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS\nshowed its superior resolution robustness with fewer than 34.7k model\nparameters. It also achieved the overall best inference time (< 0.24 s) and\nmemory efficiency (< 1.8 GiB) compared to the tested CNN and transformer\nmodels."
    },
    {
        "date": "2025-07",
        "title": "Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks",
        "author": "Sounak Bhowmik, Travis S. Humble, and Himanshu Thapliyal",
        "link": "http://arxiv.org/abs/2507.08202v1",
        "abstract": "Quantum neural networks (QNN) hold immense potential for the future of\nquantum machine learning (QML). However, QNN security and robustness remain\nlargely unexplored. In this work, we proposed novel Trojan attacks based on the\nquantum computing properties in a QNN-based binary classifier. Our proposed\nQuantum Properties Trojans (QuPTs) are based on the unitary property of quantum\ngates to insert noise and Hadamard gates to enable superposition to develop\nTrojans and attack QNNs. We showed that the proposed QuPTs are significantly\nstealthier and heavily impact the quantum circuits' performance, specifically\nQNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the\ncompromised QNN under the experimental setup. To the best of our knowledge,\nthis is the first work on the Trojan attack on a fully quantum neural network\nindependent of any hybrid classical-quantum architecture."
    },
    {
        "date": "2025-07",
        "title": "Robust Semi-Supervised CT Radiomics for Lung Cancer Prognosis: Cost-Effective Learning with Limited Labels and SHAP Interpretation",
        "author": "Mohammad R. Salmanpour, Amir Hossein Pouria, Sonia Falahati, Shahram Taeb, Somayeh Sadat Mehrnia, Mehdi Maghsudi, Ali Fathi Jouzdani, Mehrdad Oveisi, Ilker Hacihaliloglu, and Arman Rahmim",
        "link": "http://arxiv.org/abs/2507.08189v2",
        "abstract": "Background: CT imaging is vital for lung cancer management, offering detailed\nvisualization for AI-based prognosis. However, supervised learning SL models\nrequire large labeled datasets, limiting their real-world application in\nsettings with scarce annotations.\n  Methods: We analyzed CT scans from 977 patients across 12 datasets extracting\n1218 radiomics features using Laplacian of Gaussian and wavelet filters via\nPyRadiomics Dimensionality reduction was applied with 56 feature selection and\nextraction algorithms and 27 classifiers were benchmarked A semi supervised\nlearning SSL framework with pseudo labeling utilized 478 unlabeled and 499\nlabeled cases Model sensitivity was tested in three scenarios varying labeled\ndata in SL increasing unlabeled data in SSL and scaling both from 10 percent to\n100 percent SHAP analysis was used to interpret predictions Cross validation\nand external testing in two cohorts were performed.\n  Results: SSL outperformed SL, improving overall survival prediction by up to\n17 percent. The top SSL model, Random Forest plus XGBoost classifier, achieved\n0.90 accuracy in cross-validation and 0.88 externally. SHAP analysis revealed\nenhanced feature discriminability in both SSL and SL, especially for Class 1\nsurvival greater than 4 years. SSL showed strong performance with only 10\npercent labeled data, with more stable results compared to SL and lower\nvariance across external testing, highlighting SSL's robustness and cost\neffectiveness.\n  Conclusion: We introduced a cost-effective, stable, and interpretable SSL\nframework for CT-based survival prediction in lung cancer, improving\nperformance, generalizability, and clinical readiness by integrating SHAP\nexplainability and leveraging unlabeled data."
    },
    {
        "date": "2025-07",
        "title": "GPUHammer: Rowhammer Attacks on GPU Memories are Practical",
        "author": "Chris S. Lin, Joyce Qu, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2507.08166v1",
        "abstract": "Rowhammer is a read disturbance vulnerability in modern DRAM that causes\nbit-flips, compromising security and reliability. While extensively studied on\nIntel and AMD CPUs with DDR and LPDDR memories, its impact on GPUs using GDDR\nmemories, critical for emerging machine learning applications, remains\nunexplored. Rowhammer attacks on GPUs face unique challenges: (1) proprietary\nmapping of physical memory to GDDR banks and rows, (2) high memory latency and\nfaster refresh rates that hinder effective hammering, and (3) proprietary\nmitigations in GDDR memories, difficult to reverse-engineer without FPGA-based\ntest platforms. We introduce GPUHammer, the first Rowhammer attack on NVIDIA\nGPUs with GDDR6 DRAM. GPUHammer proposes novel techniques to reverse-engineer\nGDDR DRAM row mappings, and employs GPU-specific memory access optimizations to\namplify hammering intensity and bypass mitigations. Thus, we demonstrate the\nfirst successful Rowhammer attack on a discrete GPU, injecting up to 8\nbit-flips across 4 DRAM banks on an NVIDIA A6000 with GDDR6 memory. We also\nshow how an attacker can use these to tamper with ML models, causing\nsignificant accuracy drops (up to 80%)."
    },
    {
        "date": "2025-07",
        "title": "Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion",
        "author": "Frederick Shpilevskiy, Saiyue Lyu, Krishnamurthy Dj Dvijotham, Mathias L\u00e9cuyer, and Pierre-Andr\u00e9 No\u00ebl",
        "link": "http://arxiv.org/abs/2507.08163v1",
        "abstract": "We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the\npredictions of a vision model against adversarial examples, while adapting to\nthe input. Our key insight is to reinterpret a guided denoising diffusion model\nas a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms\nrefining a pure noise sample into an image. We show that these adaptive\nmechanisms can be composed through a GDP privacy filter to analyze the\nend-to-end robustness of the guided denoising process, yielding a provable\ncertification that extends the adaptive randomized smoothing analysis. We\ndemonstrate that our design, under a specific guiding strategy, can improve\nboth certified accuracy and standard accuracy on ImageNet for an $\\ell_2$\nthreat model."
    },
    {
        "date": "2025-07",
        "title": "Beyond the Worst Case: Extending Differential Privacy Guarantees to Realistic Adversaries",
        "author": "Marika Swanberg, Meenatchi Sundaram Muthu Selva Annamalai, Jamie Hayes, Borja Balle, and Adam Smith",
        "link": "http://arxiv.org/abs/2507.08158v1",
        "abstract": "Differential Privacy (DP) is a family of definitions that bound the\nworst-case privacy leakage of a mechanism. One important feature of the\nworst-case DP guarantee is it naturally implies protections against adversaries\nwith less prior information, more sophisticated attack goals, and complex\nmeasures of a successful attack. However, the analytical tradeoffs between the\nadversarial model and the privacy protections conferred by DP are not well\nunderstood thus far. To that end, this work sheds light on what the worst-case\nguarantee of DP implies about the success of attackers that are more\nrepresentative of real-world privacy risks.\n  In this paper, we present a single flexible framework that generalizes and\nextends the patchwork of bounds on DP mechanisms found in prior work. Our\nframework allows us to compute high-probability guarantees for DP mechanisms on\na large family of natural attack settings that previous bounds do not capture.\nOne class of such settings is the approximate reconstruction of multiple\nindividuals' data, such as inferring nearly entire columns of a tabular data\nset from noisy marginals and extracting sensitive information from DP-trained\nlanguage models.\n  We conduct two empirical case studies to illustrate the versatility of our\nbounds and compare them to the success of state-of-the-art attacks.\nSpecifically, we study attacks that extract non-uniform PII from a DP-trained\nlanguage model, as well as multi-column reconstruction attacks where the\nadversary has access to some columns in the clear and attempts to reconstruct\nthe remaining columns for each person's record. We find that the absolute\nprivacy risk of attacking non-uniform data is highly dependent on the\nadversary's prior probability of success. Our high probability bounds give us a\nnuanced understanding of the privacy leakage of DP mechanisms in a variety of\npreviously understudied attack settings."
    },
    {
        "date": "2025-07",
        "title": "Low Resource Reconstruction Attacks Through Benign Prompts",
        "author": "Sol Yarkoni, and Roi Livni",
        "link": "http://arxiv.org/abs/2507.07947v2",
        "abstract": "The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts."
    },
    {
        "date": "2025-07",
        "title": "KeyDroid: A Large-Scale Analysis of Secure Key Storage in Android Apps",
        "author": "Jenny Blessing, Ross J. Anderson, and Alastair R. Beresford",
        "link": "http://arxiv.org/abs/2507.07927v1",
        "abstract": "Most contemporary mobile devices offer hardware-backed storage for\ncryptographic keys, user data, and other sensitive credentials. Such hardware\nprotects credentials from extraction by an adversary who has compromised the\nmain operating system, such as a malicious third-party app. Since 2011, Android\napp developers can access trusted hardware via the Android Keystore API. In\nthis work, we conduct the first comprehensive survey of hardware-backed key\nstorage in Android devices. We analyze 490 119 Android apps, collecting data on\nhow trusted hardware is used by app developers (if used at all) and\ncross-referencing our findings with sensitive user data collected by each app,\nas self-reported by developers via the Play Store's data safety labels.\n  We find that despite industry-wide initiatives to encourage adoption, 56.3%\nof apps self-reporting as processing sensitive user data do not use Android's\ntrusted hardware capabilities at all, while just 5.03% of apps collecting some\nform of sensitive data use the strongest form of trusted hardware, a secure\nelement distinct from the main processor. To better understand the potential\ndownsides of using secure hardware, we conduct the first empirical analysis of\ntrusted hardware performance in mobile devices, measuring the runtime of common\ncryptographic operations across both software- and hardware-backed keystores.\nWe find that while hardware-backed key storage using a coprocessor is viable\nfor most common cryptographic operations, secure elements capable of preventing\nmore advanced attacks make performance infeasible for symmetric encryption with\nnon-negligible payloads and any kind of asymmetric encryption."
    },
    {
        "date": "2025-07",
        "title": "ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused Networks and a Robust Validation Framework",
        "author": "Abrar Faiyaz, Nhat Hoang, Giovanni Schifitto, and Md Nasir Uddin",
        "link": "http://arxiv.org/abs/2507.07920v1",
        "abstract": "Cerebrovascular pathology significantly contributes to cognitive decline and\nneurological disorders, underscoring the need for advanced tools to assess\nvascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance\nAngiography (3D TOF MRA) is widely used to visualize cerebral vasculature,\nhowever, clinical evaluations generally focus on major arterial abnormalities,\noverlooking quantitative metrics critical for understanding subtle vascular\nchanges. Existing methods for extracting structural, geometrical and\nmorphological arterial features from MRA - whether manual or automated - face\nchallenges including user-dependent variability, steep learning curves, and\nlack of standardized quantitative validations. We propose a novel\nsemi-supervised artery evaluation framework, named ArteryX, a MATLAB-based\ntoolbox that quantifies vascular features with high accuracy and efficiency,\nachieving processing times ~10-15 minutes per subject at 0.5 mm resolution with\nminimal user intervention. ArteryX employs a vessel-fused network based\nlandmarking approach to reliably track and manage tracings, effectively\naddressing the issue of dangling/disconnected vessels. Validation on human\nsubjects with cerebral small vessel disease demonstrated its improved\nsensitivity to subtle vascular changes and better performance than an existing\nsemi-automated method. Importantly, the ArteryX toolbox enables quantitative\nfeature validation by integrating an in-vivo like artery simulation framework\nutilizing vessel-fused graph nodes and predefined ground-truth features for\nspecific artery types. Thus, the ArteryX framework holds promise for\nbenchmarking feature extraction toolboxes and for seamless integration into\nclinical workflows, enabling early detection of cerebrovascular pathology and\nstandardized comparisons across patient cohorts to advance understanding of\nvascular contributions to brain health."
    },
    {
        "date": "2025-07",
        "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations",
        "author": "Federico Maria Cau, Giuseppe Desolda, Francesco Greco, Lucio Davide Spano, and Luca Vigan\u00f2",
        "link": "http://arxiv.org/abs/2507.07916v1",
        "abstract": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues."
    },
    {
        "date": "2025-07",
        "title": "Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key Watermarking",
        "author": "Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, and Nils Lukas",
        "link": "http://arxiv.org/abs/2507.07871v1",
        "abstract": "Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games."
    },
    {
        "date": "2025-07",
        "title": "Synergistic Prompting for Robust Visual Recognition with Missing Modalities",
        "author": "Zhihui Zhang, Luanyuan Dai, Qika Lin, Yunfeng Diao, Guangyin Jin, Yufei Guo, Jing Zhang, and Xiaoshuai Hao",
        "link": "http://arxiv.org/abs/2507.07802v2",
        "abstract": "Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability."
    },
    {
        "date": "2025-07",
        "title": "Robust and Generalizable Heart Rate Estimation via Deep Learning for Remote Photoplethysmography in Complex Scenarios",
        "author": "Kang Cen, Chang-Hong Fu, and Hong Hong",
        "link": "http://arxiv.org/abs/2507.07795v1",
        "abstract": "Non-contact remote photoplethysmography (rPPG) technology enables heart rate\nmeasurement from facial videos. However, existing network models still face\nchallenges in accu racy, robustness, and generalization capability under\ncomplex scenarios. This paper proposes an end-to-end rPPG extraction network\nthat employs 3D convolutional neural networks to reconstruct accurate rPPG\nsignals from raw facial videos. We introduce a differential frame fusion module\nthat integrates differential frames with original frames, enabling frame-level\nrepresentations to capture blood volume pulse (BVP) variations. Additionally,\nwe incorporate Temporal Shift Module (TSM) with self-attention mechanisms,\nwhich effectively enhance rPPG features with minimal computational overhead.\nFurthermore, we propose a novel dynamic hybrid loss function that provides\nstronger supervision for the network, effectively mitigating over fitting.\nComprehensive experiments were conducted on not only the PURE and UBFC-rPPG\ndatasets but also the challenging MMPD dataset under complex scenarios,\ninvolving both intra dataset and cross-dataset evaluations, which demonstrate\nthe superior robustness and generalization capability of our network.\nSpecifically, after training on PURE, our model achieved a mean absolute error\n(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models."
    },
    {
        "date": "2025-07",
        "title": "Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models",
        "author": "Hermann Klein, Max Heinz Herkersdorf, and Oliver Nelles",
        "link": "http://arxiv.org/abs/2507.07792v1",
        "abstract": "The state space dynamics representation is the most general approach for\nnonlinear systems and often chosen for system identification. During training,\nthe state trajectory can deform significantly leading to poor data coverage of\nthe state space. This can cause significant issues for space-oriented training\nalgorithms which e.g. rely on grid structures, tree partitioning, or similar.\nBesides hindering training, significant state trajectory deformations also\ndeteriorate interpretability and robustness properties. This paper proposes a\nnew type of space-filling regularization that ensures a favorable data\ndistribution in state space via introducing a data-distribution-based penalty.\nThis method is demonstrated in local model network architectures where good\ninterpretability is a major concern. The proposed approach integrates ideas\nfrom modeling and design of experiments for state space structures. This is why\nwe present two regularization techniques for the data point distributions of\nthe state trajectories for local affine state space models. Beyond that, we\ndemonstrate the results on a widely known system identification benchmark."
    },
    {
        "date": "2025-07",
        "title": "SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial Examples",
        "author": "Dren Fazlija, Monty-Maximilian Z\u00fchlke, Johanna Schrader, Arkadij Orlov, Clara Stein, Iyiola E. Olatunji, and Daniel Kudenko",
        "link": "http://arxiv.org/abs/2507.07776v2",
        "abstract": "Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark."
    },
    {
        "date": "2025-07",
        "title": "Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image Sensors",
        "author": "Youqian Zhang, Xinyu Ji, Zhihao Wang, and Qinhong Jiang",
        "link": "http://arxiv.org/abs/2507.07773v1",
        "abstract": "Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems."
    },
    {
        "date": "2025-07",
        "title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training",
        "author": "Tejaswini Medi, Steffen Jung, and Margret Keuper",
        "link": "http://arxiv.org/abs/2507.07768v1",
        "abstract": "Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense."
    },
    {
        "date": "2025-07",
        "title": "One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models",
        "author": "Jiale Zhao, Xinyang Jiang, Junyao Gao, Yuhao Xue, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2507.07709v1",
        "abstract": "Unified vision-language models(VLMs) have recently shown remarkable progress,\nenabling a single model to flexibly address diverse tasks through different\ninstructions within a shared computational architecture. This instruction-based\ncontrol mechanism creates unique security challenges, as adversarial inputs\nmust remain effective across multiple task instructions that may be\nunpredictably applied to process the same malicious content. In this paper, we\nintroduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with\nGPT-4-assisted annotations for systematically evaluating cross-task adversarial\nattacks on unified VLMs. CrossVLAD centers on the object-change\nobjective-consistently manipulating a target object's classification across\nfour downstream tasks-and proposes a novel success rate metric that measures\nsimultaneous misclassification across all tasks, providing a rigorous\nevaluation of adversarial transferability. To tackle this challenge, we present\nCRAFT (Cross-task Region-based Attack Framework with Token-alignment), an\nefficient region-centric attack method. Extensive experiments on Florence-2 and\nother popular unified VLMs demonstrate that our method outperforms existing\napproaches in both overall cross-task attack performance and targeted\nobject-change success rates, highlighting its effectiveness in adversarially\ninfluencing unified VLMs across diverse tasks."
    },
    {
        "date": "2025-07",
        "title": "May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks",
        "author": "Nishit V. Pandya, Andrey Labunets, Sicun Gao, and Earlence Fernandes",
        "link": "http://arxiv.org/abs/2507.07417v1",
        "abstract": "A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks"
    },
    {
        "date": "2025-07",
        "title": "Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors",
        "author": "Chunzhuo Wang, Hans Hallez, and Bart Vanrumste",
        "link": "http://arxiv.org/abs/2507.07261v1",
        "abstract": "Automated food intake gesture detection plays a vital role in dietary\nmonitoring, enabling objective and continuous tracking of eating behaviors to\nsupport better health outcomes. Wrist-worn inertial measurement units (IMUs)\nhave been widely used for this task with promising results. More recently,\ncontactless radar sensors have also shown potential. This study explores\nwhether combining wearable and contactless sensing modalities through\nmultimodal learning can further improve detection performance. We also address\na major challenge in multimodal learning: reduced robustness when one modality\nis missing. To this end, we propose a robust multimodal temporal convolutional\nnetwork with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and\nradar data, enhance gesture detection, and maintain performance under missing\nmodality conditions. A new dataset comprising 52 meal sessions (3,050 eating\ngestures and 797 drinking gestures) from 52 participants is developed and made\npublicly available. Experimental results show that the proposed framework\nimproves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU\nmodels, respectively. Under missing modality scenarios, the framework still\nachieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This\nis the first study to demonstrate a robust multimodal learning framework that\neffectively fuses IMU and radar data for food intake gesture detection."
    },
    {
        "date": "2025-07",
        "title": "Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning",
        "author": "Giulio Rossolini, Fabio Brau, Alessandro Biondi, Battista Biggio, and Giorgio Buttazzo",
        "link": "http://arxiv.org/abs/2507.07259v1",
        "abstract": "As machine learning models become increasingly deployed across the edge of\ninternet of things environments, a partitioned deep learning paradigm in which\nmodels are split across multiple computational nodes introduces a new dimension\nof security risk. Unlike traditional inference setups, these distributed\npipelines span the model computation across heterogeneous nodes and\ncommunication layers, thereby exposing a broader attack surface to potential\nadversaries. Building on these motivations, this work explores a previously\noverlooked vulnerability: even when both the edge and cloud components of the\nmodel are inaccessible (i.e., black-box), an adversary who intercepts the\nintermediate features transmitted between them can still pose a serious threat.\nWe demonstrate that, under these mild and realistic assumptions, an attacker\ncan craft highly transferable proxy models, making the entire deep learning\nsystem significantly more vulnerable to evasion attacks. In particular, the\nintercepted features can be effectively analyzed and leveraged to distill\nsurrogate models capable of crafting highly transferable adversarial examples\nagainst the target model. To this end, we propose an exploitation strategy\nspecifically designed for distributed settings, which involves reconstructing\nthe original tensor shape from vectorized transmitted features using simple\nstatistical analysis, and adapting surrogate architectures accordingly to\nenable effective feature distillation. A comprehensive and systematic\nexperimental evaluation has been conducted to demonstrate that surrogate models\ntrained with the proposed strategy, i.e., leveraging intermediate features,\ntremendously improve the transferability of adversarial attacks. These findings\nunderscore the urgent need to account for intermediate feature leakage in the\ndesign of secure distributed deep learning systems."
    },
    {
        "date": "2025-07",
        "title": "Automated Attack Testflow Extraction from Cyber Threat Report using BERT for Contextual Analysis",
        "author": "Faissal Ahmadou, Sepehr Ghaffarzadegan, Boubakr Nour, Makan Pourzandi, Mourad Debbabi, and Chadi Assi",
        "link": "http://arxiv.org/abs/2507.07244v1",
        "abstract": "In the ever-evolving landscape of cybersecurity, the rapid identification and\nmitigation of Advanced Persistent Threats (APTs) is crucial. Security\npractitioners rely on detailed threat reports to understand the tactics,\ntechniques, and procedures (TTPs) employed by attackers. However, manually\nextracting attack testflows from these reports requires elusive knowledge and\nis time-consuming and prone to errors. This paper proposes FLOWGUARDIAN, a\nnovel solution leveraging language models (i.e., BERT) and Natural Language\nProcessing (NLP) techniques to automate the extraction of attack testflows from\nunstructured threat reports. FLOWGUARDIAN systematically analyzes and\ncontextualizes security events, reconstructs attack sequences, and then\ngenerates comprehensive testflows. This automated approach not only saves time\nand reduces human error but also ensures comprehensive coverage and robustness\nin cybersecurity testing. Empirical validation using public threat reports\ndemonstrates FLOWGUARDIAN's accuracy and efficiency, significantly enhancing\nthe capabilities of security teams in proactive threat hunting and incident\nresponse."
    },
    {
        "date": "2025-07",
        "title": "Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture",
        "author": "Erfan Hamdi, and Emma Lejeune",
        "link": "http://arxiv.org/abs/2507.07237v1",
        "abstract": "Data driven approaches have the potential to make modeling complex, nonlinear\nphysical phenomena significantly more computationally tractable. For example,\ncomputational modeling of fracture is a core challenge where machine learning\ntechniques have the potential to provide a much needed speedup that would\nenable progress in areas such as mutli-scale modeling and uncertainty\nquantification. Currently, phase field modeling (PFM) of fracture is one such\napproach that offers a convenient variational formulation to model crack\nnucleation, branching and propagation. To date, machine learning techniques\nhave shown promise in approximating PFM simulations. However, most studies rely\non overly simple benchmarks that do not reflect the true complexity of the\nfracture processes where PFM excels as a method. To address this gap, we\nintroduce a challenging dataset based on PFM simulations designed to benchmark\nand advance ML methods for fracture modeling. This dataset includes three\nenergy decomposition methods, two boundary conditions, and 1,000 random initial\ncrack configurations for a total of 6,000 simulations. Each sample contains 100\ntime steps capturing the temporal evolution of the crack field. Alongside this\ndataset, we also implement and evaluate Physics Informed Neural Networks\n(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and\nexplore the impact of ensembling strategies on prediction accuracy. With this\ncombination of our dataset and baseline models drawn from the literature we aim\nto provide a standardized and challenging benchmark for evaluating machine\nlearning approaches to solid mechanics. Our results highlight both the promise\nand limitations of popular current models, and demonstrate the utility of this\ndataset as a testbed for advancing machine learning in fracture mechanics\nresearch."
    },
    {
        "date": "2025-07",
        "title": "Towards Evaluating Robustness of Prompt Adherence in Text to Image Models",
        "author": "Sujith Vemishetty, Advitiya Arora, and Anupama Sharma",
        "link": "http://arxiv.org/abs/2507.08039v1",
        "abstract": "The advancements in the domain of LLMs in recent years have surprised many,\nshowcasing their remarkable capabilities and diverse applications. Their\npotential applications in various real-world scenarios have led to significant\nresearch on their reliability and effectiveness. On the other hand, multimodal\nLLMs and Text-to-Image models have only recently gained prominence, especially\nwhen compared to text-only LLMs. Their reliability remains constrained due to\ninsufficient research on assessing their performance and robustness. This paper\naims to establish a comprehensive evaluation framework for Text-to-Image\nmodels, concentrating particularly on their adherence to prompts. We created a\nnovel dataset that aimed to assess the robustness of these models in generating\nimages that conform to the specified factors of variation in the input text\nprompts. Our evaluation studies present findings on three variants of Stable\nDiffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and\nStable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro\n1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions\ngenerated by the gpt-4o model for our ground-truth images, which are then used\nto generate artificial images by passing these descriptions to the\nText-to-Image models. We then pass these generated images again through gpt-4o\nusing the same system prompt and compare the variation between the two\ndescriptions. Our results reveal that these models struggle to create simple\nbinary images with only two factors of variation: a simple geometric shape and\nits location. We also show, using pre-trained VAEs on our dataset, that they\nfail to generate images that follow our input dataset distribution."
    },
    {
        "date": "2025-07",
        "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks",
        "author": "Deemah H. Tashman, Soumaya Cherkaoui, and Walaa Hamouda",
        "link": "http://arxiv.org/abs/2507.06997v1",
        "abstract": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity."
    },
    {
        "date": "2025-07",
        "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting",
        "author": "Linyun Gao, Qiang Wen, and Fumio Machida",
        "link": "http://arxiv.org/abs/2507.06907v1",
        "abstract": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions."
    },
    {
        "date": "2025-07",
        "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
        "author": "Ziang Ye, Yang Zhang, Wentao Shi, Xiaoyu You, Fuli Feng, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2507.06899v1",
        "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents."
    },
    {
        "date": "2025-07",
        "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis",
        "author": "Yifan Wang",
        "link": "http://arxiv.org/abs/2507.06890v1",
        "abstract": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids."
    },
    {
        "date": "2025-07",
        "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization",
        "author": "Subrat Kishore Dutta, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2507.06856v1",
        "abstract": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective."
    },
    {
        "date": "2025-07",
        "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover",
        "author": "Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, and Angelo Furfaro",
        "link": "http://arxiv.org/abs/2507.06850v3",
        "abstract": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
    },
    {
        "date": "2025-07",
        "title": "Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control",
        "author": "Ayoub Farkane, Mohamed Boutayeb, Mustapha Oudani, and Mounir Ghogho",
        "link": "http://arxiv.org/abs/2507.06817v1",
        "abstract": "Accurate knowledge of the state variables in a dynamical system is critical\nfor effective control, diagnosis, and supervision, especially when direct\nmeasurements of all states are infeasible. This paper presents a novel approach\nto designing software sensors for nonlinear dynamical systems expressed in\ntheir most general form. Unlike traditional model-based observers that rely on\nexplicit transformations or linearization, the proposed framework integrates\nneural networks with adaptive Sliding Mode Control (SMC) to design a robust\nstate observer under a less restrictive set of conditions. The learning process\nis driven by available sensor measurements, which are used to correct the\nobserver's state estimate. The training methodology leverages the system's\ngoverning equations as a physics-based constraint, enabling observer synthesis\nwithout access to ground-truth state trajectories. By employing a time-varying\ngain matrix dynamically adjusted by the neural network, the observer adapts in\nreal-time to system changes, ensuring robustness against noise, external\ndisturbances, and variations in system dynamics. Furthermore, we provide\nsufficient conditions to guarantee estimation error convergence, establishing a\ntheoretical foundation for the observer's reliability. The methodology's\neffectiveness is validated through simulations on challenging examples,\nincluding systems with non-differentiable dynamics and varying observability\nconditions. These examples, which are often problematic for conventional\ntechniques, serve to demonstrate the robustness and broad applicability of our\napproach. The results show rapid convergence and high accuracy, underscoring\nthe method's potential for addressing complex state estimation challenges in\nreal-world applications."
    },
    {
        "date": "2025-07",
        "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation",
        "author": "Tianzhe Zhao, Jiaoyan Chen, Yanchi Ru, Haiping Zhu, Nan Hu, Jun Liu, and Qika Lin",
        "link": "http://arxiv.org/abs/2507.08862v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge."
    },
    {
        "date": "2025-07",
        "title": "Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric",
        "author": "Enda D. V. Bigarella",
        "link": "http://arxiv.org/abs/2507.06765v1",
        "abstract": "This document proposes a parametric activation function (ac.f.) aimed at\nimproving multidimensional nonlinear data regression. It is a established\nknowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.\nThis work shows that smoothness and gradient properties of the ac.f. further\nimpact the performance of large neural networks in terms of overfitting and\nsensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as\nELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and\nLeaky-RELU further impart discontinuity in the trained model. Improved\nperformance is demonstrated with a smooth \"Leaky Exponential Linear Unit\", with\nnon-zero gradient that can be trained. A novel diffusion-loss metric is also\nproposed to gauge the performance of the trained models in terms of\noverfitting."
    },
    {
        "date": "2025-07",
        "title": "Robust Multimodal Large Language Models Against Modality Conflict",
        "author": "Zongmeng Zhang, Wengang Zhou, Jie Zhao, and Houqiang Li",
        "link": "http://arxiv.org/abs/2507.07151v1",
        "abstract": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs."
    },
    {
        "date": "2025-07",
        "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies",
        "author": "Yuhan Liu, Xinyu Zhang, Haonan Chang, and Abdeslam Boularias",
        "link": "http://arxiv.org/abs/2507.06519v1",
        "abstract": "This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where\na robot must repeatedly perform high-precision insertions, such as screwing a\nnut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving\nmillimeter-level accuracy and maintaining consistent performance over multiple\nrepetitions, particularly when factors like nut rotation and friction introduce\nadditional complexity. We propose a sim-to-real framework that integrates a\nreinforcement learning-based insertion policy with a failure forecasting\nmodule. By representing the wrench's pose in the nut's coordinate frame rather\nthan the robot's frame, our approach significantly enhances sim-to-real\ntransferability. The insertion policy, trained in simulation, leverages\nreal-time 6D pose tracking to execute precise alignment, insertion, and\nrotation maneuvers. Simultaneously, a neural network predicts potential\nexecution failures, triggering a simple recovery mechanism that lifts the\nwrench and retries the insertion. Extensive experiments in both simulated and\nreal-world environments demonstrate that our method not only achieves a high\none-time success rate but also robustly maintains performance over long-horizon\nrepetitive tasks."
    },
    {
        "date": "2025-07",
        "title": "Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning",
        "author": "Renyang Liu, Guanlin Li, Tianwei Zhang, and See-Kiong Ng",
        "link": "http://arxiv.org/abs/2507.07139v1",
        "abstract": "Recent advances in image generation models (IGMs), particularly\ndiffusion-based architectures such as Stable Diffusion (SD), have markedly\nenhanced the quality and diversity of AI-generated visual content. However,\ntheir generative capability has also raised significant ethical, legal, and\nsocietal concerns, including the potential to produce harmful, misleading, or\ncopyright-infringing content. To mitigate these concerns, machine unlearning\n(MU) emerges as a promising solution by selectively removing undesirable\nconcepts from pretrained models. Nevertheless, the robustness and effectiveness\nof existing unlearning techniques remain largely unexplored, particularly in\nthe presence of multi-modal adversarial inputs.\n  To bridge this gap, we propose Recall, a novel adversarial framework\nexplicitly designed to compromise the robustness of unlearned IGMs. Unlike\nexisting approaches that predominantly rely on adversarial text prompts, Recall\nexploits the intrinsic multi-modal conditioning capabilities of diffusion\nmodels by efficiently optimizing adversarial image prompts with guidance from a\nsingle semantically relevant reference image. Extensive experiments across ten\nstate-of-the-art unlearning methods and diverse tasks show that Recall\nconsistently outperforms existing baselines in terms of adversarial\neffectiveness, computational efficiency, and semantic fidelity with the\noriginal textual prompt. These findings reveal critical vulnerabilities in\ncurrent unlearning mechanisms and underscore the need for more robust solutions\nto ensure the safety and reliability of generative models. Code and data are\npublicly available at \\textcolor{blue}{https://github.com/ryliu68/RECALL}."
    },
    {
        "date": "2025-07",
        "title": "A Survey on Artificial Noise for Physical Layer Security: Opportunities, Technologies, Guidelines, Advances, and Trends",
        "author": "Hong Niu, Yue Xiao, Xia Lei, Jiangong Chen, Zhihan Xiao, Mao Li, and Chau Yuen",
        "link": "http://arxiv.org/abs/2507.06500v1",
        "abstract": "Due to the broadcast nature of wireless communications, physical-layer\nsecurity has attracted increasing concerns from both academia and industry.\nArtificial noise (AN), as one of the promising physical-layer security\ntechniques, is capable of utilizing the spatial degree-of-freedom of channels\nto effectively enhance the security of wireless communications. In contrast to\nother physicallayer security techniques, the key distinguishing feature of AN\nis to generate specific interfering signals according to channel\ncharacteristics, increasing the secrecy capacity by reducing the wiretap\nchannel capacity without affecting the legitimate channel capacity. Hence, this\npaper provides the latest survey of AN, including its evolution, modeling,\nbackgrounds, applications, and future trends. Initially, we introduce the\ndevelopment, fundamentals, and backgrounds of AN. Subsequently, we highlight a\ncomprehensive survey of the current state of research on various AN-empowered\nscenarios and AN-combined technologies. Finally, we discuss some technical\nchallenges to tackle for AN-aided wireless security in the future."
    },
    {
        "date": "2025-07",
        "title": "TELSAFE: Security Gap Quantitative Risk Assessment Framework",
        "author": "Sarah Ali Siddiqui, Chandra Thapa, Derui Wang, Rayne Holland, Wei Shao, Seyit Camtepe, Hajime Suzuki, and Rajiv Shah",
        "link": "http://arxiv.org/abs/2507.06497v1",
        "abstract": "Gaps between established security standards and their practical\nimplementation have the potential to introduce vulnerabilities, possibly\nexposing them to security risks. To effectively address and mitigate these\nsecurity and compliance challenges, security risk management strategies are\nessential. However, it must adhere to well-established strategies and industry\nstandards to ensure consistency, reliability, and compatibility both within and\nacross organizations. In this paper, we introduce a new hybrid risk assessment\nframework called TELSAFE, which employs probabilistic modeling for quantitative\nrisk assessment and eliminates the influence of expert opinion bias. The\nframework encompasses both qualitative and quantitative assessment phases,\nfacilitating effective risk management strategies tailored to the unique\nrequirements of organizations. A specific use case utilizing Common\nVulnerabilities and Exposures (CVE)-related data demonstrates the framework's\napplicability and implementation in real-world scenarios, such as in the\ntelecommunications industry."
    },
    {
        "date": "2025-07",
        "title": "HEMA: A Hands-on Exploration Platform for MEMS Sensor Attacks",
        "author": "Bhagawat Baanav Yedla Ravi, Md Rafiul Kabir, and Sandip Ray",
        "link": "http://arxiv.org/abs/2507.06439v1",
        "abstract": "Automotive safety and security are paramount in the rapidly advancing\nlandscape of vehicular technology. Building safe and secure vehicles demands a\nprofound understanding of automotive systems, particularly in safety and\nsecurity. Traditional learning approaches, such as reading materials or\nobserving demonstrations, often fail to provide the practical, hands-on\nexperience essential for developing this expertise. For novice users, gaining\naccess to automotive-grade systems and mastering their associated hardware and\nsoftware can be challenging and overwhelming. In this paper, we present a\nnovel, affordable, and flexible exploration platform, \\hema, that enables users\nto gain practical, hands-on insights into the security compromises of\nmicro-electromechanical systems (MEMS) sensors, a critical component in modern\nADAS systems. Furthermore, we discuss the unique challenges and design\nconsiderations involved in creating such a platform, emphasizing its role in\nenhancing the understanding of automotive safety and security. This framework\nserves as an invaluable resource for educators, researchers, and practitioners\nstriving to build expertise in the field."
    },
    {
        "date": "2025-07",
        "title": "Never Trust the Manufacturer, Never Trust the Client: A Novel Method for Streaming STL Files for Secure Additive manufacturing",
        "author": "Seyed Ali Ghazi Asgar, Narasimha Reddy, and Satish T. S. Bukkapatnam",
        "link": "http://arxiv.org/abs/2507.06421v2",
        "abstract": "While additive manufacturing has opened interesting avenues to reimagine\nmanufacturing as a service (MaaS) platform, transmission of design files from\nclient to manufacturer over networks opens up many cybersecurity challenges.\nSecuring client's intellectual property (IP) especially from cyber-attacks\nemerges as a major challenge. Earlier works introduced streaming, instead of\nsharing process plan (G-code) files, as a possible solution. However, executing\nclient's G-codes on manufacturer's machines exposes them to potential malicious\nG-codes. This paper proposes a viable approach when the client and manufacturer\ndo not trust each other and both the client and manufacturer want to preserve\ntheir IP of designs and manufacturing process respectively. The proposed\napproach is based on segmenting and streaming design (STL) files and employing\na novel machine-specific STL to G-code translator at the manufacturer's site in\nreal-time for printing. This approach secures design and manufacturing process\nIPs as demonstrated in a real-world implementation."
    },
    {
        "date": "2025-07",
        "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation",
        "author": "Habibur Rahaman, Atri Chatterjee, and Swarup Bhunia",
        "link": "http://arxiv.org/abs/2507.06380v1",
        "abstract": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications."
    },
    {
        "date": "2025-07",
        "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions",
        "author": "Fuyuan Zhang, Qichen Wang, and Jianjun Zhao",
        "link": "http://arxiv.org/abs/2507.06332v1",
        "abstract": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions."
    },
    {
        "date": "2025-07",
        "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms",
        "author": "Tarek Gasmi, Ramzi Guesmi, Ines Belhadj, and Jihene Bennaceur",
        "link": "http://arxiv.org/abs/2507.06323v1",
        "abstract": "Large Language Model (LLM) agents face security vulnerabilities spanning\nAI-specific and traditional software domains, yet current research addresses\nthese separately. This study bridges this gap through comparative evaluation of\nFunction Calling architecture and Model Context Protocol (MCP) deployment\nparadigms using a unified threat classification framework. We tested 3,250\nattack scenarios across seven language models, evaluating simple, composed, and\nchained attacks targeting both AI-specific threats (prompt injection) and\nsoftware vulnerabilities (JSON injection, denial-of-service). Function Calling\nshowed higher overall attack success rates (73.5% vs 62.59% for MCP), with\ngreater system-centric vulnerability while MCP exhibited increased LLM-centric\nexposure. Attack complexity dramatically amplified effectiveness, with chained\nattacks achieving 91-96% success rates. Counterintuitively, advanced reasoning\nmodels demonstrated higher exploitability despite better threat detection.\nResults demonstrate that architectural choices fundamentally reshape threat\nlandscapes. This work establishes methodological foundations for cross-domain\nLLM agent security assessment and provides evidence-based guidance for secure\ndeployment. Code and experimental materials are available at https: // github.\ncom/ theconsciouslab-ai/llm-agent-security."
    },
    {
        "date": "2025-07",
        "title": "Hedge Funds on a Swamp: Analyzing Patterns, Vulnerabilities, and Defense Measures in Blockchain Bridges [Experiment, Analysis & Benchmark]",
        "author": "Poupak Azad, Jiahua Xu, Yebo Feng, Preston Strowbridge, and Cuneyt Akcora",
        "link": "http://arxiv.org/abs/2507.06156v2",
        "abstract": "Blockchain bridges have become essential infrastructure for enabling\ninteroperability across different blockchain networks, with more than $24B\nmonthly bridge transaction volume. However, their growing adoption has been\naccompanied by a disproportionate rise in security breaches, making them the\nsingle largest source of financial loss in Web3. For cross-chain ecosystems to\nbe robust and sustainable, it is essential to understand and address these\nvulnerabilities. In this study, we present a comprehensive systematization of\nblockchain bridge design and security. We define three bridge security priors,\nformalize the architectural structure of 13 prominent bridges, and identify 23\nattack vectors grounded in real-world blockchain exploits. Using this\nfoundation, we evaluate 43 representative attack scenarios and introduce a\nlayered threat model that captures security failures across source chain,\noff-chain, and destination chain components.\n  Our analysis at the static code and transaction network levels reveals\nrecurring design flaws, particularly in access control, validator trust\nassumptions, and verification logic, and identifies key patterns in adversarial\nbehavior based on transaction-level traces. To support future development, we\npropose a decision framework for bridge architecture design, along with defense\nmechanisms such as layered validation and circuit breakers. This work provides\na data-driven foundation for evaluating bridge security and lays the groundwork\nfor standardizing resilient cross-chain infrastructure."
    },
    {
        "date": "2025-07",
        "title": "Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI",
        "author": "Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, and Bimal Viswanath",
        "link": "http://arxiv.org/abs/2507.06092v1",
        "abstract": "Machine learning-based supervised classifiers are widely used for security\ntasks, and their improvement has been largely focused on algorithmic\nadvancements. We argue that data challenges that negatively impact the\nperformance of these classifiers have received limited attention. We address\nthe following research question: Can developments in Generative AI (GenAI)\naddress these data challenges and improve classifier performance? We propose\naugmenting training datasets with synthetic data generated using GenAI\ntechniques to improve classifier generalization. We evaluate this approach\nacross 7 diverse security tasks using 6 state-of-the-art GenAI methods and\nintroduce a novel GenAI scheme called Nimai that enables highly controlled data\nsynthesis. We find that GenAI techniques can significantly improve the\nperformance of security classifiers, achieving improvements of up to 32.6% even\nin severely data-constrained settings (only ~180 training samples).\nFurthermore, we demonstrate that GenAI can facilitate rapid adaptation to\nconcept drift post-deployment, requiring minimal labeling in the adjustment\nprocess. Despite successes, our study finds that some GenAI schemes struggle to\ninitialize (train and produce data) on certain security tasks. We also identify\ncharacteristics of specific tasks, such as noisy labels, overlapping class\ndistributions, and sparse feature vectors, which hinder performance boost using\nGenAI. We believe that our study will drive the development of future GenAI\ntools designed for security tasks."
    },
    {
        "date": "2025-07",
        "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models",
        "author": "Chihan Huang, and Hao Tang",
        "link": "http://arxiv.org/abs/2507.06078v1",
        "abstract": "Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality. Furthermore,\nthe dynamic balance between denoising and adversarial perturbation enables\nScoreAdv to remain robust even under defensive measures."
    },
    {
        "date": "2025-07",
        "title": "CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations",
        "author": "Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, and Tieyun Qian",
        "link": "http://arxiv.org/abs/2507.06043v1",
        "abstract": "Security alignment enables the Large Language Model (LLM) to gain the\nprotection against malicious queries, but various jailbreak attack methods\nreveal the vulnerability of this security mechanism. Previous studies have\nisolated LLM jailbreak attacks and defenses. We analyze the security protection\nmechanism of the LLM, and propose a framework that combines attack and defense.\nOur method is based on the linearly separable property of LLM intermediate\nlayer embedding, as well as the essence of jailbreak attack, which aims to\nembed harmful problems and transfer them to the safe area. We utilize\ngenerative adversarial network (GAN) to learn the security judgment boundary\ninside the LLM to achieve efficient jailbreak attack and defense. The\nexperimental results indicate that our method achieves an average jailbreak\nsuccess rate of 88.85\\% across three popular LLMs, while the defense success\nrate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%.\nThis not only validates the effectiveness of our approach but also sheds light\non the internal security mechanisms of LLMs, offering new insights for\nenhancing model security The code and data are available at\nhttps://github.com/NLPGM/CAVGAN."
    },
    {
        "date": "2025-07",
        "title": "Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation",
        "author": "Haroon Wahab, Hassan Ugail, and Lujain Jaleel",
        "link": "http://arxiv.org/abs/2507.05996v1",
        "abstract": "Machine learning-based Deepfake detection models have achieved impressive\nresults on benchmark datasets, yet their performance often deteriorates\nsignificantly when evaluated on out-of-distribution data. In this work, we\ninvestigate an ensemble-based approach for improving the generalization of\ndeepfake detection systems across diverse datasets. Building on a recent\nopen-source benchmark, we combine prediction probabilities from several\nstate-of-the-art asymmetric models proposed at top venues. Our experiments span\ntwo distinct out-of-domain datasets and demonstrate that no single model\nconsistently outperforms others across settings. In contrast, ensemble-based\npredictions provide more stable and reliable performance in all scenarios. Our\nresults suggest that asymmetric ensembling offers a robust and scalable\nsolution for real-world deepfake detection where prior knowledge of forgery\ntype or quality is often unavailable."
    },
    {
        "date": "2025-07",
        "title": "Robust Speech-Workload Estimation for Intelligent Human-Robot Systems",
        "author": "Julian Fortune, Julie A. Adams, and Jamison Heard",
        "link": "http://arxiv.org/abs/2507.05985v1",
        "abstract": "Demanding task environments (e.g., supervising a remotely piloted aircraft)\nrequire performing tasks quickly and accurately; however, periods of low and\nhigh operator workload can decrease task performance. Intelligent modulation of\nthe system's demands and interaction modality in response to changes in\noperator workload state may increase performance by avoiding undesirable\nworkload states. This system requires real-time estimation of each workload\ncomponent (i.e., cognitive, physical, visual, speech, and auditory) to adapt\nthe correct modality. Existing workload systems estimate multiple workload\ncomponents post-hoc, but few estimate speech workload, or function in\nreal-time. An algorithm to estimate speech workload and mitigate undesirable\nworkload states in real-time is presented. An analysis of the algorithm's\naccuracy is presented, along with the results demonstrating the algorithm's\ngeneralizability across individuals and human-machine teaming paradigms.\nReal-time speech workload estimation is a crucial element towards developing\nadaptive human-machine systems."
    },
    {
        "date": "2025-07",
        "title": "Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation",
        "author": "Quanzhu Niu, Yikang Zhou, Shihao Chen, Tao Zhang, and Shunping Ji",
        "link": "http://arxiv.org/abs/2507.05948v2",
        "abstract": "Video Instance Segmentation (VIS) fundamentally struggles with pervasive\nchallenges including object occlusions, motion blur, and appearance variations\nduring temporal association. To overcome these limitations, this work\nintroduces geometric awareness to enhance VIS robustness by strategically\nleveraging monocular depth estimation. We systematically investigate three\ndistinct integration paradigms. Expanding Depth Channel (EDC) method\nconcatenates the depth map as input channel to segmentation networks; Sharing\nViT (SV) designs a uniform ViT backbone, shared between depth estimation and\nsegmentation branches; Depth Supervision (DS) makes use of depth prediction as\nan auxiliary training guide for feature learning. Though DS exhibits limited\neffectiveness, benchmark evaluations demonstrate that EDC and SV significantly\nenhance the robustness of VIS. When with Swin-L backbone, our EDC method gets\n56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work\nconclusively establishes depth cues as critical enablers for robust video\nunderstanding."
    },
    {
        "date": "2025-07",
        "title": "What You Have is What You Track: Adaptive and Robust Multimodal Tracking",
        "author": "Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, and Zongwei Wu",
        "link": "http://arxiv.org/abs/2507.05899v1",
        "abstract": "Multimodal data is known to be helpful for visual tracking by improving\nrobustness to appearance variations. However, sensor synchronization challenges\noften compromise data availability, particularly in video settings where\nshortages can be temporal. Despite its importance, this area remains\nunderexplored. In this paper, we present the first comprehensive study on\ntracker performance with temporally incomplete multimodal data. Unsurprisingly,\nunder such a circumstance, existing trackers exhibit significant performance\ndegradation, as their rigid architectures lack the adaptability needed to\neffectively handle missing modalities. To address these limitations, we propose\na flexible framework for robust multimodal tracking. We venture that a tracker\nshould dynamically activate computational units based on missing data rates.\nThis is achieved through a novel Heterogeneous Mixture-of-Experts fusion\nmechanism with adaptive complexity, coupled with a video-level masking strategy\nthat ensures both temporal consistency and spatial completeness which is\ncritical for effective video tracking. Surprisingly, our model not only adapts\nto varying missing rates but also adjusts to scene complexity. Extensive\nexperiments show that our model achieves SOTA performance across 9 benchmarks,\nexcelling in both conventional complete and missing modality settings. The code\nand benchmark will be publicly available at\nhttps://github.com/supertyd/FlexTrack/tree/main."
    },
    {
        "date": "2025-07",
        "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks",
        "author": "Huanming Shen, Baizhou Huang, and Xiaojun Wan",
        "link": "http://arxiv.org/abs/2507.06274v1",
        "abstract": "Watermarking is a promising defense against the misuse of large language\nmodels (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.\nThis vulnerability stems from an inherent trade-off governed by watermark\nwindow size: smaller windows resist scrubbing better but are easier to\nreverse-engineer, enabling low-cost statistics-based spoofing attacks. This\nwork breaks this trade-off by introducing a novel mechanism, equivalent texture\nkeys, where multiple tokens within a watermark window can independently support\nthe detection. Based on the redundancy, we propose a novel watermark scheme\nwith Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a\nPareto improvement, increasing the resilience against scrubbing attacks without\ncompromising robustness to spoofing. Experiments demonstrate SEEK's superiority\nover prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%\nand scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset\nsettings."
    },
    {
        "date": "2025-07",
        "title": "Robust Power System State Estimation using Physics-Informed Neural Networks",
        "author": "Solon Falas, Markos Asprou, Charalambos Konstantinou, and Maria K. Michael",
        "link": "http://arxiv.org/abs/2507.05874v1",
        "abstract": "Modern power systems face significant challenges in state estimation and\nreal-time monitoring, particularly regarding response speed and accuracy under\nfaulty conditions or cyber-attacks. This paper proposes a hybrid approach using\nphysics-informed neural networks (PINNs) to enhance the accuracy and\nrobustness, of power system state estimation. By embedding physical laws into\nthe neural network architecture, PINNs improve estimation accuracy for\ntransmission grid applications under both normal and faulty conditions, while\nalso showing potential in addressing security concerns such as data\nmanipulation attacks. Experimental results show that the proposed approach\noutperforms traditional machine learning models, achieving up to 83% higher\naccuracy on unseen subsets of the training dataset and 65% better performance\non entirely new, unrelated datasets. Experiments also show that during a data\nmanipulation attack against a critical bus in a system, the PINN can be up to\n93% more accurate than an equivalent neural network."
    },
    {
        "date": "2025-07",
        "title": "Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters",
        "author": "Marco Roschkowski",
        "link": "http://arxiv.org/abs/2507.05807v1",
        "abstract": "In this paper, we tackle two fundamental problems in few-shot domain\nadaptation of foundation models. First, hyperparameter tuning is often\nimpractical due to the lack of large validation datasets. Second, model\nrobustness under distribution shifts where test time data deviates slightly\nfrom training distributions, remains a concern. We show that by training\nmultiple independent adapters and averaging their outputs, the new model has a\nhigher performance and is more robust to distribution shifts compared to any\nindividual adapter. This improvement holds even when the adapters are trained\nwith diverse hyperparameters sampled from a wide range, resulting in varied\nindividual performance. Consequently, our method addresses both of the problems\ndescribed above. The ensemble is also significantly less sensitive to the\nresidual ratio, a critical hyperparameter of CLIP-Adapter. Since the ensemble\ncan be reparameterized to a single adapter again using a principled\nconcatenation of the parameters, we refer to our method as Soup-Adapter. This\nis also the first study to explore CLIP adapter-style techniques for DINOv2 and\nto directly compare them with CLIP in this setting."
    },
    {
        "date": "2025-07",
        "title": "Robust Bandwidth Estimation for Real-Time Communication with Offline Reinforcement Learning",
        "author": "Jian Kai, Tianwei Zhang, Zihan Ling, Yang Cao, and Can Shen",
        "link": "http://arxiv.org/abs/2507.05785v1",
        "abstract": "Accurate bandwidth estimation (BWE) is critical for real-time communication\n(RTC) systems. Traditional heuristic approaches offer limited adaptability\nunder dynamic networks, while online reinforcement learning (RL) suffers from\nhigh exploration costs and potential service disruptions. Offline RL, which\nleverages high-quality data collected from real-world environments, offers a\npromising alternative. However, challenges such as out-of-distribution (OOD)\nactions, policy extraction from behaviorally diverse datasets, and reliable\ndeployment in production systems remain unsolved. We propose RBWE, a robust\nbandwidth estimation framework based on offline RL that integrates Q-ensemble\n(an ensemble of Q-functions) with a Gaussian mixture policy to mitigate OOD\nrisks and enhance policy learning. A fallback mechanism ensures deployment\nstability by switching to heuristic methods under high uncertainty.\nExperimental results show that RBWE reduces overestimation errors by 18% and\nimproves the 10th percentile Quality of Experience (QoE) by 18.6%,\ndemonstrating its practical effectiveness in real-world RTC applications."
    },
    {
        "date": "2025-07",
        "title": "Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy",
        "author": "Radoslaw Roszczyk, Artur Krupa, and Izabella Antoniuk",
        "link": "http://arxiv.org/abs/2507.05757v1",
        "abstract": "The acquisition of accurately coloured, balanced images in an optical\nmicroscope can be a challenge even for experienced microscope operators. This\narticle presents an entirely automatic mechanism for balancing the white level\nthat allows the correction of the microscopic colour images adequately. The\nresults of the algorithm have been confirmed experimentally on a set of two\nhundred microscopic images. The images contained scans of three microscopic\nspecimens commonly used in pathomorphology. Also, the results achieved were\ncompared with other commonly used white balance algorithms in digital\nphotography. The algorithm applied in this work is more effective than the\nclassical algorithms used in colour photography for microscopic images stained\nwith hematoxylin-phloxine-saffron and for immunohistochemical staining images."
    },
    {
        "date": "2025-07",
        "title": "SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations",
        "author": "Yegyu Han, Taegyoon Yoon, Dayeon Woo, Sojeong Kim, and Hyung-Sin Kim",
        "link": "http://arxiv.org/abs/2507.05751v1",
        "abstract": "Recent advances on 6D object-pose estimation has achieved high performance on\nrepresentative benchmarks such as LM-O, YCB-V, and T-Less. However, these\ndatasets were captured under fixed illumination and camera settings, leaving\nthe impact of real-world variations in illumination, exposure, gain or\ndepth-sensor mode - and the potential of test-time sensor control to mitigate\nsuch variations - largely unexplored. To bridge this gap, we introduce\nSenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,\n9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.\nFor three common household objects (spray, pringles, and tincase), we acquire\n101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting\npermutations per object pose. Experiments with state-of-the-art models on our\ndataset show that applying sensor control during test-time induces greater\nperformance improvement over digital data augmentation, achieving performance\ncomparable to or better than costly increases in real-world training data\nquantity and diversity. Adapting either RGB or depth sensors individually is\neffective, while jointly adapting multimodal RGB-D configurations yields even\ngreater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from\ndata-centered to sensor-aware robustness, laying a foundation for adaptive,\nself-tuning perception systems capable of operating robustly in uncertain\nreal-world environments. Our dataset is available at:\nhuggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:\ngithub.com/yegyu-han/SenseShift6D"
    },
    {
        "date": "2025-07",
        "title": "DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective",
        "author": "Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, and Zhan Qin",
        "link": "http://arxiv.org/abs/2507.05622v1",
        "abstract": "The widespread application of Deep Learning across diverse domains hinges\ncritically on the quality and composition of training datasets. However, the\ncommon lack of disclosure regarding their usage raises significant privacy and\ncopyright concerns. Dataset auditing techniques, which aim to determine if a\nspecific dataset was used to train a given suspicious model, provide promising\nsolutions to addressing these transparency gaps. While prior work has developed\nvarious auditing methods, their resilience against dedicated adversarial\nattacks remains largely unexplored. To bridge the gap, this paper initiates a\ncomprehensive study evaluating dataset auditing from an adversarial\nperspective. We start with introducing a novel taxonomy, classifying existing\nmethods based on their reliance on internal features (IF) (inherent to the\ndata) versus external features (EF) (artificially introduced for auditing).\nSubsequently, we formulate two primary attack types: evasion attacks, designed\nto conceal the use of a dataset, and forgery attacks, intending to falsely\nimplicate an unused dataset. Building on the understanding of existing methods\nand attack objectives, we further propose systematic attack strategies:\ndecoupling, removal, and detection for evasion; adversarial example-based\nmethods for forgery. These formulations and strategies lead to our new\nbenchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9\nrepresentative auditing methods. Extensive evaluations using DATABench reveal\nthat none of the evaluated auditing methods are sufficiently robust or\ndistinctive under adversarial settings. These findings underscore the urgent\nneed for developing a more secure and reliable dataset auditing method capable\nof withstanding sophisticated adversarial manipulation. Code is available at\nhttps://github.com/shaoshuo-ss/DATABench."
    },
    {
        "date": "2025-07",
        "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models",
        "author": "Sangwon Hyun, Shaukat Ali, and M. Ali Babar",
        "link": "http://arxiv.org/abs/2507.05565v1",
        "abstract": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions."
    },
    {
        "date": "2025-07",
        "title": "Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge",
        "author": "Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, and Ehsan Irajizad",
        "link": "http://arxiv.org/abs/2507.05540v1",
        "abstract": "Graph Neural Networks (GNNs) often struggle with noisy edges. We propose\nLatent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate\nexternal \"clean\" links and guide embeddings of a noisy target graph. We train\ntwo encoders--one on the full graph (target plus external edges) and another on\na regularization graph excluding the target's potentially noisy links--then\npenalize discrepancies between their latent representations. This constraint\nsteers the model away from overfitting spurious edges. Experiments on benchmark\ndatasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs\nsubjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and\nvalidate it on a small protein-metabolite network, where metabolite-protein\ninteractions reduce noise in protein co-occurrence data. Our results highlight\nLSC-GNN's potential to boost predictive performance and interpretability in\nsettings with noisy relational structures."
    },
    {
        "date": "2025-07",
        "title": "Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search",
        "author": "Sanaz Kazemi Abharian, and Sai Manoj Pudukotai Dinakarrao",
        "link": "http://arxiv.org/abs/2507.05531v1",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful machine learning\nmethod for graph-structured data. A plethora of hardware accelerators has been\nintroduced to meet the performance demands of GNNs in real-world applications.\nHowever, security challenges of hardware-based attacks have been generally\noverlooked. In this paper, we investigate the vulnerability of GNN models to\nhardware-based fault attack, wherein an attacker attempts to misclassify output\nby modifying trained weight parameters through fault injection in a memory\ndevice. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware\nbit-flip fault attack, selecting a vulnerable bit in each selected weight\ngradually to compromise the GNN's performance by flipping a minimal number of\nbits. To achieve this, GBFA operates in two steps. First, a Markov model is\ncreated to predict the execution sequence of layers based on features extracted\nfrom memory access patterns, enabling the launch of the attack within a\nspecific layer. Subsequently, GBFA identifies vulnerable bits within the\nselected weights using gradient ranking through an in-layer search. We evaluate\nthe effectiveness of the proposed GBFA attack on various GNN models for node\nclassification tasks using the Cora and PubMed datasets. Our findings show that\nGBFA significantly degrades prediction accuracy, and the variation in its\nimpact across different layers highlights the importance of adopting a\nlayer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's\nprediction accuracy by 17% on the Cora dataset with only a single bit flip in\nthe last layer."
    },
    {
        "date": "2025-07",
        "title": "A Systematization of Security Vulnerabilities in Computer Use Agents",
        "author": "Daniel Jones, Giorgio Severi, Martin Pouliot, Gary Lopez, Joris de Gruyter, Santiago Zanella-Beguelin, Justin Song, Blake Bullwinkel, Pamela Cortez, and Amanda Minnich",
        "link": "http://arxiv.org/abs/2507.05445v1",
        "abstract": "Computer Use Agents (CUAs), autonomous systems that interact with software\ninterfaces via browsers or virtual machines, are rapidly being deployed in\nconsumer and enterprise environments. These agents introduce novel attack\nsurfaces and trust boundaries that are not captured by traditional threat\nmodels. Despite their growing capabilities, the security boundaries of CUAs\nremain poorly understood. In this paper, we conduct a systematic threat\nanalysis and testing of real-world CUAs under adversarial conditions. We\nidentify seven classes of risks unique to the CUA paradigm, and analyze three\nconcrete exploit scenarios in depth: (1) clickjacking via visual overlays that\nmislead interface-level reasoning, (2) indirect prompt injection that enables\nRemote Code Execution (RCE) through chained tool use, and (3) CoT exposure\nattacks that manipulate implicit interface framing to hijack multi-step\nreasoning. These case studies reveal deeper architectural flaws across current\nCUA implementations. Namely, a lack of input provenance tracking, weak\ninterface-action binding, and insufficient control over agent memory and\ndelegation. We conclude by proposing a CUA-specific security evaluation\nframework and design principles for safe deployment in adversarial and\nhigh-stakes settings."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Machine Learning Attacks on Financial Reporting via Maximum Violated Multi-Objective Attack",
        "author": "Edward Raff, Karen Kukla, Michel Benaroch, and Joseph Comprix",
        "link": "http://arxiv.org/abs/2507.05441v1",
        "abstract": "Bad actors, primarily distressed firms, have the incentive and desire to\nmanipulate their financial reports to hide their distress and derive personal\ngains. As attackers, these firms are motivated by potentially millions of\ndollars and the availability of many publicly disclosed and used financial\nmodeling frameworks. Existing attack methods do not work on this data due to\nanti-correlated objectives that must both be satisfied for the attacker to\nsucceed. We introduce Maximum Violated Multi-Objective (MVMO) attacks that\nadapt the attacker's search direction to find $20\\times$ more satisfying\nattacks compared to standard attacks. The result is that in $\\approx50\\%$ of\ncases, a company could inflate their earnings by 100-200%, while simultaneously\nreducing their fraud scores by 15%. By working with lawyers and professional\naccountants, we ensure our threat model is realistic to how such frauds are\nperformed in practice."
    },
    {
        "date": "2025-07",
        "title": "Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift",
        "author": "Gautam Sreekumar, and Vishnu Naresh Boddeti",
        "link": "http://arxiv.org/abs/2507.05412v2",
        "abstract": "We consider the problem of learning robust discriminative representations of\ncausally-related latent variables. In addition to observational data, the\ntraining dataset also includes interventional data obtained through targeted\ninterventions on some of these latent variables to learn representations robust\nagainst the resulting interventional distribution shifts. Existing approaches\ntreat interventional data like observational data, even when the underlying\ncausal model is known, and ignore the independence relations that arise from\nthese interventions. Since these approaches do not fully exploit the causal\nrelational information resulting from interventions, they learn representations\nthat produce large disparities in predictive performance on observational and\ninterventional data, which worsens when the number of interventional training\nsamples is limited. In this paper, (1) we first identify a strong correlation\nbetween this performance disparity and adherence of the representations to the\nindependence conditions induced by the interventional causal model. (2) For\nlinear models, we derive sufficient conditions on the proportion of\ninterventional data in the training dataset, for which enforcing interventional\nindependence between representations corresponding to the intervened node and\nits non-descendants lowers the error on interventional data. Combining these\ninsights, (3) we propose RepLIn, a training algorithm to explicitly enforce\nthis statistical independence during interventions. We demonstrate the utility\nof RepLIn on a synthetic dataset and on real image and text datasets on facial\nattribute classification and toxicity detection, respectively. Our experiments\nshow that RepLIn is scalable with the number of nodes in the causal graph and\nis suitable to improve the robust representations against interventional\ndistribution shifts of both continuous and discrete latent variables."
    },
    {
        "date": "2025-07",
        "title": "Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method",
        "author": "Haoqi He, Xiaokai Lin, Jiancai Chen, and Yan Xiao",
        "link": "http://arxiv.org/abs/2507.06262v1",
        "abstract": "Data poisoning attacks pose significant threats to machine learning models by\nintroducing malicious data into the training process, thereby degrading model\nperformance or manipulating predictions. Detecting and sifting out poisoned\ndata is an important method to prevent data poisoning attacks. Limited by\nclassical computation frameworks, upcoming larger-scale and more complex\ndatasets may pose difficulties for detection. We introduce the unique speedup\nof quantum computing for the first time in the task of detecting data\npoisoning. We present Q-Detection, a quantum-classical hybrid defense method\nfor detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which\nis optimized using quantum computing devices. Experimental results using\nmultiple quantum simulation libraries show that Q-Detection effectively defends\nagainst label manipulation and backdoor attacks. The metrics demonstrate that\nQ-Detection consistently outperforms the baseline methods and is comparable to\nthe state-of-the-art. Theoretical analysis shows that Q-Detection is expected\nto achieve more than a 20% speedup using quantum computing power."
    },
    {
        "date": "2025-07",
        "title": "YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries",
        "author": "Aquino Joctum, and John Kandiri",
        "link": "http://arxiv.org/abs/2507.05376v1",
        "abstract": "Autonomous vehicle perception systems require robust pedestrian detection,\nparticularly on geometrically complex roadways like Type-S curved surfaces,\nwhere standard RGB camera-based methods face limitations. This paper introduces\nYOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework\nspecifically for this challenge. YOLO-APD integrates several key architectural\nmodifications: a parameter-free SimAM attention mechanism, computationally\nefficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale\nfeature pooling, the Mish activation function for improved optimization, and an\nIntelligent Gather & Distribute (IGD) module for superior feature fusion in the\nnetwork's neck. The concept of leveraging vehicle steering dynamics for\nadaptive region-of-interest processing is also presented. Comprehensive\nevaluations on a custom CARLA dataset simulating complex scenarios demonstrate\nthat YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%\nmAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly\noutperforming baseline models, including YOLOv8. Furthermore, it maintains\nreal-time processing capabilities at 100 FPS, showcasing a superior balance\nbetween accuracy and efficiency. Ablation studies validate the synergistic\ncontribution of each integrated component. Evaluation on the KITTI dataset\nconfirms the architecture's potential while highlighting the need for domain\nadaptation. This research advances the development of highly accurate,\nefficient, and adaptable perception systems based on cost-effective sensors,\ncontributing to enhanced safety and reliability for autonomous navigation in\nchallenging, less-structured driving environments."
    },
    {
        "date": "2025-07",
        "title": "Extreme Learning Machine Based System for DDoS Attacks Detections on IoMT Devices",
        "author": "Nelly Elsayed, Lily Dzamesi, Zag ElSayed, and Murat Ozer",
        "link": "http://arxiv.org/abs/2507.05132v1",
        "abstract": "The Internet of Medical Things (IoMT) represents a paradigm shift in the\nhealthcare sector, enabling the interconnection of medical devices, sensors,\nand systems to enhance patient monitoring, diagnosis, and management. The rapid\nevolution of IoMT presents significant benefits to the healthcare domains.\nHowever, there is a rapid increase in distributed denial of service (DDoS)\nattacks on the IoMT networks due to several vulnerabilities in the\nIoMT-connected devices, which negatively impact patients' health and can even\nlead to deaths. Thus, in this paper, we aim to save lives via investigating an\nextreme learning machine for detecting DDoS attacks on IoMT devices. The\nproposed approach achieves a high accuracy at a low implementation budget.\nThus, it can reduce the implementation cost of the DDoS detection system,\nmaking the model capable of executing on the fog level."
    },
    {
        "date": "2025-07",
        "title": "CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation",
        "author": "Binyan Xu, Fan Yang, Xilin Dai, Di Tang, and Kehuan Zhang",
        "link": "http://arxiv.org/abs/2507.05113v1",
        "abstract": "Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison training data to implant backdoor into the victim model.\nCurrent backdoor defenses on poisoned data often suffer from high computational\ncosts or low effectiveness against advanced attacks like clean-label and\nclean-image backdoors. To address them, we introduce CLIP-Guided backdoor\nDefense (CGD), an efficient and effective method that mitigates various\nbackdoor attacks. CGD utilizes a publicly accessible CLIP model to identify\ninputs that are likely to be clean or poisoned. It then retrains the model with\nthese inputs, using CLIP's logits as a guidance to effectively neutralize the\nbackdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD\nreduces attack success rates (ASRs) to below 1% while maintaining clean\naccuracy (CA) with a maximum drop of only 0.3%, outperforming existing\ndefenses. Additionally, we show that clean-data-based defenses can be adapted\nto poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining\nlow ASRs even when employing a weaker CLIP model or when CLIP itself is\ncompromised by a backdoor. These findings underscore CGD's exceptional\nefficiency, effectiveness, and applicability for real-world backdoor defense\nscenarios. Code: https://github.com/binyxu/CGD."
    },
    {
        "date": "2025-07",
        "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
        "author": "Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, and Simeone Pizzi",
        "link": "http://arxiv.org/abs/2507.05093v1",
        "abstract": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations."
    },
    {
        "date": "2025-07",
        "title": "Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport",
        "author": "Qinkai Yu, Jianyang Xie, Yitian Zhao, Cheng Chen, Lijun Zhang, Liming Chen, Jun Cheng, Lu Liu, Yalin Zheng, and Yanda Meng",
        "link": "http://arxiv.org/abs/2507.04999v1",
        "abstract": "Multimodal ophthalmic imaging-based diagnosis integrates color fundus image\nwith optical coherence tomography (OCT) to provide a comprehensive view of\nocular pathologies. However, the uneven global distribution of healthcare\nresources often results in real-world clinical scenarios encountering\nincomplete multimodal data, which significantly compromises diagnostic\naccuracy. Existing commonly used pipelines, such as modality imputation and\ndistillation methods, face notable limitations: 1)Imputation methods struggle\nwith accurately reconstructing key lesion features, since OCT lesions are\nlocalized, while fundus images vary in style. 2)distillation methods rely\nheavily on fully paired multimodal training data. To address these challenges,\nwe propose a novel multimodal alignment and fusion framework capable of\nrobustly handling missing modalities in the task of ophthalmic diagnostics. By\nconsidering the distinctive feature characteristics of OCT and fundus images,\nwe emphasize the alignment of semantic features within the same category and\nexplicitly learn soft matching between modalities, allowing the missing\nmodality to utilize existing modality information, achieving robust cross-modal\nfeature alignment under the missing modality. Specifically, we leverage the\nOptimal Transport for multi-scale modality feature alignment: class-wise\nalignment through predicted class prototypes and feature-wise alignment via\ncross-modal shared feature transport. Furthermore, we propose an asymmetric\nfusion strategy that effectively exploits the distinct characteristics of OCT\nand fundus modalities. Extensive evaluations on three large ophthalmic\nmultimodal datasets demonstrate our model's superior performance under various\nmodality-incomplete scenarios, achieving Sota performance in both complete\nmodality and inter-modality incompleteness conditions. Code is available at\nhttps://github.com/Qinkaiyu/RIMA"
    },
    {
        "date": "2025-07",
        "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning",
        "author": "Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, and Kok-Seng Wong",
        "link": "http://arxiv.org/abs/2507.04903v1",
        "abstract": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed."
    },
    {
        "date": "2025-07",
        "title": "RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction",
        "author": "Johannes K\u00fcnzel, Anna Hilsmann, and Peter Eisert",
        "link": "http://arxiv.org/abs/2507.04839v2",
        "abstract": "We introduce RIPE, an innovative reinforcement learning-based framework for\nweakly-supervised training of a keypoint extractor that excels in both\ndetection and description tasks. In contrast to conventional training regimes\nthat depend heavily on artificial transformations, pre-generated models, or 3D\ndata, RIPE requires only a binary label indicating whether paired images\nrepresent the same scene. This minimal supervision significantly expands the\npool of training data, enabling the creation of a highly generalized and robust\nkeypoint extractor.\n  RIPE utilizes the encoder's intermediate layers for the description of the\nkeypoints with a hyper-column approach to integrate information from different\nscales. Additionally, we propose an auxiliary loss to enhance the\ndiscriminative capability of the learned descriptors.\n  Comprehensive evaluations on standard benchmarks demonstrate that RIPE\nsimplifies data preparation while achieving competitive performance compared to\nstate-of-the-art techniques, marking a significant advancement in robust\nkeypoint extraction and description. To support further research, we have made\nour code publicly available at https://github.com/fraunhoferhhi/RIPE."
    },
    {
        "date": "2025-07",
        "title": "Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems",
        "author": "Bo Yan, Yurong Hao, Dingqi Liu, Huabin Sun, Pengpeng Qiao, Wei Yang Bryan Lim, Yang Cao, and Chuan Shi",
        "link": "http://arxiv.org/abs/2507.06258v1",
        "abstract": "Federated recommender systems (FedRec) have emerged as a promising solution\nfor delivering personalized recommendations while safeguarding user privacy.\nHowever, recent studies have demonstrated their vulnerability to poisoning\nattacks. Existing attacks typically target the entire user group, which\ncompromises stealth and increases the risk of detection. In contrast,\nreal-world adversaries may prefer to prompt target items to specific user\nsubgroups, such as recommending health supplements to elderly users. Motivated\nby this gap, we introduce Spattack, the first targeted poisoning attack\ndesigned to manipulate recommendations for specific user subgroups in the\nfederated setting. Specifically, Spattack adopts a two-stage\napproximation-and-promotion strategy, which first simulates user embeddings of\ntarget/non-target subgroups and then prompts target items to the target\nsubgroups. To enhance the approximation stage, we push the inter-group\nembeddings away based on contrastive learning and augment the target group's\nrelevant item set based on clustering. To enhance the promotion stage, we\nfurther propose to adaptively tune the optimization weights between target and\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\nalign the embeddings between the target items and the relevant items. We\nconduct comprehensive experiments on three real-world datasets, comparing\nSpattack against seven state-of-the-art poisoning attacks and seven\nrepresentative defense mechanisms. Experimental results demonstrate that\nSpattack consistently achieves strong manipulation performance on the specific\nuser subgroup, while incurring minimal impact on non-target users, even when\nonly 0.1\\% of users are malicious. Moreover, Spattack maintains competitive\noverall recommendation performance and exhibits strong resilience against\nexisting mainstream defenses."
    },
    {
        "date": "2025-07",
        "title": "Enabling Security on the Edge: A CHERI Compartmentalized Network Stack",
        "author": "Donato Ferraro, Andrea Bastoni, Alexander Zuepke, and Andrea Marongiu",
        "link": "http://arxiv.org/abs/2507.04818v1",
        "abstract": "The widespread deployment of embedded systems in critical infrastructures,\ninterconnected edge devices like autonomous drones, and smart industrial\nsystems requires robust security measures. Compromised systems increase the\nrisks of operational failures, data breaches, and -- in safety-critical\nenvironments -- potential physical harm to people. Despite these risks, current\nsecurity measures are often insufficient to fully address the attack surfaces\nof embedded devices. CHERI provides strong security from the hardware level by\nenabling fine-grained compartmentalization and memory protection, which can\nreduce the attack surface and improve the reliability of such devices. In this\nwork, we explore the potential of CHERI to compartmentalize one of the most\ncritical and targeted components of interconnected systems: their network\nstack. Our case study examines the trade-offs of isolating applications, TCP/IP\nlibraries, and network drivers on a CheriBSD system deployed on the Arm Morello\nplatform. Our results suggest that CHERI has the potential to enhance security\nwhile maintaining performance in embedded-like environments."
    },
    {
        "date": "2025-07",
        "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning",
        "author": "Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, and Kuk-Jin Yoon",
        "link": "http://arxiv.org/abs/2507.04790v1",
        "abstract": "Motion planning is a crucial component of autonomous robot driving. While\nvarious trajectory datasets exist, effectively utilizing them for a target\ndomain remains challenging due to differences in agent interactions and\nenvironmental characteristics. Conventional approaches, such as domain\nadaptation or ensemble learning, leverage multiple source datasets but suffer\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\nTo address these challenges, we propose Interaction-Merged Motion Planning\n(IMMP), a novel approach that leverages parameter checkpoints trained on\ndifferent domains during adaptation to the target domain. IMMP follows a\ntwo-step process: pre-merging to capture agent behaviors and interactions,\nsufficiently extracting diverse information from the source domain, followed by\nmerging to construct an adaptable model that efficiently transfers diverse\ninteractions to the target domain. Our method is evaluated on various planning\nbenchmarks and models, demonstrating superior performance compared to\nconventional approaches."
    },
    {
        "date": "2025-07",
        "title": "FedPall: Prototype-based Adversarial and Collaborative Learning for Federated Learning with Feature Drift",
        "author": "Yong Zhang, Feng Liang, Guanghu Yuan, Min Yang, Chengming Li, and Xiping Hu",
        "link": "http://arxiv.org/abs/2507.04781v1",
        "abstract": "Federated learning (FL) enables collaborative training of a global model in\nthe centralized server with data from multiple parties while preserving\nprivacy. However, data heterogeneity can significantly degrade the performance\nof the global model when each party uses datasets from different sources to\ntrain a local model, thereby affecting personalized local models. Among various\ncases of data heterogeneity, feature drift, feature space difference among\nparties, is prevalent in real-life data but remains largely unexplored. Feature\ndrift can distract feature extraction learning in clients and thus lead to poor\nfeature extraction and classification performance. To tackle the problem of\nfeature drift in FL, we propose FedPall, an FL framework that utilizes\nprototype-based adversarial learning to unify feature spaces and collaborative\nlearning to reinforce class information within the features. Moreover, FedPall\nleverages mixed features generated from global prototypes and local features to\nenhance the global classifier with classification-relevant information from a\nglobal perspective. Evaluation results on three representative feature-drifted\ndatasets demonstrate FedPall's consistently superior performance in\nclassification with feature-drifted data in the FL scenario."
    },
    {
        "date": "2025-07",
        "title": "Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet",
        "author": "Raz Lapid, and Almog Dubin",
        "link": "http://arxiv.org/abs/2507.04726v1",
        "abstract": "Text-to-image diffusion models have achieved remarkable success in\ntranslating textual prompts into high-fidelity images. ControlNets further\nextend these models by allowing precise, image-based conditioning (e.g., edge\nmaps, depth, pose), enabling fine-grained control over structure and style.\nHowever, their dependence on large, publicly scraped datasets -- and the\nincreasing use of community-shared data for fine-tuning -- exposes them to\nstealthy data poisoning attacks. In this work, we introduce a novel data\npoisoning method that manipulates ControlNets to generate images containing\nspecific content without any text triggers. By injecting poisoned samples --\neach pairing a subtly triggered input with an NSFW target -- the model retains\nclean-prompt fidelity yet reliably produces NSFW outputs when the trigger is\npresent. On large-scale, high-quality datasets, our backdoor achieves high\nattack success rate while remaining imperceptible in raw inputs. These results\nreveal a critical vulnerability in open-source ControlNets pipelines and\nunderscore the need for robust data sanitization and defense mechanisms."
    },
    {
        "date": "2025-07",
        "title": "Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World",
        "author": "Vinu Sankar Sadasivan, Soheil Feizi, Rajiv Mathews, and Lun Wang",
        "link": "http://arxiv.org/abs/2507.06256v1",
        "abstract": "This paper investigates the real-world vulnerabilities of audio-based large\nlanguage models (ALLMs), such as Qwen2-Audio. We first demonstrate that an\nadversary can craft stealthy audio perturbations to manipulate ALLMs into\nexhibiting specific targeted behaviors, such as eliciting responses to\nwake-keywords (e.g., \"Hey Qwen\"), or triggering harmful behaviors (e.g. \"Change\nmy calendar event\"). Subsequently, we show that playing adversarial background\nnoise during user interaction with the ALLMs can significantly degrade the\nresponse quality. Crucially, our research illustrates the scalability of these\nattacks to real-world scenarios, impacting other innocent users when these\nadversarial noises are played through the air. Further, we discuss the\ntransferrability of the attack, and potential defensive measures."
    },
    {
        "date": "2025-07",
        "title": "Optimal Model Selection for Conformalized Robust Optimization",
        "author": "Yajie Bao, Yang Hu, Haojie Ren, Peng Zhao, and Changliang Zou",
        "link": "http://arxiv.org/abs/2507.04716v1",
        "abstract": "In decision-making under uncertainty, Contextual Robust Optimization (CRO)\nprovides reliability by minimizing the worst-case decision loss over a\nprediction set, hedging against label variability. While recent advances use\nconformal prediction to construct prediction sets for machine learning models,\nthe downstream decisions critically depend on model selection. This paper\nintroduces novel model selection frameworks for CRO that unify robustness\ncontrol with decision risk minimization. We first propose Conformalized Robust\nOptimization with Model Selection (CROMS), which automatically selects models\nto approximately minimize the average decision risk in CRO solutions. We\ndevelop two algorithms: E-CROMS, which is computationally efficient, and\nF-CROMS, which enjoys a marginal robustness guarantee in finite samples.\nFurther, we introduce Conformalized Robust Optimization with Individualized\nModel Selection (CROiMS), which performs individualized model selection by\nminimizing the conditional decision risk given the covariate of test data. This\nframework advances conformal prediction methodology by enabling covariate-aware\nmodel selection. Theoretically, CROiMS achieves asymptotic conditional\nrobustness and decision efficiency under mild assumptions. Numerical results\ndemonstrate significant improvements in decision efficiency and robustness\nacross diverse synthetic and real-world applications, outperforming baseline\napproaches."
    },
    {
        "date": "2025-07",
        "title": "Hybrid Adversarial Spectral Loss Conditional Generative Adversarial Networks for Signal Data Augmentation in Ultra-precision Machining Surface Roughness Prediction",
        "author": "Suiyan Shang, Chi Fai Cheung, and Pai Zheng",
        "link": "http://arxiv.org/abs/2507.04665v1",
        "abstract": "Accurate surface roughness prediction in ultra-precision machining (UPM) is\ncritical for real-time quality control, but small datasets hinder model\nperformance. We propose HAS-CGAN, a Hybrid Adversarial Spectral Loss CGAN, for\neffective UPM data augmentation. Among five CGAN variants tested, HAS-CGAN\nexcels in 1D force signal generation, particularly for high-frequency signals,\nachieving >0.85 wavelet coherence through Fourier-domain optimization. By\ncombining generated signals with machining parameters, prediction accuracy\nsignificantly improves. Experiments with traditional ML (SVR, RF, LSTM) and\ndeep learning models (BPNN, 1DCNN, CNN-Transformer) demonstrate that augmenting\ntraining data with 520+ synthetic samples reduces prediction error from 31.4%\n(original 52 samples) to ~9%, effectively addressing data scarcity in UPM\nroughness prediction.\""
    },
    {
        "date": "2025-07",
        "title": "Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts",
        "author": "Yun Wang, Longguang Wang, Chenghao Zhang, Yongjian Zhang, Zhanjie Zhang, Ao Ma, Chenyou Fan, Tin Lun Lam, and Junjie Hu",
        "link": "http://arxiv.org/abs/2507.04631v1",
        "abstract": "Recently, learning-based stereo matching networks have advanced\nsignificantly. However, they often lack robustness and struggle to achieve\nimpressive cross-domain performance due to domain shifts and imbalanced\ndisparity distributions among diverse datasets. Leveraging Vision Foundation\nModels (VFMs) can intuitively enhance the model's robustness, but integrating\nsuch a model into stereo matching cost-effectively to fully realize their\nrobustness remains a key challenge. To address this, we propose SMoEStereo, a\nnovel framework that adapts VFMs for stereo matching through a tailored,\nscene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and\nMoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal\nexperts within MoE to adapt varying scenes across domains, while the latter\ninjects inductive bias into frozen VFMs to improve geometric feature\nextraction. Importantly, to mitigate computational overhead, we further propose\na lightweight decision network that selectively activates MoE modules based on\ninput complexity, balancing efficiency with accuracy. Extensive experiments\ndemonstrate that our method exhibits state-of-the-art cross-domain and joint\ngeneralization across multiple benchmarks without dataset-specific adaptation.\nThe code is available at\n\\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}."
    },
    {
        "date": "2025-07",
        "title": "README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model",
        "author": "Hyunwook Choi, Sangyun Won, Daeyeon Hwang, and Junhyeok Choi",
        "link": "http://arxiv.org/abs/2507.04495v1",
        "abstract": "Deep learning-based watermarking has emerged as a promising solution for\nrobust image authentication and protection. However, existing models are\nlimited by low embedding capacity and vulnerability to bit-level errors, making\nthem unsuitable for cryptographic applications such as digital signatures,\nwhich require over 2048 bits of error-free data. In this paper, we propose\nREADME (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a\nnovel framework that enables robust, verifiable, and error-tolerant digital\nsignatures within images. Our method combines a simple yet effective\ncropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a\nlightweight error correction module designed to localize and correct bit errors\nusing Distinct Circular Subsum Sequences (DCSS). Without requiring any\nfine-tuning of existing pretrained watermarking models, README significantly\nboosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when\nembedding 2048-bit digital signatures into a single image, even under\nreal-world distortions. Moreover, our use of perceptual hash-based signature\nverification ensures public verifiability and robustness against tampering. The\nproposed framework unlocks a new class of high-assurance applications for deep\nwatermarking, bridging the gap between signal-level watermarking and\ncryptographic security."
    },
    {
        "date": "2025-07",
        "title": "Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference",
        "author": "Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, and Jeff Hawkins",
        "link": "http://arxiv.org/abs/2507.04494v1",
        "abstract": "Current AI systems achieve impressive performance on many tasks, yet they\nlack core attributes of biological intelligence, including rapid, continual\nlearning, representations grounded in sensorimotor interactions, and structured\nknowledge that enables efficient generalization. Neuroscience theory suggests\nthat mammals evolved flexible intelligence through the replication of a\nsemi-independent, sensorimotor module, a functional unit known as a cortical\ncolumn. To address the disparity between biological and artificial\nintelligence, thousand-brains systems were proposed as a means of mirroring the\narchitecture of cortical columns and their interactions.\n  In the current work, we evaluate the unique properties of Monty, the first\nimplementation of a thousand-brains system. We focus on 3D object perception,\nand in particular, the combined task of object recognition and pose estimation.\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\nsensorimotor learning to build structured representations, finding that these\nenable robust generalization. These representations include an emphasis on\nclassifying objects by their global shape, as well as a natural ability to\ndetect object symmetries. We then explore Monty's use of model-free and\nmodel-based policies to enable rapid inference by supporting principled\nmovements. We find that such policies complement Monty's modular architecture,\na design that can accommodate communication between modules to further\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\nand computationally efficient learning, properties that compare favorably to\ncurrent deep learning architectures. While Monty is still in a nascent stage of\ndevelopment, these findings support thousand-brains systems as a powerful and\npromising new approach to AI."
    },
    {
        "date": "2025-07",
        "title": "A validity-guided workflow for robust large language model research in psychology",
        "author": "Zhicheng Lin",
        "link": "http://arxiv.org/abs/2507.04491v1",
        "abstract": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research."
    },
    {
        "date": "2025-07",
        "title": "Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models",
        "author": "Sathesh P. Sivashanmugam",
        "link": "http://arxiv.org/abs/2507.04478v1",
        "abstract": "Large language models (LLMs) have transformed natural language processing,\nbut their ability to memorize training data poses significant privacy risks.\nThis paper investigates model inversion attacks on the Llama 3.2 model, a\nmultilingual LLM developed by Meta. By querying the model with carefully\ncrafted prompts, we demonstrate the extraction of personally identifiable\ninformation (PII) such as passwords, email addresses, and account numbers. Our\nfindings highlight the vulnerability of even smaller LLMs to privacy attacks\nand underscore the need for robust defenses. We discuss potential mitigation\nstrategies, including differential privacy and data sanitization, and call for\nfurther research into privacy-preserving machine learning techniques."
    },
    {
        "date": "2025-07",
        "title": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient LLM Jailbreaking",
        "author": "Tim Beyer, Yan Scholten, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2507.04446v2",
        "abstract": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety."
    },
    {
        "date": "2025-07",
        "title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs",
        "author": "Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho",
        "link": "http://arxiv.org/abs/2507.04365v1",
        "abstract": "As large language models (LLMs) become more integral to society and\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\nvulnerabilities to bypass safety guardrails, posing a significant threat.\nHowever, the mechanisms enabling these attacks are not well understood. In this\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\nAttention Slipping. During this phenomenon, the model gradually reduces the\nattention it allocates to unsafe requests in a user query during the attack\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\nconsistent across various jailbreak methods, including gradient-based token\nreplacement, prompt-level template refinement, and in-context learning.\nAdditionally, we evaluate two defenses based on query perturbation, Token\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\nSlipping, with their effectiveness positively correlated with the degree of\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\na new defense that directly counters Attention Slipping by sharpening the\nattention score distribution using temperature scaling. Experiments on four\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\nshow that our method effectively resists various jailbreak attacks while\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\nSharpening introduces no additional computational or memory overhead, making it\nan efficient and practical solution for real-world deployment."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization",
        "author": "Zuyu Zhang, Ning Chen, Yongshan Liu, Qinghua Zhang, and Xu Zhang",
        "link": "http://arxiv.org/abs/2507.04302v1",
        "abstract": "Single Domain Generalization (SDG) aims to develop models capable of\ngeneralizing to unseen target domains using only one source domain, a task\ncomplicated by substantial domain shifts and limited data diversity. Existing\nSDG approaches primarily rely on data augmentation techniques, which struggle\nto effectively adapt training dynamics to accommodate large domain shifts. To\naddress this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided\noptimization approach inspired by dynamical systems theory. By leveraging LE\nmeasurements to modulate the learning rate, LEAwareSGD encourages model\ntraining near the edge of chaos, a critical state that optimally balances\nstability and adaptability. This dynamic adjustment allows the model to explore\na wider parameter space and capture more generalizable features, ultimately\nenhancing the model's generalization capability. Extensive experiments on PACS,\nOfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial\ngeneralization gains, achieving up to 9.47\\% improvement on PACS in low-data\nregimes. These results underscore the effectiveness of training near the edge\nof chaos for enhancing model generalization capability in SDG tasks."
    },
    {
        "date": "2025-07",
        "title": "ML-Enhanced AES Anomaly Detection for Real-Time Embedded Security",
        "author": "Nishant Chinnasami, Rye Stahle-Smith, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2507.04197v1",
        "abstract": "Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm, yet its practical implementations remain susceptible to side-channel\nand fault injection attacks. In this work, we propose a comprehensive framework\nthat enhances AES-128 encryption security through controlled anomaly injection\nand real-time anomaly detection using both statistical and machine learning\n(ML) methods. We simulate timing and fault-based anomalies by injecting\nexecution delays and ciphertext perturbations during encryption, generating\nlabeled datasets for detection model training. Two complementary detection\nmechanisms are developed: a threshold-based timing anomaly detector and a\nsupervised Random Forest classifier trained on combined timing and ciphertext\nfeatures. We implement and evaluate the framework on both CPU and FPGA-based\nSoC hardware (PYNQ-Z1), measuring performance across varying block sizes,\ninjection rates, and core counts. Our results show that ML-based detection\nsignificantly outperforms threshold-based methods in precision and recall while\nmaintaining real-time performance on embedded hardware. Compared to existing\nAES anomaly detection methods, our solution offers a low-cost, real-time, and\naccurate detection approach deployable on lightweight FPGA platforms."
    },
    {
        "date": "2025-07",
        "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems",
        "author": "Samaneh Shafee, Alysson Bessani, and Pedro M. Ferreira",
        "link": "http://arxiv.org/abs/2507.06252v1",
        "abstract": "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach\nthat operates in the early phases of the cyber threat lifecycle. CTI involves\ncollecting, processing, and analyzing threat data to provide a more accurate\nand rapid understanding of cyber threats. Due to the large volume of data,\nautomation through Machine Learning (ML) and Natural Language Processing (NLP)\nmodels is essential for effective CTI extraction. These automated systems\nleverage Open Source Intelligence (OSINT) from sources like social networks,\nforums, and blogs to identify Indicators of Compromise (IoCs). Although prior\nresearch has focused on adversarial attacks on specific ML models, this study\nexpands the scope by investigating vulnerabilities within various components of\nthe entire CTI pipeline and their susceptibility to adversarial attacks. These\nvulnerabilities arise because they ingest textual inputs from various open\nsources, including real and potentially fake content. We analyse three types of\nattacks against CTI pipelines, including evasion, flooding, and poisoning, and\nassess their impact on the system's information selection capabilities.\nSpecifically, on fake text generation, the work demonstrates how adversarial\ntext generation techniques can create fake cybersecurity and cybersecurity-like\ntext that misleads classifiers, degrades performance, and disrupts system\nfunctionality. The focus is primarily on the evasion attack, as it precedes and\nenables flooding and poisoning attacks within the CTI pipeline."
    },
    {
        "date": "2025-07",
        "title": "Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking",
        "author": "Fred Lydeard, Bashar I. Ahmad, and Simon Godsill",
        "link": "http://arxiv.org/abs/2507.04116v1",
        "abstract": "This paper presents a computationally efficient multi-object tracking\napproach that can minimise track breaks (e.g., in challenging environments and\nagainst agile targets), learn the measurement model parameters on-line (e.g.,\nin dynamically changing scenes) and infer the class of the tracked objects, if\njoint tracking and kinematic behaviour classification is sought. It capitalises\non the flexibilities offered by the integrated Gaussian process as a motion\nmodel and the convenient statistical properties of non-homogeneous Poisson\nprocesses as a suitable observation model. This can be combined with the\nproposed effective track revival / stitching mechanism. We accordingly\nintroduce the two robust and adaptive trackers, Gaussian and Poisson Process\nwith Classification (GaPP-Class) and GaPP with Revival and Classification\n(GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme\nthat efficiently integrates track management and hyperparameter learning\n(including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class\nwith the addition of a Markov Chain Monte Carlo kernel applied to each particle\npermitting track revival and stitching (e.g., within a few time steps after\ndeleting a trajectory). Performance evaluation and benchmarking using synthetic\nand real data show that GaPP-Class and GaPP-ReaCtion outperform other\nstate-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly\nreduces track breaks (e.g., by around 30% from real radar data and markedly\nmore from simulated data)."
    },
    {
        "date": "2025-07",
        "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing",
        "author": "Jinwei Hu, Yi Dong, Zhengtao Ding, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2507.04105v1",
        "abstract": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments."
    },
    {
        "date": "2025-07",
        "title": "S-Leak: Leakage-Abuse Attack Against Efficient Conjunctive SSE via s-term Leakage",
        "author": "Yue Su, Meng Shen, Cong Zuo, Yuzhi Liu, and Liehuang Zhu",
        "link": "http://arxiv.org/abs/2507.04077v1",
        "abstract": "Conjunctive Searchable Symmetric Encryption (CSSE) enables secure conjunctive\nsearches over encrypted data. While leakage-abuse attacks (LAAs) against\nsingle-keyword SSE have been extensively studied, their extension to\nconjunctive queries faces a critical challenge: the combinatorial explosion of\ncandidate keyword combinations, leading to enormous time and space overhead for\nattacks. In this paper, we reveal a fundamental vulnerability in\nstate-of-the-art CSSE schemes: s-term leakage, where the keyword with the\nminimal document frequency in a query leaks distinct patterns. We propose\nS-Leak, the first passive attack framework that progressively recovers\nconjunctive queries by exploiting s-term leakage and global leakage. Our key\ninnovation lies in a three-stage approach: identifying the s-term of queries,\npruning low-probability keyword conjunctions, and reconstructing full queries.\nWe propose novel metrics to better assess attacks in conjunctive query\nscenarios. Empirical evaluations on real-world datasets demonstrate that our\nattack is effective in diverse CSSE configurations. When considering 161,700\nconjunctive keyword queries, our attack achieves a 95.15% accuracy in\nrecovering at least one keyword, 82.57% for at least two, 58% for all three\nkeywords, and maintains efficacy against defenses such as SEAL padding and CLRZ\nobfuscation. Our work exposes the underestimated risks of s-term leakage in\npractical SSE deployments and calls for a redesign of leakage models for\nmulti-keyword search scenarios."
    },
    {
        "date": "2025-07",
        "title": "Robust Low-light Scene Restoration via Illumination Transition",
        "author": "Ze Li, Feng Zhang, Xiatian Zhu, Meng Zhang, Yanghong Zhou, and P. Y. Mok",
        "link": "http://arxiv.org/abs/2507.03976v1",
        "abstract": "Synthesizing normal-light novel views from low-light multiview images is an\nimportant yet challenging task, given the low visibility and high ISO noise\npresent in the input images. Existing low-light enhancement methods often\nstruggle to effectively preprocess such low-light inputs, as they fail to\nconsider correlations among multiple views. Although other state-of-the-art\nmethods have introduced illumination-related components offering alternative\nsolutions to the problem, they often result in drawbacks such as color\ndistortions and artifacts, and they provide limited denoising effectiveness. In\nthis paper, we propose a novel Robust Low-light Scene Restoration framework\n(RoSe), which enables effective synthesis of novel views in normal lighting\nconditions from low-light multiview image inputs, by formulating the task as an\nilluminance transition estimation problem in 3D space, conceptualizing it as a\nspecialized rendering task. This multiview-consistent illuminance transition\nfield establishes a robust connection between low-light and normal-light\nconditions. By further exploiting the inherent low-rank property of\nillumination to constrain the transition representation, we achieve more\neffective denoising without complex 2D techniques or explicit noise modeling.\nTo implement RoSe, we design a concise dual-branch architecture and introduce a\nlow-rank denoising module. Experiments demonstrate that RoSe significantly\noutperforms state-of-the-art models in both rendering quality and multiview\nconsistency on standard benchmarks. The codes and data are available at\nhttps://pegasus2004.github.io/RoSe."
    },
    {
        "date": "2025-07",
        "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
        "author": "Shuliang Liu, Hongyi Liu, Aiwei Liu, Bingchen Duan, Qi Zheng, Yibo Yan, He Geng, Peijie Jiang, Jia Liu, and Xuming Hu",
        "link": "http://arxiv.org/abs/2507.05288v1",
        "abstract": "The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study",
        "author": "Kai Ye, Tianyi Chen, and Zhen Wang",
        "link": "http://arxiv.org/abs/2507.03953v1",
        "abstract": "With the increasing adoption of diffusion models for image generation and\npersonalization, concerns regarding privacy breaches and content misuse have\nbecome more pressing. In this study, we conduct a comprehensive comparison of\neight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak,\nMist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains.\nThese methods are evaluated under varying perturbation budgets, using a range\nof metrics to assess visual imperceptibility and protective efficacy. Our\nresults offer practical guidance for method selection. Code is available at:\nhttps://github.com/vkeilo/DiffAdvPerturbationBench."
    },
    {
        "date": "2025-07",
        "title": "LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing",
        "author": "Jiahao Chen, junhao li, Yiming Wang, Zhe Ma, Yi Jiang, Chunyi Zhou, Qingming Li, Tianyu Du, and Shouling Ji",
        "link": "http://arxiv.org/abs/2507.07056v1",
        "abstract": "The proliferation of Low-Rank Adaptation (LoRA) models has democratized\npersonalized text-to-image generation, enabling users to share lightweight\nmodels (e.g., personal portraits) on platforms like Civitai and Liblib.\nHowever, this \"share-and-play\" ecosystem introduces critical risks: benign\nLoRAs can be weaponized by adversaries to generate harmful content (e.g.,\npolitical, defamatory imagery), undermining creator rights and platform safety.\nExisting defenses like concept-erasure methods focus on full diffusion models\n(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability\nto adversarial prompt engineering. To bridge this gap, we propose LoRAShield,\nthe first data-free editing framework for securing LoRA models against misuse.\nOur platform-driven approach dynamically edits and realigns LoRA's weight\nsubspace via adversarial optimization and semantic augmentation. Experimental\nresults demonstrate that LoRAShield achieves remarkable effectiveness,\nefficiency, and robustness in blocking malicious generations without\nsacrificing the functionality of the benign task. By shifting the defense to\nplatforms, LoRAShield enables secure, scalable sharing of personalized models,\na critical step toward trustworthy generative ecosystems."
    },
    {
        "date": "2025-07",
        "title": "FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed",
        "author": "Jiaqi Zhang, Juntuo Wang, Zhixin Sun, John Zou, and Randall Balestriero",
        "link": "http://arxiv.org/abs/2507.03779v1",
        "abstract": "Large-scale vision foundation models such as DINOv2 boast impressive\nperformances by leveraging massive architectures and training datasets. But\nnumerous scenarios require practitioners to reproduce those pre-training\nsolutions, such as on private data, new modalities, or simply for scientific\nquestioning--which is currently extremely demanding computation-wise. We thus\npropose a novel pre-training strategy for DINOv2 that simultaneously\naccelerates convergence--and strengthens robustness to common corruptions as a\nby-product. Our approach involves a frequency filtering\ncurriculum--low-frequency being seen first--and the Gaussian noise patching\naugmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while\npre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still\nachieves matching robustness in corruption benchmarks (ImageNet-C) and\nmaintains competitive linear probing performance compared with baseline. This\ndual benefit of efficiency and robustness makes large-scale self-supervised\nfoundation modeling more attainable, while opening the door to novel\nexploration around data curriculum and augmentation as means to improve\nself-supervised learning models robustness. The code is available at\nhttps://github.com/KevinZ0217/fast_dinov2"
    },
    {
        "date": "2025-07",
        "title": "Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data",
        "author": "Rickard Karlsson, Piersilvio De Bartolomeis, Issa J. Dahabreh, and Jesse H. Krijthe",
        "link": "http://arxiv.org/abs/2507.03681v1",
        "abstract": "Randomized trials are typically designed to detect average treatment effects\nbut often lack the statistical power to uncover effect heterogeneity over\npatient characteristics, limiting their value for personalized decision-making.\nTo address this, we propose the QR-learner, a model-agnostic learner that\nestimates conditional average treatment effects (CATE) within the trial\npopulation by leveraging external data from other trials or observational\nstudies. The proposed method is robust: it has the potential to reduce the CATE\nprediction mean squared error while maintaining consistency, even when the\nexternal data is not aligned with the trial. Moreover, we introduce a procedure\nthat combines the QR-learner with a trial-only CATE learner and show that it\nasymptotically matches or exceeds the trial-only learner in terms of mean\nsquared error. We examine the performance of our approach in simulation studies\nand apply the methods to a real-world dataset, demonstrating improvements in\nboth CATE estimation and statistical power for detecting heterogeneous effects."
    },
    {
        "date": "2025-07",
        "title": "RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification",
        "author": "Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, and Bastiaan R. Bloem",
        "link": "http://arxiv.org/abs/2507.03594v1",
        "abstract": "Parkinson's Disease (PD) affects over 10 million people globally, with speech\nimpairments often preceding motor symptoms by years, making speech a valuable\nmodality for early, non-invasive detection. While recent deep-learning models\nachieve high accuracy, they typically lack the explainability required for\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\nexplainable cross-attention architecture that combines interpretable speech\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\nperformance in Speech-based PD detection while providing explanations that are\nmore consistent and more clinically meaningful. Additionally, we demonstrate\nthat performance degradation in certain speech tasks (e.g., monologue) can be\nmitigated by segmenting long recordings. Our findings indicate that performance\nand explainability are not necessarily mutually exclusive. Future work will\nenhance the usability of explanations for non-experts and explore severity\nestimation to increase the real-world clinical relevance."
    },
    {
        "date": "2025-07",
        "title": "Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation",
        "author": "Tao Tang, Shijie Xu, Yiting Wu, and Zhixiang Lu",
        "link": "http://arxiv.org/abs/2507.03585v1",
        "abstract": "The clinical utility of deep learning models for medical image segmentation\nis severely constrained by their inability to generalize to unseen domains.\nThis failure is often rooted in the models learning spurious correlations\nbetween anatomical content and domain-specific imaging styles. To overcome this\nfundamental challenge, we introduce Causal-SAM-LLM, a novel framework that\nelevates Large Language Models (LLMs) to the role of causal reasoners. Our\nframework, built upon a frozen Segment Anything Model (SAM) encoder,\nincorporates two synergistic innovations. First, Linguistic Adversarial\nDisentanglement (LAD) employs a Vision-Language Model to generate rich, textual\ndescriptions of confounding image styles. By training the segmentation model's\nfeatures to be contrastively dissimilar to these style descriptions, it learns\na representation robustly purged of non-causal information. Second, Test-Time\nCausal Intervention (TCI) provides an interactive mechanism where an LLM\ninterprets a clinician's natural language command to modulate the segmentation\ndecoder's features in real-time, enabling targeted error correction. We conduct\nan extensive empirical evaluation on a composite benchmark from four public\ndatasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under\ncross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM\nestablishes a new state of the art in out-of-distribution (OOD) robustness,\nimproving the average Dice score by up to 6.2 points and reducing the Hausdorff\nDistance by 15.8 mm over the strongest baseline, all while using less than 9%\nof the full model's trainable parameters. Our work charts a new course for\nbuilding robust, efficient, and interactively controllable medical AI systems."
    },
    {
        "date": "2025-07",
        "title": "Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right",
        "author": "Heather Lent",
        "link": "http://arxiv.org/abs/2507.03473v1",
        "abstract": "Despite mounting evidence that multilinguality can be easily weaponized\nagainst language models (LMs), works across NLP Security remain overwhelmingly\nEnglish-centric. In terms of securing LMs, the NLP norm of \"English first\"\ncollides with standard procedure in cybersecurity, whereby practitioners are\nexpected to anticipate and prepare for worst-case outcomes. To mitigate\nworst-case outcomes in NLP Security, researchers must be willing to engage with\nthe weakest links in LM security: lower-resourced languages. Accordingly, this\nwork examines the security of LMs for lower- and medium-resourced languages. We\nextend existing adversarial attacks for up to 70 languages to evaluate the\nsecurity of monolingual and multilingual LMs for these languages. Through our\nanalysis, we find that monolingual models are often too small in total number\nof parameters to ensure sound security, and that while multilinguality is\nhelpful, it does not always guarantee improved security either. Ultimately,\nthese findings highlight important considerations for more secure deployment of\nLMs, for communities of lower-resourced languages."
    },
    {
        "date": "2025-07",
        "title": "Evaluating the Evaluators: Trust in Adversarial Robustness Tests",
        "author": "Antonio Emanuele Cin\u00e0, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, and Fabio Roli",
        "link": "http://arxiv.org/abs/2507.03450v1",
        "abstract": "Despite significant progress in designing powerful adversarial evasion\nattacks for robustness verification, the evaluation of these methods often\nremains inconsistent and unreliable. Many assessments rely on mismatched\nmodels, unverified implementations, and uneven computational budgets, which can\nlead to biased results and a false sense of security. Consequently, robustness\nclaims built on such flawed testing protocols may be misleading and give a\nfalse sense of security. As a concrete step toward improving evaluation\nreliability, we present AttackBench, a benchmark framework developed to assess\nthe effectiveness of gradient-based attacks under standardized and reproducible\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\nimplementations based on a novel optimality metric, which enables researchers\nand practitioners to identify the most reliable and effective attack for use in\nsubsequent robustness evaluations. The framework enforces consistent testing\nconditions and enables continuous updates, making it a reliable foundation for\nrobustness verification."
    },
    {
        "date": "2025-07",
        "title": "Unlearning the Noisy Correspondence Makes CLIP More Robust",
        "author": "Haochen Han, Alex Jinpeng Wang, Peijun Ye, and Fangming Liu",
        "link": "http://arxiv.org/abs/2507.03434v1",
        "abstract": "The data appetite for Vision-Language Models (VLMs) has continuously scaled\nup from the early millions to billions today, which faces an untenable\ntrade-off with data quality and inevitably introduces Noisy Correspondence (NC)\nsamples. Undoubtedly, such semantically unrelated data significantly impairs\nthe performance of VLMs. Previous efforts mainly address this challenge by\nestimating refined alignment for more precise guidance. However, such\nresource-intensive pipelines that train VLMs from scratch struggle to meet\nrealistic data demands. In this paper, we present a brand new perspective that\nseeks to directly eliminate the harmful effects of NC in pre-trained VLMs.\nSpecifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning\nframework that efficiently enhances VLMs' robustness by forgetting learned\nnoisy knowledge. The key to NCU is learning the hardest negative information,\nwhich can provide explicit unlearning direction for both false positives and\nfalse negatives. Such twin goals unlearning process can be formalized into one\nunified optimal transport objective for fast fine-tuning. We validate our\napproach with the prevailing CLIP model over various downstream tasks.\nRemarkably, NCU surpasses the robust pre-trained method on zero-shot transfer\nwhile with lower computational overhead. The code will be released upon\nacceptance."
    },
    {
        "date": "2025-07",
        "title": "Rectifying Adversarial Sample with Low Entropy Prior for Test-Time Defense",
        "author": "Lina Ma, Xiaowei Fu, Fuxiang Huang, Xinbo Gao, and Lei Zhang",
        "link": "http://arxiv.org/abs/2507.03427v1",
        "abstract": "Existing defense methods fail to defend against unknown attacks and thus\nraise generalization issue of adversarial robustness. To remedy this problem,\nwe attempt to delve into some underlying common characteristics among various\nattacks for generality. In this work, we reveal the commonly overlooked low\nentropy prior (LE) implied in various adversarial samples, and shed light on\nthe universal robustness against unseen attacks in inference phase. LE prior is\nelaborated as two properties across various attacks as shown in Fig. 1 and Fig.\n2: 1) low entropy misclassification for adversarial samples and 2) lower\nentropy prediction for higher attack intensity. This phenomenon stands in stark\ncontrast to the naturally distributed samples. The LE prior can instruct\nexisting test-time defense methods, thus we propose a two-stage REAL approach:\nRectify Adversarial sample based on LE prior for test-time adversarial\nrectification. Specifically, to align adversarial samples more closely with\nclean samples, we propose to first rectify adversarial samples misclassified\nwith low entropy by reverse maximizing prediction entropy, thereby eliminating\ntheir adversarial nature. To ensure the rectified samples can be correctly\nclassified with low entropy, we carry out secondary rectification by forward\nminimizing prediction entropy, thus creating a Max-Min entropy optimization\nscheme. Further, based on the second property, we propose an attack-aware\nweighting mechanism to adaptively adjust the strengths of Max-Min entropy\nobjectives. Experiments on several datasets show that REAL can greatly improve\nthe performance of existing sample rectification models."
    },
    {
        "date": "2025-07",
        "title": "Action Robust Reinforcement Learning via Optimal Adversary Aware Policy Optimization",
        "author": "Buqing Nie, Yangqing Fu, Jingtian Ji, and Yue Gao",
        "link": "http://arxiv.org/abs/2507.03372v1",
        "abstract": "Reinforcement Learning (RL) has achieved remarkable success in sequential\ndecision tasks. However, recent studies have revealed the vulnerability of RL\npolicies to different perturbations, raising concerns about their effectiveness\nand safety in real-world applications. In this work, we focus on the robustness\nof RL policies against action perturbations and introduce a novel framework\ncalled Optimal Adversary-aware Policy Iteration (OA-PI). Our framework enhances\naction robustness under various perturbations by evaluating and improving\npolicy performance against the corresponding optimal adversaries. Besides, our\napproach can be integrated into mainstream DRL algorithms such as Twin Delayed\nDDPG (TD3) and Proximal Policy Optimization (PPO), improving action robustness\neffectively while maintaining nominal performance and sample efficiency.\nExperimental results across various environments demonstrate that our method\nenhances robustness of DRL policies against different action adversaries\neffectively."
    },
    {
        "date": "2025-07",
        "title": "Securing Mixed Rust with Hardware Capabilities",
        "author": "Jason Zhijingcheng Yu, Fangqi Han, Kaustab Choudhury, Trevor E. Carlson, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2507.03344v1",
        "abstract": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments."
    },
    {
        "date": "2025-07",
        "title": "UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis",
        "author": "Zhuo Li, Xuhang Chen, and Shuqiang Wang",
        "link": "http://arxiv.org/abs/2507.03341v1",
        "abstract": "Functional ultrasound (fUS) is a neuroimaging technique known for its high\nspatiotemporal resolution, enabling non-invasive observation of brain activity\nthrough neurovascular coupling. Despite its potential in clinical applications\nsuch as neonatal monitoring and intraoperative guidance, the development of fUS\nfaces challenges related to data scarcity and limitations in generating\nrealistic fUS images. This paper explores the use of a generative adversarial\nnetwork (GAN) framework tailored for fUS image synthesis. The proposed method\nincorporates architectural enhancements, including feature enhancement modules\nand normalization techniques, aiming to improve the fidelity and physiological\nplausibility of generated images. The study evaluates the performance of the\nframework against existing generative models, demonstrating its capability to\nproduce high-quality fUS images under various experimental conditions.\nAdditionally, the synthesized images are assessed for their utility in\ndownstream tasks, showing improvements in classification accuracy when used for\ndata augmentation. Experimental results are based on publicly available fUS\ndatasets, highlighting the framework's effectiveness in addressing data\nlimitations."
    },
    {
        "date": "2025-07",
        "title": "Global Variational Inference Enhanced Robust Domain Adaptation",
        "author": "Lingkun Luo, Shiqiang Hu, and Liming Chen",
        "link": "http://arxiv.org/abs/2507.03291v1",
        "abstract": "Deep learning-based domain adaptation (DA) methods have shown strong\nperformance by learning transferable representations. However, their reliance\non mini-batch training limits global distribution modeling, leading to unstable\nalignment and suboptimal generalization. We propose Global Variational\nInference Enhanced Domain Adaptation (GVI-DA), a framework that learns\ncontinuous, class-conditional global priors via variational inference to enable\nstructure-aware cross-domain alignment. GVI-DA minimizes domain gaps through\nlatent feature reconstruction, and mitigates posterior collapse using global\ncodebook learning with randomized sampling. It further improves robustness by\ndiscarding low-confidence pseudo-labels and generating reliable target-domain\nsamples. Extensive experiments on four benchmarks and thirty-eight DA tasks\ndemonstrate consistent state-of-the-art performance. We also derive the model's\nevidence lower bound (ELBO) and analyze the effects of prior continuity,\ncodebook size, and pseudo-label noise tolerance. In addition, we compare GVI-DA\nwith diffusion-based generative frameworks in terms of optimization principles\nand efficiency, highlighting both its theoretical soundness and practical\nadvantages."
    },
    {
        "date": "2025-07",
        "title": "Securing Transformer-based AI Execution via Unified TEEs and Crypto-protected Accelerators",
        "author": "Jiaqi Xue, Yifei Zhao, Mengxin Zheng, Fan Yao, Yan Solihin, and Qian Lou",
        "link": "http://arxiv.org/abs/2507.03278v2",
        "abstract": "Recent advances in Transformer models, e.g., large language models (LLMs),\nhave brought tremendous breakthroughs in various artificial intelligence (AI)\ntasks, leading to their wide applications in many security-critical domains.\nDue to their unprecedented scale and prohibitively high development cost, these\nmodels have become highly valuable intellectual property for AI stakeholders\nand are increasingly deployed via machine learning as a service (MLaaS).\nHowever, MLaaS often runs on untrusted cloud infrastructure, exposing data and\nmodels to potential breaches. Mainstream protection mechanisms leverage trusted\nexecution environments (TEEs) where confidentiality and integrity for secretive\ndata are shielded using hardware-based encryption and integrity checking.\nUnfortunately, running model inference entirely within TEEs is subject to\nnon-trivial slowdown, which is further exacerbated in LLMs due to the\nsubstantial computation and memory footprint involved. Recent studies reveal\nthat the hybrid TEE-based scheme offloading partial model inference operations\nto the untrusted accelerators (e.g., GPU) is a promising solution. However,\nprior offloading schemes fail to ensure dual protection of data and model in\nTransformer inference, as they cannot securely offload critical operations,\ni.e., Attention and SoftMax, forcing these computations to remain confined\nwithin TEEs. To address these challenges, we propose TwinShield, a framework\nenabling secure Transformer inference in heterogeneous TEE and accelerator\nsystems with dual protection for both model and data. TwinShield offloads ~87%\nof computation to GPUs and delivers 4.0x - 6.1x speedups over previous\napproaches across various Transformer models."
    },
    {
        "date": "2025-07",
        "title": "On Jailbreaking Quantized Language Models Through Fault Injection Attacks",
        "author": "Noureldin Zahran, Ahmad Tahmasivand, Ihsen Alouani, Khaled Khasawneh, and Mohammed E. Fouda",
        "link": "http://arxiv.org/abs/2507.03236v2",
        "abstract": "The safety alignment of Language Models (LMs) is a critical concern, yet\ntheir integrity can be challenged by direct parameter manipulation attacks,\nsuch as those potentially induced by fault injection. As LMs are increasingly\ndeployed using low-precision quantization for efficiency, this paper\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\ndifferent quantization schemes. We propose gradient-guided attacks, including a\ntailored progressive bit-level search algorithm introduced herein and a\ncomparative word-level (single weight update) attack. Our evaluation on\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\nsignificantly influences attack success. While attacks readily achieve high\nsuccess (>80% Attack Success Rate, ASR) on FP16 models, within an attack budget\nof 25 perturbations, FP8 and INT8 models exhibit ASRs below 20% and 50%,\nrespectively. Increasing the perturbation budget up to 150 bit-flips, FP8\nmodels maintained ASR below 65%, demonstrating some resilience compared to INT8\nand INT4 models that have high ASR. In addition, analysis of perturbation\nlocations revealed differing architectural targets across quantization schemes,\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\njailbreaks induced in FP16 models were highly transferable to subsequent\nFP8/INT8 quantization (<5% ASR difference), though INT4 significantly reduced\ntransferred ASR (avg. 35% drop). These findings highlight that while common\nquantization schemes, particularly FP8, increase the difficulty of direct\nparameter manipulation jailbreaks, vulnerabilities can still persist,\nespecially through post-attack quantization."
    },
    {
        "date": "2025-07",
        "title": "Adopting a human developmental visual diet yields robust, shape-based AI vision",
        "author": "Zejin Lu, Sushrut Thorat, Radoslaw M Cichy, and Tim C Kietzmann",
        "link": "http://arxiv.org/abs/2507.03168v1",
        "abstract": "Despite years of research and the dramatic scaling of artificial intelligence\n(AI) systems, a striking misalignment between artificial and human vision\npersists. Contrary to humans, AI heavily relies on texture-features rather than\nshape information, lacks robustness to image distortions, remains highly\nvulnerable to adversarial attacks, and struggles to recognise simple abstract\nshapes within complex backgrounds. To close this gap, we here introduce a\nsolution that arises from a previously underexplored direction: rather than\nscaling up, we take inspiration from how human vision develops from early\ninfancy into adulthood. We quantified the visual maturation by synthesising\ndecades of psychophysical and neurophysiological research into a novel\ndevelopmental visual diet (DVD) for AI vision. We show that guiding AI systems\nthrough this human-inspired curriculum produces models that closely align with\nhuman behaviour on every hallmark of robust vision tested yielding the\nstrongest reported reliance on shape information to date, abstract shape\nrecognition beyond the state of the art, higher robustness to image\ncorruptions, and stronger resilience to adversarial attacks. By outperforming\nhigh parameter AI foundation models trained on orders of magnitude more data,\nwe provide evidence that robust AI vision can be achieved by guiding the way\nhow a model learns, not merely how much it learns, offering a\nresource-efficient route toward safer and more human-like artificial visual\nsystems."
    },
    {
        "date": "2025-07",
        "title": "Adversarial Manipulation of Reasoning Models using Internal Representations",
        "author": "Kureha Yamaguchi, Benjamin Etheridge, and Andy Arditi",
        "link": "http://arxiv.org/abs/2507.03167v1",
        "abstract": "Reasoning models generate chain-of-thought (CoT) tokens before their final\noutput, but how this affects their vulnerability to jailbreak attacks remains\nunclear. While traditional language models make refusal decisions at the\nprompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B\nmakes these decisions within its CoT generation. We identify a linear direction\nin activation space during CoT token generation that predicts whether the model\nwill refuse or comply -- termed the \"caution\" direction because it corresponds\nto cautious reasoning patterns in the generated text. Ablating this direction\nfrom model activations increases harmful compliance, effectively jailbreaking\nthe model. We additionally show that intervening only on CoT token activations\nsuffices to control final outputs, and that incorporating this direction into\nprompt-based attacks improves success rates. Our findings suggest that the\nchain-of-thought itself is a promising new target for adversarial manipulation\nin reasoning models.\n  Code available at https://github.com/ky295/reasoning-manipulation"
    },
    {
        "date": "2025-07",
        "title": "Set Valued Predictions For Robust Domain Generalization",
        "author": "Ron Tsibulsky, Daniel Nevo, and Uri Shalit",
        "link": "http://arxiv.org/abs/2507.03146v1",
        "abstract": "Despite the impressive advancements in modern machine learning, achieving\nrobustness in Domain Generalization (DG) tasks remains a significant challenge.\nIn DG, models are expected to perform well on samples from unseen test\ndistributions (also called domains), by learning from multiple related training\ndistributions. Most existing approaches to this problem rely on single-valued\npredictions, which inherently limit their robustness. We argue that set-valued\npredictors could be leveraged to enhance robustness across unseen domains,\nwhile also taking into account that these sets should be as small as possible.\nWe introduce a theoretical framework defining successful set prediction in the\nDG setting, focusing on meeting a predefined performance criterion across as\nmany domains as possible, and provide theoretical insights into the conditions\nunder which such domain generalization is achievable. We further propose a\npractical optimization method compatible with modern learning architectures,\nthat balances robust performance on unseen domains with small prediction set\nsizes. We evaluate our approach on several real-world datasets from the WILDS\nbenchmark, demonstrating its potential as a promising direction for robust\ndomain generalization."
    },
    {
        "date": "2025-07",
        "title": "Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security",
        "author": "Ricardo Queiroz de Araujo Fernandes, Anderson Santos, Daniel Maier de Carvalho, and Andr\u00e9 Luiz Bandeira Molina",
        "link": "http://arxiv.org/abs/2507.03136v1",
        "abstract": "This article presents an in-depth exploration of the analogy between the\nHolographic Principle in theoretical physics and cyber attack surfaces in\ndigital security. Building on concepts such as black hole entropy and AdS/CFT\nduality, it highlights how complex infrastructures project their\nvulnerabilities onto their external interfaces. The paper draws a parallel\nbetween a black hole's event horizon, which encodes all internal information,\nand the attack surface, which reflects the internal architecture's security\nposture. Additionally, the article outlines how this conceptual framework can\nguide cybersecurity practices, emphasizing strategies such as attack surface\nreduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,\nand the implementation of Zero Trust Architecture. This analogy not only\nprovides a unique perspective on digital security but also underscores the\ncritical importance of boundary-level defenses in protecting vast internal\ninfrastructures."
    },
    {
        "date": "2025-07",
        "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection",
        "author": "Ziqi Miao, Yi Ding, Lijun Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2507.02844v1",
        "abstract": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack."
    },
    {
        "date": "2025-07",
        "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks",
        "author": "Sizhe Chen, Arman Zharmagambetov, David Wagner, and Chuan Guo",
        "link": "http://arxiv.org/abs/2507.02735v1",
        "abstract": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense."
    },
    {
        "date": "2025-07",
        "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents",
        "author": "Jiangrong Wu, Yuhong Nan, Jianliang Wu, Zitong Yao, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2507.02699v1",
        "abstract": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23."
    },
    {
        "date": "2025-07",
        "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures",
        "author": "Frida Sundfeldt, Bianca Widstam, Mahshid Helali Moghadam, Kuo-Yun Liang, and Anders Vesterberg",
        "link": "http://arxiv.org/abs/2507.02607v2",
        "abstract": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application."
    },
    {
        "date": "2025-07",
        "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks",
        "author": "Wei Fan, Kejiang Chen, Chang Liu, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2507.02606v1",
        "abstract": "The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io."
    },
    {
        "date": "2025-07",
        "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising",
        "author": "Hailong Yan, Junjian Huang, and Tingwen Huang",
        "link": "http://arxiv.org/abs/2507.02445v1",
        "abstract": "Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon."
    },
    {
        "date": "2025-07",
        "title": "Toward a Robust and Generalizable Metamaterial Foundation Model",
        "author": "Namjung Kim, Dongseok Lee, Jongbin Yu, Sung Woong Cho, Dosung Lee, Yesol Park, and Youngjoon Hong",
        "link": "http://arxiv.org/abs/2507.02436v1",
        "abstract": "Advances in material functionalities drive innovations across various fields,\nwhere metamaterials-defined by structure rather than composition-are leading\nthe way. Despite the rise of artificial intelligence (AI)-driven design\nstrategies, their impact is limited by task-specific retraining, poor\nout-of-distribution(OOD) generalization, and the need for separate models for\nforward and inverse design. To address these limitations, we introduce the\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\nmodel inspired by large language models. MetaFO learns the underlying mechanics\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\nunseen combinations of material properties and structural responses. It also\nexcels in nonlinear inverse design, even under OOD conditions. By treating\nmetamaterials as an operator that maps material properties to structural\nresponses, MetaFO uncovers intricate structure-property relationships and\nsignificantly expands the design space. This scalable and generalizable\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\nthe way for next-generation innovations."
    },
    {
        "date": "2025-07",
        "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool",
        "author": "Francesco Blefari, Cristian Cosentino, Francesco Aurelio Pironti, Angelo Furfaro, and Fabrizio Marozzo",
        "link": "http://arxiv.org/abs/2507.02424v1",
        "abstract": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Language Models For Threat Detection in IoT Security Logs",
        "author": "Jorge J. Tejero-Fern\u00e1ndez, and Alfonso S\u00e1nchez-Maci\u00e1n",
        "link": "http://arxiv.org/abs/2507.02390v1",
        "abstract": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance."
    },
    {
        "date": "2025-07",
        "title": "A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\\left[^{18}\\text{F}\\right]$FDG PET imaging",
        "author": "Christian Salomonsen, Luigi Tommaso Luppino, Fredrik Aspheim, Kristoffer Wickstr\u00f8m, Elisabeth Wetzer, Michael Kampffmeyer, Rodrigo Berzaghi, Rune Sundset, Robert Jenssen, and Samuel Kuttner",
        "link": "http://arxiv.org/abs/2507.02367v1",
        "abstract": "Dynamic positron emission tomography (PET) and kinetic modeling are pivotal\nin advancing tracer development research in small animal studies. Accurate\nkinetic modeling requires precise input function estimation, traditionally\nachieved via arterial blood sampling. However, arterial cannulation in small\nanimals like mice, involves intricate, time-consuming, and terminal procedures,\nprecluding longitudinal studies. This work proposes a non-invasive, fully\nconvolutional deep learning-based approach (FC-DLIF) to predict input functions\ndirectly from PET imaging, potentially eliminating the need for blood sampling\nin dynamic small-animal PET. The proposed FC-DLIF model includes a spatial\nfeature extractor acting on the volumetric time frames of the PET sequence,\nextracting spatial features. These are subsequently further processed in a\ntemporal feature extractor that predicts the arterial input function. The\nproposed approach is trained and evaluated using images and arterial blood\ncurves from [$^{18}$F]FDG data using cross validation. Further, the model\napplicability is evaluated on imaging data and arterial blood curves collected\nusing two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The\nmodel was further evaluated on data truncated and shifted in time, to simulate\nshorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts\nthe arterial input function with respect to mean squared error and correlation.\nFurthermore, the FC-DLIF model is able to predict the arterial input function\neven from truncated and shifted samples. The model fails to predict the AIF\nfrom samples collected using different radiotracers, as these are not\nrepresented in the training data. Our deep learning-based input function offers\na non-invasive and reliable alternative to arterial blood sampling, proving\nrobust and flexible to temporal shifts and different scan durations."
    },
    {
        "date": "2025-07",
        "title": "Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle",
        "author": "Anbin Wu, Zhiyong Feng, Ruitao Feng, Zhenchang Xing, and Yang Liu",
        "link": "http://arxiv.org/abs/2507.02309v2",
        "abstract": "RESTful APIs facilitate data exchange between applications, but they also\nexpose sensitive resources to potential exploitation. Broken Object Level\nAuthorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,\nexemplifies a critical access control flaw where attackers manipulate API\nparameters to gain unauthorized access. To address this, we propose BOLAZ, a\ndefense framework grounded in zero trust principles. BOLAZ analyzes the data\nflow of resource IDs, pinpointing BOLA attack injection points and determining\nthe associated authorization intervals to prevent horizontal privilege\nescalation. Our approach leverages static taint tracking to categorize APIs\ninto producers and consumers based on how they handle resource IDs. By mapping\nthe propagation paths of resource IDs, BOLAZ captures the context in which\nthese IDs are produced and consumed, allowing for precise identification of\nauthorization boundaries. Unlike defense methods based on common authorization\nmodels, BOLAZ is the first authorization-guided method that adapts defense\nrules based on the system's best-practice authorization logic. We validate\nBOLAZ through empirical research on 10 GitHub projects. The results demonstrate\nBOLAZ's effectiveness in defending against vulnerabilities collected from CVE\nand discovering 35 new BOLA vulnerabilities in the wild, demonstrating its\npracticality in real-world deployments."
    },
    {
        "date": "2025-07",
        "title": "A robust and adaptive MPC formulation for Gaussian process models",
        "author": "Mathieu Dubied, Amon Lahr, Melanie N. Zeilinger, and Johannes K\u00f6hler",
        "link": "http://arxiv.org/abs/2507.02098v1",
        "abstract": "In this paper, we present a robust and adaptive model predictive control\n(MPC) framework for uncertain nonlinear systems affected by bounded\ndisturbances and unmodeled nonlinearities. We use Gaussian Processes (GPs) to\nlearn the uncertain dynamics based on noisy measurements, including those\ncollected during system operation. As a key contribution, we derive robust\npredictions for GP models using contraction metrics, which are incorporated in\nthe MPC formulation. The proposed design guarantees recursive feasibility,\nrobust constraint satisfaction and convergence to a reference state, with high\nprobability. We provide a numerical example of a planar quadrotor subject to\ndifficult-to-model ground effects, which highlights significant improvements\nachieved through the proposed robust prediction method and through online\nlearning."
    },
    {
        "date": "2025-07",
        "title": "AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction",
        "author": "Bin Rao, Haicheng Liao, Yanchen Guan, Chengyue Wang, Bonan Wang, Jiaxun Zhang, and Zhenning Li",
        "link": "http://arxiv.org/abs/2507.01801v2",
        "abstract": "Accurately predicting the future trajectories of traffic agents is essential\nin autonomous driving. However, due to the inherent imbalance in trajectory\ndistributions, tail data in natural datasets often represents more complex and\nhazardous scenarios. Existing studies typically rely solely on a base model's\nprediction error, without considering the diversity and uncertainty of\nlong-tail trajectory patterns. We propose an adaptive momentum and decoupled\ncontrastive learning framework (AMD), which integrates unsupervised and\nsupervised contrastive learning strategies. By leveraging an improved momentum\ncontrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,\nour framework enhances the model's ability to recognize rare and complex\ntrajectories. Additionally, we design four types of trajectory random\naugmentation methods and introduce an online iterative clustering strategy,\nallowing the model to dynamically update pseudo-labels and better adapt to the\ndistributional shifts in long-tail data. We propose three different criteria to\ndefine long-tail trajectories and conduct extensive comparative experiments on\nthe nuScenes and ETH$/$UCY datasets. The results show that AMD not only\nachieves optimal performance in long-tail trajectory prediction but also\ndemonstrates outstanding overall prediction accuracy."
    },
    {
        "date": "2025-07",
        "title": "Robust brain age estimation from structural MRI with contrastive learning",
        "author": "Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, and Pietro Gori",
        "link": "http://arxiv.org/abs/2507.01794v1",
        "abstract": "Estimating brain age from structural MRI has emerged as a powerful tool for\ncharacterizing normative and pathological aging. In this work, we explore\ncontrastive learning as a scalable and robust alternative to supervised\napproaches for brain age estimation. We introduce a novel contrastive loss\nfunction, $\\mathcal{L}^{exp}$, and evaluate it across multiple public\nneuroimaging datasets comprising over 20,000 scans. Our experiments reveal four\nkey findings. First, scaling pre-training on diverse, multi-site data\nconsistently improves generalization performance, cutting external mean\nabsolute error (MAE) nearly in half. Second, $\\mathcal{L}^{exp}$ is robust to\nsite-related confounds, maintaining low scanner-predictability as training size\nincreases. Third, contrastive models reliably capture accelerated aging in\npatients with cognitive impairment and Alzheimer's disease, as shown through\nbrain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike\nsupervised baselines, $\\mathcal{L}^{exp}$ maintains a strong correlation\nbetween brain age accuracy and downstream diagnostic performance, supporting\nits potential as a foundation model for neuroimaging. These results position\ncontrastive learning as a promising direction for building generalizable and\nclinically meaningful brain representations."
    },
    {
        "date": "2025-07",
        "title": "Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation",
        "author": "Zihong Guo, Chen Wan, Yayin Zheng, Hailing Kuang, and Xiaohai Lu",
        "link": "http://arxiv.org/abs/2507.01791v1",
        "abstract": "The transferability of adversarial examples poses a significant security\nchallenge for deep neural networks, which can be attacked without knowing\nanything about them. In this paper, we propose a new Segmented Gaussian Pyramid\n(SGP) attack method to enhance the transferability, particularly against\ndefense models. Unlike existing methods that generally focus on single-scale\nimages, our approach employs Gaussian filtering and three types of downsampling\nto construct a series of multi-scale examples. Then, the gradients of the loss\nfunction with respect to each scale are computed, and their average is used to\ndetermine the adversarial perturbations. The proposed SGP can be considered an\ninput transformation with high extensibility that is easily integrated into\nmost existing adversarial attacks. Extensive experiments demonstrate that in\ncontrast to the state-of-the-art methods, SGP significantly enhances attack\nsuccess rates against black-box defense models, with average attack success\nrates increasing by 2.3% to 32.6%, based only on transferability."
    },
    {
        "date": "2025-07",
        "title": "Signals and Symptoms: ICS Attack Dataset From Railway Cyber Range",
        "author": "Anis Yusof, Yuancheng Liu, Niklaus Kang, Choon Meng Seah, Zhenkai Liang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2507.01768v1",
        "abstract": "The prevalence of cyberattacks on Industrial Control Systems (ICS) has\nhighlighted the necessity for robust security measures and incident response to\nprotect critical infrastructure. This is prominent when Operational Technology\n(OT) systems undergo digital transformation by integrating with Information\nTechnology (IT) systems to enhance operational efficiency, adaptability, and\nsafety. To support analysts in staying abreast of emerging attack patterns,\nthere is a need for ICS datasets that reflect indicators representative of\ncontemporary cyber threats. To address this, we conduct two ICS cyberattack\nsimulations to showcase the impact of trending ICS cyberattacks on a railway\ncyber range that resembles the railway infrastructure. The attack scenario is\ndesigned to blend trending attack trends with attack patterns observed from\nhistorical ICS incidents. The resulting evidence is collected as datasets,\nserving as an essential resource for cyberattack analysis. This captures key\nindicators that are relevant to the current threat landscape, augmenting the\neffectiveness of security systems and analysts to protect against ICS cyber\nthreats."
    },
    {
        "date": "2025-07",
        "title": "RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather",
        "author": "Yuran Wang, Yingping Liang, Yutao Hu, and Ying Fu",
        "link": "http://arxiv.org/abs/2507.01653v1",
        "abstract": "Learning-based stereo matching models struggle in adverse weather conditions\ndue to the scarcity of corresponding training data and the challenges in\nextracting discriminative features from degraded images. These limitations\nsignificantly hinder zero-shot generalization to out-of-distribution weather\nconditions. In this paper, we propose \\textbf{RobuSTereo}, a novel framework\nthat enhances the zero-shot generalization of stereo matching models under\nadverse weather by addressing both data scarcity and feature extraction\nchallenges. First, we introduce a diffusion-based simulation pipeline with a\nstereo consistency module, which generates high-quality stereo data tailored\nfor adverse conditions. By training stereo matching models on our synthetic\ndatasets, we reduce the domain gap between clean and degraded images,\nsignificantly improving the models' robustness to unseen weather conditions.\nThe stereo consistency module ensures structural alignment across synthesized\nimage pairs, preserving geometric integrity and enhancing depth estimation\naccuracy. Second, we design a robust feature encoder that combines a\nspecialized ConvNet with a denoising transformer to extract stable and reliable\nfeatures from degraded images. The ConvNet captures fine-grained local\nstructures, while the denoising transformer refines global representations,\neffectively mitigating the impact of noise, low visibility, and weather-induced\ndistortions. This enables more accurate disparity estimation even under\nchallenging visual conditions. Extensive experiments demonstrate that\n\\textbf{RobuSTereo} significantly improves the robustness and generalization of\nstereo matching models across diverse adverse weather scenarios."
    },
    {
        "date": "2025-07",
        "title": "SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement",
        "author": "Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, and Jiao Ran",
        "link": "http://arxiv.org/abs/2507.01643v1",
        "abstract": "Vision Transformers (ViTs) are essential as foundation backbones in\nestablishing the visual comprehension capabilities of Multimodal Large Language\nModels (MLLMs). Although most ViTs achieve impressive performance through\nimage-text pair-based contrastive learning or self-supervised mechanisms, they\nstruggle to engage in connector-based co-training directly with LLMs due to\npotential parameter initialization conflicts and modality semantic gaps. To\naddress the above challenges, this paper proposes SAILViT, a gradual feature\nlearning-enhanced ViT for facilitating MLLMs to break through performance\nbottlenecks in complex multimodal interactions. SAILViT achieves\ncoarse-to-fine-grained feature alignment and world knowledge infusion with\ngradual feature refinement, which better serves target training demands. We\nperform thorough empirical analyses to confirm the powerful robustness and\ngeneralizability of SAILViT across different dimensions, including parameter\nsizes, model architectures, training strategies, and data scales. Equipped with\nSAILViT, existing MLLMs show significant and consistent performance\nimprovements on the OpenCompass benchmark across extensive downstream tasks.\nSAILViT series models are released at\nhttps://huggingface.co/BytedanceDouyinContent."
    },
    {
        "date": "2025-07",
        "title": "Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems",
        "author": "Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, and Eric Bourbao",
        "link": "http://arxiv.org/abs/2507.01607v1",
        "abstract": "The widespread use of deep learning face recognition raises several security\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\nattacks against real-life, unconstrained systems dealing with images captured\nin the wild remain a blind spot of the literature. This paper conducts the\nfirst system-level study of backdoors in deep learning-based face recognition\nsystems. This paper yields four contributions by exploring the feasibility of\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\nfirst time two backdoor attacks on the face detection task: face generation and\nface landmark shift attacks. We then show that face feature extractors trained\nwith large margin losses also fall victim to backdoor attacks. Combining our\nmodels, we then show using 20 possible pipeline configurations and 15 attack\ncases that a single backdoor enables an attacker to bypass the entire function\nof a system. Finally, we provide stakeholders with several best practices and\ncountermeasures."
    },
    {
        "date": "2025-07",
        "title": "On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE",
        "author": "Koen T. W. Teuwen, Sam Baggen, Emmanuele Zambon, and Luca Allodi",
        "link": "http://arxiv.org/abs/2507.01571v1",
        "abstract": "Automation in Security Operations Centers (SOCs) plays a prominent role in\nalert classification and incident escalation. However, automated methods must\nbe robust in the presence of imbalanced input data, which can negatively affect\nperformance. Additionally, automated methods should make explainable decisions.\nIn this work, we evaluate the effect of label imbalance on the classification\nof network intrusion alerts. As our use-case we employ DeepCASE, the\nstate-of-the-art method for automated alert classification. We show that label\nimbalance impacts both classification performance and correctness of the\nclassification explanations offered by DeepCASE. We conclude tuning the\ndetection rules used in SOCs can significantly reduce imbalance and may benefit\nthe performance and explainability offered by alert post-processing methods\nsuch as DeepCASE. Therefore, our findings suggest that traditional methods to\nimprove the quality of input data can benefit automation."
    },
    {
        "date": "2025-07",
        "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism",
        "author": "Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, and Heng Tao Shen",
        "link": "http://arxiv.org/abs/2507.01513v1",
        "abstract": "By incorporating visual inputs, Multimodal Large Language Models (MLLMs)\nextend LLMs to support visual reasoning. However, this integration also\nintroduces new vulnerabilities, making MLLMs susceptible to multimodal\njailbreak attacks and hindering their safe deployment.Existing defense methods,\nincluding Image-to-Text Translation, Safe Prompting, and Multimodal Safety\nTuning, attempt to address this by aligning multimodal inputs with LLMs'\nbuilt-in safeguards.Yet, they fall short in uncovering root causes of\nmultimodal vulnerabilities, particularly how harmful multimodal tokens trigger\njailbreak in MLLMs? Consequently, they remain vulnerable to text-driven\nmultimodal jailbreaks, often exhibiting overdefensive behaviors and imposing\nheavy training overhead.To bridge this gap, we present an comprehensive\nanalysis of where, how and which harmful multimodal tokens bypass safeguards in\nMLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers\nare responsible for inducing unsafe behaviors, highlighting the potential of\nprecisely removing a small subset of harmful tokens, without requiring safety\ntuning, can still effectively improve safety against jailbreaks. Motivated by\nthis, we propose Safe Prune-then-Restore (SafePTR), an training-free defense\nframework that selectively prunes harmful tokens at vulnerable layers while\nrestoring benign features at subsequent layers.Without incurring additional\ncomputational overhead, SafePTR significantly enhances the safety of MLLMs\nwhile preserving efficiency. Extensive evaluations across three MLLMs and five\nbenchmarks demonstrate SafePTR's state-of-the-art performance in mitigating\njailbreak risks without compromising utility."
    },
    {
        "date": "2025-07",
        "title": "How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations",
        "author": "Marc Damie, Florian Hahn, Andreas Peter, and Jan Ramon",
        "link": "http://arxiv.org/abs/2507.01487v1",
        "abstract": "Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building\nblock for private data aggregation. Recently, the field of differential privacy\nhas revived interest in secure shufflers by highlighting the privacy\namplification they can provide in various computations. Although several works\nargue for the utility of secure shufflers, they often treat them as black\nboxes; overlooking the practical vulnerabilities and performance trade-offs of\nexisting implementations. This leaves a central question open: what makes a\ngood secure shuffler?\n  This survey addresses that question by identifying, categorizing, and\ncomparing 26 secure protocols that realize the necessary shuffling\nfunctionality. To enable a meaningful comparison, we adapt and unify existing\nsecurity definitions into a consistent set of properties. We also present an\noverview of privacy-preserving technologies that rely on secure shufflers,\noffer practical guidelines for selecting appropriate protocols, and outline\npromising directions for future work."
    },
    {
        "date": "2025-07",
        "title": "What Really Matters for Robust Multi-Sensor HD Map Construction?",
        "author": "Xiaoshuai Hao, Yuting Zhao, Yuheng Ji, Luanyuan Dai, Peng Hao, Dingzhe Li, Shuai Cheng, and Rong Yin",
        "link": "http://arxiv.org/abs/2507.01484v1",
        "abstract": "High-definition (HD) map construction methods are crucial for providing\nprecise and comprehensive static environmental information, which is essential\nfor autonomous driving systems. While Camera-LiDAR fusion techniques have shown\npromising results by integrating data from both modalities, existing approaches\nprimarily focus on improving model accuracy and often neglect the robustness of\nperception models, which is a critical aspect for real-world applications. In\nthis paper, we explore strategies to enhance the robustness of multi-modal\nfusion methods for HD map construction while maintaining high accuracy. We\npropose three key components: data augmentation, a novel multi-modal fusion\nmodule, and a modality dropout training strategy. These components are\nevaluated on a challenging dataset containing 10 days of NuScenes data. Our\nexperimental results demonstrate that our proposed methods significantly\nenhance the robustness of baseline methods. Furthermore, our approach achieves\nstate-of-the-art performance on the clean validation set of the NuScenes\ndataset. Our findings provide valuable insights for developing more robust and\nreliable HD map construction models, advancing their applicability in\nreal-world autonomous driving scenarios. Project website:\nhttps://robomap-123.github.io."
    },
    {
        "date": "2025-07",
        "title": "Rational Censorship Attack: Breaking Blockchain with a Blackboard",
        "author": "Michelle Yeo, and Haoqian Zhang",
        "link": "http://arxiv.org/abs/2507.01453v1",
        "abstract": "Censorship resilience is a fundamental assumption underlying the security of\nblockchain protocols. Additionally, the analysis of blockchain security from an\neconomic and game theoretic perspective has been growing in popularity in\nrecent years. In this work, we present a surprising rational censorship attack\non blockchain censorship resilience when we adopt the analysis of blockchain\nsecurity from a game theoretic lens and assume all users are rational. In our\nattack, a colluding group with sufficient voting power censors the remainder\nnodes such that the group alone can gain all the rewards from maintaining the\nblockchain. We show that if nodes are rational, coordinating this attack just\nrequires a public read and write blackboard and we formally model the attack\nusing a game theoretic framework. Furthermore, we note that to ensure the\nsuccess of the attack, nodes need to know the total true voting power held by\nthe colluding group. We prove that the strategy to join the rational censorship\nattack and also for nodes to honestly declare their power is a subgame perfect\nequilibrium in the corresponding extensive form game induced by our attack.\nFinally, we discuss the implications of the attack on blockchain users and\nprotocol designers as well as some potential countermeasures."
    },
    {
        "date": "2025-07",
        "title": "TurboReg: TurboClique for Robust and Efficient Point Cloud Registration",
        "author": "Shaocheng Yan, Pengcheng Shi, Zhenjun Zhao, Kaixin Wang, Kuang Cao, Ji Wu, and Jiayuan Li",
        "link": "http://arxiv.org/abs/2507.01439v2",
        "abstract": "Robust estimation is essential in correspondence-based Point Cloud\nRegistration (PCR). Existing methods using maximal clique search in\ncompatibility graphs achieve high recall but suffer from exponential time\ncomplexity, limiting their use in time-sensitive applications. To address this\nchallenge, we propose a fast and robust estimator, TurboReg, built upon a novel\nlightweight clique, TurboClique, and a highly parallelizable Pivot-Guided\nSearch (PGS) algorithm. First, we define the TurboClique as a 3-clique within a\nhighly-constrained compatibility graph. The lightweight nature of the 3-clique\nallows for efficient parallel searching, and the highly-constrained\ncompatibility graph ensures robust spatial consistency for stable\ntransformation estimation. Next, PGS selects matching pairs with high SC$^2$\nscores as pivots, effectively guiding the search toward TurboCliques with\nhigher inlier ratios. Moreover, the PGS algorithm has linear time complexity\nand is significantly more efficient than the maximal clique search with\nexponential time complexity. Extensive experiments show that TurboReg achieves\nstate-of-the-art performance across multiple real-world datasets, with\nsubstantial speed improvements. For example, on the 3DMatch+FCGF dataset,\nTurboReg (1K) operates $208.22\\times$ faster than 3DMAC while also achieving\nhigher recall. Our code is accessible at\n\\href{https://github.com/Laka-3DV/TurboReg}{\\texttt{TurboReg}}."
    },
    {
        "date": "2025-07",
        "title": "DiffMark: Diffusion-based Robust Watermark Against Deepfakes",
        "author": "Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, and Keqin Li",
        "link": "http://arxiv.org/abs/2507.01428v1",
        "abstract": "Deepfakes pose significant security and privacy threats through malicious\nfacial manipulations. While robust watermarking can aid in authenticity\nverification and source tracking, existing methods often lack the sufficient\nrobustness against Deepfake manipulations. Diffusion models have demonstrated\nremarkable performance in image generation, enabling the seamless fusion of\nwatermark with image during generation. In this study, we propose a novel\nrobust watermarking framework based on diffusion model, called DiffMark. By\nmodifying the training and sampling scheme, we take the facial image and\nwatermark as conditions to guide the diffusion model to progressively denoise\nand generate corresponding watermarked image. In the construction of facial\ncondition, we weight the facial image by a timestep-dependent factor that\ngradually reduces the guidance intensity with the decrease of noise, thus\nbetter adapting to the sampling process of diffusion model. To achieve the\nfusion of watermark condition, we introduce a cross information fusion (CIF)\nmodule that leverages a learnable embedding table to adaptively extract\nwatermark features and integrates them with image features via cross-attention.\nTo enhance the robustness of the watermark against Deepfake manipulations, we\nintegrate a frozen autoencoder during training phase to simulate Deepfake\nmanipulations. Additionally, we introduce Deepfake-resistant guidance that\nemploys specific Deepfake model to adversarially guide the diffusion sampling\nprocess to generate more robust watermarked images. Experimental results\ndemonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.\nOur code will be available at https://github.com/vpsg-research/DiffMark."
    },
    {
        "date": "2025-07",
        "title": "A Compact 16-bit S-box over Tower Field $\\F_{(((2^2)^2)^2)^2}$ with High Security",
        "author": "Bahram Rashidi, and Behrooz Khadem",
        "link": "http://arxiv.org/abs/2507.01423v1",
        "abstract": "This paper introduces a compact and secure 16-bit substitution box (S-box)\ndesigned over the composite field $\\F_{(((2^2)^2)^2)^2}$, optimized for both\nhardware efficiency and cryptographic robustness. The proposed S-box decomposes\noperations into subfields, leveraging a tower field architecture. This enables\nsignificant hardware reduction through optimized field inversion and a low-cost\naffine transformation. Security evaluations confirm resilience against linear,\ndifferential, algebraic and DPA attacks, validated via metrics including\nNonlinearity (32512), Differential Uniformity (4), Algebraic Degree (15),\nTransparency order (15.9875) and SNR (0.34e-08). The hardware results, in 65 nm\nCMOS technology, show the proposed 16-bit S-box has lower hardware resources\nconsumption and lower critical path delay (CPD) than those of other 16-bit\nS-boxes. By integrating high algebraic complexity with resource-efficient\nstructures, this work addresses the growing demand for scalable cryptographic\nprimitives in data-sensitive applications, demonstrating that larger S-boxes\ncan enhance security without proportional hardware costs. The results\nunderscore the viability of composite field-based architectures in balancing\nsecurity and efficiency for modern block ciphers."
    },
    {
        "date": "2025-07",
        "title": "3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation",
        "author": "Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2507.01367v1",
        "abstract": "Physical adversarial attack methods expose the vulnerabilities of deep neural\nnetworks and pose a significant threat to safety-critical scenarios such as\nautonomous driving. Camouflage-based physical attack is a more promising\napproach compared to the patch-based attack, offering stronger adversarial\neffectiveness in complex physical environments. However, most prior work relies\non mesh priors of the target object and virtual environments constructed by\nsimulators, which are time-consuming to obtain and inevitably differ from the\nreal world. Moreover, due to the limitations of the backgrounds in training\nimages, previous methods often fail to produce multi-view robust adversarial\ncamouflage and tend to fall into sub-optimal solutions. Due to these reasons,\nprior work lacks adversarial effectiveness and robustness across diverse\nviewpoints and physical environments. We propose a physical attack framework\nbased on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and\nprecise reconstruction with few images, along with photo-realistic rendering\ncapabilities. Our framework further enhances cross-view robustness and\nadversarial effectiveness by preventing mutual and self-occlusion among\nGaussians and employing a min-max optimization approach that adjusts the\nimaging background of each viewpoint, helping the algorithm filter out\nnon-robust adversarial features. Extensive experiments validate the\neffectiveness and superiority of PGA. Our code is available\nat:https://github.com/TRLou/PGA."
    },
    {
        "date": "2025-07",
        "title": "ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks",
        "author": "Zhiyao Ren, Siyuan Liang, Aishan Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2507.01321v1",
        "abstract": "In-context learning (ICL) has demonstrated remarkable success in large\nlanguage models (LLMs) due to its adaptability and parameter-free nature.\nHowever, it also introduces a critical vulnerability to backdoor attacks, where\nadversaries can manipulate LLM behaviors by simply poisoning a few ICL\ndemonstrations. In this paper, we propose, for the first time, the\ndual-learning hypothesis, which posits that LLMs simultaneously learn both the\ntask-relevant latent concepts and backdoor latent concepts within poisoned\ndemonstrations, jointly influencing the probability of model outputs. Through\ntheoretical analysis, we derive an upper bound for ICL backdoor effects,\nrevealing that the vulnerability is dominated by the concept preference ratio\nbetween the task and the backdoor. Motivated by these findings, we propose\nICLShield, a defense mechanism that dynamically adjusts the concept preference\nratio. Our method encourages LLMs to select clean demonstrations during the ICL\nphase by leveraging confidence and similarity scores, effectively mitigating\nsusceptibility to backdoor attacks. Extensive experiments across multiple LLMs\nand tasks demonstrate that our method achieves state-of-the-art defense\neffectiveness, significantly outperforming existing approaches (+26.02% on\naverage). Furthermore, our method exhibits exceptional adaptability and\ndefensive performance even for closed-source models (e.g., GPT-4)."
    },
    {
        "date": "2025-07",
        "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction",
        "author": "Muhammad Atta ur Rahman, Dooseop Choi, and KyoungWook Min",
        "link": "http://arxiv.org/abs/2507.01308v1",
        "abstract": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2."
    },
    {
        "date": "2025-07",
        "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer",
        "author": "Runze Cheng, Xihang Qiu, Ming Li, Ye Zhang, Chun Li, and Fei Yu",
        "link": "http://arxiv.org/abs/2507.01254v1",
        "abstract": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities."
    },
    {
        "date": "2025-07",
        "title": "FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images",
        "author": "Guang Yang",
        "link": "http://arxiv.org/abs/2507.02995v2",
        "abstract": "The rapid advancement of diffusion models, particularly Stable Diffusion 3.5,\nhas enabled the generation of highly photorealistic synthetic images that pose\nsignificant challenges to existing detection methods. This paper presents\nFreqCross, a novel multi-modal fusion network that combines spatial RGB\nfeatures, frequency domain artifacts, and radial energy distribution patterns\nto achieve robust detection of AI-generated images. Our approach leverages a\nthree-branch architecture: (1) a ResNet-18 backbone for spatial feature\nextraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and\n(3) a multi-layer perceptron for analyzing radial energy profiles. We introduce\na novel radial energy distribution analysis that captures characteristic\nfrequency artifacts inherent in diffusion-generated images, and fuse it with\nspatial and spectral cues via simple feature concatenation followed by a\ncompact classification head. Extensive experiments on a dataset of 10,000\npaired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate\nthat FreqCross achieves 97.8\\% accuracy, outperforming state-of-the-art\nbaselines by 5.2\\%. The frequency analysis further reveals that synthetic\nimages exhibit distinct spectral signatures in the 0.1--0.4 normalised\nfrequency range, providing theoretical foundation for our approach. Code and\npre-trained models are publicly available to facilitate reproducible research."
    },
    {
        "date": "2025-07",
        "title": "Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis",
        "author": "Marius Neuhalfen, Jonathan Grzymisch, and Manuel Sanchez-Gestido",
        "link": "http://arxiv.org/abs/2507.02993v1",
        "abstract": "This work introduces VISY-REVE: a novel pipeline to validate image processing\nalgorithms for Vision-Based Navigation. Traditional validation methods such as\nsynthetic rendering or robotic testbed acquisition suffer from difficult setup\nand slow runtime. Instead, we propose augmenting image datasets in real-time\nwith synthesized views at novel poses. This approach creates continuous\ntrajectories from sparse, pre-existing datasets in open or closed-loop. In\naddition, we introduce a new distance metric between camera poses, the\nBoresight Deviation Distance, which is better suited for view synthesis than\nexisting metrics. Using it, a method for increasing the density of image\ndatasets is developed."
    },
    {
        "date": "2025-07",
        "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations",
        "author": "Jack Nugent, Siyang Wu, Zeyu Ma, Beining Han, Meenal Parakh, Abhishek Joshi, Lingjie Mei, Alexander Raistrick, Xinyuan Li, and Jia Deng",
        "link": "http://arxiv.org/abs/2507.00981v2",
        "abstract": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval."
    },
    {
        "date": "2025-07",
        "title": "Reasoning as an Adaptive Defense for Safety",
        "author": "Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, and Aviral Kumar",
        "link": "http://arxiv.org/abs/2507.00971v1",
        "abstract": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt."
    },
    {
        "date": "2025-07",
        "title": "SafeMap: Robust HD Map Construction from Incomplete Observations",
        "author": "Xiaoshuai Hao, Lingdong Kong, Rong Yin, Pengwei Wang, Jing Zhang, Yunfeng Diao, and Shu Zhao",
        "link": "http://arxiv.org/abs/2507.00861v1",
        "abstract": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability."
    },
    {
        "date": "2025-07",
        "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting",
        "author": "Fatemeh Sadat Daneshmand",
        "link": "http://arxiv.org/abs/2507.00852v1",
        "abstract": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment."
    },
    {
        "date": "2025-07",
        "title": "Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing",
        "author": "Keiichiro Kimura, Hiroki Kuzuno, Yoshiaki Shiraishi, and Masakatu Morii",
        "link": "http://arxiv.org/abs/2507.00847v2",
        "abstract": "Bluetooth is a pervasive wireless communication technology used by billions\nof devices for short-range connectivity. The security of Bluetooth relies on\nthe pairing process, where devices establish shared long-term keys for secure\ncommunications. However, many commercial Bluetooth devices implement automatic\npairing functions to improve user convenience, creating a previously unexplored\nattack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in\nthe automatic pairing functions in commercial Bluetooth devices to achieve\ncompletely silent device link key overwriting. The Stealtooth attack leverages\nthe fact that Bluetooth audio devices automatically transition to pairing mode\nunder specific conditions, enabling attackers to hijack pairing processes\nwithout user awareness or specialized tools. We also extend the attack into the\nMitM Stealtooth attack, combining automatic pairing abuse with power-saving\nmode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major\nmanufacturers, demonstrating widespread vulnerabilities across diverse device\ntypes and manufacturers. Our practical implementation requires only commodity\nhardware and open-source software, highlighting the low barrier to entry for\nattackers.\n  We propose defenses both device and protocol levels, including enhanced user\nnotifications and standardized automatic pairing guidelines. Our findings\nreveal a critical tension between security and usability, showing that current\nautomatic pairing implementations create systematic vulnerabilities. We\nresponsibly disclosed our findings to affected vendors, with several already\nreleasing patches."
    },
    {
        "date": "2025-07",
        "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs",
        "author": "Jiaming Zhang, Rui Hu, Qing Guo, and Wei Yang Bryan Lim",
        "link": "http://arxiv.org/abs/2507.00817v1",
        "abstract": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems."
    },
    {
        "date": "2025-07",
        "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis",
        "author": "Qing Xu, and Xiaohua Xuan",
        "link": "http://arxiv.org/abs/2507.00810v1",
        "abstract": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc."
    },
    {
        "date": "2025-07",
        "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation",
        "author": "Hao Xing, Kai Zhe Boey, Yuankai Wu, Darius Burschka, and Gordon Cheng",
        "link": "http://arxiv.org/abs/2507.00752v1",
        "abstract": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%."
    }
]