[
    {
        "date": "2025-09",
        "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation",
        "author": "Jiajun He, Naoki Sawada, Koichi Miyazaki, and Tomoki Toda",
        "link": "http://arxiv.org/abs/2509.04357v1",
        "abstract": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech."
    },
    {
        "date": "2025-09",
        "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes",
        "author": "Isidoro Tamassia, and Wendelin B\u00f6hmer",
        "link": "http://arxiv.org/abs/2509.04317v1",
        "abstract": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub."
    },
    {
        "date": "2025-09",
        "title": "A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis",
        "author": "Charles Jones, and Ben Glocker",
        "link": "http://arxiv.org/abs/2509.04295v1",
        "abstract": "Machine learning methods often fail when deployed in the real world. Worse\nstill, they fail in high-stakes situations and across socially sensitive lines.\nThese issues have a chilling effect on the adoption of machine learning methods\nin settings such as medical diagnosis, where they are arguably best-placed to\nprovide benefits if safely deployed. In this primer, we introduce the causal\nand statistical structures which induce failure in machine learning methods for\nimage analysis. We highlight two previously overlooked problems, which we call\nthe \\textit{no fair lunch} problem and the \\textit{subgroup separability}\nproblem. We elucidate why today's fair representation learning methods fail to\nadequately solve them and propose potential paths forward for the field."
    },
    {
        "date": "2025-09",
        "title": "Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification",
        "author": "Rudi Coppola, Hovsep Touloujian, Pierfrancesco Ombrini, and Manuel Mazo Jr",
        "link": "http://arxiv.org/abs/2509.04288v1",
        "abstract": "Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell."
    },
    {
        "date": "2025-09",
        "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its Implications Across Security Tasks",
        "author": "Jintao Gu, Haolang Lu, Guoshun Nan, Yihan Lin, Kun Wang, Yuchun Guo, Yigui Cao, and Yang Liu",
        "link": "http://arxiv.org/abs/2509.04091v2",
        "abstract": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
    },
    {
        "date": "2025-09",
        "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
        "author": "Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, and Kevin Roitero",
        "link": "http://arxiv.org/abs/2509.04013v1",
        "abstract": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios."
    },
    {
        "date": "2025-09",
        "title": "Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case",
        "author": "Delphine Longuet, Amira Elouazzani, Alejandro Penacho Riveiros, and Nicola Bastianello",
        "link": "http://arxiv.org/abs/2509.03948v1",
        "abstract": "Failures in satellite components are costly and challenging to address, often\nrequiring significant human and material resources. Embedding a hybrid AI-based\nsystem for fault detection directly in the satellite can greatly reduce this\nburden by allowing earlier detection. However, such systems must operate with\nextremely high reliability. To ensure this level of dependability, we employ\nthe formal verification tool Marabou to verify the local robustness of the\nneural network models used in the AI-based algorithm. This tool allows us to\nquantify how much a model's input can be perturbed before its output behavior\nbecomes unstable, thereby improving trustworthiness with respect to its\nperformance under uncertainty."
    },
    {
        "date": "2025-09",
        "title": "LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding",
        "author": "Yifan Jia, Yanbin Wang, Jianguo Sun, Ye Tian, and Peng Qian",
        "link": "http://arxiv.org/abs/2509.03939v1",
        "abstract": "Current Ethereum fraud detection methods rely on context-independent,\nnumerical transaction sequences, failing to capture semantic of account\ntransactions. Furthermore, the pervasive homogeneity in Ethereum transaction\nrecords renders it challenging to learn discriminative account embeddings.\nMoreover, current self-supervised graph learning methods primarily learn node\nrepresentations through graph reconstruction, resulting in suboptimal\nperformance for node-level tasks like fraud account detection, while these\nmethods also encounter scalability challenges. To tackle these challenges, we\npropose LMAE4Eth, a multi-view learning framework that fuses transaction\nsemantics, masked graph embedding, and expert knowledge. We first propose a\ntransaction-token contrastive language model (TxCLM) that transforms\ncontext-independent numerical transaction records into logically cohesive\nlinguistic representations. To clearly characterize the semantic differences\nbetween accounts, we also use a token-aware contrastive learning pre-training\nobjective together with the masked transaction model pre-training objective,\nlearns high-expressive account representations. We then propose a masked\naccount graph autoencoder (MAGAE) using generative self-supervised learning,\nwhich achieves superior node-level account detection by focusing on\nreconstructing account node features. To enable MAGAE to scale for large-scale\ntraining, we propose to integrate layer-neighbor sampling into the graph, which\nreduces the number of sampled vertices by several times without compromising\ntraining quality. Finally, using a cross-attention fusion network, we unify the\nembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our\nmethod against 21 baseline approaches on three datasets. Experimental results\nshow that our method outperforms the best baseline by over 10% in F1-score on\ntwo of the datasets."
    },
    {
        "date": "2025-09",
        "title": "ShieldMMU: Detecting and Defending against Controlled-Channel Attacks in Shielding Memory System",
        "author": "Gang Liu, Ningjie Li, and Cen Chen",
        "link": "http://arxiv.org/abs/2509.03879v1",
        "abstract": "Intel SGX and hypervisors isolate non-privileged programs from other\nsoftware, ensuring confidentiality and integrity. However, side-channel attacks\ncontinue to threaten Intel SGX's security, enabling malicious OS to manipulate\nPTE present bits, induce page faults, and steal memory access traces. Despite\nextensive research, existing defenses focus on detection or rely on impractical\nsolutions. This paper presents ShieldMMU, a comprehensive solution for\nmitigating controlled channel attacks, balancing compatibility, performance,\nand usability. Leveraging a Merkle Tree-inspired Defense Tree (DD-Tree),\nShieldMMU protects PTE integrity by detecting, locating, and restoring attacked\nPTEs. It identifies MMU page table lookup events and side-channel attacks,\npromptly restoring PTE parameters to prevent page fault traps and ensure secure\nnon-privileged application operation within SGX. Our experiments confirm\nShieldMMU's enhanced security and acceptable latency performance."
    },
    {
        "date": "2025-09",
        "title": "Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations",
        "author": "Hao Nie, Wei Wang, Peng Xu, Wei Chen, Laurence T. Yang, Mauro Conti, and Kaitai Liang",
        "link": "http://arxiv.org/abs/2509.03806v1",
        "abstract": "Dynamic Searchable Symmetric Encryption (DSSE) allows secure searches over a\ndynamic encrypted database but suffers from inherent information leakage.\nExisting passive attacks against DSSE rely on persistent leakage monitoring to\ninfer leakage patterns, whereas this work targets intermittent observation - a\nmore practical threat model. We propose Peekaboo - a new universal attack\nframework - and the core design relies on inferring the search pattern and\nfurther combining it with auxiliary knowledge and other leakage. We instantiate\nPeekaboo over the SOTA attacks, Sap (USENIX' 21) and Jigsaw (USENIX' 24), to\nderive their \"+\" variants (Sap+ and Jigsaw+). Extensive experiments demonstrate\nthat our design achieves >0.9 adjusted rand index for search pattern recovery\nand 90% query accuracy vs. FMA's 30% (CCS' 23). Peekaboo's accuracy scales with\nobservation rounds and the number of observed queries but also it resists SOTA\ncountermeasures, with >40% accuracy against file size padding and >80% against\nobfuscation."
    },
    {
        "date": "2025-09",
        "title": "Learning an Adversarial World Model for Automated Curriculum Generation in MARL",
        "author": "Brennen Hill",
        "link": "http://arxiv.org/abs/2509.03771v1",
        "abstract": "World models that infer and predict environmental dynamics are foundational\nto embodied intelligence. However, their potential is often limited by the\nfinite complexity and implicit biases of hand-crafted training environments. To\ndevelop truly generalizable and robust agents, we need environments that scale\nin complexity alongside the agents learning within them. In this work, we\nreframe the challenge of environment generation as the problem of learning a\ngoal-conditioned, generative world model. We propose a system where a\ngenerative **Attacker** agent learns an implicit world model to synthesize\nincreasingly difficult challenges for a team of cooperative **Defender**\nagents. The Attacker's objective is not passive prediction, but active,\ngoal-driven interaction: it models and generates world states (i.e.,\nconfigurations of enemy units) specifically to exploit the Defenders'\nweaknesses. Concurrently, the embodied Defender team learns a cooperative\npolicy to overcome these generated worlds. This co-evolutionary dynamic creates\na self-scaling curriculum where the world model continuously adapts to\nchallenge the decision-making policy of the agents, providing an effectively\ninfinite stream of novel and relevant training scenarios. We demonstrate that\nthis framework leads to the emergence of complex behaviors, such as the world\nmodel learning to generate flanking and shielding formations, and the defenders\nlearning coordinated focus-fire and spreading tactics. Our findings position\nadversarial co-evolution as a powerful method for learning instrumental world\nmodels that drive agents toward greater strategic depth and robustness."
    },
    {
        "date": "2025-09",
        "title": "Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning",
        "author": "Duy A. Nguyen, Abhi Kamboj, and Minh N. Do",
        "link": "http://arxiv.org/abs/2509.03477v1",
        "abstract": "Addressing missing modalities and limited labeled data is crucial for\nadvancing robust multimodal learning. We propose Robult, a scalable framework\ndesigned to mitigate these challenges by preserving modality-specific\ninformation and leveraging redundancy through a novel information-theoretic\napproach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled\n(PU) contrastive loss that maximizes task-relevant feature alignment while\neffectively utilizing limited labeled data in semi-supervised settings, and (2)\na latent reconstruction loss that ensures unique modality-specific information\nis retained. These strategies, embedded within a modular design, enhance\nperformance across various downstream tasks and ensure resilience to incomplete\nmodalities during inference. Experimental results across diverse datasets\nvalidate that Robult achieves superior performance over existing approaches in\nboth semi-supervised learning and missing modality contexts. Furthermore, its\nlightweight design promotes scalability and seamless integration with existing\narchitectures, making it suitable for real-world multimodal applications."
    },
    {
        "date": "2025-09",
        "title": "Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information",
        "author": "Somiya Chhillar, Mary K. Righi, Rebecca E. Sutter, and Evgenios M. Kornaropoulos",
        "link": "http://arxiv.org/abs/2509.03350v1",
        "abstract": "Despite longstanding criticism from the privacy community, k-anonymity\nremains a widely used standard for data anonymization, mainly due to its\nsimplicity, regulatory alignment, and preservation of data utility. However,\nnon-experts often defend k-anonymity on the grounds that, in the absence of\nauxiliary information, no known attacks can compromise its protections. In this\nwork, we refute this claim by introducing Combinatorial Refinement Attacks\n(CRA), a new class of privacy attacks targeting k-anonymized datasets produced\nusing local recoding. This is the first method that does not rely on external\nauxiliary information or assumptions about the underlying data distribution.\nCRA leverages the utility-optimizing behavior of local recoding anonymization\nof ARX, which is a widely used open-source software for anonymizing data in\nclinical settings, to formulate a linear program that significantly reduces the\nspace of plausible sensitive values. To validate our findings, we partnered\nwith a network of free community health clinics, an environment where (1)\nauxiliary information is indeed hard to find due to the population they serve\nand (2) open-source k-anonymity solutions are attractive due to regulatory\nobligations and limited resources. Our results on real-world clinical microdata\nreveal that even in the absence of external information, established\nanonymization frameworks do not deliver the promised level of privacy, raising\ncritical privacy concerns."
    },
    {
        "date": "2025-09",
        "title": "Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions",
        "author": "Xizhe Zhang, and Jiayang Zhu",
        "link": "http://arxiv.org/abs/2509.03323v1",
        "abstract": "Astrocytes are critical glial cells whose altered morphology and density are\nhallmarks of many neurological disorders. However, their intricate branching\nand stain dependent variability make automated detection of histological images\na highly challenging task. To address these challenges, we propose a hybrid CNN\nTransformer detector that combines local feature extraction with global\ncontextual reasoning. A heatmap guided query mechanism generates spatially\ngrounded anchors for small and faint astrocytes, while a lightweight\nTransformer module improves discrimination in dense clusters. Evaluated on\nALDH1L1 and GFAP stained astrocyte datasets, the model consistently\noutperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with\nfewer false positives, as confirmed by FROC analysis. These results highlight\nthe potential of hybrid CNN Transformer architectures for robust astrocyte\ndetection and provide a foundation for advanced computational pathology tools."
    },
    {
        "date": "2025-09",
        "title": "Evaluating Security Properties in the Execution of Quantum Circuits",
        "author": "Paolo Bernardi, Antonio Brogi, Gian-Luigi Ferrari, and Giuseppe Bisicchia",
        "link": "http://arxiv.org/abs/2509.03306v1",
        "abstract": "Quantum computing is a disruptive technology that is expected to offer\nsignificant advantages in many critical fields (e.g. drug discovery and\ncryptography). The security of information processed by such machines is\ntherefore paramount. Currently, modest Noisy Intermediate-Scale Quantum (NISQ)\ndevices are available. The goal of this work is to identify a practical,\nheuristic methodology to evaluate security properties, such as secrecy and\nintegrity, while using quantum processors owned by potentially untrustworthy\nproviders."
    },
    {
        "date": "2025-09",
        "title": "LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking",
        "author": "Jing Zhang, Siying Tao, Jiao Li, Tianhe Wang, Junchen Wu, Ruqian Hao, Xiaohui Du, Ruirong Tan, and Rui Li",
        "link": "http://arxiv.org/abs/2509.03221v1",
        "abstract": "Organoids replicate organ structure and function, playing a crucial role in\nfields such as tumor treatment and drug screening. Their shape and size can\nindicate their developmental status, but traditional fluorescence labeling\nmethods risk compromising their structure. Therefore, this paper proposes an\nautomated, non-destructive approach to organoid segmentation and tracking. We\nintroduced the LGBP-OrgaNet, a deep learning-based system proficient in\naccurately segmenting, tracking, and quantifying organoids. The model leverages\ncomplementary information extracted from CNN and Transformer modules and\nintroduces the innovative feature fusion module, Learnable Gaussian Band Pass\nFusion, to merge data from two branches. Additionally, in the decoder, the\nmodel proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,\nand finally completes the decoding through progressive concatenation and\nupsampling. SROrga demonstrates satisfactory segmentation accuracy and\nrobustness on organoids segmentation datasets, providing a potent tool for\norganoid research."
    },
    {
        "date": "2025-09",
        "title": "Prompt-Guided Patch UNet-VAE with Adversarial Supervision for Adrenal Gland Segmentation in Computed Tomography Medical Images",
        "author": "Hania Ghouse, and Muzammil Behzad",
        "link": "http://arxiv.org/abs/2509.03188v1",
        "abstract": "Segmentation of small and irregularly shaped abdominal organs, such as the\nadrenal glands in CT imaging, remains a persistent challenge due to severe\nclass imbalance, poor spatial context, and limited annotated data. In this\nwork, we propose a unified framework that combines variational reconstruction,\nsupervised segmentation, and adversarial patch-based feedback to address these\nlimitations in a principled and scalable manner. Our architecture is built upon\na VAE-UNet backbone that jointly reconstructs input patches and generates\nvoxel-level segmentation masks, allowing the model to learn disentangled\nrepresentations of anatomical structure and appearance. We introduce a\npatch-based training pipeline that selectively injects synthetic patches\ngenerated from the learned latent space, and systematically study the effects\nof varying synthetic-to-real patch ratios during training. To further enhance\noutput fidelity, the framework incorporates perceptual reconstruction loss\nusing VGG features, as well as a PatchGAN-style discriminator for adversarial\nsupervision over spatial realism. Comprehensive experiments on the BTCV dataset\ndemonstrate that our approach improves segmentation accuracy, particularly in\nboundary-sensitive regions, while maintaining strong reconstruction quality.\nOur findings highlight the effectiveness of hybrid generative-discriminative\ntraining regimes for small-organ segmentation and provide new insights into\nbalancing realism, diversity, and anatomical consistency in data-scarce\nscenarios."
    },
    {
        "date": "2025-09",
        "title": "AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain",
        "author": "Alma M. Liezenga, Stefan Wijnja, Puck de Haan, Niels W. T. Brink, Jip J. van Stijn, Yori Kamphuis, and Klamer Schutte",
        "link": "http://arxiv.org/abs/2509.03179v1",
        "abstract": "Poisoning attacks pose an increasing threat to the security and robustness of\nArtificial Intelligence systems in the military domain. The widespread use of\nopen-source datasets and pretrained models exacerbates this risk. Despite the\nseverity of this threat, there is limited research on the application and\ndetection of poisoning attacks on object detection systems. This is especially\nproblematic in the military domain, where attacks can have grave consequences.\nIn this work, we both investigate the effect of poisoning attacks on military\nobject detectors in practice, and the best approach to detect these attacks. To\nsupport this research, we create a small, custom dataset featuring military\nvehicles: MilCivVeh. We explore the vulnerability of military object detectors\nfor poisoning attacks by implementing a modified version of the BadDet attack:\na patch-based poisoning attack. We then assess its impact, finding that while a\npositive attack success rate is achievable, it requires a substantial portion\nof the data to be poisoned -- raising questions about its practical\napplicability. To address the detection challenge, we test both specialized\npoisoning detection methods and anomaly detection methods from the visual\nindustrial inspection domain. Since our research shows that both classes of\nmethods are lacking, we introduce our own patch detection method: AutoDetect, a\nsimple, fast, and lightweight autoencoder-based method. Our method shows\npromising results in separating clean from poisoned samples using the\nreconstruction error of image slices, outperforming existing methods, while\nbeing less time- and memory-intensive. We urge that the availability of large,\nrepresentative datasets in the military domain is a prerequisite to further\nevaluate risks of poisoning attacks and opportunities patch detection."
    },
    {
        "date": "2025-09",
        "title": "From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models",
        "author": "Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Xiaoling Wang, and Linlin Wang",
        "link": "http://arxiv.org/abs/2509.03122v1",
        "abstract": "The intellectual property (IP) protection of Large Language Models (LLMs) is\nincreasingly critical. Injecting specialized fingerprints into LLMs through\ninstruction tuning is a common IP protection technique. However, this may\nsignificantly degrade model performance, requires substantial computational\nresources, and exhibits poor persistence under model modifications. We argue\nthat knowledge editing offers a lightweight alternative that is more suitable\nfor fingerprint injection. Accordingly, we apply knowledge editing to\nfingerprint injection for the first time and demonstrate its strong capability.\nDespite using scrambled text as fingerprints to prevent them from being\noverwritten during fine-tuning, degradation still occurs under large-scale\nfine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which reduces fingerprint degradation by constraining the update of the\nfingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even\nin the worst-case scenario. Additionally, we observe that the\nfingerprint-injected models struggle to distinguish between fingerprints and\nsimilar texts due to the high similarity of their features. This finding\nunderscores the urgent need for more robust and fine-grained fingerprinting\ninjection methods for LLMs."
    },
    {
        "date": "2025-09",
        "title": "Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods",
        "author": "Shota Iwamatsu, Koichi Ito, and Takafumi Aoki",
        "link": "http://arxiv.org/abs/2509.03108v1",
        "abstract": "Face recognition systems are robust against environmental changes and noise,\nand thus may be vulnerable to illegal authentication attempts using user face\nphotos, such as spoofing attacks. To prevent such spoofing attacks, it is\ncrucial to discriminate whether the input image is a live user image or a\nspoofed image prior to the face recognition process. Most existing spoofing\nattack detection methods utilize deep learning, which necessitates a\nsubstantial amount of training data. Consequently, if malicious data is\ninjected into a portion of the training dataset, a specific spoofing attack may\nbe erroneously classified as live, leading to false positives.In this paper, we\npropose a novel backdoor poisoning attack method to demonstrate the latent\nthreat of backdoor poisoning within face anti-spoofing detection. The proposed\nmethod enables certain spoofing attacks to bypass detection by embedding\nfeatures extracted from the spoofing attack's face image into a live face image\nwithout inducing any perceptible visual alterations.Through experiments\nconducted on public datasets, we demonstrate that the proposed method\nconstitutes a realistic threat to existing spoofing attack detection systems."
    },
    {
        "date": "2025-09",
        "title": "EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint",
        "author": "Zhenhua Xu, Meng Han, and Wenpeng Xing",
        "link": "http://arxiv.org/abs/2509.03058v1",
        "abstract": "The proliferation of large language models (LLMs) has intensified concerns\nover model theft and license violations, necessitating robust and stealthy\nownership verification. Existing fingerprinting methods either require\nimpractical white-box access or introduce detectable statistical anomalies. We\npropose EverTracer, a novel gray-box fingerprinting framework that ensures\nstealthy and robust model provenance tracing. EverTracer is the first to\nrepurpose Membership Inference Attacks (MIAs) for defensive use, embedding\nownership signals via memorization instead of artificial trigger-output\noverfitting. It consists of Fingerprint Injection, which fine-tunes the model\non any natural language data without detectable artifacts, and Verification,\nwhich leverages calibrated probability variation signal to distinguish\nfingerprinted models. This approach remains robust against adaptive\nadversaries, including input level modification, and model-level modifications.\nExtensive experiments across architectures demonstrate EverTracer's\nstate-of-the-art effectiveness, stealthness, and resilience, establishing it as\na practical solution for securing LLM intellectual property. Our code and data\nare publicly available at https://github.com/Xuzhenhua55/EverTracer."
    },
    {
        "date": "2025-09",
        "title": "TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum",
        "author": "Shuzheng Wang, Yue Huang, Zhuoer Xu, Yuming Huang, and Jing Tang",
        "link": "http://arxiv.org/abs/2509.03037v1",
        "abstract": "Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet\ncomprehensive security analysis remains difficult due to unverified code,\nproxy-based architectures, and the reliance on manual inspection of complex\nexecution traces. Existing approaches fall into two main categories: anomaly\ntransaction detection, which flags suspicious transactions but offers limited\ninsight into specific attack strategies hidden in execution traces inside\ntransactions, and code vulnerability detection, which cannot analyze unverified\ncontracts and struggles to show how identified flaws are exploited in real\nincidents. As a result, analysts must still manually align transaction traces\nwith contract code to reconstruct attack scenarios and conduct forensics. To\naddress this gap, TraceLLM is proposed as a framework that leverages LLMs to\nintegrate execution trace-level detection with decompiled contract code. We\nintroduce a new anomaly execution path identification algorithm and an\nLLM-refined decompile tool to identify vulnerable functions and provide\nexplicit attack paths to LLM. TraceLLM establishes the first benchmark for\njoint trace and contract code-driven security analysis. For comparison, proxy\nbaselines are created by jointly transmitting the results of three\nrepresentative code analysis along with raw traces to LLM. TraceLLM identifies\nattacker and victim addresses with 85.19\\% precision and produces automated\nreports with 70.37\\% factual precision across 27 cases with ground truth expert\nreports, achieving 25.93\\% higher accuracy than the best baseline. Moreover,\nacross 148 real-world Ethereum incidents, TraceLLM automatically generates\nreports with 66.22\\% expert-verified accuracy, demonstrating strong\ngeneralizability."
    },
    {
        "date": "2025-09",
        "title": "Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification",
        "author": "Kaicong Huang, Talha Azfar, Jack M. Reilly, Thomas Guggisberg, and Ruimin Ke",
        "link": "http://arxiv.org/abs/2509.03032v1",
        "abstract": "Person re-identification faces two core challenges: precisely locating the\nforeground target while suppressing background noise and extracting\nfine-grained features from the target region. Numerous visual-only approaches\naddress these issues by partitioning an image and applying attention modules,\nyet they rely on costly manual annotations and struggle with complex\nocclusions. Recent multimodal methods, motivated by CLIP, introduce semantic\ncues to guide visual understanding. However, they focus solely on foreground\ninformation, but overlook the potential value of background cues. Inspired by\nhuman perception, we argue that background semantics are as important as the\nforeground semantics in ReID, as humans tend to eliminate background\ndistractions while focusing on target appearance. Therefore, this paper\nproposes an end-to-end framework that jointly models foreground and background\ninformation within a dual-branch cross-modal feature extraction pipeline. To\nhelp the network distinguish between the two domains, we propose an\nintra-semantic alignment and inter-semantic adversarial learning strategy.\nSpecifically, we align visual and textual features that share the same\nsemantics across domains, while simultaneously penalizing similarity between\nforeground and background features to enhance the network's discriminative\npower. This strategy drives the model to actively suppress noisy background\nregions and enhance attention toward identity-relevant foreground cues.\nComprehensive experiments on two holistic and two occluded ReID benchmarks\ndemonstrate the effectiveness and generality of the proposed method, with\nresults that match or surpass those of current state-of-the-art approaches."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers",
        "author": "Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, and Jun-Cheng Chen",
        "link": "http://arxiv.org/abs/2509.03006v1",
        "abstract": "Recent studies on deep watermarking have predominantly focused on\nin-processing watermarking, which integrates the watermarking process into\nimage generation. However, post-processing watermarking, which embeds\nwatermarks after image generation, offers more flexibility. It can be applied\nto outputs from any generative model (e.g. GANs, diffusion models) without\nneeding access to the model's internal structure. It also allows users to embed\nunique watermarks into individual images. Therefore, this study focuses on\npost-processing watermarking and enhances its robustness by incorporating an\nensemble attack network during training. We construct various versions of\nattack networks using CNN and Transformer in both spatial and frequency domains\nto investigate how each combination influences the robustness of the\nwatermarking model. Our results demonstrate that combining a CNN-based attack\nnetwork in the spatial domain with a Transformer-based attack network in the\nfrequency domain yields the highest robustness in watermarking models.\nExtensive evaluation on the WAVES benchmark, using average bit accuracy as the\nmetric, demonstrates that our ensemble attack network significantly enhances\nthe robustness of baseline watermarking methods under various stress tests. In\nparticular, for the Regeneration Attack defined in WAVES, our method improves\nStegaStamp by 18.743%. The code is released\nat:https://github.com/aiiu-lab/DeepRobustWatermark."
    },
    {
        "date": "2025-09",
        "title": "Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation",
        "author": "Kaoru Otsuka, Yuki Takezawa, and Makoto Yamada",
        "link": "http://arxiv.org/abs/2509.02970v1",
        "abstract": "Federated Learning (FL) allows distributed model training across multiple\nclients while preserving data privacy, but it remains vulnerable to Byzantine\nclients that exhibit malicious behavior. While existing Byzantine-robust FL\nmethods provide strong convergence guarantees (e.g., to a stationary point in\nexpectation) under Byzantine attacks, they typically assume full client\nparticipation, which is unrealistic due to communication constraints and client\navailability. Under partial participation, existing methods fail immediately\nafter the sampled clients contain a Byzantine majority, creating a fundamental\nchallenge for sparse communication. First, we introduce delayed momentum\naggregation, a novel principle where the server aggregates the most recently\nreceived gradients from non-participating clients alongside fresh momentum from\nactive clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with\nMomentum) implements this delayed momentum aggregation principle for\nByzantine-robust FL with partial participation. Then, we establish convergence\nguarantees that recover previous full participation results and match the\nfundamental lower bounds we prove for the partial participation setting.\nExperiments on deep learning tasks validated our theoretical findings, showing\nstable and robust training under various Byzantine attacks."
    },
    {
        "date": "2025-09",
        "title": "STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images",
        "author": "Zeyu Liu, and Shengwei Ding",
        "link": "http://arxiv.org/abs/2509.02952v1",
        "abstract": "Registration of serial whole-slide histopathological images (WSIs) is\ncritical for enabling direct comparison across diverse stains and for preparing\npaired datasets in artificial intelligence (AI) workflows such as virtual\nstaining and biomarker prediction. While existing methods often rely on complex\ndeformable or deep learning approaches that are computationally intensive and\ndifficult to reproduce, lightweight rigid frameworks-sufficient for many\nconsecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial\nTissue Alignment for Rigid registration), a fast and robust open-source\nframework for multi-WSI alignment. STAR integrates stain-conditioned\npreprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive\nkernel scaling, and built-in quality control, achieving reliable rigid\nregistration across heterogeneous tissue types and staining protocols,\nincluding hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,\nPASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).\nEvaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs\nand scanning conditions, STAR consistently produced stable alignments within\nminutes per slide, demonstrating robustness to cross-stain variability and\npartial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC\nalignment, construction of multi-IHC panels, and typical failure modes,\nunderscoring both utility and limitations. Released as an open and lightweight\ntool, STAR provides a reproducible baseline that lowers the barrier for\nclinical adoption and enables large-scale paired data preparation for\nnext-generation computational pathology."
    },
    {
        "date": "2025-09",
        "title": "A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation",
        "author": "Kesen Wang, Daulet Toibazar, and Pedro J. Moreno",
        "link": "http://arxiv.org/abs/2509.02864v1",
        "abstract": "We present an end-to-end, self-evolving adversarial workflow for long-context\nQuestion-Answer (QA) Generation in Arabic. By orchestrating multiple\nspecialized LVLMs: a question generator, an evaluator, and a swarm of answer\ngenerators, our system iteratively refines its own performance without any\nhuman intervention. Starting from raw, multi-page Arabic documents across\ndiverse domains, the question generator produces fine-grained, context-aware\nqueries to be tackled by the answer generator swarm, and the evaluator assesses\nand feeds back quality metrics. This closed-loop cycle enables continuous\nlearning: low-confidence outputs trigger automated re-generation and model\nupdates, progressively enhancing question difficulty and relevance. Moreover,\nwe set the quality metrics as a tunable hyperparameter, enabling question\ngeneration at controllable and customizable difficulty levels. We release\nAraLongBench, a large-scale Arabic benchmark of single- and multi-page\nchallenges spanning hundreds of pages, and demonstrate that our self-evolving\nworkflow substantially outperform static pipelines, markedly boosting the\nlong-context comprehension capabilities of leading Arabic Large Vision Language\nModels (LVLMs). Lastly, we also meticulously architect a fully automated\nagentic workflow for long-context Arabic document collection."
    },
    {
        "date": "2025-09",
        "title": "GPS Spoofing Attacks on Automated Frequency Coordination System in Wi-Fi 6E and Beyond",
        "author": "Yilu Dong, Tianchang Yang, Arupjyoti Bhuyan, and Syed Rafiul Hussain",
        "link": "http://arxiv.org/abs/2509.02824v1",
        "abstract": "The 6 GHz spectrum, recently opened for unlicensed use under Wi-Fi 6E and\nWi-Fi 7, overlaps with frequencies used by mission-critical incumbent systems\nsuch as public safety communications and utility infrastructure. To prevent\ninterference, the FCC mandates the use of Automated Frequency Coordination\n(AFC) systems, which assign safe frequency and power levels based on Wi-Fi\nAccess Point (AP)-reported locations. In this work, we demonstrate that\nGPS-based location reporting, which Wi-Fi APs use, can be spoofed using\ninexpensive, off-the-shelf radio equipment. This enables attackers to\nmanipulate AP behavior, gain unauthorized spectrum access, cause harmful\ninterference, or disable APs entirely by spoofing them into foreign locations.\nWe validate these attacks in a controlled lab setting against a commercial AP\nand evaluate a commercial AFC system under spoofed scenarios. Our findings\nhighlight critical gaps in the security assumptions of AFC and motivate the\nneed for stronger location integrity protections."
    },
    {
        "date": "2025-09",
        "title": "Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs",
        "author": "Naman Deep Singh, Maximilian M\u00fcller, Francesco Croce, and Matthias Hein",
        "link": "http://arxiv.org/abs/2509.02820v1",
        "abstract": "Unlearning in large language models (LLMs) involves precisely removing\nspecific information from a pre-trained model. This is crucial to ensure safety\nof LLMs by deleting private data or harmful knowledge acquired during\npre-training. However, existing unlearning methods often fall short when\nsubjected to thorough evaluation. To overcome this, we introduce JensUn, where\nwe leverage the Jensen-Shannon Divergence as the training objective for both\nforget and retain sets for more stable and effective unlearning dynamics\ncompared to commonly used loss functions. In extensive experiments, JensUn\nachieves better forget-utility trade-off than competing methods, and even\ndemonstrates strong resilience to benign relearning. Additionally, for a\nprecise unlearning evaluation, we introduce LKF, a curated dataset of\nlesser-known facts that provides a realistic unlearning scenario. Finally, to\ncomprehensively test unlearning methods, we propose (i) employing an LLM as\nsemantic judge instead of the standard ROUGE score, and (ii) using worst-case\nunlearning evaluation over various paraphrases and input formats. Our improved\nevaluation framework reveals that many existing methods are less effective than\npreviously thought."
    },
    {
        "date": "2025-09",
        "title": "Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women",
        "author": "Gabriel A. B. do Nascimento, Vincent Dong, Guilherme J. Cavalcante, Alex Nguyen, Tha\u00eds G. do R\u00eago, Yuri Malheiros, Telmo M. Silva Filho, Carla R. Zeballos Torrez, James C. Gee, Anne Marie McCarthy, Andrew D. A. Maidment, and Bruno Barufaldi",
        "link": "http://arxiv.org/abs/2509.02710v1",
        "abstract": "Accurate breast MRI lesion detection is critical for early cancer diagnosis,\nespecially in high-risk populations. We present a classification pipeline that\nadapts a pretrained foundation model, the Medical Slice Transformer (MST), for\nbreast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI).\nLeveraging DINOv2-based self-supervised pretraining, MST generates robust\nper-slice feature embeddings, which are then used to train a Kolmogorov--Arnold\nNetwork (KAN) classifier. The KAN provides a flexible and interpretable\nalternative to conventional convolutional networks by enabling localized\nnonlinear transformations via adaptive B-spline activations. This enhances the\nmodel's ability to differentiate benign from malignant lesions in imbalanced\nand heterogeneous clinical datasets. Experimental results demonstrate that the\nMST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80\n\\pm 0.02 while preserving interpretability through attention-based heatmaps.\nOur findings highlight the effectiveness of combining foundation model\nembeddings with advanced classification strategies for building robust and\ngeneralizable breast MRI analysis tools."
    },
    {
        "date": "2025-09",
        "title": "Preference Robustness for DPO with Applications to Public Health",
        "author": "Cheol Woo Kim, Shresth Verma, Mauricio Tec, and Milind Tambe",
        "link": "http://arxiv.org/abs/2509.02709v1",
        "abstract": "We study an LLM fine-tuning task for designing reward functions for\nsequential resource allocation problems in public health, guided by human\npreferences expressed in natural language. This setting presents a challenging\ntestbed for alignment due to complex and ambiguous objectives and limited data\navailability. We propose DPO-PRO, a robust fine-tuning algorithm based on\nDirect Preference Optimization (DPO), which accounts for uncertainty in the\npreference distribution using a lightweight Distributionally Robust\nOptimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is\nsignificantly less conservative. We evaluate DPO-PRO on a real-world maternal\nmobile health program operated by the non-profit organization ARMMAN, as well\nas on standard alignment benchmarks. Experimental results demonstrate that our\nmethod consistently improves robustness to noisy preference signals compared to\nexisting DPO variants. Moreover, DPO-PRO achieves comparable performance to\nprior self-reflection-based baseline for reward function design, while\nrequiring significantly lower inference-time cost."
    },
    {
        "date": "2025-09",
        "title": "From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation",
        "author": "Tao Wang, Zhenxuan Zhang, Yuanbo Zhou, Xinlin Zhang, Yuanbin Chen, Tao Tan, Guang Yang, and Tong Tong",
        "link": "http://arxiv.org/abs/2509.02419v1",
        "abstract": "The effectiveness of convolutional neural networks in medical image\nsegmentation relies on large-scale, high-quality annotations, which are costly\nand time-consuming to obtain. Even expert-labeled datasets inevitably contain\nnoise arising from subjectivity and coarse delineations, which disrupt feature\nlearning and adversely impact model performance. To address these challenges,\nthis study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which\nintegrates geometric and structural cues to improve robustness against noisy\nannotations. It incorporates a Geometric Distance-Aware module that dynamically\nadjusts pixel-level weights using geometric features, thereby strengthening\nsupervision in reliable regions while suppressing noise. A Structure-Guided\nLabel Refinement module further refines labels with structural priors, and a\nKnowledge Transfer module enriches supervision and improves sensitivity to\nlocal details. To comprehensively assess its effectiveness, we evaluated\nGSD-Net on six publicly available datasets: four containing three types of\nsimulated label noise, and two with multi-expert annotations that reflect\nreal-world subjectivity and labeling inconsistencies. Experimental results\ndemonstrate that GSD-Net achieves state-of-the-art performance under noisy\nannotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen,\n8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of\nthis study are available at https://github.com/ortonwang/GSD-Net."
    },
    {
        "date": "2025-09",
        "title": "A Survey: Towards Privacy and Security in Mobile Large Language Models",
        "author": "Honghui Xu, Kaiyang Li, Wei Chen, Danyang Zheng, Zhiyuan Li, and Zhipeng Cai",
        "link": "http://arxiv.org/abs/2509.02411v1",
        "abstract": "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such\nas healthcare, finance, and education with their ability to perform advanced\nnatural language processing tasks on-the-go. However, the deployment of these\nmodels in mobile and edge environments introduces significant challenges\nrelated to privacy and security due to their resource-intensive nature and the\nsensitivity of the data they process. This survey provides a comprehensive\noverview of privacy and security issues associated with mobile LLMs,\nsystematically categorizing existing solutions such as differential privacy,\nfederated learning, and prompt encryption. Furthermore, we analyze\nvulnerabilities unique to mobile LLMs, including adversarial attacks,\nmembership inference, and side-channel attacks, offering an in-depth comparison\nof their effectiveness and limitations. Despite recent advancements, mobile\nLLMs face unique hurdles in achieving robust security while maintaining\nefficiency in resource-constrained environments. To bridge this gap, we propose\npotential applications, discuss open challenges, and suggest future research\ndirections, paving the way for the development of trustworthy,\nprivacy-compliant, and scalable mobile LLM systems."
    },
    {
        "date": "2025-09",
        "title": "Real-time ML-based Defense Against Malicious Payload in Reconfigurable Embedded Systems",
        "author": "Rye Stahle-Smith, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2509.02387v1",
        "abstract": "The growing use of FPGAs in reconfigurable systems introducessecurity risks\nthrough malicious bitstreams that could cause denial-of-service (DoS), data\nleakage, or covert attacks. We investigated chip-level hardware malicious\npayload in embedded systems and proposed a supervised machine learning method\nto detect malicious bitstreams via static byte-level features. Our approach\ndiverges from existing methods by analyzing bitstreams directly at the binary\nlevel, enabling real-time detection without requiring access to source code or\nnetlists. Bitstreams were sourced from state-of-the-art (SOTA) benchmarks and\nre-engineered to target the Xilinx PYNQ-Z1 FPGA Development Board. Our dataset\nincluded 122 samples of benign and malicious configurations. The data were\nvectorized using byte frequency analysis, compressed using TSVD, and balanced\nusing SMOTE to address class imbalance. The evaluated classifiers demonstrated\nthat Random Forest achieved a macro F1-score of 0.97, underscoring the\nviability of real-time Trojan detection on resource-constrained systems. The\nfinal model was serialized and successfully deployed via PYNQ to enable\nintegrated bitstream analysis."
    },
    {
        "date": "2025-09",
        "title": "Passwords and FIDO2 Are Meant To Be Secret: A Practical Secure Authentication Channel for Web Browsers",
        "author": "Anuj Gautam, Tarun Yadav, Garrett Smith, Kent Seamons, and Scott Ruoti",
        "link": "http://arxiv.org/abs/2509.02289v1",
        "abstract": "Password managers provide significant security benefits to users. However,\nmalicious client-side scripts and browser extensions can steal passwords after\nthe manager has autofilled them into the web page. In this paper, we extend\nprior work by Stock and Johns, showing how password autofill can be hardened to\nprevent these local attacks. We implement our design in the Firefox browser and\nconduct experiments demonstrating that our defense successfully protects\npasswords from XSS attacks and malicious extensions. We also show that our\nimplementation is compatible with 97% of the Alexa top 1000 websites. Next, we\ngeneralize our design, creating a second defense that prevents recently\ndiscovered local attacks against the FIDO2 protocols. We implement this second\ndefense into Firefox, demonstrating that it protects the FIDO2 protocol against\nXSS attacks and malicious extensions. This defense is compatible with all\nwebsites, though it does require a small change (2-3 lines) to web servers\nimplementing FIDO2."
    },
    {
        "date": "2025-09",
        "title": "ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking",
        "author": "Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, and Bernard Ghanem",
        "link": "http://arxiv.org/abs/2509.02182v1",
        "abstract": "We introduce a novel tracklet-based dataset for benchmarking test-time\nadaptation (TTA) methods. The aim of this dataset is to mimic the intricate\nchallenges encountered in real-world environments such as images captured by\nhand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus\non how models face distribution shifts, when deployed, and on violations to the\ncustomary independent-and-identically-distributed (i.i.d.) assumption in\nmachine learning. Yet, these benchmarks fail to faithfully represent realistic\nscenarios that naturally display temporal dependencies, such as how consecutive\nframes from a video stream likely show the same object across time. We address\nthis shortcoming of current datasets by proposing a novel TTA benchmark we call\nthe \"Inherent Temporal Dependencies\" (ITD) dataset. We ensure the instances in\nITD naturally embody temporal dependencies by collecting them from\ntracklets-sequences of object-centric images we compile from the bounding boxes\nof an object-tracking dataset. We use ITD to conduct a thorough experimental\nanalysis of current TTA methods, and shed light on the limitations of these\nmethods when faced with the challenges of temporal dependencies. Moreover, we\nbuild upon these insights and propose a novel adversarial memory initialization\nstrategy to improve memory-based TTA methods. We find this strategy\nsubstantially boosts the performance of various methods on our challenging\nbenchmark."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified Approach to Security and Safety",
        "author": "Wenxiao Zhang, Xiangrui Kong, Conan Dewitt, Thomas Br\u00e4unl, and Jin B. Hong",
        "link": "http://arxiv.org/abs/2509.02163v1",
        "abstract": "Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/"
    },
    {
        "date": "2025-09",
        "title": "From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach",
        "author": "Refat Othman, Diaeddin Rimawi, Bruno Rossi, and Barbara Russo",
        "link": "http://arxiv.org/abs/2509.02077v2",
        "abstract": "In the domain of security, vulnerabilities frequently remain undetected even\nafter their exploitation. In this work, vulnerabilities refer to publicly\ndisclosed flaws documented in Common Vulnerabilities and Exposures (CVE)\nreports. Establishing a connection between attacks and vulnerabilities is\nessential for enabling timely incident response, as it provides defenders with\nimmediate, actionable insights. However, manually mapping attacks to CVEs is\ninfeasible, thereby motivating the need for automation. This paper evaluates 14\nstate-of-the-art (SOTA) sentence transformers for automatically identifying\nvulnerabilities from textual descriptions of attacks. Our results demonstrate\nthat the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior\nclassification performance when using attack Technique descriptions, with an\nF1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was\nobserved that, on average, 56% of the vulnerabilities identified by the MMPNet\nmodel are also represented within the CVE repository in conjunction with an\nattack, while 61% of the vulnerabilities detected by the model correspond to\nthose cataloged in the CVE repository. A manual inspection of the results\nrevealed the existence of 275 predicted links that were not documented in the\nMITRE repositories. Consequently, the automation of linking attack techniques\nto vulnerabilities not only enhances the detection and response capabilities\nrelated to software security incidents but also diminishes the duration during\nwhich vulnerabilities remain exploitable, thereby contributing to the\ndevelopment of more secure systems."
    },
    {
        "date": "2025-09",
        "title": "Forecasting Future DDoS Attacks Using Long Short Term Memory (LSTM) Model",
        "author": "Kong Mun Yeen, Rafidah Md Noor, Wahidah Md Shah, Aslinda Hassan, and Muhammad Umair Munir",
        "link": "http://arxiv.org/abs/2509.02076v1",
        "abstract": "This paper forecasts future Distributed Denial of Service (DDoS) attacks\nusing deep learning models. Although several studies address forecasting DDoS\nattacks, they remain relatively limited compared to detection-focused research.\nBy studying the current trends and forecasting based on newer and updated\ndatasets, mitigation plans against the attacks can be planned and formulated.\nThe methodology used in this research work conforms to the Cross Industry\nStandard Process for Data Mining (CRISP-DM) model."
    },
    {
        "date": "2025-09",
        "title": "Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports",
        "author": "Jian Chen, Jiabao Dou, Jinbao Tian, Yunqi Xu, and Zhou Li",
        "link": "http://arxiv.org/abs/2509.02072v2",
        "abstract": "The automatic classification of occupational accident reports is a critical\nresearch area for enhancing workplace safety and enabling large-scale risk\nanalysis. However, the severe class imbalance inherent in these real-world\ndatasets often compromises the performance of analytical models, particularly\nfor rare but severe incident types, hindering the development of reliable\nautomated systems. To address this challenge, we propose ABEX-RAT, a novel and\nefficient framework that synergizes generative data augmentation with robust\nadversarial training. Our approach first employs a twostep\nabstractive-expansive (ABEX) pipeline, which leverages a large language model\nto distill core incident semantics and then uses a generative model to create\ndiverse, highquality synthetic samples for underrepresented classes.\nSubsequently, a lightweight classifier is trained on the augmented data using a\ncomputationally efficient random adversarial training (RAT) protocol, which\nstochastically applies perturbations to enhance model generalization and\nrobustness without significant overhead. Experimental results on the public\nOSHA dataset demonstrate that our method achieves new state-of-the-art\nperformance, reaching a macro-F1 score of 90.32% and significantly\noutperforming previous SOTA and fine-tuned large model baselines. Our work\nvalidates that this synergistic strategy is a highly effective and efficient\nalternative to brute-force fine-tuning for specialized, imbalanced\nclassification tasks. The code is publicly available\nat:https://github.com/nxcc-lab/ABEX-RAT."
    },
    {
        "date": "2025-09",
        "title": "Targeted Physical Evasion Attacks in the Near-Infrared Domain",
        "author": "Pascal Zimmer, Simon Lachnit, Alexander Jan Zielinski, and Ghassan Karame",
        "link": "http://arxiv.org/abs/2509.02042v1",
        "abstract": "A number of attacks rely on infrared light sources or heat-absorbing material\nto imperceptibly fool systems into misinterpreting visual input in various\nimage recognition applications. However, almost all existing approaches can\nonly mount untargeted attacks and require heavy optimizations due to the\nuse-case-specific constraints, such as location and shape. In this paper, we\npropose a novel, stealthy, and cost-effective attack to generate both targeted\nand untargeted adversarial infrared perturbations. By projecting perturbations\nfrom a transparent film onto the target object with an off-the-shelf infrared\nflashlight, our approach is the first to reliably mount laser-free targeted\nattacks in the infrared domain. Extensive experiments on traffic signs in the\ndigital and physical domains show that our approach is robust and yields higher\nattack success rates in various attack scenarios across bright lighting\nconditions, distances, and angles compared to prior work. Equally important,\nour attack is highly cost-effective, requiring less than US\\$50 and a few tens\nof seconds for deployment. Finally, we propose a novel segmentation-based\ndetection that thwarts our attack with an F1-score of up to 99%."
    },
    {
        "date": "2025-09",
        "title": "See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems",
        "author": "Halima Bouzidi, Haoyu Liu, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.02028v2",
        "abstract": "Language-vision understanding has driven the development of advanced\nperception systems, most notably the emerging paradigm of Referring\nMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOT\nsystems can selectively track objects that satisfy a given semantic\ndescription, guided through Transformer-based spatial-temporal reasoning\nmodules. End-to-End (E2E) RMOT models further unify feature extraction,\ntemporal memory, and spatial reasoning within a Transformer backbone, enabling\nlong-range spatial-temporal modeling over fused textual-visual representations.\nDespite these advances, the reliability and robustness of RMOT remain\nunderexplored. In this paper, we examine the security implications of RMOT\nsystems from a design-logic perspective, identifying adversarial\nvulnerabilities that compromise both the linguistic-visual referring and\ntrack-object matching components. Additionally, we uncover a novel\nvulnerability in advanced RMOT models employing FIFO-based memory, whereby\ntargeted and consistent attacks on their spatial-temporal reasoning introduce\nerrors that persist within the history buffer over multiple subsequent frames.\nWe present VEIL, a novel adversarial framework designed to disrupt the unified\nreferring-matching mechanisms of RMOT models. We show that carefully crafted\ndigital and physical perturbations can corrupt the tracking logic reliability,\ninducing track ID switches and terminations. We conduct comprehensive\nevaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL\nand demonstrate the urgent need for security-aware RMOT designs for critical\nlarge-scale applications."
    },
    {
        "date": "2025-09",
        "title": "A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis",
        "author": "Nsengiyumva Wilberforce",
        "link": "http://arxiv.org/abs/2509.03545v1",
        "abstract": "The proliferation of mobile money in Uganda has been a cornerstone of\nfinancial inclusion, yet its security mechanisms remain a critical concern.\nThis study investigates a significant public response to perceived security\nfailures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an\nincident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a\nvictim's account, withdrew funds, and procured a loan, the campaign revealed\ndeep seated public anxiety over the safety of mobile money. This research\nemploys qualitative analysis to systematically examine the complaints raised\nduring this campaign, extracting key themes related to security vulnerabilities\nand user dissatisfaction. By synthesizing these public sentiments, the paper\nprovides crucial insights into the specific security gaps experienced by users\nand situates these findings within the larger framework of Uganda's mobile\nmoney regulatory and operational environment. The study concludes with\nimplications for providers, policymakers, and the future of secure digital\nfinance in Uganda."
    },
    {
        "date": "2025-09",
        "title": "A Single Detect Focused YOLO Framework for Robust Mitotic Figure Detection",
        "author": "Yasemin Topuz, M. Taha G\u00f6kcan, Serdar Y\u0131ld\u0131z, and Song\u00fcl Varl\u0131",
        "link": "http://arxiv.org/abs/2509.02637v1",
        "abstract": "Mitotic figure detection is a crucial task in computational pathology, as\nmitotic activity serves as a strong prognostic marker for tumor aggressiveness.\nHowever, domain variability that arises from differences in scanners, tissue\ntypes, and staining protocols poses a major challenge to the robustness of\nautomated detection methods. In this study, we introduce SDF-YOLO (Single\nDetect Focused YOLO), a lightweight yet domain-robust detection framework\ndesigned specifically for small, rare targets such as mitotic figures. The\nmodel builds on YOLOv11 with task-specific modifications, including a single\ndetection head aligned with mitotic figure scale, coordinate attention to\nenhance positional sensitivity, and improved cross-channel feature mixing.\nExperiments were conducted on three datasets that span human and canine tumors:\nMIDOG ++, canine cutaneous mast cell tumor (CCMCT), and canine mammary\ncarcinoma (CMC). When submitted to the preliminary test set for the MIDOG2025\nchallenge, SDF-YOLO achieved an average precision (AP) of 0.799, with a\nprecision of 0.758, a recall of 0.775, an F1 score of 0.766, and an FROC-AUC of\n5.793, demonstrating both competitive accuracy and computational efficiency.\nThese results indicate that SDF-YOLO provides a reliable and efficient\nframework for robust mitotic figure detection across diverse domains."
    },
    {
        "date": "2025-09",
        "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators",
        "author": "Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, and Chenghong Wang",
        "link": "http://arxiv.org/abs/2509.01742v1",
        "abstract": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
    },
    {
        "date": "2025-09",
        "title": "Robust Anomaly Detection through Multi-Modal Autoencoder Fusion for Small Vehicle Damage Detection",
        "author": "Sara Khan, Mehmed Y\u00fcksel, and Frank Kirchner",
        "link": "http://arxiv.org/abs/2509.01719v1",
        "abstract": "Wear and tear detection in fleet and shared vehicle systems is a critical\nchallenge, particularly in rental and car-sharing services, where minor damage,\nsuch as dents, scratches, and underbody impacts, often goes unnoticed or is\ndetected too late. Currently, manual inspection methods are the default\napproach but are labour intensive and prone to human error. In contrast,\nstate-of-the-art image-based methods struggle with real-time performance and\nare less effective at detecting underbody damage due to limited visual access\nand poor spatial coverage. This work introduces a novel multi-modal\narchitecture based on anomaly detection to address these issues. Sensors such\nas IMUs and microphones are integrated into a compact device mounted on the\nvehicle's windshield. This approach supports real-time damage detection while\navoiding the need for highly resource-intensive sensors. We developed multiple\nvariants of multi-modal autoencoder-based architectures and evaluated them\nagainst unimodal and state-of-the-art methods. Our ensemble pooling multi-modal\nmodel achieved the highest performance, with a Receiver Operating\nCharacteristic-Area Under Curve (ROC-AUC) of 92%, demonstrating its\neffectiveness in real-world applications. This approach can also be extended to\nother applications, such as improving automotive safety - where it can\nintegrate with airbag systems for efficient deployment - and helping autonomous\nvehicles by complementing other sensors in collision detection."
    },
    {
        "date": "2025-09",
        "title": "Designing a Layered Framework to Secure Data via Improved Multi Stage Lightweight Cryptography in IoT Cloud Systems",
        "author": "Hojjat Farshadinia, Ali Barati, and Hamid Barati",
        "link": "http://arxiv.org/abs/2509.01717v1",
        "abstract": "This paper presents a novel multi-layered hybrid security approach aimed at\nenhancing lightweight encryption for IoT-Cloud systems. The primary goal is to\novercome limitations inherent in conventional solutions such as TPA,\nBlockchain, ECDSA and ZSS which often fall short in terms of data protection,\ncomputational efficiency and scalability. Our proposed method strategically\nrefines and integrates these technologies to address their shortcomings while\nmaximizing their individual strengths. By doing so we create a more reliable\nand high-performance framework for secure data exchange across heterogeneous\nenvironments. The model leverages the combined potential of emerging\ntechnologies, particularly Blockchain, IoT and Cloud computing which when\neffectively coordinated offer significant advancements in security\narchitecture. The proposed framework consists of three core layers: (1) the\nH.E.EZ Layer which integrates improved versions of Hyperledger Fabric,\nEnc-Block and a hybrid ECDSA-ZSS scheme to improve encryption speed,\nscalability and reduce computational cost; (2) the Credential Management Layer\nindependently verifying data integrity and authenticity; and (3) the Time and\nAuditing Layer designed to reduce traffic overhead and optimize performance\nacross dynamic workloads. Evaluation results highlight that the proposed\nsolution not only strengthens security but also significantly improves\nexecution time, communication efficiency and system responsiveness, offering a\nrobust path forward for next-generation IoT-Cloud infrastructures."
    },
    {
        "date": "2025-09",
        "title": "AmphiKey: A Dual-Mode Secure Authenticated Key Encapsulation Protocol for Smart Grid",
        "author": "Kazi Hassan Shakib, Muhammad Asfand Hafeez, and Arslan Munir",
        "link": "http://arxiv.org/abs/2509.01701v1",
        "abstract": "AmphiKey, a dual-mode post-quantum/traditional (PQ/T) hybrid authenticated\nkey exchange mechanism (AKEM) has been designed to secure smart grid\ncommunications against both classical and quantum threats. AmphiKey offers two\ndistinct operational modes within a single framework: an Authenticated Mode and\na Deniable Mode. The Authenticated Mode employs a blackbox approach, combining\nephemeral ML-KEM-768 and X25519 with long-term Raccoon DSA keys to provide\nforward secrecy and strong, non-repudiable authenticity. This design achieves\n\"OR\" confidentiality, where security holds if either of the KEMs is unbroken,\nand robust \"AND\" authenticity. For the signature operation, it leverages the\n'masking-friendly' Raccoon digital signature (DSA), which is specifically\ndesigned for side-channel attack resistance, though this protection is\nlocalized to the signing key and does not provide deniability. In contrast,\nDeniable Mode provides deniable authentication, preserving privacy. The\nprotocol used ML-KEM-768 (AKEM-1), Ephemeral X25519 (AKEM-2), Raccoon-based DSA\n(Rac) (compared performance to ML-DSA-65), and the Ascon cipher to deliver its\nsecurity guarantees. Key contributions include providing a flexible protocol\nwith enhanced security, optional deniability, and efficiency adapted to the\ndiverse needs of the smart grid infrastructure. We present a comprehensive\nperformance evaluation on a heterogeneous testbed featuring a powerful server\nand client (AMD Ryzen 5) and a resource-constrained client (Raspberry Pi). In\nefficient Deniable mode, the full handshake completes in 0.15 ms on the server\nand 0.41 ms on the Raspberry Pi client. In contrast, the Authenticated Mode is\nbottlenecked by the client-side signature generation; the handshake takes 4.8\nms for the Raspberry Pi client to initiate and 0.84 ms for the server to\nverify."
    },
    {
        "date": "2025-09",
        "title": "Challenges and Lessons from MIDOG 2025: A Two-Stage Approach to Domain-Robust Mitotic Figure Detection",
        "author": "Euiseop Song, Jaeyoung Park, and Jaewoo Park",
        "link": "http://arxiv.org/abs/2509.02630v1",
        "abstract": "Mitotic figure detection remains a challenging task in computational\npathology due to domain variability and morphological complexity. This paper\ndescribes our participation in the MIDOG 2025 challenge, focusing on robust\nmitotic figure detection across diverse tissue domains. We developed a\ntwo-stage pipeline combining Faster R-CNN for candidate detection with an\nensemble of three classifiers (DenseNet-121, EfficientNet-v2,\nInceptionResNet-v2) for false positive reduction. Our best submission achieved\nF1-score 0.2237 (Recall: 0.9528, Precision: 0.1267) using a Faster R-CNN\ntrained solely on MIDOG++ dataset. While our high recall demonstrates effective\nmitotic figure detection, the critically low precision (12.67%) reveals\nfundamental challenges in distinguishing true mitoses from morphologically\nsimilar imposters across diverse domains. Analysis of six submission variants\nshowed that subsequent optimization attempts were counterproductive,\nhighlighting the omplexity of domain generalization in histopathology. This\nwork provides valuable insights into the practical challenges of developing\nrobust mitotic figure detection algorithms and emphasizes the importance of\neffective false positive suppression strategies."
    },
    {
        "date": "2025-09",
        "title": "Securing Radiation Detection Systems with an Efficient TinyML-Based IDS for Edge Devices",
        "author": "Einstein Rivas Pizarro, Wajiha Zaheer, Li Yang, Khalil El-Khatib, and Glenn Harvel",
        "link": "http://arxiv.org/abs/2509.01592v1",
        "abstract": "Radiation Detection Systems (RDSs) play a vital role in ensuring public\nsafety across various settings, from nuclear facilities to medical\nenvironments. However, these systems are increasingly vulnerable to\ncyber-attacks such as data injection, man-in-the-middle (MITM) attacks, ICMP\nfloods, botnet attacks, privilege escalation, and distributed denial-of-service\n(DDoS) attacks. Such threats could compromise the integrity and reliability of\nradiation measurements, posing significant public health and safety risks. This\npaper presents a new synthetic radiation dataset and an Intrusion Detection\nSystem (IDS) tailored for resource-constrained environments, bringing Machine\nLearning (ML) predictive capabilities closer to the sensing edge layer of\ncritical infrastructure. Leveraging TinyML techniques, the proposed IDS employs\nan optimized XGBoost model enhanced with pruning, quantization, feature\nselection, and sampling. These TinyML techniques significantly reduce the size\nof the model and computational demands, enabling real-time intrusion detection\non low-resource devices while maintaining a reasonable balance between\nefficiency and accuracy."
    },
    {
        "date": "2025-09",
        "title": "Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing",
        "author": "Zihao Wang, Enneng Yang, Lu Yin, Shiwei Liu, and Li Shen",
        "link": "http://arxiv.org/abs/2509.01548v1",
        "abstract": "Model merging leverages multiple finetuned expert models to construct a\nmulti-task model with low cost, and is gaining increasing attention. However,\nas a growing number of finetuned models become publicly available, concerns\nabout the safety of model merging have emerged. Unauthorized merging may\ninfringe on developers' rights and risk leaking sensitive personal information.\nMost existing methods focus on detecting whether a merged model originates from\na specific source model, but fail to effectively prevent illegal merging. In\nthis paper, we propose MergeLock, an active protection mechanism that disrupts\nmodel parameters to render them unmergeable, thereby directly preventing\nunauthorized model merging. Specifically, leveraging the inherent symmetry of\nthe attention mechanism in Transformer-based models, we randomly sample two\npairs of invertible matrices and apply them to the Query-Key (QK) and\nValue-Output (VO) branches. This transformation keeps the model's output\nunchanged while pushing it away from the shared parameter space of other\nfinetuned models. Extensive experiments across both vision and language tasks\ndemonstrate that MergeLock can degrade the performance of merged models by over\n95% when a protected model is involved in most cases, demonstrating its\neffectiveness. Moreover, we further demonstrate that merged models protected by\nMergeLock cannot be effectively recovered using low-cost restoration methods,\nfurther enhancing robustness against unauthorized merging. The code is\navailable at https://github.com/hetailang/Merge-Lock."
    },
    {
        "date": "2025-09",
        "title": "LiFeChain: Lightweight Blockchain for Secure and Efficient Federated Lifelong Learning in IoT",
        "author": "Handi Chen, Jing Deng, Xiuzhe Wu, Zhihan Jiang, Xinchen Zhang, Xianhao Chen, and Edith C. H. Ngai",
        "link": "http://arxiv.org/abs/2509.01434v1",
        "abstract": "The expansion of Internet of Things (IoT) devices constantly generates\nheterogeneous data streams, driving demand for continuous, decentralized\nintelligence. Federated Lifelong Learning (FLL) provides an ideal solution by\nincorporating federated and lifelong learning to overcome catastrophic\nforgetting. The extended lifecycle of FLL in IoT systems increases their\nvulnerability to persistent attacks, and these risks may be obscured by\nperformance degradation caused by spatial-temporal data heterogeneity.\nMoreover, this problem is exacerbated by the standard single-server\narchitecture, as its single point of failure makes it difficult to maintain a\nreliable audit trail for long-term threats. Blockchain provides a tamper-proof\nfoundation for trustworthy FLL systems. Nevertheless, directly applying\nblockchain to FLL significantly increases computational and retrieval costs\nwith the expansion of the knowledge base, slowing down the training on IoT\ndevices. To address these challenges, we propose LiFeChain, a lightweight\nblockchain for secure and efficient federated lifelong learning by providing a\ntamper-resistant ledger with minimal on-chain disclosure and bidirectional\nverification. To the best of our knowledge, LiFeChain is the first blockchain\ntailored for FLL. LiFeChain incorporates two complementary mechanisms: the\nproof-of-model-correlation (PoMC) consensus on the server, which couples\nlearning and unlearning mechanisms to mitigate negative transfer, and segmented\nzero-knowledge arbitration (Seg-ZA) on the client, which detects and arbitrates\nabnormal committee behavior without compromising privacy. LiFeChain is designed\nas a plug-and-play component that can be seamlessly integrated into existing\nFLL algorithms. Experimental results demonstrate that LiFeChain not only\nenhances model performance against two long-term attacks but also sustains high\nefficiency and scalability."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Partially Relevant Video Retrieval with Robust Alignment Learning",
        "author": "Long Zhang, Peipei Song, Jianfeng Dong, Kun Li, and Xun Yang",
        "link": "http://arxiv.org/abs/2509.01383v1",
        "abstract": "Partially Relevant Video Retrieval (PRVR) aims to retrieve untrimmed videos\npartially relevant to a given query. The core challenge lies in learning robust\nquery-video alignment against spurious semantic correlations arising from\ninherent data uncertainty: 1) query ambiguity, where the query incompletely\ncharacterizes the target video and often contains uninformative tokens, and 2)\npartial video relevance, where abundant query-irrelevant segments introduce\ncontextual noise in cross-modal alignment. Existing methods often focus on\nenhancing multi-scale clip representations and retrieving the most relevant\nclip. However, the inherent data uncertainty in PRVR renders them vulnerable to\ndistractor videos with spurious similarities, leading to suboptimal\nperformance. To fill this research gap, we propose Robust Alignment Learning\n(RAL) framework, which explicitly models the uncertainty in data. Key\ninnovations include: 1) we pioneer probabilistic modeling for PRVR by encoding\nvideos and queries as multivariate Gaussian distributions. This not only\nquantifies data uncertainty but also enables proxy-level matching to capture\nthe variability in cross-modal correspondences; 2) we consider the\nheterogeneous informativeness of query words and introduce learnable confidence\ngates to dynamically weight similarity. As a plug-and-play solution, RAL can be\nseamlessly integrated into the existing architectures. Extensive experiments\nacross diverse retrieval backbones demonstrate its effectiveness."
    },
    {
        "date": "2025-09",
        "title": "An Automated Attack Investigation Approach Leveraging Threat-Knowledge-Augmented Large Language Models",
        "author": "Rujie Dai, Peizhuo Lv, Yujiang Gui, Qiujian Lv, Yuanyuan Qiao, Yan Wang, Degang Sun, Weiqing Huang, Yingjiu Li, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2509.01271v1",
        "abstract": "Advanced Persistent Threats (APTs) are prolonged, stealthy intrusions by\nskilled adversaries that compromise high-value systems to steal data or disrupt\noperations. Reconstructing complete attack chains from massive, heterogeneous\nlogs is essential for effective attack investigation, yet existing methods\nsuffer from poor platform generality, limited generalization to evolving\ntactics, and an inability to produce analyst-ready reports. Large Language\nModels (LLMs) offer strong semantic understanding and summarization\ncapabilities, but in this domain they struggle to capture the long-range,\ncross-log dependencies critical for accurate reconstruction.\n  To solve these problems, we present an LLM-empowered attack investigation\nframework augmented with a dynamically adaptable Kill-Chain-aligned threat\nknowledge base. We organizes attack-relevant behaviors into stage-aware\nknowledge units enriched with semantic annotations, enabling the LLM to\niteratively retrieve relevant intelligence, perform causal reasoning, and\nprogressively expand the investigation context. This process reconstructs\nmulti-phase attack scenarios and generates coherent, human-readable\ninvestigation reports. Evaluated on 15 attack scenarios spanning single-host\nand multi-host environments across Windows and Linux (over 4.3M log events, 7.2\nGB of data), the system achieves an average True Positive Rate (TPR) of 97.1%\nand an average False Positive Rate (FPR) of 0.2%, significantly outperforming\nthe SOTA method ATLAS, which achieves an average TPR of 79.2% and an average\nFPR of 29.1%."
    },
    {
        "date": "2025-09",
        "title": "Geometric origin of adversarial vulnerability in deep learning",
        "author": "Yixiong Ren, Wenkang Du, Jianhui Zhou, and Haiping Huang",
        "link": "http://arxiv.org/abs/2509.01235v1",
        "abstract": "How to balance training accuracy and adversarial robustness has become a\nchallenge since the birth of deep learning. Here, we introduce a geometry-aware\ndeep learning framework that leverages layer-wise local training to sculpt the\ninternal representations of deep neural networks. This framework promotes\nintra-class compactness and inter-class separation in feature space, leading to\nmanifold smoothness and adversarial robustness against white or black box\nattacks. The performance can be explained by an energy model with Hebbian\ncoupling between elements of the hidden representation. Our results thus shed\nlight on the physics of learning in the direction of alignment between\nbiological and artificial intelligence systems. Using the current framework,\nthe deep network can assimilate new information into existing knowledge\nstructures while reducing representation interference."
    },
    {
        "date": "2025-09",
        "title": "RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations",
        "author": "Weihang Ouyang, Min Zhu, Wei Xiong, Si-Wei Liu, and Lu Lu",
        "link": "http://arxiv.org/abs/2509.01234v1",
        "abstract": "Physics-informed neural networks (PINNs) and neural operators, two leading\nscientific machine learning (SciML) paradigms, have emerged as powerful tools\nfor solving partial differential equations (PDEs). Although increasing the\ntraining sample size generally enhances network performance, it also increases\ncomputational costs for physics-informed or data-driven training. To address\nthis trade-off, different sampling strategies have been developed to sample\nmore points in regions with high PDE residuals. However, existing sampling\nmethods are computationally demanding for high-dimensional problems, such as\nhigh-dimensional PDEs or operator learning tasks. Here, we propose a\nresidual-based adversarial-gradient moving sample (RAMS) method, which moves\nsamples according to the adversarial gradient direction to maximize the PDE\nresidual via gradient-based optimization. RAMS can be easily integrated into\nexisting sampling methods. Extensive experiments, ranging from PINN applied to\nhigh-dimensional PDEs to physics-informed and data-driven operator learning\nproblems, have been conducted to demonstrate the effectiveness of RAMS.\nNotably, RAMS represents the first efficient adaptive sampling approach for\noperator learning, marking a significant advancement in the SciML field."
    },
    {
        "date": "2025-09",
        "title": "PRINTER:Deformation-Aware Adversarial Learning for Virtual IHC Staining with In Situ Fidelity",
        "author": "Yizhe Yuan, Bingsen Xue, Bangzheng Pu, Chengxiang Wang, and Cheng Jin",
        "link": "http://arxiv.org/abs/2509.01214v1",
        "abstract": "Tumor spatial heterogeneity analysis requires precise correlation between\nHematoxylin and Eosin H&E morphology and immunohistochemical (IHC) biomarker\nexpression, yet current methods suffer from spatial misalignment in consecutive\nsections, severely compromising in situ pathological interpretation. In order\nto obtain a more accurate virtual staining pattern, We propose PRINTER, a\nweakly-supervised framework that integrates PRototype-drIven content and\nstaiNing patTERn decoupling and deformation-aware adversarial learning\nstrategies designed to accurately learn IHC staining patterns while preserving\nH&E staining details. Our approach introduces three key innovations: (1) A\nprototype-driven staining pattern transfer with explicit content-style\ndecoupling; and (2) A cyclic registration-synthesis framework GapBridge that\nbridges H&E and IHC domains through deformable structural alignment, where\nregistered features guide cross-modal style transfer while synthesized outputs\niteratively refine the registration;(3) Deformation-Aware Adversarial Learning:\nWe propose a training framework where a generator and deformation-aware\nregistration network jointly adversarially optimize a style-focused\ndiscriminator. Extensive experiments demonstrate that PRINTER effectively\nachieves superior performance in preserving H&E staining details and virtual\nstaining fidelity, outperforming state-of-the-art methods. Our work provides a\nrobust and scalable solution for virtual staining, advancing the field of\ncomputational pathology."
    },
    {
        "date": "2025-09",
        "title": "Web Fraud Attacks Against LLM-Driven Multi-Agent Systems",
        "author": "Dezhang Kong, Hujin Peng, Yilun Zhang, Lele Zhao, Zhenhua Xu, Shi Lin, Changting Lin, and Meng Han",
        "link": "http://arxiv.org/abs/2509.01211v1",
        "abstract": "With the proliferation of applications built upon LLM-driven multi-agent\nsystems (MAS), the security of Web links has become a critical concern in\nensuring system reliability. Once an agent is induced to visit a malicious\nwebsite, attackers can use it as a springboard to conduct diverse subsequent\nattacks, which will drastically expand the attack surface. In this paper, we\npropose Web Fraud Attacks, a novel type of attack aiming at inducing MAS to\nvisit malicious websites. We design 11 representative attack variants that\nencompass domain name tampering (homoglyph deception, character substitution,\netc.), link structure camouflage (sub-directory nesting, sub-domain grafting,\nparameter obfuscation, etc.), and other deceptive techniques tailored to\nexploit MAS's vulnerabilities in link validation. Through extensive experiments\non these crafted attack vectors, we demonstrate that Web fraud attacks not only\nexhibit significant destructive potential across different MAS architectures\nbut also possess a distinct advantage in evasion: they circumvent the need for\ncomplex input formats such as jailbreaking, which inherently carry higher\nexposure risks. These results underscore the importance of addressing Web fraud\nattacks in LLM-driven MAS, as their stealthiness and destructiveness pose\nnon-negligible threats to system security and user safety."
    },
    {
        "date": "2025-09",
        "title": "SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment",
        "author": "Bingnan Yang, Mi Zhang, Zhili Zhang, Zhan Zhang, Yuanxin Zhao, Xiangyun Hu, and Jianya Gong",
        "link": "http://arxiv.org/abs/2509.01183v1",
        "abstract": "High-quality image segmentation is fundamental to pixel-level geospatial\nanalysis in remote sensing, necessitating robust segmentation quality\nassessment (SQA), particularly in unsupervised settings lacking ground truth.\nAlthough recent deep learning (DL) based unsupervised SQA methods show\npotential, they often suffer from coarse evaluation granularity, incomplete\nassessments, and poor transferability. To overcome these limitations, this\npaper introduces Panoramic Quality Mapping (PQM) as a new paradigm for\ncomprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning\nframework realizing this approach. SegAssess distinctively formulates SQA as a\nfine-grained, four-class panoramic segmentation task, classifying pixels within\na segmentation mask under evaluation into true positive (TP), false positive\n(FP), true negative (TN), and false negative (FN) categories, thereby\ngenerating a complete quality map. Leveraging an enhanced Segment Anything\nModel (SAM) architecture, SegAssess uniquely employs the input mask as a prompt\nfor effective feature integration via cross-attention. Key innovations include\nan Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF)\nmodule to refine predictions near challenging object edges, and an Augmented\nMixup Sampling (AMS) training strategy integrating multi-source masks to\nsignificantly boost cross-domain robustness and zero-shot transferability.\nComprehensive experiments across 32 datasets derived from 6 sources demonstrate\nthat SegAssess achieves state-of-the-art (SOTA) performance and exhibits\nremarkable zero-shot transferability to unseen masks, establishing PQM via\nSegAssess as a robust and transferable solution for unsupervised SQA. The code\nis available at https://github.com/Yangbn97/SegAssess."
    },
    {
        "date": "2025-09",
        "title": "Efficient and High-Accuracy Secure Two-Party Protocols for a Class of Functions with Real-number Inputs",
        "author": "Hao Guo, Zhaoqian Liu, Liqiang Peng, Shuaishuai Li, Ximing Fu, Weiran Liu, and Lin Qu",
        "link": "http://arxiv.org/abs/2509.01178v1",
        "abstract": "In two-party secret sharing scheme, values are typically encoded as unsigned\nintegers $\\mathsf{uint}(x)$, whereas real-world applications often require\ncomputations on signed real numbers $\\mathsf{Real}(x)$. To enable secure\nevaluation of practical functions, it is essential to computing\n$\\mathsf{Real}(x)$ from shared inputs, as protocols take shares as input. At\nUSENIX'25, Guo et al. proposed an efficient method for computing signed integer\nvalues $\\mathsf{int}(x)$ from shares, which can be extended to compute\n$\\mathsf{Real}(x)$. However, their approach imposes a restrictive input\nconstraint $|x| < \\frac{L}{3}$ for $x \\in \\mathbb{Z}_L$, limiting its\napplicability in real-world scenarios. In this work, we significantly relax\nthis constraint to $|x| < B$ for any $B \\leq \\frac{L}{2}$, where $B =\n\\frac{L}{2}$ corresponding to the natural representable range in $x \\in\n\\mathbb{Z}_L$. This relaxes the restrictions and enables the computation of\n$\\mathsf{Real}(x)$ with loose or no input constraints. Building upon this\nfoundation, we present a generalized framework for designing secure protocols\nfor a broad class of functions, including integer division ($\\lfloor\n\\frac{x}{d} \\rfloor$), trigonometric ($\\sin(x)$) and exponential ($e^{-x}$)\nfunctions. Our experimental evaluation demonstrates that the proposed protocols\nachieve both high efficiency and high accuracy. Notably, our protocol for\nevaluating $e^{-x}$ reduces communication costs to approximately 31% of those\nin SirNN (S&P 21) and Bolt (S&P 24), with runtime speedups of up to $5.53\n\\times$ and $3.09 \\times$, respectively. In terms of accuracy, our protocol\nachieves a maximum ULP error of $1.435$, compared to $2.64$ for SirNN and\n$8.681$ for Bolt."
    },
    {
        "date": "2025-09",
        "title": "Lightening the Load: A Cluster-Based Framework for A Lower-Overhead, Provable Website Fingerprinting Defense",
        "author": "Khashayar Khajavi, and Tao Wang",
        "link": "http://arxiv.org/abs/2509.01046v1",
        "abstract": "Website fingerprinting (WF) attacks remain a significant threat to encrypted\ntraffic, prompting the development of a wide range of defenses. Among these,\ntwo prominent classes are regularization-based defenses, which shape traffic\nusing fixed padding rules, and supersequence-based approaches, which conceal\ntraces among predefined patterns. In this work, we present a unified framework\nfor designing an adaptive WF defense that combines the effectiveness of\nregularization with the provable security of supersequence-style grouping. The\nscheme first extracts behavioural patterns from traces and clusters them into\n(k,l)-diverse anonymity sets; an early-time-series classifier (adapted from\nECDIRE) then switches from a conservative global set of regularization\nparameters to the lighter, set-specific parameters. We instantiate the design\nas Adaptive Tamaraw, a variant of Tamaraw that assigns padding parameters on a\nper-cluster basis while retaining its original information-theoretic guarantee.\nComprehensive experiments on public real-world datasets confirm the benefits.\nBy tuning k, operators can trade privacy for efficiency: in its high-privacy\nmode Adaptive Tamaraw pushes the bound on any attacker's accuracy below 30%,\nwhereas in efficiency-centred settings it cuts total overhead by 99% compared\nwith classic Tamaraw."
    },
    {
        "date": "2025-08",
        "title": "Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play",
        "author": "Zakaria El Jaafari",
        "link": "http://arxiv.org/abs/2509.00923v1",
        "abstract": "Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a\ncornerstone algorithm for solving extensive-form games, but its integration\nwith deep neural networks introduces scale-dependent challenges that manifest\ndifferently across game complexities. This paper presents a comprehensive\nanalysis of how neural MCCFR component effectiveness varies with game scale and\nproposes an adaptive framework for selective component deployment. We identify\nthat theoretical risks such as nonstationary target distribution shifts, action\nsupport collapse, variance explosion, and warm-starting bias have\nscale-dependent manifestation patterns, requiring different mitigation\nstrategies for small versus large games. Our proposed Robust Deep MCCFR\nframework incorporates target networks with delayed updates, uniform\nexploration mixing, variance-aware training objectives, and comprehensive\ndiagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc\nPoker, we demonstrate scale-dependent component effectiveness and identify\ncritical component interactions. The best configuration achieves final\nexploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the\nclassical framework (0.156). On the more complex Leduc Poker domain, selective\ncomponent usage achieves exploitability of 0.2386, a 23.5% improvement over the\nclassical framework (0.3703) and highlighting the importance of careful\ncomponent selection over comprehensive mitigation. Our contributions include:\n(1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled\nmitigation framework with convergence guarantees, (3) comprehensive multi-scale\nexperimental validation revealing scale-dependent component interactions, and\n(4) practical guidelines for deployment in larger games."
    },
    {
        "date": "2025-08",
        "title": "Hybrid AI-Driven Intrusion Detection: Framework Leveraging Novel Feature Selection for Enhanced Network Security",
        "author": "Maryam Mahdi Alhusseini, and Mohammad Reza Feizi Derakhshi",
        "link": "http://arxiv.org/abs/2509.00896v1",
        "abstract": "In today's rapidly evolving digital landscape, safeguarding network\ninfrastructures against cyberattacks has become a critical priority. This\nresearch presents an innovative AI-driven real-time intrusion detection\nframework designed to enhance network security, particularly in Wireless Sensor\nNetworks (WSNs) and Cloud Computing (CC) environments. The system employs\nclassical machine learning models, Logistic Regression, Decision Tree, and\nK-Nearest Neighbors, optimized through the novel Energy Valley Optimization\n(EVO) method using the NSL-KDD dataset. Feature selection significantly reduced\nthe number of input features from 42 to 18 while maintaining strong detection\ncapabilities. The proposed system achieved 98.95 percent accuracy with Decision\nTree, 98.47 percent with K-Nearest Neighbors, and 88.84 percent with Logistic\nRegression. Moreover, high precision, recall, and F1-scores were attained\nacross all classifiers while substantially reducing training and testing times,\nmaking the framework highly suitable for real-time applications. To ensure fair\ndetection across diverse attack types, dataset balancing via downsampling was\napplied to address class imbalance challenges. This investigation focuses on\nthe significance of advancing intrusion detection systems in cloud computing\nand WSNs. Overall, this work advances secure communications by delivering a\nscalable, low-latency, and high-accuracy intrusion detection solution aligned\nwith the latest trends in artificial intelligence, cybersecurity, and real-time\ndigital networks"
    },
    {
        "date": "2025-08",
        "title": "Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization",
        "author": "Xinlei Liu, Tao Hu, Peng Yi, Weitao Han, Jichao Xie, and Baolin Li",
        "link": "http://arxiv.org/abs/2509.00826v1",
        "abstract": "Efficient adversarial attack methods are critical for assessing the\nrobustness of computer vision models. In this paper, we reconstruct the\noptimization objective for generating adversarial examples as \"maximizing the\ndifference between the non-true labels' probability upper bound and the true\nlabel's probability,\" and propose a gradient-based attack method termed\nSequential Difference Maximization (SDM). SDM establishes a three-layer\noptimization framework of \"cycle-stage-step.\" The processes between cycles and\nbetween iterative steps are respectively identical, while optimization stages\ndiffer in terms of loss functions: in the initial stage, the negative\nprobability of the true label is used as the loss function to compress the\nsolution space; in subsequent stages, we introduce the Directional Probability\nDifference Ratio (DPDR) loss function to gradually increase the non-true\nlabels' probability upper bound by compressing the irrelevant labels'\nprobabilities. Experiments demonstrate that compared with previous SOTA\nmethods, SDM not only exhibits stronger attack performance but also achieves\nhigher attack cost-effectiveness. Additionally, SDM can be combined with\nadversarial training methods to enhance their defensive effects. The code is\navailable at https://github.com/X-L-Liu/SDM."
    },
    {
        "date": "2025-08",
        "title": "MAESTROCUT: Dynamic, Noise-Adaptive, and Secure Quantum Circuit Cutting on Near-Term Hardware",
        "author": "Samuel Punch, and Krishnendu Guha",
        "link": "http://arxiv.org/abs/2509.00811v1",
        "abstract": "We present MaestroCut, a closed-loop framework for quantum circuit cutting\nthat adapts partitioning and shot allocation to device drift and workload\nvariation. MaestroCut tracks a variance proxy in real time, triggers re-cutting\nwhen accuracy degrades, and routes shots using topology-aware priors. An online\nestimator cascade (MLE, Bayesian, GP-assisted) selects the lowest-error\nreconstruction within a fixed budget. Tier-1 simulations show consistent\nvariance contraction and reduced mean-squared error versus uniform and\nproportional baselines. Tier-2 emulation with realistic queueing and noise\ndemonstrates stable latency targets, high reliability, and ~1% software\noverhead under stress scenarios. These results indicate that adaptive circuit\ncutting can provide accuracy and efficiency improvements with minimal\noperational cost on near-term hardware."
    },
    {
        "date": "2025-08",
        "title": "Secure and Scalable Face Retrieval via Cancelable Product Quantization",
        "author": "Haomiao Tang, Wenjie Li, Yixiang Qiu, Genping Wang, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2509.00781v1",
        "abstract": "Despite the ubiquity of modern face retrieval systems, their retrieval stage\nis often outsourced to third-party entities, posing significant risks to user\nportrait privacy. Although homomorphic encryption (HE) offers strong security\nguarantees by enabling arithmetic computations in the cipher space, its high\ncomputational inefficiency makes it unsuitable for real-time, real-world\napplications. To address this issue, we propose Cancelable Product\nQuantization, a highly efficient framework for secure face representation\nretrieval. Our hierarchical two-stage framework comprises: (i) a\nhigh-throughput cancelable PQ indexing module for fast candidate filtering, and\n(ii) a fine-grained cipher-space retrieval module for final precise face\nranking. A tailored protection mechanism is designed to secure the indexing\nmodule for cancelable biometric authentication while ensuring efficiency.\nExperiments on benchmark datasets demonstrate that our method achieves an\ndecent balance between effectiveness, efficiency and security."
    },
    {
        "date": "2025-08",
        "title": "Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition",
        "author": "Osama Ahmad, Lukas Wesemann, Fabian Waschkowski, and Zubair Khalid",
        "link": "http://arxiv.org/abs/2509.00703v1",
        "abstract": "Accurate spatiotemporal forecasting is critical for numerous complex systems\nbut remains challenging due to complex volatility patterns and spectral\nentanglement in conventional graph neural networks (GNNs). While\ndecomposition-integrated approaches like variational mode graph convolutional\nnetwork (VMGCN) improve accuracy through signal decomposition, they suffer from\ncomputational inefficiency and manual hyperparameter tuning. To address these\nlimitations, we propose the mode adaptive graph network (MAGN) that transforms\niterative variational mode decomposition (VMD) into a trainable neural module.\nOur key innovations include (1) an unfolded VMD (UVMD) module that replaces\niterative optimization with a fixed-depth network to reduce the decomposition\ntime (by 250x for the LargeST benchmark), and (2) mode-specific learnable\nbandwidth constraints ({\\alpha}k ) adapt spatial heterogeneity and eliminate\nmanual tuning while preventing spectral overlap. Evaluated on the LargeST\nbenchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reduction\nin the prediction error over VMGCN and outperforms state-of-the-art baselines."
    },
    {
        "date": "2025-08",
        "title": "Virtual Reality, Real Problems: A Longitudinal Security Analysis of VR Firmware",
        "author": "Vamsi Shankar Simhadri, Yichang Xiong, Habiba Farrukh, and Xiaokuan Zhang",
        "link": "http://arxiv.org/abs/2509.00662v1",
        "abstract": "Virtual Reality (VR) technology is rapidly growing in recent years. VR\ndevices such as Meta Quest 3 utilize numerous sensors to collect users' data to\nprovide an immersive experience. Due to the extensive data collection and the\nimmersive nature, the security of VR devices is paramount. Leading VR devices\noften adopt and customize Android systems, which makes them susceptible to both\nAndroid-based vulnerabilities and new issues introduced by VR-specific\ncustomizations (e.g., system services to support continuous head and hand\ntracking). While prior work has extensively examined the security properties of\nthe Android software stack, how these security properties hold for VR systems\nremains unexplored. In this paper, we present the first comprehensive security\nanalysis of VR firmware. We collect over 300 versions of VR firmware from two\nmajor vendors, Quest and Pico, and perform a longitudinal analysis across the\nkernel layer, the system binary and library layer, and the application layer.\nWe have identified several security issues in these VR firmware, including\nmissing kernel-level security features, insufficient binary hardening,\ninconsistent permission enforcement, and inadequate SELinux policy enforcement.\nBased on our findings, we synthesize recommendations for VR vendors to improve\nsecurity and trust for VR devices. This paper will act as an important security\nresource for VR developers, users, and vendors, and will also direct future\nadvancements in secure VR ecosystem."
    },
    {
        "date": "2025-08",
        "title": "Face4FairShifts: A Large Image Benchmark for Fairness and Robust Learning across Visual Domains",
        "author": "Yumeng Lin, Dong Li, Xintao Wu, Minglai Shao, Xujiang Zhao, Zhong Chen, and Chen Zhao",
        "link": "http://arxiv.org/abs/2509.00658v1",
        "abstract": "Ensuring fairness and robustness in machine learning models remains a\nchallenge, particularly under domain shifts. We present Face4FairShifts, a\nlarge-scale facial image benchmark designed to systematically evaluate\nfairness-aware learning and domain generalization. The dataset includes 100,000\nimages across four visually distinct domains with 39 annotations within 14\nattributes covering demographic and facial features. Through extensive\nexperiments, we analyze model performance under distribution shifts and\nidentify significant gaps. Our findings emphasize the limitations of existing\nrelated datasets and the need for more effective fairness-aware domain\nadaptation techniques. Face4FairShifts provides a comprehensive testbed for\nadvancing equitable and reliable AI systems. The dataset is available online at\nhttps://meviuslab.github.io/Face4FairShifts/."
    },
    {
        "date": "2025-08",
        "title": "RoFt-Mol: Benchmarking Robust Fine-Tuning with Molecular Graph Foundation Models",
        "author": "Shikun Liu, Deyu Zou, Nima Shoghi, Victor Fung, Kai Liu, and Pan Li",
        "link": "http://arxiv.org/abs/2509.00614v1",
        "abstract": "In the era of foundation models, fine-tuning pre-trained models for specific\ndownstream tasks has become crucial. This drives the need for robust\nfine-tuning methods to address challenges such as model overfitting and sparse\nlabeling. Molecular graph foundation models (MGFMs) face unique difficulties\nthat complicate fine-tuning. These models are limited by smaller pre-training\ndatasets and more severe data scarcity for downstream tasks, both of which\nrequire enhanced model generalization. Moreover, MGFMs must accommodate diverse\nobjectives, including both regression and classification tasks. To better\nunderstand and improve fine-tuning techniques under these conditions, we\nclassify eight fine-tuning methods into three mechanisms: weight-based,\nrepresentation-based, and partial fine-tuning. We benchmark these methods on\ndownstream regression and classification tasks across supervised and\nself-supervised pre-trained models in diverse labeling settings. This extensive\nevaluation provides valuable insights and informs the design of a refined\nrobust fine-tuning method, ROFT-MOL. This approach combines the strengths of\nsimple post-hoc weight interpolation with more complex weight ensemble\nfine-tuning methods, delivering improved performance across both task types\nwhile maintaining the ease of use inherent in post-hoc weight interpolation."
    },
    {
        "date": "2025-08",
        "title": "FreeTalk:A plug-and-play and black-box defense against speech synthesis attacks",
        "author": "Yuwen Pu, Zhou Feng, Chunyi Zhou, Jiahao Chen, Chunqiang Hu, Haibo Hu, and Shouling Ji",
        "link": "http://arxiv.org/abs/2509.00561v1",
        "abstract": "Recently, speech assistant and speech verification have been used in many\nfields, which brings much benefit and convenience for us. However, when we\nenjoy these speech applications, our speech may be collected by attackers for\nspeech synthesis. For example, an attacker generates some inappropriate\npolitical opinions with the characteristic of the victim's voice by obtaining a\npiece of the victim's speech, which will greatly influence the victim's\nreputation. Specifically, with the appearance of some zero-shot voice\nconversion methods, the cost of speech synthesis attacks has been further\nreduced, which also brings greater challenges to user voice security and\nprivacy. Some researchers have proposed the corresponding privacy-preserving\nmethods. However, the existing approaches have some non-negligible drawbacks:\nlow transferability and robustness, high computational overhead. These\ndeficiencies seriously limit the existing method deployed in practical\nscenarios. Therefore, in this paper, we propose a lightweight, robust,\nplug-and-play privacy preservation method against speech synthesis attacks in a\nblack-box setting. Our method generates and adds a frequency-domain\nperturbation to the original speech to achieve privacy protection and high\nspeech quality. Then, we present a data augmentation strategy and noise\nsmoothing mechanism to improve the robustness of the proposed method. Besides,\nto reduce the user's defense overhead, we also propose a novel identity-wise\nprotection mechanism. It can generate a universal perturbation for one speaker\nand support privacy preservation for speech of any length. Finally, we conduct\nextensive experiments on 5 speech synthesis models, 5 speech verification\nmodels, 1 speech recognition model, and 2 datasets. The experimental results\ndemonstrate that our method has satisfying privacy-preserving performance, high\nspeech quality, and utility."
    },
    {
        "date": "2025-08",
        "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models",
        "author": "Yuting Tan, Xuying Li, Zhuo Li, Huizhen Shu, and Peikang Hu",
        "link": "http://arxiv.org/abs/2509.00391v1",
        "abstract": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation."
    },
    {
        "date": "2025-08",
        "title": "Unifying Adversarial Perturbation for Graph Neural Networks",
        "author": "Jinluan Yang, Ruihao Zhang, Zhengyu Chen, Fei Wu, and Kun Kuang",
        "link": "http://arxiv.org/abs/2509.00387v1",
        "abstract": "This paper studies the vulnerability of Graph Neural Networks (GNNs) to\nadversarial attacks on node features and graph structure. Various methods have\nimplemented adversarial training to augment graph data, aiming to bolster the\nrobustness and generalization of GNNs. These methods typically involve applying\nperturbations to the node feature, weights, or graph structure and subsequently\nminimizing the loss by learning more robust graph model parameters under the\nadversarial perturbations. Despite the effectiveness of adversarial training in\nenhancing GNNs' robustness and generalization abilities, its application has\nbeen largely confined to specific datasets and GNN types. In this paper, we\npropose a novel method, PerturbEmbedding, that integrates adversarial\nperturbation and training, enhancing GNNs' resilience to such attacks and\nimproving their generalization ability. PerturbEmbedding performs perturbation\noperations directly on every hidden embedding of GNNs and provides a unified\nframework for most existing perturbation strategies/methods. We also offer a\nunified perspective on the forms of perturbations, namely random and\nadversarial perturbations. Through experiments on various datasets using\ndifferent backbone models, we demonstrate that PerturbEmbedding significantly\nimproves both the robustness and generalization abilities of GNNs,\noutperforming existing methods. The rejection of both random (non-targeted) and\nadversarial (targeted) perturbations further enhances the backbone model's\nperformance."
    },
    {
        "date": "2025-08",
        "title": "HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization",
        "author": "Joohyun Chang, Soyeon Hong, Hyogun Lee, Seong Jong Ha, Dongho Lee, Seong Tae Kim, and Jinwoo Choi",
        "link": "http://arxiv.org/abs/2509.00385v1",
        "abstract": "In this work, we tackle the egocentric visual query localization (VQL), where\na model should localize the query object in a long-form egocentric video.\nFrequent and abrupt viewpoint changes in egocentric videos cause significant\nobject appearance variations and partial occlusions, making it difficult for\nexisting methods to achieve accurate localization. To tackle these challenges,\nwe introduce Hierarchical, Egocentric and RObust Visual Query Localization\n(HERO-VQL), a novel method inspired by human cognitive process in object\nrecognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric\nAugmentation based Consistency Training (EgoACT). Top-down Attention Guidance\nrefines the attention mechanism by leveraging the class token for high-level\ncontext and principal component score maps for fine-grained localization. To\nenhance learning in diverse and challenging matching scenarios, EgoAug enhances\nquery diversity by replacing the query with a randomly selected corresponding\nobject from groundtruth annotations and simulates extreme viewpoint changes by\nreordering video frames. Additionally, CT loss enforces stable object\nlocalization across different augmentation scenarios. Extensive experiments on\nVQ2D dataset validate that HERO-VQL effectively handles egocentric challenges,\nsignificantly outperforming baselines."
    },
    {
        "date": "2025-08",
        "title": "Activation Steering Meets Preference Optimization: Defense Against Jailbreaks in Vision Language Models",
        "author": "Sihao Wu, Gaojie Jin, Wei Huang, Jianhong Wang, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.00373v1",
        "abstract": "Vision Language Models (VLMs) have demonstrated impressive capabilities in\nintegrating visual and textual information for understanding and reasoning, but\nremain highly vulnerable to adversarial attacks. While activation steering has\nemerged as a promising defence, existing approaches often rely on task-specific\ncontrastive prompts to extract harmful directions, which exhibit suboptimal\nperformance and can degrade visual grounding performance. To address these\nlimitations, we propose \\textit{Sequence-Level Preference Optimization} for VLM\n(\\textit{SPO-VLM}), a novel two-stage defense framework that combines\nactivation-level intervention with policy-level optimization to enhance model\nrobustness. In \\textit{Stage I}, we compute adaptive layer-specific steering\nvectors from diverse data sources, enabling generalized suppression of harmful\nbehaviors during inference. In \\textit{Stage II}, we refine these steering\nvectors through a sequence-level preference optimization process. This stage\nintegrates automated toxicity assessment, as well as visual-consistency rewards\nbased on caption-image alignment, to achieve safe and semantically grounded\ntext generation. The two-stage structure of SPO-VLM balances efficiency and\neffectiveness by combining a lightweight mitigation foundation in Stage I with\ndeeper policy refinement in Stage II. Extensive experiments shown SPO-VLM\nenhances safety against attacks via activation steering and preference\noptimization, while maintaining strong performance on benign tasks without\ncompromising visual understanding capabilities. We will release our code, model\nweights, and evaluation toolkit to support reproducibility and future research.\n\\textcolor{red}{Warning: This paper may contain examples of offensive or\nharmful text and images.}"
    },
    {
        "date": "2025-08",
        "title": "MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification",
        "author": "Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, and Jia Wu",
        "link": "http://arxiv.org/abs/2509.00311v1",
        "abstract": "Domain generalization in computational histopathology is hindered by\nheterogeneity in whole slide images (WSIs), caused by variations in tissue\npreparation, staining, and imaging conditions across institutions. Unlike\nmachine learning systems, pathologists rely on domain-invariant morphological\ncues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,\nchromatin texture, spatial disorganization), structural atypia (abnormal\narchitecture and gland formation), and overall morphological atypia that remain\ndiagnostic across diverse settings. Motivated by this, we hypothesize that\nexplicitly modeling biologically robust nuclear morphology and spatial\norganization will enable the learning of cancer representations that are\nresilient to domain shifts. We propose MorphGen (Morphology-Guided\nGeneralization), a method that integrates histopathology images, augmentations,\nand nuclear segmentation masks within a supervised contrastive learning\nframework. By aligning latent representations of images and nuclear masks,\nMorphGen prioritizes diagnostic features such as nuclear and morphological\natypia and spatial organization over staining artifacts and domain-specific\nfeatures. To further enhance out-of-distribution robustness, we incorporate\nstochastic weight averaging (SWA), steering optimization toward flatter minima.\nAttention map analyses revealed that MorphGen primarily relies on nuclear\nmorphology, cellular composition, and spatial cell organization within tumors\nor normal regions for final classification. Finally, we demonstrate resilience\nof the learned representations to image corruptions (such as staining\nartifacts) and adversarial attacks, showcasing not only OOD generalization but\nalso addressing critical vulnerabilities in current deep learning systems for\ndigital pathology. Code, datasets, and trained models are available at:\nhttps://github.com/hikmatkhan/MorphGen"
    },
    {
        "date": "2025-08",
        "title": "A Systematic Approach to Estimate the Security Posture of a Cyber Infrastructure: A Technical Report",
        "author": "Qishen Sam Liang",
        "link": "http://arxiv.org/abs/2509.00266v1",
        "abstract": "Academic and research Cyber Infrastructures (CI) present unique security\nchallenges due to their collaborative nature, heterogeneous components, and the\nlack of practical, tailored security assessment frameworks. Existing standards\ncan be too generic or complex for CI administrators to apply effectively. This\nreport introduces a systematic, mission-centric approach to estimate and\nanalyze the security posture of a CI. The framework guides administrators\nthrough a top-down process: (1) defining unacceptable losses and security\nmissions, (2) identifying associated system hazards and critical assets, and\n(3) modeling the CI's components and their relationships as a security\nknowledge graph. The core of this methodology is the construction of directed\nattack graphs, which systematically map all potential paths an adversary could\ntake from an entry point to a critical asset. By visualizing these attack paths\nalongside defense mechanisms, the framework provides a clear, comprehensive\noverview of the system's vulnerabilities and security gaps. This structured\napproach enables CI operators to proactively assess risks, prioritize\nmitigation strategies, and make informed, actionable decisions to strengthen\nthe overall security posture of the CI."
    },
    {
        "date": "2025-08",
        "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction",
        "author": "Xiaoyang Wang, and Christopher C. Yang",
        "link": "http://arxiv.org/abs/2508.21793v1",
        "abstract": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability."
    },
    {
        "date": "2025-08",
        "title": "Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling",
        "author": "Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, and Lunhong Dong",
        "link": "http://arxiv.org/abs/2508.21785v1",
        "abstract": "Heart rate prediction is vital for personalized health monitoring and\nfitness, while it frequently faces a critical challenge when deploying in\nreal-world: data heterogeneity. We classify it in two key dimensions: source\nheterogeneity from fragmented device markets with varying feature sets, and\nuser heterogeneity reflecting distinct physiological patterns across\nindividuals and activities. Existing methods either discard device-specific\ninformation, or fail to model user-specific differences, limiting their\nreal-world performance. To address this, we propose a framework that learns\nlatent representations agnostic to both heterogeneity, enabling downstream\npredictors to work consistently under heterogeneous data patterns.\nSpecifically, we introduce a random feature dropout strategy to handle source\nheterogeneity, making the model robust to various feature sets. To manage user\nheterogeneity, we employ a time-aware attention module to capture long-term\nphysiological traits and use a contrastive learning objective to build a\ndiscriminative representation space. To reflect the heterogeneous nature of\nreal-world data, we created and publicly released a new benchmark dataset,\nParroTao. Evaluations on both ParroTao and the public FitRec dataset show that\nour model significantly outperforms existing baselines by 17% and 15%,\nrespectively. Furthermore, analysis of the learned representations demonstrates\ntheir strong discriminative power, and one downstream application task confirm\nthe practical value of our model."
    },
    {
        "date": "2025-08",
        "title": "RF-DETR for Robust Mitotic Figure Detection: A MIDOG 2025 Track 1 Approach",
        "author": "Piotr Giedziun, Jan So\u0142tysik, Mateusz G\u00f3rczany, Norbert Ropiak, Marcin Przymus, Piotr Krajewski, Jaros\u0142aw Kwiecie\u0144, Artur Bartczak, Izabela Wasiak, and Mateusz Maniewski",
        "link": "http://arxiv.org/abs/2509.02599v1",
        "abstract": "Mitotic figure detection in histopathology images remains challenging due to\nsignificant domain shifts across different scanners, staining protocols, and\ntissue types. This paper presents our approach for the MIDOG 2025 challenge\nTrack 1, focusing on robust mitotic figure detection across diverse\nhistological contexts. While we initially planned a two-stage approach\ncombining high-recall detection with subsequent classification refinement, time\nconstraints led us to focus on optimizing a single-stage detection pipeline. We\nemployed RF-DETR (Roboflow Detection Transformer) with hard negative mining,\ntrained on MIDOG++ dataset. On the preliminary test set, our method achieved an\nF1 score of 0.789 with a recall of 0.839 and precision of 0.746, demonstrating\neffective generalization across unseen domains. The proposed solution offers\ninsights into the importance of training data balance and hard negative mining\nfor addressing domain shift challenges in mitotic figure detection."
    },
    {
        "date": "2025-08",
        "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization",
        "author": "Jiazheng Xing, Hai Ci, Hongbin Xu, Hangjie Yuan, Yong Liu, and Mike Zheng Shou",
        "link": "http://arxiv.org/abs/2508.21727v1",
        "abstract": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks."
    },
    {
        "date": "2025-08",
        "title": "I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks",
        "author": "Daryna Oliynyk, Rudolf Mayer, Kathrin Grosse, and Andreas Rauber",
        "link": "http://arxiv.org/abs/2508.21654v1",
        "abstract": "Model stealing attacks endanger the confidentiality of machine learning\nmodels offered as a service. Although these models are kept secret, a malicious\nparty can query a model to label data samples and train their own substitute\nmodel, violating intellectual property. While novel attacks in the field are\ncontinually being published, their design and evaluations are not standardised,\nmaking it challenging to compare prior works and assess progress in the field.\nThis paper is the first to address this gap by providing recommendations for\ndesigning and evaluating model stealing attacks. To this end, we study the\nlargest group of attacks that rely on training a substitute model -- those\nattacking image classification models. We propose the first comprehensive\nthreat model and develop a framework for attack comparison. Further, we analyse\nattack setups from related works to understand which tasks and models have been\nstudied the most. Based on our findings, we present best practices for attack\ndevelopment before, during, and beyond experiments and derive an extensive list\nof open research questions regarding the evaluation of model stealing attacks.\nOur findings and recommendations also transfer to other problem domains, hence\nestablishing the first generic evaluation methodology for model stealing\nattacks."
    },
    {
        "date": "2025-08",
        "title": "Detecting Stealthy Data Poisoning Attacks in AI Code Generators",
        "author": "Cristina Improta",
        "link": "http://arxiv.org/abs/2508.21636v1",
        "abstract": "Deep learning (DL) models for natural language-to-code generation have become\nintegral to modern software development pipelines. However, their heavy\nreliance on large amounts of data, often collected from unsanitized online\nsources, exposes them to data poisoning attacks, where adversaries inject\nmalicious samples to subtly bias model behavior. Recent targeted attacks\nsilently replace secure code with semantically equivalent but vulnerable\nimplementations without relying on explicit triggers to launch the attack,\nmaking it especially hard for detection methods to distinguish clean from\npoisoned samples. We present a systematic study on the effectiveness of\nexisting poisoning detection methods under this stealthy threat model.\nSpecifically, we perform targeted poisoning on three DL models (CodeBERT,\nCodeT5+, AST-T5), and evaluate spectral signatures analysis, activation\nclustering, and static analysis as defenses. Our results show that all methods\nstruggle to detect triggerless poisoning, with representation-based approaches\nfailing to isolate poisoned samples and static analysis suffering false\npositives and false negatives, highlighting the need for more robust,\ntrigger-independent defenses for AI-assisted code generation."
    },
    {
        "date": "2025-08",
        "title": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs",
        "author": "Nishant Chinnasami, and Rasha Karakchi",
        "link": "http://arxiv.org/abs/2508.21606v1",
        "abstract": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced."
    },
    {
        "date": "2025-08",
        "title": "OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories",
        "author": "Bo Li, Yingqi Feng, Ming Jin, Xin Zheng, Yufei Tang, Laurent Cherubin, Alan Wee-Chung Liew, Can Wang, Qinghua Lu, Jingwei Yao, Shirui Pan, Hong Zhang, and Xingquan Zhu",
        "link": "http://arxiv.org/abs/2508.21570v1",
        "abstract": "Ocean salinity plays a vital role in circulation, climate, and marine\necosystems, yet its measurement is often sparse, irregular, and noisy,\nespecially in drifter-based datasets. Traditional approaches, such as remote\nsensing and optimal interpolation, rely on linearity and stationarity, and are\nlimited by cloud cover, sensor drift, and low satellite revisit rates. While\nmachine learning models offer flexibility, they often fail under severe\nsparsity and lack principled ways to incorporate physical covariates without\nspecialized sensors. In this paper, we introduce the OceAn Salinity Imputation\nSystem (OASIS), a novel diffusion adversarial framework designed to address\nthese challenges."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Patch Attack for Ship Detection via Localized Augmentation",
        "author": "Chun Liu, Panpan Ding, Zheng Zheng, Hailong Wang, Bingqian Zhu, Tao Xu, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2508.21472v1",
        "abstract": "Current ship detection techniques based on remote sensing imagery primarily\nrely on the object detection capabilities of deep neural networks (DNNs).\nHowever, DNNs are vulnerable to adversarial patch attacks, which can lead to\nmisclassification by the detection model or complete evasion of the targets.\nNumerous studies have demonstrated that data transformation-based methods can\nimprove the transferability of adversarial examples. However, excessive\naugmentation of image backgrounds or irrelevant regions may introduce\nunnecessary interference, resulting in false detections of the object detection\nmodel. These errors are not caused by the adversarial patches themselves but\nrather by the over-augmentation of background and non-target areas. This paper\nproposes a localized augmentation method that applies augmentation only to the\ntarget regions, avoiding any influence on non-target areas. By reducing\nbackground interference, this approach enables the loss function to focus more\ndirectly on the impact of the adversarial patch on the detection model, thereby\nimproving the attack success rate. Experiments conducted on the HRSC2016\ndataset demonstrate that the proposed method effectively increases the success\nrate of adversarial patch attacks and enhances their transferability."
    },
    {
        "date": "2025-08",
        "title": "Robust Pan-Cancer Mitotic Figure Detection with YOLOv12",
        "author": "Rapha\u00ebl Bourgade, Guillaume Balezo, and Thomas Walter",
        "link": "http://arxiv.org/abs/2509.02593v1",
        "abstract": "Mitotic figures represent a key histoprognostic feature in tumor pathology,\nproviding crucial insights into tumor aggressiveness and proliferation.\nHowever, their identification remains challenging, subject to significant\ninter-observer variability, even among experienced pathologists. To address\nthis issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the\nthird edition of an international competition aiming to develop robust mitosis\ndetection algorithms. In this paper, we present a mitotic figures detection\napproach based on the YOLOv12 object detection architecture, achieving a\n$F_1$-score of 0.801 on the preliminary test set of the MIDOG 2025 challenge,\nwithout relying on external data."
    },
    {
        "date": "2025-08",
        "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs",
        "author": "Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2508.21393v1",
        "abstract": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."
    },
    {
        "date": "2025-08",
        "title": "Risks and Compliance with the EU's Core Cyber Security Legislation",
        "author": "Jukka Ruohonen, Jesper L\u00f8ffler Nielsen, and Jakub Sk\u00f3rczynski",
        "link": "http://arxiv.org/abs/2508.21386v1",
        "abstract": "The European Union (EU) has long favored a risk-based approach to regulation.\nSuch an approach is also used in recent cyber security legislation enacted in\nthe EU. Risks are also inherently related to compliance with the new\nlegislation. Objective: The paper investigates how risks are framed in the EU's\nfive core cyber security legislative acts, whether the framings indicate\nconvergence or divergence between the acts and their risk concepts, and what\nqualifying words and terms are used when describing the legal notions of risks.\nMethod : The paper's methodology is based on qualitative legal interpretation\nand taxonomy-building. Results: The five acts have an encompassing coverage of\ndifferent cyber security risks, including but not limited to risks related to\ntechnical, organizational, and human security as well as those not originating\nfrom man-made actions. Both technical aspects and assets are used to frame the\nlegal risk notions in many of the legislative acts. A threat-centric viewpoint\nis also present in one of the acts. Notable gaps are related to acceptable\nrisks, non-probabilistic risks, and residual risks. Conclusion: The EU's new\ncyber security legislation has significantly extended the risk-based approach\nto regulations. At the same time, complexity and compliance burden have\nincreased. With this point in mind, the paper concludes with a few practical\ntakeaways about means to deal with compliance and research it."
    },
    {
        "date": "2025-08",
        "title": "DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks",
        "author": "Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, and Yining Wang",
        "link": "http://arxiv.org/abs/2508.21340v1",
        "abstract": "Time series synthesis is an effective approach to ensuring the secure\ncirculation of time series data. Existing time series synthesis methods\ntypically perform temporal modeling based on random sequences to generate\ntarget sequences, which often struggle to ensure the temporal dependencies in\nthe generated time series. Additionally, directly modeling temporal features on\nrandom sequences makes it challenging to accurately capture the feature\ninformation of the original time series. To address the above issues, we\npropose a simple but effective generative model \\textbf{D}ual-\\textbf{L}ayer\n\\textbf{G}enerative \\textbf{A}dversarial \\textbf{N}etworks, named\n\\textbf{DLGAN}. The model decomposes the time series generation process into\ntwo stages: sequence feature extraction and sequence reconstruction. First,\nthese two stages form a complete time series autoencoder, enabling supervised\nlearning on the original time series to ensure that the reconstruction process\ncan restore the temporal dependencies of the sequence. Second, a Generative\nAdversarial Network (GAN) is used to generate synthetic feature vectors that\nalign with the real-time sequence feature vectors, ensuring that the generator\ncan capture the temporal features from real time series. Extensive experiments\non four public datasets demonstrate the superiority of this model across\nvarious evaluation metrics."
    },
    {
        "date": "2025-08",
        "title": "Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning",
        "author": "Hunter Gittlin",
        "link": "http://arxiv.org/abs/2509.02592v1",
        "abstract": "Class imbalance remains a fundamental challenge in machine learning, with\ntraditional solutions often creating as many problems as they solve. We\ndemonstrate that group-aware threshold calibration--setting different decision\nthresholds for different demographic groups--provides superior robustness\ncompared to synthetic data generation methods. Through extensive experiments,\nwe show that group-specific thresholds achieve 1.5-4% higher balanced accuracy\nthan SMOTE and CT-GAN augmented models while improving worst-group balanced\naccuracy. Unlike single-threshold approaches that apply one cutoff across all\ngroups, our group-aware method optimizes the Pareto frontier between balanced\naccuracy and worst-group balanced accuracy, enabling fine-grained control over\ngroup-level performance. Critically, we find that applying group thresholds to\nsynthetically augmented data yields minimal additional benefit, suggesting\nthese approaches are fundamentally redundant. Our results span seven model\nfamilies including linear, tree-based, instance-based, and boosting methods,\nconfirming that group-aware threshold calibration offers a simpler, more\ninterpretable, and more effective solution to class imbalance."
    },
    {
        "date": "2025-08",
        "title": "The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation",
        "author": "A H M Nazmus Sakib, Mahsin Bin Akram, Joseph Spracklen, Sahan Kalutarage, Raveen Wijewickrama, Igor Bilogrevic, and Murtuza Jadliwala",
        "link": "http://arxiv.org/abs/2508.21219v1",
        "abstract": "Browser fingerprinting defenses have historically focused on detecting\nJavaScript(JS)-based tracking techniques. However, the widespread adoption of\nWebAssembly (WASM) introduces a potential blind spot, as adversaries can\nconvert JS to WASM's low-level binary format to obfuscate malicious logic. This\npaper presents the first systematic evaluation of how such WASM-based\nobfuscation impacts the robustness of modern fingerprinting defenses. We\ndevelop an automated pipeline that translates real-world JS fingerprinting\nscripts into functional WASM-obfuscated variants and test them against two\nclasses of defenses: state-of-the-art detectors in research literature and\ncommercial, in-browser tools. Our findings reveal a notable divergence:\ndetectors proposed in the research literature that rely on feature-based\nanalysis of source code show moderate vulnerability, stemming from outdated\ndatasets or a lack of WASM compatibility. In contrast, defenses such as browser\nextensions and native browser features remained completely effective, as their\nAPI-level interception is agnostic to the script's underlying implementation.\nThese results highlight a gap between academic and practical defense strategies\nand offer insights into strengthening detection approaches against WASM-based\nobfuscation, while also revealing opportunities for more evasive techniques in\nfuture attacks."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach",
        "author": "Han Yang, Jian Lan, Yihong Liu, Hinrich Sch\u00fctze, and Thomas Seidl",
        "link": "http://arxiv.org/abs/2508.21206v1",
        "abstract": "Autoregressive language models are vulnerable to orthographic attacks, where\ninput text is perturbed with characters from multilingual alphabets, leading to\nsubstantial performance degradation. This vulnerability primarily stems from\nthe out-of-vocabulary issue inherent in subword tokenizers and their\nembeddings. To address this limitation, we propose a pixel-based generative\nlanguage model that replaces the text-based embeddings with pixel-based\nrepresentations by rendering words as individual images. This design provides\nstronger robustness to noisy inputs, while an extension of compatibility to\nmultilingual text across diverse writing systems. We evaluate the proposed\nmethod on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2\nbenchmark, demonstrating both its resilience to orthographic noise and its\neffectiveness in multilingual settings."
    },
    {
        "date": "2025-08",
        "title": "RARR : Robust Real-World Activity Recognition with Vibration by Scavenging Near-Surface Audio Online",
        "author": "Dong Yoon Lee, Alyssa Weakley, Hui Wei, Blake Brown, Keyana Carrion, and Shijia Pan",
        "link": "http://arxiv.org/abs/2508.21167v1",
        "abstract": "One in four people dementia live alone, leading family members to take on\ncaregiving roles from a distance. Many researchers have developed remote\nmonitoring solutions to lessen caregiving needs; however, limitations remain\nincluding privacy preserving solutions, activity recognition, and model\ngeneralizability to new users and environments. Structural vibration sensor\nsystems are unobtrusive solutions that have been proven to accurately monitor\nhuman information, such as identification and activity recognition, in\ncontrolled settings by sensing surface vibrations generated by activities.\nHowever, when deploying in an end user's home, current solutions require a\nsubstantial amount of labeled data for accurate activity recognition. Our\nscalable solution adapts synthesized data from near-surface acoustic audio to\npretrain a model and allows fine tuning with very limited data in order to\ncreate a robust framework for daily routine tracking."
    },
    {
        "date": "2025-08",
        "title": "Privacy Auditing Synthetic Data Release through Local Likelihood Attacks",
        "author": "Joshua Ward, Chi-Hua Wang, and Guang Cheng",
        "link": "http://arxiv.org/abs/2508.21146v1",
        "abstract": "Auditing the privacy leakage of synthetic data is an important but unresolved\nproblem. Most existing privacy auditing frameworks for synthetic data rely on\nheuristics and unreasonable assumptions to attack the failure modes of\ngenerative models, exhibiting limited capability to describe and detect the\nprivacy exposure of training data through synthetic data release. In this\npaper, we study designing Membership Inference Attacks (MIAs) that specifically\nexploit the observation that tabular generative models tend to significantly\noverfit to certain regions of the training distribution. Here, we propose\nGenerative Likelihood Ratio Attack (Gen-LRA), a novel, computationally\nefficient No-Box MIA that, with no assumption of model knowledge or access,\nformulates its attack by evaluating the influence a test observation has in a\nsurrogate model's estimation of a local likelihood ratio over the synthetic\ndata. Assessed over a comprehensive benchmark spanning diverse datasets, model\narchitectures, and attack parameters, we find that Gen-LRA consistently\ndominates other MIAs for generative models across multiple performance metrics.\nThese results underscore Gen-LRA's effectiveness as a privacy auditing tool for\nthe release of synthetic data, highlighting the significant privacy risks posed\nby generative model overfitting in real-world applications."
    },
    {
        "date": "2025-08",
        "title": "MitoDetect++: A Domain-Robust Pipeline for Mitosis Detection and Atypical Subtyping",
        "author": "Esha Sadia Nasir, Jiaqi Lv, Mostafa Jahanifar, and Shan E Ahmed Raza",
        "link": "http://arxiv.org/abs/2509.02586v2",
        "abstract": "Automated detection and classification of mitotic figures especially\ndistinguishing atypical from normal remain critical challenges in computational\npathology. We present MitoDetect++, a unified deep learning pipeline designed\nfor the MIDOG 2025 challenge, addressing both mitosis detection and atypical\nmitosis classification. For detection (Track 1), we employ a U-Net-based\nencoder-decoder architecture with EfficientNetV2-L as the backbone, enhanced\nwith attention modules, and trained via combined segmentation losses. For\nclassification (Track 2), we leverage the Virchow2 vision transformer,\nfine-tuned efficiently using Low-Rank Adaptation (LoRA) to minimize resource\nconsumption. To improve generalization and mitigate domain shifts, we integrate\nstrong augmentations, focal loss, and group-aware stratified 5-fold\ncross-validation. At inference, we deploy test-time augmentation (TTA) to boost\nrobustness. Our method achieves a balanced accuracy of 0.892 across validation\ndomains, highlighting its clinical applicability and scalability across tasks."
    },
    {
        "date": "2025-08",
        "title": "POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models",
        "author": "Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, and Qinglin Lu",
        "link": "http://arxiv.org/abs/2508.21019v1",
        "abstract": "The field of video diffusion generation faces critical bottlenecks in\nsampling efficiency, especially for large-scale models and long sequences.\nExisting video acceleration methods adopt image-based techniques but suffer\nfrom fundamental limitations: they neither model the temporal coherence of\nvideo frames nor provide single-step distillation for large-scale video models.\nTo bridge this gap, we propose POSE (Phased One-Step Equilibrium), a\ndistillation framework that reduces the sampling steps of large-scale video\ndiffusion models, enabling the generation of high-quality videos in a single\nstep. POSE employs a carefully designed two-phase process to distill video\nmodels:(i) stability priming: a warm-up mechanism to stabilize adversarial\ndistillation that adapts the high-quality trajectory of the one-step generator\nfrom high to low signal-to-noise ratio regimes, optimizing the video quality of\nsingle-step mappings near the endpoints of flow trajectories. (ii) unified\nadversarial equilibrium: a flexible self-adversarial distillation mechanism\nthat promotes stable single-step adversarial training towards a Nash\nequilibrium within the Gaussian noise space, generating realistic single-step\nvideos close to real videos. For conditional video generation, we propose (iii)\nconditional adversarial consistency, a method to improve both semantic\nconsistency and frame consistency between conditional frames and generated\nframes. Comprehensive experiments demonstrate that POSE outperforms other\nacceleration methods on VBench-I2V by average 7.15% in semantic alignment,\ntemporal conference and frame quality, reducing the latency of the pre-trained\nmodel by 100$\\times$, from 1000 seconds to 10 seconds, while maintaining\ncompetitive performance."
    },
    {
        "date": "2025-08",
        "title": "Multilingual Dataset Integration Strategies for Robust Audio Deepfake Detection: A SAFE Challenge System",
        "author": "Hashim Ali, Surya Subramani, Lekha Bollinani, Nithin Sai Adupa, Sali El-Loh, and Hafiz Malik",
        "link": "http://arxiv.org/abs/2508.20983v1",
        "abstract": "The SAFE Challenge evaluates synthetic speech detection across three tasks:\nunmodified audio, processed audio with compression artifacts, and laundered\naudio designed to evade detection. We systematically explore self-supervised\nlearning (SSL) front-ends, training data compositions, and audio length\nconfigurations for robust deepfake detection. Our AASIST-based approach\nincorporates WavLM large frontend with RawBoost augmentation, trained on a\nmultilingual dataset of 256,600 samples spanning 9 languages and over 70 TTS\nsystems from CodecFake, MLAAD v5, SpoofCeleb, Famous Figures, and MAILABS.\nThrough extensive experimentation with different SSL front-ends, three training\ndata versions, and two audio lengths, we achieved second place in both Task 1\n(unmodified audio detection) and Task 3 (laundered audio detection),\ndemonstrating strong generalization and robustness."
    },
    {
        "date": "2025-08",
        "title": "Learning Robust Spatial Representations from Binaural Audio through Feature Distillation",
        "author": "Holger Severin Bovbjerg, Jan \u00d8stergaard, Jesper Jensen, Shinji Watanabe, and Zheng-Hua Tan",
        "link": "http://arxiv.org/abs/2508.20914v1",
        "abstract": "Recently, deep representation learning has shown strong performance in\nmultiple audio tasks. However, its use for learning spatial representations\nfrom multichannel audio is underexplored. We investigate the use of a\npretraining stage based on feature distillation to learn a robust spatial\nrepresentation of binaural speech without the need for data labels. In this\nframework, spatial features are computed from clean binaural speech samples to\nform prediction labels. These clean features are then predicted from\ncorresponding augmented speech using a neural network. After pretraining, we\nthrow away the spatial feature predictor and use the learned encoder weights to\ninitialize a DoA estimation model which we fine-tune for DoA estimation. Our\nexperiments demonstrate that the pretrained models show improved performance in\nnoisy and reverberant environments after fine-tuning for direction-of-arrival\nestimation, when compared to fully supervised models and classic signal\nprocessing methods."
    },
    {
        "date": "2025-08",
        "title": "OLMoASR: Open Models and Data for Training Robust Speech Recognition Models",
        "author": "Huong Ngo, Matt Deitke, Martijn Bartelds, Sarah Pratt, Josh Gardner, Matt Jordan, and Ludwig Schmidt",
        "link": "http://arxiv.org/abs/2508.20869v1",
        "abstract": "Improvements in training data scale and quality have led to significant\nadvances, yet its influence in speech recognition remains underexplored. In\nthis paper, we present a large-scale dataset, OLMoASR-Pool, and series of\nmodels, OLMoASR, to study and develop robust zero-shot speech recognition\nmodels. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio\nand 17M transcripts, we design text heuristic filters to remove low-quality or\nmistranscribed data. Our curation pipeline produces a new dataset containing 1M\nhours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use\nOLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M\n(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR\nachieves comparable average performance to OpenAI's Whisper on short and\nlong-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a\n12.8\\% and 11.0\\% word error rate (WER) that is on par with Whisper's largest\nEnglish-only model Whisper-medium.en's 12.4\\% and 10.5\\% WER for short and\nlong-form recognition respectively (at equivalent parameter count).\nOLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will\nbe made publicly available to further research on robust speech processing."
    },
    {
        "date": "2025-08",
        "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
        "author": "Matteo Gioele Collu, Umberto Salviati, Roberto Confalonieri, Mauro Conti, and Giovanni Apruzzese",
        "link": "http://arxiv.org/abs/2508.20863v2",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
    },
    {
        "date": "2025-08",
        "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning",
        "author": "Dongjun Lee, Changho Hwang, and Kimin Lee",
        "link": "http://arxiv.org/abs/2508.21107v1",
        "abstract": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task."
    },
    {
        "date": "2025-08",
        "title": "FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning",
        "author": "He Li, Xinyu Liu, Weihang Kong, and Xingchen Zhang",
        "link": "http://arxiv.org/abs/2508.20817v2",
        "abstract": "Visible and infrared image fusion (VIF) is an important multimedia task in\ncomputer vision. Most VIF methods focus primarily on optimizing fused image\nquality. Recent studies have begun incorporating downstream tasks, such as\nsemantic segmentation and object detection, to provide semantic guidance for\nVIF. However, semantic segmentation requires extensive annotations, while\nobject detection, despite reducing annotation efforts compared with\nsegmentation, faces challenges in highly crowded scenes due to overlapping\nbounding boxes and occlusion. Moreover, although RGB-T crowd counting has\ngained increasing attention in recent years, no studies have integrated VIF and\ncrowd counting into a unified framework. To address these challenges, we\npropose FusionCounting, a novel multi-task learning framework that integrates\ncrowd counting into the VIF process. Crowd counting provides a direct\nquantitative measure of population density with minimal annotation, making it\nparticularly suitable for dense scenes. Our framework leverages both input\nimages and population density information in a mutually beneficial multi-task\ndesign. To accelerate convergence and balance tasks contributions, we introduce\na dynamic loss function weighting strategy. Furthermore, we incorporate\nadversarial training to enhance the robustness of both VIF and crowd counting,\nimproving the model's stability and resilience to adversarial attacks.\nExperimental results on public datasets demonstrate that FusionCounting not\nonly enhances image fusion quality but also achieves superior crowd counting\nperformance."
    },
    {
        "date": "2025-08",
        "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
        "author": "Yifan Zhang",
        "link": "http://arxiv.org/abs/2508.20784v1",
        "abstract": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced."
    },
    {
        "date": "2025-08",
        "title": "Occlusion Robustness of CLIP for Military Vehicle Classification",
        "author": "Jan Erik van Woerden, Gertjan Burghouts, Lotte Nijskens, Alma M. Liezenga, Sabina van Rooij, Frank Ruis, and Hugo J. Kuijf",
        "link": "http://arxiv.org/abs/2508.20760v2",
        "abstract": "Vision-language models (VLMs) like CLIP enable zero-shot classification by\naligning images and text in a shared embedding space, offering advantages for\ndefense applications with scarce labeled data. However, CLIP's robustness in\nchallenging military environments, with partial occlusion and degraded\nsignal-to-noise ratio (SNR), remains underexplored. We investigate CLIP\nvariants' robustness to occlusion using a custom dataset of 18 military vehicle\nclasses and evaluate using Normalized Area Under the Curve (NAUC) across\nocclusion percentages. Four key insights emerge: (1) Transformer-based CLIP\nmodels consistently outperform CNNs, (2) fine-grained, dispersed occlusions\ndegrade performance more than larger contiguous occlusions, (3) despite\nimproved accuracy, performance of linear-probed models sharply drops at around\n35% occlusion, (4) by finetuning the model's backbone, this performance drop\noccurs at more than 60% occlusion. These results underscore the importance of\nocclusion-specific augmentations during training and the need for further\nexploration into patch-level sensitivity and architectural resilience for\nreal-world deployment of CLIP."
    },
    {
        "date": "2025-08",
        "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
        "author": "Stefano Fumero, Kai Huang, Matteo Boffa, Danilo Giordano, Marco Mellia, Zied Ben Houidi, and Dario Rossi",
        "link": "http://arxiv.org/abs/2508.20643v1",
        "abstract": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents."
    },
    {
        "date": "2025-08",
        "title": "Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications",
        "author": "Immanuel Ro\u00dfteutscher, Klaus S. Drese, and Thorsten Uphues",
        "link": "http://arxiv.org/abs/2508.20622v1",
        "abstract": "We investigated the adaptation and performance of Masked Autoencoders (MAEs)\nwith Vision Transformer (ViT) architectures for self-supervised representation\nlearning on one-dimensional (1D) ultrasound signals. Although MAEs have\ndemonstrated significant success in computer vision and other domains, their\nuse for 1D signal analysis, especially for raw ultrasound data, remains largely\nunexplored. Ultrasound signals are vital in industrial applications such as\nnon-destructive testing (NDT) and structural health monitoring (SHM), where\nlabeled data are often scarce and signal processing is highly task-specific. We\npropose an approach that leverages MAE to pre-train on unlabeled synthetic\nultrasound signals, enabling the model to learn robust representations that\nenhance performance in downstream tasks, such as time-of-flight (ToF)\nclassification. This study systematically investigated the impact of model\nsize, patch size, and masking ratio on pre-training efficiency and downstream\naccuracy. Our results show that pre-trained models significantly outperform\nmodels trained from scratch and strong convolutional neural network (CNN)\nbaselines optimized for the downstream task. Additionally, pre-training on\nsynthetic data demonstrates superior transferability to real-world measured\nsignals compared with training solely on limited real datasets. This study\nunderscores the potential of MAEs for advancing ultrasound signal analysis\nthrough scalable, self-supervised learning."
    },
    {
        "date": "2025-08",
        "title": "Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification",
        "author": "Smriti Joshi, Lidia Garrucho, Richard Osuala, Oliver Diaz, and Karim Lekadir",
        "link": "http://arxiv.org/abs/2508.20621v1",
        "abstract": "Breast cancer is one of the leading causes of cancer-related mortality in\nwomen, and early detection is essential for improving outcomes. Magnetic\nresonance imaging (MRI) is a highly sensitive tool for breast cancer detection,\nparticularly in women at high risk or with dense breast tissue, where\nmammography is less effective. The ODELIA consortium organized a multi-center\nchallenge to foster AI-based solutions for breast cancer diagnosis and\nclassification. The dataset included 511 studies from six European centers,\nacquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study\nwas labeled for the left and right breast as no lesion, benign lesion, or\nmalignant lesion. We developed a SwinUNETR-based deep learning framework that\nincorporates breast region masking, extensive data augmentation, and ensemble\nlearning to improve robustness and generalizability. Our method achieved second\nplace on the challenge leaderboard, highlighting its potential to support\nclinical breast MRI interpretation. We publicly share our codebase at\nhttps://github.com/smriti-joshi/bcnaim-odelia-challenge.git."
    },
    {
        "date": "2025-08",
        "title": "Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization",
        "author": "Yixiang Qiu, Yanhan Liu, Hongyao Yu, Hao Fang, Bin Chen, Shu-Tao Xia, and Ke Xu",
        "link": "http://arxiv.org/abs/2508.20613v1",
        "abstract": "The growing complexity of Deep Neural Networks (DNNs) has led to the adoption\nof Split Inference (SI), a collaborative paradigm that partitions computation\nbetween edge devices and the cloud to reduce latency and protect user privacy.\nHowever, recent advances in Data Reconstruction Attacks (DRAs) reveal that\nintermediate features exchanged in SI can be exploited to recover sensitive\ninput data, posing significant privacy risks. Existing DRAs are typically\neffective only on shallow models and fail to fully leverage semantic priors,\nlimiting their reconstruction quality and generalizability across datasets and\nmodel architectures. In this paper, we propose a novel GAN-based DRA framework\nwith Progressive Feature Optimization (PFO), which decomposes the generator\ninto hierarchical blocks and incrementally refines intermediate representations\nto enhance the semantic fidelity of reconstructed images. To stabilize the\noptimization and improve image realism, we introduce an L1-ball constraint\nduring reconstruction. Extensive experiments show that our method outperforms\nprior attacks by a large margin, especially in high-resolution scenarios,\nout-of-distribution settings, and against deeper and more complex DNNs."
    },
    {
        "date": "2025-08",
        "title": "Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations",
        "author": "Mengxiao Huang, Minglei Shu, Shuwang Zhou, and Zhaoyang Liu",
        "link": "http://arxiv.org/abs/2508.20595v1",
        "abstract": "Deepfake technology, driven by Generative Adversarial Networks (GANs), poses\nsignificant risks to privacy and societal security. Existing detection methods\nare predominantly passive, focusing on post-event analysis without preventing\nattacks. To address this, we propose an active defense method based on\nlow-frequency perceptual perturbations to disrupt face swapping manipulation,\nreducing the performance and naturalness of generated content. Unlike prior\napproaches that used low-frequency perturbations to impact classification\naccuracy,our method directly targets the generative process of deepfake\ntechniques. We combine frequency and spatial domain features to strengthen\ndefenses. By introducing artifacts through low-frequency perturbations while\npreserving high-frequency details, we ensure the output remains visually\nplausible. Additionally, we design a complete architecture featuring an\nencoder, a perturbation generator, and a decoder, leveraging discrete wavelet\ntransform (DWT) to extract low-frequency components and generate perturbations\nthat disrupt facial manipulation models. Experiments on CelebA-HQ and LFW\ndemonstrate significant reductions in face-swapping effectiveness, improved\ndefense success rates, and preservation of visual quality."
    },
    {
        "date": "2025-08",
        "title": "SemSR: Semantics aware robust Session-based Recommendations",
        "author": "Jyoti Narwariya, Priyanka Gupta, Muskan Gupta, Jyotsana Khatri, and Lovekesh Vig",
        "link": "http://arxiv.org/abs/2508.20587v1",
        "abstract": "Session-based recommendation (SR) models aim to recommend items to anonymous\nusers based on their behavior during the current session. While various SR\nmodels in the literature utilize item sequences to predict the next item, they\noften fail to leverage semantic information from item titles or descriptions\nimpeding session intent identification and interpretability. Recent research\nhas explored Large Language Models (LLMs) as promising approaches to enhance\nsession-based recommendations, with both prompt-based and fine-tuning based\nmethods being widely investigated. However, prompt-based methods struggle to\nidentify optimal prompts that elicit correct reasoning and lack task-specific\nfeedback at test time, resulting in sub-optimal recommendations. Fine-tuning\nmethods incorporate domain-specific knowledge but incur significant\ncomputational costs for implementation and maintenance. In this paper, we\npresent multiple approaches to utilize LLMs for session-based recommendation:\n(i) in-context LLMs as recommendation agents, (ii) LLM-generated\nrepresentations for semantic initialization of deep learning SR models, and\n(iii) integration of LLMs with data-driven SR models. Through comprehensive\nexperiments on two real-world publicly available datasets, we demonstrate that\nLLM-based methods excel at coarse-level retrieval (high recall values), while\ntraditional data-driven techniques perform well at fine-grained ranking (high\nMean Reciprocal Rank values). Furthermore, the integration of LLMs with\ndata-driven SR models significantly out performs both standalone LLM approaches\nand data-driven deep learning models, as well as baseline SR models, in terms\nof both Recall and MRR metrics."
    },
    {
        "date": "2025-08",
        "title": "Towards Mechanistic Defenses Against Typographic Attacks in CLIP",
        "author": "Lorenz Hufe, Constantin Venhoff, Maximilian Dreyer, Sebastian Lapuschkin, and Wojciech Samek",
        "link": "http://arxiv.org/abs/2508.20570v1",
        "abstract": "Typographic attacks exploit multi-modal systems by injecting text into\nimages, leading to targeted misclassifications, malicious content generation\nand even Vision-Language Model jailbreaks. In this work, we analyze how CLIP\nvision encoders behave under typographic attacks, locating specialized\nattention heads in the latter half of the model's layers that causally extract\nand transmit typographic information to the cls token. Building on these\ninsights, we introduce a method to defend CLIP models against typographic\nattacks by selectively ablating a typographic circuit, consisting of attention\nheads. Without requiring finetuning, our method improves performance by up to\n19.6% on a typographic variant of ImageNet-100, while reducing standard\nImageNet-100 accuracy by less than 1%. Notably, our training-free approach\nremains competitive with current state-of-the-art typographic defenses that\nrely on finetuning. To this end, we release a family of dyslexic CLIP models\nwhich are significantly more robust against typographic attacks. These models\nserve as suitable drop-in replacements for a broad range of safety-critical\napplications, where the risks of text-based manipulation outweigh the utility\nof text recognition."
    },
    {
        "date": "2025-08",
        "title": "BridgeShield: Enhancing Security for Cross-chain Bridge Applications via Heterogeneous Graph Mining",
        "author": "Dan Lin, Shunfeng Lu, Ziyan Liu, Jiajing Wu, Junyuan Fang, Kaixin Lin, Bowen Song, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2508.20517v1",
        "abstract": "Cross-chain bridges play a vital role in enabling blockchain\ninteroperability. However, due to the inherent design flaws and the enormous\nvalue they hold, they have become prime targets for hacker attacks. Existing\ndetection methods show progress yet remain limited, as they mainly address\nsingle-chain behaviors and fail to capture cross-chain semantics. To address\nthis gap, we leverage heterogeneous graph attention networks, which are\nwell-suited for modeling multi-typed entities and relations, to capture the\ncomplex execution semantics of cross-chain behaviors. We propose BridgeShield,\na detection framework that jointly models the source chain, off-chain\ncoordination, and destination chain within a unified heterogeneous graph\nrepresentation. BridgeShield incorporates intra-meta-path attention to learn\nfine-grained dependencies within cross-chain paths and inter-meta-path\nattention to highlight discriminative cross-chain patterns, thereby enabling\nprecise identification of attack behaviors. Extensive experiments on 51\nreal-world cross-chain attack events demonstrate that BridgeShield achieves an\naverage F1-score of 92.58%, representing a 24.39% improvement over\nstate-of-the-art baselines. These results validate the effectiveness of\nBridgeShield as a practical solution for securing cross-chain bridges and\nenhancing the resilience of multi-chain ecosystems."
    },
    {
        "date": "2025-08",
        "title": "MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph",
        "author": "Zhiqiang Wang, Junyang Zhang, Guanquan Shi, HaoRan Cheng, Yunhao Yao, Kaiwen Guo, Haohua Du, and Xiang-Yang Li",
        "link": "http://arxiv.org/abs/2508.20412v1",
        "abstract": "The Model Context Protocol (MCP) is increasingly adopted to standardize the\ninteraction between LLM agents and external tools. However, this trend\nintroduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is\npoisoned to induce the agent to perform unauthorized operations. Existing\ndefenses that primarily focus on behavior-level analysis are fundamentally\nineffective against TPA, as poisoned tools need not be executed, leaving no\nbehavioral trace to monitor.\n  Thus, we propose MindGuard, a decision-level guardrail for LLM agents,\nproviding provenance tracking of call decisions, policy-agnostic detection, and\npoisoning source attribution against TPA. While fully explaining LLM decision\nremains challenging, our empirical findings uncover a strong correlation\nbetween LLM attention mechanisms and tool invocation decisions. Therefore, we\nchoose attention as an empirical signal for decision tracking and formalize\nthis as the Decision Dependence Graph (DDG), which models the LLM's reasoning\nprocess as a weighted, directed graph where vertices represent logical concepts\nand edges quantify the attention-based dependencies. We further design robust\nDDG construction and graph-based anomaly analysis mechanisms that efficiently\ndetect and attribute TPA attacks. Extensive experiments on real-world datasets\ndemonstrate that MindGuard achieves 94\\%-99\\% average precision in detecting\npoisoned invocations, 95\\%-100\\% attribution accuracy, with processing times\nunder one second and no additional token cost. Moreover, DDG can be viewed as\nan adaptation of the classical Program Dependence Graph (PDG), providing a\nsolid foundation for applying traditional security policies at the decision\nlevel."
    },
    {
        "date": "2025-08",
        "title": "TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin",
        "author": "Shijie Wang, and Lei Li",
        "link": "http://arxiv.org/abs/2508.20398v1",
        "abstract": "Electrocardiogram (ECG) signals serve as a foundational data source for\ncardiac digital twins, yet their diagnostic utility is frequently compromised\nby noise and artifacts. To address this issue, we propose TF-TransUNet1D, a\nnovel one-dimensional deep neural network that integrates a U-Net-based\nencoder-decoder architecture with a Transformer encoder, guided by a hybrid\ntime-frequency domain loss. The model is designed to simultaneously capture\nlocal morphological features and long-range temporal dependencies, which are\ncritical for preserving the diagnostic integrity of ECG signals. To enhance\ndenoising robustness, we introduce a dual-domain loss function that jointly\noptimizes waveform reconstruction in the time domain and spectral fidelity in\nthe frequency domain. In particular, the frequency-domain component effectively\nsuppresses high-frequency noise while maintaining the spectral structure of the\nsignal, enabling recovery of subtle but clinically significant waveform\ncomponents. We evaluate TF-TransUNet1D using synthetically corrupted signals\nfrom the MIT-BIH Arrhythmia Database and the Noise Stress Test Database\n(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate\nconsistent superiority of our model in terms of SNR improvement and error\nmetrics, achieving a mean absolute error of 0.1285 and Pearson correlation\ncoefficient of 0.9540. By delivering high-precision denoising, this work\nbridges a critical gap in pre-processing pipelines for cardiac digital twins,\nenabling more reliable real-time monitoring and personalized modeling."
    },
    {
        "date": "2025-08",
        "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models",
        "author": "Xiao Li, Yanfan Zhu, Ruining Deng, Wei-Qi Wei, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, and Yuankai Huo",
        "link": "http://arxiv.org/abs/2508.20345v1",
        "abstract": "Recent advances in medical vision-language models (VLMs) open up remarkable\nopportunities for clinical applications such as automated report generation,\ncopilots for physicians, and uncertainty quantification. However, despite their\npromise, medical VLMs introduce serious security concerns, most notably risks\nof Protected Health Information (PHI) exposure, data leakage, and vulnerability\nto cyberthreats - which are especially critical in hospital environments. Even\nwhen adopted for research or non-clinical purposes, healthcare organizations\nmust exercise caution and implement safeguards. To address these challenges, we\npresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)\nenables physicians to manually select and use different models without\nprogramming expertise, (2) supports engineers in efficiently deploying medical\nVLMs in a plug-and-play fashion, with seamless integration of Hugging Face\nopen-source models, and (3) ensures privacy-preserving inference through\nDocker-orchestrated, operating system agnostic deployment. MedFoundationHub\nrequires only an offline local workstation equipped with a single NVIDIA A6000\nGPU, making it both secure and accessible within the typical resources of\nacademic research labs. To evaluate current capabilities, we engaged\nboard-certified pathologists to deploy and assess five state-of-the-art VLMs\n(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and\nLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,\nyielding 1015 clinician-model scoring events. These assessments revealed\nrecurring limitations, including off-target answers, vague reasoning, and\ninconsistent pathology terminology."
    },
    {
        "date": "2025-08",
        "title": "Enhanced R\u00e9nyi Entropy-Based Post-Quantum Key Agreement with Provable Security and Information-Theoretic Guarantees",
        "author": "Ruopengyu Xu, and Chenglian Liu",
        "link": "http://arxiv.org/abs/2509.00104v1",
        "abstract": "This paper presents an enhanced post-quantum key agreement protocol based on\nR\\'{e}nyi entropy, addressing vulnerabilities in the original construction\nwhile preserving information-theoretic security properties. We develop a\ntheoretical framework leveraging entropy-preserving operations and\nsecret-shared verification to achieve provable security against quantum\nadversaries. Through entropy amplification techniques and quantum-resistant\ncommitments, the protocol establishes $2^{128}$ quantum security guarantees\nunder the quantum random oracle model. Key innovations include a\nconfidentiality-preserving verification mechanism using distributed polynomial\ncommitments, tightened min-entropy bounds with guaranteed non-negativity, and\ncomposable security proofs in the quantum universal composability framework.\nUnlike computational approaches, our method provides information-theoretic\nsecurity without hardness assumptions while maintaining polynomial complexity.\nTheoretical analysis demonstrates resilience against known quantum attack\nvectors, including Grover-accelerated brute force and quantum memory attacks.\nThe protocol achieves parameterization for 128-bit quantum security with\nefficient $\\mathcal{O}(n^2)$ communication complexity. Extensions to secure\nmultiparty computation and quantum network applications are established,\nproviding a foundation for long-term cryptographic security. All security\nclaims are derived from mathematical proofs; this theoretical work presents no\nexperimental validation."
    },
    {
        "date": "2025-08",
        "title": "Latent Variable Modeling for Robust Causal Effect Estimation",
        "author": "Tetsuro Morimura, Tatsushi Oka, Yugo Suzuki, and Daisuke Moriwaki",
        "link": "http://arxiv.org/abs/2508.20259v1",
        "abstract": "Latent variable models provide a powerful framework for incorporating and\ninferring unobserved factors in observational data. In causal inference, they\nhelp account for hidden factors influencing treatment or outcome, thereby\naddressing challenges posed by missing or unmeasured covariates. This paper\nproposes a new framework that integrates latent variable modeling into the\ndouble machine learning (DML) paradigm to enable robust causal effect\nestimation in the presence of such hidden factors. We consider two scenarios:\none where a latent variable affects only the outcome, and another where it may\ninfluence both treatment and outcome. To ensure tractability, we incorporate\nlatent variables only in the second stage of DML, separating representation\nlearning from latent inference. We demonstrate the robustness and effectiveness\nof our method through extensive experiments on both synthetic and real-world\ndatasets."
    },
    {
        "date": "2025-08",
        "title": "Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID",
        "author": "Xia Han, Qi Li, Jianbing Ni, and Mohammad Zulkernine",
        "link": "http://arxiv.org/abs/2508.20228v1",
        "abstract": "Recent advances in LLM watermarking methods such as SynthID-Text by Google\nDeepMind offer promising solutions for tracing the provenance of AI-generated\ntext. However, our robustness assessment reveals that SynthID-Text is\nvulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste\nmodifications, and back-translation, which can significantly degrade watermark\ndetectability. To address these limitations, we propose SynGuard, a hybrid\nframework that combines the semantic alignment strength of Semantic Information\nRetrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.\nOur approach jointly embeds watermarks at both lexical and semantic levels,\nenabling robust provenance tracking while preserving the original meaning.\nExperimental results across multiple attack scenarios show that SynGuard\nimproves watermark recovery by an average of 11.1\\% in F1 score compared to\nSynthID-Text. These findings demonstrate the effectiveness of semantic-aware\nwatermarking in resisting real-world tampering. All code, datasets, and\nevaluation scripts are publicly available at:\nhttps://github.com/githshine/SynGuard."
    },
    {
        "date": "2025-08",
        "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models",
        "author": "Cheng-Kai Yeh, Hsing-Wang Lee, Chung-Hung Kuo, and Hen-Hsen Huang",
        "link": "http://arxiv.org/abs/2509.03537v1",
        "abstract": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization."
    },
    {
        "date": "2025-08",
        "title": "PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence",
        "author": "Zheng Li, Yanming Guo, WenZhe Liu, Xueyi Zhang, Zhaoyun Ding, Long Xu, and Mingrui Lao",
        "link": "http://arxiv.org/abs/2508.20066v1",
        "abstract": "Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios."
    },
    {
        "date": "2025-08",
        "title": "Pruning Strategies for Backdoor Defense in LLMs",
        "author": "Santosh Chapagain, Shah Muhammad Hamdi, and Soukaina Filali Boubrahimi",
        "link": "http://arxiv.org/abs/2508.20032v1",
        "abstract": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks."
    },
    {
        "date": "2025-08",
        "title": "Robust Detection of Synthetic Tabular Data under Schema Variability",
        "author": "G. Charbel N. Kindji, Elisa Fromont, Lina Maria Rojas-Barahona, and Tanguy Urvoy",
        "link": "http://arxiv.org/abs/2509.00092v1",
        "abstract": "The rise of powerful generative models has sparked concerns over data\nauthenticity. While detection methods have been extensively developed for\nimages and text, the case of tabular data, despite its ubiquity, has been\nlargely overlooked. Yet, detecting synthetic tabular data is especially\nchallenging due to its heterogeneous structure and unseen formats at test time.\nWe address the underexplored task of detecting synthetic tabular data in the\nwild, where tables have variable and previously unseen schemas. We introduce a\nnovel datum-wise transformer architecture that significantly outperforms the\nonly previously published baseline, improving both AUC and accuracy by 7\npoints. By incorporating a table-adaptation component, our model gains an\nadditional 7 accuracy points, demonstrating enhanced robustness. This work\nprovides the first strong evidence that detecting synthetic tabular data in\nreal-world conditions is not only feasible, but can be done with high\nreliability."
    },
    {
        "date": "2025-08",
        "title": "Learning from Peers: Collaborative Ensemble Adversarial Training",
        "author": "Li Dengjin, Guo Yanming, Xie Yuxiang, Li Zheng, Chen Jiangming, Li Xiaolong, and Lao Mingrui",
        "link": "http://arxiv.org/abs/2509.00089v1",
        "abstract": "Ensemble Adversarial Training (EAT) attempts to enhance the robustness of\nmodels against adversarial attacks by leveraging multiple models. However,\ncurrent EAT strategies tend to train the sub-models independently, ignoring the\ncooperative benefits between sub-models. Through detailed inspections of the\nprocess of EAT, we find that that samples with classification disparities\nbetween sub-models are close to the decision boundary of ensemble, exerting\ngreater influence on the robustness of ensemble. To this end, we propose a\nnovel yet efficient Collaborative Ensemble Adversarial Training (CEAT), to\nhighlight the cooperative learning among sub-models in the ensemble. To be\nspecific, samples with larger predictive disparities between the sub-models\nwill receive greater attention during the adversarial training of the other\nsub-models. CEAT leverages the probability disparities to adaptively assign\nweights to different samples, by incorporating a calibrating distance\nregularization. Extensive experiments on widely-adopted datasets show that our\nproposed method achieves the state-of-the-art performance over competitive EAT\nmethods. It is noteworthy that CEAT is model-agnostic, which can be seamlessly\nadapted into various ensemble methods with flexible applicability."
    },
    {
        "date": "2025-08",
        "title": "Gradient Rectification for Robust Calibration under Distribution Shift",
        "author": "Yilin Zhang, Cai Xu, You Wu, Ziyu Guan, and Wei Zhao",
        "link": "http://arxiv.org/abs/2508.19830v1",
        "abstract": "Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance."
    },
    {
        "date": "2025-08",
        "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning",
        "author": "Viktor Valadi, Mattias \u00c5kesson, Johan \u00d6stman, Salman Toor, and Andreas Hellander",
        "link": "http://arxiv.org/abs/2508.19819v1",
        "abstract": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios."
    },
    {
        "date": "2025-08",
        "title": "POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection",
        "author": "Chenguang Liu, Chisheng Wang, Yuhua Cai, Chuanhua Zhu, and Qingquan Li",
        "link": "http://arxiv.org/abs/2508.19742v1",
        "abstract": "Line segment detection in images has been studied for several decades.\nExisting line segment detectors can be roughly divided into two categories:\ngeneric line segment detectors and wireframe line segment detectors. Generic\nline segment detectors aim to detect all meaningful line segments in images and\ntraditional approaches usually fall into this category. Recent deep learning\nbased approaches are mostly wireframe line segment detectors. They detect only\nline segments that are geometrically meaningful and have large spatial support.\nDue to the difference in the aim of design, the performance of generic line\nsegment detectors for the task of wireframe line segment detection won't be\nsatisfactory, and vice versa. In this work, we propose a robust framework that\ncan be used for both generic line segment detection and wireframe line segment\ndetection. The proposed method is an improved version of the Pixel Orientation\nEstimation (POE) method. It is thus named as POEv2. POEv2 detects line segments\nfrom edge strength maps, and can be combined with any edge detector. We show in\nour experiments that by combining the proposed POEv2 with an efficient edge\ndetector, it achieves state-of-the-art performance on three publicly available\ndatasets."
    },
    {
        "date": "2025-08",
        "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses",
        "author": "Lincan Li, Bolin Shen, Chenxi Zhao, Yuxiang Sun, Kaixiang Zhao, Shirui Pan, and Yushun Dong",
        "link": "http://arxiv.org/abs/2508.19641v1",
        "abstract": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community."
    },
    {
        "date": "2025-08",
        "title": "Quantization Robustness to Input Degradations for Object Detection",
        "author": "Toghrul Karimov, Hassan Imani, and Allan Kazakov",
        "link": "http://arxiv.org/abs/2508.19600v1",
        "abstract": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID."
    },
    {
        "date": "2025-08",
        "title": "Robustness is Important: Limitations of LLMs for Data Fitting",
        "author": "Hejia Liu, Mochen Yang, and Gediminas Adomavicius",
        "link": "http://arxiv.org/abs/2508.19563v2",
        "abstract": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool."
    },
    {
        "date": "2025-08",
        "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills",
        "author": "David Noever",
        "link": "http://arxiv.org/abs/2508.19500v1",
        "abstract": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite."
    },
    {
        "date": "2025-08",
        "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense",
        "author": "Xavier Cadet, Simona Boboila, Sie Hendrata Dharmawan, Alina Oprea, and Peter Chin",
        "link": "http://arxiv.org/abs/2508.19488v1",
        "abstract": "Cyber defense requires automating defensive decision-making under stealthy,\ndeceptive, and continuously evolving adversarial strategies. The FlipIt game\nprovides a foundational framework for modeling interactions between a defender\nand an advanced adversary that compromises a system without being immediately\ndetected. In FlipIt, the attacker and defender compete to control a shared\nresource by performing a Flip action and paying a cost. However, the existing\nFlipIt frameworks rely on a small number of heuristics or specialized learning\ntechniques, which can lead to brittleness and the inability to adapt to new\nattacks. To address these limitations, we introduce PoolFlip, a multi-agent gym\nenvironment that extends the FlipIt game to allow efficient learning for\nattackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent\nreinforcement learning (MARL) approach that leverages population-based training\nto train defender agents equipped to generalize against a range of unknown,\npotentially adaptive opponents. Our empirical results suggest that Flip-PSRO\ndefenders are $2\\times$ more effective than baselines to generalize to a\nheuristic attack not exposed in training. In addition, our newly designed\nownership-based utility functions ensure that Flip-PSRO defenders maintain a\nhigh level of control while optimizing performance."
    },
    {
        "date": "2025-08",
        "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification",
        "author": "Cagla Ipek Kocal, Onat Gungor, Tajana Rosing, and Baris Aksanli",
        "link": "http://arxiv.org/abs/2508.19456v1",
        "abstract": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge due to the high\ncomplexity of model architectures and the large volume of sequential data that\nmust be processed in real time. This challenge is further compounded by\nadversarial attacks, emphasizing the need for resilient methods that ensure\nrobust performance and efficient model selection. To address this challenge, we\npropose ReLATE+, a comprehensive framework that detects and classifies\nadversarial attacks, adaptively selects deep learning models based on\ndataset-level similarity, and thus substantially reduces retraining costs\nrelative to conventional methods that do not leverage prior knowledge, while\nmaintaining strong performance. ReLATE+ first checks whether the incoming data\nis adversarial and, if so, classifies the attack type, using this insight to\nidentify a similar dataset from a repository and enable the reuse of the\nbest-performing associated model. This approach ensures strong performance\nwhile reducing the need for retraining, and it generalizes well across\ndifferent domains with varying data distributions and feature spaces.\nExperiments show that ReLATE+ reduces computational overhead by an average of\n77.68%, enhancing adversarial resilience and streamlining robust model\nselection, all without sacrificing performance, within 2.02% of Oracle."
    },
    {
        "date": "2025-08",
        "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks (extended version)",
        "author": "Kangfeng Ye, Roberto Metere, Jim Woodcock, and Poonam Yadav",
        "link": "http://arxiv.org/abs/2508.19430v2",
        "abstract": "Formal verification is crucial for ensuring the robustness of security\nprotocols against adversarial attacks. The Needham-Schroeder protocol, a\nfoundational authentication mechanism, has been extensively studied, including\nits integration with Physical Layer Security (PLS) techniques such as\nwatermarking and jamming. Recent research has used ProVerif to verify these\nmechanisms in terms of secrecy. However, the ProVerif-based approach limits the\nability to improve understanding of security beyond verification results. To\novercome these limitations, we re-model the same protocol using an Isabelle\nformalism that generates sound animation, enabling interactive and automated\nformal verification of security protocols. Our modelling and verification\nframework is generic and highly configurable, supporting both cryptography and\nPLS. For the same protocol, we have conducted a comprehensive analysis (secrecy\nand authenticity in four different eavesdropper locations under both passive\nand active attacks) using our new web interface. Our findings not only\nsuccessfully reproduce and reinforce previous results on secrecy but also\nreveal an uncommon but expected outcome: authenticity is preserved across all\nexamined scenarios, even in cases where secrecy is compromised. We have\nproposed a PLS-based Diffie-Hellman protocol that integrates watermarking and\njamming, and our analysis shows that it is secure for deriving a session key\nwith required authentication. These highlight the advantages of our novel\napproach, demonstrating its robustness in formally verifying security\nproperties beyond conventional methods."
    },
    {
        "date": "2025-08",
        "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition",
        "author": "Tai Vu",
        "link": "http://arxiv.org/abs/2509.00077v1",
        "abstract": "Speech Emotion Recognition (SER) presents a significant yet persistent\nchallenge in human-computer interaction. While deep learning has advanced\nspoken language processing, achieving high performance on limited datasets\nremains a critical hurdle. This paper confronts this issue by developing and\nevaluating a suite of machine learning models, including Support Vector\nMachines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional\nNeural Networks (CNNs), for automated emotion classification in human speech.\nWe demonstrate that by strategically employing transfer learning and innovative\ndata augmentation techniques, our models can achieve impressive performance\ndespite the constraints of a relatively small dataset. Our most effective\nmodel, a ResNet34 architecture, establishes a new performance benchmark on the\ncombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1\nscore of 0.631. These results underscore the substantial benefits of leveraging\npre-trained models and data augmentation to overcome data scarcity, thereby\npaving the way for more robust and generalizable SER systems."
    },
    {
        "date": "2025-08",
        "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction",
        "author": "Fatemeh Haji, Mazal Bethany, Cho-Yu Jason Chiang, Anthony Rios, and Peyman Najafirad",
        "link": "http://arxiv.org/abs/2508.19359v1",
        "abstract": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets."
    },
    {
        "date": "2025-08",
        "title": "Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning",
        "author": "Pilsung Kang",
        "link": "http://arxiv.org/abs/2508.19327v1",
        "abstract": "Bell's theorem reveals a profound conflict between quantum mechanics and\nlocal realism, a conflict we reinterpret through the modern lens of causal\ninference. We propose and computationally validate a framework where quantum\nentanglement acts as a \"super-confounding\" resource, generating correlations\nthat violate the classical causal bounds set by Bell's inequalities. This work\nmakes three key contributions: First, we establish a physical hierarchy of\nconfounding (Quantum > Classical) and introduce Confounding Strength (CS) to\nquantify this effect. Second, we provide a circuit-based implementation of the\nquantum $\\mathcal{DO}$-calculus to distinguish causality from spurious\ncorrelation. Finally, we apply this calculus to a quantum machine learning\nproblem, where causal feature selection yields a statistically significant\n11.3% average absolute improvement in model robustness. Our framework bridges\nquantum foundations and causal AI, offering a new, practical perspective on\nquantum correlations."
    },
    {
        "date": "2025-08",
        "title": "Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness",
        "author": "Wenchuan Mu, and Kwan Hui Lim",
        "link": "http://arxiv.org/abs/2508.19183v1",
        "abstract": "In safety-critical deep learning applications, robustness measures the\nability of neural models that handle imperceptible perturbations in input data,\nwhich may lead to potential safety hazards. Existing pre-deployment robustness\nassessment methods typically suffer from significant trade-offs between\ncomputational cost and measurement precision, limiting their practical utility.\nTo address these limitations, this paper conducts a comprehensive comparative\nanalysis of existing robustness definitions and associated assessment\nmethodologies. We propose tower robustness to evaluate robustness, which is a\nnovel, practical metric based on hypothesis testing to quantitatively evaluate\nprobabilistic robustness, enabling more rigorous and efficient pre-deployment\nassessments. Our extensive comparative evaluation illustrates the advantages\nand applicability of our proposed approach, thereby advancing the systematic\nunderstanding and enhancement of model robustness in safety-critical deep\nlearning applications."
    },
    {
        "date": "2025-08",
        "title": "Random forest-based out-of-distribution detection for robust lung cancer segmentation",
        "author": "Aneesh Rangnekar, and Harini Veeraraghavan",
        "link": "http://arxiv.org/abs/2508.19112v1",
        "abstract": "Accurate detection and segmentation of cancerous lesions from computed\ntomography (CT) scans is essential for automated treatment planning and cancer\ntreatment response assessment. Transformer-based models with self-supervised\npretraining can produce reliably accurate segmentation from in-distribution\n(ID) data but degrade when applied to out-of-distribution (OOD) datasets. We\naddress this challenge with RF-Deep, a random forest classifier that utilizes\ndeep features from a pretrained transformer encoder of the segmentation model\nto detect OOD scans and enhance segmentation reliability. The segmentation\nmodel comprises a Swin Transformer encoder, pretrained with masked image\nmodeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and\nnon-cancerous conditions, with a convolution decoder, trained to segment lung\ncancers in 317 3D scans. Independent testing was performed on 603 3D CT public\ndatasets that included one ID dataset and four OOD datasets comprising chest\nCTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney\ncancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of\n18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,\nconsistently outperforming established OOD approaches. The RF-Deep classifier\nprovides a simple and effective approach to enhance reliability of cancer\nsegmentation in ID and OOD scenarios."
    },
    {
        "date": "2025-08",
        "title": "An Investigation on Group Query Hallucination Attacks",
        "author": "Kehao Miao, and Xiaolong Jin",
        "link": "http://arxiv.org/abs/2508.19321v1",
        "abstract": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models."
    },
    {
        "date": "2025-08",
        "title": "Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection",
        "author": "Sidahmed Benabderrahmane, and Talal Rahwan",
        "link": "http://arxiv.org/abs/2508.19072v1",
        "abstract": "Advanced Persistent Threats (APTs) represent a growing menace to modern\ndigital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,\nadaptive, and long-lasting, often bypassing signature-based detection systems.\nThis paper introduces a novel framework for APT detection that unites deep\nlearning, reinforcement learning (RL), and active learning into a cohesive,\nadaptive defense system. Our system combines auto-encoders for latent\nbehavioral encoding with a multi-agent ensemble of RL-based defenders, each\ntrained to distinguish between benign and malicious process behaviors. We\nidentify a critical challenge in existing detection systems: their static\nnature and inability to adapt to evolving attack strategies. To this end, our\narchitecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial\ndefenders), each analyzing latent vectors generated by an auto-encoder. When\nany agent is uncertain about its decision, the system triggers an active\nlearning loop to simulate expert feedback, thus refining decision boundaries.\nAn ensemble voting mechanism, weighted by each agent's performance, ensures\nrobust final predictions."
    },
    {
        "date": "2025-08",
        "title": "Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI",
        "author": "Marcin Moskalewicz, Anna Sterna, Marek Pokropski, and Paula Flores",
        "link": "http://arxiv.org/abs/2508.19008v1",
        "abstract": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias."
    },
    {
        "date": "2025-08",
        "title": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres",
        "author": "Ronal Singh, Shahroz Tariq, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris, and Martin Lochner",
        "link": "http://arxiv.org/abs/2508.18947v1",
        "abstract": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts."
    },
    {
        "date": "2025-08",
        "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation",
        "author": "David Egea, Barproda Halder, and Sanghamitra Dutta",
        "link": "http://arxiv.org/abs/2508.18933v1",
        "abstract": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis."
    },
    {
        "date": "2025-08",
        "title": "SegReConcat: A Data Augmentation Method for Voice Anonymization Attack",
        "author": "Ridwan Arefeen, Xiaoxiao Miao, Rong Tong, Aik Beng Ng, and Simon See",
        "link": "http://arxiv.org/abs/2508.18907v1",
        "abstract": "Anonymization of voice seeks to conceal the identity of the speaker while\nmaintaining the utility of speech data. However, residual speaker cues often\npersist, which pose privacy risks. We propose SegReConcat, a data augmentation\nmethod for attacker-side enhancement of automatic speaker verification systems.\nSegReConcat segments anonymized speech at the word level, rearranges segments\nusing random or similarity-based strategies to disrupt long-term contextual\ncues, and concatenates them with the original utterance, allowing an attacker\nto learn source speaker traits from multiple perspectives. The proposed method\nhas been evaluated in the VoicePrivacy Attacker Challenge 2024 framework across\nseven anonymization systems, SegReConcat improves de-anonymization on five out\nof seven systems."
    },
    {
        "date": "2025-08",
        "title": "Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models",
        "author": "Yuexuan Xia, Benteng Ma, Jiang He, Zhiyong Wang, Qi Dou, and Yong Xia",
        "link": "http://arxiv.org/abs/2508.18886v1",
        "abstract": "Ensuring fairness across demographic groups in medical diagnosis is essential\nfor equitable healthcare, particularly under distribution shifts caused by\nvariations in imaging equipment and clinical practice. Vision-language models\n(VLMs) exhibit strong generalization, and text prompts encode identity\nattributes, enabling explicit identification and removal of sensitive\ndirections. However, existing debiasing approaches typically address vision and\ntext modalities independently, leaving residual cross-modal misalignment and\nfairness gaps. To address this challenge, we propose DualFairVL, a multimodal\nprompt-learning framework that jointly debiases and aligns cross-modal\nrepresentations. DualFairVL employs a parallel dual-branch architecture that\nseparates sensitive and target attributes, enabling disentangled yet aligned\nrepresentations across modalities. Approximately orthogonal text anchors are\nconstructed via linear projections, guiding cross-attention mechanisms to\nproduce fused features. A hypernetwork further disentangles attribute-related\ninformation and generates instance-aware visual prompts, which encode\ndual-modal cues for fairness and robustness. Prototype-based regularization is\napplied in the visual branch to enforce separation of sensitive features and\nstrengthen alignment with textual anchors. Extensive experiments on eight\nmedical imaging datasets across four modalities show that DualFairVL achieves\nstate-of-the-art fairness and accuracy under both in- and out-of-distribution\nsettings, outperforming full fine-tuning and parameter-efficient baselines with\nonly 3.6M trainable parameters. Code will be released upon publication."
    },
    {
        "date": "2025-08",
        "title": "Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models",
        "author": "Rui Zhang, Zihan Wang, Tianli Yang, Hongwei Li, Wenbo Jiang, Qingchuan Zhao, Yang Liu, and Guowen Xu",
        "link": "http://arxiv.org/abs/2508.18805v1",
        "abstract": "Vision-Language Models (VLMs) are increasingly deployed in real-world\napplications, but their high inference cost makes them vulnerable to resource\nconsumption attacks. Prior attacks attempt to extend VLM output sequences by\noptimizing adversarial images, thereby increasing inference costs. However,\nthese extended outputs often introduce irrelevant abnormal content,\ncompromising attack stealthiness. This trade-off between effectiveness and\nstealthiness poses a major limitation for existing attacks. To address this\nchallenge, we propose \\textit{Hidden Tail}, a stealthy resource consumption\nattack that crafts prompt-agnostic adversarial images, inducing VLMs to\ngenerate maximum-length outputs by appending special tokens invisible to users.\nOur method employs a composite loss function that balances semantic\npreservation, repetitive special token induction, and suppression of the\nend-of-sequence (EOS) token, optimized via a dynamic weighting strategy.\nExtensive experiments show that \\textit{Hidden Tail} outperforms existing\nattacks, increasing output length by up to 19.2$\\times$ and reaching the\nmaximum token limit, while preserving attack stealthiness. These results\nhighlight the urgent need to improve the robustness of VLMs against\nefficiency-oriented adversarial threats. Our code is available at\nhttps://github.com/zhangrui4041/Hidden_Tail."
    },
    {
        "date": "2025-08",
        "title": "Robust and Label-Efficient Deep Waste Detection",
        "author": "Hassan Abid, Khan Muhammad, and Muhammad Haris Khan",
        "link": "http://arxiv.org/abs/2508.18799v1",
        "abstract": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection."
    },
    {
        "date": "2025-08",
        "title": "Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges",
        "author": "Peng Gu, Shuangchen Li, Dylan Stow, Russell Barnes, Liu Liu, Yuan Xie, and Eren Kursshan",
        "link": "http://arxiv.org/abs/2508.19309v1",
        "abstract": "3D die stacking and 2.5D interposer design are promising technologies to\nimprove integration density, performance and cost. Current approaches face\nserious issues in dealing with emerging security challenges such as side\nchannel attacks, hardware trojans, secure IC manufacturing and IP piracy. By\nutilizing intrinsic characteristics of 2.5D and 3D technologies, we propose\nnovel opportunities in designing secure systems. We present: (i) a 3D\narchitecture for shielding side-channel information; (ii) split fabrication\nusing active interposers; (iii) circuit camouflage on monolithic 3D IC, and\n(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges\nof these designs are discussed, showing that the new designs can improve\nexisting countermeasures against security threats and further provide new\nsecurity features."
    },
    {
        "date": "2025-08",
        "title": "A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework",
        "author": "Yuhui Tao, Yizhe Zhang, and Qiang Chen",
        "link": "http://arxiv.org/abs/2508.18790v1",
        "abstract": "The development of artificial intelligence models for macular edema (ME)\nanaly-sis always relies on expert-annotated pixel-level image datasets which\nare expen-sive to collect prospectively. While anomaly-detection-based\nweakly-supervised methods have shown promise in edema area (EA) segmentation\ntask, their per-formance still lags behind fully-supervised approaches. In this\npaper, we leverage the strong correlation between EA and retinal layers in\nspectral-domain optical coherence tomography (SD-OCT) images, along with the\nupdate characteristics of weakly-supervised learning, to enhance an\noff-the-shelf adversarial framework for EA segmentation with a novel\nlayer-structure-guided post-processing step and a test-time-adaptation (TTA)\nstrategy. By incorporating additional retinal lay-er information, our framework\nreframes the dense EA prediction task as one of confirming intersection points\nbetween the EA contour and retinal layers, result-ing in predictions that\nbetter align with the shape prior of EA. Besides, the TTA framework further\nhelps address discrepancies in the manifestations and presen-tations of EA\nbetween training and test sets. Extensive experiments on two pub-licly\navailable datasets demonstrate that these two proposed ingredients can im-prove\nthe accuracy and robustness of EA segmentation, bridging the gap between\nweakly-supervised and fully-supervised models."
    },
    {
        "date": "2025-08",
        "title": "Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation",
        "author": "Hiroto Sakimura, Takayuki Nagaya, Tomoki Nishi, Tetsuo Kurahashi, Katsunori Kohda, and Nobuhiko Muramoto",
        "link": "http://arxiv.org/abs/2508.18782v1",
        "abstract": "Estimating emotional states from physiological signals is a central topic in\naffective computing and psychophysiology. While many emotion estimation systems\nimplicitly assume a stable relationship between physiological features and\nsubjective affect, this assumption has rarely been tested over long timeframes.\nThis study investigates whether such relationships remain consistent across\nseveral months within individuals. We developed a custom measurement system and\nconstructed a longitudinal dataset by collecting physiological signals --\nincluding blood volume pulse, electrodermal activity (EDA), skin temperature,\nand acceleration--along with self-reported emotional states from 24\nparticipants over two three-month periods. Data were collected in naturalistic\nworking environments, allowing analysis of the relationship between\nphysiological features and subjective arousal in everyday contexts. We examined\nhow physiological-arousal relationships evolve over time by using Explainable\nBoosting Machines (EBMs) to ensure model interpretability. A model trained on\n1st-period data showed a 5\\% decrease in accuracy when tested on 2nd-period\ndata, indicating long-term variability in physiological-arousal associations.\nEBM-based comparisons further revealed that while heart rate remained a\nrelatively stable predictor, minimum EDA exhibited substantial individual-level\nfluctuations between periods. While the number of participants is limited,\nthese findings highlight the need to account for temporal variability in\nphysiological-arousal relationships and suggest that emotion estimation models\nshould be periodically updated -- e.g., every five months -- based on observed\nshift trends to maintain robust performance over time."
    },
    {
        "date": "2025-08",
        "title": "Hybrid Perception and Equivariant Diffusion for Robust Multi-Node Rebar Tying",
        "author": "Zhitao Wang, Yirong Xiong, Roberto Horowitz, Yanke Wang, and Yuxing Han",
        "link": "http://arxiv.org/abs/2509.00065v1",
        "abstract": "Rebar tying is a repetitive but critical task in reinforced concrete\nconstruction, typically performed manually at considerable ergonomic risk.\nRecent advances in robotic manipulation hold the potential to automate the\ntying process, yet face challenges in accurately estimating tying poses in\ncongested rebar nodes. In this paper, we introduce a hybrid perception and\nmotion planning approach that integrates geometry-based perception with\nEquivariant Denoising Diffusion on SE(3) (Diffusion-EDFs) to enable robust\nmulti-node rebar tying with minimal training data. Our perception module\nutilizes density-based clustering (DBSCAN), geometry-based node feature\nextraction, and principal component analysis (PCA) to segment rebar bars,\nidentify rebar nodes, and estimate orientation vectors for sequential ranking,\neven in complex, unstructured environments. The motion planner, based on\nDiffusion-EDFs, is trained on as few as 5-10 demonstrations to generate\nsequential end-effector poses that optimize collision avoidance and tying\nefficiency. The proposed system is validated on various rebar meshes, including\nsingle-layer, multi-layer, and cluttered configurations, demonstrating high\nsuccess rates in node detection and accurate sequential tying. Compared with\nconventional approaches that rely on large datasets or extensive manual\nparameter tuning, our method achieves robust, efficient, and adaptable\nmulti-node tying while significantly reducing data requirements. This result\nunderscores the potential of hybrid perception and diffusion-driven planning to\nenhance automation in on-site construction tasks, improving both safety and\nlabor efficiency."
    },
    {
        "date": "2025-08",
        "title": "FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks",
        "author": "Enrique M\u00e1rmol Campos, Aurora Gonz\u00e1lez Vidal, Jos\u00e9 Luis Hern\u00e1ndez Ramos, and Antonio Skarmeta",
        "link": "http://arxiv.org/abs/2508.18737v1",
        "abstract": "Federated Learning (FL) has become a powerful technique for training Machine\nLearning (ML) models in a decentralized manner, preserving the privacy of the\ntraining datasets involved. However, the decentralized nature of FL limits the\nvisibility of the training process, relying heavily on the honesty of\nparticipating clients. This assumption opens the door to malicious third\nparties, known as Byzantine clients, which can poison the training process by\nsubmitting false model updates. Such malicious clients may engage in poisoning\nattacks, manipulating either the dataset or the model parameters to induce\nmisclassification. In response, this study introduces FLAegis, a two-stage\ndefensive framework designed to identify Byzantine clients and improve the\nrobustness of FL systems. Our approach leverages symbolic time series\ntransformation (SAX) to amplify the differences between benign and malicious\nmodels, and spectral clustering, which enables accurate detection of\nadversarial behavior. Furthermore, we incorporate a robust FFT-based\naggregation function as a final layer to mitigate the impact of those Byzantine\nclients that manage to evade prior defenses. We rigorously evaluate our method\nagainst five poisoning attacks, ranging from simple label flipping to adaptive\noptimization-based strategies. Notably, our approach outperforms\nstate-of-the-art defenses in both detection precision and final model accuracy,\nmaintaining consistently high performance even under strong adversarial\nconditions."
    },
    {
        "date": "2025-08",
        "title": "SkyTrust: Blockchain-Enhanced UAV Security for NTNs with Dynamic Trust and Energy-Aware Consensus",
        "author": "Afan Ali, and Irfanullah Khan",
        "link": "http://arxiv.org/abs/2508.18735v1",
        "abstract": "Non-Terrestrial Networks (NTNs) based on Unmanned Aerial Vehicles (UAVs) as\nbase stations are extremely susceptible to security attacks due to their\ndistributed and dynamic nature, which makes them vulnerable to rogue nodes. In\nthis paper, a new Dynamic Trust Score Adjustment Mechanism with Energy-Aware\nConsensus (DTSAM-EAC) is proposed to enhance security in UAV-based NTNs. The\nproposed framework integrates a permissioned Hyperledger Fabric blockchain with\nFederated Learning (FL) to support privacy-preserving trust evaluation. Trust\nratings are updated continuously through weighted aggregation of past trust,\npresent behavior, and energy contribution, thus making the system adaptive to\nchanging network conditions. An energy-aware consensus mechanism prioritizes\nUAVs with greater available energy for block validation, ensuring efficient use\nof resources under resource-constrained environments. FL aggregation with\ntrust-weighting further increases the resilience of the global trust model.\nSimulation results verify the designed framework achieves 94\\% trust score\nprediction accuracy and 96\\% rogue UAV detection rate while outperforming\ncentralized and static baselines of trust-based solutions on privacy, energy\nefficiency, and reliability. It complies with 6G requirements in terms of\ndistributed intelligence and sustainability and is an energy-efficient and\nscalable solution to secure NTNs."
    },
    {
        "date": "2025-08",
        "title": "Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion",
        "author": "DongHoon Lim, YoungChae Kim, Dong-Hyun Kim, Da-Hee Yang, and Joon-Hyuk Chang",
        "link": "http://arxiv.org/abs/2508.18734v1",
        "abstract": "Robust audio-visual speech recognition (AVSR) in noisy environments remains\nchallenging, as existing systems struggle to estimate audio reliability and\ndynamically adjust modality reliance. We propose router-gated cross-modal\nfeature fusion, a novel AVSR framework that adaptively reweights audio and\nvisual features based on token-level acoustic corruption scores. Using an\naudio-visual feature fusion-based router, our method down-weights unreliable\naudio tokens and reinforces visual cues through gated cross-attention in each\ndecoder layer. This enables the model to pivot toward the visual modality when\naudio quality deteriorates. Experiments on LRS3 demonstrate that our approach\nachieves an 16.51-42.67% relative reduction in word error rate compared to\nAV-HuBERT. Ablation studies confirm that both the router and gating mechanism\ncontribute to improved robustness under real-world acoustic noise."
    },
    {
        "date": "2025-08",
        "title": "Flatness-aware Curriculum Learning via Adversarial Difficulty",
        "author": "Hiroaki Aizawa, and Yoshikazu Hayashi",
        "link": "http://arxiv.org/abs/2508.18726v1",
        "abstract": "Neural networks trained by empirical risk minimization often suffer from\noverfitting, especially to specific samples or domains, which leads to poor\ngeneralization. Curriculum Learning (CL) addresses this issue by selecting\ntraining samples based on the difficulty. From the optimization perspective,\nmethods such as Sharpness-Aware Minimization (SAM) improve robustness and\ngeneralization by seeking flat minima. However, combining CL with SAM is not\nstraightforward. In flat regions, both the loss values and the gradient norms\ntend to become uniformly small, which makes it difficult to evaluate sample\ndifficulty and design an effective curriculum. To overcome this problem, we\npropose the Adversarial Difficulty Measure (ADM), which quantifies adversarial\nvulnerability by leveraging the robustness properties of models trained toward\nflat minima. Unlike loss- or gradient-based measures, which become ineffective\nas training progresses into flatter regions, ADM remains informative by\nmeasuring the normalized loss gap between original and adversarial examples. We\nincorporate ADM into CL-based training with SAM to dynamically assess sample\ndifficulty. We evaluated our approach on image classification tasks,\nfine-grained recognition, and domain generalization. The results demonstrate\nthat our method preserves the strengths of both CL and SAM while outperforming\nexisting curriculum-based and flatness-aware training strategies."
    },
    {
        "date": "2025-08",
        "title": "A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition",
        "author": "Wasi Ullah, Yasir Noman Khalid, and Saddam Hussain Khan",
        "link": "http://arxiv.org/abs/2508.18695v2",
        "abstract": "Human Activity Recognition (HAR) plays a pivotal role in various\napplications, including smart surveillance, healthcare, assistive technologies,\nsports analytics, etc. However, HAR systems still face critical challenges,\nincluding high computational costs, redundant features, and limited scalability\nin real-time scenarios. An optimized hybrid deep learning framework is\nintroduced that integrates a customized InceptionV3, an LSTM architecture, and\na novel ensemble-based feature selection strategy. The proposed framework first\nextracts spatial descriptors using the customized InceptionV3 model, which\ncaptures multilevel contextual patterns, region homogeneity, and fine-grained\nlocalization cues. The temporal dependencies across frames are then modeled\nusing LSTMs to effectively encode motion dynamics. Finally, an ensemble-based\ngenetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)\nis employed to select a compact and optimized feature set by dynamically\nbalancing objectives such as accuracy, redundancy, uniqueness, and complexity\nreduction. Consequently, the selected feature subsets, which are both diverse\nand discriminative, enable various lightweight machine learning classifiers to\nachieve accurate and robust HAR in heterogeneous environments. Experimental\nresults on the robust UCF-YouTube dataset, which presents challenges such as\nocclusion, cluttered backgrounds, motion dynamics, and poor illumination,\ndemonstrate good performance. The proposed approach achieves 99.65% recognition\naccuracy, reduces features to as few as 7, and enhances inference time. The\nlightweight and scalable nature of the HAR system supports real-time deployment\non edge devices such as Raspberry Pi, enabling practical applications in\nintelligent, resource-aware environments, including public safety, assistive\ntechnology, and autonomous monitoring systems."
    },
    {
        "date": "2025-08",
        "title": "Membership Inference Attacks on LLM-based Recommender Systems",
        "author": "Jiajie He, Yuechun Gu, Min-Chun Chen, and Keke Chen",
        "link": "http://arxiv.org/abs/2508.18665v1",
        "abstract": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots."
    },
    {
        "date": "2025-08",
        "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation",
        "author": "Runpeng Geng, Yanting Wang, Ying Chen, and Jinyuan Jia",
        "link": "http://arxiv.org/abs/2508.18652v1",
        "abstract": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems."
    },
    {
        "date": "2025-08",
        "title": "PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality",
        "author": "Nanxi Li, Zhengyue Zhao, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2508.18649v1",
        "abstract": "Safeguarding vision-language models (VLMs) is a critical challenge, as\nexisting methods often suffer from over-defense, which harms utility, or rely\non shallow alignment, failing to detect complex threats that require deep\nreasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated\nSafety in Multimodality), a system2-like framework that aligns VLMs by\nembedding a structured, safety-aware reasoning process. Our framework consists\nof two key components: PRISM-CoT, a dataset that teaches safety-aware\nchain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree\nSearch (MCTS) to further refine this reasoning through Direct Preference\nOptimization to help obtain a delicate safety boundary. Comprehensive\nevaluations demonstrate PRISM's effectiveness, achieving remarkably low attack\nsuccess rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%\nimprovement over the previous best method on VLBreak for LLaVA-1.5. PRISM also\nexhibits strong robustness against adaptive attacks, significantly increasing\ncomputational costs for adversaries, and generalizes effectively to\nout-of-distribution challenges, reducing attack success rates to just 8.70% on\nthe challenging multi-image MIS benchmark. Remarkably, this robust defense is\nachieved while preserving, and in some cases enhancing, model utility. To\npromote reproducibility, we have made our code, data, and model weights\navailable at https://github.com/SaFoLab-WISC/PRISM."
    },
    {
        "date": "2025-08",
        "title": "Secure Password Generator Based on Secure Pseudo-Random Number Generator",
        "author": "Abel C. H. Chen",
        "link": "http://arxiv.org/abs/2509.02578v1",
        "abstract": "In recent years, numerous incidents involving the leakage of website accounts\nand text passwords (referred to as passwords) have raised significant concerns\nregarding the potential exposure of personal information. These events\nunderscore the critical importance of both information security and password\nprotection. While many of these breaches are attributable to vulnerabilities\nwithin website infrastructure, the strength and security of the passwords\nthemselves also play a crucial role. Consequently, the creation of secure\npasswords constitutes a fundamental aspect of enhancing overall system security\nand protecting personal data. In response to these challenges, this study\npresents a secure password generation approach utilizing a cryptographically\nsecure Pseudo-Random Number Generator (PRNG). The generator is implemented\nusing a range of Message Authentication Code (MAC) algorithms, including the\nKeyed-Hash Message Authentication Code (HMAC), Cipher-based Message\nAuthentication Code (CMAC), and KECCAK Message Authentication Code (KMAC), to\nproduce robust random values suitable for password generation. To evaluate the\nproposed method, empirical assessments were conducted in accordance with the\nguidelines provided in the National Institute of Standards and Technology\n(NIST) Special Publication (SP) 800-90B. The evaluation focused on two primary\naspects: entropy estimation and verification of independent and identically\ndistributed (IID) properties. Experimental results indicate that the proposed\nmethod satisfies both entropy and IID requirements, thereby demonstrating its\nability to generate passwords with a high degree of randomness and security."
    },
    {
        "date": "2025-08",
        "title": "Securing Face and Fingerprint Templates in Humanitarian Biometric Systems",
        "author": "Giuseppe Stragapede, Sam Merrick, Vedrana Krivoku\u0107a Hahn, Justin Sukaitis, and Vincent Graf Narbel",
        "link": "http://arxiv.org/abs/2508.18415v1",
        "abstract": "In humanitarian and emergency scenarios, the use of biometrics can\ndramatically improve the efficiency of operations, but it poses risks for the\ndata subjects, which are exacerbated in contexts of vulnerability. To address\nthis, we present a mobile biometric system implementing a biometric template\nprotection (BTP) scheme suitable for these scenarios. After rigorously\nformulating the functional, operational, and security and privacy requirements\nof these contexts, we perform a broad comparative analysis of the BTP\nlandscape. PolyProtect, a method designed to operate on neural network face\nembeddings, is identified as the most suitable method due to its effectiveness,\nmodularity, and lightweight computational burden. We evaluate PolyProtect in\nterms of verification and identification accuracy, irreversibility, and\nunlinkability, when this BTP method is applied to face embeddings extracted\nusing EdgeFace, a novel state-of-the-art efficient feature extractor, on a\nreal-world face dataset from a humanitarian field project in Ethiopia.\nMoreover, as PolyProtect promises to be modality-independent, we extend its\nevaluation to fingerprints. To the best of our knowledge, this is the first\ntime that PolyProtect has been evaluated for the identification scenario and\nfor fingerprint biometrics. Our experimental results are promising, and we plan\nto release our code"
    },
    {
        "date": "2025-08",
        "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning",
        "author": "Antonio Guillen-Perez",
        "link": "http://arxiv.org/abs/2508.18397v1",
        "abstract": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents."
    },
    {
        "date": "2025-08",
        "title": "Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability",
        "author": "Syamantak Sarkar, Revoti P. Bora, Bhupender Kaushal, Sudhish N George, and Kiran Raja",
        "link": "http://arxiv.org/abs/2508.18154v1",
        "abstract": "Class Activation Maps (CAMs) are one of the important methods for visualizing\nregions used by deep learning models. Yet their robustness to different noise\nremains underexplored. In this work, we evaluate and report the resilience of\nvarious CAM methods for different noise perturbations across multiple\narchitectures and datasets. By analyzing the influence of different noise types\non CAM explanations, we assess the susceptibility to noise and the extent to\nwhich dataset characteristics may impact explanation stability. The findings\nhighlight considerable variability in noise sensitivity for various CAMs. We\npropose a robustness metric for CAMs that captures two key properties:\nconsistency and responsiveness. Consistency reflects the ability of CAMs to\nremain stable under input perturbations that do not alter the predicted class,\nwhile responsiveness measures the sensitivity of CAMs to changes in the\nprediction caused by such perturbations. The metric is evaluated empirically\nacross models, different perturbations, and datasets along with complementary\nstatistical tests to exemplify the applicability of our proposed approach."
    },
    {
        "date": "2025-08",
        "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code",
        "author": "Keke Lian, Bin Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichuan Chen, Jie Wu, Haoling Li, Zhaoxuan Li, Jiongchi Yu, Hui Li, and Dong Zhang",
        "link": "http://arxiv.org/abs/2508.18106v1",
        "abstract": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching."
    },
    {
        "date": "2025-08",
        "title": "FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning",
        "author": "Emmanouil Kritharakis, Antonios Makris, Dusan Jakovetic, and Konstantinos Tserpes",
        "link": "http://arxiv.org/abs/2508.18060v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across multiple\nclients while preserving data privacy by keeping local datasets on-device. In\nthis work, we address FL settings where clients may behave adversarially,\nexhibiting Byzantine attacks, while the central server is trusted and equipped\nwith a reference dataset. We propose FedGreed, a resilient aggregation strategy\nfor federated learning that does not require any assumptions about the fraction\nof adversarial participants. FedGreed orders clients' local model updates based\non their loss metrics evaluated against a trusted dataset on the server and\ngreedily selects a subset of clients whose models exhibit the minimal\nevaluation loss. Unlike many existing approaches, our method is designed to\noperate reliably under heterogeneous (non-IID) data distributions, which are\nprevalent in real-world deployments. FedGreed exhibits convergence guarantees\nand bounded optimality gaps under strong adversarial behavior. Experimental\nevaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method\nsignificantly outperforms standard and robust federated learning baselines,\nsuch as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of\nadversarial scenarios considered, including label flipping and Gaussian noise\ninjection attacks. All experiments were conducted using the Flower federated\nlearning framework."
    },
    {
        "date": "2025-08",
        "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience",
        "author": "Xi Wang, Songlei Jian, Shasha Li, Xiaopeng Li, Bin Ji, Jun Ma, Xiaodong Liu, Jing Wang, Feilong Bao, Jianfeng Zhang, Baosheng Wang, and Jie Yu",
        "link": "http://arxiv.org/abs/2508.19292v1",
        "abstract": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}"
    },
    {
        "date": "2025-08",
        "title": "Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation",
        "author": "Xiuheng Wang, Ricardo Borsoi, Arnaud Breloy, and C\u00e9dric Richard",
        "link": "http://arxiv.org/abs/2508.18045v1",
        "abstract": "Non-parametric change-point detection in streaming time series data is a\nlong-standing challenge in signal processing. Recent advancements in statistics\nand machine learning have increasingly addressed this problem for data residing\non Riemannian manifolds. One prominent strategy involves monitoring abrupt\nchanges in the center of mass of the time series. Implemented in a streaming\nfashion, this strategy, however, requires careful step size tuning when\ncomputing the updates of the center of mass. In this paper, we propose to\nleverage robust centroid on manifolds from M-estimation theory to address this\nissue. Our proposal consists of comparing two centroid estimates: the classical\nKarcher mean (sensitive to change) versus one defined from Huber's function\n(robust to change). This comparison leads to the definition of a test statistic\nwhose performance is less sensitive to the underlying estimation method. We\npropose a stochastic Riemannian optimization algorithm to estimate both robust\ncentroids efficiently. Experiments conducted on both simulated and real-world\ndata across two representative manifolds demonstrate the superior performance\nof our proposed method."
    },
    {
        "date": "2025-08",
        "title": "Does simple trump complex? Comparing strategies for adversarial robustness in DNNs",
        "author": "William Brooks, Marelie H. Davel, and Coenraad Mouton",
        "link": "http://arxiv.org/abs/2508.18019v1",
        "abstract": "Deep Neural Networks (DNNs) have shown substantial success in various\napplications but remain vulnerable to adversarial attacks. This study aims to\nidentify and isolate the components of two different adversarial training\ntechniques that contribute most to increased adversarial robustness,\nparticularly through the lens of margins in the input space -- the minimal\ndistance between data points and decision boundaries. Specifically, we compare\ntwo methods that maximize margins: a simple approach which modifies the loss\nfunction to increase an approximation of the margin, and a more complex\nstate-of-the-art method (Dynamics-Aware Robust Training) which builds upon this\napproach. Using a VGG-16 model as our base, we systematically isolate and\nevaluate individual components from these methods to determine their relative\nimpact on adversarial robustness. We assess the effect of each component on the\nmodel's performance under various adversarial attacks, including AutoAttack and\nProjected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset reveals\nwhich elements most effectively enhance adversarial robustness, providing\ninsights for designing more robust DNNs."
    },
    {
        "date": "2025-08",
        "title": "MoveScanner: Analysis of Security Risks of Move Smart Contracts",
        "author": "Yuhe Luo, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2508.17964v2",
        "abstract": "As blockchain technology continues to evolve, the security of smart contracts\nhas increasingly drawn attention from both academia and industry. The Move\nlanguage, with its unique resource model and linear type system, provides a\nsolid foundation for the security of digital assets. However, smart contracts\nstill face new security challenges due to developer programming errors and the\npotential risks associated with cross-module interactions. This paper\nsystematically analyzes the limitations of existing security tools within the\nMove ecosystem and reveals their unique vulnerability patterns. To address\nthese issues, it introduces MoveScanner, a static analysis tool based on a\ncontrol flow graph and data flow analysis architecture. By incorporating\ncross-module call graph tracking, MoveScanner can effectively identify five key\ntypes of security vulnerabilities, including resource leaks, weak permission\nmanagement, and arithmetic overflows. In terms of design, MoveScanner adheres\nto a modular principle, supports bytecode-level analysis and multi-chain\nadaptation, and introduces innovative resource trajectory tracking algorithms\nand capability matrix analysis methods, thereby significantly reducing the\nfalse positive rate. Empirical results show that MoveScanner achieved 88.2%\ndetection accuracy in benchmark testing, filling the gap in security tools in\nthe Move ecosystem. Furthermore, this paper identifies twelve new types of\nsecurity risks based on the resource-oriented programming paradigm and provides\na theoretical foundation and practical experience for the development of smart\ncontract security mechanisms. Future work will focus on combining formal\nverification and dynamic analysis techniques to build a security protection\nframework covering the entire contract lifecycle"
    },
    {
        "date": "2025-08",
        "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities",
        "author": "Yagmur Yigit, Mehmet Ali Erturk, Kerem Gursu, and Berk Canberk",
        "link": "http://arxiv.org/abs/2508.17913v1",
        "abstract": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication."
    },
    {
        "date": "2025-08",
        "title": "FasterVoiceGrad: Faster One-step Diffusion-Based Voice Conversion with Adversarial Diffusion Conversion Distillation",
        "author": "Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Yuto Kondo",
        "link": "http://arxiv.org/abs/2508.17868v1",
        "abstract": "A diffusion-based voice conversion (VC) model (e.g., VoiceGrad) can achieve\nhigh speech quality and speaker similarity; however, its conversion process is\nslow owing to iterative sampling. FastVoiceGrad overcomes this limitation by\ndistilling VoiceGrad into a one-step diffusion model. However, it still\nrequires a computationally intensive content encoder to disentangle the\nspeaker's identity and content, which slows conversion. Therefore, we propose\nFasterVoiceGrad, a novel one-step diffusion-based VC model obtained by\nsimultaneously distilling a diffusion model and content encoder using\nadversarial diffusion conversion distillation (ADCD), where distillation is\nperformed in the conversion process while leveraging adversarial and score\ndistillation training. Experimental evaluations of one-shot VC demonstrated\nthat FasterVoiceGrad achieves competitive VC performance compared to\nFastVoiceGrad, with 6.6-6.9 and 1.8 times faster speed on a GPU and CPU,\nrespectively."
    },
    {
        "date": "2025-08",
        "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation",
        "author": "Alexandros Gkillas, Ioulia Kapsali, Nikos Piperigkos, and Aris S. Lalos",
        "link": "http://arxiv.org/abs/2508.19290v1",
        "abstract": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios."
    },
    {
        "date": "2025-08",
        "title": "Software Unclonable Functions for IoT Devices Identification and Security",
        "author": "Saeed Alshehhi",
        "link": "http://arxiv.org/abs/2508.17853v1",
        "abstract": "In the evolving landscape of IoT ecosystem, distinguishing between legitimate\nand compromised devices is a critical challenge. This research investigates the\neffectiveness of hardware performance counter (HPC)-derived signatures'\nuniqueness under the umbrella of a concept that we introduced as software\nunclonable functions (SUFs)."
    },
    {
        "date": "2025-08",
        "title": "Robust Anomaly Detection in Industrial Environments via Meta-Learning",
        "author": "Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, and Francesco Setti",
        "link": "http://arxiv.org/abs/2508.17789v1",
        "abstract": "Anomaly detection is fundamental for ensuring quality control and operational\nefficiency in industrial environments, yet conventional approaches face\nsignificant challenges when training data contains mislabeled samples-a common\noccurrence in real-world scenarios. This paper presents RAD, a robust anomaly\ndetection framework that integrates Normalizing Flows with Model-Agnostic\nMeta-Learning to address the critical challenge of label noise in industrial\nsettings. Our approach employs a bi-level optimization strategy where\nmeta-learning enables rapid adaptation to varying noise conditions, while\nuncertainty quantification guides adaptive L2 regularization to maintain model\nstability. The framework incorporates multiscale feature processing through\npretrained feature extractors and leverages the precise likelihood estimation\ncapabilities of Normalizing Flows for robust anomaly scoring. Comprehensive\nevaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance,\nachieving I-AUROC scores of 95.4% and 94.6% respectively under clean\nconditions, while maintaining robust detection capabilities above 86.8% and\n92.1% even when 50% of training samples are mislabeled. The results highlight\nRAD's exceptional resilience to noisy training conditions and its ability to\ndetect subtle anomalies across diverse industrial scenarios, making it a\npractical solution for real-world anomaly detection applications where perfect\ndata curation is challenging."
    },
    {
        "date": "2025-08",
        "title": "CATformer: Contrastive Adversarial Transformer for Image Super-Resolution",
        "author": "Qinyi Tian, Spence Cox, and Laura E. Dalton",
        "link": "http://arxiv.org/abs/2508.17708v1",
        "abstract": "Super-resolution remains a promising technique to enhance the quality of\nlow-resolution images. This study introduces CATformer (Contrastive Adversarial\nTransformer), a novel neural network integrating diffusion-inspired feature\nrefinement with adversarial and contrastive learning. CATformer employs a\ndual-branch architecture combining a primary diffusion-inspired transformer,\nwhich progressively refines latent representations, with an auxiliary\ntransformer branch designed to enhance robustness to noise through learned\nlatent contrasts. These complementary representations are fused and decoded\nusing deep Residual-in-Residual Dense Blocks for enhanced reconstruction\nquality. Extensive experiments on benchmark datasets demonstrate that CATformer\noutperforms recent transformer-based and diffusion-inspired methods both in\nefficiency and visual image quality. This work bridges the performance gap\namong transformer-, diffusion-, and GAN-based methods, laying a foundation for\npractical applications of diffusion-inspired transformers in super-resolution."
    },
    {
        "date": "2025-08",
        "title": "Robustness Feature Adapter for Efficient Adversarial Training",
        "author": "Quanwei Wu, Jun Guo, Wei Wang, and Yi Wang",
        "link": "http://arxiv.org/abs/2508.17680v1",
        "abstract": "Adversarial training (AT) with projected gradient descent is the most popular\nmethod to improve model robustness under adversarial attacks. However,\ncomputational overheads become prohibitively large when AT is applied to large\nbackbone models. AT is also known to have the issue of robust overfitting. This\npaper contributes to solving both problems simultaneously towards building more\ntrustworthy foundation models. In particular, we propose a new adapter-based\napproach for efficient AT directly in the feature space. We show that the\nproposed adapter-based approach can improve the inner-loop convergence quality\nby eliminating robust overfitting. As a result, it significantly increases\ncomputational efficiency and improves model accuracy by generalizing\nadversarial robustness to unseen attacks. We demonstrate the effectiveness of\nthe new adapter-based approach in different backbone architectures and in AT at\nscale."
    },
    {
        "date": "2025-08",
        "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior",
        "author": "Zhuotao Lian, Weiyu Wang, Qingkui Zeng, Toru Nakanishi, Teruaki Kitasuka, and Chunhua Su",
        "link": "http://arxiv.org/abs/2508.19287v1",
        "abstract": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows."
    },
    {
        "date": "2025-08",
        "title": "Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models",
        "author": "Qiming Guo, Jinwen Tang, and Xingran Huang",
        "link": "http://arxiv.org/abs/2508.17674v1",
        "abstract": "We introduce Advertisement Embedding Attacks (AEA), a new class of LLM\nsecurity threats that stealthily inject promotional or malicious content into\nmodel outputs and AI agents. AEA operate through two low-cost vectors: (1)\nhijacking third-party service-distribution platforms to prepend adversarial\nprompts, and (2) publishing back-doored open-source checkpoints fine-tuned with\nattacker data. Unlike conventional attacks that degrade accuracy, AEA subvert\ninformation integrity, causing models to return covert ads, propaganda, or hate\nspeech while appearing normal. We detail the attack pipeline, map five\nstakeholder victim groups, and present an initial prompt-based self-inspection\ndefense that mitigates these injections without additional model retraining.\nOur findings reveal an urgent, under-addressed gap in LLM security and call for\ncoordinated detection, auditing, and policy responses from the AI-safety\ncommunity."
    },
    {
        "date": "2025-08",
        "title": "ClearMask: Noise-Free and Naturalness-Preserving Protection Against Voice Deepfake Attacks",
        "author": "Yuanda Wang, Bocheng Chen, Hanqing Guo, Guangjing Wang, Weikang Ding, and Qiben Yan",
        "link": "http://arxiv.org/abs/2508.17660v1",
        "abstract": "Voice deepfake attacks, which artificially impersonate human speech for\nmalicious purposes, have emerged as a severe threat. Existing defenses\ntypically inject noise into human speech to compromise voice encoders in speech\nsynthesis models. However, these methods degrade audio quality and require\nprior knowledge of the attack approaches, limiting their effectiveness in\ndiverse scenarios. Moreover, real-time audios, such as speech in virtual\nmeetings and voice messages, are still exposed to voice deepfake threats. To\novercome these limitations, we propose ClearMask, a noise-free defense\nmechanism against voice deepfake attacks. Unlike traditional approaches,\nClearMask modifies the audio mel-spectrogram by selectively filtering certain\nfrequencies, inducing a transferable voice feature loss without injecting\nnoise. We then apply audio style transfer to further deceive voice decoders\nwhile preserving perceived sound quality. Finally, optimized reverberation is\nintroduced to disrupt the output of voice generation models without affecting\nthe naturalness of the speech. Additionally, we develop LiveMask to protect\nstreaming speech in real-time through a universal frequency filter and\nreverberation generator. Our experimental results show that ClearMask and\nLiveMask effectively prevent voice deepfake attacks from deceiving speaker\nverification models and human listeners, even for unseen voice synthesis models\nand black-box API services. Furthermore, ClearMask demonstrates resilience\nagainst adaptive attackers who attempt to recover the original audio signal\nfrom the protected speech samples."
    },
    {
        "date": "2025-08",
        "title": "DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers",
        "author": "Michael Podsiadly, and Brendon K Lay",
        "link": "http://arxiv.org/abs/2508.17509v1",
        "abstract": "Training AI models to understand images without costly labeled data remains a\nchallenge. We combine two techniques--DINO (teacher-student learning) and\nBarlow Twins (redundancy reduction)--to create a model that learns better with\nfewer labels and less compute. While both DINO and Barlow Twins have\nindependently demonstrated strong performance in self-supervised learning, each\ncomes with limitations--DINO may be sensitive to certain augmentations, and\nBarlow Twins often requires batch sizes too large to fit on consumer hardware.\nBy combining the redundancy-reduction objective of Barlow Twins with the\nself-distillation strategy of DINO, we aim to leverage their complementary\nstrengths. We train a hybrid model on the MS COCO dataset using only 10\\% of\nlabeled data for linear probing, and evaluate its performance against\nstandalone DINO and Barlow Twins implementations. Preliminary results show that\nthe combined approach achieves comparable loss and classification accuracy to\nDINO while maintaining strong feature representations. Attention visualizations\nfurther suggest improved semantic segmentation capability in the hybrid model.\nThis combined method offers a scalable, label-efficient alternative for\ntraining ViTs in resource-constrained environments."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Examples Are Not Bugs, They Are Superposition",
        "author": "Liv Gorton, and Owen Lewis",
        "link": "http://arxiv.org/abs/2508.17456v1",
        "abstract": "Adversarial examples -- inputs with imperceptible perturbations that fool\nneural networks -- remain one of deep learning's most perplexing phenomena\ndespite nearly a decade of research. While numerous defenses and explanations\nhave been proposed, there is no consensus on the fundamental mechanism. One\nunderexplored hypothesis is that superposition, a concept from mechanistic\ninterpretability, may be a major contributing factor, or even the primary\ncause. We present four lines of evidence in support of this hypothesis, greatly\nextending prior arguments by Elhage et al. (2022): (1) superposition can\ntheoretically explain a range of adversarial phenomena, (2) in toy models,\nintervening on superposition controls robustness, (3) in toy models,\nintervening on robustness (via adversarial training) controls superposition,\nand (4) in ResNet18, intervening on robustness (via adversarial training)\ncontrols superposition."
    },
    {
        "date": "2025-08",
        "title": "Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality",
        "author": "Shaocong Ma, Ziyi Chen, Yi Zhou, and Heng Huang",
        "link": "http://arxiv.org/abs/2508.17448v1",
        "abstract": "The goal of robust constrained reinforcement learning (RL) is to optimize an\nagent's performance under the worst-case model uncertainty while satisfying\nsafety or resource constraints. In this paper, we demonstrate that strong\nduality does not generally hold in robust constrained RL, indicating that\ntraditional primal-dual methods may fail to find optimal feasible policies. To\novercome this limitation, we propose a novel primal-only algorithm called\nRectified Robust Policy Optimization (RRPO), which operates directly on the\nprimal problem without relying on dual formulations. We provide theoretical\nconvergence guarantees under mild regularity assumptions, showing convergence\nto an approximately optimal feasible policy with iteration complexity matching\nthe best-known lower bound when the uncertainty set diameter is controlled in a\nspecific level. Empirical results in a grid-world environment validate the\neffectiveness of our approach, demonstrating that RRPO achieves robust and safe\nperformance under model uncertainties while the non-robust method can violate\nthe worst-case safety constraints."
    },
    {
        "date": "2025-08",
        "title": "Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search",
        "author": "Zhao Zheng, Jingfan Fan, Long Shao, Hong Song, Danni Ai, Tianyu Fu, Deqiang Xiao, Yongtian Wang, and Jian Yang",
        "link": "http://arxiv.org/abs/2508.17427v1",
        "abstract": "Point cloud registration based on correspondences computes the rigid\ntransformation that maximizes the number of inliers constrained within the\nnoise threshold. Current state-of-the-art (SOTA) methods employing spatial\ncompatibility graphs or branch-and-bound (BnB) search mainly focus on\nregistration under high outlier ratios. However, graph-based methods require at\nleast quadratic space and time complexity for graph construction, while\nmulti-stage BnB search methods often suffer from inaccuracy due to local optima\nbetween decomposed stages. This paper proposes a geometric maximum overlapping\nregistration framework via rotation-only BnB search. The rigid transformation\nis decomposed using Chasles' theorem into a translation along rotation axis and\na 2D rigid transformation. The optimal rotation axis and angle are searched via\nBnB, with residual parameters formulated as range maximum query (RMQ) problems.\nFirstly, the top-k candidate rotation axes are searched within a hemisphere\nparameterized by cube mapping, and the translation along each axis is estimated\nthrough interval stabbing of the correspondences projected onto that axis.\nSecondly, the 2D registration is relaxed to 1D rotation angle search with 2D\nRMQ of geometric overlapping for axis-aligned rectangles, which is solved\ndeterministically in polynomial time using sweep line algorithm with segment\ntree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets\ndemonstrate superior accuracy and efficiency over SOTA methods, while the time\ncomplexity is polynomial and the space complexity increases linearly with the\nnumber of points, even in the worst case."
    },
    {
        "date": "2025-08",
        "title": "Cyber Security Educational Games for Children: A Systematic Literature Review",
        "author": "Temesgen Kitaw Damenu, \u0130nci Zaim G\u00f6kbay, Alexandra Covaci, and Shujun Li",
        "link": "http://arxiv.org/abs/2508.17414v1",
        "abstract": "Educational games have been widely used to teach children about cyber\nsecurity. This systematic literature review reveals evidence of positive\nlearning outcomes, after analysing 91 such games reported in 68 papers\npublished between 2010 and 2024. However, critical gaps have also been\nidentified regarding the design processes and the methodological rigour,\nincluding lack of systematic design, misalignment between proposed and achieved\nlearning outcomes, rare use of control groups, limited discussions on ethical\nconsiderations, and underutilisation of emerging technologies. We recommend\nmultiple future research directions, e.g., a hybrid approach to game design and\nevaluation that combines bottom-up and top-down approaches."
    },
    {
        "date": "2025-08",
        "title": "FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats",
        "author": "Avishag Shapira, Simon Shigol, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2508.17405v1",
        "abstract": "The widespread adoption of machine learning (ML) systems increased attention\nto their security and emergence of adversarial machine learning (AML)\ntechniques that exploit fundamental vulnerabilities in ML systems, creating an\nurgent need for comprehensive risk assessment for ML-based systems. While\ntraditional risk assessment frameworks evaluate conventional cybersecurity\nrisks, they lack ability to address unique challenges posed by AML threats.\nExisting AML threat evaluation approaches focus primarily on technical attack\nrobustness, overlooking crucial real-world factors like deployment\nenvironments, system dependencies, and attack feasibility. Attempts at\ncomprehensive AML risk assessment have been limited to domain-specific\nsolutions, preventing application across diverse systems. Addressing these\nlimitations, we present FRAME, the first comprehensive and automated framework\nfor assessing AML risks across diverse ML-based systems. FRAME includes a novel\nrisk assessment method that quantifies AML risks by systematically evaluating\nthree key dimensions: target system's deployment environment, characteristics\nof diverse AML techniques, and empirical insights from prior research. FRAME\nincorporates a feasibility scoring mechanism and LLM-based customization for\nsystem-specific assessments. Additionally, we developed a comprehensive\nstructured dataset of AML attacks enabling context-aware risk assessment. From\nan engineering application perspective, FRAME delivers actionable results\ndesigned for direct use by system owners with only technical knowledge of their\nsystems, without expertise in AML. We validated it across six diverse\nreal-world applications. Our evaluation demonstrated exceptional accuracy and\nstrong alignment with analysis by AML experts. FRAME enables organizations to\nprioritize AML risks, supporting secure AI deployment in real-world\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "FedERL: Federated Efficient and Robust Learning for Common Corruptions",
        "author": "Omar Bekdache, and Naresh Shanbhag",
        "link": "http://arxiv.org/abs/2508.17381v1",
        "abstract": "Federated learning (FL) accelerates the deployment of deep learning models on\nedge devices while preserving data privacy. However, FL systems face challenges\ndue to client-side constraints on computational resources, and from a lack of\nrobustness to common corruptions such as noise, blur, and weather effects.\nExisting robust training methods are computationally expensive and unsuitable\nfor resource-constrained clients. We propose FedERL, federated efficient and\nrobust learning, as the first work to explicitly address corruption robustness\nunder time and energy constraints on the client side. At its core, FedERL\nemploys a novel data-agnostic robust training (DART) method on the server to\nenhance robustness without access to the training data. In doing so, FedERL\nensures zero robustness overhead for clients. Extensive experiments demonstrate\nFedERL's ability to handle common corruptions at a fraction of the time and\nenergy cost of traditional robust training methods. In scenarios with limited\ntime and energy budgets, FedERL surpasses the performance of traditional robust\ntraining, establishing it as a practical and scalable solution for real-world\nFL applications."
    },
    {
        "date": "2025-08",
        "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures",
        "author": "Mark Dorsett, Scott Mann, Jabed Chowdhury, and Abdun Mahmood",
        "link": "http://arxiv.org/abs/2508.19284v1",
        "abstract": "The Denial of Wallet (DoW) attack poses a unique and growing threat to\nserverless architectures that rely on Function-as-a-Service (FaaS) models,\nexploiting the cost structure of pay-as-you-go billing to financially burden\napplication owners. Unlike traditional Denial of Service (DoS) attacks, which\naim to exhaust resources and disrupt service availability, DoW attacks focus on\nescalating costs without impacting service operation. This review traces the\nevolution of DoW research, from initial awareness and attack classification to\nadvancements in detection and mitigation strategies. Key developments include\nthe categorisation of attack types-such as Blast DDoW, Continual Inconspicuous\nDDoW, and Background Chained DDoW-and the creation of simulation tools like\nDoWTS, which enable safe experimentation and data generation. Recent\nadvancements highlight machine learning approaches, including systems like\nGringotts and DoWNet, which leverage deep learning and anomaly detection to\nidentify malicious traffic patterns. Although substantial progress has been\nmade, challenges persist, notably the lack of real-world data and the need for\nadaptive billing models. This is the first comprehensive literature review\ndedicated strictly to Denial of Wallet attacks, providing an in-depth analysis\nof their financial impacts, attack techniques, mitigation strategies, and\ndetection mechanisms within serverless computing. The paper also presents the\nfirst detailed examination of simulation and data generation tools used for DoW\nresearch, addressing a critical gap in existing cybersecurity literature. By\nsynthesising these key areas, this study serves as a foundational resource for\nfuture research and industry efforts in securing pay-as-you-go cloud\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection",
        "author": "Lianrui Mu, Zou Xingze, Jianhong Bai, Jiaqi Hu, Wenjie Zheng, Jiangnan Ye, Jiedong Zhuang, Mudassar Ali, Jing Wang, and Haoji Hu",
        "link": "http://arxiv.org/abs/2508.17346v1",
        "abstract": "The rapid growth of high-resolution, meticulously crafted AI-generated images\nposes a significant challenge to existing detection methods, which are often\ntrained and evaluated on low-resolution, automatically generated datasets that\ndo not align with the complexities of high-resolution scenarios. A common\npractice is to resize or center-crop high-resolution images to fit standard\nnetwork inputs. However, without full coverage of all pixels, such strategies\nrisk either obscuring subtle, high-frequency artifacts or discarding\ninformation from uncovered regions, leading to input information loss. In this\npaper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net),\na novel framework that ensures no pixel is left behind. We use the Feature\nAggregation Module (FAM), which fuses features from multiple full-resolution\nlocal tiles with a down-sampled global view of the image. These local features\nare aggregated and fused with global representations for final prediction,\nensuring that native-resolution details are preserved and utilized for\ndetection. To enhance robustness against challenges such as localized AI\nmanipulations and compression, we introduce Token-wise Forgery Localization\n(TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor\nEstimation (QFE) module to disentangle generative artifacts from compression\nnoise explicitly. Furthermore, to facilitate future research, we introduce\nHiRes-50K, a new challenging benchmark consisting of 50,568 images with up to\n64 megapixels. Extensive experiments show that HiDA-Net achieves\nstate-of-the-art, increasing accuracy by over 13% on the challenging Chameleon\ndataset and 10% on our HiRes-50K."
    },
    {
        "date": "2025-08",
        "title": "Risk Assessment and Security Analysis of Large Language Models",
        "author": "Xiaoyan Zhang, Dongyang Lyu, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2508.17329v1",
        "abstract": "As large language models (LLMs) expose systemic security challenges in high\nrisk applications, including privacy leaks, bias amplification, and malicious\nabuse, there is an urgent need for a dynamic risk assessment and collaborative\ndefence framework that covers their entire life cycle. This paper focuses on\nthe security problems of large language models (LLMs) in critical application\nscenarios, such as the possibility of disclosure of user data, the deliberate\ninput of harmful instructions, or the models bias. To solve these problems, we\ndescribe the design of a system for dynamic risk assessment and a hierarchical\ndefence system that allows different levels of protection to cooperate. This\npaper presents a risk assessment system capable of evaluating both static and\ndynamic indicators simultaneously. It uses entropy weighting to calculate\nessential data, such as the frequency of sensitive words, whether the API call\nis typical, the realtime risk entropy value is significant, and the degree of\ncontext deviation. The experimental results show that the system is capable of\nidentifying concealed attacks, such as role escape, and can perform rapid risk\nevaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional\nEncoder Representation from Transformers) at the input layer to identify and\nfilter malicious commands. The model layer uses dynamic adversarial training\nand differential privacy noise injection technology together. The output layer\nalso has a neural watermarking system that can track the source of the content.\nIn practice, the quality of this method, especially important in terms of\ncustomer service in the financial industry."
    },
    {
        "date": "2025-08",
        "title": "An Efficient Recommendation Filtering-based Trust Model for Securing Internet of Things",
        "author": "Muhammad Ibn Ziauddin, Rownak Rahad Rabbi, SM Mehrab, Fardin Faiyaz, and Mosarrat Jahan",
        "link": "http://arxiv.org/abs/2508.17304v1",
        "abstract": "Trust computation is crucial for ensuring the security of the Internet of\nThings (IoT). However, current trust-based mechanisms for IoT have limitations\nthat impact data security. Sliding window-based trust schemes cannot ensure\nreliable trust computation due to their inability to select appropriate window\nlengths. Besides, recent trust scores are emphasized when considering the\neffect of time on trust. This can cause a sudden change in overall trust score\nbased on recent behavior, potentially misinterpreting an honest service\nprovider as malicious and vice versa. Moreover, clustering mechanisms used to\nfilter recommendations in trust computation often lead to slower results. In\nthis paper, we propose a robust trust model to address these limitations. The\nproposed approach determines the window length dynamically to guarantee\naccurate trust computation. It uses the harmonic mean of average trust score\nand time to prevent sudden fluctuations in trust scores. Additionally, an\nefficient personalized subspace clustering algorithm is used to exclude\nrecommendations. We present a security analysis demonstrating the resiliency of\nthe proposed scheme against bad-mouthing, ballot-stuffing, and on-off attacks.\nThe proposed scheme demonstrates a competitive performance in detecting\nbad-mouthing attacks, while outperforming existing works with an approximately\n44% improvement in accuracy for detecting on-off attacks. It maintains its\neffectiveness even when the percentage of on-off attackers increases and in\nscenarios where multiple attacks occur simultaneously. Additionally, the\nproposed scheme reduces the recommendation filtering time by 95%."
    },
    {
        "date": "2025-08",
        "title": "AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks",
        "author": "Zhenyu Liu, Huizhi Liang, Xinrun Li, Vaclav Snasel, and Varun Ojha",
        "link": "http://arxiv.org/abs/2508.17265v1",
        "abstract": "Adversarial distillation (AD) is a knowledge distillation technique that\nfacilitates the transfer of robustness from teacher deep neural network (DNN)\nmodels to lightweight target (student) DNN models, enabling the target models\nto perform better than only training the student model independently. Some\nprevious works focus on using a small, learnable teacher (guide) model to\nimprove the robustness of a student model. Since a learnable guide model starts\nlearning from scratch, maintaining its optimal state for effective knowledge\ntransfer during co-training is challenging. Therefore, we propose a novel\nAdaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT,\ndynamically adjusts the training state of the guide model to install robustness\nto the target model. Specifically, we develop two separate loss functions as\npart of the AdaGAT method, allowing the guide model to participate more\nactively in backpropagation to achieve its optimal state. We evaluated our\napproach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and\nTinyImageNet, using the WideResNet-34-10 model as the target model. Our\nobservations reveal that appropriately adjusting the guide model within a\ncertain accuracy range enhances the target model's robustness across various\nadversarial attacks compared to a variety of baseline models."
    },
    {
        "date": "2025-08",
        "title": "Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics",
        "author": "Lixin Jia, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Dan Ma, and Gaobo Yang",
        "link": "http://arxiv.org/abs/2508.17247v1",
        "abstract": "With the rapid evolution of deepfake technologies and the wide dissemination\nof digital media, personal privacy is facing increasingly serious security\nthreats. Deepfake proactive forensics, which involves embedding imperceptible\nwatermarks to enable reliable source tracking, serves as a crucial defense\nagainst these threats. Although existing methods show strong forensic ability,\nthey rely on an idealized assumption of single watermark embedding, which\nproves impractical in real-world scenarios. In this paper, we formally define\nand demonstrate the existence of Multi-Embedding Attacks (MEA) for the first\ntime. When a previously protected image undergoes additional rounds of\nwatermark embedding, the original forensic watermark can be destroyed or\nremoved, rendering the entire proactive forensic mechanism ineffective. To\naddress this vulnerability, we propose a general training paradigm named\nAdversarial Interference Simulation (AIS). Rather than modifying the network\narchitecture, AIS explicitly simulates MEA scenarios during fine-tuning and\nintroduces a resilience-driven loss function to enforce the learning of sparse\nand stable watermark representations. Our method enables the model to maintain\nthe ability to extract the original watermark correctly even after a second\nembedding. Extensive experiments demonstrate that our plug-and-play AIS\ntraining paradigm significantly enhances the robustness of various existing\nmethods against MEA."
    },
    {
        "date": "2025-08",
        "title": "Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting",
        "author": "Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Cuiqun Chen, Zhuo Zheng, Bo Du, and Liangpei Zhang",
        "link": "http://arxiv.org/abs/2508.17186v1",
        "abstract": "Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object\nchanges (e.g., objects appearing or disappearing) from background variations\n(e.g., environmental changes due to light, weather, or seasonal shifts) in\npaired satellite images, relying only on paired image (i.e., image-level)\nclassification labels. This technique significantly reduces the need for dense\nannotations required in fully-supervised change detection. However, as\nimage-level supervision only indicates whether objects have changed in a scene,\nWSCD methods often misclassify background variations as object changes,\nespecially in complex remote-sensing scenarios. In this work, we propose an\nAdversarial Class Prompting (AdvCP) method to address this co-occurring noise\nproblem, including two phases: a) Adversarial Prompt Mining: After each\ntraining iteration, we introduce adversarial prompting perturbations, using\nincorrect one-hot image-level labels to activate erroneous feature mappings.\nThis process reveals co-occurring adversarial samples under weak supervision,\nnamely background variation features that are likely to be misclassified as\nobject changes. b) Adversarial Sample Rectification: We integrate these\nadversarially prompt-activated pixel samples into training by constructing an\nonline global prototype. This prototype is built from an exponentially weighted\nmoving average of the current batch and all historical training data. Our AdvCP\ncan be seamlessly integrated into current WSCD methods without adding\nadditional inference cost. Experiments on ConvNet, Transformer, and Segment\nAnything Model (SAM)-based baselines demonstrate significant performance\nenhancements. Furthermore, we demonstrate the generalizability of AdvCP to\nother multi-class weakly-supervised dense prediction scenarios. Code is\navailable at https://github.com/zhenghuizhao/AdvCP"
    },
    {
        "date": "2025-08",
        "title": "Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection",
        "author": "Jeng-Lin Li, Ming-Ching Chang, and Wei-Chao Chen",
        "link": "http://arxiv.org/abs/2508.17174v1",
        "abstract": "Out-of-distribution (OOD) detection ensures safe and reliable model\ndeployment. Contemporary OOD algorithms using geometry projection can detect\nOOD or adversarial samples from clean in-distribution (ID) samples. However,\nthis setting regards adversarial ID samples as OOD, leading to incorrect OOD\npredictions. Existing efforts on OOD detection with ID and OOD data under\nattacks are minimal. In this paper, we develop a robust OOD detection method\nthat distinguishes adversarial ID samples from OOD ones. The sharp loss\nlandscape created by adversarial training hinders model convergence, impacting\nthe latent embedding quality for OOD score calculation. Therefore, we introduce\na {\\bf Sharpness-aware Geometric Defense (SaGD)} framework to smooth out the\nrugged adversarial loss landscape in the projected latent geometry. Enhanced\ngeometric embedding convergence enables accurate ID data characterization,\nbenefiting OOD detection against adversarial attacks. We use Jitter-based\nperturbation in adversarial training to extend the defense ability against\nunseen attacks. Our SaGD framework significantly improves FPR and AUC over the\nstate-of-the-art defense approaches in differentiating CIFAR-100 from six other\nOOD datasets under various attacks. We further examine the effects of\nperturbations at various adversarial training levels, revealing the\nrelationship between the sharp loss landscape and adversarial OOD detection."
    },
    {
        "date": "2025-08",
        "title": "Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks",
        "author": "Jack Youstra, Mohammed Mahfoud, Yang Yan, Henry Sleight, Ethan Perez, and Mrinank Sharma",
        "link": "http://arxiv.org/abs/2508.17158v1",
        "abstract": "Large language model fine-tuning APIs enable widespread model customization,\nyet pose significant safety risks. Recent work shows that adversaries can\nexploit access to these APIs to bypass model safety mechanisms by encoding\nharmful content in seemingly harmless fine-tuning data, evading both human\nmonitoring and standard content filters. We formalize the fine-tuning API\ndefense problem, and introduce the Cipher Fine-tuning Robustness benchmark\n(CIFR), a benchmark for evaluating defense strategies' ability to retain model\nsafety in the face of cipher-enabled attackers while achieving the desired\nlevel of fine-tuning functionality. We include diverse cipher encodings and\nfamilies, with some kept exclusively in the test set to evaluate for\ngeneralization across unseen ciphers and cipher families. We then evaluate\ndifferent defenses on the benchmark and train probe monitors on model internal\nactivations from multiple fine-tunes. We show that probe monitors achieve over\n99% detection accuracy, generalize to unseen cipher variants and families, and\ncompare favorably to state-of-the-art monitoring approaches. We open-source\nCIFR and the code to reproduce our experiments to facilitate further research\nin this critical area. Code and data are available online\nhttps://github.com/JackYoustra/safe-finetuning-api"
    },
    {
        "date": "2025-08",
        "title": "Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning",
        "author": "Diksha Gupta, Nirupam Gupta, Chuan Xu, and Giovanni Neglia",
        "link": "http://arxiv.org/abs/2508.17129v1",
        "abstract": "Distributed learning (DL) enables scalable model training over decentralized\ndata, but remains challenged by Byzantine faults and high communication costs.\nWhile both issues have been studied extensively in isolation, their interaction\nis less explored. Prior work shows that naively combining communication\ncompression with Byzantine-robust aggregation degrades resilience to faulty\nnodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29],\nmakes use of the momentum variance reduction scheme to mitigate the detrimental\nimpact of compression noise on Byzantine-robustness. We propose a new\nalgorithm, named RoSDHB, that integrates the classic Polyak's momentum with a\nnew coordinated compression mechanism. We show that RoSDHB performs comparably\nto Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity\nheterogeneity model, while it relies on fewer assumptions. In particular, we\nonly assume Lipschitz smoothness of the average loss function of the honest\nworkers, in contrast to [29]that additionally assumes a special smoothness of\nbounded global Hessian variance. Empirical results on benchmark image\nclassification task show that RoSDHB achieves strong robustness with\nsignificant communication savings."
    },
    {
        "date": "2025-08",
        "title": "SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks",
        "author": "Zhenliang Gan, Xiaoxiao Hu, Sheng Li, Zhenxing Qian, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2508.17121v2",
        "abstract": "Audio watermarking has been widely applied in copyright protection and source\ntracing. However, due to the inherent characteristics of audio signals,\nwatermark localization and resistance to desynchronization attacks remain\nsignificant challenges. In this paper, we propose a learning-based scheme named\nSyncGuard to address these challenges. Specifically, we design a frame-wise\nbroadcast embedding strategy to embed the watermark in arbitrary-length audio,\nenhancing time-independence and eliminating the need for localization during\nwatermark extraction. To further enhance robustness, we introduce a\nmeticulously designed distortion layer. Additionally, we employ dilated\nresidual blocks in conjunction with dilated gated blocks to effectively capture\nmulti-resolution time-frequency features. Extensive experimental results show\nthat SyncGuard efficiently handles variable-length audio segments, outperforms\nstate-of-the-art methods in robustness against various attacks, and delivers\nsuperior auditory quality."
    },
    {
        "date": "2025-08",
        "title": "ZAPS: A Zero-Knowledge Proof Protocol for Secure UAV Authentication with Flight Path Privacy",
        "author": "Shayesta Naziri, Xu Wang, Guangsheng Yu, Christy Jie Liang, and Wei Ni",
        "link": "http://arxiv.org/abs/2508.17043v1",
        "abstract": "The increasing deployment of Unmanned Aerial Vehicles (UAVs) for military,\ncommercial, and logistics applications has raised significant concerns\nregarding flight path privacy. Conventional UAV communication systems often\nexpose flight path data to third parties, making them vulnerable to tracking,\nsurveillance, and location inference attacks. Existing encryption techniques\nprovide security but fail to ensure complete privacy, as adversaries can still\ninfer movement patterns through metadata analysis. To address these challenges,\nwe propose a zk-SNARK(Zero-Knowledge Succinct Non-Interactive Argument of\nKnowledge)-based privacy-preserving flight path authentication and verification\nframework. Our approach ensures that a UAV can prove its authorisation,\nvalidate its flight path with a control centre, and comply with regulatory\nconstraints without revealing any sensitive trajectory information. By\nleveraging zk-SNARKs, the UAV can generate cryptographic proofs that verify\ncompliance with predefined flight policies while keeping the exact path and\nlocation undisclosed. This method mitigates risks associated with real-time\ntracking, identity exposure, and unauthorised interception, thereby enhancing\nUAV operational security in adversarial environments. Our proposed solution\nbalances privacy, security, and computational efficiency, making it suitable\nfor resource-constrained UAVs in both civilian and military applications."
    },
    {
        "date": "2025-08",
        "title": "Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation",
        "author": "Konstantina Nikolaidou, George Retsinas, Giorgos Sfikas, Silvia Cascianelli, Rita Cucchiara, and Marcus Liwicki",
        "link": "http://arxiv.org/abs/2508.17017v1",
        "abstract": "Diffusion-based Handwritten Text Generation (HTG) approaches achieve\nimpressive results on frequent, in-vocabulary words observed at training time\nand on regular styles. However, they are prone to memorizing training samples\nand often struggle with style variability and generation clarity. In\nparticular, standard diffusion models tend to produce artifacts or distortions\nthat negatively affect the readability of the generated text, especially when\nthe style is hard to produce. To tackle these issues, we propose a novel\nsampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an\northogonal projection of a negatively perturbed prompt onto the original\npositive prompt. This approach helps steer the generation away from artifacts\nwhile maintaining the intended content, and encourages more diverse, yet\nplausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which\nrelies on unconditional predictions and produces noise at high guidance scales,\nDOG introduces a more stable, disentangled direction in the latent space. To\ncontrol the strength of the guidance across the denoising process, we apply a\ntriangular schedule: weak at the start and end of denoising, when the process\nis most sensitive, and strongest in the middle steps. Experimental results on\nthe state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both\ncontent clarity and style variability, even for out-of-vocabulary words and\nchallenging writing styles."
    }
]