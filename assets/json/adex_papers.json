[
    {
        "date": "2025-09",
        "title": "Are Robust LLM Fingerprints Adversarially Robust?",
        "author": "Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, and Sewoong Oh",
        "link": "http://arxiv.org/abs/2509.26598v1",
        "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
    },
    {
        "date": "2025-09",
        "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids",
        "author": "Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, and Sean Warnick",
        "link": "http://arxiv.org/abs/2509.26532v1",
        "abstract": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms."
    },
    {
        "date": "2025-09",
        "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors",
        "author": "Aleksandra Knapi\u0144ska, and Marija Furdek",
        "link": "http://arxiv.org/abs/2509.26530v1",
        "abstract": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data."
    },
    {
        "date": "2025-09",
        "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance",
        "author": "Jijun Xiang, Longliang Liu, Xuan Zhu, Xianqi Wang, Min Lin, and Xin Yang",
        "link": "http://arxiv.org/abs/2509.26498v1",
        "abstract": "Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach."
    },
    {
        "date": "2025-09",
        "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
        "author": "Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2509.26473v1",
        "abstract": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs."
    },
    {
        "date": "2025-09",
        "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks",
        "author": "Tharindu Lakshan Yasarathna, and Nhien-An Le-Khac",
        "link": "http://arxiv.org/abs/2509.26350v1",
        "abstract": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
    },
    {
        "date": "2025-09",
        "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models",
        "author": "Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2509.26345v1",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
    },
    {
        "date": "2025-09",
        "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2509.26275v1",
        "abstract": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning."
    },
    {
        "date": "2025-09",
        "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
        "author": "Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, and Di Jin",
        "link": "http://arxiv.org/abs/2509.26032v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines."
    },
    {
        "date": "2025-09",
        "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
        "author": "Gaojie Jin, Xinping Yi, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.25979v1",
        "abstract": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod."
    },
    {
        "date": "2025-09",
        "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions",
        "author": "Junbeom Kim, Kyuyoung Kim, Jihoon Tack, Dongha Lim, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2509.25973v1",
        "abstract": "Language models trained on web-scale corpora risk memorizing and exposing\nsensitive information, prompting the need for effective machine unlearning.\nPrior methods mainly focus on input queries to suppress sensitive outputs, yet\nthis often fails to eliminate the underlying knowledge and limits scalability.\nTo address this, we propose Corrective Unlearning with Retrieved Exclusions\n(CURE), a novel unlearning framework that verifies model outputs for leakage\nand revises them into safe responses. Specifically, CURE employs a lightweight\ncorrector that is applied to the original model to verify whether outputs\ncontain target knowledge and to rewrite them if any leakage is detected. To\nefficiently handle large-scale unlearning requests, CURE retrieves unlearning\ntargets that are relevant to the initial response and provides them as\nin-context references to the corrector for detection and conditional revision.\nBy leveraging this retrieval augmentation, the corrector can adapt to new\nunlearning requests without additional training. Extensive evaluations\ndemonstrate that CURE substantially reduces information leakage, even from\nindirect queries where prior works fall short, while maintaining response\nquality and general utility. Moreover, it demonstrates robustness under\ncontinual unlearning scenarios, making it practical for real-world\napplications."
    },
    {
        "date": "2025-09",
        "title": "The Impact of Scaling Training Data on Adversarial Robustness",
        "author": "Marco Zimmerli, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2509.25927v1",
        "abstract": "Deep neural networks remain vulnerable to adversarial examples despite\nadvances in architectures and training paradigms. We investigate how training\ndata characteristics affect adversarial robustness across 36 state-of-the-art\nvision models spanning supervised, self-supervised, and contrastive learning\napproaches, trained on datasets from 1.2M to 22B images. Models were evaluated\nunder six black-box attack categories: random perturbations, two types of\ngeometric masks, COCO object manipulations, ImageNet-C corruptions, and\nImageNet-R style shifts. Robustness follows a logarithmic scaling law with both\ndata volume and model size: a tenfold increase in data reduces attack success\nrate (ASR) on average by ~3.2%, whereas a tenfold increase in model size\nreduces ASR on average by ~13.4%. Notably, some self-supervised models trained\non curated datasets, such as DINOv2, outperform others trained on much larger\nbut less curated datasets, challenging the assumption that scale alone drives\nrobustness. Adversarial fine-tuning of ResNet50s improves generalization across\nstructural variations but not across color distributions. Human evaluation\nreveals persistent gaps between human and machine vision. These results show\nthat while scaling improves robustness, data quality, architecture, and\ntraining objectives play a more decisive role than raw scale in achieving\nbroad-spectrum adversarial resilience."
    },
    {
        "date": "2025-09",
        "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack",
        "author": "Yein Park, Jungwoo Park, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2509.25843v1",
        "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety."
    },
    {
        "date": "2025-09",
        "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks",
        "author": "Alexander Branch, Omead Pooladzandi, Radin Khosraviani, Sunay Gajanan Bhat, Jeffrey Jiang, and Gregory Pottie",
        "link": "http://arxiv.org/abs/2509.25792v1",
        "abstract": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines."
    },
    {
        "date": "2025-09",
        "title": "Lightweight and Robust Federated Data Valuation",
        "author": "Guojun Tang, Jiayu Zhou, Mohammad Mamun, and Steve Drew",
        "link": "http://arxiv.org/abs/2509.25560v1",
        "abstract": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments."
    },
    {
        "date": "2025-09",
        "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
        "author": "Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, and Philip Twu",
        "link": "http://arxiv.org/abs/2509.25520v1",
        "abstract": "We consider the problem of vision-based 6-DoF object pose estimation in the\ncontext of the notional Mars Sample Return campaign, in which a robotic arm\nwould need to localize multiple objects of interest for low-clearance pickup\nand insertion, under severely constrained hardware. We propose a novel\nlocalization algorithm leveraging a custom renderer together with a new\ntemplate matching metric tailored to the edge domain to achieve robust pose\nestimation using only low-fidelity, textureless 3D models as inputs. Extensive\nevaluations on synthetic datasets as well as from physical testbeds on Earth\nand in situ Mars imagery shows that our method consistently beats the state of\nthe art in compute and memory-constrained localization, both in terms of\nrobustness and accuracy, in turn enabling new possibilities for cheap and\nreliable localization on general-purpose hardware."
    },
    {
        "date": "2025-09",
        "title": "Environmental Rate Manipulation Attacks on Power Grid Security",
        "author": "Yonatan Gizachew Achamyeleh, Yang Xiang, Yun-Ping Hsiao, Yasamin Moghaddas, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.25476v1",
        "abstract": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security."
    },
    {
        "date": "2025-09",
        "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System",
        "author": "Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, and Andreas Veneris",
        "link": "http://arxiv.org/abs/2509.25469v1",
        "abstract": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection."
    },
    {
        "date": "2025-09",
        "title": "Managing Differentiated Secure Connectivity using Intents",
        "author": "Loay Abdelrazek, and Filippo Rebecchi",
        "link": "http://arxiv.org/abs/2509.25462v1",
        "abstract": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks."
    },
    {
        "date": "2025-09",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
        "author": "Zhibo Hou, Zhiyu An, and Wan Du",
        "link": "http://arxiv.org/abs/2509.25438v1",
        "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration"
    },
    {
        "date": "2025-09",
        "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors",
        "author": "Hui Wang, Nima Tashakor, Xiaoyang Tian, Hans D. Schotten, and Stefan M. Goetz",
        "link": "http://arxiv.org/abs/2509.25394v1",
        "abstract": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention."
    },
    {
        "date": "2025-09",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
        "author": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, and Yichao Wu",
        "link": "http://arxiv.org/abs/2509.25148v1",
        "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
    },
    {
        "date": "2025-09",
        "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary",
        "author": "Daniil Dmitriev, Harald Eskelund Franck, Carolin Heinzler, and Amartya Sanyal",
        "link": "http://arxiv.org/abs/2509.25135v1",
        "abstract": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms."
    },
    {
        "date": "2025-09",
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "author": "Xiaoyi Huang, Junwei Wu, Kejia Zhang, Carl Yang, and Zhiming Luo",
        "link": "http://arxiv.org/abs/2509.25082v1",
        "abstract": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method."
    },
    {
        "date": "2025-09",
        "title": "Fast Real-Time Pipeline for Robust Arm Gesture Recognition",
        "author": "Mil\u00e1n Zsolt Bagladi, L\u00e1szl\u00f3 Guly\u00e1s, and Gerg\u0151 Szalay",
        "link": "http://arxiv.org/abs/2509.25042v1",
        "abstract": "This paper presents a real-time pipeline for dynamic arm gesture recognition\nbased on OpenPose keypoint estimation, keypoint normalization, and a recurrent\nneural network classifier. The 1 x 1 normalization scheme and two feature\nrepresentations (coordinate- and angle-based) are presented for the pipeline.\nIn addition, an efficient method to improve robustness against camera angle\nvariations is also introduced by using artificially rotated training data.\nExperiments on a custom traffic-control gesture dataset demonstrate high\naccuracy across varying viewing angles and speeds. Finally, an approach to\ncalculate the speed of the arm signal (if necessary) is also presented."
    },
    {
        "date": "2025-09",
        "title": "SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation",
        "author": "Shuang Liang, Jing He, Chuanmeizhi Wang, Lejun Liao, Guo Zhang, Yingcong Chen, and Yuan Yuan",
        "link": "http://arxiv.org/abs/2509.24980v1",
        "abstract": "Pre-trained diffusion models provide rich multi-scale latent features and are\nemerging as powerful vision backbones. While recent works such as\nMarigold~\\citep{ke2024repurposing} and Lotus~\\citep{he2024lotus} adapt\ndiffusion priors for dense prediction with strong cross-domain generalization,\ntheir potential for structured outputs (e.g., human pose estimation) remains\nunderexplored. In this paper, we propose \\textbf{SDPose}, a fine-tuning\nframework built upon Stable Diffusion to fully exploit pre-trained diffusion\npriors for human pose estimation. First, rather than modifying cross-attention\nmodules or introducing learnable embeddings, we directly predict keypoint\nheatmaps in the SD U-Net's image latent space to preserve the original\ngenerative priors. Second, we map these latent features into keypoint heatmaps\nthrough a lightweight convolutional pose head, which avoids disrupting the\npre-trained backbone. Finally, to prevent overfitting and enhance\nout-of-distribution robustness, we incorporate an auxiliary RGB reconstruction\nbranch that preserves domain-transferable generative semantics. To evaluate\nrobustness under domain shift, we further construct \\textbf{COCO-OOD}, a\nstyle-transferred variant of COCO with preserved annotations. With just\none-fifth of the training schedule used by Sapiens on COCO, SDPose attains\nparity with Sapiens-1B/2B on the COCO validation set and establishes a new\nstate of the art on the cross-domain benchmarks HumanArt and COCO-OOD.\nFurthermore, we showcase SDPose as a zero-shot pose annotator for downstream\ncontrollable generation tasks, including ControlNet-based image synthesis and\nvideo generation, where it delivers qualitatively superior pose guidance."
    },
    {
        "date": "2025-09",
        "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks",
        "author": "Tereza Burianov\u00e1, Martin Pere\u0161\u00edni, and Ivan Homoliak",
        "link": "http://arxiv.org/abs/2509.24955v1",
        "abstract": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets."
    },
    {
        "date": "2025-09",
        "title": "VAGUEGAN: Stealthy Poisoning and Backdoor Attacks on Image Generative Pipelines",
        "author": "Mostafa Mohaimen Akand Faisal, and Rabeya Amin Jhuma",
        "link": "http://arxiv.org/abs/2509.24891v1",
        "abstract": "Generative models such as GANs and diffusion models are widely used to\nsynthesize photorealistic images and to support downstream creative and editing\ntasks. While adversarial attacks on discriminative models are well studied,\nattacks targeting generative pipelines where small, stealthy perturbations in\ninputs lead to controlled changes in outputs are less explored. This study\nintroduces VagueGAN, an attack pipeline combining a modular perturbation\nnetwork PoisonerNet with a Generator Discriminator pair to craft stealthy\ntriggers that cause targeted changes in generated images. Attack efficacy is\nevaluated using a custom proxy metric, while stealth is analyzed through\nperceptual and frequency domain measures. The transferability of the method to\na modern diffusion based pipeline is further examined through ControlNet guided\nediting. Interestingly, the experiments show that poisoned outputs can display\nhigher visual quality compared to clean counterparts, challenging the\nassumption that poisoning necessarily reduces fidelity. Unlike conventional\npixel level perturbations, latent space poisoning in GANs and diffusion\npipelines can retain or even enhance output aesthetics, exposing a blind spot\nin pixel level defenses. Moreover, carefully optimized perturbations can\nproduce consistent, stealthy effects on generator outputs while remaining\nvisually inconspicuous, raising concerns for the integrity of image generation\npipelines."
    },
    {
        "date": "2025-09",
        "title": "Vision At Night: Exploring Biologically Inspired Preprocessing For Improved Robustness Via Color And Contrast Transformations",
        "author": "Lorena Stracke, Lia Nimmermann, Shashank Agnihotri, Margret Keuper, and Volker Blanz",
        "link": "http://arxiv.org/abs/2509.24863v1",
        "abstract": "Inspired by the human visual system's mechanisms for contrast enhancement and\ncolor-opponency, we explore biologically motivated input preprocessing for\nrobust semantic segmentation. By applying Difference-of-Gaussians (DoG)\nfiltering to RGB, grayscale, and opponent-color channels, we enhance local\ncontrast without modifying model architecture or training. Evaluations on\nCityscapes, ACDC, and Dark Zurich show that such preprocessing maintains\nin-distribution performance while improving robustness to adverse conditions\nlike night, fog, and snow. As this processing is model-agnostic and\nlightweight, it holds potential for integration into imaging pipelines,\nenabling imaging systems to deliver task-ready, robust inputs for downstream\nvision models in safety-critical environments."
    },
    {
        "date": "2025-09",
        "title": "Fidelity-Aware Data Composition for Robust Robot Generalization",
        "author": "Zizhao Tong, Di Chen, Sicheng Hu, Hongwei Fan, Liliang Chen, Guanghui Ren, Hao Tang, Hao Dong, and Ling Shao",
        "link": "http://arxiv.org/abs/2509.24797v1",
        "abstract": "Generalist robot policies trained on large-scale, visually homogeneous\ndatasets can be susceptible to shortcut learning, which impairs their\nout-of-distribution (OOD) generalization. While generative data augmentation is\na common approach to introduce diversity, it presents a subtle challenge: data\ncomposition. Naively mixing real and synthetic data can corrupt the learning\nsignal, as this process often prioritizes visual diversity at the expense of\ninformation fidelity. This paper suggests that robust generalization depends on\nprincipled, fidelity-aware data composition. We introduce Coherent Information\nFidelity Tuning (CIFT), a framework that treats data composition as an\noptimization problem. CIFT uses a practical proxy for Information Fidelity\nbased on the feature-space geometry of a dataset. This enables the\nidentification of a phase transition, termed the Decoherence Point, where\ntraining stability degrades. The framework includes a generative engine,\nMulti-View Video Augmentation (MVAug), to synthesize a causally disentangled\ndata spectrum for this tuning process. Applying CIFT to policy architectures\nsuch as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%.\nThese results indicate that fidelity-aware composition, beyond data synthesis\nalone, is an important component for developing robust, general-purpose robots."
    },
    {
        "date": "2025-09",
        "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption",
        "author": "Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, and Li Shen",
        "link": "http://arxiv.org/abs/2509.24748v1",
        "abstract": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$."
    },
    {
        "date": "2025-09",
        "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF",
        "author": "Jing Liu",
        "link": "http://arxiv.org/abs/2509.24713v1",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness."
    },
    {
        "date": "2025-09",
        "title": "Community detection robustness of graph neural networks",
        "author": "Jaidev Goel, Pablo Moriano, Ramakrishnan Kannan, and Yulia R. Gel",
        "link": "http://arxiv.org/abs/2509.24662v1",
        "abstract": "Graph neural networks (GNNs) are increasingly widely used for community\ndetection in attributed networks. They combine structural topology with node\nattributes through message passing and pooling. However, their robustness or\nlack of thereof with respect to different perturbations and targeted attacks in\nconjunction with community detection tasks is not well understood. To shed\nlight into latent mechanisms behind GNN sensitivity on community detection\ntasks, we conduct a systematic computational evaluation of six widely adopted\nGNN architectures: GCN, GAT, Graph-SAGE, DiffPool, MinCUT, and DMoN. The\nanalysis covers three perturbation categories: node attribute manipulations,\nedge topology distortions, and adversarial attacks. We use element-centric\nsimilarity as the evaluation metric on synthetic benchmarks and real-world\ncitation networks. Our findings indicate that supervised GNNs tend to achieve\nhigher baseline accuracy, while unsupervised methods, particularly DMoN,\nmaintain stronger resilience under targeted and adversarial perturbations.\nFurthermore, robustness appears to be strongly influenced by community\nstrength, with well-defined communities reducing performance loss. Across all\nmodels, node attribute perturbations associated with targeted edge deletions\nand shift in attribute distributions tend to cause the largest degradation in\ncommunity recovery. These findings highlight important trade-offs between\naccuracy and robustness in GNN-based community detection and offer new insights\ninto selecting architectures resilient to noise and adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models",
        "author": "Zhifang Zhang, Qiqi Tao, Jiaqi Lv, Na Zhao, Lei Feng, and Joey Tianyi Zhou",
        "link": "http://arxiv.org/abs/2509.24566v1",
        "abstract": "Large vision-language models (LVLMs) have achieved impressive performance\nacross a wide range of vision-language tasks, while they remain vulnerable to\nbackdoor attacks. Existing backdoor attacks on LVLMs aim to force the victim\nmodel to generate a predefined target pattern, which is either inserted into or\nreplaces the original content. We find that these fixed-pattern attacks are\nrelatively easy to detect, because the attacked LVLM tends to memorize such\nfrequent patterns in the training dataset, thereby exhibiting overconfidence on\nthese targets given poisoned inputs. To address these limitations, we introduce\nTokenSwap, a more evasive and stealthy backdoor attack that focuses on the\ncompositional understanding capabilities of LVLMs. Instead of enforcing a fixed\ntargeted content, TokenSwap subtly disrupts the understanding of object\nrelationships in text. Specifically, it causes the backdoored model to generate\noutputs that mention the correct objects in the image but misrepresent their\nrelationships (i.e., bags-of-words behavior). During training, TokenSwap\ninjects a visual trigger into selected samples and simultaneously swaps the\ngrammatical roles of key tokens in the corresponding textual answers. However,\nthe poisoned samples exhibit only subtle differences from the original ones,\nmaking it challenging for the model to learn the backdoor behavior. To address\nthis, TokenSwap employs an adaptive token-weighted loss that explicitly\nemphasizes the learning of swapped tokens, such that the visual triggers and\nbags-of-words behavior are associated. Extensive experiments demonstrate that\nTokenSwap achieves high attack success rates while maintaining superior\nevasiveness and stealthiness across multiple benchmarks and various LVLM\narchitectures."
    },
    {
        "date": "2025-09",
        "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection",
        "author": "Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, and Linh Ngo Van",
        "link": "http://arxiv.org/abs/2509.24547v1",
        "abstract": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance."
    },
    {
        "date": "2025-09",
        "title": "Robust Multimodal Semantic Segmentation with Balanced Modality Contributions",
        "author": "Jiaqi Tan, Xu Zheng, Fangyu Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2509.24505v1",
        "abstract": "Multimodal semantic segmentation enhances model robustness by exploiting\ncross-modal complementarities. However, existing methods often suffer from\nimbalanced modal dependencies, where overall performance degrades significantly\nonce a dominant modality deteriorates in real-world scenarios. Thus, modality\nbalance has become acritical challenge for practical multimodal segmentation.\nTo address this issue, we propose EQUISeg, a multimodal segmentation framework\nthat balances modality contributions through equal encoding of modalities.\nBuilt upon a four-stage Cross-modal Transformer Block(CMTB), EQUISeg enables\nefficient multimodal fusion and hierarchical selection. Furthermore, we design\na Self-guided Module(SGM) that mitigates modality imbalance by introducing a\nmutual guidance mechanism, enabling each modality to adaptively adjust its\ncontribution and enhance robustness under degraded conditions. Extensive\nexperiments on multiple datasets demonstrate that EQUISeg achieves significant\nperformance gains and effectively alleviates the adverse effects of modality\nimbalance in segmentation tasks."
    },
    {
        "date": "2025-09",
        "title": "Distributionally Robust Federated Learning with Outlier Resilience",
        "author": "Zifan Wang, Xinlei Yi, Xenia Konti, Michael M. Zavlanos, and Karl H. Johansson",
        "link": "http://arxiv.org/abs/2509.24462v1",
        "abstract": "Federated learning (FL) enables collaborative model training without direct\ndata sharing, but its performance can degrade significantly in the presence of\ndata distribution perturbations. Distributionally robust optimization (DRO)\nprovides a principled framework for handling this by optimizing performance\nagainst the worst-case distributions within a prescribed ambiguity set.\nHowever, existing DRO-based FL methods often overlook the detrimental impact of\noutliers in local datasets, which can disproportionately bias the learned\nmodels. In this work, we study distributionally robust federated learning with\nexplicit outlier resilience. We introduce a novel ambiguity set based on the\nunbalanced Wasserstein distance, which jointly captures geometric\ndistributional shifts and incorporates a non-geometric Kullback--Leibler\npenalization to mitigate the influence of outliers. This formulation naturally\nleads to a challenging min--max--max optimization problem. To enable\ndecentralized training, we reformulate the problem as a tractable Lagrangian\npenalty optimization, which admits robustness certificates. Building on this\nreformulation, we propose the distributionally outlier-robust federated\nlearning algorithm and establish its convergence guarantees. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach."
    },
    {
        "date": "2025-09",
        "title": "DRIFT: Divergent Response in Filtered Transformations for Robust Adversarial Defense",
        "author": "Amira Guesmi, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2509.24359v1",
        "abstract": "Deep neural networks remain highly vulnerable to adversarial examples, and\nmost defenses collapse once gradients can be reliably estimated. We identify\n\\emph{gradient consensus} -- the tendency of randomized transformations to\nyield aligned gradients -- as a key driver of adversarial transferability.\nAttackers exploit this consensus to construct perturbations that remain\neffective across transformations. We introduce \\textbf{DRIFT} (Divergent\nResponse in Filtered Transformations), a stochastic ensemble of lightweight,\nlearnable filters trained to actively disrupt gradient consensus. Unlike prior\nrandomized defenses that rely on gradient masking, DRIFT enforces\n\\emph{gradient dissonance} by maximizing divergence in Jacobian- and\nlogit-space responses while preserving natural predictions. Our contributions\nare threefold: (i) we formalize gradient consensus and provide a theoretical\nanalysis linking consensus to transferability; (ii) we propose a\nconsensus-divergence training strategy combining prediction consistency,\nJacobian separation, logit-space separation, and adversarial robustness; and\n(iii) we show that DRIFT achieves substantial robustness gains on ImageNet\nacross CNNs and Vision Transformers, outperforming state-of-the-art\npreprocessing, adversarial training, and diffusion-based defenses under\nadaptive white-box, transfer-based, and gradient-free attacks. DRIFT delivers\nthese improvements with negligible runtime and memory cost, establishing\ngradient divergence as a practical and generalizable principle for adversarial\ndefense."
    },
    {
        "date": "2025-09",
        "title": "OMeGa: Joint Optimization of Explicit Meshes and Gaussian Splats for Robust Scene-Level Surface Reconstruction",
        "author": "Yuhang Cao, Haojun Yan, and Danya Yao",
        "link": "http://arxiv.org/abs/2509.24308v1",
        "abstract": "Neural rendering with Gaussian splatting has advanced novel view synthesis,\nand most methods reconstruct surfaces via post-hoc mesh extraction. However,\nexisting methods suffer from two limitations: (i) inaccurate geometry in\ntexture-less indoor regions, and (ii) the decoupling of mesh extraction from\noptimization, thereby missing the opportunity to leverage mesh geometry to\nguide splat optimization. In this paper, we present OMeGa, an end-to-end\nframework that jointly optimizes an explicit triangle mesh and 2D Gaussian\nsplats via a flexible binding strategy, where spatial attributes of Gaussian\nSplats are expressed in the mesh frame and texture attributes are retained on\nsplats. To further improve reconstruction accuracy, we integrate mesh\nconstraints and monocular normal supervision into the optimization, thereby\nregularizing geometry learning. In addition, we propose a heuristic, iterative\nmesh-refinement strategy that splits high-error faces and prunes unreliable\nones to further improve the detail and accuracy of the reconstructed mesh.\nOMeGa achieves state-of-the-art performance on challenging indoor\nreconstruction benchmarks, reducing Chamfer-$L_1$ by 47.3\\% over the 2DGS\nbaseline while maintaining competitive novel-view rendering quality. The\nexperimental results demonstrate that OMeGa effectively addresses prior\nlimitations in indoor texture-less reconstruction."
    },
    {
        "date": "2025-09",
        "title": "Robust Partial 3D Point Cloud Registration via Confidence Estimation under Global Context",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhe Xu, and Zhiqiang Tian",
        "link": "http://arxiv.org/abs/2509.24275v1",
        "abstract": "Partial point cloud registration is essential for autonomous perception and\n3D scene understanding, yet it remains challenging owing to structural\nambiguity, partial visibility, and noise. We address these issues by proposing\nConfidence Estimation under Global Context (CEGC), a unified, confidence-driven\nframework for robust partial 3D registration. CEGC enables accurate alignment\nin complex scenes by jointly modeling overlap confidence and correspondence\nreliability within a shared global context. Specifically, the hybrid overlap\nconfidence estimation module integrates semantic descriptors and geometric\nsimilarity to detect overlapping regions and suppress outliers early. The\ncontext-aware matching strategy smitigates ambiguity by employing global\nattention to assign soft confidence scores to correspondences, improving\nrobustness. These scores guide a differentiable weighted singular value\ndecomposition solver to compute precise transformations. This tightly coupled\npipeline adaptively down-weights uncertain regions and emphasizes contextually\nreliable matches. Experiments on ModelNet40, ScanObjectNN, and 7Scenes 3D\nvision datasets demonstrate that CEGC outperforms state-of-the-art methods in\naccuracy, robustness, and generalization. Overall, CEGC offers an interpretable\nand scalable solution to partial point cloud registration under challenging\nconditions."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation",
        "author": "Inkyu Park, Jeong-Gwan Lee, Taehwan Kwon, Juheon Choi, Seungku Kim, Junsu Kim, and Kimin Lee",
        "link": "http://arxiv.org/abs/2509.24274v1",
        "abstract": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game\ninformation such as enemy locations, are difficult to detect because their\neffects are not directly observable in player behavior. The lack of observable\nevidence makes it difficult to collect reliably labeled data, which is\nessential for training effective anti-cheat systems. Furthermore, cheaters\noften adapt their behavior by limiting or disguising their cheat usage, which\nfurther complicates detection and detector development. To address these\nchallenges, we propose a simulation framework for controlled modeling of ESP\ncheaters, non-cheaters, and trajectory-based detectors. We model cheaters and\nnon-cheaters as reinforcement learning agents with different levels of\nobservability, while detectors classify their behavioral trajectories. Next, we\nformulate the interaction between the cheater and the detector as an\nadversarial game, allowing both players to co-adapt over time. To reflect\nrealistic cheater strategies, we introduce a structured cheater model that\ndynamically switches between cheating and non-cheating behaviors based on\ndetection risk. Experiments demonstrate that our framework successfully\nsimulates adaptive cheater behaviors that strategically balance reward\noptimization and detection evasion. This work provides a controllable and\nextensible platform for studying adaptive cheating behaviors and developing\neffective cheat detectors."
    },
    {
        "date": "2025-09",
        "title": "Skeleton-based Robust Registration Framework for Corrupted 3D Point Clouds",
        "author": "Yongqiang Wang, Weigang Li, Wenping Liu, Zhiqiang Tian, and Jinling Li",
        "link": "http://arxiv.org/abs/2509.24273v1",
        "abstract": "Point cloud registration is fundamental in 3D vision applications, including\nautonomous driving, robotics, and medical imaging, where precise alignment of\nmultiple point clouds is essential for accurate environment reconstruction.\nHowever, real-world point clouds are often affected by sensor limitations,\nenvironmental noise, and preprocessing errors, making registration challenging\ndue to density distortions, noise contamination, and geometric deformations.\nExisting registration methods rely on direct point matching or surface feature\nextraction, which are highly susceptible to these corruptions and lead to\nreduced alignment accuracy. To address these challenges, a skeleton-based\nrobust registration framework is presented, which introduces a\ncorruption-resilient skeletal representation to improve registration robustness\nand accuracy. The framework integrates skeletal structures into the\nregistration process and combines the transformations obtained from both the\ncorrupted point cloud alignment and its skeleton alignment to achieve optimal\nregistration. In addition, a distribution distance loss function is designed to\nenforce the consistency between the source and target skeletons, which\nsignificantly improves the registration performance. This framework ensures\nthat the alignment considers both the original local geometric features and the\nglobal stability of the skeleton structure, resulting in robust and accurate\nregistration results. Experimental evaluations on diverse corrupted datasets\ndemonstrate that SRRF consistently outperforms state-of-the-art registration\nmethods across various corruption scenarios, including density distortions,\nnoise contamination, and geometric deformations. The results confirm the\nrobustness of SRRF in handling corrupted point clouds, making it a potential\napproach for 3D perception tasks in real-world scenarios."
    },
    {
        "date": "2025-09",
        "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation",
        "author": "Weibo Zhao, Jiahao Liu, Bonan Ruan, Shaofei Li, and Zhenkai Liang",
        "link": "http://arxiv.org/abs/2509.24272v1",
        "abstract": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem."
    },
    {
        "date": "2025-09",
        "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models",
        "author": "Zihao Zhu, Xinyu Wu, Gehan Hu, Siwei Lyu, Ke Xu, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2509.24269v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models."
    },
    {
        "date": "2025-09",
        "title": "PROFusion: Robust and Accurate Dense Reconstruction via Camera Pose Regression and Optimization",
        "author": "Siyan Dong, Zijun Wang, Lulu Cai, Yi Ma, and Yanchao Yang",
        "link": "http://arxiv.org/abs/2509.24236v1",
        "abstract": "Real-time dense scene reconstruction during unstable camera motions is\ncrucial for robotics, yet current RGB-D SLAM systems fail when cameras\nexperience large viewpoint changes, fast motions, or sudden shaking. Classical\noptimization-based methods deliver high accuracy but fail with poor\ninitialization during large motions, while learning-based approaches provide\nrobustness but lack sufficient accuracy for dense reconstruction. We address\nthis challenge through a combination of learning-based initialization with\noptimization-based refinement. Our method employs a camera pose regression\nnetwork to predict metric-aware relative poses from consecutive RGB-D frames,\nwhich serve as reliable starting points for a randomized optimization algorithm\nthat further aligns depth images with the scene geometry. Extensive experiments\ndemonstrate promising results: our approach outperforms the best competitor on\nchallenging benchmarks, while maintaining comparable accuracy on stable motion\nsequences. The system operates in real-time, showcasing that combining simple\nand principled techniques can achieve both robustness for unstable motions and\naccuracy for dense reconstruction. Project page:\nhttps://github.com/siyandong/PROFusion."
    },
    {
        "date": "2025-09",
        "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
        "author": "Payal Mohapatra, Yueyuan Sui, Akash Pandey, Stephen Xia, and Qi Zhu",
        "link": "http://arxiv.org/abs/2509.25278v1",
        "abstract": "From clinical healthcare to daily living, continuous sensor monitoring across\nmultiple modalities has shown great promise for real-world intelligent\ndecision-making but also faces various challenges. In this work, we introduce\nMAESTRO, a novel framework that overcomes key limitations of existing\nmultimodal learning approaches: (1) reliance on a single primary modality for\nalignment, (2) pairwise modeling of modalities, and (3) assumption of complete\nmodality observations. These limitations hinder the applicability of these\napproaches in real-world multimodal time-series settings, where primary\nmodality priors are often unclear, the number of modalities can be large\n(making pairwise modeling impractical), and sensor failures often result in\narbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-\nand cross-modal interactions based on task relevance, and leverages symbolic\ntokenization and adaptive attention budgeting to construct long multimodal\nsequences, which are processed via sparse cross-modal attention. The resulting\ncross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)\nmechanism, enabling black-box specialization under varying modality\ncombinations. We evaluate MAESTRO against 10 baselines on four diverse datasets\nspanning three applications, and observe average relative improvements of 4%\nand 8% over the best existing multimodal and multivariate approaches,\nrespectively, under complete observations. Under partial observations -- with\nup to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.\nFurther analysis also demonstrates the robustness and efficiency of MAESTRO's\nsparse, modality-aware design for learning from dynamic time series."
    },
    {
        "date": "2025-09",
        "title": "Latent Collective Preference Optimization: A General Framework for Robust LLM Alignment",
        "author": "Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, and Chao Yu",
        "link": "http://arxiv.org/abs/2509.24159v2",
        "abstract": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Latent Collective Preference Optimization (LCPO). LCPO leverages\nan Expectation-Maximization (EM) algorithm to learn the latent collective\nconsensus from noisy data. It operates by inferring the correctness of each\npreference label and using this probability as an adaptive weight to\nre-calibrate each data point's contribution to the training loss, thereby\nmitigating noise. We generalize this approach by establishing a theoretical\nlink between arbitrary preference losses and their corresponding probabilistic\nmodels, elevating LCPO from a specific algorithm to a general framework for\nrobust preference alignment. Theoretically, we prove that under the condition\nof a perfectly calibrated model, LCPO is guaranteed to converge to the true\nnoise level of the dataset. Our experiments demonstrate LCPO's effectiveness as\na general framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the LCPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% on both\nbenchmarks."
    },
    {
        "date": "2025-09",
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "author": "Zhecheng Li, Guoxian Song, Yiwei Wang, Zhen Xiong, Junsong Yuan, and Yujun Cai",
        "link": "http://arxiv.org/abs/2509.24133v1",
        "abstract": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding."
    },
    {
        "date": "2025-09",
        "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization",
        "author": "Dongki Jung, Jaehoon Choi, Yonghan Lee, and Dinesh Manocha",
        "link": "http://arxiv.org/abs/2509.23991v1",
        "abstract": "The increasing use of 360 images across various domains has emphasized the\nneed for robust depth estimation techniques tailored for omnidirectional\nimages. However, obtaining large-scale labeled datasets for 360 depth\nestimation remains a significant challenge. In this paper, we propose RPG360, a\ntraining-free robust 360 monocular depth estimation method that leverages\nperspective foundation models and graph optimization. Our approach converts 360\nimages into six-face cubemap representations, where a perspective foundation\nmodel is employed to estimate depth and surface normals. To address depth scale\ninconsistencies across different faces of the cubemap, we introduce a novel\ndepth scale alignment technique using graph-based optimization, which\nparameterizes the predicted depth and normal maps while incorporating an\nadditional per-face scale parameter. This optimization ensures depth scale\nconsistency across the six-face cubemap while preserving 3D structural\nintegrity. Furthermore, as foundation models exhibit inherent robustness in\nzero-shot settings, our method achieves superior performance across diverse\ndatasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate\nthe versatility of our depth estimation approach by validating its benefits in\ndownstream tasks such as feature matching 3.2 ~ 5.4% and Structure from Motion\n0.2 ~ 9.7% in AUC@5."
    },
    {
        "date": "2025-09",
        "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling",
        "author": "Rylan Schaeffer, Noam Levi, Andreas Kirsch, Theo Guenais, Brando Miranda, Elyas Obbad, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2509.23963v1",
        "abstract": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of\ncompute-optimal scaling, laying a foundation for future scaling of language\nmodels. In the years since, however, valid concerns about Chinchilla have been\nraised: wide confidence intervals, discrepancies between its three approaches,\nand incongruities with other scaling laws. This raises a critical question for\nthe field: Can practitioners still rely on Chinchilla's prescriptions? Our work\ndemonstrates the answer is yes. We begin by uncovering that the model\nparameters central to Chinchilla's analyses were ambiguous: three\ninterpretations are possible, with relative differences between different\ninterpretations of model parameters as high as 15.2%. We find that, perhaps\nsurprisingly, which model parameters are used for the analyses do not\nmeaningfully affect key results: the scaling law estimates and the\ncompute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,\nthe tokens-to-parameter ratio becomes more constant with the target compute\nbudget. We then ask how distorted the Chinchilla model parameters could have\nbeen without meaningfully affecting the key results. By deliberately perturbing\nmodel parameters in four structured ways, we find that key Chinchilla results\nare most sensitive to additive or systematic errors, which can alter the\notherwise flat trend of the optimal tokens-to-parameter ratio, but overall,\nChinchilla's key results withstand sizable perturbations. Altogether, our\nfindings offer the field renewed confidence in Chinchilla as a durable guide\nfor scaling language models."
    },
    {
        "date": "2025-09",
        "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization",
        "author": "Sheikh Md Mushfiqur Rahman, and Nasir Eisty",
        "link": "http://arxiv.org/abs/2509.23961v1",
        "abstract": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications."
    },
    {
        "date": "2025-09",
        "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems",
        "author": "Guojian Li, Chengyou Wang, Hongfei Xue, Shuiyuan Wang, Dehui Gao, Zihan Zhang, Yuke Lin, Wenjie Li, Longshuai Xiao, Zhonghua Fu, and Lei Xie",
        "link": "http://arxiv.org/abs/2509.23938v1",
        "abstract": "Full-duplex interaction is crucial for natural human-machine communication,\nyet remains challenging as it requires robust turn-taking detection to decide\nwhen the system should speak, listen, or remain silent. Existing solutions\neither rely on dedicated turn-taking models, most of which are not\nopen-sourced. The few available ones are limited by their large parameter size\nor by supporting only a single modality, such as acoustic or linguistic.\nAlternatively, some approaches finetune LLM backbones to enable full-duplex\ncapability, but this requires large amounts of full-duplex data, which remain\nscarce in open-source form. To address these issues, we propose Easy Turn, an\nopen-source, modular turn-taking detection model that integrates acoustic and\nlinguistic bimodal information to predict four dialogue turn states: complete,\nincomplete, backchannel, and wait, accompanied by the release of Easy Turn\ntrainset, a 1,145-hour speech dataset designed for training turn-taking\ndetection models. Compared to existing open-source models like TEN Turn\nDetection and Smart Turn V2, our model achieves state-of-the-art turn-taking\ndetection accuracy on our open-source Easy Turn testset. The data and model\nwill be made publicly available on GitHub."
    },
    {
        "date": "2025-09",
        "title": "Bridging the Task Gap: Multi-Task Adversarial Transferability in CLIP and Its Derivatives",
        "author": "Kuanrong Liu, Siyuan Liang, Cheng Qian, Ming Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2509.23917v1",
        "abstract": "As a general-purpose vision-language pretraining model, CLIP demonstrates\nstrong generalization ability in image-text alignment tasks and has been widely\nadopted in downstream applications such as image classification and image-text\nretrieval. However, it struggles with fine-grained tasks such as object\ndetection and semantic segmentation. While many variants aim to improve CLIP on\nthese tasks, its robustness to adversarial perturbations remains underexplored.\nUnderstanding how adversarial examples transfer across tasks is key to\nassessing CLIP's generalization limits and security risks. In this work, we\nconduct a systematic empirical analysis of the cross-task transfer behavior of\nCLIP-based models on image-text retrieval, object detection, and semantic\nsegmentation under adversarial perturbations. We find that adversarial examples\ngenerated from fine-grained tasks (e.g., object detection and semantic\nsegmentation) often exhibit stronger transfer potential than those from\ncoarse-grained tasks, enabling more effective attacks against the original CLIP\nmodel. Motivated by this observation, we propose a novel framework, Multi-Task\nAdversarial CLIP (MT-AdvCLIP), which introduces a task-aware feature\naggregation loss and generates perturbations with enhanced cross-task\ngeneralization capability. This design strengthens the attack effectiveness of\nfine-grained task models on the shared CLIP backbone. Experimental results on\nmultiple public datasets show that MT-AdvCLIP significantly improves the\nadversarial transfer success rate (The average attack success rate across\nmultiple tasks is improved by over 39%.) against various CLIP-derived models,\nwithout increasing the perturbation budget. This study reveals the transfer\nmechanism of adversarial examples in multi-task CLIP models, offering new\ninsights into multi-task robustness evaluation and adversarial example design."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Versus Federated: An Adversarial Learning based Multi-Modality Cross-Domain Federated Medical Segmentation",
        "author": "You Zhou, Lijiang Chen, Shuchang Lyu, Guangxia Cui, Wenpei Bai, Zheng Zhou, Meng Li, Guangliang Cheng, Huiyu Zhou, and Qi Zhao",
        "link": "http://arxiv.org/abs/2509.23907v1",
        "abstract": "Federated learning enables collaborative training of machine learning models\namong different clients while ensuring data privacy, emerging as the mainstream\nfor breaking data silos in the healthcare domain. However, the imbalance of\nmedical resources, data corruption or improper data preservation may lead to a\nsituation where different clients possess medical images of different modality.\nThis heterogeneity poses a significant challenge for cross-domain medical image\nsegmentation within the federated learning framework. To address this\nchallenge, we propose a new Federated Domain Adaptation (FedDA) segmentation\ntraining framework. Specifically, we propose a feature-level adversarial\nlearning among clients by aligning feature maps across clients through\nembedding an adversarial training mechanism. This design can enhance the\nmodel's generalization on multiple domains and alleviate the negative impact\nfrom domain-shift. Comprehensive experiments on three medical image datasets\ndemonstrate that our proposed FedDA substantially achieves cross-domain\nfederated aggregation, endowing single modality client with cross-modality\nprocessing capabilities, and consistently delivers robust performance compared\nto state-of-the-art federated aggregation algorithms in objective and\nsubjective assessment. Our code are available at\nhttps://github.com/GGbond-study/FedDA."
    },
    {
        "date": "2025-09",
        "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
        "author": "Hitesh Laxmichand Patel, Amit Agarwal, Srikant Panda, Hansa Meghwani, Karan Dua, Paul Li, Tao Sheng, Sujith Ravi, and Dan Roth",
        "link": "http://arxiv.org/abs/2509.23879v1",
        "abstract": "The reliability of Multimodal Large Language Models (MLLMs) in real-world\nsettings is often undermined by sensitivity to irrelevant or distracting visual\ncontext, an aspect not captured by existing evaluation metrics. We introduce\nthe \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and\ninterpretable score for quantifying MLLM robustness to variations in visual\ncontext granularity, measuring performance changes between localized image\npatches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language\nbenchmarks, we find that most leading models remain brittle to background\nnoise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating\nconsistent robustness across tasks. PCRI analysis also highlights how different\nmodel architectures handle and integrate visual context, offering actionable\ndiagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled\nmodel selection and guiding the development of future architectures and\ntraining strategies for robust, real-world deployment."
    },
    {
        "date": "2025-09",
        "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
        "author": "Yukun Chen, Boheng Li, Yu Yuan, Leyi Qi, Yiming Li, Tianwei Zhang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2509.23871v1",
        "abstract": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR."
    },
    {
        "date": "2025-09",
        "title": "FairViT-GAN: A Hybrid Vision Transformer with Adversarial Debiasing for Fair and Explainable Facial Beauty Prediction",
        "author": "Djamel Eddine Boukhari",
        "link": "http://arxiv.org/abs/2509.23859v1",
        "abstract": "Facial Beauty Prediction (FBP) has made significant strides with the\napplication of deep learning, yet state-of-the-art models often exhibit\ncritical limitations, including architectural constraints, inherent demographic\nbiases, and a lack of transparency. Existing methods, primarily based on\nConvolutional Neural Networks (CNNs), excel at capturing local texture but\nstruggle with global facial harmony, while Vision Transformers (ViTs)\neffectively model long-range dependencies but can miss fine-grained details.\nFurthermore, models trained on benchmark datasets can inadvertently learn and\nperpetuate societal biases related to protected attributes like ethnicity. To\naddress these interconnected challenges, we propose \\textbf{FairViT-GAN}, a\nnovel hybrid framework that synergistically integrates a CNN branch for local\nfeature extraction and a ViT branch for global context modeling. More\nsignificantly, we introduce an adversarial debiasing mechanism where the\nfeature extractor is explicitly trained to produce representations that are\ninvariant to protected attributes, thereby actively mitigating algorithmic\nbias. Our framework's transparency is enhanced by visualizing the distinct\nfocus of each architectural branch. Extensive experiments on the SCUT-FBP5500\nbenchmark demonstrate that FairViT-GAN not only sets a new state-of-the-art in\npredictive accuracy, achieving a Pearson Correlation of \\textbf{0.9230} and\nreducing RMSE to \\textbf{0.2650}, but also excels in fairness. Our analysis\nreveals a remarkable \\textbf{82.9\\% reduction in the performance gap} between\nethnic subgroups, with the adversary's classification accuracy dropping to\nnear-random chance (52.1\\%). We believe FairViT-GAN provides a robust,\ntransparent, and significantly fairer blueprint for developing responsible AI\nsystems for subjective visual assessment."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Diffusion for Robust Reinforcement Learning",
        "author": "Daniele Foffano, Alessio Russo, and Alexandre Proutiere",
        "link": "http://arxiv.org/abs/2509.23846v1",
        "abstract": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods."
    },
    {
        "date": "2025-09",
        "title": "Influence-Guided Concolic Testing of Transformer Robustness",
        "author": "Chih-Duo Hong, Yu Wang, Yao-Chen Chang, and Fang Yu",
        "link": "http://arxiv.org/abs/2509.23806v1",
        "abstract": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing."
    },
    {
        "date": "2025-09",
        "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
        "author": "Nayeong Kim, Seong Joon Oh, and Suha Kwak",
        "link": "http://arxiv.org/abs/2509.23781v1",
        "abstract": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs)\nexcels in various vision tasks thanks to the rich knowledge and generalization\nability of VLMs. However, recent studies revealed that such fine-tuned VLMs are\nvulnerable to spurious correlations stemming from the subgroup imbalance in the\nfine-tuning datasets. To resolve this issue, we propose Group Context\nOptimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm\nthat enhances the group robustness of fine-tuned VLMs. Its key idea is to\nemploy group-specific text prompts as group representatives serving as multiple\nclassifiers for their target class. The rich semantic knowledge of the text\nencoder of VLM enables the discovery of effective group prompts even for groups\nwith a small number of training samples. Leveraging the group prompts for each\nclass addresses the issues caused by the group-imbalanced training set, such as\nthe neglect of minority groups and the scattered distribution of each class in\nthe embedding space. GroupCoOp achieved the best results on five benchmarks\nacross five CLIP architectures and occasionally outperformed prior methods that\nfine-tune the entire network, despite training only 0.016\\% of the network's\nparameters."
    },
    {
        "date": "2025-09",
        "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail",
        "author": "Nhan T. Luu",
        "link": "http://arxiv.org/abs/2509.23762v1",
        "abstract": "Spiking Neural Networks (SNNs) have attracted growing interest in both\ncomputational neuroscience and artificial intelligence, primarily due to their\ninherent energy efficiency and compact memory footprint. However, achieving\nadversarial robustness in SNNs, particularly for vision-related tasks, remains\na nascent and underexplored challenge. Recent studies have proposed leveraging\nsparse gradients as a form of regularization to enhance robustness against\nadversarial perturbations. In this work, we present a surprising finding: under\nspecific architectural configurations, SNNs exhibit natural gradient sparsity\nand can achieve state-of-the-art adversarial defense performance without the\nneed for any explicit regularization. Further analysis reveals a trade-off\nbetween robustness and generalization: while sparse gradients contribute to\nimproved adversarial resilience, they can impair the model's ability to\ngeneralize; conversely, denser gradients support better generalization but\nincrease vulnerability to attacks."
    },
    {
        "date": "2025-09",
        "title": "Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability",
        "author": "Ankit Gangwal, and Aaryan Ajay Sharma",
        "link": "http://arxiv.org/abs/2509.23689v1",
        "abstract": "Model Merging (MM) has emerged as a promising alternative to multi-task\nlearning, where multiple fine-tuned models are combined, without access to\ntasks' training data, into a single model that maintains performance across\ntasks. Recent works have explored the impact of MM on adversarial attacks,\nparticularly backdoor attacks. However, none of them have sufficiently explored\nits impact on transfer attacks using adversarial examples, i.e., a black-box\nadversarial attack where examples generated for a surrogate model successfully\nmislead a target model.\n  In this work, we study the effect of MM on the transferability of adversarial\nexamples. We perform comprehensive evaluations and statistical analysis\nconsisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336\ndistinct attack settings. Through it, we first challenge the prevailing notion\nof MM conferring free adversarial robustness, and show MM cannot reliably\ndefend against transfer attacks, with over 95% relative transfer attack success\nrate. Moreover, we reveal 3 key insights for machine-learning practitioners\nregarding MM and transferability for a robust system design: (1) stronger MM\nmethods increase vulnerability to transfer attacks; (2) mitigating\nrepresentation bias increases vulnerability to transfer attacks; and (3) weight\naveraging, despite being the weakest MM method, is the most vulnerable MM\nmethod to transfer attacks. Finally, we analyze the underlying reasons for this\nincreased vulnerability, and provide potential solutions to the problem. Our\nfindings offer critical insights for designing more secure systems employing\nMM."
    },
    {
        "date": "2025-09",
        "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks",
        "author": "Runze Dong, Buhong Wang, Cunqian Feng, Jiang Weng, Chen Han, and Jiwei Tian",
        "link": "http://arxiv.org/abs/2509.23687v1",
        "abstract": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes."
    },
    {
        "date": "2025-09",
        "title": "Color-Pair Guided Robust Zero-Shot 6D Pose Estimation and Tracking of Cluttered Objects on Edge Devices",
        "author": "Xingjian Yang, and Ashis G. Banerjee",
        "link": "http://arxiv.org/abs/2509.23647v1",
        "abstract": "Robust 6D pose estimation of novel objects under challenging illumination\nremains a significant challenge, often requiring a trade-off between accurate\ninitial pose estimation and efficient real-time tracking. We present a unified\nframework explicitly designed for efficient execution on edge devices, which\nsynergizes a robust initial estimation module with a fast motion-based tracker.\nThe key to our approach is a shared, lighting-invariant color-pair feature\nrepresentation that forms a consistent foundation for both stages. For initial\nestimation, this feature facilitates robust registration between the live RGB-D\nview and the object's 3D mesh. For tracking, the same feature logic validates\ntemporal correspondences, enabling a lightweight model to reliably regress the\nobject's motion. Extensive experiments on benchmark datasets demonstrate that\nour integrated approach is both effective and robust, providing competitive\npose estimation accuracy while maintaining high-fidelity tracking even through\nabrupt pose changes."
    },
    {
        "date": "2025-09",
        "title": "DRIK: Distribution-Robust Inductive Kriging without Information Leakage",
        "author": "Chen Yang, Changhao Zhao, Chen Wang, and Jiansheng Fan",
        "link": "http://arxiv.org/abs/2509.23631v1",
        "abstract": "Inductive kriging supports high-resolution spatio-temporal estimation with\nsparse sensor networks, but conventional training-evaluation setups often\nsuffer from information leakage and poor out-of-distribution (OOD)\ngeneralization. We find that the common 2x2 spatio-temporal split allows test\ndata to influence model selection through early stopping, obscuring the true\nOOD characteristics of inductive kriging. To address this issue, we propose a\n3x3 partition that cleanly separates training, validation, and test sets,\neliminating leakage and better reflecting real-world applications. Building on\nthis redefined setting, we introduce DRIK, a Distribution-Robust Inductive\nKriging approach designed with the intrinsic properties of inductive kriging in\nmind to explicitly enhance OOD generalization, employing a three-tier strategy\nat the node, edge, and subgraph levels. DRIK perturbs node coordinates to\ncapture continuous spatial relationships, drops edges to reduce ambiguity in\ninformation flow and increase topological diversity, and adds pseudo-labeled\nsubgraphs to strengthen domain generalization. Experiments on six diverse\nspatio-temporal datasets show that DRIK consistently outperforms existing\nmethods, achieving up to 12.48% lower MAE while maintaining strong scalability."
    },
    {
        "date": "2025-09",
        "title": "Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment",
        "author": "Pu Huang, Shouguang Wang, Siya Yao, and Mengchu Zhou",
        "link": "http://arxiv.org/abs/2509.23618v1",
        "abstract": "Neural speech synthesis techniques have enabled highly realistic speech\ndeepfakes, posing major security risks. Speech deepfake detection is\nchallenging due to distribution shifts across spoofing methods and variability\nin speakers, channels, and recording conditions. We explore learning shared\ndiscriminative features as a path to robust detection and propose Information\nBottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN).\nConfidence-guided adversarial alignment adaptively suppresses attack-specific\nartifacts without erasing discriminative cues, while the information bottleneck\nremoves nuisance variability to preserve transferable features. Experiments on\nASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN\nconsistently outperforms baseline and achieves state-of-the-art performance on\nmany benchmarks."
    },
    {
        "date": "2025-09",
        "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data",
        "author": "Yixu Wang, Yan Teng, Yingchun Wang, and Xingjun Ma",
        "link": "http://arxiv.org/abs/2509.23594v1",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks."
    },
    {
        "date": "2025-09",
        "title": "RobuQ: Pushing DiTs to W1.58A2 via Robust Activation Quantization",
        "author": "Kaicheng Yang, Xun Zhang, Haotong Qin, Yucheng Lin, Kaisen Yang, Xianglong Yan, and Yulun Zhang",
        "link": "http://arxiv.org/abs/2509.23582v1",
        "abstract": "Diffusion Transformers (DiTs) have recently emerged as a powerful backbone\nfor image generation, demonstrating superior scalability and performance over\nU-Net architectures. However, their practical deployment is hindered by\nsubstantial computational and memory costs. While Quantization-Aware Training\n(QAT) has shown promise for U-Nets, its application to DiTs faces unique\nchallenges, primarily due to the sensitivity and distributional complexity of\nactivations. In this work, we identify activation quantization as the primary\nbottleneck for pushing DiTs to extremely low-bit settings. To address this, we\npropose a systematic QAT framework for DiTs, named RobuQ. We start by\nestablishing a strong ternary weight (W1.58A4) DiT baseline. Building upon\nthis, we propose RobustQuantizer to achieve robust activation quantization. Our\ntheoretical analyses show that the Hadamard transform can convert unknown\nper-token distributions into per-token normal distributions, providing a strong\nfoundation for this method. Furthermore, we propose AMPN, the first\nActivation-only Mixed-Precision Network pipeline for DiTs. This method applies\nternary weights across the entire network while allocating different activation\nprecisions to each layer to eliminate information bottlenecks. Through\nextensive experiments on unconditional and conditional image generation, our\nRobuQ framework achieves state-of-the-art performance for DiT quantization in\nsub-4-bit quantization configuration. To the best of our knowledge, RobuQ is\nthe first achieving stable and competitive image generation on large datasets\nlike ImageNet-1K with activations quantized to average 2 bits. The code and\nmodels will be available at https://github.com/racoonykc/RobuQ ."
    },
    {
        "date": "2025-09",
        "title": "Improving constraint-based discovery with robust propagation and reliable LLM priors",
        "author": "Ruiqi Lyu, Alistair Turcan, Martin Jinye Zhang, and Bryan Wilder",
        "link": "http://arxiv.org/abs/2509.23570v1",
        "abstract": "Learning causal structure from observational data is central to scientific\nmodeling and decision-making. Constraint-based methods aim to recover\nconditional independence (CI) relations in a causal directed acyclic graph\n(DAG). Classical approaches such as PC and subsequent methods orient\nv-structures first and then propagate edge directions from these seeds,\nassuming perfect CI tests and exhaustive search of separating subsets --\nassumptions often violated in practice, leading to cascading errors in the\nfinal graph. Recent work has explored using large language models (LLMs) as\nexperts, prompting sets of nodes for edge directions, and could augment edge\norientation when assumptions are not met. However, such methods implicitly\nassume perfect experts, which is unrealistic for hallucination-prone LLMs. We\npropose MosaCD, a causal discovery method that propagates edges from a\nhigh-confidence set of seeds derived from both CI tests and LLM annotations. To\nfilter hallucinations, we introduce shuffled queries that exploit LLMs'\npositional bias, retaining only high-confidence seeds. We then apply a novel\nconfidence-down propagation strategy that orients the most reliable edges\nfirst, and can be integrated with any skeleton-based discovery method. Across\nmultiple real-world graphs, MosaCD achieves higher accuracy in final graph\nconstruction than existing constraint-based methods, largely due to the\nimproved reliability of initial seeds and robust propagation strategies."
    },
    {
        "date": "2025-09",
        "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search",
        "author": "Zeyu Shen, Basileal Imana, Tong Wu, Chong Xiang, Prateek Mittal, and Aleksandra Korolova",
        "link": "http://arxiv.org/abs/2509.23519v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG."
    },
    {
        "date": "2025-09",
        "title": "Robust Multi-Modal Face Anti-Spoofing with Domain Adaptation: Tackling Missing Modalities, Noisy Pseudo-Labels, and Model Degradation",
        "author": "Ming-Tsung Hsu, Fang-Yu Hsu, Yi-Ting Lin, Kai-Heng Chien, Jun-Ren Chen, Cheng-Hsiang Su, Yi-Chen Ou, Chiou-Ting Hsu, and Pei-Kai Huang",
        "link": "http://arxiv.org/abs/2509.23475v1",
        "abstract": "Recent multi-modal face anti-spoofing (FAS) methods have investigated the\npotential of leveraging multiple modalities to distinguish live and spoof\nfaces. However, pre-adapted multi-modal FAS models often fail to detect unseen\nattacks from new target domains. Although a more realistic domain adaptation\n(DA) scenario has been proposed for single-modal FAS to learn specific spoof\nattacks during inference, DA remains unexplored in multi-modal FAS methods. In\nthis paper, we propose a novel framework, MFAS-DANet, to address three major\nchallenges in multi-modal FAS under the DA scenario: missing modalities, noisy\npseudo labels, and model degradation. First, to tackle the issue of missing\nmodalities, we propose extracting complementary features from other modalities\nto substitute missing modality features or enhance existing ones. Next, to\nreduce the impact of noisy pseudo labels during model adaptation, we propose\nderiving reliable pseudo labels by leveraging prediction uncertainty across\ndifferent modalities. Finally, to prevent model degradation, we design an\nadaptive mechanism that decreases the loss weight during unstable adaptations\nand increasing it during stable ones. Extensive experiments demonstrate the\neffectiveness and state-of-the-art performance of our proposed MFAS-DANet."
    },
    {
        "date": "2025-09",
        "title": "3DPCNet: Pose Canonicalization for Robust Viewpoint-Invariant 3D Kinematic Analysis from Monocular RGB cameras",
        "author": "Tharindu Ekanayake, Constantino \u00c1lvarez Casado, and Miguel Bordallo L\u00f3pez",
        "link": "http://arxiv.org/abs/2509.23455v1",
        "abstract": "Monocular 3D pose estimators produce camera-centered skeletons, creating\nview-dependent kinematic signals that complicate comparative analysis in\napplications such as health and sports science. We present 3DPCNet, a compact,\nestimator-agnostic module that operates directly on 3D joint coordinates to\nrectify any input pose into a consistent, body-centered canonical frame. Its\nhybrid encoder fuses local skeletal features from a graph convolutional network\nwith global context from a transformer via a gated cross-attention mechanism.\nFrom this representation, the model predicts a continuous 6D rotation that is\nmapped to an $SO(3)$ matrix to align the pose. We train the model in a\nself-supervised manner on the MM-Fi dataset using synthetically rotated poses,\nguided by a composite loss ensuring both accurate rotation and pose\nreconstruction. On the MM-Fi benchmark, 3DPCNet reduces the mean rotation error\nfrom over 20$^{\\circ}$ to 3.4$^{\\circ}$ and the Mean Per Joint Position Error\nfrom ~64 mm to 47 mm compared to a geometric baseline. Qualitative evaluations\non the TotalCapture dataset further demonstrate that our method produces\nacceleration signals from video that show strong visual correspondence to\nground-truth IMU sensor data, confirming that our module removes viewpoint\nvariability to enable physically plausible motion analysis."
    },
    {
        "date": "2025-09",
        "title": "AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification",
        "author": "Md. Saiful Bari Siddiqui, and Utsab Saha",
        "link": "http://arxiv.org/abs/2509.23454v1",
        "abstract": "Biomedical audio signals, such as phonocardiograms (PCG), are inherently\nrhythmic and contain diagnostic information in both their spectral (tonal) and\ntemporal domains. Standard 2D spectrograms provide rich spectral features but\ncompromise the phase information and temporal precision of the 1D waveform. We\npropose AudioFuse, an architecture that simultaneously learns from both\ncomplementary representations to classify PCGs. To mitigate the overfitting\nrisk common in fusion models, we integrate a custom, wide-and-shallow Vision\nTransformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On\nthe PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive\nROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram\n(0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior\nrobustness to domain shift on the challenging PASCAL dataset, maintaining an\nROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing\ncomplementary representations thus provides a strong inductive bias, enabling\nthe creation of efficient, generalizable classifiers without requiring\nlarge-scale pre-training."
    },
    {
        "date": "2025-09",
        "title": "Flow Matching for Robust Simulation-Based Inference under Model Misspecification",
        "author": "Pierre-Louis Ruhlmann, Pedro L. C. Rodrigues, Michael Arbel, and Florence Forbes",
        "link": "http://arxiv.org/abs/2509.23385v1",
        "abstract": "Simulation-based inference (SBI) is transforming experimental sciences by\nenabling parameter estimation in complex non-linear models from simulated data.\nA persistent challenge, however, is model misspecification: simulators are only\napproximations of reality, and mismatches between simulated and real data can\nyield biased or overconfident posteriors. We address this issue by introducing\nFlow Matching Corrected Posterior Estimation (FMCPE), a framework that\nleverages the flow matching paradigm to refine simulation-trained posterior\nestimators using a small set of real calibration samples. Our approach proceeds\nin two stages: first, a posterior approximator is trained on abundant simulated\ndata; second, flow matching transports its predictions toward the true\nposterior supported by real observations, without requiring explicit knowledge\nof the misspecification. This design enables FMCPE to combine the scalability\nof SBI with robustness to distributional shift. Across synthetic benchmarks and\nreal-world datasets, we show that our proposal consistently mitigates the\neffects of misspecification, delivering improved inference accuracy and\nuncertainty calibration compared to standard SBI baselines, while remaining\ncomputationally efficient."
    },
    {
        "date": "2025-09",
        "title": "Dual-Space Smoothness for Robust and Balanced LLM Unlearning",
        "author": "Han Yan, Zheyuan Liu, and Meng Jiang",
        "link": "http://arxiv.org/abs/2509.23362v1",
        "abstract": "With the rapid advancement of large language models, Machine Unlearning has\nemerged to address growing concerns around user privacy, copyright\ninfringement, and overall safety. Yet state-of-the-art (SOTA) unlearning\nmethods often suffer from catastrophic forgetting and metric imbalance, for\nexample by over-optimizing one objective (e.g., unlearning effectiveness,\nutility preservation, or privacy protection) at the expense of others. In\naddition, small perturbations in the representation or parameter space can be\nexploited by relearn and jailbreak attacks. To address these challenges, we\npropose PRISM, a unified framework that enforces dual-space smoothness in\nrepresentation and parameter spaces to improve robustness and balance\nunlearning metrics. PRISM consists of two smoothness optimization stages: (i) a\nrepresentation space stage that employs a robustly trained probe to defend\nagainst jailbreak attacks, and (ii) a parameter-space stage that decouples\nretain-forget gradient conflicts, reduces imbalance, and smooths the parameter\nspace to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,\nacross conversational-dialogue and continuous-text settings, show that PRISM\noutperforms SOTA baselines under multiple attacks while achieving a better\nbalance among key metrics."
    },
    {
        "date": "2025-09",
        "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling",
        "author": "Jonas Ngnaw\u00e9, Maxime Heuillet, Sabyasachi Sahoo, Yann Pequignot, Ola Ahmad, Audrey Durand, Fr\u00e9d\u00e9ric Precioso, and Christian Gagn\u00e9",
        "link": "http://arxiv.org/abs/2509.23325v1",
        "abstract": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness."
    },
    {
        "date": "2025-09",
        "title": "ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research",
        "author": "Jaxson Brown, Duc-Son Pham, Sie-Teng Soh, Foad Motalebi, Sivaraman Eswaran, and Mahathir Almashor",
        "link": "http://arxiv.org/abs/2509.23305v1",
        "abstract": "Industrial Control Systems (ICSs) are complex interconnected systems used to\nmanage process control within industrial environments, such as chemical\nprocessing plants and water treatment facilities. As the modern industrial\nenvironment moves towards Internet-facing services, ICSs face an increased risk\nof attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).\nThe development of such IDS relies significantly on a simulated testbed as it\nis unrealistic and sometimes hazardous to utilize an operational control\nsystem. Whilst some testbeds have been proposed, they often use a limited\nselection of virtual ICS simulations to test and verify cyber security\nsolutions. There is a lack of investigation done on developing systems that can\nefficiently simulate multiple ICS architectures. Currently, the trend within\nresearch involves developing security solutions on just one ICS simulation,\nwhich can result in bias to its specific architecture. We present ICS-SimLab,\nan end-to-end software suite that utilizes Docker containerization technology\nto create a highly configurable ICS simulation environment. This software\nframework enables researchers to rapidly build and customize different ICS\nenvironments, facilitating the development of security solutions across\ndifferent systems that adhere to the Purdue Enterprise Reference Architecture.\nTo demonstrate its capability, we present three virtual ICS simulations: a\nsolar panel smart grid, a water bottle filling facility, and a system of\nintelligent electronic devices. Furthermore, we run cyber-attacks on these\nsimulations and construct a dataset of recorded malicious and benign network\ntraffic to be used for IDS development."
    },
    {
        "date": "2025-09",
        "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning",
        "author": "Raviteja Anantha, Soheil Hor, Teodor Nicola Antoniu, and Layne C. Price",
        "link": "http://arxiv.org/abs/2509.23252v1",
        "abstract": "We present NanoFlux, a novel adversarial framework for generating targeted\ntraining data to improve LLM reasoning, where adversarially-generated datasets\ncontaining fewer than 200 examples outperform conventional fine-tuning\napproaches. The framework employs a competitive dynamic between models\nalternating as Attacker and Defender, supervised by a tool-augmented Judge,\nsynthesizing multi-step questions with explanatory annotations that target\nspecific reasoning capabilities. Fine-tuning a 4B-parameter model on\nNanoFlux-generated data yields performance gains across diverse domains\ncompared to full-benchmark fine-tuning: +5.9% on mathematical reasoning\n(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical\nreasoning (MultiMedQA), while reducing computational requirements by 3-14x.\nAblation studies reveal a non-monotonic relationship between dataset\ncharacteristics and model performance, uncovering domain-specific optimal\npoints for question complexity and reasoning quality. NanoFlux automates\ntraining data generation through embedding-based novelty filtering,\ntool-augmented evaluation, and multi-hop reasoning, suggesting that future\nmodel improvements may lie in the intelligent synthesis of small, precisely\ntargeted training datasets."
    },
    {
        "date": "2025-09",
        "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding",
        "author": "Runyan Tan, Shuang Wu, and Phillip Howard",
        "link": "http://arxiv.org/abs/2509.23234v2",
        "abstract": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments."
    },
    {
        "date": "2025-09",
        "title": "Real-World Transferable Adversarial Attack on Face-Recognition Systems",
        "author": "Andrey Kaznacheev, Matvey Mikhalchuk, Andrey Kuznetsov, Aleksandr Petiushko, and Anton Razzhigaev",
        "link": "http://arxiv.org/abs/2509.23198v1",
        "abstract": "Adversarial attacks on face recognition (FR) systems pose a significant\nsecurity threat, yet most are confined to the digital domain or require\nwhite-box access. We introduce GaP (Gaussian Patch), a novel method to generate\na universal, physically transferable adversarial patch under a strict black-box\nsetting. Our approach uses a query-efficient, zero-order greedy algorithm to\niteratively construct a symmetric, grayscale pattern for the forehead. The\npatch is optimized by successively adding Gaussian blobs, guided only by the\ncosine similarity scores from a surrogate FR model to maximally degrade\nidentity recognition. We demonstrate that with approximately 10,000 queries to\na black-box ArcFace model, the resulting GaP achieves a high attack success\nrate in both digital and real-world physical tests. Critically, the attack\nshows strong transferability, successfully deceiving an entirely unseen FaceNet\nmodel. Our work highlights a practical and severe vulnerability, proving that\nrobust, transferable attacks can be crafted with limited knowledge of the\ntarget system."
    },
    {
        "date": "2025-09",
        "title": "CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy",
        "author": "Zhanhong Xie, Meifan Zhang, and Lihua Yin",
        "link": "http://arxiv.org/abs/2509.23190v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data locality. However, it still faces\nchallenges from malicious or compromised clients, as well as difficulties in\nincentivizing participants to contribute high-quality data under strict privacy\nrequirements. Motivated by these considerations, we propose CoSIFL, a novel\nframework that integrates proactive alarming for robust security and local\ndifferential privacy (LDP) for inference attacks, together with a\nStackelberg-based incentive scheme to encourage client participation and data\nsharing. Specifically, CoSIFL uses an active alarming mechanism and robust\naggregation to defend against Byzantine and inference attacks, while a Tullock\ncontest-inspired incentive module rewards honest clients for both data\ncontributions and reliable alarm triggers. We formulate the interplay between\nthe server and clients as a two-stage game: in the first stage, the server\ndetermines total rewards, selects participants, and fixes global iteration\nsettings, whereas in the second stage, each client decides its mini-batch size,\nprivacy noise scale, and alerting strategy. We prove that the server-client\ngame admits a unique equilibrium, and analyze how clients' multi-dimensional\nattributes - such as non-IID degrees and privacy budgets - jointly affect\nsystem efficiency. Experimental results on standard benchmarks demonstrate that\nCoSIFL outperforms state-of-the-art solutions in improving model robustness and\nreducing total server costs, highlighting the effectiveness of our integrated\ndesign."
    },
    {
        "date": "2025-09",
        "title": "Confidence-Calibrating Regularization for Robust Brain MRI Segmentation Under Domain Shift",
        "author": "Behraj Khan, and Tahir Qasim Syed",
        "link": "http://arxiv.org/abs/2509.23176v1",
        "abstract": "The Segment Anything Model (SAM) exhibits strong zero-shot performance on\nnatural images but suffers from domain shift and overconfidence when applied to\nmedical volumes. We propose \\textbf{CalSAM}, a lightweight adaptation framework\nthat (i) reduces encoder sensitivity to domain shift via a \\emph{Feature Fisher\nInformation Penalty} (FIP) computed on 3D feature maps and (ii) penalizes\noverconfident voxel-wise errors through a \\emph{Confidence Misalignment\nPenalty} (CMP). The combined loss, \\(\\mathcal{L}_{\\mathrm{CalSAM}}\\) fine-tunes\nonly the mask decoder while keeping SAM's encoders frozen. On cross-center and\nscanner-shift evaluations, CalSAM substantially improves accuracy and\ncalibration: e.g., on the BraTS scanner split (Siemens$\\to$GE) CalSAM shows a\n$+7.4\\%$ relative improvement in $\\mathrm{DSC}$ (80.1\\% vs.\\ 74.6\\%), a\n$-26.9\\%$ reduction in $\\mathrm{HD95}$ (4.6 mm vs.\\ 6.3 mm), and a $-39.5\\%$\nreduction in $\\mathrm{ECE}$ (5.2\\% vs.\\ 8.6\\%). On ATLAS-C (motion\ncorruptions), CalSAM achieves a $+5.3\\%$ relative improvement in $\\mathrm{DSC}$\n(75.9\\%) and a $-32.6\\%$ reduction in $\\mathrm{ECE}$ (5.8\\%). Ablations show\nFIP and CMP contribute complementary gains ($p<0.01$), and the Fisher penalty\nincurs a modest $\\sim$15\\% training-time overhead. CalSAM therefore delivers\nimproved domain generalization and better-calibrated uncertainty estimates for\nbrain MRI segmentation, while retaining the computational benefits of freezing\nSAM's encoder."
    },
    {
        "date": "2025-09",
        "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
        "author": "Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, and Haibo Hu",
        "link": "http://arxiv.org/abs/2509.23041v1",
        "abstract": "Synthetic data refers to artificial samples generated by models. While it has\nbeen validated to significantly enhance the performance of large language\nmodels (LLMs) during training and has been widely adopted in LLM development,\npotential security risks it may introduce remain uninvestigated. This paper\nsystematically evaluates the resilience of synthetic-data-integrated training\nparadigm for LLMs against mainstream poisoning and backdoor attacks. We reveal\nthat such a paradigm exhibits strong resistance to existing attacks, primarily\nthanks to the different distribution patterns between poisoning data and\nqueries used to generate synthetic samples. To enhance the effectiveness of\nthese attacks and further investigate the security risks introduced by\nsynthetic data, we introduce a novel and universal attack framework, namely,\nVirus Infection Attack (VIA), which enables the propagation of current attacks\nthrough synthetic data even under purely clean queries. Inspired by the\nprinciples of virus design in cybersecurity, VIA conceals the poisoning payload\nwithin a protective \"shell\" and strategically searches for optimal hijacking\npoints in benign samples to maximize the likelihood of generating malicious\ncontent. Extensive experiments on both data poisoning and backdoor attacks show\nthat VIA significantly increases the presence of poisoning content in synthetic\ndata and correspondingly raises the attack success rate (ASR) on downstream\nmodels to levels comparable to those observed in the poisoned upstream models."
    },
    {
        "date": "2025-09",
        "title": "GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models",
        "author": "Javad Forough, Mohammad Maheri, and Hamed Haddadi",
        "link": "http://arxiv.org/abs/2509.23037v1",
        "abstract": "Large Language Models (LLMs) are increasingly susceptible to jailbreak\nattacks, which are adversarial prompts that bypass alignment constraints and\ninduce unauthorized or harmful behaviors. These vulnerabilities undermine the\nsafety, reliability, and trustworthiness of LLM outputs, posing critical risks\nin domains such as healthcare, finance, and legal compliance. In this paper, we\npropose GuardNet, a hierarchical filtering framework that detects and filters\njailbreak prompts prior to inference. GuardNet constructs structured graphs\nthat combine sequential links, syntactic dependencies, and attention-derived\ntoken relations to capture both linguistic structure and contextual patterns\nindicative of jailbreak behavior. It then applies graph neural networks at two\nlevels: (i) a prompt-level filter that detects global adversarial prompts, and\n(ii) a token-level filter that pinpoints fine-grained adversarial spans.\nExtensive experiments across three datasets and multiple attack settings show\nthat GuardNet substantially outperforms prior defenses. It raises prompt-level\nF$_1$ scores from 66.4\\% to 99.8\\% on LLM-Fuzzer, and from 67-79\\% to over 94\\%\non PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\\% to\n74-91\\%, with IoU gains up to +28\\%. Despite its structural complexity,\nGuardNet maintains acceptable latency and generalizes well in cross-domain\nevaluations, making it a practical and robust defense against jailbreak threats\nin real-world LLM deployments."
    },
    {
        "date": "2025-09",
        "title": "Desensitizing for Improving Corruption Robustness in Point Cloud Classification through Adversarial Training",
        "author": "Zhiqiang Tian, Weigang Li, Chunhua Deng, Junwei Hu, Yongqiang Wang, and Wenping Liu",
        "link": "http://arxiv.org/abs/2509.23010v1",
        "abstract": "Due to scene complexity, sensor inaccuracies, and processing imprecision,\npoint cloud corruption is inevitable. Over-reliance on input features is the\nroot cause of DNN vulnerabilities. It remains unclear whether this issue exists\nin 3D tasks involving point clouds and whether reducing dependence on these\nfeatures can enhance the model's robustness to corrupted point clouds. This\nstudy attempts to answer these questions. Specifically, we quantified the\nsensitivity of the DNN to point cloud features using Shapley values and found\nthat models trained using traditional methods exhibited high sensitivity values\nfor certain features. Furthermore, under an equal pruning ratio, prioritizing\nthe pruning of highly sensitive features causes more severe damage to model\nperformance than random pruning. We propose `Desensitized Adversarial Training'\n(DesenAT), generating adversarial samples using feature desensitization and\nconducting training within a self-distillation framework, which aims to\nalleviate DNN's over-reliance on point clouds features by smoothing\nsensitivity. First, data points with high contribution components are\neliminated, and spatial transformation is used to simulate corruption scenes,\ngenerate adversarial samples, and conduct adversarial training on the model.\nNext, to compensate for information loss in adversarial samples, we use the\nself-distillation method to transfer knowledge from clean samples to\nadversarial samples, and perform adversarial training in a distillation\nmanner.Extensive experiments on ModelNet-C and PointCloud-C demonstrate show\nthat the propose method can effectively improve the robustness of the model\nwithout reducing the performance of clean data sets. This code is publicly\navailable at\n\\href{https://github.com/JerkyT/DesenAT/tree/master}{https://github.com/JerkyT/DesenAT}."
    },
    {
        "date": "2025-09",
        "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems",
        "author": "Hassen Dhrif",
        "link": "http://arxiv.org/abs/2509.23006v1",
        "abstract": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems."
    },
    {
        "date": "2025-09",
        "title": "Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas",
        "author": "Luke Guerdan, Justin Whitehouse, Kimberly Truong, Kenneth Holstein, and Zhiwei Steven Wu",
        "link": "http://arxiv.org/abs/2509.22957v1",
        "abstract": "As Generative AI (GenAI) systems see growing adoption, a key concern involves\nthe external validity of evaluations, or the extent to which they generalize\nfrom lab-based to real-world deployment conditions. Threats to the external\nvalidity of GenAI evaluations arise when the source sample of human raters and\nsystem outputs used to obtain a system quality estimate differs from the target\ndistribution at deployment time. In this work, we propose a doubly-robust\nestimation framework designed to address this evaluation sampling bias. Key to\nour approach is the use of \"persona\" ratings produced by prompting an LLM\nevaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific\nsociodemographic characteristics. Our doubly-robust framework combines these\ninformative yet imperfect persona ratings with human ratings obtained under\nevaluation sampling bias to produce statistically valid system quality\nestimates. In particular, we show that our approach yields valid system quality\nestimates when either (i) a model trained to predict human ratings using\npersona ratings and source data observed under sampling bias, or (ii) a\nreweighting model that corrects for sampling bias is of sufficient quality. We\nvalidate our framework theoretically and via a novel Persona Simulation\nFramework (PSF) designed to systematically manipulate persona quality and the\ndegree of evaluation sampling bias present in source data. Our work provides a\nprincipled foundation for combining imperfect persona ratings with human\nratings observed under sampling bias to obtain valid system quality estimates."
    },
    {
        "date": "2025-09",
        "title": "AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning",
        "author": "Aashnan Rahman, Abid Hasan, Sherajul Arifin, Faisal Haque Bappy, Tahrim Hossain, Tariqul Islam, Abu Raihan Mostofa Kamal, and Md. Azam Hossain",
        "link": "http://arxiv.org/abs/2509.22873v2",
        "abstract": "Federated learning (FL) enables privacy-preserving model training by keeping\ndata decentralized. However, it remains vulnerable to label-flipping attacks,\nwhere malicious clients manipulate labels to poison the global model. Despite\ntheir simplicity, these attacks can severely degrade model performance, and\ndefending against them remains challenging. We introduce AntiFLipper, a novel\nand computationally efficient defense against multi-class label-flipping\nattacks in FL. Unlike existing methods that ensure security at the cost of high\ncomputational overhead, AntiFLipper employs a novel client-side detection\nstrategy, significantly reducing the central server's burden during\naggregation. Comprehensive empirical evaluations across multiple datasets under\ndifferent distributions demonstrate that AntiFLipper achieves accuracy\ncomparable to state-of-the-art defenses while requiring substantially fewer\ncomputational resources in server side. By balancing security and efficiency,\nAntiFLipper addresses a critical gap in existing defenses, making it\nparticularly suitable for resource-constrained FL deployments where both model\nintegrity and operational efficiency are essential."
    },
    {
        "date": "2025-09",
        "title": "Observation-Free Attacks on Online Learning to Rank",
        "author": "Sameep Chattopadhyay, Nikhil Karamchandani, and Sharayu Mohair",
        "link": "http://arxiv.org/abs/2509.22855v1",
        "abstract": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data."
    },
    {
        "date": "2025-09",
        "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data",
        "author": "Roie Kazoom, Yuval Ratzabi, Etamar Rothstein, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2509.22850v1",
        "abstract": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems."
    },
    {
        "date": "2025-09",
        "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN",
        "author": "Roie Kazoom, Alon Goldberg, Hodaya Cohen, and Ofer Hadar",
        "link": "http://arxiv.org/abs/2509.22836v1",
        "abstract": "Adversarial patch attacks pose a severe threat to deep neural networks, yet\nmost existing approaches rely on unrealistic white-box assumptions, untargeted\nobjectives, or produce visually conspicuous patches that limit real-world\napplicability. In this work, we introduce a novel framework for fully\ncontrollable adversarial patch generation, where the attacker can freely choose\nboth the input image x and the target class y target, thereby dictating the\nexact misclassification outcome. Our method combines a generative U-Net design\nwith Grad-CAM-guided patch placement, enabling semantic-aware localization that\nmaximizes attack effectiveness while preserving visual realism. Extensive\nexperiments across convolutional networks (DenseNet-121, ResNet-50) and vision\ntransformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach\nachieves state-of-the-art performance across all settings, with attack success\nrates (ASR) and target-class success (TCS) consistently exceeding 99%.\n  Importantly, we show that our method not only outperforms prior white-box\nattacks and untargeted baselines, but also surpasses existing non-realistic\napproaches that produce detectable artifacts. By simultaneously ensuring\nrealism, targeted control, and black-box applicability-the three most\nchallenging dimensions of patch-based attacks-our framework establishes a new\nbenchmark for adversarial robustness research, bridging the gap between\ntheoretical attack strength and practical stealthiness."
    },
    {
        "date": "2025-09",
        "title": "Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions",
        "author": "Aditi Tiwari, Akshit Bhalla, and Darshan Prasad",
        "link": "http://arxiv.org/abs/2509.22814v1",
        "abstract": "The Model Context Protocol (MCP) defines a schema bound execution model for\nagent-tool interaction, enabling modular computer vision workflows without\nretraining. To our knowledge, this is the first protocol level, deployment\nscale audit of MCP in vision systems, identifying systemic weaknesses in schema\nsemantics, interoperability, and runtime coordination. We analyze 91 publicly\nregistered vision centric MCP servers, annotated along nine dimensions of\ncompositional fidelity, and develop an executable benchmark with validators to\ndetect and categorize protocol violations. The audit reveals high prevalence of\nschema format divergence, missing runtime schema validation, undeclared\ncoordinate conventions, and reliance on untracked bridging scripts. Validator\nbased testing quantifies these failures, with schema format checks flagging\nmisalignments in 78.0 percent of systems, coordinate convention checks\ndetecting spatial reference errors in 24.6 percent, and memory scope checks\nissuing an average of 33.8 warnings per 100 executions. Security probes show\nthat dynamic and multi agent workflows exhibit elevated risks of privilege\nescalation and untyped tool connections. The proposed benchmark and validator\nsuite, implemented in a controlled testbed and to be released on GitHub,\nestablishes a reproducible framework for measuring and improving the\nreliability and security of compositional vision workflows."
    },
    {
        "date": "2025-09",
        "title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs",
        "author": "Xingyu Li, Juefei Pu, Yifan Wu, Xiaochen Zou, Shitong Zhu, Xiaochen Zou, Shitong Zhu, Qiushi Wu, Zheng Zhang, Joshua Hsu, Yue Dong, Zhiyun Qian, Kangjie Lu, Trent Jaeger, Michael De Lucia, and Srikanth V. Krishnamurthy",
        "link": "http://arxiv.org/abs/2509.22796v1",
        "abstract": "Open-source software projects are foundational to modern software ecosystems,\nwith the Linux kernel standing out as a critical exemplar due to its ubiquity\nand complexity. Although security patches are continuously integrated into the\nLinux mainline kernel, downstream maintainers often delay their adoption,\ncreating windows of vulnerability. A key reason for this lag is the difficulty\nin identifying security-critical patches, particularly those addressing\nexploitable vulnerabilities such as out-of-bounds (OOB) accesses and\nuse-after-free (UAF) bugs. This challenge is exacerbated by intentionally\nsilent bug fixes, incomplete or missing CVE assignments, delays in CVE\nissuance, and recent changes to the CVE assignment criteria for the Linux\nkernel. While fine-grained patch classification approaches exist, they exhibit\nlimitations in both coverage and accuracy. In this work, we identify previously\nunexplored opportunities to significantly improve fine-grained patch\nclassification. Specifically, by leveraging cues from commit titles/messages\nand diffs alongside appropriate code context, we develop DUALLM, a dual-method\npipeline that integrates two approaches based on a Large Language Model (LLM)\nand a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an\nF1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM\nsuccessfully identified 111 of 5,140 recent Linux kernel patches as addressing\nOOB or UAF vulnerabilities, with 90 true positives confirmed by manual\nverification (many do not have clear indications in patch descriptions).\nMoreover, we constructed proof-of-concepts for two identified bugs (one UAF and\none OOB), including one developed to conduct a previously unknown control-flow\nhijack as further evidence of the correctness of the classification."
    },
    {
        "date": "2025-09",
        "title": "Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks",
        "author": "Gaurav Bagwe, Saket S. Chaturvedi, Xiaolong Ma, Xiaoyong Yuan, Kuang-Ching Wang, and Lan Zhang",
        "link": "http://arxiv.org/abs/2509.22486v1",
        "abstract": "Retrieval-augmented generation (RAG) enhances factual grounding by\nintegrating retrieval mechanisms with generative models but introduces new\nattack surfaces, particularly through backdoor attacks. While prior research\nhas largely focused on disinformation threats, fairness vulnerabilities remain\nunderexplored. Unlike conventional backdoors that rely on direct\ntrigger-to-target mappings, fairness-driven attacks exploit the interaction\nbetween retrieval and generation models, manipulating semantic relationships\nbetween target groups and social biases to establish a persistent and covert\ninfluence on content generation.\n  This paper introduces BiasRAG, a systematic framework that exposes fairness\nvulnerabilities in RAG through a two-phase backdoor attack. During the\npre-training phase, the query encoder is compromised to align the target group\nwith the intended social bias, ensuring long-term persistence. In the\npost-deployment phase, adversarial documents are injected into knowledge bases\nto reinforce the backdoor, subtly influencing retrieved content while remaining\nundetectable under standard fairness evaluations. Together, BiasRAG ensures\nprecise target alignment over sensitive attributes, stealthy execution, and\nresilience. Empirical evaluations demonstrate that BiasRAG achieves high attack\nsuccess rates while preserving contextual relevance and utility, establishing a\npersistent and evolving threat to fairness in RAG."
    },
    {
        "date": "2025-09",
        "title": "B\u00e9zier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation",
        "author": "Chen Li, Meilong Xu, Xiaoling Hu, Weimin Lyu, and Chao Chen",
        "link": "http://arxiv.org/abs/2509.22476v1",
        "abstract": "Training robust learning algorithms across different medical imaging\nmodalities is challenging due to the large domain gap. Unsupervised domain\nadaptation (UDA) mitigates this problem by using annotated images from the\nsource domain and unlabeled images from the target domain to train the deep\nmodels. Existing approaches often rely on GAN-based style transfer, but these\nmethods struggle to capture cross-domain mappings in regions with high\nvariability. In this paper, we propose a unified framework, B\\'ezier Meets\nDiffusion, for cross-domain image generation. First, we introduce a\nB\\'ezier-curve-based style transfer strategy that effectively reduces the\ndomain gap between source and target domains. The transferred source images\nenable the training of a more robust segmentation model across domains.\nThereafter, using pseudo-labels generated by this segmentation model on the\ntarget domain, we train a conditional diffusion model (CDM) to synthesize\nhigh-quality, labeled target-domain images. To mitigate the impact of noisy\npseudo-labels, we further develop an uncertainty-guided score matching method\nthat improves the robustness of CDM training. Extensive experiments on public\ndatasets demonstrate that our approach generates realistic labeled images,\nsignificantly augmenting the target domain and improving segmentation\nperformance."
    },
    {
        "date": "2025-09",
        "title": "Text Adversarial Attacks with Dynamic Outputs",
        "author": "Wenqiang Wang, Siyuan Liang, Xiao Yan, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2509.22393v1",
        "abstract": "Text adversarial attack methods are typically designed for static scenarios\nwith fixed numbers of output labels and a predefined label space, relying on\nextensive querying of the victim model (query-based attacks) or the surrogate\nmodel (transfer-based attacks). To address this gap, we introduce the Textual\nDynamic Outputs Attack (TDOA) method, which employs a clustering-based\nsurrogate model training approach to convert the dynamic-output scenario into a\nstatic single-output scenario. To improve attack effectiveness, we propose the\nfarthest-label targeted attack strategy, which selects adversarial vectors that\ndeviate most from the model's coarse-grained labels, thereby maximizing\ndisruption. We extensively evaluate TDOA on four datasets and eight victim\nmodels (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting\nadversarial examples and its strong potential to compromise large language\nmodels with limited access. With a single query per text, TDOA achieves a\nmaximum attack success rate of 50.81\\%. Additionally, we find that TDOA also\nachieves state-of-the-art performance in conventional static output scenarios,\nreaching a maximum ASR of 82.68\\%. Meanwhile, by conceptualizing translation\ntasks as classification problems with unbounded output spaces, we extend the\nTDOA framework to generative settings, surpassing prior results by up to 0.64\nRDBLEU and 0.62 RDchrF."
    },
    {
        "date": "2025-09",
        "title": "Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning",
        "author": "Nakyeong Yang, Dong-Kyum Kim, Jea Kwon, Minsung Kim, Kyomin Jung, and Meeyoung Cha",
        "link": "http://arxiv.org/abs/2509.22263v1",
        "abstract": "Large language models trained on web-scale data can memorize private or\nsensitive knowledge, raising significant privacy risks. Although some\nunlearning methods mitigate these risks, they remain vulnerable to \"relearning\"\nduring subsequent training, allowing a substantial portion of forgotten\nknowledge to resurface. In this paper, we show that widely used unlearning\nmethods cause shallow alignment: instead of faithfully erasing target\nknowledge, they generate spurious unlearning neurons that amplify negative\ninfluence to hide it. To overcome this limitation, we introduce Ssiuu, a new\nclass of unlearning methods that employs attribution-guided regularization to\nprevent spurious negative influence and faithfully remove target knowledge.\nExperimental results confirm that our method reliably erases target knowledge\nand outperforms strong baselines across two practical retraining scenarios: (1)\nadversarial injection of private data, and (2) benign attack using an\ninstruction-following benchmark. Our findings highlight the necessity of robust\nand faithful unlearning methods for safe deployment of language models."
    },
    {
        "date": "2025-09",
        "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space",
        "author": "Haochen Gong, Chenxiao Li, Rui Chang, and Wenbo Shen",
        "link": "http://arxiv.org/abs/2509.22256v1",
        "abstract": "Large language model (LLM)-based computer-use agents represent a convergence\nof AI and OS capabilities, enabling natural language to control system- and\napplication-level functions. However, due to LLMs' inherent uncertainty issues,\ngranting agents control over computers poses significant security risks. When\nagent actions deviate from user intentions, they can cause irreversible\nconsequences. Existing mitigation approaches, such as user confirmation and\nLLM-based dynamic action validation, still suffer from limitations in\nusability, security, and performance. To address these challenges, we propose\nCSAgent, a system-level, static policy-based access control framework for\ncomputer-use agents. To bridge the gap between static policy and dynamic\ncontext and user intent, CSAgent introduces intent- and context-aware policies,\nand provides an automated toolchain to assist developers in constructing and\nrefining them. CSAgent enforces these policies through an optimized OS service,\nensuring that agent actions can only be executed under specific user intents\nand contexts. CSAgent supports protecting agents that control computers through\ndiverse interfaces, including API, CLI, and GUI. We implement and evaluate\nCSAgent, which successfully defends against more than 99.36% of attacks while\nintroducing only 6.83% performance overhead."
    },
    {
        "date": "2025-09",
        "title": "COMPASS: Robust Feature Conformal Prediction for Medical Segmentation Metrics",
        "author": "Matt Y. Cheung, Ashok Veeraraghavan, and Guha Balakrishnan",
        "link": "http://arxiv.org/abs/2509.22240v1",
        "abstract": "In clinical applications, the utility of segmentation models is often based\non the accuracy of derived downstream metrics such as organ size, rather than\nby the pixel-level accuracy of the segmentation masks themselves. Thus,\nuncertainty quantification for such metrics is crucial for decision-making.\nConformal prediction (CP) is a popular framework to derive such principled\nuncertainty guarantees, but applying CP naively to the final scalar metric is\ninefficient because it treats the complex, non-linear segmentation-to-metric\npipeline as a black box. We introduce COMPASS, a practical framework that\ngenerates efficient, metric-based CP intervals for image segmentation models by\nleveraging the inductive biases of their underlying deep neural networks.\nCOMPASS performs calibration directly in the model's representation space by\nperturbing intermediate features along low-dimensional subspaces maximally\nsensitive to the target metric. We prove that COMPASS achieves valid marginal\ncoverage under exchangeability and nestedness assumptions. Empirically, we\ndemonstrate that COMPASS produces significantly tighter intervals than\ntraditional CP baselines on four medical image segmentation tasks for area\nestimation of skin lesions and anatomical structures. Furthermore, we show that\nleveraging learned internal features to estimate importance weights allows\nCOMPASS to also recover target coverage under covariate shifts. COMPASS paves\nthe way for practical, metric-based uncertainty quantification for medical\nimage segmentation."
    },
    {
        "date": "2025-09",
        "title": "Learn, Check, Test -- Security Testing Using Automata Learning and Model Checking",
        "author": "Stefan Marksteiner, Mikael Sj\u00f6din, and Marjan Sirjani",
        "link": "http://arxiv.org/abs/2509.22215v1",
        "abstract": "Cyber-physical systems are part of industrial systems and critical\ninfrastructure. Therefore, they should be examined in a comprehensive manner to\nverify their correctness and security. At the same time, the complexity of such\nsystems demands such examinations to be systematic and, if possible, automated\nfor efficiency and accuracy. A method that can be useful in this context is\nmodel checking. However, this requires a model that faithfully represents the\nbehavior of the examined system. Obtaining such a model is not trivial, as many\nof these systems can be examined only in black box settings due to, e.g., long\nsupply chains or secrecy. We therefore utilize active black box learning\ntechniques to infer behavioral models in the form of Mealy machines of such\nsystems and translate them into a form that can be evaluated using a model\nchecker. To this end, we will investigate a cyber-physical systems as a black\nbox using its external communication interface. We first annotate the model\nwith propositions by mapping context information from the respective protocol\nto the model using Context-based Proposition Maps (CPMs). We gain annotated\nMealy machines that resemble Kripke structures. We then formally define a\ntemplate, to transfer the structures model checker-compatible format. We\nfurther define generic security properties based on basic security\nrequirements. Due to the used CPMs, we can instantiate these properties with a\nmeaningful context to check a specific protocol, which makes the approach\nflexible and scalable. The gained model can be easily altered to introduce\nnon-deterministic behavior (like timeouts) or faults and examined if the\nproperties still. Lastly, we demonstrate the versatility of the approach by\nproviding case studies of different communication protocols (NFC and UDS),\nchecked with the same tool chain and the same security properties."
    },
    {
        "date": "2025-09",
        "title": "Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security",
        "author": "Petar Radanliev",
        "link": "http://arxiv.org/abs/2509.22757v1",
        "abstract": "This study presents a structured approach to evaluating vulnerabilities\nwithin quantum cryptographic protocols, focusing on the BB84 quantum key\ndistribution method and National Institute of Standards and Technology (NIST)\napproved quantum-resistant algorithms. By integrating AI-driven red teaming,\nautomated penetration testing, and real-time anomaly detection, the research\ndevelops a framework for assessing and mitigating security risks in quantum\nnetworks. The findings demonstrate that AI can be effectively used to simulate\nadversarial attacks, probe weaknesses in cryptographic implementations, and\nrefine security mechanisms through iterative feedback. The use of automated\nexploit simulations and protocol fuzzing provides a scalable means of\nidentifying latent vulnerabilities, while adversarial machine learning\ntechniques highlight novel attack surfaces within AI-enhanced cryptographic\nprocesses. This study offers a comprehensive methodology for strengthening\nquantum security and provides a foundation for integrating AI-driven\ncybersecurity practices into the evolving quantum landscape."
    },
    {
        "date": "2025-09",
        "title": "Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting",
        "author": "Zhou Xu, Guyue Li, Zhe Peng, and Aiqun Hu",
        "link": "http://arxiv.org/abs/2509.22154v1",
        "abstract": "Radio frequency fingerprint (RFF) is a promising device identification\ntechnology, with recent research shifting from robustness to security due to\ngrowing concerns over vulnerabilities. To date, while the security of RFF\nagainst basic spoofing such as MAC address tampering has been validated, its\nresilience to advanced mimicry remains unknown. To address this gap, we propose\na collusion-driven impersonation attack that achieves RF-level mimicry,\nsuccessfully breaking RFF identification systems across diverse environments.\nSpecifically, the attacker synchronizes with a colluding receiver to match the\ncentralized logarithmic power spectrum (CLPS) of the legitimate transmitter;\nonce the colluder deems the CLPS identical, the victim receiver will also\naccept the forged fingerprint, completing RF-level spoofing. Given that the\ndistribution of CLPS features is relatively concentrated and has a clear\nunderlying structure, we design a spoofed signal generation network that\nintegrates a variational autoencoder (VAE) with a multi-objective loss function\nto enhance the similarity and deceptive capability of the generated samples. We\ncarry out extensive simulations, validating cross-channel attacks in\nenvironments that incorporate standard channel variations including additive\nwhite Gaussian noise (AWGN), multipath fading, and Doppler shift. The results\nindicate that the proposed attack scheme essentially maintains a success rate\nof over 95% under different channel conditions, revealing the effectiveness of\nthis attack."
    },
    {
        "date": "2025-09",
        "title": "Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions",
        "author": "Zhiqiang Tian, Weigang Li, Junwei Hu, and Chunhua Deng",
        "link": "http://arxiv.org/abs/2509.22150v1",
        "abstract": "Classification tasks in 3D point clouds often assume that class events\n\\replaced{are }{follow }independent and identically distributed (IID), although\nthis assumption destroys the correlation between classes. This \\replaced{study\n}{paper }proposes a classification strategy, \\textbf{J}oint \\textbf{G}raph\n\\textbf{E}ntropy \\textbf{K}nowledge \\textbf{D}istillation (JGEKD), suitable for\nnon-independent and identically distributed 3D point cloud data,\n\\replaced{which }{the strategy } achieves knowledge transfer of class\ncorrelations through knowledge distillation by constructing a loss function\nbased on joint graph entropy. First\\deleted{ly}, we employ joint graphs to\ncapture add{the }hidden relationships between classes\\replaced{ and}{,}\nimplement knowledge distillation to train our model by calculating the entropy\nof add{add }graph.\\replaced{ Subsequently}{ Then}, to handle 3D point clouds\n\\deleted{that is }invariant to spatial transformations, we construct\n\\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge\ndistillation and teacher-knowledge distillation, to facilitate information\ntransfer between different transformation forms of the same data. \\replaced{In\naddition}{ Additionally}, we use the above framework to achieve knowledge\ntransfer between point clouds and their corrupted forms, and increase the\nrobustness against corruption of model. Extensive experiments on ScanObject,\nModelNet40, ScanntV2\\_cls and ModelNet-C demonstrate that the proposed strategy\ncan achieve competitive results."
    },
    {
        "date": "2025-09",
        "title": "Countering adversarial evasion in regression analysis",
        "author": "David Benfield, Phan Tu Vuong, and Alain Zemkoho",
        "link": "http://arxiv.org/abs/2509.22113v1",
        "abstract": "Adversarial machine learning challenges the assumption that the underlying\ndistribution remains consistent throughout the training and implementation of a\nprediction model. In particular, adversarial evasion considers scenarios where\nadversaries adapt their data to influence particular outcomes from established\nprediction models, such scenarios arise in applications such as spam email\nfiltering, malware detection and fake-image generation, where security methods\nmust be actively updated to keep up with the ever-improving generation of\nmalicious data. Game theoretic models have been shown to be effective at\nmodelling these scenarios and hence training resilient predictors against such\nadversaries. Recent advancements in the use of pessimistic bilevel optimsiation\nwhich remove assumptions about the convexity and uniqueness of the adversary's\noptimal strategy have proved to be particularly effective at mitigating threats\nto classifiers due to its ability to capture the antagonistic nature of the\nadversary. However, this formulation has not yet been adapted to regression\nscenarios. This article serves to propose a pessimistic bilevel optimisation\nprogram for regression scenarios which makes no assumptions on the convexity or\nuniqueness of the adversary's solutions."
    },
    {
        "date": "2025-09",
        "title": "Concept activation vectors: a unifying view and adversarial attacks",
        "author": "Ekkehard Schnoor, Malik Tiomoko, Jawher Said, Alex Jung, and Wojciech Samek",
        "link": "http://arxiv.org/abs/2509.22755v1",
        "abstract": "Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a\npromising approach for understanding how human-understandable concepts are\nencoded in a model's latent spaces. They are computed from hidden-layer\nactivations of inputs belonging either to a concept class or to non-concept\nexamples. Adopting a probabilistic perspective, the distribution of the\n(non-)concept inputs induces a distribution over the CAV, making it a random\nvector in the latent space. This enables us to derive mean and covariance for\ndifferent types of CAVs, leading to a unified theoretical view. This\nprobabilistic perspective also reveals a potential vulnerability: CAVs can\nstrongly depend on the rather arbitrary non-concept distribution, a factor\nlargely overlooked in prior work. We illustrate this with a simple yet\neffective adversarial attack, underscoring the need for a more systematic\nstudy."
    },
    {
        "date": "2025-09",
        "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios",
        "author": "Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, and David Lo",
        "link": "http://arxiv.org/abs/2509.22097v1",
        "abstract": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs."
    },
    {
        "date": "2025-09",
        "title": "Non-Linear Trajectory Modeling for Multi-Step Gradient Inversion Attacks in Federated Learning",
        "author": "Li Xia, Zheng Liu, Sili Huang, Wei Tang, and Xuan Liu",
        "link": "http://arxiv.org/abs/2509.22082v1",
        "abstract": "Federated Learning (FL) preserves privacy by keeping raw data local, yet\nGradient Inversion Attacks (GIAs) pose significant threats. In FedAVG\nmulti-step scenarios, attackers observe only aggregated gradients, making data\nreconstruction challenging. Existing surrogate model methods like SME assume\nlinear parameter trajectories, but we demonstrate this severely underestimates\nSGD's nonlinear complexity, fundamentally limiting attack effectiveness. We\npropose Non-Linear Surrogate Model Extension (NL-SME), the first method to\nintroduce nonlinear parametric trajectory modeling for GIAs. Our approach\nreplaces linear interpolation with learnable quadratic B\\'ezier curves that\ncapture SGD's curved characteristics through control points, combined with\nregularization and dvec scaling mechanisms for enhanced expressiveness.\nExtensive experiments on CIFAR-100 and FEMNIST datasets show NL-SME\nsignificantly outperforms baselines across all metrics, achieving\norder-of-magnitude improvements in cosine similarity loss while maintaining\ncomputational efficiency.This work exposes heightened privacy vulnerabilities\nin FL's multi-step update paradigm and offers novel perspectives for developing\nrobust defense strategies."
    },
    {
        "date": "2025-09",
        "title": "SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection",
        "author": "Inzamamul Alam, Md Tanvir Islam, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2509.22070v1",
        "abstract": "The increasing realism of content generated by GANs and diffusion models has\nmade deepfake detection significantly more challenging. Existing approaches\noften focus solely on spatial or frequency-domain features, limiting their\ngeneralization to unseen manipulations. We propose the Spectral\nCross-Attentional Network (SpecXNet), a dual-domain architecture for robust\ndeepfake detection. The core \\textbf{Dual-Domain Feature Coupler (DDFC)}\ndecomposes features into a local spatial branch for capturing texture-level\nanomalies and a global spectral branch that employs Fast Fourier Transform to\nmodel periodic inconsistencies. This dual-domain formulation allows SpecXNet to\njointly exploit localized detail and global structural coherence, which are\ncritical for distinguishing authentic from manipulated images. We also\nintroduce the \\textbf{Dual Fourier Attention (DFA)} module, which dynamically\nfuses spatial and spectral features in a content-aware manner. Built atop a\nmodified XceptionNet backbone, we embed the DDFC and DFA modules within a\nseparable convolution block. Extensive experiments on multiple deepfake\nbenchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly\nunder cross-dataset and unseen manipulation scenarios, while maintaining\nreal-time feasibility. Our results highlight the effectiveness of unified\nspatial-spectral learning for robust and generalizable deepfake detection. To\nensure reproducibility, we released the full code on\n\\href{https://github.com/inzamamulDU/SpecXNet}{\\textcolor{blue}{\\textbf{GitHub}}}."
    },
    {
        "date": "2025-09",
        "title": "Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks",
        "author": "Aravindhan G, Yuvaraj Govindarajulu, and Parin Shah",
        "link": "http://arxiv.org/abs/2509.22060v1",
        "abstract": "Recent studies have demonstrated the vulnerability of Automatic Speech\nRecognition systems to adversarial examples, which can deceive these systems\ninto misinterpreting input speech commands. While previous research has\nprimarily focused on white-box attacks with constrained optimizations, and\ntransferability based black-box attacks against commercial Automatic Speech\nRecognition devices, this paper explores cost efficient white-box attack and\nnon transferability black-box adversarial attacks on Automatic Speech\nRecognition systems, drawing insights from approaches such as Fast Gradient\nSign Method and Zeroth-Order Optimization. Further, the novelty of the paper\nincludes how poisoning attack can degrade the performances of state-of-the-art\nmodels leading to misinterpretation of audio signals. Through experimentation\nand analysis, we illustrate how hybrid models can generate subtle yet impactful\nadversarial examples with very little perturbation having Signal Noise Ratio of\n35dB that can be generated within a minute. These vulnerabilities of\nstate-of-the-art open source model have practical security implications, and\nemphasize the need for adversarial security."
    },
    {
        "date": "2025-09",
        "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors",
        "author": "Yue Liu, Yanjie Zhao, Yunbo Lyu, Ting Zhang, Haoyu Wang, and David Lo",
        "link": "http://arxiv.org/abs/2509.22040v1",
        "abstract": "Agentic AI coding editors driven by large language models have recently\nbecome more popular due to their ability to improve developer productivity\nduring software development. Modern editors such as Cursor are designed not\njust for code completion, but also with more system privileges for complex\ncoding tasks (e.g., run commands in the terminal, access development\nenvironments, and interact with external systems). While this brings us closer\nto the \"fully automated programming\" dream, it also raises new security\nconcerns. In this study, we present the first empirical analysis of prompt\ninjection attacks targeting these high-privilege agentic AI coding editors. We\nshow how attackers can remotely exploit these systems by poisoning external\ndevelopment resources with malicious instructions, effectively hijacking AI\nagents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To\nperform this analysis, we implement AIShellJack, an automated testing framework\nfor assessing prompt injection vulnerabilities in agentic AI coding editors.\nAIShellJack contains 314 unique attack payloads that cover 70 techniques from\nthe MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale\nevaluation on GitHub Copilot and Cursor, and our evaluation results show that\nattack success rates can reach as high as 84% for executing malicious commands.\nMoreover, these attacks are proven effective across a wide range of objectives,\nranging from initial access and system discovery to credential theft and data\nexfiltration."
    },
    {
        "date": "2025-09",
        "title": "Active Attacks: Red-teaming LLMs via Adaptive Environments",
        "author": "Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, and Minsu Kim",
        "link": "http://arxiv.org/abs/2509.21947v1",
        "abstract": "We address the challenge of generating diverse attack prompts for large\nlanguage models (LLMs) that elicit harmful behaviors (e.g., insults, sexual\ncontent) and are used for safety fine-tuning. Rather than relying on manual\nprompt engineering, attacker LLMs can be trained with reinforcement learning\n(RL) to automatically generate such prompts using only a toxicity classifier as\na reward. However, capturing a wide range of harmful behaviors is a significant\nchallenge that requires explicit diversity objectives. Existing\ndiversity-seeking RL methods often collapse to limited modes: once high-reward\nprompts are found, exploration of new regions is discouraged. Inspired by the\nactive learning paradigm that encourages adaptive exploration, we introduce\n\\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its\nattacks as the victim evolves. By periodically safety fine-tuning the victim\nLLM with collected attack prompts, rewards in exploited regions diminish, which\nforces the attacker to seek unexplored vulnerabilities. This process naturally\ninduces an easy-to-hard exploration curriculum, where the attacker progresses\nbeyond easy modes toward increasingly difficult ones. As a result, Active\nAttacks uncovers a wide range of local attack modes step by step, and their\ncombination achieves wide coverage of the multi-mode distribution. Active\nAttacks, a simple plug-and-play module that seamlessly integrates into existing\nRL objectives, unexpectedly outperformed prior RL-based methods -- including\nGFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates\nagainst GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a\nrelative gain greater than $400\\ \\times$) with only a 6% increase in\ncomputation. Our code is publicly available\n\\href{https://github.com/dbsxodud-11/active_attacks}{here}."
    },
    {
        "date": "2025-09",
        "title": "DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling",
        "author": "Ruiqi Chen, Yi Mei, Fangfang Zhang, and Mengjie Zhang",
        "link": "http://arxiv.org/abs/2509.21902v1",
        "abstract": "Dynamic job shop scheduling, a fundamental combinatorial optimisation problem\nin various industrial sectors, poses substantial challenges for effective\nscheduling due to frequent disruptions caused by the arrival of new jobs.\nState-of-the-art methods employ machine learning to learn scheduling policies\noffline, enabling rapid responses to dynamic events. However, these offline\npolicies are often imperfect, necessitating the use of planning techniques such\nas Monte Carlo Tree Search (MCTS) to improve performance at online decision\ntime. The unpredictability of new job arrivals complicates online planning, as\ndecisions based on incomplete problem information are vulnerable to\ndisturbances. To address this issue, we propose the Dynamic Robust MCTS\n(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.\nDyRo-MCTS guides the production environment toward states that not only yield\ngood scheduling outcomes but are also easily adaptable to future job arrivals.\nExtensive experiments show that DyRo-MCTS significantly improves the\nperformance of offline-learned policies with negligible additional online\nplanning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across\nvarious scheduling scenarios. Further analysis reveals that its ability to make\nrobust scheduling decisions leads to long-term, sustainable performance gains\nunder disturbances."
    },
    {
        "date": "2025-09",
        "title": "Zubov-Net: Adaptive Stability for Neural ODEs Reconciling Accuracy with Robustness",
        "author": "Chaoyang Luo, Yan Zou, and Nanjing Huang",
        "link": "http://arxiv.org/abs/2509.21879v1",
        "abstract": "Despite neural ordinary differential equations (Neural ODEs) exhibiting\nintrinsic robustness under input perturbations due to their dynamical systems\nnature, recent approaches often involve imposing Lyapunov-based stability\nconditions to provide formal robustness guarantees. However, a fundamental\nchallenge remains: the tension between robustness and accuracy, primarily\nstemming from the difficulty in imposing appropriate stability conditions. To\naddress this, we propose an adaptive stable learning framework named Zubov-Net,\nwhich innovatively reformulates Zubov's equation into a consistency\ncharacterization between regions of attraction (RoAs) and prescribed RoAs\n(PRoAs). Building on this consistency, we introduce a new paradigm for actively\ncontrolling the geometry of RoAs by directly optimizing PRoAs to reconcile\naccuracy and robustness. Our approach is realized through tripartite losses\n(consistency, classification, and separation losses) and a parallel boundary\nsampling algorithm that co-optimizes the Neural ODE and the Lyapunov function.\nTo enhance the discriminativity of Lyapunov functions, we design an\ninput-attention-based convex neural network via a softmax attention mechanism\nthat focuses on equilibrium-relevant features and also serves as weight\nnormalization to maintain training stability in deep architectures.\nTheoretically, we prove that minimizing the tripartite loss guarantees\nconsistent alignment of PRoAs-RoAs, trajectory stability, and non-overlapping\nPRoAs. Moreover, we establish stochastic convex separability with tighter\nprobability bounds and fewer dimensionality requirements to justify the convex\ndesign in Lyapunov functions. Experimentally, Zubov-Net maintains high\nclassification accuracy while significantly improving robustness against\nvarious stochastic noises and adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models",
        "author": "Jingkai Guo, Chaitali Chakrabarti, and Deliang Fan",
        "link": "http://arxiv.org/abs/2509.21843v1",
        "abstract": "Model integrity of Large language models (LLMs) has become a pressing\nsecurity concern with their massive online deployment. Prior Bit-Flip Attacks\n(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can\nseverely compromise Deep Neural Networks (DNNs): as few as tens of bit flips\ncan degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs\nand reveal that, despite the intuition of better robustness from modularity and\nredundancy, only a handful of adversarial bit flips can also cause LLMs'\ncatastrophic accuracy degradation. However, existing BFA methods typically\nfocus on either integer or floating-point models separately, limiting attack\nflexibility. Moreover, in floating-point models, random bit flips often cause\nperturbed parameters to extreme values (e.g., flipping in exponent bit), making\nit not stealthy and leading to numerical runtime error (e.g., invalid tensor\nvalues (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky\nBit-Flip Attack), which collapses LLM performance with only one single bit flip\nwhile keeping perturbed values within benign layer-wise weight distribution. It\nis achieved through iterative searching and ranking through our defined\nparameter sensitivity metric, ImpactScore, which combines gradient sensitivity\nand perturbation range constrained by the benign layer-wise weight\ndistribution. A novel lightweight SKIP searching algorithm is also proposed to\ngreatly reduce searching complexity, which leads to successful SBFA searching\ntaking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma\nmodels, with only one single bit flip, SBFA successfully degrades accuracy to\nbelow random levels on MMLU and SST-2 in both BF16 and INT8 data formats.\nRemarkably, flipping a single bit out of billions of parameters reveals a\nsevere security concern of SOTA LLM models."
    },
    {
        "date": "2025-09",
        "title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety",
        "author": "Junliang Liu, Jingyu Xiao, Wenxin Tang, Wenxuan Wang, Zhixian Wang, Minrui Zhang, and Shuanghe Yu",
        "link": "http://arxiv.org/abs/2509.21782v1",
        "abstract": "Multimodal large language models (MLLMs) are increasingly positioned as AI\ncollaborators for building complex web-related applications like GUI agents and\nfront-end code generation. However, existing benchmarks largely emphasize\nvisual perception or UI code generation, showing insufficient evaluation on the\nreasoning, robustness and safety capability required for end-to-end web\napplications. To bridge the gap, we introduce a comprehensive web understanding\nbenchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and\nSafety across eight tasks, such as position relationship reasoning, color\nrobustness, and safety critical detection, etc. The benchmark is constructed\nfrom 729 websites and contains 3799 question answer pairs that probe multi-step\ninference over page structure, text, widgets, and safety-critical interactions.\nTo ensure reliable measurement, we adopt standardized prompts, deterministic\nevaluation scripts, and multi-stage quality control combining automatic checks\nwith targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The\nresults reveal significant gaps, models still struggle with compositional and\ncross-element reasoning over realistic layouts, show limited robustness when\nfacing perturbations in user interfaces and content such as layout\nrearrangements or visual style shifts, and are rather conservative in\nrecognizing and avoiding safety critical or irreversible actions. Our code is\navailable at https://github.com/jinliang-byte/webssrbench."
    },
    {
        "date": "2025-09",
        "title": "Learning What To Hear: Boosting Sound-Source Association For Robust Audiovisual Instance Segmentation",
        "author": "Jinbae Seo, Hyeongjun Kwon, Kwonyoung Kim, Jiyoung Lee, and Kwanghoon Sohn",
        "link": "http://arxiv.org/abs/2509.22740v1",
        "abstract": "Audiovisual instance segmentation (AVIS) requires accurately localizing and\ntracking sounding objects throughout video sequences. Existing methods suffer\nfrom visual bias stemming from two fundamental issues: uniform additive fusion\nprevents queries from specializing to different sound sources, while\nvisual-only training objectives allow queries to converge to arbitrary salient\nobjects. We propose Audio-Centric Query Generation using cross-attention,\nenabling each query to selectively attend to distinct sound sources and carry\nsound-specific priors into visual decoding. Additionally, we introduce\nSound-Aware Ordinal Counting (SAOC) loss that explicitly supervises sounding\nobject numbers through ordinal regression with monotonic consistency\nconstraints, preventing visual-only convergence during training. Experiments on\nAVISeg benchmark demonstrate consistent improvements: +1.64 mAP, +0.6 HOTA, and\n+2.06 FSLA, validating that query specialization and explicit counting\nsupervision are crucial for accurate audiovisual instance segmentation."
    },
    {
        "date": "2025-09",
        "title": "TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning",
        "author": "Hongyang He, Xinyuan Song, Yangfan He, Zeyu Zhang, Yanshu Li, Haochen You, Lifan Sun, and Wenqiao Zhang",
        "link": "http://arxiv.org/abs/2509.21526v1",
        "abstract": "We introduce TRiCo, a novel triadic game-theoretic co-training framework that\nrethinks the structure of semi-supervised learning by incorporating a teacher,\ntwo students, and an adversarial generator into a unified training paradigm.\nUnlike existing co-training or teacher-student approaches, TRiCo formulates SSL\nas a structured interaction among three roles: (i) two student classifiers\ntrained on frozen, complementary representations, (ii) a meta-learned teacher\nthat adaptively regulates pseudo-label selection and loss balancing via\nvalidation-based feedback, and (iii) a non-parametric generator that perturbs\nembeddings to uncover decision boundary weaknesses. Pseudo-labels are selected\nbased on mutual information rather than confidence, providing a more robust\nmeasure of epistemic uncertainty. This triadic interaction is formalized as a\nStackelberg game, where the teacher leads strategy optimization and students\nfollow under adversarial perturbations. By addressing key limitations in\nexisting SSL frameworks, such as static view interactions, unreliable\npseudo-labels, and lack of hard sample modeling, TRiCo provides a principled\nand generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,\nand ImageNet demonstrate that TRiCo consistently achieves state-of-the-art\nperformance in low-label regimes, while remaining architecture-agnostic and\ncompatible with frozen vision backbones."
    },
    {
        "date": "2025-09",
        "title": "Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations",
        "author": "Micha Livne",
        "link": "http://arxiv.org/abs/2509.21511v1",
        "abstract": "Learning representations that transfer well to diverse downstream tasks\nremains a central challenge in representation learning. Existing paradigms --\ncontrastive learning, self-supervised masking, and denoising auto-encoders --\nbalance this challenge with different trade-offs. We introduce the {contrastive\nMutual Information Machine} (cMIM), a probabilistic framework that extends the\nMutual Information Machine (MIM) with a contrastive objective. While MIM\nmaximizes mutual information between inputs and latents and promotes clustering\nof codes, it falls short on discriminative tasks. cMIM addresses this gap by\nimposing global discriminative structure while retaining MIM's generative\nfidelity. Our contributions are threefold. First, we propose cMIM, a\ncontrastive extension of MIM that removes the need for positive data\naugmentation and is substantially less sensitive to batch size than InfoNCE.\nSecond, we introduce {informative embeddings}, a general technique for\nextracting enriched features from encoder-decoder models that boosts\ndiscriminative performance without additional training and applies broadly\nbeyond MIM. Third, we provide empirical evidence across vision and molecular\nbenchmarks showing that cMIM outperforms MIM and InfoNCE on classification and\nregression tasks while preserving competitive reconstruction quality. These\nresults position cMIM as a unified framework for representation learning,\nadvancing the goal of models that serve both discriminative and generative\napplications effectively."
    },
    {
        "date": "2025-09",
        "title": "Functional Encryption in Secure Neural Network Training: Data Leakage and Practical Mitigations",
        "author": "Alexandru Ioni\u0163\u0103, and Andreea Ioni\u0163\u0103",
        "link": "http://arxiv.org/abs/2509.21497v1",
        "abstract": "With the increased interest in artificial intelligence, Machine Learning as a\nService provides the infrastructure in the Cloud for easy training, testing,\nand deploying models. However, these systems have a major privacy issue:\nuploading sensitive data to the Cloud, especially during training. Therefore,\nachieving secure Neural Network training has been on many researchers' minds\nlately. More and more solutions for this problem are built around a main\npillar: Functional Encryption (FE). Although these approaches are very\ninteresting and offer a new perspective on ML training over encrypted data,\nsome vulnerabilities do not seem to be taken into consideration. In our paper,\nwe present an attack on neural networks that uses FE for secure training over\nencrypted data. Our approach uses linear programming to reconstruct the\noriginal input, unveiling the previous security promises. To address the\nattack, we propose two solutions for secure training and inference that involve\nthe client during the computation phase. One approach ensures security without\nrelying on encryption, while the other uses function-hiding inner-product\ntechniques."
    },
    {
        "date": "2025-09",
        "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
        "author": "Yehonatan Refael, Guy Smorodinsky, Ofir Lindenbaum, and Itay Safran",
        "link": "http://arxiv.org/abs/2509.21296v1",
        "abstract": "The memorization of training data by neural networks raises pressing concerns\nfor privacy and security. Recent work has shown that, under certain conditions,\nportions of the training set can be reconstructed directly from model\nparameters. Some of these methods exploit implicit bias toward margin\nmaximization, suggesting that properties often regarded as beneficial for\ngeneralization may actually compromise privacy. Yet despite striking empirical\ndemonstrations, the reliability of these attacks remains poorly understood and\nlacks a solid theoretical foundation. In this work, we take a complementary\nperspective: rather than designing stronger attacks, we analyze the inherent\nweaknesses and limitations of existing reconstruction methods and identify\nconditions under which they fail. We rigorously prove that, without\nincorporating prior knowledge about the data, there exist infinitely many\nalternative solutions that may lie arbitrarily far from the true training set,\nrendering reconstruction fundamentally unreliable. Empirically, we further\ndemonstrate that exact duplication of training examples occurs only by chance.\nOur results refine the theoretical understanding of when training set leakage\nis possible and offer new insights into mitigating reconstruction attacks.\nRemarkably, we demonstrate that networks trained more extensively, and\ntherefore satisfying implicit bias conditions more strongly -- are, in fact,\nless susceptible to reconstruction attacks, reconciling privacy with the need\nfor strong generalization in this setting."
    },
    {
        "date": "2025-09",
        "title": "Optimal Robust Recourse with $L^p$-Bounded Model Change",
        "author": "Phone Kyaw, Kshitij Kayastha, and Shahin Jabbari",
        "link": "http://arxiv.org/abs/2509.21293v1",
        "abstract": "Recourse provides individuals who received undesirable labels (e.g., denied a\nloan) from algorithmic decision-making systems with a minimum-cost improvement\nsuggestion to achieve the desired outcome. However, in practice, models often\nget updated to reflect changes in the data distribution or environment,\ninvalidating the recourse recommendations (i.e., following the recourse will\nnot lead to the desirable outcome). The robust recourse literature addresses\nthis issue by providing a framework for computing recourses whose validity is\nresilient to slight changes in the model. However, since the optimization\nproblem of computing robust recourse is non-convex (even for linear models),\nmost of the current approaches do not have any theoretical guarantee on the\noptimality of the recourse. Recent work by Kayastha et. al. provides the first\nprovably optimal algorithm for robust recourse with respect to generalized\nlinear models when the model changes are measured using the $L^{\\infty}$ norm.\nHowever, using the $L^{\\infty}$ norm can lead to recourse solutions with a high\nprice. To address this shortcoming, we consider more constrained model changes\ndefined by the $L^p$ norm, where $p\\geq 1$ but $p\\neq \\infty$, and provide a\nnew algorithm that provably computes the optimal robust recourse for\ngeneralized linear models. Empirically, for both linear and non-linear models,\nwe demonstrate that our algorithm achieves a significantly lower price of\nrecourse (up to several orders of magnitude) compared to prior work and also\nexhibits a better trade-off between the implementation cost of recourse and its\nvalidity. Our empirical analysis also illustrates that our approach provides\nmore sparse recourses compared to prior work and remains resilient to\npost-processing approaches that guarantee feasibility."
    },
    {
        "date": "2025-09",
        "title": "Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization",
        "author": "Feng-Qi Cui, Jinyang Huang, Anyang Tong, Ziyu Jia, Jie Zhang, Zhi Liu, Dan Guo, Jianwei Lu, and Meng Wang",
        "link": "http://arxiv.org/abs/2509.21261v2",
        "abstract": "Micro-action Recognition is vital for psychological assessment and\nhuman-computer interaction. However, existing methods often fail in real-world\nscenarios because inter-person variability causes the same action to manifest\ndifferently, hindering robust generalization. To address this, we propose the\nPerson Independence Universal Micro-action Recognition Framework, which\nintegrates Distributionally Robust Optimization principles to learn\nperson-agnostic representations. Our framework contains two plug-and-play\ncomponents operating at the feature and loss levels. At the feature level, the\nTemporal-Frequency Alignment Module normalizes person-specific motion\ncharacteristics with a dual-branch design: the temporal branch applies\nWasserstein-regularized alignment to stabilize dynamic trajectories, while the\nfrequency branch introduces variance-guided perturbations to enhance robustness\nagainst person-specific spectral differences. A consistency-driven fusion\nmechanism integrates both branches. At the loss level, the Group-Invariant\nRegularized Loss partitions samples into pseudo-groups to simulate unseen\nperson-specific distributions. By up-weighting boundary cases and regularizing\nsubgroup variance, it forces the model to generalize beyond easy or frequent\nsamples, thus enhancing robustness to difficult variations. Experiments on the\nlarge-scale MA-52 dataset demonstrate that our framework outperforms existing\nmethods in both accuracy and robustness, achieving stable generalization under\nfine-grained conditions."
    },
    {
        "date": "2025-09",
        "title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding",
        "author": "I\u00f1igo Alonso, Imanol Miranda, Eneko Agirre, and Mirella Lapata",
        "link": "http://arxiv.org/abs/2509.21205v1",
        "abstract": "While table understanding increasingly relies on pixel-only settings where\ntables are processed as visual representations, current benchmarks\npredominantly use synthetic renderings that lack the complexity and visual\ndiversity of real-world tables. Additionally, existing visual table\nunderstanding (VTU) datasets offer fixed examples with single visualizations\nand pre-defined instructions, providing no access to underlying serialized data\nfor reformulation. We introduce TABLET, a large-scale VTU dataset with 4\nmillion examples across 20 tasks, grounded in 2 million unique tables where 88%\npreserve original visualizations. Each example includes paired image-HTML\nrepresentations, comprehensive metadata, and provenance information linking\nback to the source datasets. Fine-tuning vision-language models like\nQwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while\nincreasing robustness on real-world table visualizations. By preserving\noriginal visualizations and maintaining example traceability in a unified\nlarge-scale collection, TABLET establishes a foundation for robust training and\nextensible evaluation of future VTU models."
    },
    {
        "date": "2025-09",
        "title": "Emerging Paradigms for Securing Federated Learning Systems",
        "author": "Amr Akmal Abouelmagd, and Amr Hilal",
        "link": "http://arxiv.org/abs/2509.21147v1",
        "abstract": "Federated Learning (FL) facilitates collaborative model training while\nkeeping raw data decentralized, making it a conduit for leveraging the power of\nIoT devices while maintaining privacy of the locally collected data. However,\nexisting privacy- preserving techniques present notable hurdles. Methods such\nas Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential\nPrivacy (DP) often incur high compu- tational costs and suffer from limited\nscalability. This survey examines emerging approaches that hold promise for\nenhancing both privacy and efficiency in FL, including Trusted Execution\nEnvironments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing\n(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm\nIntelligence (SI). For each paradigm, we assess its relevance to the FL\npipeline, outlining its strengths, limitations, and practical considerations.\nWe conclude by highlighting open challenges and prospective research avenues,\noffering a detailed roadmap for advancing secure and scalable FL systems."
    },
    {
        "date": "2025-09",
        "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks",
        "author": "Haibo Tong, Dongcheng Zhao, Guobin Shen, Xiang He, Dachuan Lin, Feifei Zhao, and Yi Zeng",
        "link": "http://arxiv.org/abs/2509.22732v1",
        "abstract": "The remarkable capabilities of Large Language Models (LLMs) have raised\nsignificant safety concerns, particularly regarding \"jailbreak\" attacks that\nexploit adversarial prompts to bypass safety alignment mechanisms. Existing\ndefense research primarily focuses on single-turn attacks, whereas multi-turn\njailbreak attacks progressively break through safeguards through by concealing\nmalicious intent and tactical manipulation, ultimately rendering conventional\nsingle-turn defenses ineffective. To address this critical challenge, we\npropose the Bidirectional Intention Inference Defense (BIID). The method\nintegrates forward request-based intention inference with backward\nresponse-based intention retrospection, establishing a bidirectional synergy\nmechanism to detect risks concealed within seemingly benign inputs, thereby\nconstructing a more robust guardrails that effectively prevents harmful content\ngeneration. The proposed method undergoes systematic evaluation compared with a\nno-defense baseline and seven representative defense methods across three LLMs\nand two safety benchmarks under 10 different attack methods. Experimental\nresults demonstrate that the proposed method significantly reduces the Attack\nSuccess Rate (ASR) across both single-turn and multi-turn jailbreak attempts,\noutperforming all existing baseline methods while effectively maintaining\npractical utility. Notably, comparative experiments across three multi-turn\nsafety datasets further validate the proposed model's significant advantages\nover other defense approaches."
    },
    {
        "date": "2025-09",
        "title": "Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers",
        "author": "Killian Steunou, Sigurd Saue, and Th\u00e9o Druilhe",
        "link": "http://arxiv.org/abs/2509.21130v1",
        "abstract": "Deep neural networks perform remarkably well on image classification tasks\nbut remain vulnerable to carefully crafted adversarial perturbations. This work\nrevisits linear dimensionality reduction as a simple, data-adapted defense. We\nempirically compare standard Principal Component Analysis (PCA) with its sparse\nvariant (SPCA) as front-end feature extractors for downstream classifiers, and\nwe complement these experiments with a theoretical analysis. On the theory\nside, we derive exact robustness certificates for linear heads applied to SPCA\nfeatures: for both $\\ell_\\infty$ and $\\ell_2$ threat models (binary and\nmulticlass), the certified radius grows as the dual norms of $W^\\top u$ shrink,\nwhere $W$ is the projection and $u$ the head weights. We further show that for\ngeneral (non-linear) heads, sparsity reduces operator-norm bounds through a\nLipschitz composition argument, predicting lower input sensitivity.\nEmpirically, with a small non-linear network after the projection, SPCA\nconsistently degrades more gracefully than PCA under strong white-box and\nblack-box attacks while maintaining competitive clean accuracy. Taken together,\nthe theory identifies the mechanism (sparser projections reduce adversarial\nleverage) and the experiments verify that this benefit persists beyond the\nlinear setting. Our code is available at\nhttps://github.com/killian31/SPCARobustness."
    },
    {
        "date": "2025-09",
        "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense",
        "author": "Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, and Hai-Feng Lin",
        "link": "http://arxiv.org/abs/2509.21129v1",
        "abstract": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."
    },
    {
        "date": "2025-09",
        "title": "Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?",
        "author": "Rostislav Makarov, Lea Sch\u00f6nherr, and Timo Gerkmann",
        "link": "http://arxiv.org/abs/2509.21087v1",
        "abstract": "Machine learning approaches for speech enhancement are becoming increasingly\nexpressive, enabling ever more powerful modifications of input signals. In this\npaper, we demonstrate that this expressiveness introduces a vulnerability:\nadvanced speech enhancement models can be susceptible to adversarial attacks.\nSpecifically, we show that adversarial noise, carefully crafted and\npsychoacoustically masked by the original input, can be injected such that the\nenhanced speech output conveys an entirely different semantic meaning. We\nexperimentally verify that contemporary predictive speech enhancement models\ncan indeed be manipulated in this way. Furthermore, we highlight that diffusion\nmodels with stochastic samplers exhibit inherent robustness to such adversarial\nattacks by design."
    },
    {
        "date": "2025-09",
        "title": "Vision Transformers: the threat of realistic adversarial patches",
        "author": "Kasper Cools, Clara Maathuis, Alexander M. van Oers, Claudia S. H\u00fcbner, Nikos Deligiannis, Marijke Vandewal, and Geert De Cubber",
        "link": "http://arxiv.org/abs/2509.21084v1",
        "abstract": "The increasing reliance on machine learning systems has made their security a\ncritical concern. Evasion attacks enable adversaries to manipulate the\ndecision-making processes of AI systems, potentially causing security breaches\nor misclassification of targets. Vision Transformers (ViTs) have gained\nsignificant traction in modern machine learning due to increased 1) performance\ncompared to Convolutional Neural Networks (CNNs) and 2) robustness against\nadversarial perturbations. However, ViTs remain vulnerable to evasion attacks,\nparticularly to adversarial patches, unique patterns designed to manipulate AI\nclassification systems. These vulnerabilities are investigated by designing\nrealistic adversarial patches to cause misclassification in person vs.\nnon-person classification tasks using the Creases Transformation (CT)\ntechnique, which adds subtle geometric distortions similar to those occurring\nnaturally when wearing clothing. This study investigates the transferability of\nadversarial attack techniques used in CNNs when applied to ViT classification\nmodels. Experimental evaluation across four fine-tuned ViT models on a binary\nperson classification task reveals significant vulnerability variations: attack\nsuccess rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%\n(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and\nfacebook/dinov3-vitb16 reaching 65.17%. These results confirm the\ncross-architectural transferability of adversarial patches from CNNs to ViTs,\nwith pre-training dataset scale and methodology strongly influencing model\nresilience to adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints",
        "author": "Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, and Mingxun Zhou",
        "link": "http://arxiv.org/abs/2509.21057v1",
        "abstract": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."
    },
    {
        "date": "2025-09",
        "title": "FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction",
        "author": "Runqi Lin, Alasdair Paren, Suqin Yuan, Muyang Li, Philip Torr, Adel Bibi, and Tongliang Liu",
        "link": "http://arxiv.org/abs/2509.21029v2",
        "abstract": "The integration of new modalities enhances the capabilities of multimodal\nlarge language models (MLLMs) but also introduces additional vulnerabilities.\nIn particular, simple visual jailbreaking attacks can manipulate open-source\nMLLMs more readily than sophisticated textual attacks. However, these\nunderdeveloped attacks exhibit extremely limited cross-model transferability,\nfailing to reliably identify vulnerabilities in closed-source MLLMs. In this\nwork, we analyse the loss landscape of these jailbreaking attacks and find that\nthe generated attacks tend to reside in high-sharpness regions, whose\neffectiveness is highly sensitive to even minor parameter changes during\ntransfer. To further explain the high-sharpness localisations, we analyse their\nfeature representations in both the intermediate layers and the spectral\ndomain, revealing an improper reliance on narrow layer representations and\nsemantically poor frequency components. Building on this, we propose a Feature\nOver-Reliance CorrEction (FORCE) method, which guides the attack to explore\nbroader feasible regions across layer features and rescales the influence of\nfrequency features according to their semantic content. By eliminating\nnon-generalizable reliance on both layer and spectral features, our method\ndiscovers flattened feasible regions for visual jailbreaking attacks, thereby\nimproving cross-model transferability. Extensive experiments demonstrate that\nour approach effectively facilitates visual red-teaming evaluations against\nclosed-source MLLMs."
    },
    {
        "date": "2025-09",
        "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
        "author": "Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, and Shuiguang Deng",
        "link": "http://arxiv.org/abs/2509.20979v1",
        "abstract": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
    },
    {
        "date": "2025-09",
        "title": "Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models",
        "author": "Bum Jun Kim, Makoto Kawano, Yusuke Iwasawa, and Yutaka Matsuo",
        "link": "http://arxiv.org/abs/2509.20939v1",
        "abstract": "While the robustness of vision models is often measured, their dependence on\nspecific architectural design choices is rarely dissected. We investigate why\ncertain vision architectures are inherently more robust to additive Gaussian\nnoise and convert these empirical insights into simple, actionable design\nrules. Specifically, we performed extensive evaluations on 1,174 pretrained\nvision models, empirically identifying four consistent design patterns for\nimproved robustness against Gaussian noise: larger stem kernels, smaller input\nresolutions, average pooling, and supervised vision transformers (ViTs) rather\nthan CLIP ViTs, which yield up to 506 rank improvements and 21.6\\%p accuracy\ngains. We then develop a theoretical analysis that explains these findings,\nconverting observed correlations into causal mechanisms. First, we prove that\nlow-pass stem kernels attenuate noise with a gain that decreases quadratically\nwith kernel size and that anti-aliased downsampling reduces noise energy\nroughly in proportion to the square of the downsampling factor. Second, we\ndemonstrate that average pooling is unbiased and suppresses noise in proportion\nto the pooling window area, whereas max pooling incurs a positive bias that\ngrows slowly with window size and yields a relatively higher mean-squared error\nand greater worst-case sensitivity. Third, we reveal and explain the\nvulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller\nnormalization standard deviations used in CLIP preprocessing amplify worst-case\nsensitivity by up to 1.91 times relative to the Inception-style preprocessing\ncommon in supervised ViTs. Our results collectively disentangle robustness into\ninterpretable modules, provide a theory that explains the observed trends, and\nbuild practical, plug-and-play guidelines for designing vision models more\nrobust against Gaussian noise."
    },
    {
        "date": "2025-09",
        "title": "RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks",
        "author": "Hanbo Huang, Yiran Zhang, Hao Zheng, Xuan Gong, Yihan Li, Lin Liu, and Shiyu Liang",
        "link": "http://arxiv.org/abs/2509.20924v1",
        "abstract": "Large Language Models (LLMs) watermarking has shown promise in detecting\nAI-generated content and mitigating misuse, with prior work claiming robustness\nagainst paraphrasing and text editing. In this paper, we argue that existing\nevaluations are not sufficiently adversarial, obscuring critical\nvulnerabilities and overstating the security. To address this, we introduce\nadaptive robustness radius, a formal metric that quantifies watermark\nresilience against adaptive adversaries. We theoretically prove that optimizing\nthe attack context and model parameters can substantially reduce this radius,\nmaking watermarks highly susceptible to paraphrase attacks. Leveraging this\ninsight, we propose RLCracker, a reinforcement learning (RL)-based adaptive\nattack that erases watermarks while preserving semantic fidelity. RLCracker\nrequires only limited watermarked examples and zero access to the detector.\nDespite weak supervision, it empowers a 3B model to achieve 98.5% removal\nsuccess and an average 0.92 P-SP score on 1,500-token Unigram-marked texts\nafter training on only 100 short samples. This performance dramatically exceeds\n6.75% by GPT-4o and generalizes across five model sizes over ten watermarking\nschemes. Our results confirm that adaptive attacks are broadly effective and\npose a fundamental threat to current watermarking defenses."
    },
    {
        "date": "2025-09",
        "title": "Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease",
        "author": "Sungjoon Park, Kyungwook Lee, Soorin Yim, Doyeong Hwang, Dongyun Kim, Soonyoung Lee, Amy Dunn, Daniel Gatti, Elissa Chesler, Kristen O'Connell, and Kiyoung Kim",
        "link": "http://arxiv.org/abs/2509.20842v1",
        "abstract": "Multi-omics data capture complex biomolecular interactions and provide\ninsights into metabolism and disease. However, missing modalities hinder\nintegrative analysis across heterogeneous omics. To address this, we present\nMOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early\nintegration method enabling robust learning from incomplete omics data via\nrepresentation alignment and adaptive aggregation. MOIRA leverages all samples,\nincluding those with missing modalities, by projecting each omics dataset onto\na shared embedding space where a learnable weighting mechanism fuses them.\nEvaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)\ndataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,\nand further ablation studies confirmed modality-wise contributions. Feature\nimportance analysis revealed AD-related biomarkers consistent with prior\nliterature, highlighting the biological relevance of our approach."
    },
    {
        "date": "2025-09",
        "title": "Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks",
        "author": "Yu Liu, Boxiang He, and Fanggang Wang",
        "link": "http://arxiv.org/abs/2509.20835v1",
        "abstract": "This paper proposes a novel and flexible security-aware semantic-driven\nintegrated sensing and communication (ISAC) framework, namely security semantic\nISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a\npair of pluggable encryption and decryption modules is designed in the proposed\nSS-ISAC framework. The encryption module is installed after the semantic\ntransmitter, adopting a trainable adversarial residual network (ARN) to create\nthe adversarial attack. Correspondingly, the decryption module before the\nsemantic receiver utilizes another trainable ARN to mitigate the adversarial\nattack and noise. These two modules can be flexibly assembled considering the\nsystem security demands, without drastically modifying the hardware\ninfrastructure. To ensure the sensing and communication (SAC) performance while\npreventing the eavesdropping threat, the above ARNs are jointly optimized by\nminimizing a carefully designed loss function that relates to the adversarial\nattack power, SAC performance, as well as the privacy leakage risk. Simulation\nresults validate the effectiveness of the proposed SS-ISAC framework in terms\nof both SAC and eavesdropping prevention performance."
    },
    {
        "date": "2025-09",
        "title": "FERD: Fairness-Enhanced Data-Free Robustness Distillation",
        "author": "Zhengxiao Li, Liming Lu, Xu Zheng, Siyuan Liang, Zhenghan Chen, Yongbin Zhou, and Shuchao Pang",
        "link": "http://arxiv.org/abs/2509.20793v2",
        "abstract": "Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from\nthe teacher to the student without accessing the training data. While existing\nmethods focus on overall robustness, they overlook the robust fairness issues,\nleading to severe disparity of robustness across different categories. In this\npaper, we find two key problems: (1) student model distilled with equal class\nproportion data behaves significantly different across distinct categories; and\n(2) the robustness of student model is not stable across different attacks\ntarget. To bridge these gaps, we present the first Fairness-Enhanced data-free\nRobustness Distillation (FERD) framework to adjust the proportion and\ndistribution of adversarial examples. For the proportion, FERD adopts a\nrobustness-guided class reweighting strategy to synthesize more samples for the\nless robust categories, thereby improving robustness of them. For the\ndistribution, FERD generates complementary data samples for advanced robustness\ndistillation. It generates Fairness-Aware Examples (FAEs) by enforcing a\nuniformity constraint on feature-level predictions, which suppress the\ndominance of class-specific non-robust features, providing a more balanced\nrepresentation across all categories. Then, FERD constructs Uniform-Target\nAdversarial Examples (UTAEs) from FAEs by applying a uniform target class\nconstraint to avoid biased attack directions, which distribute the attack\ntargets across all categories and prevents overfitting to specific vulnerable\ncategories. Extensive experiments on three public datasets show that FERD\nachieves state-of-the-art worst-class robustness under all adversarial attack\n(e.g., the worst-class robustness under FGSM and AutoAttack are improved by\n15.1\\% and 6.4\\% using MobileNet-V2 on CIFAR-10), demonstrating superior\nperformance in both robustness and fairness aspects."
    },
    {
        "date": "2025-09",
        "title": "DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation",
        "author": "Ved Umrajkar",
        "link": "http://arxiv.org/abs/2509.20792v1",
        "abstract": "Vision-Language Models (VLMs) are foundational to critical applications like\nautonomous driving, medical diagnosis, and content moderation. While\nParameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient\nadaptation to specialized tasks, these models remain vulnerable to adversarial\nattacks that can compromise safety-critical decisions. CLIP, the backbone for\nnumerous downstream VLMs, is a high-value target whose vulnerabilities can\ncascade across the multimodal AI ecosystem. We propose Dynamic Adversarial\nCurriculum DAC-LoRA, a novel framework that integrates adversarial training\ninto PEFT. The core principle of our method i.e. an intelligent curriculum of\nprogressively challenging attack, is general and can potentially be applied to\nany iterative attack method. Guided by the First-Order Stationary Condition\n(FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements\nin adversarial robustness without significantly compromising clean accuracy.\nOur work presents an effective, lightweight, and broadly applicable method to\ndemonstrate that the DAC-LoRA framework can be easily integrated into a\nstandard PEFT pipeline to significantly enhance robustness."
    },
    {
        "date": "2025-09",
        "title": "RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking",
        "author": "Zhenguo Sun, Yibo Peng, Yuan Meng, Xukun Li, Bo-Sheng Huang, Zhenshan Bing, Xinlong Wang, and Alois Knoll",
        "link": "http://arxiv.org/abs/2509.20717v1",
        "abstract": "Long-horizon, high-dynamic motion tracking on humanoids remains brittle\nbecause absolute joint commands cannot compensate model-plant mismatch, leading\nto error accumulation. We propose RobotDancing, a simple, scalable framework\nthat predicts residual joint targets to explicitly correct dynamics\ndiscrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and\nzero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)\nsetup with a unified observation, reward, and hyperparameter configuration. We\nevaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and\nvalidate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy\nbehaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with\nhigh motion tracking quality."
    },
    {
        "date": "2025-09",
        "title": "Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection",
        "author": "Duc-Tuan Truong, Tianchi Liu, Junjie Li, Ruijie Tao, Kong Aik Lee, and Eng Siong Chng",
        "link": "http://arxiv.org/abs/2509.20682v1",
        "abstract": "In speech deepfake detection (SDD), data augmentation (DA) is commonly used\nto improve model generalization across varied speech conditions and spoofing\nattacks. However, during training, the backpropagated gradients from original\nand augmented inputs may misalign, which can result in conflicting parameter\nupdates. These conflicts could hinder convergence and push the model toward\nsuboptimal solutions, thereby reducing the benefits of DA. To investigate and\naddress this issue, we design a dual-path data-augmented (DPDA) training\nframework with gradient alignment for SDD. In our framework, each training\nutterance is processed through two input paths: one using the original speech\nand the other with its augmented version. This design allows us to compare and\nalign their backpropagated gradient directions to reduce optimization\nconflicts. Our analysis shows that approximately 25% of training iterations\nexhibit gradient conflicts between the original inputs and their augmented\ncounterparts when using RawBoost augmentation. By resolving these conflicts\nwith gradient alignment, our method accelerates convergence by reducing the\nnumber of training epochs and achieves up to an 18.69% relative reduction in\nEqual Error Rate on the In-the-Wild dataset compared to the baseline."
    },
    {
        "date": "2025-09",
        "title": "Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation",
        "author": "Wenkai Guo, Xuefeng Liu, Haolin Wang, Jianwei Niu, Shaojie Tang, and Jing Yuan",
        "link": "http://arxiv.org/abs/2509.20680v1",
        "abstract": "Fine-tuning large language models (LLMs) with local data is a widely adopted\napproach for organizations seeking to adapt LLMs to their specific domains.\nGiven the shared characteristics in data across different organizations, the\nidea of collaboratively fine-tuning an LLM using data from multiple sources\npresents an appealing opportunity. However, organizations are often reluctant\nto share local data, making centralized fine-tuning impractical. Federated\nlearning (FL), a privacy-preserving framework, enables clients to retain local\ndata while sharing only model parameters for collaborative training, offering a\npotential solution. While fine-tuning LLMs on centralized datasets risks data\nleakage through next-token prediction, the iterative aggregation process in FL\nresults in a global model that encapsulates generalized knowledge, which some\nbelieve protects client privacy. In this paper, however, we present\ncontradictory findings through extensive experiments. We show that attackers\ncan still extract training data from the global model, even using\nstraightforward generation methods, with leakage increasing as the model size\ngrows. Moreover, we introduce an enhanced attack strategy tailored to FL, which\ntracks global model updates during training to intensify privacy leakage. To\nmitigate these risks, we evaluate privacy-preserving techniques in FL,\nincluding differential privacy, regularization-constrained updates and adopting\nLLMs with safety alignment. Our results provide valuable insights and practical\nguidelines for reducing privacy risks when training LLMs with FL."
    },
    {
        "date": "2025-09",
        "title": "A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks",
        "author": "Adam Swanda, Amy Chang, Alexander Chen, Fraser Burch, Paul Kassianik, and Konstantin Berlin",
        "link": "http://arxiv.org/abs/2509.20639v1",
        "abstract": "The widespread adoption of Large Language Models (LLMs) has revolutionized AI\ndeployment, enabling autonomous and semi-autonomous applications across\nindustries through intuitive language interfaces and continuous improvements in\nmodel development. However, the attendant increase in autonomy and expansion of\naccess permissions among AI applications also make these systems compelling\ntargets for malicious attacks. Their inherent susceptibility to security flaws\nnecessitates robust defenses, yet no known approaches can prevent zero-day or\nnovel attacks against LLMs. This places AI protection systems in a category\nsimilar to established malware protection systems: rather than providing\nguaranteed immunity, they minimize risk through enhanced observability,\nmulti-layered defense, and rapid threat response, supported by a threat\nintelligence function designed specifically for AI-related threats.\n  Prior work on LLM protection has largely evaluated individual detection\nmodels rather than end-to-end systems designed for continuous, rapid adaptation\nto a changing threat landscape. We present a production-grade defense system\nrooted in established malware detection and threat intelligence practices. Our\nplatform integrates three components: a threat intelligence system that turns\nemerging threats into protections; a data platform that aggregates and enriches\ninformation while providing observability, monitoring, and ML operations; and a\nrelease platform enabling safe, rapid detection updates without disrupting\ncustomer workflows. Together, these components deliver layered protection\nagainst evolving LLM threats while generating training data for continuous\nmodel improvement and deploying updates without interrupting production."
    },
    {
        "date": "2025-09",
        "title": "Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities",
        "author": "Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, and Subhadeep Paul",
        "link": "http://arxiv.org/abs/2509.20634v1",
        "abstract": "We find AI embeddings obtained using a pre-trained transformer-based Large\nLanguage Model (LLM) of 80,000-120,000 written affirmations and correction\nexchanges among residents in low-security correctional facilities to be highly\npredictive of recidivism. The prediction accuracy is 30\\% higher with embedding\nvectors than with only pre-entry covariates. However, since the text embedding\nvectors are high-dimensional, we perform Zero-Shot classification of these\ntexts to a low-dimensional vector of user-defined classes to aid interpretation\nwhile retaining the predictive power. To shed light on the social dynamics\ninside the correctional facilities, we estimate peer effects in these\nLLM-generated numerical representations of language with a multivariate peer\neffect model, adjusting for network endogeneity. We develop new methodology and\ntheory for peer effect estimation that accommodate sparse networks,\nmultivariate latent variables, and correlated multivariate outcomes. With these\nnew methods, we find significant peer effects in language usage for interaction\nand feedback."
    },
    {
        "date": "2025-09",
        "title": "Every Character Counts: From Vulnerability to Defense in Phishing Detection",
        "author": "Maria Chiper, and Radu Tudor Ionescu",
        "link": "http://arxiv.org/abs/2509.20589v1",
        "abstract": "Phishing attacks targeting both organizations and individuals are becoming an\nincreasingly significant threat as technology advances. Current automatic\ndetection methods often lack explainability and robustness in detecting new\nphishing attacks. In this work, we investigate the effectiveness of\ncharacter-level deep learning models for phishing detection, which can provide\nboth robustness and interpretability. We evaluate three neural architectures\nadapted to operate at the character level, namely CharCNN, CharGRU, and\nCharBiLSTM, on a custom-built email dataset, which combines data from multiple\nsources. Their performance is analyzed under three scenarios: (i) standard\ntraining and testing, (ii) standard training and testing under adversarial\nattacks, and (iii) training and testing with adversarial examples. Aiming to\ndevelop a tool that operates as a browser extension, we test all models under\nlimited computational resources. In this constrained setup, CharGRU proves to\nbe the best-performing model across all scenarios. All models show\nvulnerability to adversarial attacks, but adversarial training substantially\nimproves their robustness. In addition, by adapting the Gradient-weighted Class\nActivation Mapping (Grad-CAM) technique to character-level inputs, we are able\nto visualize which parts of each email influence the decision of each model.\nOur open-source code and data is released at\nhttps://github.com/chipermaria/every-character-counts."
    },
    {
        "date": "2025-09",
        "title": "Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits",
        "author": "Weixin Chen, and Han Zhao",
        "link": "http://arxiv.org/abs/2509.20549v1",
        "abstract": "Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck\nmodels, comprise an attribute recognition model and a probabilistic circuit for\nreasoning. By integrating the outputs from these two modules, NPCs produce\ncompositional and interpretable predictions. While offering enhanced\ninterpretability and high performance on downstream tasks, the\nneural-network-based attribute recognition model remains a black box. This\nvulnerability allows adversarial attacks to manipulate attribute predictions by\nintroducing carefully crafted subtle perturbations to input images, potentially\ncompromising the final predictions. In this paper, we theoretically analyze the\nadversarial robustness of NPC and demonstrate that it only depends on the\nrobustness of the attribute recognition model and is independent of the\nrobustness of the probabilistic circuit. Moreover, we propose RNPC, the first\nrobust neural probabilistic circuit against adversarial attacks on the\nrecognition module. RNPC introduces a novel class-wise integration for\ninference, ensuring a robust combination of outputs from the two modules. Our\ntheoretical analysis demonstrates that RNPC exhibits provably improved\nadversarial robustness compared to NPC. Empirical results on image\nclassification tasks show that RNPC achieves superior adversarial robustness\ncompared to existing concept bottleneck models while maintaining high accuracy\non benign inputs."
    },
    {
        "date": "2025-09",
        "title": "Efficiently Attacking Memorization Scores",
        "author": "Tue Do, Varun Chandrasekaran, and Daniel Alabi",
        "link": "http://arxiv.org/abs/2509.20463v2",
        "abstract": "Influence estimation tools -- such as memorization scores -- are widely used\nto understand model behavior, attribute training data, and inform dataset\ncuration. However, recent applications in data valuation and responsible\nmachine learning raise the question: can these scores themselves be\nadversarially manipulated? In this work, we present a systematic study of the\nfeasibility of attacking memorization-based influence estimators. We\ncharacterize attacks for producing highly memorized samples as highly sensitive\nqueries in the regime where a trained algorithm is accurate. Our attack\n(calculating the pseudoinverse of the input) is practical, requiring only\nblack-box access to model outputs and incur modest computational overhead. We\nempirically validate our attack across a wide suite of image classification\ntasks, showing that even state-of-the-art proxies are vulnerable to targeted\nscore manipulations. In addition, we provide a theoretical analysis of the\nstability of memorization scores under adversarial perturbations, revealing\nconditions under which influence estimates are inherently fragile. Our findings\nhighlight critical vulnerabilities in influence-based attribution and suggest\nthe need for robust defenses. All code can be found at\nhttps://github.com/tuedo2/MemAttack"
    },
    {
        "date": "2025-09",
        "title": "FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems",
        "author": "Shaoyuan Xie, Mohamad Habib Fakih, Junchi Lu, Fayzah Alshammari, Ningfei Wang, Takami Sato, Halima Bouzidi, Mohammad Abdullah Al Faruque, and Qi Alfred Chen",
        "link": "http://arxiv.org/abs/2509.20362v1",
        "abstract": "Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely\nused in applications such as surveillance, border control, and law enforcement,\nwhile also being misused in stalking and destructive actions. Thus, the\nsecurity of ATT is highly critical for real-world applications. Under the\nscope, we present a new type of attack: distance-pulling attacks (DPA) and a\nsystematic study of it, which exploits vulnerabilities in ATT systems to\ndangerously reduce tracking distances, leading to drone capturing, increased\nsusceptibility to sensor attacks, or even physical collisions. To achieve these\ngoals, we present FlyTrap, a novel physical-world attack framework that employs\nan adversarial umbrella as a deployable and domain-specific attack vector.\nFlyTrap is specifically designed to meet key desired objectives in attacking\nATT drones: physical deployability, closed-loop effectiveness, and\nspatial-temporal consistency. Through novel progressive distance-pulling\nstrategy and controllable spatial-temporal consistency designs, FlyTrap\nmanipulates ATT drones in real-world setups to achieve significant system-level\nimpacts. Our evaluations include new datasets, metrics, and closed-loop\nexperiments on real-world white-box and even commercial ATT drones, including\nDJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking\ndistances within the range to be captured, sensor attacked, or even directly\ncrashed, highlighting urgent security risks and practical implications for the\nsafe deployment of ATT systems."
    },
    {
        "date": "2025-09",
        "title": "chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets",
        "author": "Mohamed E. Najd, and Ghada Almashaqbeh",
        "link": "http://arxiv.org/abs/2509.20356v1",
        "abstract": "Decentralized resource markets are Web 3.0 applications that build\nopen-access platforms for trading digital resources among users without any\ncentral management. They promise cost reduction, transparency, and flexible\nservice provision. However, these markets usually have large workload that must\nbe processed in a timely manner, leading to serious scalability problems.\nDespite the large amount of work on blockchain scalability, existing solutions\nare ineffective as they do not account for these markets' work models and\ntraffic patterns.\n  We introduce chainScale, a secure hybrid sidechain-sharding solution that\naims to boost throughput of decentralized resource markets and reduce their\nlatency and storage footprint. At its core, chainScale leverages dependent\nsidechains and functionality-oriented workload splitting to parallelize traffic\nprocessing by having each market module assigned to a sidechain. Different from\nsharding, chainScale does not incur any cross-sidechain transactions that tend\nto be costly. chainScale introduces several techniques, including hierarchical\nworkload sharing that further sub-divides overloaded modules, and weighted\nminer assignment that assigns miners with vested interest in the system to\ncritical modules' sidechains. Furthermore, chainScale employs sidechain syncing\nto maintain the mainchain as the single truth of system state, and pruning to\ndiscard stale records. Beside analyzing security, we build a proof-of-concept\nimplementation for a distributed file storage market as a use case. Our\nexperiments show that, compared to a single sidechain-based prior solution,\nchainScale boosts throughput by 4x and reduces confirmation latency by 5x.\nAlso, they show that chainScale outperforms sharding by 2.5x in throughput and\n3.5x in latency."
    },
    {
        "date": "2025-09",
        "title": "RAG Security and Privacy: Formalizing the Threat Model and Attack Surface",
        "author": "Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, and Kaushik Dutta",
        "link": "http://arxiv.org/abs/2509.20324v1",
        "abstract": "Retrieval-Augmented Generation (RAG) is an emerging approach in natural\nlanguage processing that combines large language models (LLMs) with external\ndocument retrieval to produce more accurate and grounded responses. While RAG\nhas shown strong potential in reducing hallucinations and improving factual\nconsistency, it also introduces new privacy and security challenges that differ\nfrom those faced by traditional LLMs. Existing research has demonstrated that\nLLMs can leak sensitive information through training data memorization or\nadversarial prompts, and RAG systems inherit many of these vulnerabilities. At\nthe same time, reliance of RAG on an external knowledge base opens new attack\nsurfaces, including the potential for leaking information about the presence or\ncontent of retrieved documents, or for injecting malicious content to\nmanipulate model behavior. Despite these risks, there is currently no formal\nframework that defines the threat landscape for RAG systems. In this paper, we\naddress a critical gap in the literature by proposing, to the best of our\nknowledge, the first formal threat model for retrieval-RAG systems. We\nintroduce a structured taxonomy of adversary types based on their access to\nmodel components and data, and we formally define key threat vectors such as\ndocument-level membership inference and data poisoning, which pose serious\nprivacy and integrity risks in real-world deployments. By establishing formal\ndefinitions and attack models, our work lays the foundation for a more rigorous\nand principled understanding of privacy and security in RAG systems."
    },
    {
        "date": "2025-09",
        "title": "Investigating Security Implications of Automatically Generated Code on the Software Supply Chain",
        "author": "Xiaofan Li, and Xing Gao",
        "link": "http://arxiv.org/abs/2509.20277v1",
        "abstract": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats."
    },
    {
        "date": "2025-09",
        "title": "Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization",
        "author": "Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, and Kaize Ding",
        "link": "http://arxiv.org/abs/2509.20230v3",
        "abstract": "Current LLM unlearning methods face a critical security vulnerability that\nundermines their fundamental purpose: while they appear to successfully remove\nsensitive or harmful knowledge, this ``forgotten\" information remains\nprecariously recoverable through relearning attacks. We identify that the root\ncause is that conventional methods optimizing the forgetting loss at individual\ndata points will drive model parameters toward sharp minima in the loss\nlandscape. In these unstable regions, even minimal parameter perturbations can\ndrastically alter the model's behaviors. Consequently, relearning attacks\nexploit this vulnerability by using just a few fine-tuning samples to navigate\nthe steep gradients surrounding these unstable regions, thereby rapidly\nrecovering knowledge that was supposedly erased. This exposes a critical\nrobustness gap between apparent unlearning and actual knowledge removal. To\naddress this issue, we propose StableUN, a bi-level feedback-guided\noptimization framework that explicitly seeks more stable parameter regions via\nneighborhood-aware optimization. It integrates forgetting feedback, which uses\nadversarial perturbations to probe parameter neighborhoods, with remembering\nfeedback to preserve model utility, aligning the two objectives through\ngradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that\nour method is significantly more robust against both relearning and\njailbreaking attacks while maintaining competitive utility performance."
    },
    {
        "date": "2025-09",
        "title": "Universal Camouflage Attack on Vision-Language Models for Autonomous Driving",
        "author": "Dehong Kong, Sifan Yu, Siyuan Liang, Jiawei Liang, Jianhou Gan, Aishan Liu, and Wenqi Ren",
        "link": "http://arxiv.org/abs/2509.20196v1",
        "abstract": "Visual language modeling for automated driving is emerging as a promising\nresearch direction with substantial improvements in multimodal reasoning\ncapabilities. Despite its advanced reasoning abilities, VLM-AD remains\nvulnerable to serious security threats from adversarial attacks, which involve\nmisleading model decisions through carefully crafted perturbations. Existing\nattacks have obvious challenges: 1) Physical adversarial attacks primarily\ntarget vision modules. They are difficult to directly transfer to VLM-AD\nsystems because they typically attack low-level perceptual components. 2)\nAdversarial attacks against VLM-AD have largely concentrated on the digital\nlevel. To address these challenges, we propose the first Universal Camouflage\nAttack (UCA) framework for VLM-AD. Unlike previous methods that focus on\noptimizing the logit layer, UCA operates in the feature space to generate\nphysically realizable camouflage textures that exhibit strong generalization\nacross different user commands and model architectures. Motivated by the\nobserved vulnerability of encoder and projection layers in VLM-AD, UCA\nintroduces a feature divergence loss (FDL) that maximizes the representational\ndiscrepancy between clean and adversarial images. In addition, UCA incorporates\na multi-scale learning strategy and adjusts the sampling ratio to enhance its\nadaptability to changes in scale and viewpoint diversity in real-world\nscenarios, thereby improving training stability. Extensive experiments\ndemonstrate that UCA can induce incorrect driving commands across various\nVLM-AD models and driving scenarios, significantly surpassing existing\nstate-of-the-art attack methods (improving 30\\% in 3-P metrics). Furthermore,\nUCA exhibits strong attack robustness under diverse viewpoints and dynamic\nconditions, indicating high potential for practical deployment."
    },
    {
        "date": "2025-09",
        "title": "Examining the robustness of Physics-Informed Neural Networks to noise for Inverse Problems",
        "author": "Aleksandra Jekic, Afroditi Natsaridou, Signe Riemer-S\u00f8rensen, Helge Langseth, and Odd Erik Gundersen",
        "link": "http://arxiv.org/abs/2509.20191v1",
        "abstract": "Approximating solutions to partial differential equations (PDEs) is\nfundamental for the modeling of dynamical systems in science and engineering.\nPhysics-informed neural networks (PINNs) are a recent machine learning-based\napproach, for which many properties and limitations remain unknown. PINNs are\nwidely accepted as inferior to traditional methods for solving PDEs, such as\nthe finite element method, both with regard to computation time and accuracy.\nHowever, PINNs are commonly claimed to show promise in solving inverse problems\nand handling noisy or incomplete data. We compare the performance of PINNs in\nsolving inverse problems with that of a traditional approach using the finite\nelement method combined with a numerical optimizer. The models are tested on a\nseries of increasingly difficult fluid mechanics problems, with and without\nnoise. We find that while PINNs may require less human effort and specialized\nknowledge, they are outperformed by the traditional approach. However, the\ndifference appears to decrease with higher dimensions and more data. We\nidentify common failures during training to be addressed if the performance of\nPINNs on noisy inverse problems is to become more competitive."
    },
    {
        "date": "2025-09",
        "title": "STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation",
        "author": "Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, and Apurva Narayan",
        "link": "http://arxiv.org/abs/2509.20190v1",
        "abstract": "In modern automotive development, security testing is critical for\nsafeguarding systems against increasingly advanced threats. Attack trees are\nwidely used to systematically represent potential attack vectors, but\ngenerating comprehensive test cases from these trees remains a labor-intensive,\nerror-prone task that has seen limited automation in the context of testing\nvehicular systems. This paper introduces STAF (Security Test Automation\nFramework), a novel approach to automating security test case generation.\nLeveraging Large Language Models (LLMs) and a four-step self-corrective\nRetrieval-Augmented Generation (RAG) framework, STAF automates the generation\nof executable security test cases from attack trees, providing an end-to-end\nsolution that encompasses the entire attack surface. We particularly show the\nelements and processes needed to provide an LLM to actually produce sensible\nand executable automotive security test suites, along with the integration with\nan automated testing framework. We further compare our tailored approach with\ngeneral purpose (vanilla) LLMs and the performance of different LLMs (namely\nGPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our\noperation step-by-step in a concrete case study. Our results show significant\nimprovements in efficiency, accuracy, scalability, and easy integration in any\nworkflow, marking a substantial advancement in automating automotive security\ntesting methodologies. Using TARAs as an input for verfication tests, we create\nsynergies by connecting two vital elements of a secure automotive development\nprocess."
    },
    {
        "date": "2025-09",
        "title": "C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis",
        "author": "Min Cen, Zhenfeng Zhuang, Yuzhe Zhang, Min Zeng, Baptiste Magnier, Lequan Yu, Hong Zhang, and Liansheng Wang",
        "link": "http://arxiv.org/abs/2509.20152v1",
        "abstract": "Graph-based Multiple Instance Learning (MIL) is widely used in survival\nanalysis with Hematoxylin and Eosin (H\\&E)-stained whole slide images (WSIs)\ndue to its ability to capture topological information. However, variations in\nstaining and scanning can introduce semantic bias, while topological subgraphs\nthat are not relevant to the causal relationships can create noise, resulting\nin biased slide-level representations. These issues can hinder both the\ninterpretability and generalization of the analysis. To tackle this, we\nintroduce a dual structural causal model as the theoretical foundation and\npropose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.\nC$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module\nfor semantic causal intervention and a new Bernoulli differentiable causal\nsubgraph sampling method for topological causal discovery. A joint optimization\nstrategy combining disentangling supervision and contrastive learning enables\nsimultaneous refinement of both semantic and topological causalities.\nExperiments demonstrate that C$^2$MIL consistently improves generalization and\ninterpretability over existing methods and can serve as a causal enhancement\nfor diverse MIL baselines. The code is available at\nhttps://github.com/mimic0127/C2MIL."
    },
    {
        "date": "2025-09",
        "title": "Beyond Slater's Condition in Online CMDPs with Stochastic and Adversarial Constraints",
        "author": "Francesco Emanuele Stradi, Eleonora Fidelia Chiefari, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti",
        "link": "http://arxiv.org/abs/2509.20114v1",
        "abstract": "We study \\emph{online episodic Constrained Markov Decision Processes} (CMDPs)\nunder both stochastic and adversarial constraints. We provide a novel algorithm\nwhose guarantees greatly improve those of the state-of-the-art\nbest-of-both-worlds algorithm introduced by Stradi et al. (2025). In the\nstochastic regime, \\emph{i.e.}, when the constraints are sampled from fixed but\nunknown distributions, our method achieves $\\widetilde{\\mathcal{O}}(\\sqrt{T})$\nregret and constraint violation without relying on Slater's condition, thereby\nhandling settings where no strictly feasible solution exists. Moreover, we\nprovide guarantees on the stronger notion of \\emph{positive} constraint\nviolation, which does not allow to recover from large violation in the early\nepisodes by playing strictly safe policies. In the adversarial regime,\n\\emph{i.e.}, when the constraints may change arbitrarily between episodes, our\nalgorithm ensures sublinear constraint violation without Slater's condition,\nand achieves sublinear $\\alpha$-regret with respect to the \\emph{unconstrained}\noptimum, where $\\alpha$ is a suitably defined multiplicative approximation\nfactor. We further validate our results through synthetic experiments, showing\nthe practical effectiveness of our algorithm."
    },
    {
        "date": "2025-09",
        "title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment",
        "author": "Tong Nie, Yuewen Mei, Yihong Tang, Junlin He, Jie Sun, Haotian Shi, Wei Ma, and Jian Sun",
        "link": "http://arxiv.org/abs/2509.20102v1",
        "abstract": "Adversarial scenario generation is a cost-effective approach for safety\nassessment of autonomous driving systems. However, existing methods are often\nconstrained to a single, fixed trade-off between competing objectives such as\nadversariality and realism. This yields behavior-specific models that cannot be\nsteered at inference time, lacking the efficiency and flexibility to generate\ntailored scenarios for diverse training and testing requirements. In view of\nthis, we reframe the task of adversarial scenario generation as a\nmulti-objective preference alignment problem and introduce a new framework\nnamed \\textbf{S}teerable \\textbf{A}dversarial scenario \\textbf{GE}nerator\n(SAGE). SAGE enables fine-grained test-time control over the trade-off between\nadversariality and realism without any retraining. We first propose\nhierarchical group-based preference optimization, a data-efficient offline\nalignment method that learns to balance competing objectives by decoupling hard\nfeasibility constraints from soft preferences. Instead of training a fixed\nmodel, SAGE fine-tunes two experts on opposing preferences and constructs a\ncontinuous spectrum of policies at inference time by linearly interpolating\ntheir weights. We provide theoretical justification for this framework through\nthe lens of linear mode connectivity. Extensive experiments demonstrate that\nSAGE not only generates scenarios with a superior balance of adversariality and\nrealism but also enables more effective closed-loop training of driving\npolicies. Project page: https://tongnie.github.io/SAGE/."
    },
    {
        "date": "2025-09",
        "title": "SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models",
        "author": "Xiyu Zeng, Siyuan Liang, Liming Lu, Haotian Zhu, Enguang Liu, Jisheng Dang, Yongbin Zhou, and Shuchao Pang",
        "link": "http://arxiv.org/abs/2509.21400v1",
        "abstract": "As the capabilities of Vision Language Models (VLMs) continue to improve,\nthey are increasingly targeted by jailbreak attacks. Existing defense methods\nface two major limitations: (1) they struggle to ensure safety without\ncompromising the model's utility; and (2) many defense mechanisms significantly\nreduce the model's inference efficiency. To address these challenges, we\npropose SafeSteer, a lightweight, inference-time steering framework that\neffectively defends against diverse jailbreak attacks without modifying model\nweights. At the core of SafeSteer is the innovative use of Singular Value\nDecomposition to construct a low-dimensional \"safety subspace.\" By projecting\nand reconstructing the raw steering vector into this subspace during inference,\nSafeSteer adaptively removes harmful generation signals while preserving the\nmodel's ability to handle benign inputs. The entire process is executed in a\nsingle inference pass, introducing negligible overhead. Extensive experiments\nshow that SafeSteer reduces the attack success rate by over 60% and improves\naccuracy on normal tasks by 1-2%, without introducing significant inference\nlatency. These results demonstrate that robust and practical jailbreak defense\ncan be achieved through simple, efficient inference-time control."
    },
    {
        "date": "2025-09",
        "title": "Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations",
        "author": "Rami Zewail",
        "link": "http://arxiv.org/abs/2509.20048v2",
        "abstract": "Learning robust representations for biosignals is often hampered by the\nchallenge of designing effective data augmentations.Traditional methods can\nfail to capture the complex variations inherent in physiological data. Within\nthis context, we propose a novel hybrid framework, Diffusion-Augmented\nContrastive Learning (DACL), that fuses concepts from diffusion models and\nsupervised contrastive learning. The DACL framework operates on a latent space\ncreated by a lightweight Variational Autoencoder (VAE) trained on our novel\nScattering Transformer (ST) features [12]. It utilizes the diffusion forward\nprocess as a principled data augmentation technique to generate multiple noisy\nviews of these latent embeddings. A U-Net style encoder is then trained with a\nsupervised contrastive objective to learn a representation that balances class\ndiscrimination with robustness to noise across various diffusion time steps. We\nevaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,\nachieving a competitive AUROC of 0.7815. This work establishes a new paradigm\nfor representation learning by using the diffusion process itself to drive the\ncontrastive objective, creating noise-invariant embeddings that demonstrate a\nstrong foundation for class separability."
    },
    {
        "date": "2025-09",
        "title": "Predictive Quality Assessment for Mobile Secure Graphics",
        "author": "Cas Steigstra, Sergey Milyaev, and Shaodi You",
        "link": "http://arxiv.org/abs/2509.20028v1",
        "abstract": "The reliability of secure graphic verification, a key anti-counterfeiting\ntool, is undermined by poor image acquisition on smartphones. Uncontrolled user\ncaptures of these high-entropy patterns cause high false rejection rates,\ncreating a significant 'reliability gap'. To bridge this gap, we depart from\ntraditional perceptual IQA and introduce a framework that predictively\nestimates a frame's utility for the downstream verification task. We propose a\nlightweight model to predict a quality score for a video frame, determining its\nsuitability for a resource-intensive oracle model. Our framework is validated\nusing re-contextualized FNMR and ISRR metrics on a large-scale dataset of\n32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis\non graphics from different industrial printing presses reveals a key finding: a\nlightweight probe on a frozen, ImageNet-pretrained network generalizes better\nto an unseen printing technology than a fully fine-tuned model. This provides a\nkey insight for real-world generalization: for domain shifts from physical\nmanufacturing, a frozen general-purpose backbone can be more robust than full\nfine-tuning, which can overfit to source-domain artifacts."
    },
    {
        "date": "2025-09",
        "title": "Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification",
        "author": "Lubos Mjachky, and Ivan Homoliak",
        "link": "http://arxiv.org/abs/2509.20024v1",
        "abstract": "Biometric-based authentication systems are getting broadly adopted in many\nareas. However, these systems do not allow participating users to influence the\nway their data is used. Furthermore, the data may leak and can be misused\nwithout the users' knowledge. In this paper, we propose a new authentication\nmethod that preserves the privacy of individuals and is based on a generative\nadversarial network (GAN). Concretely, we suggest using the GAN for translating\nimages of faces to a visually private domain (e.g., flowers or shoes).\nClassifiers, which are used for authentication purposes, are then trained on\nthe images from the visually private domain. Based on our experiments, the\nmethod is robust against attacks and still provides meaningful utility."
    },
    {
        "date": "2025-09",
        "title": "Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation",
        "author": "Raphael Simon, Pieter Libin, and Wim Mees",
        "link": "http://arxiv.org/abs/2509.20008v1",
        "abstract": "Penetration testing, the simulation of cyberattacks to identify security\nvulnerabilities, presents a sequential decision-making problem well-suited for\nreinforcement learning (RL) automation. Like many applications of RL to\nreal-world problems, partial observability presents a major challenge, as it\ninvalidates the Markov property present in Markov Decision Processes (MDPs).\nPartially Observable MDPs require history aggregation or belief state\nestimation to learn successful policies. We investigate stochastic, partially\nobservable penetration testing scenarios over host networks of varying size,\naiming to better reflect real-world complexity through more challenging and\nrepresentative benchmarks. This approach leads to the development of more\nrobust and transferable policies, which are crucial for ensuring reliable\nperformance across diverse and unpredictable real-world environments. Using\nvanilla Proximal Policy Optimization (PPO) as a baseline, we compare a\nselection of PPO variants designed to mitigate partial observability, including\nframe-stacking, augmenting observations with historical information, and\nemploying recurrent or transformer-based architectures. We conduct a systematic\nempirical analysis of these algorithms across different host network sizes. We\nfind that this task greatly benefits from history aggregation. Converging three\ntimes faster than other approaches. Manual inspection of the learned policies\nby the algorithms reveals clear distinctions and provides insights that go\nbeyond quantitative results."
    },
    {
        "date": "2025-09",
        "title": "Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models",
        "author": "Zhifang Zhang, Jiahan Zhang, Shengjie Zhou, Qi Wei, Shuo He, Feng Liu, and Lei Feng",
        "link": "http://arxiv.org/abs/2509.19994v2",
        "abstract": "Multimodal pre-trained models (e.g., ImageBind), which align distinct data\nmodalities into a shared embedding space, have shown remarkable success across\ndownstream tasks. However, their increasing adoption raises serious security\nconcerns, especially regarding targeted adversarial attacks. In this paper, we\nshow that existing targeted adversarial attacks on multimodal pre-trained\nmodels still have limitations in two aspects: generalizability and\nundetectability. Specifically, the crafted targeted adversarial examples (AEs)\nexhibit limited generalization to partially known or semantically similar\ntargets in cross-modal alignment tasks (i.e., limited generalizability) and can\nbe easily detected by simple anomaly detection methods (i.e., limited\nundetectability). To address these limitations, we propose a novel method\ncalled Proxy Targeted Attack (PTA), which leverages multiple source-modal and\ntarget-modal proxies to optimize targeted AEs, ensuring they remain evasive to\ndefenses while aligning with multiple potential targets. We also provide\ntheoretical analyses to highlight the relationship between generalizability and\nundetectability and to ensure optimal generalizability while meeting the\nspecified requirements for undetectability. Furthermore, experimental results\ndemonstrate that our PTA can achieve a high success rate across various related\ntargets and remain undetectable against multiple anomaly detection methods."
    },
    {
        "date": "2025-09",
        "title": "OpenGL GPU-Based Rowhammer Attack (Work in Progress)",
        "author": "Antoine Plin, Fr\u00e9d\u00e9ric Fauberteau, and Nga Nguyen",
        "link": "http://arxiv.org/abs/2509.19959v1",
        "abstract": "Rowhammer attacks have emerged as a significant threat to modern DRAM-based\nmemory systems, leveraging frequent memory accesses to induce bit flips in\nadjacent memory cells. This work-in-progress paper presents an adaptive,\nmany-sided Rowhammer attack utilizing GPU compute shaders to systematically\nachieve high-frequency memory access patterns. Our approach employs statistical\ndistributions to optimize row targeting and avoid current mitigations. The\nmethodology involves initializing memory with known patterns, iteratively\nhammering victim rows, monitoring for induced errors, and dynamically adjusting\nparameters to maximize success rates. The proposed attack exploits the parallel\nprocessing capabilities of GPUs to accelerate hammering operations, thereby\nincreasing the probability of successful bit flips within a constrained\ntimeframe. By leveraging OpenGL compute shaders, our implementation achieves\nhighly efficient row hammering with minimal software overhead. Experimental\nresults on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a\nhigh rate of bit flips compared to traditional CPU-based hammering, confirming\nits effectiveness in compromising DRAM integrity. Our findings align with\nexisting research on microarchitectural attacks in heterogeneous systems that\nhighlight the susceptibility of GPUs to security vulnerabilities. This study\ncontributes to the understanding of GPU-assisted fault-injection attacks and\nunderscores the need for improved mitigation strategies in future memory\narchitectures."
    },
    {
        "date": "2025-09",
        "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers",
        "author": "Zhixiao Wu, Yao Lu, Jie Wen, Hao Sun, Qi Zhou, and Guangming Lu",
        "link": "http://arxiv.org/abs/2509.19947v1",
        "abstract": "Poison-only Clean-label Backdoor Attacks aim to covertly inject\nattacker-desired behavior into DNNs by merely poisoning the dataset without\nchanging the labels. To effectively implant a backdoor, multiple\n\\textbf{triggers} are proposed for various attack requirements of Attack\nSuccess Rate (ASR) and stealthiness. Additionally, sample selection enhances\nclean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples\ninstead of random samples to poison. Current methods 1) usually handle the\nsample selection and triggers in isolation, leading to severely limited\nimprovements on both ASR and stealthiness. Consequently, attacks exhibit\nunsatisfactory performance on evaluation metrics when converted to PCBAs via a\nmere stacking of methods. Therefore, we seek to explore the bidirectional\ncollaborative relations between the sample selection and triggers to address\nthe above dilemma. 2) Since the strong specificity within triggers, the simple\ncombination of sample selection and triggers fails to substantially enhance\nboth evaluation metrics, with generalization preserved among various attacks.\nTherefore, we seek to propose a set of components to significantly improve both\nstealthiness and ASR based on the commonalities of attacks. Specifically,\nComponent A ascertains two critical selection factors, and then makes them an\nappropriate combination based on the trigger scale to select more reasonable\n``hard'' samples for improving ASR. Component B is proposed to select samples\nwith similarities to relevant trigger implanted samples to promote\nstealthiness. Component C reassigns trigger poisoning intensity on RGB colors\nthrough distinct sensitivity of the human visual system to RGB for higher ASR,\nwith stealthiness ensured by sample selection, including Component B.\nFurthermore, all components can be strategically integrated into diverse PCBAs."
    },
    {
        "date": "2025-09",
        "title": "CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation",
        "author": "Miren Samaniego, Igor Rodriguez, and Elena Lazkano",
        "link": "http://arxiv.org/abs/2509.19936v1",
        "abstract": "We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze\nestimation that integrates a ConvNeXt backbone, capsule formation with\nattention routing, and dual GRU decoders specialized for slow and rapid gaze\ndynamics. This modular design enables efficient part-whole reasoning and\ndisentangled temporal modeling, achieving state-of-the-art performance on\nETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference\n(< 10 ms). The model also generalizes well to unconstrained conditions in\nGaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),\noutperforming or matching existing methods with fewer parameters and greater\ninterpretability. These results demonstrate that CapStARE offers a practical\nand robust solution for real-time gaze estimation in interactive systems. The\nrelated code and results for this article can be found on:\nhttps://github.com/toukapy/capsStare"
    },
    {
        "date": "2025-09",
        "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models",
        "author": "Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, and Yu-Gang Jiang",
        "link": "http://arxiv.org/abs/2509.19870v1",
        "abstract": "Vision-Language-Action (VLA) models are driving rapid progress in robotics by\nenabling agents to interpret multimodal inputs and execute complex,\nlong-horizon tasks. However, their safety and robustness against adversarial\nattacks remain largely underexplored. In this work, we identify and formalize a\ncritical adversarial vulnerability in which adversarial images can \"freeze\" VLA\nmodels and cause them to ignore subsequent instructions. This threat\neffectively disconnects the robot's digital mind from its physical actions,\npotentially inducing inaction during critical interventions. To systematically\nstudy this vulnerability, we propose FreezeVLA, a novel attack framework that\ngenerates and evaluates action-freezing attacks via min-max bi-level\noptimization. Experiments on three state-of-the-art VLA models and four robotic\nbenchmarks show that FreezeVLA attains an average attack success rate of 76.2%,\nsignificantly outperforming existing methods. Moreover, adversarial images\ngenerated by FreezeVLA exhibit strong transferability, with a single image\nreliably inducing paralysis across diverse language prompts. Our findings\nexpose a critical safety risk in VLA models and highlight the urgent need for\nrobust defense mechanisms."
    },
    {
        "date": "2025-09",
        "title": "LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation",
        "author": "Huizhen Shu, Xuying Li, and Zhuo Li",
        "link": "http://arxiv.org/abs/2509.19839v1",
        "abstract": "Achieving robust safety alignment in large language models (LLMs) while\npreserving their utility remains a fundamental challenge. Existing approaches\noften struggle to balance comprehensive safety with fine-grained\ncontrollability at the representation level. We introduce LATENTGUARD, a novel\nthree-stage framework that combines behavioral alignment with supervised latent\nspace control for interpretable and precise safety steering. Our approach\nbegins by fine-tuning an LLM on rationalized datasets containing both\nreasoning-enhanced refusal responses to adversarial prompts and\nreasoning-enhanced normal responses to benign queries, establishing robust\nbehavioral priors across both safety-critical and utility-preserving scenarios.\nWe then train a structured variational autoencoder (VAE) on intermediate MLP\nactivations, supervised by multi-label annotations including attack types,\nattack methods, and benign indicators. This supervision enables the VAE to\nlearn disentangled latent representations that capture distinct adversarial\ncharacteristics while maintaining semantic interpretability. Through targeted\nmanipulation of learned latent dimensions, LATENTGUARD achieves selective\nrefusal behavior, effectively blocking harmful requests while preserving\nhelpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate\nsignificant improvements in both safety controllability and response\ninterpretability without compromising utility. Cross-architecture validation on\nMistral-7B confirms the generalizability of our latent steering approach,\nshowing consistent effectiveness across different model families. Our results\nsuggest that structured representation-level intervention offers a promising\npathway toward building safer yet practical LLM systems."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation",
        "author": "Tharcisse Ndayipfukamiye, Jianguo Ding, Doreen Sebastian Sarwatt, Adamu Gaston Philipo, and Huansheng Ning",
        "link": "http://arxiv.org/abs/2509.20411v2",
        "abstract": "Machine learning-based cybersecurity systems are highly vulnerable to\nadversarial attacks, while Generative Adversarial Networks (GANs) act as both\npowerful attack enablers and promising defenses. This survey systematically\nreviews GAN-based adversarial defenses in cybersecurity (2021--August 31,\n2025), consolidating recent progress, identifying gaps, and outlining future\ndirections. Using a PRISMA-compliant systematic literature review protocol, we\nsearched five major digital libraries. From 829 initial records, 185\npeer-reviewed studies were retained and synthesized through quantitative trend\nanalysis and thematic taxonomy development. We introduce a four-dimensional\ntaxonomy spanning defensive function, GAN architecture, cybersecurity domain,\nand adversarial threat model. GANs improve detection accuracy, robustness, and\ndata utility across network intrusion detection, malware analysis, and IoT\nsecurity. Notable advances include WGAN-GP for stable training, CGANs for\ntargeted synthesis, and hybrid GAN models for improved resilience. Yet,\npersistent challenges remain such as instability in training, lack of\nstandardized benchmarks, high computational cost, and limited explainability.\nGAN-based defenses demonstrate strong potential but require advances in stable\narchitectures, benchmarking, transparency, and deployment. We propose a roadmap\nemphasizing hybrid models, unified evaluation, real-world integration, and\ndefenses against emerging threats such as LLM-driven cyberattacks. This survey\nestablishes the foundation for scalable, trustworthy, and adaptive GAN-powered\ndefenses."
    },
    {
        "date": "2025-09",
        "title": "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting",
        "author": "Yixun Zhang, Feng Zhou, and Jianqin Yin",
        "link": "http://arxiv.org/abs/2509.19793v1",
        "abstract": "Camera-based perception is critical to autonomous driving yet remains\nvulnerable to task-specific adversarial manipulations in object detection and\nmonocular depth estimation. Most existing 2D/3D attacks are developed in task\nsilos, lack mechanisms to induce controllable depth bias, and offer no\nstandardized protocol to quantify cross-task transfer, leaving the interaction\nbetween detection and depth underexplored. We present BiTAA, a bi-task\nadversarial attack built on 3D Gaussian Splatting that yields a single\nperturbation capable of simultaneously degrading detection and biasing\nmonocular depth. Specifically, we introduce a dual-model attack framework that\nsupports both full-image and patch settings and is compatible with common\ndetectors and depth estimators, with optional expectation-over-transformation\n(EOT) for physical reality. In addition, we design a composite loss that\ncouples detection suppression with a signed, magnitude-controlled log-depth\nbias within regions of interest (ROIs) enabling controllable near or far\nmisperception while maintaining stable optimization across tasks. We also\npropose a unified evaluation protocol with cross-task transfer metrics and\nreal-world evaluations, showing consistent cross-task degradation and a clear\nasymmetry between Det to Depth and from Depth to Det transfer. The results\nhighlight practical risks for multi-task camera-only perception and motivate\ncross-task-aware defenses in autonomous driving scenarios."
    },
    {
        "date": "2025-09",
        "title": "Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation",
        "author": "Hongtao Yang, Bineng Zhong, Qihua Liang, Zhiruo Zhu, Yaozong Zheng, and Ning Li",
        "link": "http://arxiv.org/abs/2509.19733v1",
        "abstract": "Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking\nas a parameter-efficient finetuning (PEFT) method. However, these PEFT-based\nRGB-T tracking methods typically rely solely on spatial domain information as\nprompts for feature extraction. As a result, they often fail to achieve optimal\nperformance by overlooking the crucial role of frequency-domain information in\nprompt learning. To address this issue, we propose an efficient Visual Fourier\nPrompt Tracking (named VFPTrack) method to learn modality-related prompts via\nFast Fourier Transform (FFT). Our method consists of symmetric feature\nextraction encoder with shared parameters, visual fourier prompts, and Modality\nFusion Prompt Generator that generates bidirectional interaction prompts\nthrough multi-modal feature fusion. Specifically, we first use a frozen feature\nextraction encoder to extract RGB and thermal infrared (TIR) modality features.\nThen, we combine the visual prompts in the spatial domain with the frequency\ndomain prompts obtained from the FFT, which allows for the full extraction and\nunderstanding of modality features from different domain information. Finally,\nunlike previous fusion methods, the modality fusion prompt generation module we\nuse combines features from different modalities to generate a fused modality\nprompt. This modality prompt is interacted with each individual modality to\nfully enable feature interaction across different modalities. Extensive\nexperiments conducted on three popular RGB-T tracking benchmarks show that our\nmethod demonstrates outstanding performance."
    },
    {
        "date": "2025-09",
        "title": "Dynamic Dual-level Defense Routing for Continual Adversarial Training",
        "author": "Wenxuan Wang, Chenglei Wang, and Xuelin Qian",
        "link": "http://arxiv.org/abs/2509.21392v1",
        "abstract": "As adversarial attacks continue to evolve, defense models face the risk of\nrecurrent vulnerabilities, underscoring the importance of continuous\nadversarial training (CAT). Existing CAT approaches typically balance decision\nboundaries by either data replay or optimization strategy to constrain shared\nmodel parameters. However, due to the diverse and aggressive nature of\nadversarial examples, these methods suffer from catastrophic forgetting of\nprevious defense knowledge after continual learning. In this paper, we propose\na novel framework, called Dual-level Defense Routing or DDeR, that can\nautonomously select appropriate routers to integrate specific defense experts,\nthereby adapting to evolving adversarial attacks. Concretely, the first-level\ndefense routing comprises multiple defense experts and routers, with each\nrouter dynamically selecting and combining suitable experts to process attacked\nfeatures. Routers are independently incremented as continuous adversarial\ntraining progresses, and their selections are guided by an Adversarial Sentinel\nNetwork (ASN) in the second-level defense routing. To compensate for the\ninability to test due to the independence of routers, we further present a\nPseudo-task Substitution Training (PST) strategy, which leverages\ndistributional discrepancy in data to facilitate inter-router communication\nwithout storing historical data. Extensive experiments demonstrate that DDeR\nachieves superior continuous defense performance and classification accuracy\ncompared to existing methods."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis",
        "author": "Jiesi Hu, Yanwu Yang, Zhiyu Ye, Chenfei Ye, Hanyang Peng, Jianfeng Cao, and Ting Ma",
        "link": "http://arxiv.org/abs/2509.19711v1",
        "abstract": "The rise of In-Context Learning (ICL) for universal medical image\nsegmentation has introduced an unprecedented demand for large-scale, diverse\ndatasets for training, exacerbating the long-standing problem of data scarcity.\nWhile data synthesis offers a promising solution, existing methods often fail\nto simultaneously achieve both high data diversity and a domain distribution\nsuitable for medical data. To bridge this gap, we propose \\textbf{SynthICL}, a\nnovel data synthesis framework built upon domain randomization. SynthICL\nensures realism by leveraging anatomical priors from real-world datasets,\ngenerates diverse anatomical structures to cover a broad data distribution, and\nexplicitly models inter-subject variations to create data cohorts suitable for\nICL. Extensive experiments on four held-out datasets validate our framework's\neffectiveness, showing that models trained with our data achieve performance\ngains of up to 63\\% in average Dice and substantially enhanced generalization\nto unseen anatomical domains. Our work helps mitigate the data bottleneck for\nICL-based segmentation, paving the way for robust models. Our code and the\ngenerated dataset are publicly available at\nhttps://github.com/jiesihu/Neuroverse3D."
    },
    {
        "date": "2025-09",
        "title": "Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification",
        "author": "Kevin Garcia, Cassandra Garza, Brooklyn Berry, and Yifeng Gao",
        "link": "http://arxiv.org/abs/2509.19654v1",
        "abstract": "The surge in the significance of time series in digital health domains\nnecessitates advanced methodologies for extracting meaningful patterns and\nrepresentations. Self-supervised contrastive learning has emerged as a\npromising approach for learning directly from raw data. However, time series\ndata in digital health is known to be highly noisy, inherently involves concept\ndrifting, and poses a challenge for training a generalizable deep learning\nmodel. In this paper, we specifically focus on data distribution shift caused\nby different human behaviors and propose a self-supervised learning framework\nthat is aware of the bag-of-symbol representation. The bag-of-symbol\nrepresentation is known for its insensitivity to data warping, location shifts,\nand noise existed in time series data, making it potentially pivotal in guiding\ndeep learning to acquire a representation resistant to such data shifting. We\ndemonstrate that the proposed method can achieve significantly better\nperformance where significant data shifting exists."
    },
    {
        "date": "2025-09",
        "title": "TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation",
        "author": "MohammadReza EskandariNasab, Shah Muhammad Hamdi, and Soukaina Filali Boubrahimi",
        "link": "http://arxiv.org/abs/2509.19638v1",
        "abstract": "Generating high-quality synthetic time series is a fundamental yet\nchallenging task across domains such as forecasting and anomaly detection,\nwhere real data can be scarce, noisy, or costly to collect. Unlike static data\ngeneration, synthesizing time series requires modeling both the marginal\ndistribution of observations and the conditional temporal dependencies that\ngovern sequential dynamics. We propose TIMED, a unified generative framework\nthat integrates a denoising diffusion probabilistic model (DDPM) to capture\nglobal structure via a forward-reverse diffusion process, a supervisor network\ntrained with teacher forcing to learn autoregressive dependencies through\nnext-step prediction, and a Wasserstein critic that provides adversarial\nfeedback to ensure temporal smoothness and fidelity. To further align the real\nand synthetic distributions in feature space, TIMED incorporates a Maximum Mean\nDiscrepancy (MMD) loss, promoting both diversity and sample quality. All\ncomponents are built using masked attention architectures optimized for\nsequence modeling and are trained jointly to effectively capture both\nunconditional and conditional aspects of time series data. Experimental results\nacross diverse multivariate time series benchmarks demonstrate that TIMED\ngenerates more realistic and temporally coherent sequences than\nstate-of-the-art generative models."
    },
    {
        "date": "2025-09",
        "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise",
        "author": "Pavan Reddy, and Aditya Sanjay Gujral",
        "link": "http://arxiv.org/abs/2509.22710v1",
        "abstract": "Adversarial attacks in machine learning traditionally focus on global\nperturbations to input data, yet the potential of localized adversarial noise\nremains underexplored. This study systematically evaluates localized\nadversarial attacks across widely-used methods, including FGSM, PGD, and C&W,\nto quantify their effectiveness, imperceptibility, and computational\nefficiency. By introducing a binary mask to constrain noise to specific\nregions, localized attacks achieve significantly lower mean pixel\nperturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved\nStructural Similarity Index (SSIM) compared to global attacks. However, these\nbenefits come at the cost of increased computational effort and a modest\nreduction in Attack Success Rate (ASR). Our results highlight that iterative\nmethods, such as PGD and C&W, are more robust to localization constraints than\nsingle-step methods like FGSM, maintaining higher ASR and imperceptibility\nmetrics. This work provides a comprehensive analysis of localized adversarial\nattacks, offering practical insights for advancing attack strategies and\ndesigning robust defensive systems."
    },
    {
        "date": "2025-09",
        "title": "What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities",
        "author": "Nathanael Jo, and Ashia Wilson",
        "link": "http://arxiv.org/abs/2509.19590v1",
        "abstract": "Evaluations of generative models on benchmark data are now ubiquitous, and\ntheir outcomes critically shape public and scientific expectations of AI's\ncapabilities. Yet growing skepticism surrounds their reliability. How can we\nknow that a reported accuracy genuinely reflects a model's true performance?\nEvaluations are often presented as simple measurements, but in reality they are\ninferences: to treat benchmark scores as evidence of capability is already to\nassume a theory of what capability is and how it manifests in a test. We make\nthis step explicit by proposing a principled framework for evaluation as\ninference: begin from a theory of capability, and then derive methods for\nestimating it. This perspective, familiar in fields such as psychometrics, has\nnot yet become commonplace in AI evaluation. As a proof of concept, we address\na central challenge that undermines reliability: sensitivity to perturbations.\nAfter formulating a model of ability, we introduce methods that infer ability\nwhile accounting for uncertainty from sensitivity and finite samples, including\nan adaptive algorithm that significantly reduces sample complexity. Together,\nthese contributions lay the groundwork for more reliable and trustworthy\nestimates of AI capabilities as measured through benchmarks."
    },
    {
        "date": "2025-09",
        "title": "A Survey of Recent Advancements in Secure Peer-to-Peer Networks",
        "author": "Raj Patel, Umesh Biswas, Surya Kodipaka, Will Carroll, Preston Peranich, and Maxwell Young",
        "link": "http://arxiv.org/abs/2509.19539v1",
        "abstract": "Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their\nsecurity is an active area of research. Many defenses with strong security\nguarantees have been proposed; however, the most-recent survey is over a decade\nold. This paper delivers an updated review of recent theoretical advances that\naddress classic threats, such as the Sybil and routing attacks, while\nhighlighting how emerging trends -- such as machine learning, social networks,\nand dynamic systems -- pose new challenges and drive novel solutions. We\nevaluate the strengths and weaknesses of these solutions and suggest directions\nfor future research."
    },
    {
        "date": "2025-09",
        "title": "Identifying and Addressing User-level Security Concerns in Smart Homes Using \"Smaller\" LLMs",
        "author": "Hafijul Hoque Chowdhury, Riad Ahmed Anonto, Sourov Jajodia, Suryadipta Majumdar, and Md. Shohrab Hossain",
        "link": "http://arxiv.org/abs/2509.19485v1",
        "abstract": "With the rapid growth of smart home IoT devices, users are increasingly\nexposed to various security risks, as evident from recent studies. While\nseeking answers to know more on those security concerns, users are mostly left\nwith their own discretion while going through various sources, such as online\nblogs and technical manuals, which may render higher complexity to regular\nusers trying to extract the necessary information. This requirement does not go\nalong with the common mindsets of smart home users and hence threatens the\nsecurity of smart homes furthermore. In this paper, we aim to identify and\naddress the major user-level security concerns in smart homes. Specifically, we\ndevelop a novel dataset of Q&A from public forums, capturing practical security\nchallenges faced by smart home users. We extract major security concerns in\nsmart homes from our dataset by leveraging the Latent Dirichlet Allocation\n(LDA). We fine-tune relatively \"smaller\" transformer models, such as T5 and\nFlan-T5, on this dataset to build a QA system tailored for smart home security.\nUnlike larger models like GPT and Gemini, which are powerful but often resource\nhungry and require data sharing, smaller models are more feasible for\ndeployment in resource-constrained or privacy-sensitive environments like smart\nhomes. The dataset is manually curated and supplemented with synthetic data to\nexplore its potential impact on model performance. This approach significantly\nimproves the system's ability to deliver accurate and relevant answers, helping\nusers address common security concerns with smart home IoT devices. Our\nexperiments on real-world user concerns show that our work improves the\nperformance of the base models."
    },
    {
        "date": "2025-09",
        "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps",
        "author": "Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, and Hamed Tabkhi",
        "link": "http://arxiv.org/abs/2509.19252v1",
        "abstract": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization."
    },
    {
        "date": "2025-09",
        "title": "Stability and Generalization of Adversarial Diffusion Training",
        "author": "Hesam Hosseini, Ying Cao, and Ali H. Sayed",
        "link": "http://arxiv.org/abs/2509.19234v1",
        "abstract": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions."
    },
    {
        "date": "2025-09",
        "title": "FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity",
        "author": "Ferdinand Kahenga, Antoine Bagula, Patrick Sello, and Sajal K. Das",
        "link": "http://arxiv.org/abs/2509.19220v1",
        "abstract": "Federated learning in practice must contend with heterogeneous feature\nspaces, severe non-IID data, and scarce labels across clients. We present\nFedFusion, a federated transfer-learning framework that unifies domain\nadaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,\nDivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via\nconfidence-filtered pseudo-labels and domain-adaptive transfer, while clients\nmaintain personalised encoders tailored to local data. To preserve global\ncoherence under heterogeneity, FedFusion employs similarity-weighted classifier\ncoupling (with optional cluster-wise averaging), mitigating dominance by\ndata-rich sites and improving minority-client performance. The frugal-labelling\npipeline combines self-/semi-supervised pretext training with selective\nfine-tuning, reducing annotation demands without sharing raw data. Across\ntabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,\nFedFusion consistently outperforms state-of-the-art baselines in accuracy,\nrobustness, and fairness while maintaining comparable communication and\ncomputation budgets. These results show that harmonising personalisation,\ndomain adaptation, and label efficiency is an effective recipe for robust\nfederated learning under real-world constraints."
    },
    {
        "date": "2025-09",
        "title": "A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness",
        "author": "Abdul-Rauf Nuhu, Parham Kebria, Vahid Hemmati, Benjamin Lartey, Mahmoud Nabil Mahmoud, Abdollah Homaifar, and Edward Tunstel",
        "link": "http://arxiv.org/abs/2509.19197v1",
        "abstract": "Data-driven models, especially deep learning classifiers often demonstrate\ngreat success on clean datasets. Yet, they remain vulnerable to common data\ndistortions such as adversarial and common corruption perturbations. These\nperturbations can significantly degrade performance, thereby challenging the\noverall reliability of the models. Traditional robustness validation typically\nrelies on perturbed test datasets to assess and improve model performance. In\nour framework, however, we propose a validation approach that extracts \"weak\nrobust\" samples directly from the training dataset via local robustness\nanalysis. These samples, being the most susceptible to perturbations, serve as\nan early and sensitive indicator of the model's vulnerabilities. By evaluating\nmodels on these challenging training instances, we gain a more nuanced\nunderstanding of its robustness, which informs targeted performance\nenhancement. We demonstrate the effectiveness of our approach on models trained\nwith CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation\nguided by weak robust samples can drive meaningful improvements in model\nreliability under adversarial and common corruption scenarios."
    },
    {
        "date": "2025-09",
        "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions",
        "author": "Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, and Dapeng Oliver Wu",
        "link": "http://arxiv.org/abs/2509.19165v1",
        "abstract": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}."
    },
    {
        "date": "2025-09",
        "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
        "author": "Sharan Sahu, and Martin T. Wells",
        "link": "http://arxiv.org/abs/2509.19104v1",
        "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates."
    },
    {
        "date": "2025-09",
        "title": "Algorithms for Adversarially Robust Deep Learning",
        "author": "Alexander Robey",
        "link": "http://arxiv.org/abs/2509.19100v1",
        "abstract": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents."
    },
    {
        "date": "2025-09",
        "title": "Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks",
        "author": "Yang Li, Chenyu Wang, Tingrui Wang, Yongwei Wang, Haonan Li, Zhunga Liu, and Quan Pan",
        "link": "http://arxiv.org/abs/2509.19044v1",
        "abstract": "Black-box adversarial attacks remain challenging due to limited access to\nmodel internals. Existing methods often depend on specific network\narchitectures or require numerous queries, resulting in limited\ncross-architecture transferability and high query costs. To address these\nlimitations, we propose JAD, a latent diffusion model framework for black-box\nadversarial attacks. JAD generates adversarial examples by leveraging a latent\ndiffusion model guided by attention maps distilled from both a convolutional\nneural network (CNN) and a Vision Transformer (ViT) models. By focusing on\nimage regions that are commonly sensitive across architectures, this approach\ncrafts adversarial perturbations that transfer effectively between different\nmodel types. This joint attention distillation strategy enables JAD to be\narchitecture-agnostic, achieving superior attack generalization across diverse\nmodels. Moreover, the generative nature of the diffusion framework yields high\nadversarial sample generation efficiency by reducing reliance on iterative\nqueries. Experiments demonstrate that JAD offers improved attack\ngeneralization, generation efficiency, and cross-architecture transferability\ncompared to existing methods, providing a promising and effective paradigm for\nblack-box adversarial attacks."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
        "author": "Minoo Dolatabadi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, and Mahdi Javanmardi",
        "link": "http://arxiv.org/abs/2509.18954v1",
        "abstract": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness."
    },
    {
        "date": "2025-09",
        "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
        "author": "Hanqing Liu, Jiahuan Long, Junqi Wu, Jiacheng Hou, Huili Tang, Tingsong Jiang, Weien Zhou, and Wen Yao",
        "link": "http://arxiv.org/abs/2509.18953v1",
        "abstract": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges."
    },
    {
        "date": "2025-09",
        "title": "Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM",
        "author": "Yating Liu, Xing Su, Hao Wu, Sijin Li, Yuxi Cheng, Fengyuan Xu, and Sheng Zhong",
        "link": "http://arxiv.org/abs/2509.18934v1",
        "abstract": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum\nand BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts\ntypically for financial gains. Detecting such malicious contracts at the time\nof deployment is an important proactive strategy preventing loss from victim\ncontracts. It offers a better cost-benefit than detecting vulnerabilities on\ndiverse potential victims. However, existing works are not generic with limited\ndetection types and effectiveness due to imbalanced samples, while the emerging\nLLM technologies, which show its potentials in generalization, have two key\nproblems impeding its application in this task: hard digestion of compiled-code\ninputs, especially those with task-specific logic, and hard assessment of LLMs'\ncertainty in their binary answers, i.e., yes-or-no answers. Therefore, we\npropose a generic adversarial smart contracts detection framework FinDet, which\nleverages LLMs with two enhancements addressing above two problems. FinDet\ntakes as input only the EVM-bytecode contracts and identifies adversarial ones\namong them with high balanced accuracy. The first enhancement extracts concise\nsemantic intentions and high-level behavioral logic from the low-level bytecode\ninputs, unleashing the LLM reasoning capability restricted by the task input.\nThe second enhancement probes and measures the LLM uncertainty to its\nmulti-round answering to the same query, improving the LLM answering robustness\nfor binary classifications required by the task output. Our comprehensive\nevaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,\nsignificantly outperforming existing baselines. It remains robust under\nchallenging conditions including unseen attack patterns, low-data settings, and\nfeature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial\ncontracts in a 10-day real-world test, confirmed manually."
    },
    {
        "date": "2025-09",
        "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation",
        "author": "Yunzhe Shen, Kai Peng, Leiye Liu, Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu",
        "link": "http://arxiv.org/abs/2509.18912v1",
        "abstract": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper."
    },
    {
        "date": "2025-09",
        "title": "Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction",
        "author": "Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2509.18904v1",
        "abstract": "Federated learning allows multiple participants to collaboratively train a\ncentral model without sharing their private data. However, this distributed\nnature also exposes new attack surfaces. In particular, backdoor attacks allow\nattackers to implant malicious behaviors into the global model while\nmaintaining high accuracy on benign inputs. Existing attacks usually rely on\nfixed patterns or adversarial perturbations as triggers, which tightly couple\nthe main and backdoor tasks. This coupling makes them vulnerable to dilution by\nhonest updates and limits their persistence under federated defenses. In this\nwork, we propose an approach to decouple the backdoor task from the main task\nby dynamically optimizing the backdoor trigger within a min-max framework. The\ninner layer maximizes the performance gap between poisoned and benign samples,\nensuring that the contributions of benign users have minimal impact on the\nbackdoor. The outer process injects the adaptive triggers into the local model.\nWe evaluate our method on both computer vision and natural language tasks, and\ncompare it with six backdoor attack methods under six defense algorithms.\nExperimental results show that our method achieves good attack performance and\ncan be easily integrated into existing backdoor attack techniques."
    },
    {
        "date": "2025-09",
        "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
        "author": "Pengteng Li, Yunfan Lu, Pinhao Song, Weiyu Guo, Huizai Yao, F. Richard Yu, and Hui Xiong",
        "link": "http://arxiv.org/abs/2509.18898v1",
        "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS."
    },
    {
        "date": "2025-09",
        "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model",
        "author": "Xueyu Liu, Xiaoyi Zhang, Guangze Shi, Meilin Liu, Yexin Lai, Yongfei Wu, and Mingqiang Wei",
        "link": "http://arxiv.org/abs/2509.18891v1",
        "abstract": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation."
    },
    {
        "date": "2025-09",
        "title": "Uncovering Privacy Vulnerabilities through Analytical Gradient Inversion Attacks",
        "author": "Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, and Adnan Qayyum",
        "link": "http://arxiv.org/abs/2509.18871v2",
        "abstract": "Federated learning has emerged as a prominent privacy-preserving technique\nfor leveraging large-scale distributed datasets by sharing gradients instead of\nraw data. However, recent studies indicate that private training data can still\nbe exposed through gradient inversion attacks. While earlier analytical methods\nhave demonstrated success in reconstructing input data from fully connected\nlayers, their effectiveness significantly diminishes when applied to\nconvolutional layers, high-dimensional inputs, and scenarios involving multiple\ntraining examples. This paper extends our previous work \\cite{eltaras2024r} and\nproposes three advanced algorithms to broaden the applicability of gradient\ninversion attacks. The first algorithm presents a novel data leakage method\nthat efficiently exploits convolutional layer gradients, demonstrating that\neven with non-fully invertible activation functions, such as ReLU, training\nsamples can be analytically reconstructed directly from gradients without the\nneed to reconstruct intermediate layer outputs. Building on this foundation,\nthe second algorithm extends this analytical approach to support\nhigh-dimensional input data, substantially enhancing its utility across complex\nreal-world datasets. The third algorithm introduces an innovative analytical\nmethod for reconstructing mini-batches, addressing a critical gap in current\nresearch that predominantly focuses on reconstructing only a single training\nexample. Unlike previous studies that focused mainly on the weight constraints\nof convolutional layers, our approach emphasizes the pivotal role of gradient\nconstraints, revealing that successful attacks can be executed with fewer than\n5\\% of the constraints previously deemed necessary in certain layers."
    },
    {
        "date": "2025-09",
        "title": "Centralized vs. Decentralized Security for Space AI Systems? A New Look",
        "author": "Noam Schmitt, and Marc Antoine Lacoste",
        "link": "http://arxiv.org/abs/2509.20395v1",
        "abstract": "This paper investigates the trade-off between centralized and decentralized\nsecurity management in constellations of satellites to balance security and\nperformance. We highlight three key AI architectures for automated security\nmanagement: (a) centralized, (b) distributed and (c) federated. The centralized\narchitecture is the best option short term, providing fast training, despite\nthe hard challenge of the communication latency overhead across space.\nDecentralized architectures are better alternatives in the longer term,\nproviding enhanced scalability and security."
    },
    {
        "date": "2025-09",
        "title": "Security Evaluation of Android apps in budget African Mobile Devices",
        "author": "Alioune Diallo, Anta Diop, Abdoul Kader Kabore, Jordan Samhi, Aleksandr Pilgun, Tegawend\u00e9 F. Bissyande, and Jacque Klein",
        "link": "http://arxiv.org/abs/2509.18800v1",
        "abstract": "Android's open-source nature facilitates widespread smartphone accessibility,\nparticularly in price-sensitive markets. System and vendor applications that\ncome pre-installed on budget Android devices frequently operate with elevated\nprivileges, yet they receive limited independent examination. To address this\ngap, we developed a framework that extracts APKs from physical devices and\napplies static analysis to identify privacy and security issues in embedded\nsoftware. Our study examined 1,544 APKs collected from seven African\nsmartphones. The analysis revealed that 145 applications (9%) disclose\nsensitive data, 249 (16%) expose critical components without sufficient\nsafeguards, and many present additional risks: 226 execute privileged or\ndangerous commands, 79 interact with SMS messages (read, send, or delete), and\n33 perform silent installation operations. We also uncovered a vendor-supplied\npackage that appears to transmit device identifiers and location details to an\nexternal third party. These results demonstrate that pre-installed applications\non widely distributed low-cost devices represent a significant and\nunderexplored threat to user security and privacy."
    },
    {
        "date": "2025-09",
        "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing",
        "author": "Aicha War, Adnan A. Rawass, Abdoul K. Kabore, Jordan Samhi, Jacques Klein, and Tegawende F. Bissyande",
        "link": "http://arxiv.org/abs/2509.18790v1",
        "abstract": "Infrastructure as Code (IaC) automates the provisioning and management of IT\ninfrastructure through scripts and tools, streamlining software deployment.\nPrior studies have shown that IaC scripts often contain recurring security\nmisconfigurations, and several detection and mitigation approaches have been\nproposed. Most of these rely on static analysis, using statistical code\nrepresentations or Machine Learning (ML) classifiers to distinguish insecure\nconfigurations from safe code.\n  In this work, we introduce a novel approach that enhances static analysis\nwith semantic understanding by jointly leveraging natural language and code\nrepresentations. Our method builds on two complementary ML models: CodeBERT, to\ncapture semantics across code and text, and LongFormer, to represent long IaC\nscripts without losing contextual information. We evaluate our approach on\nmisconfiguration datasets from two widely used IaC tools, Ansible and Puppet.\nTo validate its effectiveness, we conduct two ablation studies (removing code\ntext from the natural language input and truncating scripts to reduce context)\nand compare against four large language models (LLMs) and prior work. Results\nshow that semantic enrichment substantially improves detection, raising\nprecision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from\n0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively."
    },
    {
        "date": "2025-09",
        "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins",
        "author": "Aicha War, Serge L. B. Nikiema, Jordan Samhi, Jacques Klein, and Tegawende F. Bissyande",
        "link": "http://arxiv.org/abs/2509.18761v1",
        "abstract": "Infrastructure as Code (IaC) has become essential for modern software\nmanagement, yet security flaws in IaC scripts can have severe consequences, as\nexemplified by the recurring exploits of Cloud Web Services. Prior work has\nrecognized the need to build a precise taxonomy of security smells in IaC\nscripts as a first step towards developing approaches to improve IaC security.\nThis first effort led to the unveiling of seven sins, limited by the focus on a\nsingle IaC tool as well as by the extensive, and potentially biased, manual\neffort that was required. We propose, in our work, to revisit this taxonomy:\nfirst, we extend the study of IaC security smells to a more diverse dataset\nwith scripts associated with seven popular IaC tools, including Terraform,\nAnsible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some\nautomation for the analysis by relying on an LLM. While we leverage LLMs for\ninitial pattern processing, all taxonomic decisions underwent systematic human\nvalidation and reconciliation with established security standards. Our study\nyields a comprehensive taxonomy of 62 security smell categories, significantly\nexpanding beyond the previously known seven. We demonstrate actionability by\nimplementing new security checking rules within linters for seven popular IaC\ntools, often achieving 1.00 precision score. Our evolution study of security\nsmells in GitHub projects reveals that these issues persist for extended\nperiods, likely due to inadequate detection and mitigation tools. This work\nprovides IaC practitioners with insights for addressing common security smells\nand systematically adopting DevSecOps practices to build safer infrastructure\ncode."
    },
    {
        "date": "2025-09",
        "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing",
        "author": "Susmit Neogi",
        "link": "http://arxiv.org/abs/2509.18743v1",
        "abstract": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning."
    }
]