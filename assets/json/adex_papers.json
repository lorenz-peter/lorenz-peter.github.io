[
    {
        "date": "2025-05",
        "title": "FMG-Det: Foundation Model Guided Robust Object Detection",
        "author": "Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, and Yijing Watkins",
        "link": "http://arxiv.org/abs/2505.23726v1",
        "abstract": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches."
    },
    {
        "date": "2025-05",
        "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
        "author": "Utku Demir, Yalin E. Sagduyu, Tugba Erpek, Hossein Jafari, Sastry Kompella, and Mengran Xue",
        "link": "http://arxiv.org/abs/2505.23706v1",
        "abstract": "In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks."
    },
    {
        "date": "2025-05",
        "title": "Keyed Chaotic Masking: A Functional Privacy Framework for Neural Inference",
        "author": "Peter David Fagan",
        "link": "http://arxiv.org/abs/2505.23655v2",
        "abstract": "This work introduces a lightweight framework for privacy-preserving neural\nnetwork inference based on keyed chaotic masking a deterministic, user-specific\nobfuscation method derived from cryptographically seeded chaotic dynamical\nsystems. The approach applies masks to input and output tensors using\nkey-conditioned graph dynamics, enabling authenticated inference, user\nattribution, and soft output watermarking without modifying model\narchitectures. While the underlying chaotic system used to generate each mask\nis not analytically invertible, the masking operation itself is algebraically\nreversible by authorized key holders, offering functional privacy without\nformal cryptographic guarantees. Unlike traditional encryption or secure\nmulti-party computation, this method operates in continuous space and imposes\nminimal computational overhead. We describe the construction of the masking\nsystem, including graph sampling, dynamical rule selection, and chaos\ndiagnostics. Applications include privacy-preserving inference, secure data\ncontribution, and per-user watermarking in shared model pipelines. This\nframework offers a practical and modular building block for user-controlled\nprivacy in modern AI systems."
    },
    {
        "date": "2025-05",
        "title": "Securing AI Agents with Information-Flow Control",
        "author": "Manuel Costa, Boris K\u00f6pf, Aashish Kolluri, Andrew Paverd, Mark Russinovich, Ahmed Salem, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-B\u00e9guelin",
        "link": "http://arxiv.org/abs/2505.23643v1",
        "abstract": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides"
    },
    {
        "date": "2025-05",
        "title": "DRO: A Python Library for Distributionally Robust Optimization in Machine Learning",
        "author": "Jiashuo Liu, Tianyu Wang, Henry Lam, Hongseok Namkoong, and Jose Blanchet",
        "link": "http://arxiv.org/abs/2505.23565v1",
        "abstract": "We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org."
    },
    {
        "date": "2025-05",
        "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
        "author": "Zenghui Yuan, Yangming Xu, Jiawen Shi, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.23561v1",
        "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning)."
    },
    {
        "date": "2025-05",
        "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
        "author": "Ahmad Mohsin, Helge Janicke, Ahmed Ibrahim, Iqbal H. Sarker, and Seyit Camtepe",
        "link": "http://arxiv.org/abs/2505.23397v1",
        "abstract": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making."
    },
    {
        "date": "2025-05",
        "title": "Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment",
        "author": "Yun-Cheng Tsai",
        "link": "http://arxiv.org/abs/2505.23392v1",
        "abstract": "Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment."
    },
    {
        "date": "2025-05",
        "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis",
        "author": "Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang",
        "link": "http://arxiv.org/abs/2505.23325v1",
        "abstract": "Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition",
        "author": "Weizhe Kong, Xiao Wang, Ruichong Gao, Chenglong Li, Yu Zhang, Xing Yang, Yaowei Wang, and Jin Tang",
        "link": "http://arxiv.org/abs/2505.23313v1",
        "abstract": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR."
    },
    {
        "date": "2025-05",
        "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
        "author": "Chunlong Xie, Jialing He, Shangwei Guo, Jiacheng Wang, Shudong Zhang, Tianwei Zhang, and Tao Xiang",
        "link": "http://arxiv.org/abs/2505.23266v1",
        "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
        "author": "Zhaokai Sun, Li Zhang, Qing Wang, Pan Zhou, and Lei Xie",
        "link": "http://arxiv.org/abs/2505.23207v1",
        "abstract": "Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD."
    },
    {
        "date": "2025-05",
        "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
        "author": "Run Hao, and Peng Ying",
        "link": "http://arxiv.org/abs/2505.23192v1",
        "abstract": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems."
    },
    {
        "date": "2025-05",
        "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
        "author": "Junyan Liu, Arnab Maiti, Artin Tajdini, Kevin Jamieson, and Lillian J. Ratliff",
        "link": "http://arxiv.org/abs/2505.23124v1",
        "abstract": "We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round."
    },
    {
        "date": "2025-05",
        "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
        "author": "Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, and Yanmin Qian",
        "link": "http://arxiv.org/abs/2505.23049v1",
        "abstract": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator."
    },
    {
        "date": "2025-05",
        "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
        "author": "Minh Nguyen Nhat To, Paul F RWilson, Viet Nguyen, Mohamed Harmanani, Michael Cooper, Fahimeh Fooladgar, Purang Abolmaesumi, Parvin Mousavi, and Rahul G. Krishnan",
        "link": "http://arxiv.org/abs/2505.23027v1",
        "abstract": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop"
    },
    {
        "date": "2025-05",
        "title": "Context Robust Knowledge Editing for Language Models",
        "author": "Haewon Park, Gyubin Choi, Minjun Kim, and Yohan Jo",
        "link": "http://arxiv.org/abs/2505.23026v1",
        "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success."
    },
    {
        "date": "2025-05",
        "title": "Hybrid Cross-domain Robust Reinforcement Learning",
        "author": "Linh Le Pham Van, Minh Hoang Nguyen, Hung Le, Hung The Tran, and Sunil Gupta",
        "link": "http://arxiv.org/abs/2505.23003v1",
        "abstract": "Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL."
    },
    {
        "date": "2025-05",
        "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates",
        "author": "Jaewoo Ahn, Heeseung Yun, Dayoon Ko, and Gunhee Kim",
        "link": "http://arxiv.org/abs/2505.22943v1",
        "abstract": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios."
    },
    {
        "date": "2025-05",
        "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging",
        "author": "Haobo Zhang, and Jiayu Zhou",
        "link": "http://arxiv.org/abs/2505.22934v1",
        "abstract": "Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models."
    },
    {
        "date": "2025-05",
        "title": "Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints",
        "author": "Hyungki Im, Wyame Benslimane, and Paul Grigas",
        "link": "http://arxiv.org/abs/2505.22881v1",
        "abstract": "We study an extension of contextual stochastic linear optimization (CSLO)\nthat, in contrast to most of the existing literature, involves inequality\nconstraints that depend on uncertain parameters predicted by a machine learning\nmodel. To handle the constraint uncertainty, we use contextual uncertainty sets\nconstructed via methods like conformal prediction. Given a contextual\nuncertainty set method, we introduce the \"Smart Predict-then-Optimize with\nRobust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the\nSPO loss that measures decision error of predicted objective parameters. We\nalso introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with\nSPO-RC. To enhance performance, we train on truncated datasets where true\nconstraint parameters lie within the uncertainty sets, and we correct the\ninduced sample selection bias using importance reweighting techniques. Through\nexperiments on fractional knapsack and alloy production problem instances, we\ndemonstrate that SPO-RC+ effectively handles uncertainty in constraints and\nthat combining truncation with importance reweighting can further improve\nperformance."
    },
    {
        "date": "2025-05",
        "title": "Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment",
        "author": "Krti Tallam, and Emma Miller",
        "link": "http://arxiv.org/abs/2505.22852v1",
        "abstract": "CaMeL (Capabilities for Machine Learning) introduces a capability-based\nsandbox to mitigate prompt injection attacks in large language model (LLM)\nagents. While effective, CaMeL assumes a trusted user prompt, omits\nside-channel concerns, and incurs performance tradeoffs due to its dual-LLM\ndesign. This response identifies these issues and proposes engineering\nimprovements to expand CaMeL's threat coverage and operational usability. We\nintroduce: (1) prompt screening for initial inputs, (2) output auditing to\ndetect instruction leakage, (3) a tiered-risk access model to balance usability\nand control, and (4) a verified intermediate language for formal guarantees.\nTogether, these upgrades align CaMeL with best practices in enterprise security\nand support scalable deployment."
    },
    {
        "date": "2025-05",
        "title": "Security Benefits and Side Effects of Labeling AI-Generated Images",
        "author": "Sandra H\u00f6ltervennhoff, Jonas Ricker, Maike M. Raphael, Charlotte Schwedes, Rebecca Weil, Asja Fischer, Thorsten Holz, Lea Sch\u00f6nherr, and Sascha Fahl",
        "link": "http://arxiv.org/abs/2505.22845v1",
        "abstract": "Generative artificial intelligence is developing rapidly, impacting humans'\ninteraction with information and digital media. It is increasingly used to\ncreate deceptively realistic misinformation, so lawmakers have imposed\nregulations requiring the disclosure of AI-generated content. However, only\nlittle is known about whether these labels reduce the risks of AI-generated\nmisinformation.\n  Our work addresses this research gap. Focusing on AI-generated images, we\nstudy the implications of labels, including the possibility of mislabeling.\nAssuming that simplicity, transparency, and trust are likely to impact the\nsuccessful adoption of such labels, we first qualitatively explore users'\nopinions and expectations of AI labeling using five focus groups. Second, we\nconduct a pre-registered online survey with over 1300 U.S. and EU participants\nto quantitatively assess the effect of AI labels on users' ability to recognize\nmisinformation containing either human-made or AI-generated images. Our focus\ngroups illustrate that, while participants have concerns about the practical\nimplementation of labeling, they consider it helpful in identifying\nAI-generated images and avoiding deception. However, considering security\nbenefits, our survey revealed an ambiguous picture, suggesting that users might\nover-rely on labels. While inaccurate claims supported by labeled AI-generated\nimages were rated less credible than those with unlabeled AI-images, the belief\nin accurate claims also decreased when accompanied by a labeled AI-generated\nimage. Moreover, we find the undesired side effect that human-made images\nconveying inaccurate claims were perceived as more credible in the presence of\nlabels."
    },
    {
        "date": "2025-05",
        "title": "How Do Diffusion Models Improve Adversarial Robustness?",
        "author": "Liu Yuezhang, and Xue-Xin Wei",
        "link": "http://arxiv.org/abs/2505.22839v1",
        "abstract": "Recent findings suggest that diffusion models significantly enhance empirical\nadversarial robustness. While some intuitive explanations have been proposed,\nthe precise mechanisms underlying these improvements remain unclear. In this\nwork, we systematically investigate how and how well diffusion models improve\nadversarial robustness. First, we observe that diffusion models intriguingly\nincrease, rather than decrease, the $\\ell_p$ distance to clean\nsamples--challenging the intuition that purification denoises inputs closer to\nthe original data. Second, we find that the purified images are heavily\ninfluenced by the internal randomness of diffusion models, where a compression\neffect arises within each randomness configuration. Motivated by this\nobservation, we evaluate robustness under fixed randomness and find that the\nimprovement drops to approximately 24% on CIFAR-10--substantially lower than\nprior reports approaching 70%. Importantly, we show that this remaining\nrobustness gain strongly correlates with the model's ability to compress the\ninput space, revealing the compression rate as a reliable robustness indicator\nwithout requiring gradient-based analysis. Our findings provide novel insights\ninto the mechanisms underlying diffusion-based purification, and offer guidance\nfor developing more effective and principled adversarial purification systems."
    },
    {
        "date": "2025-05",
        "title": "Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook",
        "author": "Banafsheh Saber Latibari, Najmeh Nazari, Avesta Sasan, Houman Homayoun, Pratik Satam, Soheil Salehi, and Hossein Sayadi",
        "link": "http://arxiv.org/abs/2505.22605v1",
        "abstract": "The rise of hardware-level security threats, such as side-channel attacks,\nhardware Trojans, and firmware vulnerabilities, demands advanced detection\nmechanisms that are more intelligent and adaptive. Traditional methods often\nfall short in addressing the complexity and evasiveness of modern attacks,\ndriving increased interest in machine learning-based solutions. Among these,\nTransformer models, widely recognized for their success in natural language\nprocessing and computer vision, have gained traction in the security domain due\nto their ability to model complex dependencies, offering enhanced capabilities\nin identifying vulnerabilities, detecting anomalies, and reinforcing system\nintegrity. This survey provides a comprehensive review of recent advancements\non the use of Transformers in hardware security, examining their application\nacross key areas such as side-channel analysis, hardware Trojan detection,\nvulnerability classification, device fingerprinting, and firmware security.\nFurthermore, we discuss the practical challenges of applying Transformers to\nsecure hardware systems, and highlight opportunities and future research\ndirections that position them as a foundation for next-generation\nhardware-assisted security. These insights pave the way for deeper integration\nof AI-driven techniques into hardware security frameworks, enabling more\nresilient and intelligent defenses."
    },
    {
        "date": "2025-05",
        "title": "Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective",
        "author": "Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, and Meng Wang",
        "link": "http://arxiv.org/abs/2505.22604v2",
        "abstract": "Rapid advances in Artificial Intelligence Generated Images (AIGI) have\nfacilitated malicious use, such as forgery and misinformation. Therefore,\nnumerous methods have been proposed to detect fake images. Although such\ndetectors have been proven to be universally vulnerable to adversarial attacks,\ndefenses in this field are scarce. In this paper, we first identify that\nadversarial training (AT), widely regarded as the most effective defense,\nsuffers from performance collapse in AIGI detection. Through an\ninformation-theoretic lens, we further attribute the cause of collapse to\nfeature entanglement, which disrupts the preservation of feature-label mutual\ninformation. Instead, standard detectors show clear feature separation.\nMotivated by this difference, we propose Training-free Robust Detection via\nInformation-theoretic Measures (TRIM), the first training-free adversarial\ndefense for AIGI detection. TRIM builds on standard detectors and quantifies\nfeature shifts using prediction entropy and KL divergence. Extensive\nexperiments across multiple datasets and attacks validate the superiority of\nour TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)\non ProGAN (GenImage), while well maintaining original accuracy."
    },
    {
        "date": "2025-05",
        "title": "Training RL Agents for Multi-Objective Network Defense Tasks",
        "author": "Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, and Ahmed Ridley",
        "link": "http://arxiv.org/abs/2505.22531v1",
        "abstract": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work."
    },
    {
        "date": "2025-05",
        "title": "IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas",
        "author": "Agnideep Aich, Ashit Baran Aich, and Bruce Wade",
        "link": "http://arxiv.org/abs/2505.22518v1",
        "abstract": "Parameter estimation for Archimedean copulas remains a challenging problem,\nparticularly for the recently developed A1 and A2 families that exhibit complex\ndependency structures. Traditional methods, such as the Method of Moments\n(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood\n(MPL), often struggle due to issues of non-monotonic relationship with\ndependency measures such as Kendall's tau (as in the case of A1) and numerical\ninstability. In this paper, we present the IGNIS Network, a novel, unified\nneural framework that learns a direct mapping from observable dependency\nmeasures to copula parameters, thereby overcoming the limitations of classical\napproaches. Our approach is trained on simulated data spanning five Archimedean\ncopula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its\ngeneral applicability across the entire family. Extensive simulation studies\ndemonstrate that the IGNIS Network reduces estimation errors compared to MoM,\nwhile inherently enforcing parameter constraints through theory-guided\npost-processing. We further validate the practical utility of our method on\ndiverse real-world datasets, including financial returns (AAPL-MSFT),\nhealthcare metrics (CDC Diabetes indicators), and environmental measurements\n(PM2.5 air quality). Our results underscore the transformative potential of\nneural methods for robust and accurate dependence modeling in modern\napplications."
    },
    {
        "date": "2025-05",
        "title": "The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector",
        "author": "Aixuan Li, Mochu Xiang, Jing Zhang, and Yuchao Dai",
        "link": "http://arxiv.org/abs/2505.22499v2",
        "abstract": "3D object detection is a critical component in autonomous driving systems. It\nallows real-time recognition and detection of vehicles, pedestrians and\nobstacles under varying environmental conditions. Among existing methods, 3D\nobject detection in the Bird's Eye View (BEV) has emerged as the mainstream\nframework. To guarantee a safe, robust and trustworthy 3D object detection, 3D\nadversarial attacks are investigated, where attacks are placed in 3D\nenvironments to evaluate the model performance, e.g. putting a film on a car,\nclothing a pedestrian. The vulnerability of 3D object detection models to 3D\nadversarial attacks serves as an important indicator to evaluate the robustness\nof the model against perturbations. To investigate this vulnerability, we\ngenerate non-invasive 3D adversarial objects tailored for real-world attack\nscenarios. Our method verifies the existence of universal adversarial objects\nthat are spatially consistent across time and camera views. Specifically, we\nemploy differentiable rendering techniques to accurately model the spatial\nrelationship between adversarial objects and the target vehicle. Furthermore,\nwe introduce an occlusion-aware module to enhance visual consistency and\nrealism under different viewpoints. To maintain attack effectiveness across\nmultiple frames, we design a BEV spatial feature-guided optimization strategy.\nExperimental results demonstrate that our approach can reliably suppress\nvehicle predictions from state-of-the-art 3D object detectors, serving as an\nimportant tool to test robustness of 3D object detection models before\ndeployment. Moreover, the generated adversarial objects exhibit strong\ngeneralization capabilities, retaining its effectiveness at various positions\nand distances in the scene."
    },
    {
        "date": "2025-05",
        "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods",
        "author": "Michal Kmicikiewicz, Vincent Fortuin, and Ewa Szczurek",
        "link": "http://arxiv.org/abs/2505.22494v1",
        "abstract": "Designing protein sequences of both high fitness and novelty is a challenging\ntask in data-efficient protein engineering. Exploration beyond wild-type\nneighborhoods often leads to biologically implausible sequences or relies on\nsurrogate models that lose fidelity in novel regions. Here, we propose\nProSpero, an active learning framework in which a frozen pre-trained generative\nmodel is guided by a surrogate updated from oracle feedback. By integrating\nfitness-relevant residue selection with biologically-constrained Sequential\nMonte Carlo sampling, our approach enables exploration beyond wild-type\nneighborhoods while preserving biological plausibility. We show that our\nframework remains effective even when the surrogate is misspecified. ProSpero\nconsistently outperforms or matches existing methods across diverse protein\nengineering tasks, retrieving sequences of both high fitness and novelty."
    },
    {
        "date": "2025-05",
        "title": "Understanding Adversarial Training with Energy-based Models",
        "author": "Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.22486v1",
        "abstract": "We aim at using Energy-based Model (EBM) framework to better understand\nadversarial training (AT) in classifiers, and additionally to analyze the\nintrinsic generative capabilities of robust classifiers. By viewing standard\nclassifiers through an energy lens, we begin by analyzing how the energies of\nadversarial examples, generated by various attacks, differ from those of the\nnatural samples. The central focus of our work is to understand the critical\nphenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT\nfrom an energy perspective. We analyze the impact of existing AT approaches on\nthe energy of samples during training and observe that the behavior of the\n``delta energy' -- change in energy between original sample and its adversarial\ncounterpart -- diverges significantly when CO or RO occurs. After a thorough\nanalysis of these energy dynamics and their relationship with overfitting, we\npropose a novel regularizer, the Delta Energy Regularizer (DER), designed to\nsmoothen the energy landscape during training. We demonstrate that DER is\neffective in mitigating both CO and RO across multiple benchmarks. We further\nshow that robust classifiers, when being used as generative models, have limits\nin handling trade-off between image quality and variability. We propose an\nimproved technique based on a local class-wise principal component analysis\n(PCA) and energy-based guidance for better class-specific initialization and\nadaptive stopping, enhancing sample diversity and generation quality.\nConsidering that we do not explicitly train for generative modeling, we achieve\na competitive Inception Score (IS) and Fr\\'echet inception distance (FID)\ncompared to hybrid discriminative-generative models."
    },
    {
        "date": "2025-05",
        "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models",
        "author": "Yongcan Yu, Yanbo Wang, Ran He, and Jian Liang",
        "link": "http://arxiv.org/abs/2505.22271v1",
        "abstract": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."
    },
    {
        "date": "2025-05",
        "title": "Accountable, Scalable and DoS-resilient Secure Vehicular Communication",
        "author": "Hongyu Jin, and Panos Papadimitratos",
        "link": "http://arxiv.org/abs/2505.22162v1",
        "abstract": "Paramount to vehicle safety, broadcasted Cooperative Awareness Messages\n(CAMs) and Decentralized Environmental Notification Messages (DENMs) are\npseudonymously authenticated for security and privacy protection, with each\nnode needing to have all incoming messages validated within an expiration\ndeadline. This creates an asymmetry that can be easily exploited by external\nadversaries to launch a clogging Denial of Service (DoS) attack: each forged VC\nmessage forces all neighboring nodes to cryptographically validate it; at\nincreasing rates, easy to generate forged messages gradually exhaust processing\nresources and severely degrade or deny timely validation of benign CAMs/DENMs.\nThe result can be catastrophic when awareness of neighbor vehicle positions or\ncritical reports are missed. We address this problem making the standardized VC\npseudonymous authentication DoS-resilient. We propose efficient cryptographic\nconstructs, which we term message verification facilitators, to prioritize\nprocessing resources for verification of potentially valid messages among bogus\nmessages and verify multiple messages based on one signature verification. Any\nmessage acceptance is strictly based on public-key based message\nauthentication/verification for accountability, i.e., non-repudiation is not\nsacrificed, unlike symmetric key based approaches. This further enables drastic\nmisbehavior detection, also exploiting the newly introduced facilitators, based\non probabilistic signature verification and cross-checking over multiple\nfacilitators verifying the same message; while maintaining verification latency\nlow even when under attack, trading off modest communication overhead. Our\nfacilitators can also be used for efficient discovery and verification of DENM\nor any event-driven message, including misbehavior evidence used for our\nscheme."
    },
    {
        "date": "2025-05",
        "title": "Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance",
        "author": "Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, and Zhenyu He",
        "link": "http://arxiv.org/abs/2505.22154v1",
        "abstract": "RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images\nto complement RGB data, improving robustness in challenging conditions.\nTraditional RGB-T detectors assume balanced training data, where both\nmodalities contribute equally. However, in real-world scenarios, modality\ndegradation-due to environmental factors or technical issues-can lead to\nextreme modality imbalance, causing out-of-distribution (OOD) issues during\ntesting and disrupting model convergence during training. This paper addresses\nthese challenges by proposing a novel base-and-auxiliary detector architecture.\nWe introduce a modality interaction module to adaptively weigh modalities based\non their quality and handle imbalanced samples effectively. Additionally, we\nleverage modality pseudo-degradation to simulate real-world imbalances in\ntraining data. The base detector, trained on high-quality pairs, provides a\nconsistency constraint for the auxiliary detector, which receives degraded\nsamples. This framework enhances model robustness, ensuring reliable\nperformance even under severe modality degradation. Experimental results\ndemonstrate the effectiveness of our method in handling extreme modality\nimbalances~(decreasing the Missing Rate by 55%) and improving performance\nacross various baseline detectors."
    },
    {
        "date": "2025-05",
        "title": "Securing the Software Package Supply Chain for Critical Systems",
        "author": "Ritwik Murali, and Akash Ravi",
        "link": "http://arxiv.org/abs/2505.22023v1",
        "abstract": "Software systems have grown as an indispensable commodity used across various\nindustries, and almost all essential services depend on them for effective\noperation. The software is no longer an independent or stand-alone piece of\ncode written by a developer but rather a collection of packages designed by\nmultiple developers across the globe. Ensuring the reliability and resilience\nof these systems is crucial since emerging threats target software supply\nchains, as demonstrated by the widespread SolarWinds hack in late 2020. These\nsupply chains extend beyond patches and updates, involving distribution\nnetworks throughout the software lifecycle. Industries like smart grids,\nmanufacturing, healthcare, and finance rely on interconnected software systems\nand their dependencies for effective functioning. To secure software modules\nand add-ons, robust distribution architectures are essential. The proposed\nchapter enhances the existing delivery frameworks by including a permissioned\nledger with Proof of Authority consensus and multi-party signatures. The\nproposed system aims to prevent attacks while permitting every stakeholder to\nverify the same. Critical systems can interface with the secure pipeline\nwithout disrupting existing functionalities, thus preventing the cascading\neffect of an attack at any point in the supply chain."
    },
    {
        "date": "2025-05",
        "title": "GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement",
        "author": "Zhihong Tang, and Yang Li",
        "link": "http://arxiv.org/abs/2505.22021v1",
        "abstract": "Document Image Enhancement (DIE) serves as a critical component in Document\nAI systems, where its performance substantially determines the effectiveness of\ndownstream tasks. To address the limitations of existing methods confined to\nsingle-degradation restoration or grayscale image processing, we present Global\nwith Local Parametric Generation Enhancement Network (GL-PGENet), a novel\narchitecture designed for multi-degraded color document images, ensuring both\nefficiency and robustness in real-world scenarios. Our solution incorporates\nthree key innovations: First, a hierarchical enhancement framework that\nintegrates global appearance correction with local refinement, enabling\ncoarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network\nwith parametric generation mechanisms that replaces conventional direct\nprediction, producing enhanced outputs through learned intermediate parametric\nrepresentations rather than pixel-wise mapping. This approach enhances local\nconsistency while improving model generalization. Finally, a modified NestUNet\narchitecture incorporating dense block to effectively fuse low-level pixel\nfeatures and high-level semantic features, specifically adapted for document\nimage characteristics. In addition, to enhance generalization performance, we\nadopt a two-stage training strategy: large-scale pretraining on a synthetic\ndataset of 500,000+ samples followed by task-specific fine-tuning. Extensive\nexperiments demonstrate the superiority of GL-PGENet, achieving\nstate-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The\nmodel also exhibits remarkable cross-domain adaptability and maintains\ncomputational efficiency for high-resolution images without performance\ndegradation, confirming its practical utility in real-world scenarios."
    },
    {
        "date": "2025-05",
        "title": "Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection",
        "author": "Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, and Jinhang Zuo",
        "link": "http://arxiv.org/abs/2505.21938v1",
        "abstract": "Adversarial attacks on stochastic bandits have traditionally relied on some\nunrealistic assumptions, such as per-round reward manipulation and unbounded\nperturbations, limiting their relevance to real-world systems. We propose a\nmore practical threat model, Fake Data Injection, which reflects realistic\nadversarial constraints: the attacker can inject only a limited number of\nbounded fake feedback samples into the learner's history, simulating legitimate\ninteractions. We design efficient attack strategies under this model,\nexplicitly addressing both magnitude constraints (on reward values) and\ntemporal constraints (on when and how often data can be injected). Our\ntheoretical analysis shows that these attacks can mislead both Upper Confidence\nBound (UCB) and Thompson Sampling algorithms into selecting a target arm in\nnearly all rounds while incurring only sublinear attack cost. Experiments on\nsynthetic and real-world datasets validate the effectiveness of our strategies,\nrevealing significant vulnerabilities in widely used stochastic bandit\nalgorithms under practical adversarial scenarios."
    },
    {
        "date": "2025-05",
        "title": "Evaluating the Retrieval Robustness of Large Language Models",
        "author": "Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, and Shiyue Zhang",
        "link": "http://arxiv.org/abs/2505.21870v1",
        "abstract": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."
    },
    {
        "date": "2025-05",
        "title": "Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification",
        "author": "Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, and Chongshou Li",
        "link": "http://arxiv.org/abs/2505.21854v1",
        "abstract": "Gradient-based adversarial attacks have become a dominant approach for\nevaluating the robustness of point cloud classification models. However,\nexisting methods often rely on uniform update rules that fail to consider the\nheterogeneous nature of point clouds, resulting in excessive and perceptible\nperturbations. In this paper, we rethink the design of gradient-based attacks\nby analyzing the limitations of conventional gradient update mechanisms and\npropose two new strategies to improve both attack effectiveness and\nimperceptibility. First, we introduce WAAttack, a novel framework that\nincorporates weighted gradients and an adaptive step-size strategy to account\nfor the non-uniform contribution of points during optimization. This approach\nenables more targeted and subtle perturbations by dynamically adjusting updates\naccording to the local structure and sensitivity of each point. Second, we\npropose SubAttack, a complementary strategy that decomposes the point cloud\ninto subsets and focuses perturbation efforts on structurally critical regions.\nTogether, these methods represent a principled rethinking of gradient-based\nadversarial attacks for 3D point cloud classification. Extensive experiments\ndemonstrate that our approach outperforms state-of-the-art baselines in\ngenerating highly imperceptible adversarial examples. Code will be released\nupon paper acceptance."
    },
    {
        "date": "2025-05",
        "title": "An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints",
        "author": "Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, and Honghao Wei",
        "link": "http://arxiv.org/abs/2505.21841v1",
        "abstract": "Online safe reinforcement learning (RL) plays a key role in dynamic\nenvironments, with applications in autonomous driving, robotics, and\ncybersecurity. The objective is to learn optimal policies that maximize rewards\nwhile satisfying safety constraints modeled by constrained Markov decision\nprocesses (CMDPs). Existing methods achieve sublinear regret under stochastic\nconstraints but often fail in adversarial settings, where constraints are\nunknown, time-varying, and potentially adversarially designed. In this paper,\nwe propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the\nfirst to address online CMDPs with anytime adversarial constraints. OMDPD\nachieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))\nwithout relying on Slater's condition or the existence of a strictly known safe\npolicy. We further show that access to accurate estimates of rewards and\ntransitions can further improve these bounds. Our results offer practical\nguarantees for safe decision-making in adversarial environments."
    },
    {
        "date": "2025-05",
        "title": "Faster Rates for Private Adversarial Bandits",
        "author": "Hilal Asi, Vinod Raman, and Kunal Talwar",
        "link": "http://arxiv.org/abs/2505.21790v1",
        "abstract": "We design new differentially private algorithms for the problems of\nadversarial bandits and bandits with expert advice. For adversarial bandits, we\ngive a simple and efficient conversion of any non-private bandit algorithm to a\nprivate bandit algorithm. Instantiating our conversion with existing\nnon-private bandit algorithms gives a regret upper bound of\n$O\\left(\\frac{\\sqrt{KT}}{\\sqrt{\\epsilon}}\\right)$, improving upon the existing\nupper bound $O\\left(\\frac{\\sqrt{KT \\log(KT)}}{\\epsilon}\\right)$ for all\n$\\epsilon \\leq 1$. In particular, our algorithms allow for sublinear expected\nregret even when $\\epsilon \\leq \\frac{1}{\\sqrt{T}}$, establishing the first\nknown separation between central and local differential privacy for this\nproblem. For bandits with expert advice, we give the first differentially\nprivate algorithms, with expected regret\n$O\\left(\\frac{\\sqrt{NT}}{\\sqrt{\\epsilon}}\\right),\nO\\left(\\frac{\\sqrt{KT\\log(N)}\\log(KT)}{\\epsilon}\\right)$, and\n$\\tilde{O}\\left(\\frac{N^{1/6}K^{1/2}T^{2/3}\\log(NT)}{\\epsilon ^{1/3}} +\n\\frac{N^{1/2}\\log(NT)}{\\epsilon}\\right)$, where $K$ and $N$ are the number of\nactions and experts respectively. These rates allow us to get sublinear regret\nfor different combinations of small and large $K, N$ and $\\epsilon.$"
    },
    {
        "date": "2025-05",
        "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
        "author": "Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira",
        "link": "http://arxiv.org/abs/2505.21755v1",
        "abstract": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."
    },
    {
        "date": "2025-05",
        "title": "What is Adversarial Training for Diffusion Models?",
        "author": "Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, and Iacopo Masi",
        "link": "http://arxiv.org/abs/2505.21742v1",
        "abstract": "We answer the question in the title, showing that adversarial training (AT)\nfor diffusion models (DMs) fundamentally differs from classifiers: while AT in\nclassifiers enforces output invariance, AT in DMs requires equivariance to keep\nthe diffusion process aligned with the data distribution. AT is a way to\nenforce smoothness in the diffusion flow, improving robustness to outliers and\ncorrupted data. Unlike prior art, our method makes no assumptions about the\nnoise model and integrates seamlessly into diffusion training by adding random\nnoise, similar to randomized smoothing, or adversarial noise, akin to AT. This\nenables intrinsic capabilities such as handling noisy data, dealing with\nextreme variability such as outliers, preventing memorization, and improving\nrobustness. We rigorously evaluate our approach with proof-of-concept datasets\nwith known distributions in low- and high-dimensional space, thereby taking a\nperfect measure of errors; we further evaluate on standard benchmarks such as\nCIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe\nnoise, data corruption, and iterative adversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks",
        "author": "Julia Boone, Tolunay Seyfi, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2505.21703v1",
        "abstract": "Internet of Vehicles (IoV) systems, while offering significant advancements\nin transportation efficiency and safety, introduce substantial security\nvulnerabilities due to their highly interconnected nature. These dynamic\nsystems produce massive amounts of data between vehicles, infrastructure, and\ncloud services and present a highly distributed framework with a wide attack\nsurface. In considering network-centered attacks on IoV systems, attacks such\nas Denial-of-Service (DoS) can prohibit the communication of essential physical\ntraffic safety information between system elements, illustrating that the\nsecurity concerns for these systems go beyond the traditional confidentiality,\nintegrity, and availability concerns of enterprise systems. Given the\ncomplexity and volume of data generated by IoV systems, traditional security\nmechanisms are often inadequate for accurately detecting sophisticated and\nevolving cyberattacks. Here, we present an unsupervised autoencoder method\ntrained entirely on benign network data for the purpose of unseen attack\ndetection in IoV networks. We leverage a weighted combination of reconstruction\nand triplet margin loss to guide the autoencoder training and develop a diverse\nrepresentation of the benign training set. We conduct extensive experiments on\nrecent network intrusion datasets from two different application domains,\nindustrial IoT and home IoT, that represent the modern IoV task. We show that\nour method performs robustly for all unseen attack types, with roughly 99%\naccuracy on benign data and between 97% and 100% performance on anomaly data.\nWe extend these results to show that our model is adaptable through the use of\ntransfer learning, achieving similarly high results while leveraging domain\nfeatures from one domain to another."
    },
    {
        "date": "2025-05",
        "title": "Expert Survey: AI Reliability & Security Research Priorities",
        "author": "Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, and Cara Labrador",
        "link": "http://arxiv.org/abs/2505.21664v1",
        "abstract": "Our survey of 53 specialists across 105 AI reliability and security research\nareas identifies the most promising research prospects to guide strategic AI\nR&D investment. As companies are seeking to develop AI systems with broadly\nhuman-level capabilities, research on reliability and security is urgently\nneeded to ensure AI's benefits can be safely and broadly realized and prevent\nsevere harms. This study is the first to quantify expert priorities across a\ncomprehensive taxonomy of AI safety and security research directions and to\nproduce a data-driven ranking of their potential impact. These rankings may\nsupport evidence-based decisions about how to effectively deploy resources\ntoward AI reliability and security research."
    },
    {
        "date": "2025-05",
        "title": "VideoMarkBench: Benchmarking Robustness of Video Watermarking",
        "author": "Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.21620v1",
        "abstract": "The rapid development of video generative models has led to a surge in highly\nrealistic synthetic videos, raising ethical concerns related to disinformation\nand copyright infringement. Recently, video watermarking has been proposed as a\nmitigation strategy by embedding invisible marks into AI-generated videos to\nenable subsequent detection. However, the robustness of existing video\nwatermarking methods against both common and adversarial perturbations remains\nunderexplored. In this work, we introduce VideoMarkBench, the first systematic\nbenchmark designed to evaluate the robustness of video watermarks under\nwatermark removal and watermark forgery attacks. Our study encompasses a\nunified dataset generated by three state-of-the-art video generative models,\nacross three video styles, incorporating four watermarking methods and seven\naggregation strategies used during detection. We comprehensively evaluate 12\ntypes of perturbations under white-box, black-box, and no-box threat models.\nOur findings reveal significant vulnerabilities in current watermarking\napproaches and highlight the urgent need for more robust solutions. Our code is\navailable at https://github.com/zhengyuan-jiang/VideoMarkBench."
    },
    {
        "date": "2025-05",
        "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery",
        "author": "Haowei Wang, Junjie Wang, Xiaojun Jia, Rupeng Zhang, Mingyang Li, Zhe Liu, Yang Liu, and Qing Wang",
        "link": "http://arxiv.org/abs/2505.21499v1",
        "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."
    },
    {
        "date": "2025-05",
        "title": "Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study",
        "author": "Mathew J. Walter, Aaron Barrett, and Kimberly Tam",
        "link": "http://arxiv.org/abs/2505.21609v1",
        "abstract": "Adversarial artificial intelligence (AI) attacks pose a significant threat to\nautonomous transportation, such as maritime vessels, that rely on AI\ncomponents. Malicious actors can exploit these systems to deceive and\nmanipulate AI-driven operations. This paper addresses three critical research\nchallenges associated with adversarial AI: the limited scope of traditional\ndefences, inadequate security metrics, and the need to build resilience beyond\nmodel-level defences. To address these challenges, we propose building defences\nutilising multiple inputs and data fusion to create defensive components and an\nAI security metric as a novel approach toward developing more secure AI\nsystems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,\nand we evaluate it through real-world demonstrations and comprehensive\nquantitative analyses, comparing a system built with the DFCR method against\nsingle-input models and models utilising existing state-of-the-art defences.\nThe findings show that the DFCR approach significantly enhances resilience\nagainst adversarial machine learning attacks in maritime autonomous system\noperations, achieving up to a 35\\% reduction in loss for successful\nmulti-pronged perturbation attacks, up to a 100\\% reduction in loss for\nsuccessful adversarial patch attacks and up to 100\\% reduction in loss for\nsuccessful spoofing attacks when using these more resilient systems. We\ndemonstrate how DFCR and DFCR confidence scores can reduce adversarial AI\ncontact confidence and improve decision-making by the system, even when typical\nadversarial defences have been compromised. Ultimately, this work contributes\nto the development of more secure and resilient AI-driven systems against\nadversarial attacks."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, and Yang Liu",
        "link": "http://arxiv.org/abs/2505.21494v1",
        "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."
    },
    {
        "date": "2025-05",
        "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive Logic Programming",
        "author": "Yang Yang, Jiemin Wu, and Yutao Yue",
        "link": "http://arxiv.org/abs/2505.21486v1",
        "abstract": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation."
    },
    {
        "date": "2025-05",
        "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment",
        "author": "Brett Bissey, Kyle Gatesman, Walker Dimon, Mohammad Alam, Luis Robaina, and Joseph Weissman",
        "link": "http://arxiv.org/abs/2505.21414v1",
        "abstract": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments."
    },
    {
        "date": "2025-05",
        "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling",
        "author": "Hovhannes Tamoyan, Subhabrata Dutta, and Iryna Gurevych",
        "link": "http://arxiv.org/abs/2505.21399v1",
        "abstract": "Factual incorrectness in generated content is one of the primary concerns in\nubiquitous deployment of large language models (LLMs). Prior findings suggest\nLLMs can (sometimes) detect factual incorrectness in their generated content\n(i.e., fact-checking post-generation). In this work, we provide evidence\nsupporting the presence of LLMs' internal compass that dictate the correctness\nof factual recall at the time of generation. We demonstrate that for a given\nsubject entity and a relation, LLMs internally encode linear features in the\nTransformer's residual stream that dictate whether it will be able to recall\nthe correct attribute (that forms a valid entity-relation-attribute triplet).\nThis self-awareness signal is robust to minor formatting variations. We\ninvestigate the effects of context perturbation via different example selection\nstrategies. Scaling experiments across model sizes and training dynamics\nhighlight that self-awareness emerges rapidly during training and peaks in\nintermediate layers. These findings uncover intrinsic self-monitoring\ncapabilities within LLMs, contributing to their interpretability and\nreliability."
    },
    {
        "date": "2025-05",
        "title": "Square$\u03c7$PO: Differentially Private and Robust $\u03c7^2$-Preference Optimization in Offline Direct Alignment",
        "author": "Xingyu Zhou, Yulian Wu, Wenqian Weng, and Francesco Orabona",
        "link": "http://arxiv.org/abs/2505.21395v1",
        "abstract": "In this paper, we theoretically study the offline alignment of language\nmodels with human preference feedback, under both preference label corruption\nand privacy protections. To this end, we propose Square$\\chi$PO, a simple\none-line change to $\\chi$PO where the standard log-loss is replaced by a new\nsquare loss over probability. Thanks to the inherent properties of this new\nloss, we have advanced the state-of-the-art of differentially private and\nrobust offline direct alignment. Specifically, for the local model of label\nprivacy, Square$\\chi$PO is the first algorithm that attains an optimal rate\nbased on single-policy concentrability even with general function\napproximations. It also gives the first result under the central model of\nprivacy protection over both prompts (responses) and labels. On the robustness\nside against Huber label corruption, Square$\\chi$PO is the first alignment\nmethod that has a meaningful theoretical guarantee under general function\napproximations. More importantly, Square$\\chi$PO can address privacy protection\nand corruption simultaneously, where an interesting separation is observed,\nimplying that the order of privacy and corruption matters. Furthermore, we show\nthat Square$\\chi$PO can also be easily extended to handle the scenario of the\ngeneral preference model with state-of-the-art guarantees under corruption and\nprivacy. Last but not least, all of our theoretical guarantees enjoy a unified\nanalysis, building upon a new result on the generalization error bounds of\nleast-square regression under corruption and privacy constraints, which we\nbelieve is of independent interest to the community."
    },
    {
        "date": "2025-05",
        "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios",
        "author": "Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, and Yueming Jin",
        "link": "http://arxiv.org/abs/2505.21387v1",
        "abstract": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github."
    },
    {
        "date": "2025-05",
        "title": "Subgroups Matter for Robust Bias Mitigation",
        "author": "Anissa Alloula, Charles Jones, Ben Glocker, and Bart\u0142omiej W. Papie\u017c",
        "link": "http://arxiv.org/abs/2505.21363v2",
        "abstract": "Despite the constant development of new bias mitigation methods for machine\nlearning, no method consistently succeeds, and a fundamental question remains\nunanswered: when and why do bias mitigation techniques fail? In this paper, we\nhypothesise that a key factor may be the often-overlooked but crucial step\nshared by many bias mitigation methods: the definition of subgroups. To\ninvestigate this, we conduct a comprehensive evaluation of state-of-the-art\nbias mitigation methods across multiple vision and language classification\ntasks, systematically varying subgroup definitions, including coarse,\nfine-grained, intersectional, and noisy subgroups. Our results reveal that\nsubgroup choice significantly impacts performance, with certain groupings\nparadoxically leading to worse outcomes than no mitigation at all. Our findings\nsuggest that observing a disparity between a set of subgroups is not a\nsufficient reason to use those subgroups for mitigation. Through theoretical\nanalysis, we explain these phenomena and uncover a counter-intuitive insight\nthat, in some cases, improving fairness with respect to a particular set of\nsubgroups is best achieved by using a different set of subgroups for\nmitigation. Our work highlights the importance of careful subgroup definition\nin bias mitigation and presents it as an alternative lever for improving the\nrobustness and fairness of machine learning models."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations",
        "author": "Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, and Yu Tsao",
        "link": "http://arxiv.org/abs/2505.21356v3",
        "abstract": "Perceptual voice quality assessment is essential for diagnosing and\nmonitoring voice disorders by providing standardized evaluations of vocal\nfunction. Traditionally, expert raters use standard scales such as the\nConsensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade,\nRoughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics\nare subjective and prone to inter-rater variability, motivating the need for\nautomated, objective assessment methods. This study proposes Voice Quality\nAssessment Network (VOQANet), a deep learning-based framework with an attention\nmechanism that leverages a Speech Foundation Model (SFM) to extract high-level\nacoustic and prosodic information from raw speech. To enhance robustness and\ninterpretability, we also introduce VOQANet+, which integrates low-level speech\ndescriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with\nSFM embeddings into a hybrid representation. Unlike prior studies focused only\non vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality\nDataset (PVQD), we evaluate our models on both vowel-based and sentence-level\nspeech (PVQD-S subset) to improve generalizability. Results show that\nsentence-based input outperforms vowel-based input, especially at the patient\nlevel, underscoring the value of longer utterances for capturing perceptual\nvoice attributes. VOQANet consistently surpasses baseline methods in root mean\nsquared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V\nand GRBAS dimensions, with VOQANet+ achieving even better performance.\nAdditional experiments under noisy conditions show that VOQANet+ maintains high\nprediction accuracy and robustness, supporting its potential for real-world and\ntelehealth deployment."
    },
    {
        "date": "2025-05",
        "title": "Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space",
        "author": "Yao Huang, Yitong Sun, Shouwei Ruan, Yichi Zhang, Yinpeng Dong, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.21277v2",
        "abstract": "Large Language Models (LLMs), despite advanced general capabilities, still\nsuffer from numerous safety risks, especially jailbreak attacks that bypass\nsafety protocols. Understanding these vulnerabilities through black-box\njailbreak attacks, which better reflect real-world scenarios, offers critical\ninsights into model robustness. While existing methods have shown improvements\nthrough various prompt engineering techniques, their success remains limited\nagainst safety-aligned models, overlooking a more fundamental problem: the\neffectiveness is inherently bounded by the predefined strategy spaces. However,\nexpanding this space presents significant challenges in both systematically\ncapturing essential attack patterns and efficiently navigating the increased\ncomplexity. To better explore the potential of expanding the strategy space, we\naddress these challenges through a novel framework that decomposes jailbreak\nstrategies into essential components based on the Elaboration Likelihood Model\n(ELM) theory and develops genetic-based optimization with intention evaluation\nmechanisms. To be striking, our experiments reveal unprecedented jailbreak\ncapabilities by expanding the strategy space: we achieve over 90% success rate\non Claude-3.5 where prior methods completely fail, while demonstrating strong\ncross-model transferability and surpassing specialized safeguard models in\nevaluation accuracy. The code is open-sourced at:\nhttps://github.com/Aries-iai/CL-GSO."
    },
    {
        "date": "2025-05",
        "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion",
        "author": "Yayin Zheng, Chen Wan, Zihong Guo, Hailing Kuang, and Xiaohai Lu",
        "link": "http://arxiv.org/abs/2505.21181v1",
        "abstract": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels."
    },
    {
        "date": "2025-05",
        "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images",
        "author": "Xurui Li, Zhonesheng Jiang, Tingxuan Ai, and Yu Zhou",
        "link": "http://arxiv.org/abs/2505.21152v1",
        "abstract": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS."
    },
    {
        "date": "2025-05",
        "title": "HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs",
        "author": "Honglin Gao, Xiang Li, Lan Zhao, and Gaoxi Xiao",
        "link": "http://arxiv.org/abs/2505.21140v1",
        "abstract": "Heterogeneous graph neural networks (HGNNs) have recently drawn increasing\nattention for modeling complex multi-relational data in domains such as\nrecommendation, finance, and social networks. While existing research has been\nlargely focused on enhancing HGNNs' predictive performance, their robustness\nand security, especially under backdoor attacks, remain underexplored. In this\npaper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework\nfor node classification tasks on heterogeneous graphs. HeteroBA inserts\ncarefully crafted trigger nodes with realistic features and targeted structural\nconnections, leveraging attention-based and clustering-based strategies to\nselect influential auxiliary nodes for effective trigger propagation, thereby\ncausing the model to misclassify specific nodes into a target label while\nmaintaining accuracy on clean data. Experimental results on three datasets and\nvarious HGNN architectures demonstrate that HeteroBA achieves high attack\nsuccess rates with minimal impact on the clean accuracy. Our method sheds light\non potential vulnerabilities in HGNNs and calls for more robust defenses\nagainst backdoor threats in multi-relational graph scenarios."
    },
    {
        "date": "2025-05",
        "title": "Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach",
        "author": "Subhagata Chattopadhyay, and Amit K Chattopadhyay",
        "link": "http://arxiv.org/abs/2505.21139v1",
        "abstract": "The COVID-19 pandemic has significantly increased the incidence of\npost-infection cardiovascular events, particularly myocardial infarction, in\nindividuals over 40. While the underlying mechanisms remain elusive, this study\nemploys a hybrid machine learning approach to analyze epidemiological data in\nassessing 13 key heart attack risk factors and their susceptibility. Based on a\nunique dataset that combines demographic, biochemical, ECG, and thallium\nstress-tests, this study categorizes distinct subpopulations against varying\nrisk profiles and then divides the population into 'at-risk' (AR) and\n'not-at-risk' (NAR) groups using clustering algorithms. The study reveals\nstrong association between the likelihood of experiencing a heart attack on the\n13 risk factors studied. The aggravated risk for postmenopausal patients\nindicates compromised individual risk factors due to estrogen depletion that\nmay be, further compromised by extraneous stress impacts, like anxiety and\nfear, aspects that have traditionally eluded data modeling predictions."
    },
    {
        "date": "2025-05",
        "title": "Robust and Computation-Aware Gaussian Processes",
        "author": "Marshal Arijona Sinaga, Julien Martinelli, and Samuel Kaski",
        "link": "http://arxiv.org/abs/2505.21133v1",
        "abstract": "Gaussian processes (GPs) are widely used for regression and optimization\ntasks such as Bayesian optimization (BO) due to their expressiveness and\nprincipled uncertainty estimates. However, in settings with large datasets\ncorrupted by outliers, standard GPs and their sparse approximations struggle\nwith computational tractability and robustness. We introduce Robust\nComputation-aware Gaussian Process (RCaGP), a novel GP model that jointly\naddresses these challenges by combining a principled treatment of\napproximation-induced uncertainty with robust generalized Bayesian updating.\nThe key insight is that robustness and approximation-awareness are not\northogonal but intertwined: approximations can exacerbate the impact of\noutliers, and mitigating one without the other is insufficient. Unlike previous\nwork that focuses narrowly on either robustness or approximation quality, RCaGP\ncombines both in a principled and scalable framework, thus effectively managing\nboth outliers and computational uncertainties introduced by approximations such\nas low-rank matrix multiplications. Our model ensures more conservative and\nreliable uncertainty estimates, a property we rigorously demonstrate.\nAdditionally, we establish a robustness property and show that the mean\nfunction is key to preserving it, motivating a tailored model selection scheme\nfor robust mean functions. Empirical results confirm that solving these\nchallenges jointly leads to superior performance across both clean and\noutlier-contaminated settings, both on regression and high-throughput Bayesian\noptimization benchmarks."
    },
    {
        "date": "2025-05",
        "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing",
        "author": "Dehao Wang, Haohang Zhu, Yiwen Xu, and Kaiqi Liu",
        "link": "http://arxiv.org/abs/2505.21049v1",
        "abstract": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability."
    },
    {
        "date": "2025-05",
        "title": "TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data",
        "author": "Zhipeng He, Chun Ouyang, Lijie Wen, Cong Liu, and Catarina Moreira",
        "link": "http://arxiv.org/abs/2505.21027v1",
        "abstract": "Adversarial attacks pose a significant threat to machine learning models by\ninducing incorrect predictions through imperceptible perturbations to input\ndata. While these attacks have been extensively studied in unstructured data\nlike images, their application to tabular data presents new challenges. These\nchallenges arise from the inherent heterogeneity and complex feature\ninterdependencies in tabular data, which differ significantly from those in\nimage data. To address these differences, it is crucial to consider\nimperceptibility as a key criterion specific to tabular data. Most current\nresearch focuses primarily on achieving effective adversarial attacks, often\noverlooking the importance of maintaining imperceptibility. To address this\ngap, we propose a new benchmark for adversarial attacks on tabular data that\nevaluates both effectiveness and imperceptibility. In this study, we assess the\neffectiveness and imperceptibility of five adversarial attacks across four\nmodels using eleven tabular datasets, including both mixed and numerical-only\ndatasets. Our analysis explores how these factors interact and influence the\noverall performance of the attacks. We also compare the results across\ndifferent dataset types to understand the broader implications of these\nfindings. The findings from this benchmark provide valuable insights for\nimproving the design of adversarial attack algorithms, thereby advancing the\nfield of adversarial machine learning on tabular data."
    },
    {
        "date": "2025-05",
        "title": "Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models",
        "author": "Puwei Lian, Yujun Cai, and Songze Li",
        "link": "http://arxiv.org/abs/2505.20955v1",
        "abstract": "Diffusion models have achieved tremendous success in image generation, but\nthey also raise significant concerns regarding privacy and copyright issues.\nMembership Inference Attacks (MIAs) are designed to ascertain whether specific\ndata were utilized during a model's training phase. As current MIAs for\ndiffusion models typically exploit the model's image prediction ability, we\nformalize them into a unified general paradigm which computes the membership\nscore for membership identification. Under this paradigm, we empirically find\nthat existing attacks overlook the inherent deficiency in how diffusion models\nprocess high-frequency information. Consequently, this deficiency leads to\nmember data with more high-frequency content being misclassified as hold-out\ndata, and hold-out data with less high-frequency content tend to be\nmisclassified as member data. Moreover, we theoretically demonstrate that this\ndeficiency reduces the membership advantage of attacks, thereby interfering\nwith the effective discrimination of member data and hold-out data. Based on\nthis insight, we propose a plug-and-play high-frequency filter module to\nmitigate the adverse effects of the deficiency, which can be seamlessly\nintegrated into any attacks within this general paradigm without additional\ntime costs. Extensive experiments corroborate that this module significantly\nimproves the performance of baseline attacks across different datasets and\nmodels."
    },
    {
        "date": "2025-05",
        "title": "NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion",
        "author": "Max Collins, Jordan Vice, Tim French, and Ajmal Mian",
        "link": "http://arxiv.org/abs/2505.20934v1",
        "abstract": "Adversarial samples exploit irregularities in the manifold ``learned'' by\ndeep learning models to cause misclassifications. The study of these\nadversarial samples provides insight into the features a model uses to classify\ninputs, which can be leveraged to improve robustness against future attacks.\nHowever, much of the existing literature focuses on constrained adversarial\nsamples, which do not accurately reflect test-time errors encountered in\nreal-world settings. To address this, we propose `NatADiff', an adversarial\nsampling scheme that leverages denoising diffusion to generate natural\nadversarial samples. Our approach is based on the observation that natural\nadversarial samples frequently contain structural elements from the adversarial\nclass. Deep learning models can exploit these structural elements to shortcut\nthe classification process, rather than learning to genuinely distinguish\nbetween classes. To leverage this behavior, we guide the diffusion trajectory\ntowards the intersection of the true and adversarial classes, combining\ntime-travel sampling with augmented classifier guidance to enhance attack\ntransferability while preserving image fidelity. Our method achieves comparable\nattack success rates to current state-of-the-art techniques, while exhibiting\nsignificantly higher transferability across model architectures and better\nalignment with natural test-time errors as measured by FID. These results\ndemonstrate that NatADiff produces adversarial samples that not only transfer\nmore effectively across models, but more faithfully resemble naturally\noccurring test-time errors."
    },
    {
        "date": "2025-05",
        "title": "Towards a DSL for hybrid secure computation",
        "author": "Romain de Laage",
        "link": "http://arxiv.org/abs/2505.20912v1",
        "abstract": "Fully homomorphic encryption (FHE) and trusted execution environments (TEE)\nare two approaches to provide confidentiality during data processing. Each\napproach has its own strengths and weaknesses. In certain scenarios,\ncomputations can be carried out in a hybrid environment, using both FHE and\nTEE. However, processing data in such hybrid settings presents challenges, as\nit requires to adapt and rewrite the algorithms for the chosen technique. We\npropose a domain-specific language (DSL) for secure computation that allows to\nexpress the computations to perform and execute them using a backend that\nleverages either FHE or TEE, depending on what is available."
    },
    {
        "date": "2025-05",
        "title": "Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties",
        "author": "Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, and Edward Choi",
        "link": "http://arxiv.org/abs/2505.20875v1",
        "abstract": "Large Language Models (LLMs) are predominantly evaluated on Standard American\nEnglish (SAE), often overlooking the diversity of global English varieties.\nThis narrow focus may raise fairness concerns as degraded performance on\nnon-standard varieties can lead to unequal benefits for users worldwide.\nTherefore, it is critical to extensively evaluate the linguistic robustness of\nLLMs on multiple non-standard English varieties. We introduce Trans-EnV, a\nframework that automatically transforms SAE datasets into multiple English\nvarieties to evaluate the linguistic robustness. Our framework combines (1)\nlinguistics expert knowledge to curate variety-specific features and\ntransformation guidelines from linguistic literature and corpora, and (2)\nLLM-based transformations to ensure both linguistic validity and scalability.\nUsing Trans-EnV, we transform six benchmark datasets into 38 English varieties\nand evaluate seven state-of-the-art LLMs. Our results reveal significant\nperformance disparities, with accuracy decreasing by up to 46.3% on\nnon-standard varieties. These findings highlight the importance of\ncomprehensive linguistic robustness evaluation across diverse English\nvarieties. Each construction of Trans-EnV was validated through rigorous\nstatistical testing and consultation with a researcher in the field of second\nlanguage acquisition, ensuring its linguistic validity. Our\n\\href{https://github.com/jiyounglee-0523/TransEnV}{code} and\n\\href{https://huggingface.co/collections/jiyounglee0523/transenv-681eadb3c0c8cf363b363fb1}{datasets}\nare publicly available."
    },
    {
        "date": "2025-05",
        "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks",
        "author": "Ta\u00efga Gon\u00e7alves, Tomo Miyazaki, and Shinichiro Omachi",
        "link": "http://arxiv.org/abs/2505.20782v1",
        "abstract": "We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for\ngenerating adversarial examples that mislead image classifiers toward any\ntarget class, including those not seen during training. Traditional targeted\nattacks are limited to one class per model, requiring expensive retraining for\neach target. Multi-targeted attacks address this by introducing a perturbation\ngenerator with a conditional input to specify the target class. However,\nexisting methods are constrained to classes observed during training and\nrequire access to the black-box model's training data--introducing a form of\ndata leakage that undermines realistic evaluation in practical black-box\nscenarios. We identify overreliance on class embeddings as a key limitation,\nleading to overfitting and poor generalization to unseen classes. To address\nthis, CD-MTA replaces class-level supervision with an image-based conditional\ninput and introduces class-agnostic losses that align the perturbed and target\nimages in the feature space. This design removes dependence on class semantics,\nthereby enabling generalization to unseen classes across datasets. Experiments\non ImageNet and seven other datasets show that CD-MTA outperforms prior\nmulti-targeted attacks in both standard and cross-domain settings--without\naccessing the black-box model's training data."
    },
    {
        "date": "2025-05",
        "title": "Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies",
        "author": "Kohei Obata, Yasuko Matsubara, and Yasushi Sakurai",
        "link": "http://arxiv.org/abs/2505.20765v1",
        "abstract": "Unsupervised anomaly detection in time series has been a pivotal research\narea for decades. Current mainstream approaches focus on learning normality, on\nthe assumption that all or most of the samples in the training set are normal.\nHowever, anomalies in the training set (i.e., anomaly contamination) can be\nmisleading. Recent studies employ data augmentation to generate\npseudo-anomalies and learn the boundary separating the training samples from\nthe augmented samples. Although this approach mitigates anomaly contamination\nif augmented samples mimic unseen real anomalies, it suffers from several\nlimitations. (1) Covering a wide range of time series anomalies is challenging.\n(2) It disregards augmented samples that resemble normal samples (i.e., false\nanomalies). (3) It places too much trust in the labels of training and\naugmented samples. In response, we propose RedLamp, which employs diverse data\naugmentations to generate multiclass pseudo-anomalies and learns the multiclass\nboundary. Such multiclass pseudo-anomalies cover a wide variety of time series\nanomalies. We conduct multiclass classification using soft labels, which\nprevents the model from being overconfident and ensures its robustness against\ncontaminated/false anomalies. The learned latent space is inherently\nexplainable as it is trained to separate pseudo-anomalies into multiclasses.\nExtensive experiments demonstrate the effectiveness of RedLamp in anomaly\ndetection and its robustness against anomaly contamination."
    },
    {
        "date": "2025-05",
        "title": "Adversarial bandit optimization for approximately linear functions",
        "author": "Zhuoyu Cheng, Kohei Hatano, and Eiji Takimoto",
        "link": "http://arxiv.org/abs/2505.20734v1",
        "abstract": "We consider a bandit optimization problem for nonconvex and non-smooth\nfunctions, where in each trial the loss function is the sum of a linear\nfunction and a small but arbitrary perturbation chosen after observing the\nplayer's choice. We give both expected and high probability regret bounds for\nthe problem. Our result also implies an improved high-probability regret bound\nfor the bandit linear optimization, a special case with no perturbation. We\nalso give a lower bound on the expected regret."
    },
    {
        "date": "2025-05",
        "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment",
        "author": "Lingyu Qiu, Ke Jiang, and Xiaoyang Tan",
        "link": "http://arxiv.org/abs/2505.20653v1",
        "abstract": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA."
    },
    {
        "date": "2025-05",
        "title": "Plug-and-Play Co-Occurring Face Attention for Robust Audio-Visual Speaker Extraction",
        "author": "Zexu Pan, Shengkui Zhao, Tingting Wang, Kun Zhou, Yukun Ma, Chong Zhang, and Bin Ma",
        "link": "http://arxiv.org/abs/2505.20635v1",
        "abstract": "Audio-visual speaker extraction isolates a target speaker's speech from a\nmixture speech signal conditioned on a visual cue, typically using the target\nspeaker's face recording. However, in real-world scenarios, other co-occurring\nfaces are often present on-screen, providing valuable speaker activity cues in\nthe scene. In this work, we introduce a plug-and-play inter-speaker attention\nmodule to process these flexible numbers of co-occurring faces, allowing for\nmore accurate speaker extraction in complex multi-person environments. We\nintegrate our module into two prominent models: the AV-DPRNN and the\nstate-of-the-art AV-TFGridNet. Extensive experiments on diverse datasets,\nincluding the highly overlapped VoxCeleb2 and sparsely overlapped MISP,\ndemonstrate that our approach consistently outperforms baselines. Furthermore,\ncross-dataset evaluations on LRS2 and LRS3 confirm the robustness and\ngeneralizability of our method."
    },
    {
        "date": "2025-05",
        "title": "Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning",
        "author": "Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2505.20621v1",
        "abstract": "Similar to other machine learning frameworks, Offline Reinforcement Learning\n(RL) is shown to be vulnerable to poisoning attacks, due to its reliance on\nexternally sourced datasets, a vulnerability that is exacerbated by its\nsequential nature. To mitigate the risks posed by RL poisoning, we extend\ncertified defenses to provide larger guarantees against adversarial\nmanipulation, ensuring robustness for both per-state actions, and the overall\nexpected cumulative reward. Our approach leverages properties of Differential\nPrivacy, in a manner that allows this work to span both continuous and discrete\nspaces, as well as stochastic and deterministic environments -- significantly\nexpanding the scope and applicability of achievable guarantees. Empirical\nevaluations demonstrate that our approach ensures the performance drops to no\nmore than $50\\%$ with up to $7\\%$ of the training data poisoned, significantly\nimproving over the $0.008\\%$ in prior work~\\citep{wu_copa_2022}, while\nproducing certified radii that is $5$ times larger as well. This highlights the\npotential of our framework to enhance safety and reliability in offline RL."
    },
    {
        "date": "2025-05",
        "title": "EarthOL: A Proof-of-Human-Contribution Consensus Protocol -- Addressing Fundamental Challenges in Decentralized Value Assessment with Enhanced Verification and Security Mechanisms",
        "author": "Jiaxiong He",
        "link": "http://arxiv.org/abs/2505.20614v1",
        "abstract": "This paper introduces EarthOL, a novel consensus protocol that attempts to\nreplace computational waste in blockchain systems with verifiable human\ncontributions within bounded domains. While recognizing the fundamental\nimpossibility of universal value assessment, we propose a domain-restricted\napproach that acknowledges cultural diversity and subjective preferences while\nmaintaining cryptographic security. Our enhanced Proof-of-Human-Contribution\n(PoHC) protocol uses a multi-layered verification system with domain-specific\nevaluation criteria, time-dependent validation mechanisms, and comprehensive\nsecurity frameworks. We present theoretical analysis demonstrating meaningful\nprogress toward incentive-compatible human contribution verification in\nhigh-consensus domains, achieving Byzantine fault tolerance in controlled\nscenarios while addressing significant scalability and cultural bias\nchallenges. Through game-theoretic analysis, probabilistic modeling, and\nenhanced security protocols, we identify specific conditions under which the\nprotocol remains stable and examine failure modes with comprehensive mitigation\nstrategies. This work contributes to understanding the boundaries of\ndecentralized value assessment and provides a framework for future research in\nhuman-centered consensus mechanisms for specific application domains, with\nparticular emphasis on validator and security specialist incentive systems."
    },
    {
        "date": "2025-05",
        "title": "One-shot Robust Federated Learning of Independent Component Analysis",
        "author": "Dian Jin, Xin Bing, and Yuqian Zhang",
        "link": "http://arxiv.org/abs/2505.20532v1",
        "abstract": "This paper investigates a general robust one-shot aggregation framework for\ndistributed and federated Independent Component Analysis (ICA) problem. We\npropose a geometric median-based aggregation algorithm that leverages $k$-means\nclustering to resolve the permutation ambiguity in local client estimations.\nOur method first performs k-means to partition client-provided estimators into\nclusters and then aggregates estimators within each cluster using the geometric\nmedian. This approach provably remains effective even in highly heterogeneous\nscenarios where at most half of the clients can observe only a minimal number\nof samples. The key theoretical contribution lies in the combined analysis of\nthe geometric median's error bound-aided by sample quantiles-and the maximum\nmisclustering rates of the aforementioned solution of $k$-means. The\neffectiveness of the proposed approach is further supported by simulation\nstudies conducted under various heterogeneous settings."
    },
    {
        "date": "2025-05",
        "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
        "author": "Aideen Fay, In\u00e9s Garc\u00eda-Redondo, Qiquan Wang, Haim Dubossarsky, and Anthea Monod",
        "link": "http://arxiv.org/abs/2505.20435v1",
        "abstract": "Understanding how adversarial conditions affect language models requires\ntechniques that capture both global structure and local detail within\nhigh-dimensional activation spaces. We propose persistent homology (PH), a tool\nfrom topological data analysis, to systematically characterize multiscale\nlatent space dynamics in LLMs under two distinct attack modes -- backdoor\nfine-tuning and indirect prompt injection. By analyzing six state-of-the-art\nLLMs, we show that adversarial conditions consistently compress latent\ntopologies, reducing structural diversity at smaller scales while amplifying\ndominant features at coarser ones. These topological signatures are\nstatistically robust across layers, architectures, model sizes, and align with\nthe emergence of adversarial effects deeper in the network. To capture\nfiner-grained mechanisms underlying these shifts, we introduce a neuron-level\nPH framework that quantifies how information flows and transforms within and\nacross layers. Together, our findings demonstrate that PH offers a principled\nand unifying approach to interpreting representational dynamics in LLMs,\nparticularly under distributional shift."
    },
    {
        "date": "2025-05",
        "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
        "author": "Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, Ali Vosoughi, Luchuan Song, Zeliang Zhang, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2505.20426v1",
        "abstract": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/"
    },
    {
        "date": "2025-05",
        "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
        "author": "Simin Fan, Maria Ios Glarou, and Martin Jaggi",
        "link": "http://arxiv.org/abs/2505.20380v1",
        "abstract": "The performance of large language models (LLMs) across diverse downstream\napplications is fundamentally governed by the quality and composition of their\npretraining corpora. Existing domain reweighting algorithms primarily optimize\ndata mixtures for a single target task, thereby resulting in models that\noverfit to specialized objectives while exhibiting substantial performance\ndegradation on other benchmarks. This paper introduces Group Robust\nMulti-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target\ndomain reweighting framework designed to calibrate pretraining data mixtures\nfor robust performance across multiple target tasks simultaneously. GRAPE\ndynamically adjusts sampling weights across source domains (domain weights)\nwhile concurrently modulating task weights that quantify the relative\nimportance of each individual target task. This adaptive process prioritizes\ntasks based on their learning difficulty throughout training. We formulate this\ninterleaved reweighting mechanism as a minimax optimization problem: The inner\nmaximization adjusts task weights leveraging group\ndistributed-robust-optimization (DRO), where those tasks demonstrating the\nleast improvement under the current data mixture are prioritized with higher\nweights; The outer minimization then optimizes domain weights to maximize loss\nreduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama\ndatasets demonstrate that GRAPE consistently outperforms baseline methods in\nterms of reasoning performance across 6 benchmarks. Furthermore, when applied\nto multilingual targets, GRAPE effectively identifies optimal training mixtures\nfrom mainstream languages, achieving superior language modeling capabilities\nacross 8 low-resource target languages."
    },
    {
        "date": "2025-05",
        "title": "An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks",
        "author": "Jinyan Wang, Liu Yang, Yuecen Wei, Jiaxuan Si, Chenhao Guo, Qingyun Sun, Xianxian Li, and Xingcheng Fu",
        "link": "http://arxiv.org/abs/2505.20074v1",
        "abstract": "Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains."
    },
    {
        "date": "2025-05",
        "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
        "author": "Xinping Chen, and Chen Liu",
        "link": "http://arxiv.org/abs/2505.20026v1",
        "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters."
    },
    {
        "date": "2025-05",
        "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
        "author": "Elvir Karimov, Alexander Varlamov, Danil Ivanov, Dmitrii Korzh, and Oleg Y. Rogov",
        "link": "http://arxiv.org/abs/2505.19951v1",
        "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths."
    },
    {
        "date": "2025-05",
        "title": "Cellwise and Casewise Robust Covariance in High Dimensions",
        "author": "Fabio Centofanti, Mia Hubert, and Peter J. Rousseeuw",
        "link": "http://arxiv.org/abs/2505.19925v1",
        "abstract": "The sample covariance matrix is a cornerstone of multivariate statistics, but\nit is highly sensitive to outliers. These can be casewise outliers, such as\ncases belonging to a different population, or cellwise outliers, which are\ndeviating cells (entries) of the data matrix. Recently some robust covariance\nestimators have been developed that can handle both types of outliers, but\ntheir computation is only feasible up to at most 20 dimensions. To remedy this\nwe propose the cellRCov method, a robust covariance estimator that\nsimultaneously handles casewise outliers, cellwise outliers, and missing data.\nIt relies on a decomposition of the covariance on principal and orthogonal\nsubspaces, leveraging recent work on robust PCA. It also employs a ridge-type\nregularization to stabilize the estimated covariance matrix. We establish some\ntheoretical properties of cellRCov, including its casewise and cellwise\ninfluence functions as well as consistency and asymptotic normality. A\nsimulation study demonstrates the superior performance of cellRCov in\ncontaminated and missing data scenarios. Furthermore, its practical utility is\nillustrated in a real-world application to anomaly detection. We also construct\nand illustrate the cellRCCA method for robust and regularized canonical\ncorrelation analysis."
    },
    {
        "date": "2025-05",
        "title": "CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models",
        "author": "Chunyang Li, Junwei Zhang, Anda Cheng, Zhuo Ma, Xinghua Li, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2505.19864v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge, but its openness introduces vulnerabilities\nthat can be exploited by poisoning attacks. Existing poisoning methods for RAG\nsystems have limitations, such as poor generalization and lack of fluency in\nadversarial texts. In this paper, we propose CPA-RAG, a black-box adversarial\nframework that generates query-relevant texts capable of manipulating the\nretrieval process to induce target answers. The proposed method integrates\nprompt-based text generation, cross-guided optimization through multiple LLMs,\nand retriever-based scoring to construct high-quality adversarial samples. We\nconduct extensive experiments across multiple datasets and LLMs to evaluate its\neffectiveness. Results show that the framework achieves over 90\\% attack\nsuccess when the top-k retrieval setting is 5, matching white-box performance,\nand maintains a consistent advantage of approximately 5 percentage points\nacross different top-k values. It also outperforms existing black-box baselines\nby 14.5 percentage points under various defense strategies. Furthermore, our\nmethod successfully compromises a commercial RAG system deployed on Alibaba's\nBaiLian platform, demonstrating its practical threat in real-world\napplications. These findings underscore the need for more robust and secure RAG\nframeworks to defend against poisoning attacks."
    },
    {
        "date": "2025-05",
        "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
        "author": "Binyan Xu, Xilin Dai, Di Tang, and Kehuan Zhang",
        "link": "http://arxiv.org/abs/2505.19840v1",
        "abstract": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone\nto adversarial attacks. Typically, such attacks either involve frequent queries\nto the target model or rely on surrogate models closely mirroring the target\nmodel -- often trained with subsets of the target model's training data -- to\nachieve high attack success rates through transferability. However, in\nrealistic scenarios where training data is inaccessible and excessive queries\ncan raise alarms, crafting adversarial examples becomes more challenging. In\nthis paper, we present UnivIntruder, a novel attack framework that relies\nsolely on a single, publicly available CLIP model and publicly available\ndatasets. By using textual concepts, UnivIntruder generates universal,\ntransferable, and targeted adversarial perturbations that mislead DNNs into\nmisclassifying inputs into adversary-specified classes defined by textual\nconcepts.\n  Our extensive experiments show that our approach achieves an Attack Success\nRate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly\noutperforming existing transfer-based methods. Additionally, we reveal\nreal-world vulnerabilities, showing that even without querying target models,\nUnivIntruder compromises image search engines like Google and Baidu with ASR\nrates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR\nrates up to 80%. These findings underscore the practicality of our attack in\nscenarios where traditional avenues are blocked, highlighting the need to\nreevaluate security paradigms in AI applications."
    },
    {
        "date": "2025-05",
        "title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks",
        "author": "Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, and Shouling Ji",
        "link": "http://arxiv.org/abs/2505.19821v1",
        "abstract": "Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations."
    },
    {
        "date": "2025-05",
        "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning",
        "author": "Bariscan Bozkurt, Houssam Zenati, Dimitri Meunier, Liyuan Xu, and Arthur Gretton",
        "link": "http://arxiv.org/abs/2505.19807v1",
        "abstract": "We study the problem of causal function estimation in the Proxy Causal\nLearning (PCL) framework, where confounders are not observed but proxies for\nthe confounders are available. Two main approaches have been proposed: outcome\nbridge-based and treatment bridge-based methods. In this work, we propose two\nkernel-based doubly robust estimators that combine the strengths of both\napproaches, and naturally handle continuous and high-dimensional variables. Our\nidentification strategy builds on a recent density ratio-free method for\ntreatment bridge-based PCL; furthermore, in contrast to previous approaches, it\ndoes not require indicator functions or kernel smoothing over the treatment\nvariable. These properties make it especially well-suited for continuous or\nhigh-dimensional treatments. By using kernel mean embeddings, we have\nclosed-form solutions and strong consistency guarantees. Our estimators\noutperform existing methods on PCL benchmarks, including a prior doubly robust\nmethod that requires both kernel smoothing and density ratio estimation."
    },
    {
        "date": "2025-05",
        "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
        "author": "Sangyeop Kim, Yohan Lee, Yongwoo Song, and Kimin Lee",
        "link": "http://arxiv.org/abs/2505.19773v1",
        "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."
    },
    {
        "date": "2025-05",
        "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
        "author": "Bingrui Sima, Linhua Cong, Wenxuan Wang, and Kun He",
        "link": "http://arxiv.org/abs/2505.19684v2",
        "abstract": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks."
    },
    {
        "date": "2025-05",
        "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2505.19613v1",
        "abstract": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization."
    },
    {
        "date": "2025-05",
        "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts",
        "author": "Shadi Alijani, and Homayoun Najjaran",
        "link": "http://arxiv.org/abs/2505.19587v1",
        "abstract": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."
    },
    {
        "date": "2025-05",
        "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare",
        "author": "Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, and Jie M. Zhang",
        "link": "http://arxiv.org/abs/2505.19562v1",
        "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI."
    },
    {
        "date": "2025-05",
        "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds",
        "author": "Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Mehrdad Noori, Gustavo Adolfo Vargas Hakim, David Osowiechi, Farzad Beizaee, Ismail Ben Ayed, and Christian Desrosiers",
        "link": "http://arxiv.org/abs/2505.19546v1",
        "abstract": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC."
    },
    {
        "date": "2025-05",
        "title": "Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning",
        "author": "Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2505.19532v1",
        "abstract": "The current state-of-the-art backdoor attacks against Reinforcement Learning\n(RL) rely upon unrealistically permissive access models, that assume the\nattacker can read (or even write) the victim's policy parameters, observations,\nor rewards. In this work, we question whether such a strong assumption is\nrequired to launch backdoor attacks against RL. To answer this question, we\npropose the \\underline{S}upply-\\underline{C}h\\underline{a}in\n\\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:\ntraining agents using external agents that are provided separately or embedded\nwithin the environment. In contrast to prior works, our attack only relies on\nlegitimate interactions of the RL agent with the supplied agents. Despite this\nlimited access model, by poisoning a mere $3\\%$ of training experiences, our\nattack can successfully activate over $90\\%$ of triggered actions, reducing the\naverage episodic return by $80\\%$ for the victim. Our novel attack demonstrates\nthat RL attacks are likely to become a reality under untrusted RL training\nsupply-chains."
    },
    {
        "date": "2025-05",
        "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation",
        "author": "Mohammed D. Belgoumri, Mohamed Reda Bouadjenek, Hakim Hacid, Imran Razzak, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2505.19527v1",
        "abstract": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry."
    },
    {
        "date": "2025-05",
        "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning",
        "author": "Jiyu Hu, Haijiang Zeng, and Zhen Tian",
        "link": "http://arxiv.org/abs/2505.19522v1",
        "abstract": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments."
    },
    {
        "date": "2025-05",
        "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation",
        "author": "Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, and Tianlong Chen",
        "link": "http://arxiv.org/abs/2505.19504v1",
        "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."
    },
    {
        "date": "2025-05",
        "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation",
        "author": "Kaichao Jiang, He Wang, Xiaoshuai Hao, Xiulong Yang, Ajian Liu, Qi Chu, and Yunfeng Diao",
        "link": "http://arxiv.org/abs/2505.19459v1",
        "abstract": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs."
    },
    {
        "date": "2025-05",
        "title": "An Empirical Study of JavaScript Inclusion Security Issues in Chrome Extensions",
        "author": "Chong Guan",
        "link": "http://arxiv.org/abs/2505.19456v1",
        "abstract": "JavaScript, a scripting language employed to augment the capabilities of web\nbrowsers within web pages or browser extensions, utilizes code segments termed\nJavaScript inclusions. While the security aspects of JavaScript inclusions in\nweb pages have undergone substantial scrutiny, a thorough investigation into\nthe security of such inclusions within browser extensions remains absent,\ndespite the divergent security paradigms governing these environments. This\nstudy presents a systematic measurement of JavaScript inclusions in Chrome\nextensions, employing a hybrid methodology encompassing static and dynamic\nanalysis to identify these inclusions. The analysis of 36,324 extensions\nrevealed 350,784 JavaScript inclusions. Subsequent security assessment\nindicated that, although the majority of these inclusions originate from local\nfiles within the extensions rather than external servers, 22 instances of\nvulnerable remote JavaScript inclusions were identified. These remote\ninclusions present potential avenues for malicious actors to execute arbitrary\ncode within the extension's execution context. Furthermore, an analysis of\nJavaScript library utilization within Chrome extensions disclosed the prevalent\nuse of susceptible and outdated libraries, notably within numerous widely\nadopted extensions."
    },
    {
        "date": "2025-05",
        "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents",
        "author": "Ye Ye",
        "link": "http://arxiv.org/abs/2505.19436v1",
        "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."
    },
    {
        "date": "2025-05",
        "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains",
        "author": "Jiawen Zhang, Zhenwei Zhang, Shun Zheng, Xumeng Wen, Jia Li, and Jiang Bian",
        "link": "http://arxiv.org/abs/2505.19397v1",
        "abstract": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs."
    },
    {
        "date": "2025-05",
        "title": "SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition",
        "author": "Yunbo Liu, Xukui Qin, Yifan Gao, Xiang Li, and Chengwei Feng",
        "link": "http://arxiv.org/abs/2505.19369v1",
        "abstract": "Human Activity Recognition (HAR) using wearable sensor data has become a\ncentral task in mobile computing, healthcare, and human-computer interaction.\nDespite the success of traditional deep learning models such as CNNs and RNNs,\nthey often struggle to capture long-range temporal dependencies and contextual\nrelevance across multiple sensor channels. To address these limitations, we\npropose SETransformer, a hybrid deep neural architecture that combines\nTransformer-based temporal modeling with channel-wise squeeze-and-excitation\n(SE) attention and a learnable temporal attention pooling mechanism. The model\ntakes raw triaxial accelerometer data as input and leverages global\nself-attention to capture activity-specific motion dynamics over extended time\nwindows, while adaptively emphasizing informative sensor channels and critical\ntime steps.\n  We evaluate SETransformer on the WISDM dataset and demonstrate that it\nsignificantly outperforms conventional models including LSTM, GRU, BiLSTM, and\nCNN baselines. The proposed model achieves a validation accuracy of 84.68\\% and\na macro F1-score of 84.64\\%, surpassing all baseline architectures by a notable\nmargin. Our results show that SETransformer is a competitive and interpretable\nsolution for real-world HAR tasks, with strong potential for deployment in\nmobile and ubiquitous sensing applications."
    },
    {
        "date": "2025-05",
        "title": "RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks",
        "author": "Amit Chakraborty, Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, and Sachin Shetty",
        "link": "http://arxiv.org/abs/2505.19364v1",
        "abstract": "Machine Learning as a Service (MLaaS) enables users to leverage powerful\nmachine learning models through cloud-based APIs, offering scalability and ease\nof deployment. However, these services are vulnerable to model extraction\nattacks, where adversaries repeatedly query the application programming\ninterface (API) to reconstruct a functionally similar model, compromising\nintellectual property and security. Despite various defense strategies being\nproposed, many suffer from high computational costs, limited adaptability to\nevolving attack techniques, and a reduction in performance for legitimate\nusers. In this paper, we introduce a Resilient Adaptive Defense Framework for\nModel Extraction Attack Protection (RADEP), a multifaceted defense framework\ndesigned to counteract model extraction attacks through a multi-layered\nsecurity approach. RADEP employs progressive adversarial training to enhance\nmodel resilience against extraction attempts. Malicious query detection is\nachieved through a combination of uncertainty quantification and behavioral\npattern analysis, effectively identifying adversarial queries. Furthermore, we\ndevelop an adaptive response mechanism that dynamically modifies query outputs\nbased on their suspicion scores, reducing the utility of stolen models.\nFinally, ownership verification is enforced through embedded watermarking and\nbackdoor triggers, enabling reliable identification of unauthorized model use.\nExperimental evaluations demonstrate that RADEP significantly reduces\nextraction success rates while maintaining high detection accuracy with minimal\nimpact on legitimate queries. Extensive experiments show that RADEP effectively\ndefends against model extraction attacks and remains resilient even against\nadaptive adversaries, making it a reliable security framework for MLaaS models."
    },
    {
        "date": "2025-05",
        "title": "Co-evolutionary Dynamics of Attack and Defence in Cybersecurity",
        "author": "Adeela Bashir, Zia Ush Shamszaman, Zhao Song, and The Anh Han",
        "link": "http://arxiv.org/abs/2505.19338v1",
        "abstract": "In the evolving digital landscape, it is crucial to study the dynamics of\ncyberattacks and defences. This study uses an Evolutionary Game Theory (EGT)\nframework to investigate the evolutionary dynamics of attacks and defences in\ncyberspace. We develop a two-population asymmetric game between attacker and\ndefender to capture the essential factors of costs, potential benefits, and the\nprobability of successful defences. Through mathematical analysis and numerical\nsimulations, we find that systems with high defence intensities show stability\nwith minimal attack frequencies, whereas low-defence environments show\ninstability, and are vulnerable to attacks. Furthermore, we find five\nequilibria, where the strategy pair always defend and attack emerged as the\nmost likely stable state as cyber domain is characterised by a continuous\nbattle between defenders and attackers. Our theoretical findings align with\nreal-world data from past cyber incidents, demonstrating the interdisciplinary\nimpact, such as fraud detection, risk management and cybersecurity\ndecision-making. Overall, our analysis suggests that adaptive cybersecurity\nstrategies based on EGT can improve resource allocation, enhance system\nresilience, and reduce the overall risk of cyberattacks. By incorporating\nreal-world data, this study demonstrates the applicability of EGT in addressing\nthe evolving nature of cyber threats and the need for secure digital ecosystems\nthrough strategic planning and proactive defence measures."
    },
    {
        "date": "2025-05",
        "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis",
        "author": "Kazi Mahathir Rahman, Showrin Rahman, and Sharmin Sultana Srishty",
        "link": "http://arxiv.org/abs/2505.19291v1",
        "abstract": "Text-embedded image generation plays a critical role in industries such as\ngraphic design, advertising, and digital content creation. Text-to-Image\ngeneration methods leveraging diffusion models, such as TextDiffuser-2, have\ndemonstrated promising results in producing images with embedded text.\nTextDiffuser-2 effectively generates bounding box layouts that guide the\nrendering of visual text, achieving high fidelity and coherence. However,\nexisting approaches often rely on resource-intensive processes and are limited\nin their ability to run efficiently on both CPU and GPU platforms. To address\nthese challenges, we propose a novel two-stage pipeline that integrates\nreinforcement learning (RL) for rapid and optimized text layout generation with\na diffusion-based image synthesis model. Our RL-based approach significantly\naccelerates the bounding box prediction step while reducing overlaps, allowing\nthe system to run efficiently on both CPUs and GPUs. Extensive evaluations\ndemonstrate that our framework maintains or surpasses TextDiffuser-2's quality\nin text placement and image synthesis, with markedly faster runtime and\nincreased flexibility. Extensive evaluations demonstrate that our framework\nmaintains or surpasses TextDiffuser-2's quality in text placement and image\nsynthesis, with markedly faster runtime and increased flexibility. Our approach\nhas been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore\nmetrics close to state-of-the-art models, while being 97.64% more faster and\nrequiring only 2MB of memory to run."
    },
    {
        "date": "2025-05",
        "title": "BSAGIoT: A Bayesian Security Aspect Graph for Internet of Things (IoT)",
        "author": "Zeinab Lashkaripour, Masoud Khosravi-Farmad, AhmadReza Montazerolghaem, and Razieh Rezaee",
        "link": "http://arxiv.org/abs/2505.19283v1",
        "abstract": "IoT is a dynamic network of interconnected things that communicate and\nexchange data, where security is a significant issue. Previous studies have\nmainly focused on attack classifications and open issues rather than presenting\na comprehensive overview on the existing threats and vulnerabilities. This\nknowledge helps analyzing the network in the early stages even before any\nattack takes place. In this paper, the researchers have proposed different\nsecurity aspects and a novel Bayesian Security Aspects Dependency Graph for IoT\n(BSAGIoT) to illustrate their relations. The proposed BSAGIoT is a generic\nmodel applicable to any IoT network and contains aspects from five categories\nnamed data, access control, standard, network, and loss. This proposed Bayesian\nSecurity Aspect Graph (BSAG) presents an overview of the security aspects in\nany given IoT network. The purpose of BSAGIoT is to assist security experts in\nanalyzing how a successful compromise and/or a failed breach could impact the\noverall security and privacy of the respective IoT network. In addition, root\ncause identification of security challenges, how they affect one another, their\nimpact on IoT networks via topological sorting, and risk assessment could be\nachieved. Hence, to demonstrate the feasibility of the proposed method,\nexperimental results with various scenarios has been presented, in which the\nsecurity aspects have been quantified based on the network configurations. The\nresults indicate the impact of the aspects on each other and how they could be\nutilized to mitigate and/or eliminate the security and privacy deficiencies in\nIoT networks."
    },
    {
        "date": "2025-05",
        "title": "Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning",
        "author": "Hui Ma, Kai Yang, and Yang Jiao",
        "link": "http://arxiv.org/abs/2505.19263v1",
        "abstract": "Network traffic prediction plays a crucial role in intelligent network\noperation. Traditional prediction methods often rely on centralized training,\nnecessitating the transfer of vast amounts of traffic data to a central server.\nThis approach can lead to latency and privacy concerns. To address these\nissues, federated learning integrated with differential privacy has emerged as\na solution to improve data privacy and model robustness in distributed\nsettings. Nonetheless, existing federated learning protocols are vulnerable to\nByzantine attacks, which may significantly compromise model robustness.\nDeveloping a robust and privacy-preserving prediction model in the presence of\nByzantine clients remains a significant challenge. To this end, we propose an\nasynchronous differential federated learning framework based on\ndistributionally robust optimization. The proposed framework utilizes multiple\nclients to train the prediction model collaboratively with local differential\nprivacy. In addition, regularization techniques have been employed to further\nimprove the Byzantine robustness of the models. We have conducted extensive\nexperiments on three real-world datasets, and the results elucidate that our\nproposed distributed algorithm can achieve superior performance over existing\nmethods."
    },
    {
        "date": "2025-05",
        "title": "ALRPHFS: Adversarially Learned Risk Patterns with Hierarchical Fast \\& Slow Reasoning for Robust Agent Defense",
        "author": "Shiyu Xiang, Tong Zhang, and Ronghao Chen",
        "link": "http://arxiv.org/abs/2505.19260v1",
        "abstract": "LLM Agents are becoming central to intelligent systems. However, their\ndeployment raises serious safety concerns. Existing defenses largely rely on\n\"Safety Checks\", which struggle to capture the complex semantic risks posed by\nharmful user inputs or unsafe agent behaviors - creating a significant semantic\ngap between safety checks and real-world risks. To bridge this gap, we propose\na novel defense framework, ALRPHFS (Adversarially Learned Risk Patterns with\nHierarchical Fast & Slow Reasoning). ALRPHFS consists of two core components:\n(1) an offline adversarial self-learning loop to iteratively refine a\ngeneralizable and balanced library of risk patterns, substantially enhancing\nrobustness without retraining the base LLM, and (2) an online hierarchical fast\n& slow reasoning engine that balances detection effectiveness with\ncomputational efficiency. Experimental results demonstrate that our approach\nachieves superior overall performance compared to existing baselines, achieving\na best-in-class average accuracy of 80% and exhibiting strong generalizability\nacross agents and tasks."
    },
    {
        "date": "2025-05",
        "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees",
        "author": "Sourav Ganguly, Arnob Ghosh, Kishan Panaganti, and Adam Wierman",
        "link": "http://arxiv.org/abs/2505.19238v1",
        "abstract": "Constrained decision-making is essential for designing safe policies in\nreal-world control systems, yet simulated environments often fail to capture\nreal-world adversities. We consider the problem of learning a policy that will\nmaximize the cumulative reward while satisfying a constraint, even when there\nis a mismatch between the real model and an accessible simulator/nominal model.\nIn particular, we consider the robust constrained Markov decision problem\n(RCMDP) where an agent needs to maximize the reward and satisfy the constraint\nagainst the worst possible stochastic model under the uncertainty set centered\naround an unknown nominal model. Primal-dual methods, effective for standard\nconstrained MDP (CMDP), are not applicable here because of the lack of the\nstrong duality property. Further, one cannot apply the standard robust\nvalue-iteration based approach on the composite value function either as the\nworst case models may be different for the reward value function and the\nconstraint value function. We propose a novel technique that effectively\nminimizes the constraint value function--to satisfy the constraints; on the\nother hand, when all the constraints are satisfied, it can simply maximize the\nrobust reward value function. We prove that such an algorithm finds a policy\nwith at most $\\epsilon$ sub-optimality and feasible policy after\n$O(\\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we\ndo not need to employ a binary search, thus, we reduce the computation time by\nat least 4x for smaller value of discount factor ($\\gamma$) and by at least 6x\nfor larger value of $\\gamma$."
    },
    {
        "date": "2025-05",
        "title": "Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation",
        "author": "Peiran Sun",
        "link": "http://arxiv.org/abs/2505.19194v1",
        "abstract": "Adversarial attack reveals the vulnerability of deep learning models. For\nabout a decade, countless attack and defense methods have been proposed,\nleading to robustified classifiers and better understanding of models. Among\nthese methods, curvature-based approaches have attracted attention because it\nis assumed that high curvature may give rise to rough decision boundary.\nHowever, the most commonly used \\textit{curvature} is the curvature of loss\nfunction, scores or other parameters from within the model as opposed to\ndecision boundary curvature, since the former can be relatively easily formed\nusing second order derivative. In this paper, we propose a new query-efficient\nmethod, dynamic curvature estimation(DCE), to estimate the decision boundary\ncurvature in a black-box setting. Our approach is based on CGBA, a black-box\nadversarial attack. By performing DCE on a wide range of classifiers, we\ndiscovered, statistically, a connection between decision boundary curvature and\nadversarial robustness. We also propose a new attack method, curvature dynamic\nblack-box attack(CDBA) with improved performance using the dynamically\nestimated curvature."
    },
    {
        "date": "2025-05",
        "title": "Penetration Testing for System Security: Methods and Practical Approaches",
        "author": "Wei Zhang, Ju Xing, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2505.19174v1",
        "abstract": "Penetration testing refers to the process of simulating hacker attacks to\nevaluate the security of information systems . This study aims not only to\nclarify the theoretical foundations of penetration testing but also to explain\nand demonstrate the complete testing process, including how network system\nadministrators may simulate attacks using various penetration testing methods.\nMethodologically, the paper outlines the five basic stages of a typical\npenetration test: intelligence gathering, vulnerability scanning, vulnerability\nexploitation, privilege escalation, and post-exploitation activities. In each\nphase, specific tools and techniques are examined in detail, along with\npractical guidance on their use. To enhance the practical relevance of the\nstudy, the paper also presents a real-life case study, illustrating how a\ncomplete penetration test is conducted in a real-world environment. Through\nthis case, readers can gain insights into the detailed procedures and applied\ntechniques, thereby deepening their understanding of the practical value of\npenetration testing. Finally, the paper summarizes the importance and necessity\nof penetration testing in securing information systems and maintaining network\nintegrity, and it explores future trends and development directions for the\nfield. Overall, the findings of this paper offer valuable references for both\nresearchers and practitioners, contributing meaningfully to the improvement of\npenetration testing practices and the advancement of cybersecurity as a whole."
    },
    {
        "date": "2025-05",
        "title": "Towards Robust Influence Functions with Flat Validation Minima",
        "author": "Xichen Ye, Yifan Wu, Weizhong Zhang, Cheng Jin, and Yifan Chen",
        "link": "http://arxiv.org/abs/2505.19097v1",
        "abstract": "The Influence Function (IF) is a widely used technique for assessing the\nimpact of individual training samples on model predictions. However, existing\nIF methods often fail to provide reliable influence estimates in deep neural\nnetworks, particularly when applied to noisy training data. This issue does not\nstem from inaccuracies in parameter change estimation, which has been the\nprimary focus of prior research, but rather from deficiencies in loss change\nestimation, specifically due to the sharpness of validation risk. In this work,\nwe establish a theoretical connection between influence estimation error,\nvalidation set risk, and its sharpness, underscoring the importance of flat\nvalidation minima for accurate influence estimation. Furthermore, we introduce\na novel estimation form of Influence Function specifically designed for flat\nvalidation minima. Experimental results across various tasks validate the\nsuperiority of our approach."
    },
    {
        "date": "2025-05",
        "title": "Towards Generalized Proactive Defense against Face Swapping with Contour-Hybrid Watermark",
        "author": "Ruiyang Xia, Dawei Zhou, Decheng Liu, Lin Yuan, Jie Li, Nannan Wang, and Xinbo Gao",
        "link": "http://arxiv.org/abs/2505.19081v2",
        "abstract": "Face swapping, recognized as a privacy and security concern, has prompted\nconsiderable defensive research. With the advancements in AI-generated content,\nthe discrepancies between the real and swapped faces have become nuanced.\nConsidering the difficulty of forged traces detection, we shift the focus to\nthe face swapping purpose and proactively embed elaborate watermarks against\nunknown face swapping techniques. Given that the constant purpose is to swap\nthe original face identity while preserving the background, we concentrate on\nthe regions surrounding the face to ensure robust watermark generation, while\nembedding the contour texture and face identity information to achieve\nprogressive image determination. The watermark is located in the facial contour\nand contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our\napproach generalizes face swapping detection without requiring any swapping\ntechniques during training and the storage of large-scale messages in advance.\nExperiments conducted across 8 face swapping techniques demonstrate the\nsuperiority of our approach compared with state-of-the-art passive and\nproactive detectors while achieving a favorable balance between the image\nquality and watermark robustness."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management",
        "author": "Chen Avin, Zvi Lotker, Shie Mannor, Gil Shabat, Hanan Shteingart, and Roey Yadgar",
        "link": "http://arxiv.org/abs/2505.19061v1",
        "abstract": "Motivated by dynamic parameter optimization in finite, but large action\n(configurations) spaces, this work studies the nonstochastic multi-armed bandit\n(MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We\npropose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can\nuse state-of-the-art existing \"flat\" algorithms, but additionally clusters\nsimilar configurations to exploit local structures and adapt to changing\nenvironments. We prove that in the worst-case scenario, such clustering\napproach cannot hurt too much and ABoB guarantees a standard worst-case regret\nbound of $O\\left(k^{\\frac{1}{2}}T^{\\frac{1}{2}}\\right)$, where $T$ is the\nnumber of rounds and $k$ is the number of arms, matching the traditional flat\napproach. However, under favorable conditions related to the algorithm\nproperties, clusters properties, and certain Lipschitz conditions, the regret\nbound can be improved to $O\\left(k^{\\frac{1}{4}}T^{\\frac{1}{2}}\\right)$.\nSimulations and experiments on a real storage system demonstrate that ABoB,\nusing standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and\nfaster convergence than the flat method, up to 50% improvement in known\nprevious setups, nonstochastic and stochastic, as well as in our settings."
    },
    {
        "date": "2025-05",
        "title": "Distributionally Robust Deep Q-Learning",
        "author": "Chung I Lu, Julian Sester, and Aijia Zhang",
        "link": "http://arxiv.org/abs/2505.19058v1",
        "abstract": "We propose a novel distributionally robust $Q$-learning algorithm for the\nnon-tabular case accounting for continuous state spaces where the state\ntransition of the underlying Markov decision process is subject to model\nuncertainty. The uncertainty is taken into account by considering the\nworst-case transition from a ball around a reference probability measure. To\ndetermine the optimal policy under the worst-case state transition, we solve\nthe associated non-linear Bellman equation by dualising and regularising the\nBellman operator with the Sinkhorn distance, which is then parameterized with\ndeep neural networks. This approach allows us to modify the Deep Q-Network\nalgorithm to optimise for the worst case state transition.\n  We illustrate the tractability and effectiveness of our approach through\nseveral applications, including a portfolio optimisation task based on\nS\\&{P}~500 data."
    },
    {
        "date": "2025-05",
        "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
        "author": "Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, and George Turkiyyah",
        "link": "http://arxiv.org/abs/2505.19056v1",
        "abstract": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."
    },
    {
        "date": "2025-05",
        "title": "A quantitative notion of economic security for smart contract compositions",
        "author": "Emily Priyadarshini, and Massimo Bartoletti",
        "link": "http://arxiv.org/abs/2505.19006v1",
        "abstract": "Decentralized applications are often composed of multiple interconnected\nsmart contracts. This is especially evident in DeFi, where protocols are\nheavily intertwined and rely on a variety of basic building blocks such as\ntokens, decentralized exchanges and lending protocols. A crucial security\nchallenge in this setting arises when adversaries target individual components\nto cause systemic economic losses. Existing security notions focus on\ndetermining the existence of these attacks, but fail to quantify the effect of\nmanipulating individual components on the overall economic security of the\nsystem. In this paper, we introduce a quantitative security notion that\nmeasures how an attack on a single component can amplify economic losses of the\noverall system. We study the fundamental properties of this notion and apply it\nto assess the security of key compositions. In particular, we analyse\nunder-collateralized loan attacks in systems made of lending protocols and\ndecentralized exchanges."
    },
    {
        "date": "2025-05",
        "title": "Secure IVSHMEM: End-to-End Shared-Memory Protocol with Hypervisor-CA Handshake and In-Kernel Access Control",
        "author": "Hyunwoo Kim, Jaeseong Lee, Sunpyo Hong, and Changmin Han",
        "link": "http://arxiv.org/abs/2505.19004v1",
        "abstract": "In-host shared memory (IVSHMEM) enables high-throughput, zero-copy\ncommunication between virtual machines, but today's implementations lack any\nsecurity control, allowing any application to eavesdrop or tamper with the\nIVSHMEM region. This paper presents Secure IVSHMEM, a protocol that provides\nend-to-end mutual authentication and fine-grained access enforcement with\nnegligible performance cost. We combine three techniques to ensure security:\n(1) channel separation and kernel module access control, (2)hypervisor-mediated\nhandshake for end-to-end service authentication, and (3)application-level\nintegration for abstraction and performance mitigation. In microbenchmarks,\nSecure IVSHMEM completes its one-time handshake in under 200ms and sustains\ndata-plane round-trip latencies within 5\\% of the unmodified baseline, with\nnegligible bandwidth overhead. We believe this design is ideally suited for\nsafety and latency-critical in-host domains, such as automotive systems, where\nboth performance and security are paramount."
    },
    {
        "date": "2025-05",
        "title": "SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours",
        "author": "Catalina Tan, Yipeng Hu, and Shaheer U. Saeed",
        "link": "http://arxiv.org/abs/2505.18989v1",
        "abstract": "Accurate tumour segmentation is vital for various targeted diagnostic and\ntherapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.\nManual delineation is extremely labour-intensive, requiring substantial expert\ntime. Fully-supervised machine learning models aim to automate such\nlocalisation tasks, but require a large number of costly and often subjective\n3D voxel-level labels for training. The high-variance and subjectivity in such\nlabels impacts model generalisability, even when large datasets are available.\nHistopathology labels may offer more objective labels but the infeasibility of\nacquiring pixel-level annotations to develop tumour localisation methods based\non histology remains challenging in-vivo. In this work, we propose a novel\nweakly-supervised semantic segmentation framework called SPARS (Self-Play\nAdversarial Reinforcement Learning for Segmentation), which utilises an object\npresence classifier, trained on a small number of image-level binary cancer\npresence labels, to localise cancerous regions on CT scans. Such binary labels\nof patient-level cancer presence can be sourced more feasibly from biopsies and\nhistopathology reports, enabling a more objective cancer localisation on\nmedical images. Evaluating with real patient data, we observed that SPARS\nyielded a mean dice score of $77.3 \\pm 9.4$, which outperformed other\nweakly-supervised methods by large margins. This performance was comparable\nwith recent fully-supervised methods that require voxel-level annotations. Our\nresults demonstrate the potential of using SPARS to reduce the need for\nextensive human-annotated labels to detect cancer in real-world healthcare\nsettings."
    },
    {
        "date": "2025-05",
        "title": "Exemplifying Emerging Phishing: QR-based Browser-in-The-Browser (BiTB) Attack",
        "author": "Muhammad Wahid Akram, Keshav Sood, Muneeb Ul Hassan, and Basant Subba",
        "link": "http://arxiv.org/abs/2505.18944v1",
        "abstract": "Lately, cybercriminals constantly formulate productive approaches to exploit\nindividuals. This article exemplifies an innovative attack, namely QR-based\nBrowser-in-The-Browser (BiTB), using proficiencies of Large Language Model\n(LLM) i.e. Google Gemini. The presented attack is a fusion of two emerging\nattacks: BiTB and Quishing (QR code phishing). Our study underscores attack's\nsimplistic implementation utilizing malicious prompts provided to Gemini-LLM.\nMoreover, we presented a case study to highlight a lucrative attack method, we\nalso performed an experiment to comprehend the attack execution on victims'\ndevice. The findings of this work obligate the researchers' contributions in\nconfronting this type of phishing attempts through LLMs."
    },
    {
        "date": "2025-05",
        "title": "Robust Stability Analysis of Positive Lure System with Neural Network Feedback",
        "author": "Hamidreza Montazeri Hedesh, Moh. Kamalul Wafi, Bahram Shafai, and Milad Siami",
        "link": "http://arxiv.org/abs/2505.18912v1",
        "abstract": "This paper investigates the robustness of the Lur'e problem under positivity\nconstraints, drawing on results from the positive Aizerman conjecture and the\nrobustness properties of Metzler matrices. Specifically, we consider a control\nsystem of Lur'e type in which not only the linear part includes parametric\nuncertainty but also the nonlinear sector bound is unknown. We investigate\ntools from positive linear systems to effectively solve the problems in\ncomplicated and uncertain nonlinear systems. By leveraging the positivity\ncharacteristic of the system, we derive an explicit formula for the stability\nradius of Lur'e systems. Furthermore, we extend our analysis to systems with\nneural network (NN) feedback loops. Building on this approach, we also propose\na refinement method for sector bounds of feedforward neural networks (FFNNs).\nThis study introduces a scalable and efficient approach for robustness analysis\nof both Lur'e and NN-controlled systems. Finally, the proposed results are\nsupported by illustrative examples."
    },
    {
        "date": "2025-05",
        "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos",
        "author": "Andrea Ramazzina, Vittorio Giammarino, Matteo El-Hariry, and Mario Bijelic",
        "link": "http://arxiv.org/abs/2505.18899v1",
        "abstract": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO."
    },
    {
        "date": "2025-05",
        "title": "Security Concerns for Large Language Models: A Survey",
        "author": "Miles Q. Li, and Benjamin C. M. Fung",
        "link": "http://arxiv.org/abs/2505.18889v1",
        "abstract": "Large Language Models (LLMs) such as GPT-4 (and its recent iterations like\nGPT-4o and the GPT-4.1 series), Google's Gemini, Anthropic's Claude 3 models,\nand xAI's Grok have caused a revolution in natural language processing, but\ntheir capabilities also introduce new security vulnerabilities. In this survey,\nwe provide a comprehensive overview of the emerging security concerns around\nLLMs, categorizing threats into prompt injection and jailbreaking, adversarial\nattacks (including input perturbations and data poisoning), misuse by malicious\nactors (e.g., for disinformation, phishing, and malware generation), and\nworrisome risks inherent in autonomous LLM agents. A significant focus has been\nrecently placed on the latter, exploring goal misalignment, emergent deception,\nself-preservation instincts, and the potential for LLMs to develop and pursue\ncovert, misaligned objectives (scheming), which may even persist through safety\ntraining. We summarize recent academic and industrial studies (2022-2025) that\nexemplify each threat, analyze proposed defenses and their limitations, and\nidentify open challenges in securing LLM-based applications. We conclude by\nemphasizing the importance of advancing robust, multi-layered security\nstrategies to ensure LLMs are safe and beneficial."
    },
    {
        "date": "2025-05",
        "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders",
        "author": "Borna Khodabandeh, Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, Sanjay Lall, Sajjad Amini, and Seyed-Mohsen Moosavi-Dezfooli",
        "link": "http://arxiv.org/abs/2505.18884v1",
        "abstract": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."
    },
    {
        "date": "2025-05",
        "title": "Securing Credit Inquiries: The Role of Real-Time User Approval in Preventing SSN Identity Theft",
        "author": "Gogulakrishnan Thiyagarajan, Vinay Bist, and Prabhudarshi Nayak",
        "link": "http://arxiv.org/abs/2505.18861v1",
        "abstract": "Unauthorized credit inquiries are also a central entry point for identity\ntheft, with Social Security Numbers (SSNs) being widely utilized in fraudulent\ncases. Traditional credit inquiry systems do not usually possess strict user\nauthentication, making them vulnerable to unauthorized access. This paper\nproposes a real-time user authorization system to enhance security by enforcing\nexplicit user approval before processing any credit inquiry. The system employs\nreal-time verification and approval techniques. This ensures that the\nauthorized user only approves or rejects a credit check request. It minimizes\nthe risks of interference by third parties. Apart from enhancing security, this\nsystem complies with regulations like the General Data Protection Regulation\n(GDPR) and the Fair Credit Reporting Act (FCRA) while maintaining a seamless\nuser experience. This article discusses the technical issues, scaling-up\nissues, and ways of implementing real-time user authorization in financial\nsystems. Through this framework, financial institutions can drastically\nminimize the risk of identity theft, avert unauthorized credit checks, and\nincrease customer trust in the credit verification system."
    },
    {
        "date": "2025-05",
        "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation",
        "author": "Chika Maduabuchi, Hao Chen, Yujin Han, and Jindong Wang",
        "link": "http://arxiv.org/abs/2505.21545v1",
        "abstract": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm"
    },
    {
        "date": "2025-05",
        "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models",
        "author": "Jamie Hayes, Ilia Shumailov, Christopher A. Choquette-Choo, Matthew Jagielski, George Kaissis, Katherine Lee, Milad Nasr, Sahra Ghalebikesabi, Niloofar Mireshghallah, Meenatchi Sundaram Mutu Selva Annamalai, Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye, Franziska Boenisch, Adam Dziedzic, and A. Feder Cooper",
        "link": "http://arxiv.org/abs/2505.18773v1",
        "abstract": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested."
    },
    {
        "date": "2025-05",
        "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
        "author": "Yanjie Li, Wenxuan Zhang, Xinqi Lyu, Yihao Liu, and Bin Xiao",
        "link": "http://arxiv.org/abs/2505.18766v1",
        "abstract": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion."
    },
    {
        "date": "2025-05",
        "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models",
        "author": "Yuanhe Zhang, Xinyue Wang, Haoran Gao, Zhenhong Zhou, Fanyu Meng, Yuyao Zhang, and Sen Su",
        "link": "http://arxiv.org/abs/2505.18680v1",
        "abstract": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks."
    },
    {
        "date": "2025-05",
        "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics",
        "author": "Pankaj Kumar, and Subhankar Mishra",
        "link": "http://arxiv.org/abs/2505.18658v1",
        "abstract": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."
    },
    {
        "date": "2025-05",
        "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework",
        "author": "Yifan Zhu, Chao Zhang, Xin Shi, Xueqiao Zhang, Yi Yang, and Yawei Luo",
        "link": "http://arxiv.org/abs/2505.18572v1",
        "abstract": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges."
    },
    {
        "date": "2025-05",
        "title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation",
        "author": "Baolei Zhang, Haoran Xin, Jiatong Li, Dongzhe Zhang, Minghong Fang, Zhuqing Liu, Lihai Nie, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.18543v1",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating\nhallucinations in large language models by incorporating external knowledge\nduring inference. However, this integration introduces new security\nvulnerabilities, particularly to poisoning attacks. Although prior work has\nexplored various poisoning strategies, a thorough assessment of their practical\nthreat to RAG systems remains missing. To address this gap, we propose the\nfirst comprehensive benchmark framework for evaluating poisoning attacks on\nRAG. Our benchmark covers 5 standard question answering (QA) datasets and 10\nexpanded variants, along with 13 poisoning attack methods and 7 defense\nmechanisms, representing a broad spectrum of existing techniques. Using this\nbenchmark, we conduct a comprehensive evaluation of all included attacks and\ndefenses across the full dataset spectrum. Our findings show that while\nexisting attacks perform well on standard QA datasets, their effectiveness\ndrops significantly on the expanded versions. Moreover, our results demonstrate\nthat various advanced RAG architectures, such as sequential, branching,\nconditional, and loop RAG, as well as multi-turn conversational RAG, multimodal\nRAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning\nattacks. Notably, current defense techniques fail to provide robust protection,\nunderscoring the pressing need for more resilient and generalizable defense\nstrategies."
    },
    {
        "date": "2025-05",
        "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network",
        "author": "Xiaobin Rong, Dahan Wang, Qinwen Hu, Yushi Wang, Yuxiang Hu, and Jing Lu",
        "link": "http://arxiv.org/abs/2505.18533v1",
        "abstract": "Universal speech enhancement aims to handle input speech with different\ndistortions and input formats. To tackle this challenge, we present TS-URGENet,\na Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.\nTo address various distortions, the proposed system employs a novel three-stage\narchitecture consisting of a filling stage, a separation stage, and a\nrestoration stage. The filling stage mitigates packet loss by preliminarily\nfilling lost regions under noise interference, ensuring signal continuity. The\nseparation stage suppresses noise, reverberation, and clipping distortion to\nimprove speech clarity. Finally, the restoration stage compensates for\nbandwidth limitation, codec artifacts, and residual packet loss distortion,\nrefining the overall speech quality. Our proposed TS-URGENet achieved\noutstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd\nin Track 1."
    },
    {
        "date": "2025-05",
        "title": "Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise",
        "author": "Lucas Tecot, Di Luo, and Cho-Jui Hsieh",
        "link": "http://arxiv.org/abs/2505.18478v1",
        "abstract": "Advancements in quantum computing have spurred significant interest in\nharnessing its potential for speedups over classical systems. However, noise\nremains a major obstacle to achieving reliable quantum algorithms. In this\nwork, we present a provably noise-resilient training theory and algorithm to\nenhance the robustness of parameterized quantum circuit classifiers. Our\nmethod, with a natural connection to Evolutionary Strategies, guarantees\nresilience to parameter noise with minimal adjustments to commonly used\noptimization algorithms. Our approach is function-agnostic and adaptable to\nvarious quantum circuits, successfully demonstrated in quantum phase\nclassification tasks. By developing provably guaranteed optimization theory\nwith quantum circuits, our work opens new avenues for practical, robust\napplications of near-term quantum computers."
    },
    {
        "date": "2025-05",
        "title": "A Dual Basis Approach for Structured Robust Euclidean Distance Geometry",
        "author": "Chandra Kundu, Abiy Tasissa, and HanQin Cai",
        "link": "http://arxiv.org/abs/2505.18414v1",
        "abstract": "Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean\ndistances of a given point configuration, finds many applications in modern\nmachine learning. This paper considers the setting where only a set of anchor\nnodes is used to collect the distances between themselves and the rest. In the\npresence of potential outliers, it results in a structured partial observation\non EDM with partial corruptions. Note that an EDM can be connected to a\npositive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by\nrecent development of non-orthogonal dual basis in optimization, we propose a\nnovel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual\nBasis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the\nunderlying point configuration. The exact recovery guarantees have been\nestablished in terms of both the Gram matrix and point configuration, under\nsome mild conditions. Empirical experiments show superior performance of\nRoDEoDB on sensor localization and molecular conformation datasets."
    },
    {
        "date": "2025-05",
        "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks",
        "author": "Yuqi Jia, Zedian Shao, Yupei Liu, Jinyuan Jia, Dawn Song, and Neil Zhenqiang Gong",
        "link": "http://arxiv.org/abs/2505.18333v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and\nseveral defenses have recently been proposed, often claiming to mitigate these\nattacks successfully. However, we argue that existing studies lack a principled\napproach to evaluating these defenses. In this paper, we argue the need to\nassess defenses across two critical dimensions: (1) effectiveness, measured\nagainst both existing and adaptive prompt injection attacks involving diverse\ntarget and injected prompts, and (2) general-purpose utility, ensuring that the\ndefense does not compromise the foundational capabilities of the LLM. Our\ncritical evaluation reveals that prior studies have not followed such a\ncomprehensive evaluation methodology. When assessed using this principled\napproach, we show that existing defenses are not as successful as previously\nreported. This work provides a foundation for evaluating future defenses and\nguiding their development. Our code and data are available at:\nhttps://github.com/PIEval123/PIEval."
    },
    {
        "date": "2025-05",
        "title": "An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs",
        "author": "Rahul Thomas, Louai Zahran, Erica Choi, Akilesh Potti, Micah Goldblum, and Arka Pal",
        "link": "http://arxiv.org/abs/2505.18332v1",
        "abstract": "Recent advances in Large Language Models (LLMs) have led to the widespread\nadoption of third-party inference services, raising critical privacy concerns.\nExisting methods of performing private third-party inference, such as Secure\nMultiparty Computation (SMPC), often rely on cryptographic methods. However,\nthese methods are thousands of times slower than standard unencrypted\ninference, and fail to scale to large modern LLMs. Therefore, recent lines of\nwork have explored the replacement of expensive encrypted nonlinear\ncomputations in SMPC with statistical obfuscation methods - in particular,\nrevealing permuted hidden states to the third parties, with accompanying strong\nclaims of the difficulty of reversal into the unpermuted states. In this work,\nwe begin by introducing a novel reconstruction technique that can recover\noriginal prompts from hidden states with nearly perfect accuracy across\nmultiple state-of-the-art LLMs. We then show that extensions of our attack are\nnearly perfectly effective in reversing permuted hidden states of LLMs,\ndemonstrating the insecurity of three recently proposed privacy schemes. We\nfurther dissect the shortcomings of prior theoretical `proofs' of permuation\nsecurity which allow our attack to succeed. Our findings highlight the\nimportance of rigorous security analysis in privacy-preserving LLM inference."
    },
    {
        "date": "2025-05",
        "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
        "author": "Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, and Ningyu Zhang",
        "link": "http://arxiv.org/abs/2505.20322v1",
        "abstract": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."
    },
    {
        "date": "2025-05",
        "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles",
        "author": "Varun Ajith, Anindya Pal, Saumik Bhattacharya, and Sayantari Ghosh",
        "link": "http://arxiv.org/abs/2505.18106v1",
        "abstract": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields."
    },
    {
        "date": "2025-05",
        "title": "Towards more transferable adversarial attack in black-box manner",
        "author": "Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, and Chun Pong Lau",
        "link": "http://arxiv.org/abs/2505.18097v1",
        "abstract": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses."
    },
    {
        "date": "2025-05",
        "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
        "author": "Zhishuai Liu, and Pan Xu",
        "link": "http://arxiv.org/abs/2505.18044v1",
        "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting."
    },
    {
        "date": "2025-05",
        "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
        "author": "Changyeol Lee, Yongho Shin, and Hyung-Chan An",
        "link": "http://arxiv.org/abs/2505.18043v1",
        "abstract": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature."
    },
    {
        "date": "2025-05",
        "title": "Superplatforms Have to Attack AI Agents",
        "author": "Jianghao Lin, Jiachen Zhu, Zheli Zhou, Yunjia Xi, Weiwen Liu, Yong Yu, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2505.17861v1",
        "abstract": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents."
    },
    {
        "date": "2025-05",
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
        "author": "Masahiro Fujisawa, Masaki Adachi, and Michael A. Osborne",
        "link": "http://arxiv.org/abs/2505.17859v1",
        "abstract": "Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods."
    },
    {
        "date": "2025-05",
        "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems",
        "author": "Yuanya She",
        "link": "http://arxiv.org/abs/2505.18234v1",
        "abstract": "In this paper, we propose a robust and reinforcement-learning-enhanced\nnetwork intrusion detection system (NIDS) designed for class-imbalanced and\nfew-shot attack scenarios in Industrial Internet of Things (IIoT) environments.\nOur model integrates a TabTransformer for effective tabular feature\nrepresentation with Proximal Policy Optimization (PPO) to optimize\nclassification decisions via policy learning. Evaluated on the\nTON\\textunderscore IoT benchmark, our method achieves a macro F1-score of\n97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes\nlike man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%,\nshowcasing strong robustness and few-shot detection capabilities. Extensive\nablation experiments confirm the complementary roles of TabTransformer and PPO\nin mitigating class imbalance and improving generalization. These results\nhighlight the potential of combining transformer-based tabular learning with\nreinforcement learning for real-world NIDS applications."
    },
    {
        "date": "2025-05",
        "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
        "author": "Anna Van Elst, Igor Colin, and Stephan Cl\u00e9men\u00e7on",
        "link": "http://arxiv.org/abs/2505.17836v2",
        "abstract": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(\\log(t)/t)$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes."
    },
    {
        "date": "2025-05",
        "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
        "author": "Nicolas Castanet, Olivier Sigaud, and Sylvain Lamprier",
        "link": "http://arxiv.org/abs/2505.17830v1",
        "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge."
    },
    {
        "date": "2025-05",
        "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition",
        "author": "Ping Li, Jianan Ni, and Bo Pang",
        "link": "http://arxiv.org/abs/2505.17807v1",
        "abstract": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid."
    },
    {
        "date": "2025-05",
        "title": "Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture",
        "author": "Ildi Alla, and Valeria Loscri",
        "link": "http://arxiv.org/abs/2505.17776v1",
        "abstract": "Emerging 5G millimeter-wave and sub-6 GHz networks enable high-accuracy\nindoor localization, but security and privacy vulnerabilities pose serious\nchallenges. In this paper, we identify and address threats including location\nspoofing and adversarial signal manipulation against 5G-based indoor\nlocalization. We formalize a threat model encompassing attackers who inject\nforged radio signals or perturb channel measurements to mislead the\nlocalization system. To defend against these threats, we propose an\nadversary-resilient localization architecture that combines deep learning\nfingerprinting with physical domain knowledge. Our approach integrates\nmulti-anchor Channel Impulse Response (CIR) fingerprints with Time Difference\nof Arrival (TDoA) features and known anchor positions in a hybrid Convolutional\nNeural Network (CNN) and multi-head attention network. This design inherently\nchecks geometric consistency and dynamically down-weights anomalous signals,\nmaking localization robust to tampering. We formulate the secure localization\nproblem and demonstrate, through extensive experiments on a public 5G indoor\ndataset, that the proposed system achieves a mean error approximately 0.58 m\nunder mixed Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) trajectories in\nbenign conditions and gracefully degrades to around 0.81 m under attack\nscenarios. We also show via ablation studies that each architecture component\n(attention mechanism, TDoA, etc.) is critical for both accuracy and resilience,\nreducing errors by 4-5 times compared to baselines. In addition, our system\nruns in real-time, localizing the user in just 1 ms on a simple CPU. The code\nhas been released to ensure reproducibility\n(https://github.com/sec5gloc/Sec5GLoc)."
    },
    {
        "date": "2025-05",
        "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation",
        "author": "Akira Tanimoto",
        "link": "http://arxiv.org/abs/2505.17717v1",
        "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods."
    },
    {
        "date": "2025-05",
        "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors",
        "author": "Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, and Yong Li",
        "link": "http://arxiv.org/abs/2505.17682v1",
        "abstract": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports",
        "author": "Hayato Aida, Kosuke Takahashi, and Takahiro Omi",
        "link": "http://arxiv.org/abs/2505.17625v1",
        "abstract": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats."
    },
    {
        "date": "2025-05",
        "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
        "author": "Linbao Li, Yannan Liu, Daojing He, and Yu Li",
        "link": "http://arxiv.org/abs/2505.17598v1",
        "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack."
    },
    {
        "date": "2025-05",
        "title": "Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications",
        "author": "Kushal Khatiwada, Jayden Hopper, Joseph Cheatham, Ayan Joshi, and Sabur Baidya",
        "link": "http://arxiv.org/abs/2505.17586v1",
        "abstract": "The Internet of Things (IoT) and Large Language Models (LLMs) have been two\nmajor emerging players in the information technology era. Although there has\nbeen significant coverage of their individual capabilities, our literature\nsurvey sheds some light on the integration and interaction of LLMs and IoT\ndevices - a mutualistic relationship in which both parties leverage the\ncapabilities of the other. LLMs like OpenAI's ChatGPT, Anthropic's Claude,\nGoogle's Gemini/BERT, any many more, all demonstrate powerful capabilities in\nnatural language understanding and generation, enabling more intuitive and\ncontext-aware interactions across diverse IoT applications such as smart\ncities, healthcare systems, industrial automation, and smart home environments.\nDespite these opportunities, integrating these resource-intensive LLMs into IoT\ndevices that lack the state-of-the-art computational power is a challenging\ntask. The security of these edge devices is another major concern as they can\neasily act as a backdoor to private networks if the LLM integration is sloppy\nand unsecured. This literature survey systematically explores the current\nstate-of-the-art in applying LLMs within IoT, emphasizing their applications in\nvarious domains/sectors of society, the significant role they play in enhancing\nIoT security through anomaly detection and threat mitigation, and strategies\nfor effective deployment using edge computing frameworks. Finally, this survey\nhighlights existing challenges, identifies future research directions, and\nunderscores the need for cross-disciplinary collaboration to fully realize the\ntransformative potential of integrating LLMs and IoT."
    },
    {
        "date": "2025-05",
        "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation",
        "author": "Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, and Eisuke Koizumi",
        "link": "http://arxiv.org/abs/2505.17579v1",
        "abstract": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack."
    },
    {
        "date": "2025-05",
        "title": "Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters",
        "author": "Kwangsu Lee",
        "link": "http://arxiv.org/abs/2505.17527v1",
        "abstract": "Distributed broadcast encryption (DBE) is a variant of broadcast encryption\n(BE) that can efficiently transmit a message to a subset of users, in which\nusers independently generate user private keys and user public keys instead of\na central trusted authority generating user keys. In this paper, we propose a\nDBE scheme with constant size ciphertexts, constant size private keys, and\nlinear size public parameters, and prove the adaptive security of our DBE\nscheme under static assumptions in composite-order bilinear groups. The\nprevious efficient DBE schemes with constant size ciphertexts and constant size\nprivate keys are proven secure under the $q$-Type assumption or have a drawback\nof having quadratic size public parameters. In contrast, our DBE scheme is the\nfirst DBE scheme with linear size public parameters proven adaptively secure\nunder static assumptions in composite-order bilinear groups."
    },
    {
        "date": "2025-05",
        "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning",
        "author": "Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2505.17509v1",
        "abstract": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings."
    },
    {
        "date": "2025-05",
        "title": "Demonstration of Quantum-Secure Communications in a Nuclear Reactor",
        "author": "Konstantinos Gkouliaras, Vasileios Theos, True Miller, Brian Jowers, George Kennedy, Andy Grant, Terry Cronin, Philip G. Evans, and Stylianos Chatzidakis",
        "link": "http://arxiv.org/abs/2505.17502v2",
        "abstract": "Quantum key distribution (QKD), one of the latest cryptographic techniques,\nfounded on the laws of quantum mechanics rather than mathematical complexity,\npromises for the first time unconditional secure remote communications.\nIntegrating this technology into the next generation nuclear systems - designed\nfor universal data collection and real-time sharing as well as cutting-edge\ninstrumentation and increased dependency on digital technologies - could\nprovide significant benefits enabling secure, unattended, and autonomous\noperation in remote areas, e.g., microreactors and fission batteries. However,\nany practical implementation on a critical reactor system must meet strict\nrequirements on latency, control system compatibility, stability, and\nperformance under operational transients. Here, we report the complete\nend-to-end demonstration of a phase-encoding decoy-state BB84 protocol QKD\nsystem under prototypic conditions on Purdue's fully digital nuclear reactor,\nPUR-1. The system was installed in PUR-1 successfully executing real-time\nencryption and decryption of 2,000 signals over optic fiber distances up to 82\nkm using OTP-based encryption and up to 140 km with AES-based encryption. For a\ncore of 68 signals, OTP-secure communication was achieved for up to 135 km. The\nQKD system maintained a stable secret key rate of 320 kbps and a quantum bit\nerror of 3.8% at 54 km. Our results demonstrate that OTP-based encryption\nintroduces minimal latency while the more key-efficient AES and ASCON\nencryption schemes can significantly increase the number of signals encrypted\nwithout latency penalties. Additionally, implementation of a dynamic key pool\nensures several hours of secure key availability during potential system\ndowntimes. This work shows the potential of quantum-based secure remote\ncommunications for future digitally driven nuclear reactor technologies."
    },
    {
        "date": "2025-05",
        "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
        "author": "Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, and Kaixiang Yang",
        "link": "http://arxiv.org/abs/2505.17501v1",
        "abstract": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance."
    },
    {
        "date": "2025-05",
        "title": "SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy",
        "author": "Junru Lin, Mingzhe Liu, Songze Li, and Xuechao Wang",
        "link": "http://arxiv.org/abs/2505.17466v1",
        "abstract": "Recent years have witnessed a rapid development of platform economy, as it\neffectively addresses the trust dilemma between untrusted online buyers and\nmerchants. However, malicious platforms can misuse users' funds and\ninformation, causing severe security concerns. Previous research efforts aimed\nat enhancing security in platform payment systems often sacrificed processing\nperformance, while those focusing on processing efficiency struggled to\ncompletely prevent fund and information misuse. In this paper, we introduce\nSecurePay, a secure, yet performant payment processing system for platform\neconomy. SecurePay is the first payment system that combines permissioned\nblockchain with central bank digital currency (CBDC) to ensure fund security,\ninformation security, and resistance to collusion by intermediaries; it also\nfacilitates counter-party auditing, closed-loop regulation, and enhances\noperational efficiency for transaction settlement. We develop a full\nimplementation of the proposed SecurePay system, and our experiments conducted\non personal devices demonstrate a throughput of 256.4 transactions per second\nand an average latency of 4.29 seconds, demonstrating a comparable processing\nefficiency with a centralized system, with a significantly improved security\nlevel."
    },
    {
        "date": "2025-05",
        "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds",
        "author": "Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, and Junhui Hou",
        "link": "http://arxiv.org/abs/2505.17442v1",
        "abstract": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline."
    },
    {
        "date": "2025-05",
        "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models",
        "author": "Hefei Mei, Zirui Wang, Shen You, Minjing Dong, and Chang Xu",
        "link": "http://arxiv.org/abs/2505.17440v1",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM"
    },
    {
        "date": "2025-05",
        "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness",
        "author": "Enyi Jiang, Changming Xu, Nischay Singh, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2505.17406v1",
        "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Robustness of Nonparametric Regression",
        "author": "Parsa Moradi, Hanzaleh Akabrinodehi, and Mohammad Ali Maddah-Ali",
        "link": "http://arxiv.org/abs/2505.17356v1",
        "abstract": "In this paper, we investigate the adversarial robustness of regression, a\nfundamental problem in machine learning, under the setting where an adversary\ncan arbitrarily corrupt a subset of the input data. While the robustness of\nparametric regression has been extensively studied, its nonparametric\ncounterpart remains largely unexplored. We characterize the adversarial\nrobustness in nonparametric regression, assuming the regression function\nbelongs to the second-order Sobolev space (i.e., it is square integrable up to\nits second derivative).\n  The contribution of this paper is two-fold: (i) we establish a minimax lower\nbound on the estimation error, revealing a fundamental limit that no estimator\ncan overcome, and (ii) we show that, perhaps surprisingly, the classical\nsmoothing spline estimator, when properly regularized, exhibits robustness\nagainst adversarial corruption. These results imply that if $o(n)$ out of $n$\nsamples are corrupted, the estimation error of the smoothing spline vanishes as\n$n \\to \\infty$. On the other hand, when a constant fraction of the data is\ncorrupted, no estimator can guarantee vanishing estimation error, implying the\noptimality of the smoothing spline in terms of maximum tolerable number of\ncorrupted samples."
    },
    {
        "date": "2025-05",
        "title": "Secure Parsing and Serializing with Separation Logic Applied to CBOR, CDDL, and COSE",
        "author": "Tahina Ramananandro, Gabriel Ebner, Guido Mart\u00ednez, and Nikhil Swamy",
        "link": "http://arxiv.org/abs/2505.17335v1",
        "abstract": "Incorrect handling of security-critical data formats, particularly in\nlow-level languages, are the root cause of many security vulnerabilities.\nProvably correct parsing and serialization tools that target languages like C\ncan help. Towards this end, we present PulseParse, a library of verified parser\nand serializer combinators for non-malleable binary formats. Specifications and\nproofs in PulseParse are in separation logic, offering a more abstract and\ncompositional interface, with full support for data validation, parsing, and\nserialization. PulseParse also supports a class of recursive formats -- with a\nfocus on security and handling adversarial inputs, we show how to parse such\nformats with only a constant amount of stack space.\n  We use PulseParse at scale by providing the first formalization of CBOR, a\nrecursive, binary data format standard, with growing adoption in various\nindustrial standards. We prove that the deterministic fragment of CBOR is\nnon-malleable and provide EverCBOR, a verified library in both C and Rust to\nvalidate, parse, and serialize CBOR objects implemented using PulseParse. Next,\nwe provide the first formalization of CDDL, a schema definition language for\nCBOR. We identify well-formedness conditions on CDDL definitions that ensure\nthat they yield unambiguous, non-malleable formats, and implement EverCDDL, a\ntool that checks that a CDDL definition is well-formed, and then produces\nverified parsers and serializers for it.\n  To evaluate our work, we use EverCDDL to generate verified parsers and\nserializers for various security-critical applications. Notably, we build a\nformally verified implementation of COSE signing, a standard for\ncryptographically signed objects. We also use our toolchain to generate\nverified code for other standards specified in CDDL, including DICE Protection\nEnvironment, a secure boot protocol standard."
    },
    {
        "date": "2025-05",
        "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning",
        "author": "Dylan Kline",
        "link": "http://arxiv.org/abs/2505.17328v1",
        "abstract": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames."
    },
    {
        "date": "2025-05",
        "title": "Advancing Security with Digital Twins: A Comprehensive Survey",
        "author": "Blessing Airehenbuwa, Touseef Hasan, Souvika Sarkar, and Ujjwal Guin",
        "link": "http://arxiv.org/abs/2505.17310v1",
        "abstract": "The proliferation of electronic devices has greatly transformed every aspect\nof human life, such as communication, healthcare, transportation, and energy.\nUnfortunately, the global electronics supply chain is vulnerable to various\nattacks, including piracy of intellectual properties, tampering,\ncounterfeiting, information leakage, side-channel, and fault injection attacks,\ndue to the complex nature of electronic products and vulnerabilities present in\nthem. Although numerous solutions have been proposed to address these threats,\nsignificant gaps remain, particularly in providing scalable and comprehensive\nprotection against emerging attacks. Digital twin, a dynamic virtual replica of\na physical system, has emerged as a promising solution to address these issues\nby providing backward traceability, end-to-end visibility, and continuous\nverification of component integrity and behavior. In this paper, we present a\ncomprehensive survey of the application of digital twins based on their\nfunctional role and application domains. We comprehensively present recent\ndigital twin-based security implementations, including their role in\ncyber-physical systems, Internet of Things, and cryptographic systems,\ndetection of counterfeit electronics, intrusion detection, fault injection, and\nside-channel leakage. To the best of our knowledge, it is the first study to\nconsolidate these security use cases into a unified reference. The paper also\nexplores the integration of large language models with digital twins for\nenhanced security and discusses current challenges, solutions, and future\nresearch directions."
    },
    {
        "date": "2025-05",
        "title": "Approach to Finding a Robust Deep Learning Model",
        "author": "Alexey Boldyrev, Fedor Ratnikov, and Andrey Shevelev",
        "link": "http://arxiv.org/abs/2505.17254v1",
        "abstract": "The rapid development of machine learning (ML) and artificial intelligence\n(AI) applications requires the training of large numbers of models. This\ngrowing demand highlights the importance of training models without human\nsupervision, while ensuring that their predictions are reliable. In response to\nthis need, we propose a novel approach for determining model robustness. This\napproach, supplemented with a proposed model selection algorithm designed as a\nmeta-algorithm, is versatile and applicable to any machine learning model,\nprovided that it is appropriate for the task at hand. This study demonstrates\nthe application of our approach to evaluate the robustness of deep learning\nmodels. To this end, we study small models composed of a few convolutional and\nfully connected layers, using common optimizers due to their ease of\ninterpretation and computational efficiency. Within this framework, we address\nthe influence of training sample size, model weight initialization, and\ninductive bias on the robustness of deep learning models."
    },
    {
        "date": "2025-05",
        "title": "Understanding the Security Landscape of Embedded Non-Volatile Memories: A Comprehensive Survey",
        "author": "Zakia Tamanna Tisha, and Ujjwal Guin",
        "link": "http://arxiv.org/abs/2505.17253v1",
        "abstract": "The modern semiconductor industry requires memory solutions that can keep\npace with the high-speed demands of high-performance computing. Embedded\nnon-volatile memories (eNVMs) address these requirements by offering faster\naccess to stored data at an improved computational throughput and efficiency.\nFurthermore, these technologies offer numerous appealing features, including\nlimited area-energy-runtime budget and data retention capabilities. Among\nthese, the data retention feature of eNVMs has garnered particular interest\nwithin the semiconductor community. Although this property allows eNVMs to\nretain data even in the absence of a continuous power supply, it also\nintroduces some vulnerabilities, prompting security concerns. These concerns\nhave sparked increased interest in examining the broader security implications\nassociated with eNVM technologies. This paper examines the security aspects of\neNVMs by discussing the reasons for vulnerabilities in specific memories from\nan architectural point of view. Additionally, this paper extensively reviews\neNVM-based security primitives, such as physically unclonable functions and\ntrue random number generators, as well as techniques like logic obfuscation.\nThe paper also explores a broad spectrum of security threats to eNVMs,\nincluding physical attacks such as side-channel attacks, fault injection, and\nprobing, as well as logical threats like information leakage,\ndenial-of-service, and thermal attacks. Finally, the paper presents a study of\npublication trends in the eNVM domain since the early 2000s, reflecting the\nrising momentum and research activity in this field."
    },
    {
        "date": "2025-05",
        "title": "Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation",
        "author": "Kun Yang, and Neena Imam",
        "link": "http://arxiv.org/abs/2505.17226v1",
        "abstract": "Federated Learning (FL) enables collaborative machine learning across\ndecentralized data sources without sharing raw data. It offers a promising\napproach to privacy-preserving AI. However, FL remains vulnerable to\nadversarial threats from malicious participants, referred to as Byzantine\nclients, who can send misleading updates to corrupt the global model.\nTraditional aggregation methods, such as simple averaging, are not robust to\nsuch attacks. More resilient approaches, like the Krum algorithm, require prior\nknowledge of the number of malicious clients, which is often unavailable in\nreal-world scenarios. To address these limitations, we propose Average-rKrum\n(ArKrum), a novel aggregation strategy designed to enhance both the resilience\nand privacy guarantees of FL systems. Building on our previous work (rKrum),\nArKrum introduces two key innovations. First, it includes a median-based\nfiltering mechanism that removes extreme outliers before estimating the number\nof adversarial clients. Second, it applies a multi-update averaging scheme to\nimprove stability and performance, particularly when client data distributions\nare not identical. We evaluate ArKrum on benchmark image and text datasets\nunder three widely studied Byzantine attack types. Results show that ArKrum\nconsistently achieves high accuracy and stability. It performs as well as or\nbetter than other robust aggregation methods. These findings demonstrate that\nArKrum is an effective and practical solution for secure FL systems in\nadversarial environments."
    },
    {
        "date": "2025-05",
        "title": "Dynamic Encryption-Based Cloud Security Model using Facial Image and Password-based Key Generation for Multimedia Data",
        "author": "Naima Sultana Ayesha, Mehrin Anannya, Md Biplob Hosen, and Rashed Mazumder",
        "link": "http://arxiv.org/abs/2505.17224v1",
        "abstract": "In this cloud-dependent era, various security techniques, such as encryption,\nsteganography, and hybrid approaches, have been utilized in cloud computing to\nenhance security, maintain enormous storage capacity, and provide ease of\naccess. However, the absence of data type-specific encryption and decryption\nprocedures renders multimedia data vulnerable. To address this issue, this\nstudy presents a dynamic encryption-based security architecture that adapts\nencryption methods to any file type, using keys generated from facial images\nand passwords. Four diverse datasets are created, each with a consistent size\nof 2GB, containing varying combinations of image, audio (MP3 and MPEG), video,\ntext, CSV, PPT, and PDF files, to implement the proposed methodology. AES is\nused to encrypt image data, AES-CTR is employed for audio or video data to meet\nreal-time streaming needs, and Blowfish is used for other types of data.\nPerformance analysis on all four datasets is conducted using AWS servers, where\nDATASET-1 demonstrates the best performance compared to the others."
    },
    {
        "date": "2025-05",
        "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval",
        "author": "Nandan Thakur, Crystina Zhang, Xueguang Ma, and Jimmy Lin",
        "link": "http://arxiv.org/abs/2505.16967v1",
        "abstract": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."
    },
    {
        "date": "2025-05",
        "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
        "author": "Csaba D\u00e9k\u00e1ny, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16947v1",
        "abstract": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."
    },
    {
        "date": "2025-05",
        "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models",
        "author": "Xiang Li, Yong Tao, Siyuan Zhang, Siwei Liu, Zhitong Xiong, Chunbo Luo, Lu Liu, Mykola Pechenizkiy, Xiao Xiang Zhu, and Tianjin Huang",
        "link": "http://arxiv.org/abs/2505.16793v1",
        "abstract": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models."
    },
    {
        "date": "2025-05",
        "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models",
        "author": "Zhenzhen Ren, GuoBiao Li, Sheng Li, Zhenxing Qian, and Xinpeng Zhang",
        "link": "http://arxiv.org/abs/2505.16785v1",
        "abstract": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."
    },
    {
        "date": "2025-05",
        "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques",
        "author": "Jianing Geng, Biao Yi, Zekun Fei, Tongxi Wu, Lihai Nie, and Zheli Liu",
        "link": "http://arxiv.org/abs/2505.16765v1",
        "abstract": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"
    },
    {
        "date": "2025-05",
        "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP",
        "author": "Alya Zouzou, L\u00e9o and\u00e9ol, M\u00e9lanie Ducoffe, and Ryma Boumazouza",
        "link": "http://arxiv.org/abs/2505.16740v1",
        "abstract": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain."
    },
    {
        "date": "2025-05",
        "title": "Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting",
        "author": "Bang Trinh Tran To, and Thai Le",
        "link": "http://arxiv.org/abs/2505.17160v1",
        "abstract": "This work presents LURK (Latent UnleaRned Knowledge), a novel framework that\nprobes for hidden retained knowledge in unlearned LLMs through adversarial\nsuffix prompting. LURK automatically generates adversarial prompt suffixes\ndesigned to elicit residual knowledge about the Harry Potter domain, a commonly\nused benchmark for unlearning. Our experiments reveal that even models deemed\nsuccessfully unlearned can leak idiosyncratic information under targeted\nadversarial conditions, highlighting critical limitations of current unlearning\nevaluation standards. By uncovering latent knowledge through indirect probing,\nLURK offers a more rigorous and diagnostic tool for assessing the robustness of\nunlearning algorithms. All code will be publicly available."
    },
    {
        "date": "2025-05",
        "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting",
        "author": "Youngmoon Jung, Yong-Hyeok Lee, Myunghun Jung, Jaeyoung Roh, Chang Woo Han, and Hoon-Young Cho",
        "link": "http://arxiv.org/abs/2505.16735v2",
        "abstract": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct extensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach."
    },
    {
        "date": "2025-05",
        "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks",
        "author": "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi\u0107, and Martin Vechev",
        "link": "http://arxiv.org/abs/2505.16723v1",
        "abstract": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."
    },
    {
        "date": "2025-05",
        "title": "Experimental robustness benchmark of quantum neural network on a superconducting quantum processor",
        "author": "Hai-Feng Zhang, Zhao-Yun Chen, Peng Wang, Liang-Liang Guo, Tian-Le Wang, Xiao-Yan Yang, Ren-Ze Zhao, Ze-An Zhao, Sheng Zhang, Lei Du, Hao-Ran Tao, Zhi-Long Jia, Wei-Cheng Kong, Huan-Yu Liu, Athanasios V. Vasilakos, Yang Yang, Yu-Chun Wu, Ji Guan, Peng Duan, and Guo-Ping Guo",
        "link": "http://arxiv.org/abs/2505.16714v1",
        "abstract": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications."
    },
    {
        "date": "2025-05",
        "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models",
        "author": "Xiaobei Yan, Yiming Li, Zhaoxin Fan, Han Qiu, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2505.16670v2",
        "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs."
    },
    {
        "date": "2025-05",
        "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models",
        "author": "Yiwei Sun, Peiqi Jiang, Chuanbin Liu, Luohao Lin, Zhiying Lu, and Hongtao Xie",
        "link": "http://arxiv.org/abs/2505.16643v1",
        "abstract": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}"
    },
    {
        "date": "2025-05",
        "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization",
        "author": "Xueyang Zhou, Guiyao Tie, Guowen Zhang, Hechang Wang, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2505.16640v1",
        "abstract": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/."
    },
    {
        "date": "2025-05",
        "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving",
        "author": "Huitong Yang, Zhuoxiao Chen, Fengyi Zhang, Zi Huang, and Yadan Luo",
        "link": "http://arxiv.org/abs/2505.16524v1",
        "abstract": "Maintaining robust 3D perception under dynamic and unpredictable test-time\nconditions remains a critical challenge for autonomous driving systems.\nExisting test-time adaptation (TTA) methods often fail in high-variance tasks\nlike 3D object detection due to unstable optimization and sharp minima. While\nrecent model merging strategies based on linear mode connectivity (LMC) offer\nimproved stability by interpolating between fine-tuned checkpoints, they are\ncomputationally expensive, requiring repeated checkpoint access and multiple\nforward passes. In this paper, we introduce CodeMerge, a lightweight and\nscalable model merging framework that bypasses these limitations by operating\nin a compact latent space. Instead of loading full models, CodeMerge represents\neach checkpoint with a low-dimensional fingerprint derived from the source\nmodel's penultimate features and constructs a key-value codebook. We compute\nmerging coefficients using ridge leverage scores on these fingerprints,\nenabling efficient model composition without compromising adaptation quality.\nOur method achieves strong performance across challenging benchmarks, improving\nend-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by\nover 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as\nonline mapping, motion prediction and planning even without training. Code and\npretrained models are released in the supplementary material."
    },
    {
        "date": "2025-05",
        "title": "Language-based Security and Time-inserting Supervisor",
        "author": "Damas P. Gruska",
        "link": "http://arxiv.org/abs/2505.16503v1",
        "abstract": "Algebraic methods are employed in order to define language-based security\nproperties of processes. A supervisor is introduced that can disable unwanted\nbehavior of an insecure process by controlling some of its actions or by\ninserting timed actions to make an insecure process secure. We assume a\nsituation where neither the supervisor nor the attacker has complete\ninformation about the ongoing systems behavior. We study the conditions under\nwhich such a supervisor exists, as well as its properties and limitations."
    },
    {
        "date": "2025-05",
        "title": "Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models",
        "author": "Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Jin",
        "link": "http://arxiv.org/abs/2505.16446v1",
        "abstract": "Multimodal large language models (MLLMs) enable powerful cross-modal\nreasoning capabilities. However, the expanded input space introduces new attack\nsurfaces. Previous jailbreak attacks often inject malicious instructions from\ntext into less aligned modalities, such as vision. As MLLMs increasingly\nincorporate cross-modal consistency and alignment mechanisms, such explicit\nattacks become easier to detect and block. In this work, we propose a novel\nimplicit jailbreak framework termed IJA that stealthily embeds malicious\ninstructions into images via least significant bit steganography and couples\nthem with seemingly benign, image-related textual prompts. To further enhance\nattack effectiveness across diverse MLLMs, we incorporate adversarial suffixes\ngenerated by a surrogate model and introduce a template optimization module\nthat iteratively refines both the prompt and embedding based on model feedback.\nOn commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack\nsuccess rates of over 90% using an average of only 3 queries."
    },
    {
        "date": "2025-05",
        "title": "Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach",
        "author": "Huazi Pan, Yanjun Zhang, Leo Yu Zhang, Scott Adams, Abbas Kouzani, and Suiyang Khoo",
        "link": "http://arxiv.org/abs/2505.16403v2",
        "abstract": "Manipulation of local training data and local updates, i.e., the poisoning\nattack, is the main threat arising from the collaborative nature of the\nfederated learning (FL) paradigm. Most existing poisoning attacks aim to\nmanipulate local data/models in a way that causes denial-of-service (DoS)\nissues. In this paper, we introduce a novel attack method, named Federated\nLearning Sliding Attack (FedSA) scheme, aiming at precisely introducing the\nextent of poisoning in a subtle controlled manner. It operates with a\npredefined objective, such as reducing global model's prediction accuracy by\n10%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)\ntheory with model poisoning attacks. It can manipulate the updates from\nmalicious clients to drive the global model towards a compromised state,\nachieving this at a controlled and inconspicuous rate. Additionally, leveraging\nthe robust control properties of FedSA allows precise control over the\nconvergence bounds, enabling the attacker to set the global accuracy of the\npoisoned model to any desired level. Experimental results demonstrate that\nFedSA can accurately achieve a predefined global accuracy with fewer malicious\nclients while maintaining a high level of stealth and adjustable learning\nrates."
    },
    {
        "date": "2025-05",
        "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems",
        "author": "Yuanhao Huang, Yilong Ren, Jinlei Wang, Lujia Huo, Xuesong Bai, Jinchuan Zhang, and Haiyan Yu",
        "link": "http://arxiv.org/abs/2505.16402v1",
        "abstract": "Autonomous vehicles are typical complex intelligent systems with artificial\nintelligence at their core. However, perception methods based on deep learning\nare extremely vulnerable to adversarial samples, resulting in safety accidents.\nHow to generate effective adversarial examples in the physical world and\nevaluate object detection systems is a huge challenge. In this study, we\npropose a unified joint adversarial training framework for both 2D and 3D\nsamples to address the challenges of intra-class diversity and environmental\nvariations in real-world scenarios. Building upon this framework, we introduce\nan adversarial sample reality enhancement approach that incorporates non-rigid\nsurface modeling and a realistic 3D matching mechanism. We compare with 5\nadvanced adversarial patches and evaluate their attack performance on 8 object\ndetecotrs, including single-stage, two-stage, and transformer-based models.\nExtensive experiment results in digital and physical environments demonstrate\nthat the adversarial textures generated by our method can effectively mislead\nthe target detection model. Moreover, proposed method demonstrates excellent\nrobustness and transferability under multi-angle attacks, varying lighting\nconditions, and different distance in the physical world. The demo video and\ncode can be obtained at https://github.com/Huangyh98/AdvReal.git."
    },
    {
        "date": "2025-05",
        "title": "Consistent and Compatible Modelling of Cyber Intrusions and Incident Response Demonstrated in the Context of Malware Attacks on Critical Infrastructure",
        "author": "Peter Maynard, Yulia Cherdantseva, Avi Shaked, Pete Burnap, and Arif Mehmood",
        "link": "http://arxiv.org/abs/2505.16398v1",
        "abstract": "Cyber Security Incident Response (IR) Playbooks are used to capture the steps\nrequired to recover from a cyber intrusion. Individual IR playbooks should\nfocus on a specific type of incident and be aligned with the architecture of a\nsystem under attack. Intrusion modelling focuses on a specific potential cyber\nintrusion and is used to identify where and what countermeasures are needed,\nand the resulting intrusion models are expected to be used in effective IR,\nideally by feeding IR Playbooks designs. IR playbooks and intrusion models,\nhowever, are created in isolation and at varying stages of the system's\nlifecycle. We take nine critical national infrastructure intrusion models -\nexpressed using Sequential AND Attack Trees - and transform them into models of\nthe same format as IR playbooks. We use Security Modelling Framework for\nmodelling attacks and playbooks, and for demonstrating the feasibility of the\nbetter integration between risk assessment and IR at the modelling level. This\nresults in improved intrusion models and tighter coupling between IR playbooks\nand threat modelling which - as we demonstrate - yields novel insights into the\nanalysis of attacks and response actions. The main contributions of this paper\nare (a) a novel way of representing attack trees using the Security Modelling\nFramework,(b) a new tool for converting Sequential AND attack trees into models\ncompatible with playbooks, and (c) the examples of nine intrusion models\nrepresented using the Security Modelling Framework."
    },
    {
        "date": "2025-05",
        "title": "Efficient Motion Prompt Learning for Robust Visual Tracking",
        "author": "Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, and Huchuan Lu",
        "link": "http://arxiv.org/abs/2505.16321v1",
        "abstract": "Due to the challenges of processing temporal information, most trackers\ndepend solely on visual discriminability and overlook the unique temporal\ncoherence of video data. In this paper, we propose a lightweight and\nplug-and-play motion prompt tracking method. It can be easily integrated into\nexisting vision-based trackers to build a joint tracking framework leveraging\nboth motion and vision cues, thereby achieving robust tracking through\nefficient prompt learning. A motion encoder with three different positional\nencodings is proposed to encode the long-term motion trajectory into the visual\nembedding space, while a fusion decoder and an adaptive weight mechanism are\ndesigned to dynamically fuse visual and motion features. We integrate our\nmotion module into three different trackers with five models in total.\nExperiments on seven challenging tracking benchmarks demonstrate that the\nproposed motion module significantly improves the robustness of vision-based\ntrackers, with minimal training costs and negligible speed sacrifice. Code is\navailable at https://github.com/zj5559/Motion-Prompt-Tracking."
    },
    {
        "date": "2025-05",
        "title": "SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models",
        "author": "Hossein Khalili, Seongbin Park, Venkat Bollapragada, and Nader Sehatbakhsh",
        "link": "http://arxiv.org/abs/2505.16318v1",
        "abstract": "As vision-based machine learning models are increasingly integrated into\nautonomous and cyber-physical systems, concerns about (physical) adversarial\npatch attacks are growing. While state-of-the-art defenses can achieve\ncertified robustness with minimal impact on utility against highly-concentrated\nlocalized patch attacks, they fall short in two important areas: (i)\nState-of-the-art methods are vulnerable to low-noise distributed patches where\nperturbations are subtly dispersed to evade detection or masking, as shown\nrecently by the DorPatch attack; (ii) Achieving high robustness with\nstate-of-the-art methods is extremely time and resource-consuming, rendering\nthem impractical for latency-sensitive applications in many cyber-physical\nsystems.\n  To address both robustness and latency issues, this paper proposes a new\ndefense strategy for adversarial patch attacks called SuperPure. The key\nnovelty is developing a pixel-wise masking scheme that is robust against both\ndistributed and localized patches. The masking involves leveraging a GAN-based\nsuper-resolution scheme to gradually purify the image from adversarial patches.\nOur extensive evaluations using ImageNet and two standard classifiers, ResNet\nand EfficientNet, show that SuperPure advances the state-of-the-art in three\nmajor directions: (i) it improves the robustness against conventional localized\npatches by more than 20%, on average, while also improving top-1 clean accuracy\nby almost 10%; (ii) It achieves 58% robustness against distributed patch\nattacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It\ndecreases the defense end-to-end latency by over 98% compared to PatchCleanser.\nOur further analysis shows that SuperPure is robust against white-box attacks\nand different patch sizes. Our code is open-source."
    },
    {
        "date": "2025-05",
        "title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings",
        "author": "Arjhun Swaminathan, and Mete Akg\u00fcn",
        "link": "http://arxiv.org/abs/2505.16313v1",
        "abstract": "Deep neural networks for image classification remain vulnerable to\nadversarial examples -- small, imperceptible perturbations that induce\nmisclassifications. In black-box settings, where only the final prediction is\naccessible, crafting targeted attacks that aim to misclassify into a specific\ntarget class is particularly challenging due to narrow decision regions.\nCurrent state-of-the-art methods often exploit the geometric properties of the\ndecision boundary separating a source image and a target image rather than\nincorporating information from the images themselves. In contrast, we propose\nTargeted Edge-informed Attack (TEA), a novel attack that utilizes edge\ninformation from the target image to carefully perturb it, thereby producing an\nadversarial image that is closer to the source image while still achieving the\ndesired target classification. Our approach consistently outperforms current\nstate-of-the-art methods across different models in low query settings (nearly\n70\\% fewer queries are used), a scenario especially relevant in real-world\napplications with limited queries and black-box access. Furthermore, by\nefficiently generating a suitable adversarial example, TEA provides an improved\ntarget initialization for established geometry-based attacks."
    },
    {
        "date": "2025-05",
        "title": "Paired and Unpaired Image to Image Translation using Generative Adversarial Networks",
        "author": "Gaurav Kumar, Soham Satyadharma, and Harpreet Singh",
        "link": "http://arxiv.org/abs/2505.16310v1",
        "abstract": "Image to image translation is an active area of research in the field of\ncomputer vision, enabling the generation of new images with different styles,\ntextures, or resolutions while preserving their characteristic properties.\nRecent architectures leverage Generative Adversarial Networks (GANs) to\ntransform input images from one domain to another. In this work, we focus on\nthe study of both paired and unpaired image translation across multiple image\ndomains. For the paired task, we used a conditional GAN model, and for the\nunpaired task, we trained it using cycle consistency loss. We experimented with\ndifferent types of loss functions, multiple Patch-GAN sizes, and model\narchitectures. New quantitative metrics - precision, recall, and FID score -\nwere used for analysis. In addition, a qualitative study of the results of\ndifferent experiments was conducted."
    },
    {
        "date": "2025-05",
        "title": "Poster: Towards an Automated Security Testing Framework for Industrial UEs",
        "author": "Sotiris Michaelides, Daniel Eguiguren Chavez, and Martin Henze",
        "link": "http://arxiv.org/abs/2505.16300v1",
        "abstract": "With the ongoing adoption of 5G for communication in industrial systems and\ncritical infrastructure, the security of industrial UEs such as 5G-enabled\nindustrial robots becomes an increasingly important topic. Most notably, to\nmeet the stringent security requirements of industrial deployments, industrial\nUEs not only have to fully comply with the 5G specifications but also implement\nand use correctly secure communication protocols such as TLS. To ensure the\nsecurity of industrial UEs, operators of industrial 5G networks rely on\nsecurity testing before deploying new devices to their production networks.\nHowever, currently only isolated tests for individual security aspects of\nindustrial UEs exist, severely hindering comprehensive testing. In this paper,\nwe report on our ongoing efforts to alleviate this situation by creating an\nautomated security testing framework for industrial UEs to comprehensively\nevaluate their security posture before deployment. With this framework, we aim\nto provide stakeholders with a fully automated-method to verify that\nhigher-layer security protocols are correctly implemented, while simultaneously\nensuring that the UE's protocol stack adheres to 3GPP specifications."
    },
    {
        "date": "2025-05",
        "title": "Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces",
        "author": "Preeti Mehta, Aman Sagar, and Suchi Kumari",
        "link": "http://arxiv.org/abs/2505.16253v1",
        "abstract": "This study aims to address the growing challenge of distinguishing\ncomputer-generated imagery (CGI) from authentic digital images across three\ndifferent color spaces; RGB, YCbCr, and HSV. Given the limitations of existing\nclassification methods in handling the complexity and variability of CGI, this\nresearch proposes a Swin Transformer based model for accurate differentiation\nbetween natural and synthetic images. The proposed model leverages the Swin\nTransformer's hierarchical architecture to capture local and global features\nfor distinguishing CGI from natural images. Its performance was assessed\nthrough intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,\nand Columbia. The model was evaluated individually on each dataset (D1, D2, D3)\nand on the combined datasets (D1+D2+D3) to test its robustness and domain\ngeneralization. To address dataset imbalance, data augmentation techniques were\napplied. Additionally, t-SNE visualization was used to demonstrate the feature\nseparability achieved by the Swin Transformer across the selected color spaces.\nThe model's performance was tested across all color schemes, with the RGB color\nscheme yielding the highest accuracy for each dataset. As a result, RGB was\nselected for domain generalization analysis and compared with other CNN-based\nmodels, VGG-19 and ResNet-50. The comparative results demonstrate the proposed\nmodel's effectiveness in detecting CGI, highlighting its robustness and\nreliability in both intra-dataset and inter-dataset evaluations. The findings\nof this study highlight the Swin Transformer model's potential as an advanced\ntool for digital image forensics, particularly in distinguishing CGI from\nnatural images. The model's strong performance indicates its capability for\ndomain generalization, making it a valuable asset in scenarios requiring\nprecise and reliable image classification."
    },
    {
        "date": "2025-05",
        "title": "VIVID: A Novel Approach to Remediation Prioritization in Static Application Security Testing (SAST)",
        "author": "Naeem Budhwani, Mohammad Faghani, and Hayden Richard",
        "link": "http://arxiv.org/abs/2505.16205v1",
        "abstract": "Static Application Security Testing (SAST) enables organizations to detect\nvulnerabilities in code early; however, major SAST platforms do not include\nvisual aids and present little insight on correlations between tainted data\nchains. We propose VIVID - Vulnerability Information Via Data flow - a novel\nmethod to extract and consume SAST insights, which is to graph the\napplication's vulnerability data flows (VDFs) and carry out graph theory\nanalysis on the resulting VDF directed graph. Nine metrics were assessed to\nevaluate their effectiveness in analyzing the VDF graphs of deliberately\ninsecure web applications. These metrics include 3 centrality metrics, 2\nstructural metrics, PageRank, in-degree, out-degree, and cross-clique\nconnectivity. We present simulations that find that out-degree, betweenness\ncentrality, in-eigenvector centrality, and cross-clique connectivity were found\nto be associated with files exhibiting high vulnerability traffic, making them\nrefactoring candidates where input sanitization may have been missed.\nMeanwhile, out-eigenvector centrality, PageRank, and in-degree were found to be\nassociated with nodes enabling vulnerability flow and sinks, but not\nnecessarily where input validation should be placed. This is a novel method to\nautomatically provide development teams an evidence-based prioritized list of\nfiles to embed security controls into, informed by vulnerability propagation\npatterns in the application architecture."
    },
    {
        "date": "2025-05",
        "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation",
        "author": "Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Yiwei Jin, Keyu Li, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2505.16196v1",
        "abstract": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines."
    },
    {
        "date": "2025-05",
        "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion",
        "author": "Yuhao Xue, Zhifei Zhang, Xinyang Jiang, Yifei Shen, Junyao Gao, Wentao Gu, Jiale Zhao, Miaojing Shi, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2505.16166v1",
        "abstract": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks."
    },
    {
        "date": "2025-05",
        "title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World",
        "author": "Ji Guo, Long Zhou, Zhijin Wang, Jiaming He, Qiyang Song, Aiguo Chen, and Wenbo Jiang",
        "link": "http://arxiv.org/abs/2505.16154v1",
        "abstract": "In recent years, deep learning-based Monocular Depth Estimation (MDE) models\nhave been widely applied in fields such as autonomous driving and robotics.\nHowever, their vulnerability to backdoor attacks remains unexplored. To fill\nthe gap in this area, we conduct a comprehensive investigation of backdoor\nattacks against MDE models. Typically, existing backdoor attack methods can not\nbe applied to MDE models. This is because the label used in MDE is in the form\nof a depth map. To address this, we propose BadDepth, the first backdoor attack\ntargeting MDE models. BadDepth overcomes this limitation by selectively\nmanipulating the target object's depth using an image segmentation model and\nrestoring the surrounding areas via depth completion, thereby generating\npoisoned datasets for object-level backdoor attacks. To improve robustness in\nphysical world scenarios, we further introduce digital-to-physical augmentation\nto adapt to the domain gap between the physical world and the digital domain.\nExtensive experiments on multiple models validate the effectiveness of BadDepth\nin both the digital domain and the physical world, without being affected by\nenvironmental factors."
    }
]