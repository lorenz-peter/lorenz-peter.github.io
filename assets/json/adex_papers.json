[
    {
        "date": "2025-08",
        "title": "Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms",
        "author": "Jonathan N\u00f6ther, Adish Singla, and Goran Radanovic",
        "link": "http://arxiv.org/abs/2508.16481v1",
        "abstract": "Ensuring the safe use of agentic systems requires a thorough understanding of\nthe range of malicious behaviors these systems may exhibit when under attack.\nIn this paper, we evaluate the robustness of LLM-based agentic systems against\nattacks that aim to elicit harmful actions from agents. To this end, we propose\na novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,\nfor studying the security of agentic systems with respect to a wide range of\nharmful actions. BAD-ACTS consists of 4 implementations of agentic systems in\ndistinct application environments, as well as a dataset of 188 high-quality\nexamples of harmful actions. This enables a comprehensive study of the\nrobustness of agentic systems across a wide range of categories of harmful\nbehaviors, available tools, and inter-agent communication structures. Using\nthis benchmark, we analyze the robustness of agentic systems against an\nattacker that controls one of the agents in the system and aims to manipulate\nother agents to execute a harmful target action. Our results show that the\nattack has a high success rate, demonstrating that even a single adversarial\nagent within the system can have a significant impact on the security. This\nattack remains effective even when agents use a simple prompting-based defense\nstrategy. However, we additionally propose a more effective defense based on\nmessage monitoring. We believe that this benchmark provides a diverse testbed\nfor the security research of agentic systems. The benchmark can be found at\ngithub.com/JNoether/BAD-ACTS"
    },
    {
        "date": "2025-08",
        "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python",
        "author": "Akshay Mhatre, Noujoud Nader, Patrick Diehl, and Deepti Gupta",
        "link": "http://arxiv.org/abs/2508.16419v1",
        "abstract": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools."
    },
    {
        "date": "2025-08",
        "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models",
        "author": "Guangyu Yang, Jinghong Chen, Jingbiao Mei, Weizhe Lin, and Bill Byrne",
        "link": "http://arxiv.org/abs/2508.16406v1",
        "abstract": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."
    },
    {
        "date": "2025-08",
        "title": "Robust Small Methane Plume Segmentation in Satellite Imagery",
        "author": "Khai Duc Minh Tran, Hoa Van Nguyen, Aimuni Binti Muhammad Rawi, Hareeshrao Athinarayanarao, and Ba-Ngu Vo",
        "link": "http://arxiv.org/abs/2508.16282v1",
        "abstract": "This paper tackles the challenging problem of detecting methane plumes, a\npotent greenhouse gas, using Sentinel-2 imagery. This contributes to the\nmitigation of rapid climate change. We propose a novel deep learning solution\nbased on U-Net with a ResNet34 encoder, integrating dual spectral enhancement\ntechniques (Varon ratio and Sanchez regression) to optimise input features for\nheightened sensitivity. A key achievement is the ability to detect small plumes\ndown to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing\ntraditional methods limited to larger plumes. Experiments show our approach\nachieves a 78.39% F1-score on the validation set, demonstrating superior\nperformance in sensitivity and precision over existing remote sensing\ntechniques for automated methane monitoring, especially for small plumes."
    },
    {
        "date": "2025-08",
        "title": "From Confidence to Collapse in LLM Factual Robustness",
        "author": "Alina Fastowski, Bardh Prenkaj, and Gjergji Kasneci",
        "link": "http://arxiv.org/abs/2508.16267v1",
        "abstract": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."
    },
    {
        "date": "2025-08",
        "title": "An Investigation of Visual Foundation Models Robustness",
        "author": "Sandeep Gupta, and Roberto Passerone",
        "link": "http://arxiv.org/abs/2508.16225v1",
        "abstract": "Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,\npowering systems for diverse tasks such as object detection, image\nclassification, segmentation, pose estimation, and motion tracking. VFMs are\ncapitalizing on seminal innovations in deep learning models, such as LeNet-5,\nAlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver\nsuperior performance across a range of critical computer vision applications.\nThese include security-sensitive domains like biometric verification,\nautonomous vehicle perception, and medical image analysis, where robustness is\nessential to fostering trust between technology and the end-users. This article\ninvestigates network robustness requirements crucial in computer vision systems\nto adapt effectively to dynamic environments influenced by factors such as\nlighting, weather conditions, and sensor characteristics. We examine the\nprevalent empirical defenses and robust training employed to enhance vision\nnetwork robustness against real-world challenges such as distributional shifts,\nnoisy and spatially distorted inputs, and adversarial attacks. Subsequently, we\nprovide a comprehensive analysis of the challenges associated with these\ndefense mechanisms, including network properties and components to guide\nablation studies and benchmarking metrics to evaluate network robustness."
    },
    {
        "date": "2025-08",
        "title": "PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting",
        "author": "Hohyun Na, Seunghoo Hong, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2508.16217v1",
        "abstract": "The success of diffusion models has enabled effortless, high-quality image\nmodifications that precisely align with users' intentions, thereby raising\nconcerns about their potential misuse by malicious actors. Previous studies\nhave attempted to mitigate such misuse through adversarial attacks. However,\nthese approaches heavily rely on image-level inconsistencies, which pose\nfundamental limitations in addressing the influence of textual prompts. In this\npaper, we propose PromptFlare, a novel adversarial protection method designed\nto protect images from malicious modifications facilitated by diffusion-based\ninpainting models. Our approach leverages the cross-attention mechanism to\nexploit the intrinsic properties of prompt embeddings. Specifically, we\nidentify and target shared token of prompts that is invariant and semantically\nuninformative, injecting adversarial noise to suppress the sampling process.\nThe injected noise acts as a cross-attention decoy, diverting the model's focus\naway from meaningful prompt-image alignments and thereby neutralizing the\neffect of prompt. Extensive experiments on the EditBench dataset demonstrate\nthat our method achieves state-of-the-art performance across various metrics\nwhile significantly reducing computational overhead and GPU memory usage. These\nfindings highlight PromptFlare as a robust and efficient protection against\nunauthorized image manipulations. The code is available at\nhttps://github.com/NAHOHYUN-SKKU/PromptFlare."
    },
    {
        "date": "2025-08",
        "title": "Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks",
        "author": "Aristeidis Sidiropoulos, Christos Chrysanthos Nikolaidis, Theodoros Tsiolakis, Nikolaos Pavlidis, Vasilis Perifanis, and Pavlos S. Efraimidis",
        "link": "http://arxiv.org/abs/2508.16150v1",
        "abstract": "Membership Inference Attacks (MIAs) pose a significant privacy risk, as they\nenable adversaries to determine whether a specific data point was included in\nthe training dataset of a model. While Machine Unlearning is primarily designed\nas a privacy mechanism to efficiently remove private data from a machine\nlearning model without the need for full retraining, its impact on the\nsusceptibility of models to MIA remains an open question. In this study, we\nsystematically assess the vulnerability of models to MIA after applying\nstate-of-art Machine Unlearning algorithms. Our analysis spans four diverse\ndatasets (two from the image domain and two in tabular format), exploring how\ndifferent unlearning approaches influence the exposure of models to membership\ninference. The findings highlight that while Machine Unlearning is not\ninherently a countermeasure against MIA, the unlearning algorithm and data\ncharacteristics can significantly affect a model's vulnerability. This work\nprovides essential insights into the interplay between Machine Unlearning and\nMIAs, offering guidance for the design of privacy-preserving machine learning\nsystems."
    },
    {
        "date": "2025-08",
        "title": "Neural-Network Chemical Emulator for First-Star Formation: Robust Iterative Predictions over a Wide Density Range",
        "author": "Sojun Ono, and Kazuyuki Sugimura",
        "link": "http://arxiv.org/abs/2508.16114v1",
        "abstract": "We present a neural-network emulator for the thermal and chemical evolution\nin Population~III star formation. The emulator accurately reproduces the\nthermochemical evolution over a wide density range spanning 21 orders of\nmagnitude (10$^{-3}$-10$^{18}$ cm$^{-3}$), tracking six primordial species: H,\nH$_2$, e$^{-}$, H$^{+}$, H$^{-}$, and H$_2^{+}$. To handle the broad dynamic\nrange, we partition the density range into five subregions and train separate\ndeep operator networks (DeepONets) in each region. When applied to randomly\nsampled thermochemical states, the emulator achieves relative errors below 10%\nin over 90% of cases for both temperature and chemical abundances (except for\nthe rare species H$_2^{+}$). The emulator is roughly ten times faster on a CPU\nand more than 1000 times faster for batched predictions on a GPU, compared with\nconventional numerical integration. Furthermore, to ensure robust predictions\nunder many iterations, we introduce a novel timescale-based update method,\nwhere a short-timestep update of each variable is computed by rescaling the\npredicted change over a longer timestep equal to its characteristic variation\ntimescale. In one-zone collapse calculations, the results from the\ntimescale-based method agree well with traditional numerical integration even\nwith many iterations at a timestep as short as 10$^{-4}$ of the free-fall time.\nThis proof-of-concept study suggests the potential for neural network-based\nchemical emulators to accelerate hydrodynamic simulations of star formation."
    },
    {
        "date": "2025-08",
        "title": "Two-flow Feedback Multi-scale Progressive Generative Adversarial Network",
        "author": "Sun Weikai, Song Shijie, and Chi Wenjie",
        "link": "http://arxiv.org/abs/2508.16089v1",
        "abstract": "Although diffusion model has made good progress in the field of image\ngeneration, GAN\\cite{huang2023adaptive} still has a large development space due\nto its unique advantages, such as WGAN\\cite{liu2021comparing},\nSSGAN\\cite{guibas2021adaptive} \\cite{zhang2022vsa} \\cite{zhou2024adapt} and so\non. In this paper, we propose a novel two-flow feedback multi-scale progressive\ngenerative adversarial network (MSPG-SEN) for GAN models. This paper has four\ncontributions: 1) : We propose a two-flow feedback multi-scale progressive\nGenerative Adversarial network (MSPG-SEN), which not only improves image\nquality and human visual perception on the basis of retaining the advantages of\nthe existing GAN model, but also simplifies the training process and reduces\nthe training cost of GAN networks. Our experimental results show that, MSPG-SEN\nhas achieved state-of-the-art generation results on the following five\ndatasets,INKK The dataset is 89.7\\%,AWUN The dataset is 78.3\\%,IONJ The dataset\nis 85.5\\%,POKL The dataset is 88.7\\%,OPIN The dataset is 96.4\\%. 2) : We\npropose an adaptive perception-behavioral feedback loop (APFL), which\neffectively improves the robustness and training stability of the model and\nreduces the training cost. 3) : We propose a globally connected two-flow\ndynamic residual network(). After ablation experiments, it can effectively\nimprove the training efficiency and greatly improve the generalization ability,\nwith stronger flexibility. 4) : We propose a new dynamic embedded attention\nmechanism (DEMA). After experiments, the attention can be extended to a variety\nof image processing tasks, which can effectively capture global-local\ninformation, improve feature separation capability and feature expression\ncapabilities, and requires minimal computing resources only 88.7\\% with INJK\nWith strong cross-task capability."
    },
    {
        "date": "2025-08",
        "title": "GelSLAM: A Real-time, High-Fidelity, and Robust 3D Tactile SLAM System",
        "author": "Hung-Jui Huang, Mohammad Amin Mirzaee, Michael Kaess, and Wenzhen Yuan",
        "link": "http://arxiv.org/abs/2508.15990v1",
        "abstract": "Accurately perceiving an object's pose and shape is essential for precise\ngrasping and manipulation. Compared to common vision-based methods, tactile\nsensing offers advantages in precision and immunity to occlusion when tracking\nand reconstructing objects in contact. This makes it particularly valuable for\nin-hand and other high-precision manipulation tasks. In this work, we present\nGelSLAM, a real-time 3D SLAM system that relies solely on tactile sensing to\nestimate object pose over long periods and reconstruct object shapes with high\nfidelity. Unlike traditional point cloud-based approaches, GelSLAM uses\ntactile-derived surface normals and curvatures for robust tracking and loop\nclosure. It can track object motion in real time with low error and minimal\ndrift, and reconstruct shapes with submillimeter accuracy, even for low-texture\nobjects such as wooden tools. GelSLAM extends tactile sensing beyond local\ncontact to enable global, long-horizon spatial perception, and we believe it\nwill serve as a foundation for many precise manipulation tasks involving\ninteraction with objects in hand. The video demo is available on our website:\nhttps://joehjhuang.github.io/gelslam."
    },
    {
        "date": "2025-08",
        "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models",
        "author": "Andreas D. Kellas, Neophytos Christou, Wenxin Jiang, Penghui Li, Laurent Simon, Yaniv David, Vasileios P. Kemerlis, James C. Davis, and Junfeng Yang",
        "link": "http://arxiv.org/abs/2508.15987v1",
        "abstract": "Machine learning model repositories such as the Hugging Face Model Hub\nfacilitate model exchanges. However, bad actors can deliver malware through\ncompromised models. Existing defenses such as safer model formats, restrictive\n(but inflexible) loading policies, and model scanners have shortcomings: 44.9%\nof popular models on Hugging Face still use the insecure pickle format, 15% of\nthese cannot be loaded by restrictive loading policies, and model scanners have\nboth false positives and false negatives. Pickle remains the de facto standard\nfor model exchange, and the ML community lacks a tool that offers transparent\nsafe loading.\n  We present PickleBall to help machine learning engineers load pickle-based\nmodels safely. PickleBall statically analyzes the source code of a given\nmachine learning library and computes a custom policy that specifies a safe\nload-time behavior for benign models. PickleBall then dynamically enforces the\npolicy during load time as a drop-in replacement for the pickle module.\nPickleBall generates policies that correctly load 79.8% of benign pickle-based\nmodels in our dataset, while rejecting all (100%) malicious examples in our\ndataset. In comparison, evaluated model scanners fail to identify known\nmalicious models, and the state-of-art loader loads 22% fewer benign models\nthan PickleBall. PickleBall removes the threat of arbitrary function invocation\nfrom malicious pickle-based models, raising the bar for attackers to depend on\ncode reuse techniques."
    },
    {
        "date": "2025-08",
        "title": "Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification",
        "author": "Onur Alp Kirci, and M. Emre Gursoy",
        "link": "http://arxiv.org/abs/2508.15934v1",
        "abstract": "Backdoor attacks pose a significant threat to the integrity of text\nclassification models used in natural language processing. While several\ndirty-label attacks that achieve high attack success rates (ASR) have been\nproposed, clean-label attacks are inherently more difficult. In this paper, we\npropose three sample selection strategies to improve attack effectiveness in\nclean-label scenarios: Minimum, Above50, and Below50. Our strategies identify\nthose samples which the model predicts incorrectly or with low confidence, and\nby injecting backdoor triggers into such samples, we aim to induce a stronger\nassociation between the trigger patterns and the attacker-desired target label.\nWe apply our methods to clean-label variants of four canonical backdoor attacks\n(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets\n(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,\nRoBERTa). Results show that the proposed strategies, particularly the Minimum\nstrategy, significantly improve the ASR over random sample selection with\nlittle or no degradation in the model's clean accuracy. Furthermore,\nclean-label attacks enhanced by our strategies outperform BITE, a state of the\nart clean-label attack method, in many configurations."
    },
    {
        "date": "2025-08",
        "title": "Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space",
        "author": "Kiarash Kazari, Ezzeldin Shereen, and Gy\u00f6rgy D\u00e1n",
        "link": "http://arxiv.org/abs/2508.15764v1",
        "abstract": "We address the problem of detecting adversarial attacks against cooperative\nmulti-agent reinforcement learning with continuous action space. We propose a\ndecentralized detector that relies solely on the local observations of the\nagents and makes use of a statistical characterization of the normal behavior\nof observable agents. The proposed detector utilizes deep neural networks to\napproximate the normal behavior of agents as parametric multivariate Gaussian\ndistributions. Based on the predicted density functions, we define a normality\nscore and provide a characterization of its mean and variance. This\ncharacterization allows us to employ a two-sided CUSUM procedure for detecting\ndeviations of the normality score from its mean, serving as a detector of\nanomalous behavior in real-time. We evaluate our scheme on various multi-agent\nPettingZoo benchmarks against different state-of-the-art attack methods, and\nour results demonstrate the effectiveness of our method in detecting impactful\nadversarial attacks. Particularly, it outperforms the discrete counterpart by\nachieving AUC-ROC scores of over 0.95 against the most impactful attacks in all\nevaluated environments."
    },
    {
        "date": "2025-08",
        "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models",
        "author": "Xinyi Ling, Hanwen Du, Zhihui Zhu, and Xia Ning",
        "link": "http://arxiv.org/abs/2508.15721v1",
        "abstract": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25."
    },
    {
        "date": "2025-08",
        "title": "Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance",
        "author": "Shuchao Pang, Zhenghan Chen, Shen Zhang, Liming Lu, Siyuan Liang, Anan Du, and Yongbin Zhou",
        "link": "http://arxiv.org/abs/2508.15650v1",
        "abstract": "Deep neural networks for 3D point clouds have been demonstrated to be\nvulnerable to adversarial examples. Previous 3D adversarial attack methods\noften exploit certain information about the target models, such as model\nparameters or outputs, to generate adversarial point clouds. However, in\nrealistic scenarios, it is challenging to obtain any information about the\ntarget models under conditions of absolute security. Therefore, we focus on\ntransfer-based attacks, where generating adversarial point clouds does not\nrequire any information about the target models. Based on our observation that\nthe critical features used for point cloud classification are consistent across\ndifferent DNN architectures, we propose CFG, a novel transfer-based black-box\nattack method that improves the transferability of adversarial point clouds via\nthe proposed Critical Feature Guidance. Specifically, our method regularizes\nthe search of adversarial point clouds by computing the importance of the\nextracted features, prioritizing the corruption of critical features that are\nlikely to be adopted by diverse architectures. Further, we explicitly constrain\nthe maximum deviation extent of the generated adversarial point clouds in the\nloss function to ensure their imperceptibility. Extensive experiments conducted\non the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the\nproposed CFG outperforms the state-of-the-art attack methods by a large margin."
    },
    {
        "date": "2025-08",
        "title": "A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification",
        "author": "Ahmed Nasir, and Abdelhafid Zenati",
        "link": "http://arxiv.org/abs/2508.15588v1",
        "abstract": "The application of reinforcement learning to safety-critical systems is\nlimited by the lack of formal methods for verifying the robustness and safety\nof learned policies. This paper introduces a novel framework that addresses\nthis gap by analyzing the combination of an RL agent and its environment as a\ndiscrete-time autonomous dynamical system. By leveraging tools from dynamical\nsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we\nidentify and visualize Lagrangian Coherent Structures (LCS) that act as the\nhidden \"skeleton\" governing the system's behavior. We demonstrate that\nrepelling LCS function as safety barriers around unsafe regions, while\nattracting LCS reveal the system's convergence properties and potential failure\nmodes, such as unintended \"trap\" states. To move beyond qualitative\nvisualization, we introduce a suite of quantitative metrics, Mean Boundary\nRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and\nTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure a\npolicy's safety margin and robustness. We further provide a method for deriving\nlocal stability guarantees and extend the analysis to handle model uncertainty.\nThrough experiments in both discrete and continuous control environments, we\nshow that this framework provides a comprehensive and interpretable assessment\nof policy behavior, successfully identifying critical flaws in policies that\nappear successful based on reward alone."
    },
    {
        "date": "2025-08",
        "title": "BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning",
        "author": "Bingguang Lu, Hongsheng Hu, Yuantian Miao, Shaleeza Sohail, Chaoxiang He, Shuo Wang, and Xiao Chen",
        "link": "http://arxiv.org/abs/2508.15541v1",
        "abstract": "Federated learning (FL) has been widely adopted as a decentralized training\nparadigm that enables multiple clients to collaboratively learn a shared model\nwithout exposing their local data. As concerns over data privacy and regulatory\ncompliance grow, machine unlearning, which aims to remove the influence of\nspecific data from trained models, has become increasingly important in the\nfederated setting to meet legal, ethical, or user-driven demands. However,\nintegrating unlearning into FL introduces new challenges and raises largely\nunexplored security risks. In particular, adversaries may exploit the\nunlearning process to compromise the integrity of the global model. In this\npaper, we present the first backdoor attack in the context of federated\nunlearning, demonstrating that an adversary can inject backdoors into the\nglobal model through seemingly legitimate unlearning requests. Specifically, we\npropose BadFU, an attack strategy where a malicious client uses both backdoor\nand camouflage samples to train the global model normally during the federated\ntraining process. Once the client requests unlearning of the camouflage\nsamples, the global model transitions into a backdoored state. Extensive\nexperiments under various FL frameworks and unlearning strategies validate the\neffectiveness of BadFU, revealing a critical vulnerability in current federated\nunlearning practices and underscoring the urgent need for more secure and\nrobust federated unlearning mechanisms."
    },
    {
        "date": "2025-08",
        "title": "Mini-Batch Robustness Verification of Deep Neural Networks",
        "author": "Saar Tzour-Shaday, and Dana Drachsler Cohen",
        "link": "http://arxiv.org/abs/2508.15454v1",
        "abstract": "Neural network image classifiers are ubiquitous in many safety-critical\napplications. However, they are susceptible to adversarial attacks. To\nunderstand their robustness to attacks, many local robustness verifiers have\nbeen proposed to analyze $\\epsilon$-balls of inputs. Yet, existing verifiers\nintroduce a long analysis time or lose too much precision, making them less\neffective for a large set of inputs. In this work, we propose a new approach to\nlocal robustness: group local robustness verification. The key idea is to\nleverage the similarity of the network computations of certain $\\epsilon$-balls\nto reduce the overall analysis time. We propose BaVerLy, a sound and complete\nverifier that boosts the local robustness verification of a set of\n$\\epsilon$-balls by dynamically constructing and verifying mini-batches.\nBaVerLy adaptively identifies successful mini-batch sizes, accordingly\nconstructs mini-batches of $\\epsilon$-balls that have similar network\ncomputations, and verifies them jointly. If a mini-batch is verified, all\n$\\epsilon$-balls are proven robust. Otherwise, one $\\epsilon$-ball is suspected\nas not being robust, guiding the refinement. In the latter case, BaVerLy\nleverages the analysis results to expedite the analysis of that $\\epsilon$-ball\nas well as the other $\\epsilon$-balls in the batch. We evaluate BaVerLy on\nfully connected and convolutional networks for MNIST and CIFAR-10. Results show\nthat BaVerLy scales the common one by one verification by 2.3x on average and\nup to 4.1x, in which case it reduces the total analysis time from 24 hours to 6\nhours."
    },
    {
        "date": "2025-08",
        "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems",
        "author": "Frederik Vandeputte",
        "link": "http://arxiv.org/abs/2508.15411v1",
        "abstract": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework."
    },
    {
        "date": "2025-08",
        "title": "IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents",
        "author": "Hengyu An, Jinghuai Zhang, Tianyu Du, Chunyi Zhou, Qingming Li, Tao Lin, and Shouling Ji",
        "link": "http://arxiv.org/abs/2508.15310v1",
        "abstract": "Large language model (LLM) agents are widely deployed in real-world\napplications, where they leverage tools to retrieve and manipulate external\ndata for complex tasks. However, when interacting with untrusted data sources\n(e.g., fetching information from public websites), tool responses may contain\ninjected instructions that covertly influence agent behaviors and lead to\nmalicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).\nExisting defenses typically rely on advanced prompting strategies or auxiliary\ndetection models. While these methods have demonstrated some effectiveness,\nthey fundamentally rely on assumptions about the model's inherent security,\nwhich lacks structural constraints on agent behaviors. As a result, agents\nstill retain unrestricted access to tool invocations, leaving them vulnerable\nto stronger attack vectors that can bypass the security guardrails of the\nmodel. To prevent malicious tool invocations at the source, we propose a novel\ndefensive task execution paradigm, called IPIGuard, which models the agents'\ntask execution process as a traversal over a planned Tool Dependency Graph\n(TDG). By explicitly decoupling action planning from interaction with external\ndata, IPIGuard significantly reduces unintended tool invocations triggered by\ninjected instructions, thereby enhancing robustness against IPI attacks.\nExperiments on the AgentDojo benchmark show that IPIGuard achieves a superior\nbalance between effectiveness and robustness, paving the way for the\ndevelopment of safer agentic systems in dynamic environments."
    },
    {
        "date": "2025-08",
        "title": "Robust and Efficient Quantum Reservoir Computing with Discrete Time Crystal",
        "author": "Da Zhang, Xin Li, Yibin Guo, Haifeng Yu, Yirong Jin, and Zhang-Qi Yin",
        "link": "http://arxiv.org/abs/2508.15230v1",
        "abstract": "The rapid development of machine learning and quantum computing has placed\nquantum machine learning at the forefront of research. However, existing\nquantum machine learning algorithms based on quantum variational algorithms\nface challenges in trainability and noise robustness. In order to address these\nchallenges, we introduce a gradient-free, noise-robust quantum reservoir\ncomputing algorithm that harnesses discrete time crystal dynamics as a\nreservoir. We first calibrate the memory, nonlinear, and information scrambling\ncapacities of the quantum reservoir, revealing their correlation with dynamical\nphases and non-equilibrium phase transitions. We then apply the algorithm to\nthe binary classification task and establish a comparative quantum kernel\nadvantage. For ten-class classification, both noisy simulations and\nexperimental results on superconducting quantum processors match ideal\nsimulations, demonstrating the enhanced accuracy with increasing system size\nand confirming the topological noise robustness. Our work presents the first\nexperimental demonstration of quantum reservoir computing for image\nclassification based on digital quantum simulation. It establishes the\ncorrelation between quantum many-body non-equilibrium phase transitions and\nquantum machine learning performance, providing new design principles for\nquantum reservoir computing and broader quantum machine learning algorithms in\nthe NISQ era."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning",
        "author": "Arjun Srinivasan, Anubhav Paras, and Aniket Bera",
        "link": "http://arxiv.org/abs/2508.15207v1",
        "abstract": "Existing approaches in reinforcement learning train an agent to learn desired\noptimal behavior in an environment with rule based surrounding agents. In\nsafety critical applications such as autonomous driving it is crucial that the\nrule based agents are modelled properly. Several behavior modelling strategies\nand IDM models are used currently to model the surrounding agents. We present a\nlearning based method to derive the adversarial behavior for the rule based\nagents to cause failure scenarios. We evaluate our adversarial agent against\nall the rule based agents and show the decrease in cumulative reward."
    },
    {
        "date": "2025-08",
        "title": "SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks",
        "author": "Xiangman Li, Xiaodong Wu, Qi Li, Jianbing Ni, and Rongxing Lu",
        "link": "http://arxiv.org/abs/2508.15182v1",
        "abstract": "Jailbreak attacks pose a serious threat to the safety of Large Language\nModels (LLMs) by crafting adversarial prompts that bypass alignment mechanisms,\ncausing the models to produce harmful, restricted, or biased content. In this\npaper, we propose SafeLLM, a novel unlearning-based defense framework that\nunlearn the harmful knowledge from LLMs while preserving linguistic fluency and\ngeneral capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic\nunsafe output detection using a hybrid approach that integrates external\nclassifiers with model-internal evaluations; (2) token-level harmful content\ntracing through feedforward network (FFN) activations to localize harmful\nknowledge; and (3) constrained optimization to suppress unsafe behavior without\ndegrading overall model quality. SafeLLM achieves targeted and irreversible\nforgetting by identifying and neutralizing FFN substructures responsible for\nharmful generation pathways. Extensive experiments on prominent LLMs (Vicuna,\nLLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM\nsubstantially reduces attack success rates while maintaining high\ngeneral-purpose performance. Compared to standard defense methods such as\nsupervised fine-tuning and direct preference optimization, SafeLLM offers\nstronger safety guarantees, more precise control over harmful behavior, and\ngreater robustness to unseen attacks. Moreover, SafeLLM maintains the general\nperformance after the harmful knowledge unlearned. These results highlight\nunlearning as a promising direction for scalable and effective LLM safety."
    },
    {
        "date": "2025-08",
        "title": "Conditional Cube Attack on Round-Reduced ASCON",
        "author": "Zheng Li, Xiaoyang Dong, and Xiaoyun Wang",
        "link": "http://arxiv.org/abs/2508.15172v1",
        "abstract": "This paper evaluates the secure level of authenticated encryption\n\\textsc{Ascon} against cube-like method. \\textsc{Ascon} submitted by Dobraunig\n\\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The\ncube-like method is first used by Dinur \\emph{et~al.} to analyze Keccak keyed\nmodes. At CT-RSA 2015, Dobraunig \\emph{et~al.} applied this method to 5/6-round\nreduced \\textsc{Ascon}, whose structure is similar to Keccak keyed modes.\nHowever, for \\textsc{Ascon} the non-linear layer is more complex and state is\nmuch smaller, which make it hard for the attackers to select enough cube\nvariables that do not multiply with each other after the first round. This\nseems to be the reason why the best previous key-recovery attack is on 6-round\n\\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the\nattacked round is no less than 7-round.\n  In this paper, we generalize the conditional cube attack proposed by Huang\n\\emph{et~al.}, and find new cubes depending on some key bit conditions for\n5/6-round reduced \\textsc{Ascon}, and translate the previous theoretic 6-round\nattack with $2^{66}$ time complexity to a practical one with $2^{40}$ time\ncomplexity. Moreover, we propose the first 7-round key-recovery attack on\n\\textsc{Ascon}. By introducing \\emph{the cube-like key-subset technique}, we\ndivide the full key space into many subsets according to different key\nconditions. For each key subset, we launch the cube tester to determine if the\nkey falls into it. Finally, we recover the full key space by testing all the\nkey subsets. The total time complexity is about $2^{103.9}$. In addition, for a\nweak-key subset, whose size is $2^{117}$, the attack is more efficient and\ncosts only $2^{77}$ time complexity. Those attacks do not threaten the full\nround (12 rounds) \\textsc{Ascon}."
    },
    {
        "date": "2025-08",
        "title": "A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports",
        "author": "Minh Tran, Jeffery C. Chan, Min Li Huang, Maya Kansara, John P. Grady, Christine E. Napier, Subotheni Thavaneswaran, Mandy L. Ballinger, David M. Thomas, and Frank P. Lin",
        "link": "http://arxiv.org/abs/2508.15149v1",
        "abstract": "The accurate extraction of clinical information from electronic medical\nrecords is particularly critical to clinical research but require much trained\nexpertise and manual labor. In this study we developed a robust system for\nautomated extraction of the specific cancer types for the purpose of supporting\nprecision oncology research. from pathology reports using a fine-tuned RoBERTa\nmodel. This model significantly outperformed the baseline model and a Large\nLanguage Model, Mistral 7B, achieving F1_Bertscore 0.98 and overall exact match\nof 80.61%. This fine-tuning approach demonstrates the potential for scalability\nthat can integrate seamlessly into the molecular tumour board process.\nFine-tuning domain-specific models for precision tasks in oncology, may pave\nthe way for more efficient and accurate clinical information extraction."
    },
    {
        "date": "2025-08",
        "title": "LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text",
        "author": "MohamamdJavad Ardestani, Ehsan Kamalloo, and Davood Rafiei",
        "link": "http://arxiv.org/abs/2508.15085v1",
        "abstract": "LongRecall. The completeness of machine-generated text, ensuring that it\ncaptures all relevant information, is crucial in domains such as medicine and\nlaw and in tasks like list-based question answering (QA), where omissions can\nhave serious consequences. However, existing recall metrics often depend on\nlexical overlap, leading to errors with unsubstantiated entities and\nparaphrased answers, while LLM-as-a-Judge methods with long holistic prompts\ncapture broader semantics but remain prone to misalignment and hallucinations\nwithout structured verification. We introduce LongRecall, a general three-stage\nrecall evaluation framework that decomposes answers into self-contained facts,\nsuccessively narrows plausible candidate matches through lexical and semantic\nfiltering, and verifies their alignment through structured entailment checks.\nThis design reduces false positives and false negatives while accommodating\ndiverse phrasings and contextual variations, serving as a foundational building\nblock for systematic recall assessment. We evaluate LongRecall on three\nchallenging long-form QA benchmarks using both human annotations and LLM-based\njudges, demonstrating substantial improvements in recall accuracy over strong\nlexical and LLM-as-a-Judge baselines."
    },
    {
        "date": "2025-08",
        "title": "Robust Estimation Under Heterogeneous Corruption Rates",
        "author": "Syomantak Chaudhuri, Jerry Li, and Thomas A. Courtade",
        "link": "http://arxiv.org/abs/2508.15051v1",
        "abstract": "We study the problem of robust estimation under heterogeneous corruption\nrates, where each sample may be independently corrupted with a known but\nnon-identical probability. This setting arises naturally in distributed and\nfederated learning, crowdsourcing, and sensor networks, yet existing robust\nestimators typically assume uniform or worst-case corruption, ignoring\nstructural heterogeneity. For mean estimation for multivariate bounded\ndistributions and univariate gaussian distributions, we give tight minimax\nrates for all heterogeneous corruption patterns. For multivariate gaussian mean\nestimation and linear regression, we establish the minimax rate for squared\nerror up to a factor of $\\sqrt{d}$, where $d$ is the dimension. Roughly, our\nfindings suggest that samples beyond a certain corruption threshold may be\ndiscarded by the optimal estimators -- this threshold is determined by the\nempirical distribution of the corruption rates given."
    },
    {
        "date": "2025-08",
        "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs",
        "author": "Ruyi Ding, Tianhong Xu, Xinyi Shen, Aidong Adam Ding, and Yunsi Fei",
        "link": "http://arxiv.org/abs/2508.15036v1",
        "abstract": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
    },
    {
        "date": "2025-08",
        "title": "Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection",
        "author": "Julia Boone, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2508.15865v1",
        "abstract": "Cyber-physical systems (CPS) are being increasingly utilized for critical\napplications. CPS combines sensing and computing elements, often having\nmulti-layer designs with networking, computational, and physical interfaces,\nwhich provide them with enhanced capabilities for a variety of application\nscenarios. However, the combination of physical and computational elements also\nmakes CPS more vulnerable to attacks compared to network-only systems, and the\nresulting impacts of CPS attacks can be substantial. Intelligent intrusion\ndetection systems (IDS) are an effective mechanism by which CPS can be secured,\nbut the majority of current solutions often train and validate on network\ntraffic-only datasets, ignoring the distinct attacks that may occur on other\nsystem layers. In order to address this, we develop an adaptable CPS anomaly\ndetection model that can detect attacks within CPS without the need for\npreviously labeled data. To achieve this, we utilize domain adaptation\ntechniques that allow us to transfer known attack knowledge from a network\ntraffic-only environment to a CPS environment. We validate our approach using a\nstate-of-the-art CPS intrusion dataset that combines network, operating system\n(OS), and Robot Operating System (ROS) data. Through this dataset, we are able\nto demonstrate the effectiveness of our model across network traffic-only and\nCPS environments with distinct attack types and its ability to outperform other\nanomaly detection methods."
    },
    {
        "date": "2025-08",
        "title": "A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives",
        "author": "Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, and Yushun Dong",
        "link": "http://arxiv.org/abs/2508.15031v1",
        "abstract": "Machine learning (ML) models have significantly grown in complexity and\nutility, driving advances across multiple domains. However, substantial\ncomputational resources and specialized expertise have historically restricted\ntheir wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have\naddressed these barriers by providing scalable, convenient, and affordable\naccess to sophisticated ML models through user-friendly APIs. While this\naccessibility promotes widespread use of advanced ML capabilities, it also\nintroduces vulnerabilities exploited through Model Extraction Attacks (MEAs).\nRecent studies have demonstrated that adversaries can systematically replicate\na target model's functionality by interacting with publicly exposed interfaces,\nposing threats to intellectual property, privacy, and system security. In this\npaper, we offer a comprehensive survey of MEAs and corresponding defense\nstrategies. We propose a novel taxonomy that classifies MEAs according to\nattack mechanisms, defense approaches, and computing environments. Our analysis\ncovers various attack techniques, evaluates their effectiveness, and highlights\nchallenges faced by existing defenses, particularly the critical trade-off\nbetween preserving model utility and ensuring security. We further assess MEAs\nwithin different computing paradigms and discuss their technical, ethical,\nlegal, and societal implications, along with promising directions for future\nresearch. This systematic survey aims to serve as a valuable reference for\nresearchers, practitioners, and policymakers engaged in AI security and\nprivacy. Additionally, we maintain an online repository continuously updated\nwith related literature at https://github.com/kzhao5/ModelExtractionPapers."
    },
    {
        "date": "2025-08",
        "title": "TAIGen: Training-Free Adversarial Image Generation via Diffusion Models",
        "author": "Susim Roy, Anubhooti Jain, Mayank Vatsa, and Richa Singh",
        "link": "http://arxiv.org/abs/2508.15020v1",
        "abstract": "Adversarial attacks from generative models often produce low-quality images\nand require substantial computational resources. Diffusion models, though\ncapable of high-quality generation, typically need hundreds of sampling steps\nfor adversarial generation. This paper introduces TAIGen, a training-free\nblack-box method for efficient adversarial image generation. TAIGen produces\nadversarial examples using only 3-20 sampling steps from unconditional\ndiffusion models. Our key finding is that perturbations injected during the\nmixing step interval achieve comparable attack effectiveness without processing\nall timesteps. We develop a selective RGB channel strategy that applies\nattention maps to the red channel while using GradCAM-guided perturbations on\ngreen and blue channels. This design preserves image structure while maximizing\nmisclassification in target models. TAIGen maintains visual quality with PSNR\nabove 30 dB across all tested datasets. On ImageNet with VGGNet as source,\nTAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%\nagainst ShuffleNet. The method generates adversarial examples 10x faster than\nexisting diffusion-based attacks. Our method achieves the lowest robust\naccuracy, indicating it is the most impactful attack as the defense mechanism\nis least successful in purifying the images generated by TAIGen."
    },
    {
        "date": "2025-08",
        "title": "Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection",
        "author": "Andrei Balykin, Anvar Ganiev, Denis Kondranin, Kirill Polevoda, Nikolai Liudkevich, and Artem Petrov",
        "link": "http://arxiv.org/abs/2508.14980v1",
        "abstract": "Modern face recognition systems remain vulnerable to spoofing attempts,\nincluding both physical presentation attacks and digital forgeries.\nTraditionally, these two attack vectors have been handled by separate models,\neach targeting its own artifacts and modalities. However, maintaining distinct\ndetectors increases system complexity and inference latency and leaves systems\nexposed to combined attack vectors. We propose the Paired-Sampling Contrastive\nFramework, a unified training approach that leverages automatically matched\npairs of genuine and attack selfies to learn modality-agnostic liveness cues.\nEvaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital\nAttack Detection benchmark, our method achieves an average classification error\nrate (ACER) of 2.10 percent, outperforming prior solutions. The framework is\nlightweight (4.46 GFLOPs) and trains in under one hour, making it practical for\nreal-world deployment. Code and pretrained models are available at\nhttps://github.com/xPONYx/iccv2025_deepfake_challenge."
    },
    {
        "date": "2025-08",
        "title": "Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent",
        "author": "Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, and Xiuwen Liu",
        "link": "http://arxiv.org/abs/2508.14853v1",
        "abstract": "As large language models (LLMs) are increasingly deployed in critical\napplications, ensuring their robustness and safety alignment remains a major\nchallenge. Despite the overall success of alignment techniques such as\nreinforcement learning from human feedback (RLHF) on typical prompts, LLMs\nremain vulnerable to jailbreak attacks enabled by crafted adversarial triggers\nappended to user prompts. Most existing jailbreak methods either rely on\ninefficient searches over discrete token spaces or direct optimization of\ncontinuous embeddings. While continuous embeddings can be given directly to\nselected open-source models as input, doing so is not feasible for proprietary\nmodels. On the other hand, projecting these embeddings back into valid discrete\ntokens introduces additional complexity and often reduces attack effectiveness.\nWe propose an intrinsic optimization method which directly optimizes relaxed\none-hot encodings of the adversarial suffix tokens using exponentiated gradient\ndescent coupled with Bregman projection, ensuring that the optimized one-hot\nencoding of each token always remains within the probability simplex. We\nprovide theoretical proof of convergence for our proposed method and implement\nan efficient algorithm that effectively jailbreaks several widely used LLMs.\nOur method achieves higher success rates and faster convergence compared to\nthree state-of-the-art baselines, evaluated on five open-source LLMs and four\nadversarial behavior datasets curated for evaluating jailbreak methods. In\naddition to individual prompt attacks, we also generate universal adversarial\nsuffixes effective across multiple prompts and demonstrate transferability of\noptimized suffixes to different LLMs."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification",
        "author": "Mengliang Zhang, and Jacob M. Luber",
        "link": "http://arxiv.org/abs/2508.14779v1",
        "abstract": "Pathology foundation models (PFMs) have demonstrated remarkable potential in\nwhole-slide image (WSI) diagnosis. However, pathology images from different\nhospitals often vary due to differences in scanning hardware and preprocessing\nstyles, which may lead PFMs to inadvertently learn hospital-specific features,\nposing risks for clinical deployment. In this work, we present the first\nsystematic study of domain bias in PFMs arising from hospital source\ncharacteristics. Specifically, we (1) construct a pipeline for quantifying\ndomain bias in PFMs, (2) evaluate and compare the performance of multiple\nmodels, and (3) propose a lightweight adversarial framework that removes latent\nhospital-specific features from frozen representations without modifying the\nencoder itself. By introducing a trainable adapter and a domain classifier\nconnected through a gradient reversal layer (GRL), our method learns\ntask-discriminative yet domain-invariant representations. Experiments on\nmulti-center histopathology datasets demonstrate that our approach\nsubstantially reduces domain predictability while maintaining or even improving\ndisease classification performance, particularly in out-of-domain (unseen\nhospital) scenarios. Further analyses, including hospital detection and feature\nspace visualization, confirm the effectiveness of our method in mitigating\nhospital bias. We will provide our code based on acceptance."
    },
    {
        "date": "2025-08",
        "title": "Robust Residual Finite Scalar Quantization for Neural Compression",
        "author": "Xiaoxu Zhu",
        "link": "http://arxiv.org/abs/2508.15860v1",
        "abstract": "Finite Scalar Quantization (FSQ) has emerged as a promising alternative to\nVector Quantization (VQ) in neural compression, offering simplified training\nand improved stability. However, naive application of FSQ in residual\nquantization frameworks suffers from the \\textbf{residual magnitude decay\nproblem}, where subsequent FSQ layers receive progressively weaker signals,\nseverely limiting their effectiveness. We propose \\textbf{Robust Residual\nFinite Scalar Quantization (RFSQ)}, a general framework that addresses this\nfundamental limitation through two novel conditioning strategies: learnable\nscaling factors and invertible layer normalization. Our approach maintains the\nsimplicity of FSQ while enabling effective multi-stage residual quantization.\nComprehensive experiments on ImageNet demonstrate that RFSQ variants\nsignificantly outperform strong baselines including VQ-EMA, FSQ, and LFQ,\nachieving up to 45\\% improvement in perceptual loss and 28.7\\% reduction in L1\nreconstruction error. The proposed LayerNorm strategy shows the most consistent\nimprovements across different configurations, establishing RFSQ as a superior\nquantization method for neural compression."
    },
    {
        "date": "2025-08",
        "title": "Distributional Adversarial Attacks and Training in Deep Hedging",
        "author": "Guangyi He, Tobias Sutter, and Lukas Gonon",
        "link": "http://arxiv.org/abs/2508.14757v1",
        "abstract": "In this paper, we study the robustness of classical deep hedging strategies\nunder distributional shifts by leveraging the concept of adversarial attacks.\nWe first demonstrate that standard deep hedging models are highly vulnerable to\nsmall perturbations in the input distribution, resulting in significant\nperformance degradation. Motivated by this, we propose an adversarial training\nframework tailored to increase the robustness of deep hedging strategies. Our\napproach extends pointwise adversarial attacks to the distributional setting\nand introduces a computationally tractable reformulation of the adversarial\noptimization problem over a Wasserstein ball. This enables the efficient\ntraining of hedging strategies that are resilient to distributional\nperturbations. Through extensive numerical experiments, we show that\nadversarially trained deep hedging strategies consistently outperform their\nclassical counterparts in terms of out-of-sample performance and resilience to\nmodel misspecification. Our findings establish a practical and effective\nframework for robust deep hedging under realistic market uncertainties."
    },
    {
        "date": "2025-08",
        "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis",
        "author": "Abbas Sabra, Olivier Schmitt, and Joseph Tyler",
        "link": "http://arxiv.org/abs/2508.14727v1",
        "abstract": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."
    },
    {
        "date": "2025-08",
        "title": "Foe for Fraud: Transferable Adversarial Attacks in Credit Card Fraud Detection",
        "author": "Jan Lum Fok, Qingwen Zeng, Shiping Chen, Oscar Fawkes, and Huaming Chen",
        "link": "http://arxiv.org/abs/2508.14699v1",
        "abstract": "Credit card fraud detection (CCFD) is a critical application of Machine\nLearning (ML) in the financial sector, where accurately identifying fraudulent\ntransactions is essential for mitigating financial losses. ML models have\ndemonstrated their effectiveness in fraud detection task, in particular with\nthe tabular dataset. While adversarial attacks have been extensively studied in\ncomputer vision and deep learning, their impacts on the ML models, particularly\nthose trained on CCFD tabular datasets, remains largely unexplored. These\nlatent vulnerabilities pose significant threats to the security and stability\nof the financial industry, especially in high-value transactions where losses\ncould be substantial. To address this gap, in this paper, we present a holistic\nframework that investigate the robustness of CCFD ML model against adversarial\nperturbations under different circumstances. Specifically, the gradient-based\nattack methods are incorporated into the tabular credit card transaction data\nin both black- and white-box adversarial attacks settings. Our findings confirm\nthat tabular data is also susceptible to subtle perturbations, highlighting the\nneed for heightened awareness among financial technology practitioners\nregarding ML model security and trustworthiness. Furthermore, the experiments\nby transferring adversarial samples from gradient-based attack method to\nnon-gradient-based models also verify our findings. Our results demonstrate\nthat such attacks remain effective, emphasizing the necessity of developing\nrobust defenses for CCFD algorithms."
    },
    {
        "date": "2025-08",
        "title": "Potential and challenges of generative adversarial networks for super-resolution in 4D Flow MRI",
        "author": "Oliver Welin Odeback, Arivazhagan Geetha Balasubramanian, Jonas Schollenberger, Edward Ferdiand, Alistair A. Young, C. Alberto Figueroa, Susanne Schnell, Outi Tammisola, Ricardo Vinuesa, Tobias Granberg, Alexander Fyrdahl, and David Marlevi",
        "link": "http://arxiv.org/abs/2508.14950v1",
        "abstract": "4D Flow Magnetic Resonance Imaging (4D Flow MRI) enables non-invasive\nquantification of blood flow and hemodynamic parameters. However, its clinical\napplication is limited by low spatial resolution and noise, particularly\naffecting near-wall velocity measurements. Machine learning-based\nsuper-resolution has shown promise in addressing these limitations, but\nchallenges remain, not least in recovering near-wall velocities. Generative\nadversarial networks (GANs) offer a compelling solution, having demonstrated\nstrong capabilities in restoring sharp boundaries in non-medical\nsuper-resolution tasks. Yet, their application in 4D Flow MRI remains\nunexplored, with implementation challenged by known issues such as training\ninstability and non-convergence. In this study, we investigate GAN-based\nsuper-resolution in 4D Flow MRI. Training and validation were conducted using\npatient-specific cerebrovascular in-silico models, converted into synthetic\nimages via an MR-true reconstruction pipeline. A dedicated GAN architecture was\nimplemented and evaluated across three adversarial loss functions: Vanilla,\nRelativistic, and Wasserstein. Our results demonstrate that the proposed GAN\nimproved near-wall velocity recovery compared to a non-adversarial reference\n(vNRMSE: 6.9% vs. 9.6%); however, that implementation specifics are critical\nfor stable network training. While Vanilla and Relativistic GANs proved\nunstable compared to generator-only training (vNRMSE: 8.1% and 7.8% vs. 7.2%),\na Wasserstein GAN demonstrated optimal stability and incremental improvement\n(vNRMSE: 6.9% vs. 7.2%). The Wasserstein GAN further outperformed the\ngenerator-only baseline at low SNR (vNRMSE: 8.7% vs. 10.7%). These findings\nhighlight the potential of GAN-based super-resolution in enhancing 4D Flow MRI,\nparticularly in challenging cerebrovascular regions, while emphasizing the need\nfor careful selection of adversarial strategies."
    },
    {
        "date": "2025-08",
        "title": "MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr",
        "author": "Xuwen Yang",
        "link": "http://arxiv.org/abs/2508.15853v1",
        "abstract": "End-to-end ASR models, despite their success on benchmarks, often pro-duce\ncatastrophic semantic errors in noisy environments. We attribute this fragility\nto the prevailing 'direct mapping' objective, which solely penalizes final\noutput errors while leaving the model's internal computational pro-cess\nunconstrained. To address this, we introduce the Multi-Granularity Soft\nConsistency (MGSC) framework, a model-agnostic, plug-and-play module that\nenforces internal self-consistency by simultaneously regulariz-ing macro-level\nsentence semantics and micro-level token alignment. Cru-cially, our work is the\nfirst to uncover a powerful synergy between these two consistency\ngranularities: their joint optimization yields robustness gains that\nsignificantly surpass the sum of their individual contributions. On a public\ndataset, MGSC reduces the average Character Error Rate by a relative 8.7%\nacross diverse noise conditions, primarily by preventing se-vere\nmeaning-altering mistakes. Our work demonstrates that enforcing in-ternal\nconsistency is a crucial step towards building more robust and trust-worthy AI."
    },
    {
        "date": "2025-08",
        "title": "Mamba2 Meets Silence: Robust Vocal Source Separation for Sparse Regions",
        "author": "Euiyeon Kim, and Yong-Hoon Choi",
        "link": "http://arxiv.org/abs/2508.14556v1",
        "abstract": "We introduce a new music source separation model tailored for accurate vocal\nisolation. Unlike Transformer-based approaches, which often fail to capture\nintermittently occurring vocals, our model leverages Mamba2, a recent state\nspace model, to better capture long-range temporal dependencies. To handle long\ninput sequences efficiently, we combine a band-splitting strategy with a\ndual-path architecture. Experiments show that our approach outperforms recent\nstate-of-the-art models, achieving a cSDR of 11.03 dB-the best reported to\ndate-and delivering substantial gains in uSDR. Moreover, the model exhibits\nstable and consistent performance across varying input lengths and vocal\noccurrence patterns. These results demonstrate the effectiveness of Mamba-based\nmodels for high-resolution audio processing and open up new directions for\nbroader applications in audio research."
    },
    {
        "date": "2025-08",
        "title": "Adaptively Robust LLM Inference Optimization under Prediction Uncertainty",
        "author": "Zixi Chen, Yinyu Ye, and Zijie Zhou",
        "link": "http://arxiv.org/abs/2508.14544v1",
        "abstract": "We study the problem of optimizing Large Language Model (LLM) inference\nscheduling to minimize total latency. LLM inference is an online and multi-task\nservice process and also heavily energy consuming by which a pre-trained LLM\nprocesses input requests and generates output tokens sequentially. Therefore,\nit is vital to improve its scheduling efficiency and reduce the power\nconsumption while a great amount of prompt requests are arriving. A key\nchallenge in LLM inference scheduling is that while the prompt length is known\nupon arrival, the output length, which critically impacts memory usage and\nprocessing time, is unknown. To address this uncertainty, we propose algorithms\nthat leverage machine learning to predict output lengths, assuming the\nprediction provides an interval classification (min-max range) for each\nrequest.\n  We first design a conservative algorithm, $\\mathcal{A}_{\\max}$, which\nschedules requests based on the upper bound of predicted output lengths to\nprevent memory overflow. However, this approach is overly conservative: as\nprediction accuracy decreases, performance degrades significantly due to\npotential overestimation. To overcome this limitation, we propose\n$\\mathcal{A}_{\\min}$, an adaptive algorithm that initially treats the predicted\nlower bound as the output length and dynamically refines this estimate during\ninferencing. We prove that $\\mathcal{A}_{\\min}$ achieves a log-scale\ncompetitive ratio. Through numerical simulations, we demonstrate that\n$\\mathcal{A}_{\\min}$ often performs nearly as well as the hindsight scheduler,\nhighlighting both its efficiency and robustness in practical scenarios.\nMoreover, $\\mathcal{A}_{\\min}$ relies solely on the lower bound of the\nprediction interval--an advantageous design choice since upper bounds on output\nlength are typically more challenging to predict accurately."
    },
    {
        "date": "2025-08",
        "title": "DOPA: Stealthy and Generalizable Backdoor Attacks from a Single Client under Challenging Federated Constraints",
        "author": "Xuezheng Qin, Ruwei Huang, Xiaolong Tang, and Feng Li",
        "link": "http://arxiv.org/abs/2508.14530v1",
        "abstract": "Federated Learning (FL) is increasingly adopted for privacy-preserving\ncollaborative training, but its decentralized nature makes it particularly\nsusceptible to backdoor attacks. Existing attack methods, however, often rely\non idealized assumptions and fail to remain effective under real-world\nconstraints, such as limited attacker control, non-IID data distributions, and\nthe presence of diverse defense mechanisms. To address this gap, we propose\nDOPA (Divergent Optimization Path Attack), a novel framework that simulates\nheterogeneous local training dynamics and seeks consensus across divergent\noptimization trajectories to craft universally effective and stealthy backdoor\ntriggers. By leveraging consistency signals across simulated paths to guide\noptimization, DOPA overcomes the challenge of heterogeneity-induced instability\nand achieves practical attack viability under stringent federated constraints.\nWe validate DOPA on a comprehensive suite of 12 defense strategies, two model\narchitectures (ResNet18/VGG16), two datasets (CIFAR-10/TinyImageNet), and both\nmild and extreme non-IID settings. Despite operating under a single-client,\nblack-box, and sparsely participating threat model, DOPA consistently achieves\nhigh attack success, minimal accuracy degradation, low runtime, and long-term\npersistence. These results demonstrate a more practical attack paradigm,\noffering new perspectives for designing robust defense strategies in federated\nlearning systems"
    },
    {
        "date": "2025-08",
        "title": "Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles",
        "author": "Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2508.14527v2",
        "abstract": "The generation of safety-critical scenarios in simulation has become\nincreasingly crucial for safety evaluation in autonomous vehicles prior to road\ndeployment in society. However, current approaches largely rely on predefined\nthreat patterns or rule-based strategies, which limit their ability to expose\ndiverse and unforeseen failure modes. To overcome these, we propose ScenGE, a\nframework that can generate plentiful safety-critical scenarios by reasoning\nnovel adversarial cases and then amplifying them with complex traffic flows.\nGiven a simple prompt of a benign scene, it first performs Meta-Scenario\nGeneration, where a large language model, grounded in structured driving\nknowledge, infers an adversarial agent whose behavior poses a threat that is\nboth plausible and deliberately challenging. This meta-scenario is then\nspecified in executable code for precise in-simulator control. Subsequently,\nComplex Scenario Evolution uses background vehicles to amplify the core threat\nintroduced by Meta-Scenario. It builds an adversarial collaborator graph to\nidentify key agent trajectories for optimization. These perturbations are\ndesigned to simultaneously reduce the ego vehicle's maneuvering space and\ncreate critical occlusions. Extensive experiments conducted on multiple\nreinforcement learning based AV models show that ScenGE uncovers more severe\ncollision cases (+31.96%) on average than SoTA baselines. Additionally, our\nScenGE can be applied to large model based AV systems and deployed on different\nsimulators; we further observe that adversarial training on our scenarios\nimproves the model robustness. Finally, we validate our framework through\nreal-world vehicle tests and human evaluation, confirming that the generated\nscenarios are both plausible and critical. We hope our paper can build up a\ncritical step towards building public trust and ensuring their safe deployment."
    },
    {
        "date": "2025-08",
        "title": "CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production",
        "author": "Stefan Lenz, David Schachtschneider, Simon Jonas, Liam Tirpitz, Sandra Geisler, and Martin Henze",
        "link": "http://arxiv.org/abs/2508.14526v1",
        "abstract": "While the digitization of industrial factories provides tremendous\nimprovements for the production of goods, it also renders such systems\nvulnerable to serious cyber-attacks. To research, test, and validate security\nmeasures protecting industrial networks against such cyber-attacks, the\nsecurity community relies on testbeds to simulate industrial systems, as\nutilizing live systems endangers costly components or even human life. However,\nexisting testbeds focus on individual parts of typically complex production\nlines in industrial factories. Consequently, the impact of cyber-attacks on\nindustrial networks as well as the effectiveness of countermeasures cannot be\nevaluated in an end-to-end manner. To address this issue and facilitate\nresearch on novel security mechanisms, we present CoFacS, the first COmplete\nFACtory Simulation that replicates an entire production line and affords the\nintegration of real-life industrial applications. To showcase that CoFacS\naccurately captures real-world behavior, we validate it against a physical\nmodel factory widely used in security research. We show that CoFacS has a\nmaximum deviation of 0.11% to the physical reference, which enables us to study\nthe impact of physical attacks or network-based cyber-attacks. Moreover, we\nhighlight how CoFacS enables security research through two cases studies\nsurrounding attack detection and the resilience of 5G-based industrial\ncommunication against jamming."
    },
    {
        "date": "2025-08",
        "title": "EffiFusion-GAN: Efficient Fusion Generative Adversarial Network for Speech Enhancement",
        "author": "Bin Wen, and Tien-Ping Tan",
        "link": "http://arxiv.org/abs/2508.14525v1",
        "abstract": "We introduce EffiFusion-GAN (Efficient Fusion Generative Adversarial\nNetwork), a lightweight yet powerful model for speech enhancement. The model\nintegrates depthwise separable convolutions within a multi-scale block to\ncapture diverse acoustic features efficiently. An enhanced attention mechanism\nwith dual normalization and residual refinement further improves training\nstability and convergence. Additionally, dynamic pruning is applied to reduce\nmodel size while maintaining performance, making the framework suitable for\nresource-constrained environments. Experimental evaluation on the public\nVoiceBank+DEMAND dataset shows that EffiFusion-GAN achieves a PESQ score of\n3.45, outperforming existing models under the same parameter settings."
    },
    {
        "date": "2025-08",
        "title": "Linkage Attacks Expose Identity Risks in Public ECG Data Sharing",
        "author": "Ziyu Wang, Elahe Khatibi, Farshad Firouzi, Sanaz Rahimi Mousavi, Krishnendu Chakrabarty, and Amir M. Rahmani",
        "link": "http://arxiv.org/abs/2508.15850v1",
        "abstract": "The increasing availability of publicly shared electrocardiogram (ECG) data\nraises critical privacy concerns, as its biometric properties make individuals\nvulnerable to linkage attacks. Unlike prior studies that assume idealized\nadversarial capabilities, we evaluate ECG privacy risks under realistic\nconditions where attackers operate with partial knowledge. Using data from 109\nparticipants across diverse real-world datasets, our approach achieves 85%\naccuracy in re-identifying individuals in public datasets while maintaining a\n14.2% overall misclassification rate at an optimal confidence threshold, with\n15.6% of unknown individuals misclassified as known and 12.8% of known\nindividuals misclassified as unknown. These results highlight the inadequacy of\nsimple anonymization techniques in preventing re-identification, demonstrating\nthat even limited adversarial knowledge enables effective identity linkage. Our\nfindings underscore the urgent need for privacy-preserving strategies, such as\ndifferential privacy, access control, and encrypted computation, to mitigate\nre-identification risks while ensuring the utility of shared biosignal data in\nhealthcare applications."
    },
    {
        "date": "2025-08",
        "title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion",
        "author": "Yinghan Zhou, Juan Wen, Wanli Peng, Zhengxian Wu, Ziwei Zhang, and Yiming Xue",
        "link": "http://arxiv.org/abs/2508.15848v1",
        "abstract": "AI-generated text (AIGT) detection evasion aims to reduce the detection\nprobability of AIGT, helping to identify weaknesses in detectors and enhance\ntheir effectiveness and reliability in practical applications. Although\nexisting evasion methods perform well, they suffer from high computational\ncosts and text quality degradation. To address these challenges, we propose\nSelf-Disguise Attack (SDA), a novel approach that enables Large Language Models\n(LLM) to actively disguise its output, reducing the likelihood of detection by\nclassifiers. The SDA comprises two main components: the adversarial feature\nextractor and the retrieval-based context examples optimizer. The former\ngenerates disguise features that enable LLMs to understand how to produce more\nhuman-like text. The latter retrieves the most relevant examples from an\nexternal knowledge base as in-context examples, further enhancing the\nself-disguise ability of LLMs and mitigating the impact of the disguise process\non the diversity of the generated text. The SDA directly employs prompts\ncontaining disguise features and optimized context examples to guide the LLM in\ngenerating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average\ndetection accuracy of various AIGT detectors across texts generated by three\ndifferent LLMs, while maintaining the quality of AIGT."
    },
    {
        "date": "2025-08",
        "title": "Precision over Noise: Tailoring S3 Public Access Detection to Reduce False Positives in Cloud Security Platforms",
        "author": "Dikshant, and Geetika Verma",
        "link": "http://arxiv.org/abs/2508.14402v1",
        "abstract": "Excessive and spurious alert generation by cloud security solutions is a root\ncause of analyst fatigue and operational inefficiencies. In this study, the\nlong-standing issue of false positives from publicly accessible alerts in\nAmazon S3, as generated by a licensed cloud-native security solution, is\nexamined. In a simulated production test environment, which consisted of over\n1,000 Amazon S3 buckets with diverse access configurations, it was discovered\nthat over 80\\% of the alerts generated by default rules were classified as\nfalse positives, thus demonstrating the severity of the detection issue. This\nseverely impacted detection accuracy and generated a heavier workload for\nanalysts due to redundant manual triage efforts. For addressing this problem,\ncustom detection logic was created as an exercise of the native rule\ncustomization capabilities of the solution. A unified titled ``S3 Public Access\nValidation and Data Exposure'' was created in an effort to consolidate\ndifferent forms of alerts into one, context-aware logic that systematically\nscans ACL configurations, bucket policies, indicators of public exposure, and\nthe presence of sensitive data, and then marks only those S3 buckets that\nindeed denote security risk and are publicly exposed on the internet with no\nauthentication. The results demonstrate a significant reduction in false\npositives, more precise alert fidelity, and significant time saving for\nsecurity analysts, thus demonstrating an actionable and reproducible solution\nto enhance the accuracy of security alerting in compliance-focused cloud\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models",
        "author": "Thanh-Dat Truong, Huu-Thien Tran, Tran Thai Son, Bhiksha Raj, and Khoa Luu",
        "link": "http://arxiv.org/abs/2508.14264v1",
        "abstract": "Large multimodal models (LMMs) have gained impressive performance due to\ntheir outstanding capability in various understanding tasks. However, these\nmodels still suffer from some fundamental limitations related to robustness and\ngeneralization due to the alignment and correlation between visual and textual\nfeatures. In this paper, we introduce a simple but efficient learning mechanism\nfor improving the robust alignment between visual and textual modalities by\nsolving shuffling problems. In particular, the proposed approach can improve\nreasoning capability, visual understanding, and cross-modality alignment by\nintroducing two new tasks: reconstructing the image order and the text order\ninto the LMM's pre-training and fine-tuning phases. In addition, we propose a\nnew directed-token approach to capture visual and textual knowledge, enabling\nthe capability to reconstruct the correct order of visual inputs. Then, we\nintroduce a new Image-to-Response Guided loss to further improve the visual\nunderstanding of the LMM in its responses. The proposed approach consistently\nachieves state-of-the-art (SoTA) performance compared with prior LMMs on\nacademic task-oriented and instruction-following LMM benchmarks."
    },
    {
        "date": "2025-08",
        "title": "Noise Robust One-Class Intrusion Detection on Dynamic Graphs",
        "author": "Aleksei Liuliakov, Alexander Schulz, Luca Hermes, and Barbara Hammer",
        "link": "http://arxiv.org/abs/2508.14192v1",
        "abstract": "In the domain of network intrusion detection, robustness against contaminated\nand noisy data inputs remains a critical challenge. This study introduces a\nprobabilistic version of the Temporal Graph Network Support Vector Data\nDescription (TGN-SVDD) model, designed to enhance detection accuracy in the\npresence of input noise. By predicting parameters of a Gaussian distribution\nfor each network event, our model is able to naturally address noisy\nadversarials and improve robustness compared to a baseline model. Our\nexperiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate\nsignificant improvements in detection performance compared to the baseline\nTGN-SVDD model, especially as noise levels increase."
    },
    {
        "date": "2025-08",
        "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
        "author": "Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu",
        "link": "http://arxiv.org/abs/2508.14041v1",
        "abstract": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/"
    },
    {
        "date": "2025-08",
        "title": "ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery",
        "author": "Mohammad Izadi, and Mehran Safayani",
        "link": "http://arxiv.org/abs/2508.14005v1",
        "abstract": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition\nmarked by disruptions in brain connectivity. Functional MRI (fMRI) offers a\nnon-invasive window into large-scale neural dynamics by measuring\nblood-oxygen-level-dependent (BOLD) signals across the brain. These signals can\nbe modeled as interactions among Regions of Interest (ROIs), which are grouped\ninto functional communities based on their underlying roles in brain function.\nEmerging evidence suggests that connectivity patterns within and between these\ncommunities are particularly sensitive to ASD-related alterations. Effectively\ncapturing these patterns and identifying interactions that deviate from typical\ndevelopment is essential for improving ASD diagnosis and enabling biomarker\ndiscovery. In this work, we introduce ASDFormer, a Transformer-based\narchitecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to\ncapture neural signatures associated with ASD. By integrating multiple\nspecialized expert branches with attention mechanisms, ASDFormer adaptively\nemphasizes different brain regions and connectivity patterns relevant to\nautism. This enables both improved classification performance and more\ninterpretable identification of disorder-related biomarkers. Applied to the\nABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and\nreveals robust insights into functional connectivity disruptions linked to ASD,\nhighlighting its potential as a tool for biomarker discovery."
    },
    {
        "date": "2025-08",
        "title": "FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks",
        "author": "Nicol\u00f2 Romandini, Cristian Borcea, Rebecca Montanari, and Luca Foschini",
        "link": "http://arxiv.org/abs/2508.13853v1",
        "abstract": "Federated Learning (FL) can be vulnerable to attacks, such as model\npoisoning, where adversaries send malicious local weights to compromise the\nglobal model. Federated Unlearning (FU) is emerging as a solution to address\nsuch vulnerabilities by selectively removing the influence of detected\nmalicious contributors on the global model without complete retraining.\nHowever, unlike typical FU scenarios where clients are trusted and cooperative,\napplying FU with malicious and possibly colluding clients is challenging\nbecause their collaboration in unlearning their data cannot be assumed. This\nwork presents FedUP, a lightweight FU algorithm designed to efficiently\nmitigate malicious clients' influence by pruning specific connections within\nthe attacked model. Our approach achieves efficiency by relying only on\nclients' weights from the last training round before unlearning to identify\nwhich connections to inhibit. Isolating malicious influence is non-trivial due\nto overlapping updates from benign and malicious clients. FedUP addresses this\nby carefully selecting and zeroing the highest magnitude weights that diverge\nthe most between the latest updates from benign and malicious clients while\npreserving benign information. FedUP is evaluated under a strong adversarial\nthreat model, where up to 50%-1 of the clients could be malicious and have full\nknowledge of the aggregation process. We demonstrate the effectiveness,\nrobustness, and efficiency of our solution through experiments across IID and\nNon-IID data, under label-flipping and backdoor attacks, and by comparing it\nwith state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces\nmalicious influence, lowering accuracy on malicious data to match that of a\nmodel retrained from scratch while preserving performance on benign data. FedUP\nachieves effective unlearning while consistently being faster and saving\nstorage compared to the SOTA."
    },
    {
        "date": "2025-08",
        "title": "Timestep-Compressed Attack on Spiking Neural Networks through Timestep-Level Backpropagation",
        "author": "Donghwa Kang, Doohyun Kim, Sang-Ki Ko, Jinkyu Lee, Hyeongboo Baek, and Brent ByungHoon Kang",
        "link": "http://arxiv.org/abs/2508.13812v1",
        "abstract": "State-of-the-art (SOTA) gradient-based adversarial attacks on spiking neural\nnetworks (SNNs), which largely rely on extending FGSM and PGD frameworks, face\na critical limitation: substantial attack latency from multi-timestep\nprocessing, rendering them infeasible for practical real-time applications.\nThis inefficiency stems from their design as direct extensions of ANN\nparadigms, which fail to exploit key SNN properties. In this paper, we propose\nthe timestep-compressed attack (TCA), a novel framework that significantly\nreduces attack latency. TCA introduces two components founded on key insights\ninto SNN behavior. First, timestep-level backpropagation (TLBP) is based on our\nfinding that global temporal information in backpropagation to generate\nperturbations is not critical for an attack's success, enabling per-timestep\nevaluation for early stopping. Second, adversarial membrane potential reuse\n(A-MPR) is motivated by the observation that initial timesteps are\ninefficiently spent accumulating membrane potential, a warm-up phase that can\nbe pre-calculated and reused. Our experiments on VGG-11 and ResNet-17 with the\nCIFAR-10/100 and CIFAR10-DVS datasets show that TCA significantly reduces the\nrequired attack latency by up to 56.6% and 57.1% compared to SOTA methods in\nwhite-box and black-box settings, respectively, while maintaining a comparable\nattack success rate."
    },
    {
        "date": "2025-08",
        "title": "NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js",
        "author": "Eric Cornelissen, and Musard Balliu",
        "link": "http://arxiv.org/abs/2508.13750v1",
        "abstract": "The software supply chain is an increasingly common attack vector for\nmalicious actors. The Node.js ecosystem has been subject to a wide array of\nattacks, likely due to its size and prevalence. To counter such attacks, the\nresearch community and practitioners have proposed a range of static and\ndynamic mechanisms, including process- and language-level sandboxing,\npermission systems, and taint tracking. Drawing on valuable insight from these\nworks, this paper studies a runtime protection mechanism for (the supply chain\nof) Node.js applications with the ambitious goals of compatibility, automation,\nminimal overhead, and policy conciseness.\n  Specifically, we design, implement and evaluate NodeShield, a protection\nmechanism for Node.js that enforces an application's dependency hierarchy and\ncontrols access to system resources at runtime. We leverage the up-and-coming\nSBOM standard as the source of truth for the dependency hierarchy of the\napplication, thus preventing components from stealthily abusing undeclared\ncomponents. We propose to enhance the SBOM with a notion of capabilities that\nrepresents a set of related system resources a component may access. Our\nproposed SBOM extension, the Capability Bill of Materials or CBOM, records the\nrequired capabilities of each component, providing valuable insight into the\npotential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime\nvia code outlining (as opposed to inlining) with no modifications to the\noriginal code or Node.js runtime, thus preventing unexpected, potentially\nmalicious behavior. Our evaluation shows that NodeShield can prevent over 98%\nout of 67 known supply chain attacks while incurring minimal overhead on\nservers at less than 1ms per request. We achieve this while maintaining broad\ncompatibility with vanilla Node.js and a concise policy language that consists\nof at most 7 entries per dependency."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance",
        "author": "Yiming Cao, Yanjie Li, Kaisheng Liang, Yuni Lai, and Bin Xiao",
        "link": "http://arxiv.org/abs/2508.13739v1",
        "abstract": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT."
    },
    {
        "date": "2025-08",
        "title": "On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions",
        "author": "Daniel M. Jimenez-Gutierrez, Yelizaveta Falkouskaya, Jose L. Hernandez-Ramos, Aris Anagnostopoulos, Ioannis Chatzigiannakis, and Andrea Vitaletti",
        "link": "http://arxiv.org/abs/2508.13730v1",
        "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm\nenabling multiple clients to train a global model collaboratively without\nsharing their raw data. While FL enhances data privacy by design, it remains\nvulnerable to various security and privacy threats. This survey provides a\ncomprehensive overview of more than 200 papers regarding the state-of-the-art\nattacks and defense mechanisms developed to address these challenges,\ncategorizing them into security-enhancing and privacy-preserving techniques.\nSecurity-enhancing methods aim to improve FL robustness against malicious\nbehaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same\ntime, privacy-preserving techniques focus on protecting sensitive data through\ncryptographic approaches, differential privacy, and secure aggregation. We\ncritically analyze the strengths and limitations of existing methods, highlight\nthe trade-offs between privacy, security, and model performance, and discuss\nthe implications of non-IID data distributions on the effectiveness of these\ndefenses. Furthermore, we identify open research challenges and future\ndirections, including the need for scalable, adaptive, and energy-efficient\nsolutions operating in dynamic and heterogeneous FL environments. Our survey\naims to guide researchers and practitioners in developing robust and\nprivacy-preserving FL systems, fostering advancements safeguarding\ncollaborative learning frameworks' integrity and confidentiality."
    },
    {
        "date": "2025-08",
        "title": "MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers",
        "author": "Zhiqiang Wang, Yichao Gao, Yanting Wang, Suyuan Liu, Haifeng Sun, Haoran Cheng, Guanquan Shi, Haohua Du, and Xiangyang Li",
        "link": "http://arxiv.org/abs/2508.14925v1",
        "abstract": "By providing a standardized interface for LLM agents to interact with\nexternal tools, the Model Context Protocol (MCP) is quickly becoming a\ncornerstone of the modern autonomous agent ecosystem. However, it creates novel\nattack surfaces due to untrusted external tools. While prior work has focused\non attacks injected through external tool outputs, we investigate a more\nfundamental vulnerability: Tool Poisoning, where malicious instructions are\nembedded within a tool's metadata without execution. To date, this threat has\nbeen primarily demonstrated through isolated cases, lacking a systematic,\nlarge-scale evaluation.\n  We introduce MCPTox, the first benchmark to systematically evaluate agent\nrobustness against Tool Poisoning in realistic MCP settings. MCPTox is\nconstructed upon 45 live, real-world MCP servers and 353 authentic tools. To\nachieve this, we design three distinct attack templates to generate a\ncomprehensive suite of 1312 malicious test cases by few-shot learning, covering\n10 categories of potential risks. Our evaluation on 20 prominent LLM agents\nsetting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,\nachieving an attack success rate of 72.8\\%. We find that more capable models\nare often more susceptible, as the attack exploits their superior\ninstruction-following abilities. Finally, the failure case analysis reveals\nthat agents rarely refuse these attacks, with the highest refused rate\n(Claude-3.7-Sonnet) less than 3\\%, demonstrating that existing safety alignment\nis ineffective against malicious actions that use legitimate tools for\nunauthorized operation. Our findings create a crucial empirical baseline for\nunderstanding and mitigating this widespread threat, and we release MCPTox for\nthe development of verifiably safer AI agents. Our dataset is available at an\nanonymized repository: \\textit{https://anonymous.4open.science/r/AAAI26-7C02}."
    },
    {
        "date": "2025-08",
        "title": "Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond",
        "author": "Canzhe Zhao, Shinji Ito, and Shuai Li",
        "link": "http://arxiv.org/abs/2508.13679v1",
        "abstract": "Heavy-tailed bandits have been extensively studied since the seminal work of\n\\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,\nenabling efficient learning with both a large number of arms and heavy-tailed\nnoises, have recently attracted significant attention\n\\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.\nHowever, prior studies focus almost exclusively on stochastic regimes, with few\nexceptions limited to the special case of heavy-tailed multi-armed bandits\n(MABs) \\citep{Huang0H22,ChengZ024,Chen2024uniINF}.\n  In this work, we propose a general framework for adversarial heavy-tailed\nbandit problems, which performs follow-the-regularized-leader (FTRL) over the\nloss estimates shifted by a bonus function. Via a delicate setup of the bonus\nfunction, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm\nfor heavy-tailed MABs, which does not require the truncated non-negativity\nassumption and achieves an $\\widetilde{O}(T^{\\frac{1}{\\varepsilon}})$\nworst-case regret in the adversarial regime as well as an $\\widetilde{O}(\\log\nT)$ gap-dependent regret in the stochastic regime. We then extend our framework\nto the linear case, proposing the first algorithm for adversarial heavy-tailed\nlinear bandits with finite arm sets. This algorithm achieves an\n$\\widetilde{O}(d^{\\frac{1}{2}}T^{\\frac{1}{\\varepsilon}})$ regret, matching the\nbest-known worst-case regret bound in stochastic regimes. Moreover, we propose\na general data-dependent learning rate, termed \\textit{heavy-tailed noise aware\nstability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW\nregret bounds for general heavy-tailed bandit problems once certain conditions\nare satisfied. By using HT-SPM and, in particular, a variance-reduced linear\nloss estimator, we obtain the first BOBW result for heavy-tailed linear\nbandits."
    },
    {
        "date": "2025-08",
        "title": "V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task",
        "author": "Jikai Chen, Long Chen, Dong Wang, Leilei Gan, Chenyi Zhuang, and Jinjie Gu",
        "link": "http://arxiv.org/abs/2508.13634v1",
        "abstract": "Precise localization of GUI elements is crucial for the development of GUI\nagents. Traditional methods rely on bounding box or center-point regression,\nneglecting spatial interaction uncertainty and visual-semantic hierarchies.\nRecent methods incorporate attention mechanisms but still face two key issues:\n(1) ignoring processing background regions causes attention drift from the\ndesired area, and (2) uniform labeling fails to distinguish between center and\nedges of the target UI element, leading to click imprecision. Inspired by how\nhumans visually process and interact with GUI elements, we propose the\nValley-to-Peak (V2P) method to address these issues. To mitigate background\ndistractions, V2P introduces a suppression attention mechanism that minimizes\nthe model's focus on irrelevant regions to highlight the intended region. For\nthe issue of center-edge distinction, V2P applies a Fitts' Law-inspired\napproach by modeling GUI interactions as 2D Gaussian heatmaps where the weight\ngradually decreases from the center towards the edges. The weight distribution\nfollows a Gaussian function, with the variance determined by the target's size.\nConsequently, V2P effectively isolates the target area and teaches the model to\nconcentrate on the most essential point of the UI element. The model trained by\nV2P achieves the performance with 92.3% and 50.5% on two benchmarks\nScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's\ncontribution, highlighting V2P's generalizability for precise GUI grounding\ntasks."
    },
    {
        "date": "2025-08",
        "title": "DDoS Attacks in Cloud Computing: Detection and Prevention",
        "author": "Zain Ahmad, Musab Ahmad, and Bilal Ahmad",
        "link": "http://arxiv.org/abs/2508.13522v1",
        "abstract": "DDoS attacks are one of the most prevalent and harmful cybersecurity threats\nfaced by organizations and individuals today. In recent years, the complexity\nand frequency of DDoS attacks have increased significantly, making it\nchallenging to detect and mitigate them effectively. The study analyzes various\ntypes of DDoS attacks, including volumetric, protocol, and application layer\nattacks, and discusses the characteristics, impact, and potential targets of\neach type. It also examines the existing techniques used for DDoS attack\ndetection, such as packet filtering, intrusion detection systems, and machine\nlearning-based approaches, and their strengths and limitations. Moreover, the\nstudy explores the prevention techniques employed to mitigate DDoS attacks,\nsuch as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the\neffectiveness of each approach and its suitability for different types of\nattacks and environments. In conclusion, this study provides a comprehensive\noverview of the different types of DDoS attacks, their detection, and\nprevention techniques. It aims to provide insights and guidelines for\norganizations and individuals to enhance their cybersecurity posture and\nprotect against DDoS attacks."
    },
    {
        "date": "2025-08",
        "title": "Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security",
        "author": "Takreem Haider",
        "link": "http://arxiv.org/abs/2508.13520v1",
        "abstract": "Elliptic Curve Cryptography (ECC) is a fundamental component of modern\npublic-key cryptosystems that enable efficient and secure digital signatures,\nkey exchanges, and encryption. Its core operation, scalar multiplication,\ndenoted as $k \\cdot P$, where $P$ is a base point and $k$ is a private scalar,\nrelies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$\nis selected using user input or pseudorandom number generators. However, in\nresource-constrained environments with weak entropy sources, these approaches\nmay yield low-entropy or biased scalars, increasing susceptibility to\nside-channel and key recovery attacks. To mitigate these vulnerabilities, we\nintroduce an optimization-driven scalar generation method that explicitly\nmaximizes bit-level entropy. Our approach uses differential evolution (DE), a\npopulation-based metaheuristic algorithm, to search for scalars whose binary\nrepresentations exhibit maximal entropy, defined by an even and statistically\nuniform distribution of ones and zeros. This reformulation of scalar selection\nas an entropy-optimization problem enhances resistance to entropy-based\ncryptanalytic techniques and improves overall unpredictability. Experimental\nresults demonstrate that DE-optimized scalars achieve entropy significantly\nhigher than conventionally generated scalars. The proposed method can be\nintegrated into existing ECC-based protocols, offering a deterministic, tunable\nalternative to traditional randomness, ideal for applications in blockchain,\nsecure messaging, IoT, and other resource-constrained environments."
    },
    {
        "date": "2025-08",
        "title": "CCFC: Core & Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection",
        "author": "Jiaming Hu, Haoyu Wang, Debarghya Mukherjee, and Ioannis Ch. Paschalidis",
        "link": "http://arxiv.org/abs/2508.14128v1",
        "abstract": "Jailbreak attacks pose a serious challenge to the safe deployment of large\nlanguage models (LLMs). We introduce CCFC (Core & Core-Full-Core), a\ndual-track, prompt-level defense framework designed to mitigate LLMs'\nvulnerabilities from prompt injection and structure-aware jailbreak attacks.\nCCFC operates by first isolating the semantic core of a user query via few-shot\nprompting, and then evaluating the query using two complementary tracks: a\ncore-only track to ignore adversarial distractions (e.g., toxic suffixes or\nprefix injections), and a core-full-core (CFC) track to disrupt the structural\npatterns exploited by gradient-based or edit-based attacks. The final response\nis selected based on a safety consistency check across both tracks, ensuring\nrobustness without compromising on response quality. We demonstrate that CCFC\ncuts attack success rates by 50-75% versus state-of-the-art defenses against\nstrong adversaries (e.g., DeepInception, GCG), without sacrificing fidelity on\nbenign queries. Our method consistently outperforms state-of-the-art\nprompt-level defenses, offering a practical and effective solution for safer\nLLM deployment."
    },
    {
        "date": "2025-08",
        "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments",
        "author": "Jingwen Yu, Jiayi Yang, Anjun Hu, Jiankun Wang, Ping Tan, and Hong Zhang",
        "link": "http://arxiv.org/abs/2508.13488v1",
        "abstract": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Robustness of Implicit Neural Representations Against Weight Perturbations",
        "author": "Wenyong Zhou, Yuxin Cheng, Zhengwu Liu, Taiqiang Wu, Chen Zhang, and Ngai Wong",
        "link": "http://arxiv.org/abs/2508.13481v1",
        "abstract": "Implicit Neural Representations (INRs) encode discrete signals in a\ncontinuous manner using neural networks, demonstrating significant value across\nvarious multimedia applications. However, the vulnerability of INRs presents a\ncritical challenge for their real-world deployments, as the network weights\nmight be subjected to unavoidable perturbations. In this work, we investigate\nthe robustness of INRs for the first time and find that even minor\nperturbations can lead to substantial performance degradation in the quality of\nsignal reconstruction. To mitigate this issue, we formulate the robustness\nproblem in INRs by minimizing the difference between loss with and without\nweight perturbations. Furthermore, we derive a novel robust loss function to\nregulate the gradient of the reconstruction loss with respect to weights,\nthereby enhancing the robustness. Extensive experiments on reconstruction tasks\nacross multiple modalities demonstrate that our method achieves up to a 7.5~dB\nimprovement in peak signal-to-noise ratio (PSNR) values compared to original\nINRs under noisy conditions."
    },
    {
        "date": "2025-08",
        "title": "When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks",
        "author": "Mohamed Elmahallawy, and Tie Luo",
        "link": "http://arxiv.org/abs/2508.13425v1",
        "abstract": "Secure aggregation is a common technique in federated learning (FL) for\nprotecting data privacy from both curious internal entities (clients or server)\nand external adversaries (eavesdroppers). However, in dynamic and\nresource-constrained environments such as low Earth orbit (LEO) satellite\nnetworks, traditional secure aggregation methods fall short in two aspects: (1)\nthey assume continuous client availability while LEO satellite visibility is\nintermittent and irregular; (2) they consider privacy in each communication\nround but have overlooked the possible privacy leakage through multiple rounds.\nTo address these limitations, we propose LTP-FLEO, an asynchronous FL framework\nthat preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO\nintroduces (i) privacy-aware satellite partitioning, which groups satellites\nbased on their predictable visibility to the server and enforces joint\nparticipation; (ii) model age balancing, which mitigates the adverse impact of\nstale model updates; and (iii) fair global aggregation, which treats satellites\nof different visibility durations in an equitable manner. Theoretical analysis\nand empirical validation demonstrate that LTP-FLEO effectively safeguards both\nmodel and data privacy across multi-round training, promotes fairness in line\nwith satellite contributions, accelerates global convergence, and achieves\ncompetitive model accuracy."
    },
    {
        "date": "2025-08",
        "title": "DAASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples",
        "author": "Abdullah Al Nomaan Nafi, Habibur Rahaman, Zafaryab Haider, Tanzim Mahfuz, Fnu Suya, Swarup Bhunia, and Prabuddha Chakraborty",
        "link": "http://arxiv.org/abs/2508.13309v1",
        "abstract": "Numerous techniques have been proposed for generating adversarial examples in\nwhite-box settings under strict Lp-norm constraints. However, such norm-bounded\nexamples often fail to align well with human perception, and only recently have\na few methods begun specifically exploring perceptually aligned adversarial\nexamples. Moreover, it remains unclear whether insights from Lp-constrained\nattacks can be effectively leveraged to improve perceptual efficacy. In this\npaper, we introduce DAASH, a fully differentiable meta-attack framework that\ngenerates effective and perceptually aligned adversarial examples by\nstrategically composing existing Lp-based attack methods. DAASH operates in a\nmulti-stage fashion: at each stage, it aggregates candidate adversarial\nexamples from multiple base attacks using learned, adaptive weights and\npropagates the result to the next stage. A novel meta-loss function guides this\nprocess by jointly minimizing misclassification loss and perceptual distortion,\nenabling the framework to dynamically modulate the contribution of each base\nattack throughout the stages. We evaluate DAASH on adversarially trained models\nacross CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on\nLp-constrained based methods, DAASH significantly outperforms state-of-the-art\nperceptual attacks such as AdvAD -- achieving higher attack success rates\n(e.g., 20.63\\% improvement) and superior visual quality, as measured by SSIM,\nLPIPS, and FID (improvements $\\approx$ of 11, 0.015, and 5.7, respectively).\nFurthermore, DAASH generalizes well to unseen defenses, making it a practical\nand strong baseline for evaluating robustness without requiring handcrafted\nadaptive attacks for each new defense."
    },
    {
        "date": "2025-08",
        "title": "CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification",
        "author": "Zeynep Ozdemir, Hacer Yalim Keles, and Omer Ozgur Tanriover",
        "link": "http://arxiv.org/abs/2508.13280v1",
        "abstract": "Estimating disease severity from endoscopic images is essential in assessing\nulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to\ngrade inflammation. However, MES classification remains challenging due to\nlabel noise from inter-observer variability and the ordinal nature of the\nscore, which standard models often ignore. We propose CLoE, a curriculum\nlearning framework that accounts for both label reliability and ordinal\nstructure. Image quality, estimated via a lightweight model trained on Boston\nBowel Preparation Scale (BBPS) labels, is used as a proxy for annotation\nconfidence to order samples from easy (clean) to hard (noisy). This curriculum\nis further combined with ResizeMix augmentation to improve robustness.\nExperiments on the LIMUC and HyperKvasir datasets, using both CNNs and\nTransformers, show that CLoE consistently improves performance over strong\nsupervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches\n82.5\\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These\nresults highlight the potential of difficulty-aware training strategies for\nimproving ordinal classification under label uncertainty. Code will be released\nat https://github.com/zeynepozdemir/CLoE."
    },
    {
        "date": "2025-08",
        "title": "The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks",
        "author": "Bipin Chhetri, and Akbar Siami Namin",
        "link": "http://arxiv.org/abs/2508.13030v1",
        "abstract": "Cyberattacks are increasing, and securing against such threats is costing\nindustries billions of dollars annually. Threat Modeling, that is,\ncomprehending the consequences of these attacks, can provide critical support\nto cybersecurity professionals, enabling them to take timely action and\nallocate resources that could be used elsewhere. Cybersecurity is heavily\ndependent on threat modeling, as it assists security experts in assessing and\nmitigating risks related to identifying vulnerabilities and threats. Recently,\nthere has been a pressing need for automated methods to assess attack\ndescriptions and forecast the future consequences of the increasing complexity\nof cyberattacks. This study examines how Natural Language Processing (NLP) and\ndeep learning can be applied to analyze the potential impact of cyberattacks by\nleveraging textual descriptions from the MITRE Common Weakness Enumeration\n(CWE) database. We emphasize classifying attack consequences into five\nprincipal categories: Availability, Access Control, Confidentiality, Integrity,\nand Other. This paper investigates the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) in combination with Hierarchical\nAttention Networks (HANs) for Multi-label classification, evaluating their\nperformance in comparison with conventional CNN and LSTM-based models.\nExperimental findings show that BERT achieves an overall accuracy of $0.972$,\nfar higher than conventional deep learning models in multi-label\nclassification. HAN outperforms baseline forms of CNN and LSTM-based models on\nspecific cybersecurity labels. However, BERT consistently achieves better\nprecision and recall, making it more suitable for predicting the consequences\nof a cyberattack."
    },
    {
        "date": "2025-08",
        "title": "Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control",
        "author": "Iam Kim de S. Hermont, Andre R. Flores, and Rodrigo C. de Lamare",
        "link": "http://arxiv.org/abs/2508.13018v1",
        "abstract": "In this work, we propose a robust adaptive filtering approach for active\nnoise control applications in the presence of impulsive noise. In particular,\nwe develop the filtered-x hyperbolic tangent exponential generalized Kernel\nM-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis\nof the proposed FXHEKM algorithm is carried out along with a study of its\ncomputational cost. {In order to evaluate the proposed FXHEKM algorithm, the\nmean-square error (MSE) and the average noise reduction (ANR) performance\nmetrics have been adopted.} Numerical results show the efficiency of the\nproposed FXHEKM algorithm to cancel the presence of the additive spurious\nsignals, such as \\textbf{$\\alpha$}-stable noises against competing algorithms."
    },
    {
        "date": "2025-08",
        "title": "Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning",
        "author": "Yue Xia, Tayyebeh Jahani-Nezhad, and Rawad Bitar",
        "link": "http://arxiv.org/abs/2508.12978v1",
        "abstract": "We propose Fed-DPRoC, a novel federated learning framework that\nsimultaneously ensures differential privacy (DP), Byzantine robustness, and\ncommunication efficiency. We introduce the concept of robust-compatible\ncompression, which enables users to compress DP-protected updates while\nmaintaining the robustness of the aggregation rule. We instantiate our\nframework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for\ncompression with robust averaging for robust aggregation. We theoretically\nprove the compatibility of JL transform with robust averaging and show that\nRobAJoL preserves robustness guarantees, ensures DP, and reduces communication\ncost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims\nand demonstrate that RobAJoL outperforms existing methods in terms of\nrobustness and utility under different Byzantine attacks."
    },
    {
        "date": "2025-08",
        "title": "Prescriptive Zero Trust- Assessing the impact of zero trust on cyber attack prevention",
        "author": "Samuel Aiello",
        "link": "http://arxiv.org/abs/2508.12953v1",
        "abstract": "Increasingly sophisticated and varied cyber threats necessitate ever\nimproving enterprise security postures. For many organizations today, those\npostures have a foundation in the Zero Trust Architecture. This strategy sees\ntrust as something an enterprise must not give lightly or assume too broadly.\nUnderstanding the ZTA and its numerous controls centered around the idea of not\ntrusting anything inside or outside the network without verification, will\nallow organizations to comprehend and leverage this increasingly common\nparadigm. The ZTA, unlike many other regulatory frameworks, is not tightly\ndefined. The research assesses the likelihood of quantifiable guidelines that\nmeasure cybersecurity maturity for an enterprise organization in relation to\nZTA implementation. This is a new, data driven methodology for quantifying\ncyber resilience enabled by the adoption of Zero Trust principles to\npragmatically address the critical need of organizations. It also looks at the\npractical aspects ZTA has on capabilities in deterring cyberattacks on a\nnetwork. The outcomes of this research define a prescriptive set of key\ntechnical controls across identity verification, microsegmentation, data\nencryption, analytics, and orchestration that characterize the comprehensive\nZTA deployment. By evaluating the depth of integration for each control\ncomponent and aligning to industry best practices, the study's results help\nassess an organization's ZTA maturity level on a scale from Initial to\nOptimized adoption. The research's resultant four tier model demarcates phases\nfor an organization on its security transformation journey, with each tier\nadding to the capability of the last."
    },
    {
        "date": "2025-08",
        "title": "SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip",
        "author": "Ziteng Hu, Yingjie Xia, Xiyuan Chen, and Li Kuang",
        "link": "http://arxiv.org/abs/2508.12910v2",
        "abstract": "Finite State Machines (FSMs) play a critical role in implementing control\nlogic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by\nhardware engineers through Verilog coding, which is often tedious and\ntime-consuming. Recently, with the remarkable progress of Large Language Models\n(LLMs) in code generation, LLMs have been increasingly explored for automating\nVerilog code generation. However, LLM-generated Verilog code often suffers from\nsecurity vulnerabilities, which is particularly concerning for\nsecurity-sensitive FSM implementations. To address this issue, we propose\nSecFSM, a novel method that leverages a security-oriented knowledge graph to\nguide LLMs in generating more secure Verilog code. Specifically, we first\nconstruct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.\nSubsequently, we analyze users' requirements to identify vulnerabilities and\nget a list of vulnerabilities in the requirements. Then, we retrieve knowledge\nfrom FSKG based on the vulnerabilities list. Finally, we construct security\nprompts based on the security knowledge for Verilog code generation. To\nevaluate SecFSM, we build a dedicated dataset collected from academic datasets,\nartificial datasets, papers, and industrial cases. Extensive experiments\ndemonstrate that SecFSM outperforms state-of-the-art baselines. In particular,\non a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM\nachieves an outstanding pass rate of 21/25."
    },
    {
        "date": "2025-08",
        "title": "A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance",
        "author": "Jie Su, Weiwei Wang, Zhaotian Gu, Dahui Wang, and Tianyi Qian",
        "link": "http://arxiv.org/abs/2508.12702v1",
        "abstract": "Robust information representation and its persistent maintenance are\nfundamental for higher cognitive functions. Existing models employ distinct\nneural mechanisms to separately address noise-resistant processing or\ninformation maintenance, yet a unified framework integrating both operations\nremains elusive -- a critical gap in understanding cortical computation. Here,\nwe introduce a recurrent neural circuit that combines divisive normalization\nwith self-excitation to achieve both robust encoding and stable retention of\nnormalized inputs. Mathematical analysis shows that, for suitable parameter\nregimes, the system forms a continuous attractor with two key properties: (1)\ninput-proportional stabilization during stimulus presentation; and (2)\nself-sustained memory states persisting after stimulus offset. We demonstrate\nthe model's versatility in two canonical tasks: (a) noise-robust encoding in a\nrandom-dot kinematogram (RDK) paradigm; and (b) approximate Bayesian belief\nupdating in a probabilistic Wisconsin Card Sorting Test (pWCST). This work\nestablishes a unified mathematical framework that bridges noise suppression,\nworking memory, and approximate Bayesian inference within a single cortical\nmicrocircuit, offering fresh insights into the brain's canonical computation\nand guiding the design of biologically plausible artificial neural\narchitectures."
    },
    {
        "date": "2025-08",
        "title": "Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering",
        "author": "Emmanouil Kritharakis, Dusan Jakovetic, Antonios Makris, and Konstantinos Tserpes",
        "link": "http://arxiv.org/abs/2508.12672v2",
        "abstract": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing private data. We consider FL scenarios wherein FL\nclients are subject to adversarial (Byzantine) attacks, while the FL server is\ntrusted (honest) and has a trustworthy side dataset. This may correspond to,\ne.g., cases where the server possesses trusted data prior to federation, or to\nthe presence of a trusted client that temporarily assumes the server role. Our\napproach requires only two honest participants, i.e., the server and one\nclient, to function effectively, without prior knowledge of the number of\nmalicious clients. Theoretical analysis demonstrates bounded optimality gaps\neven under strong Byzantine attacks. Experimental results show that our\nalgorithm significantly outperforms standard and robust FL baselines such as\nMean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack\nstrategies including label flipping, sign flipping, and Gaussian noise addition\nacross MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework."
    },
    {
        "date": "2025-08",
        "title": "Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis",
        "author": "Soham Hans, Nikolos Gurney, Stacy Marsella, and Sofia Hirschmann",
        "link": "http://arxiv.org/abs/2508.13240v1",
        "abstract": "Understanding and quantifying human cognitive biases from empirical data has\nlong posed a formidable challenge, particularly in cybersecurity, where\ndefending against unknown adversaries is paramount. Traditional cyber defense\nstrategies have largely focused on fortification, while some approaches attempt\nto anticipate attacker strategies by mapping them to cognitive vulnerabilities,\nyet they fall short in dynamically interpreting attacks in progress. In\nrecognition of this gap, IARPA's ReSCIND program seeks to infer, defend\nagainst, and even exploit attacker cognitive traits. In this paper, we present\na novel methodology that leverages large language models (LLMs) to extract\nquantifiable insights into the cognitive bias of loss aversion from hacker\nbehavior. Our data are collected from an experiment in which hackers were\nrecruited to attack a controlled demonstration network. We process the hacker\ngenerated notes using LLMs using it to segment the various actions and\ncorrelate the actions to predefined persistence mechanisms used by hackers. By\ncorrelating the implementation of these mechanisms with various operational\ntriggers, our analysis provides new insights into how loss aversion manifests\nin hacker decision-making. The results demonstrate that LLMs can effectively\ndissect and interpret nuanced behavioral patterns, thereby offering a\ntransformative approach to enhancing cyber defense strategies through\nreal-time, behavior-based analysis."
    },
    {
        "date": "2025-08",
        "title": "How can we trust opaque systems? Criteria for robust explanations in XAI",
        "author": "Florian J. Boge, and Annika Schuster",
        "link": "http://arxiv.org/abs/2508.12623v1",
        "abstract": "Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in\nscientific research. However, the price we pay for their impressively accurate\npredictions is significant: their inner workings are notoriously opaque - it is\nunknown to laypeople and researchers alike what features of the data a DL\nsystem focuses on and how it ultimately succeeds in predicting correct outputs.\nA necessary criterion for trustworthy explanations is that they should reflect\nthe relevant processes the algorithms' predictions are based on. The field of\neXplainable Artificial Intelligence (XAI) presents promising methods to create\nsuch explanations. But recent reviews about their performance offer reasons for\nskepticism. As we will argue, a good criterion for trustworthiness is\nexplanatory robustness: different XAI methods produce the same explanations in\ncomparable contexts. However, in some instances, all methods may give the same,\nbut still wrong, explanation. We therefore argue that in addition to\nexplanatory robustness (ER), a prior requirement of explanation method\nrobustness (EMR) has to be fulfilled by every XAI method. Conversely, the\nrobustness of an individual method is in itself insufficient for\ntrustworthiness. In what follows, we develop and formalize criteria for ER as\nwell as EMR, providing a framework for explaining and establishing trust in DL\nalgorithms. We also highlight interesting application cases and outline\ndirections for future work."
    },
    {
        "date": "2025-08",
        "title": "Reducing False Positives with Active Behavioral Analysis for Cloud Security",
        "author": "Dikshant, and Verma",
        "link": "http://arxiv.org/abs/2508.12584v1",
        "abstract": "Rule-based cloud security posture management (CSPM) solutions are known to\nproduce a lot of false positives based on the limited contextual understanding\nand dependence on static heuristics testing. This paper introduces a\nvalidation-driven methodology that integrates active behavioral testing in\ncloud security posture management solution(s) to evaluate the exploitability of\npolicy violations in real time. The proposed system employs lightweight and\nautomated probes, built from open-source tools, validation scripts, and\npenetration testing test cases, to simulate adversarial attacks on\nmisconfigured or vulnerable cloud assets without any impact to the cloud\nservices or environment. For instance, cloud services may be flagged as\npublicly exposed and vulnerable despite being protected by access control\nlayers, or secure policies, resulting in non-actionable alerts that consumes\nanalysts time during manual validation. Through controlled experimentation in a\nreproducible AWS setup, we evaluated the reduction in false positive rates\nacross various misconfiguration and vulnerable alerts. Our findings indicate an\naverage reduction of 93\\% in false positives. Furthermore, the framework\ndemonstrates low latency performance. These results demonstrate a scalable\nmethod to improve detection accuracy and analyst productivity in large cloud\nenvironments. While our evaluation focuses on AWS, the architecture is modular\nand extensible to multi-cloud setups."
    },
    {
        "date": "2025-08",
        "title": "DEFENDCLI: {Command-Line} Driven Attack Provenance Examination",
        "author": "Peilun Wu, Nan Sun, Nour Moustafa, Youyang Qu, and Ming Ding",
        "link": "http://arxiv.org/abs/2508.12553v1",
        "abstract": "Endpoint Detection and Response (EDR) solutions embrace the method of attack\nprovenance graph to discover unknown threats through system event correlation.\nHowever, this method still faces some unsolved problems in the fields of\ninteroperability, reliability, flexibility, and practicability to deliver\nactionable results. Our research highlights the limitations of current\nsolutions in detecting obfuscation, correlating attacks, identifying\nlow-frequency events, and ensuring robust context awareness in relation to\ncommand-line activities. To address these challenges, we introduce DEFENDCLI,\nan innovative system leveraging provenance graphs that, for the first time,\ndelves into command-line-level detection. By offering finer detection\ngranularity, it addresses a gap in modern EDR systems that has been overlooked\nin previous research. Our solution improves the precision of the information\nrepresentation by evaluating differentiation across three levels: unusual\nsystem process calls, suspicious command-line executions, and infrequent\nexternal network connections. This multi-level approach enables EDR systems to\nbe more reliable in complex and dynamic environments. Our evaluation\ndemonstrates that DEFENDCLI improves precision by approximately 1.6x compared\nto the state-of-the-art methods on the DARPA Engagement Series attack datasets.\nExtensive real-time industrial testing across various attack scenarios further\nvalidates its practical effectiveness. The results indicate that DEFENDCLI not\nonly detects previously unknown attack instances, which are missed by other\nmodern commercial solutions, but also achieves a 2.3x improvement in precision\nover the state-of-the-art research work."
    },
    {
        "date": "2025-08",
        "title": "Systematic Analysis of MCP Security",
        "author": "Yongjian Guo, Puzhuo Liu, Wanlun Ma, Zehang Deng, Xiaogang Zhu, Peng Di, Xi Xiao, and Sheng Wen",
        "link": "http://arxiv.org/abs/2508.12538v1",
        "abstract": "The Model Context Protocol (MCP) has emerged as a universal standard that\nenables AI agents to seamlessly connect with external tools, significantly\nenhancing their functionality. However, while MCP brings notable benefits, it\nalso introduces significant vulnerabilities, such as Tool Poisoning Attacks\n(TPA), where hidden malicious instructions exploit the sycophancy of large\nlanguage models (LLMs) to manipulate agent behavior. Despite these risks,\ncurrent academic research on MCP security remains limited, with most studies\nfocusing on narrow or qualitative analyses that fail to capture the diversity\nof real-world threats. To address this gap, we present the MCP Attack Library\n(MCPLIB), which categorizes and implements 31 distinct attack methods under\nfour key classifications: direct tool injection, indirect tool injection,\nmalicious user attacks, and LLM inherent attack. We further conduct a\nquantitative analysis of the efficacy of each attack. Our experiments reveal\nkey insights into MCP vulnerabilities, including agents' blind reliance on tool\ndescriptions, sensitivity to file-based attacks, chain attacks exploiting\nshared context, and difficulty distinguishing external data from executable\ncommands. These insights, validated through attack experiments, underscore the\nurgency for robust defense strategies and informed MCP design. Our\ncontributions include 1) constructing a comprehensive MCP attack taxonomy, 2)\nintroducing a unified attack framework MCPLIB, and 3) conducting empirical\nvulnerability analysis to enhance MCP security mechanisms. This work provides a\nfoundational framework, supporting the secure evolution of MCP ecosystems."
    },
    {
        "date": "2025-08",
        "title": "A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security",
        "author": "Afrah Gueriani, Hamza Kheddar, Ahmed Cherif Mazari, and Mohamed Chahine Ghanem",
        "link": "http://arxiv.org/abs/2508.12470v1",
        "abstract": "The increased Internet of Medical Things IoMT and the Industrial Internet of\nThings IIoT interconnectivity has introduced complex cybersecurity challenges,\nexposing sensitive data, patient safety, and industrial operations to advanced\ncyber threats. To mitigate these risks, this paper introduces a novel\ntransformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid\nmodel that combines bidirectional gated recurrent units BiGRU, long short-term\nmemory LSTM networks, and multi-head attention MHA. The proposed architecture\nis designed to effectively capture bidirectional temporal dependencies, model\nsequential patterns, and enhance contextual feature representation. Extensive\nexperiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset\nindustrial IoT demonstrate the model's cross-domain robustness, achieving\ndetection accuracies of 99.13 percent and 99.34 percent, respectively.\nAdditionally, the model exhibits exceptional runtime efficiency, with inference\ntimes as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT\nscenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a\nreliable and efficient IDS for deployment in real-world heterogeneous IoT\nenvironments"
    },
    {
        "date": "2025-08",
        "title": "Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations",
        "author": "Yahsin Yeh, Yilun Wu, Bokai Ruan, and Honghan Shuai",
        "link": "http://arxiv.org/abs/2508.12430v1",
        "abstract": "Natural language explanations in visual question answering (VQA-NLE) aim to\nmake black-box models more transparent by elucidating their decision-making\nprocesses. However, we find that existing VQA-NLE systems can produce\ninconsistent explanations and reach conclusions without genuinely understanding\nthe underlying context, exposing weaknesses in either their inference pipeline\nor explanation-generation mechanism. To highlight these vulnerabilities, we not\nonly leverage an existing adversarial strategy to perturb questions but also\npropose a novel strategy that minimally alters images to induce contradictory\nor spurious outputs. We further introduce a mitigation method that leverages\nexternal knowledge to alleviate these inconsistencies, thereby bolstering model\nrobustness. Extensive evaluations on two standard benchmarks and two widely\nused VQA-NLE models underscore the effectiveness of our attacks and the\npotential of knowledge-based defenses, ultimately revealing pressing security\nand reliability concerns in current VQA-NLE systems."
    },
    {
        "date": "2025-08",
        "title": "ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers",
        "author": "Hanwen Cao, Haobo Lu, Xiaosen Wang, and Kun He",
        "link": "http://arxiv.org/abs/2508.12384v1",
        "abstract": "Ensemble-based attacks have been proven to be effective in enhancing\nadversarial transferability by aggregating the outputs of models with various\narchitectures. However, existing research primarily focuses on refining\nensemble weights or optimizing the ensemble path, overlooking the exploration\nof ensemble models to enhance the transferability of adversarial attacks. To\naddress this gap, we propose applying adversarial augmentation to the surrogate\nmodels, aiming to boost overall generalization of ensemble models and reduce\nthe risk of adversarial overfitting. Meanwhile, observing that ensemble Vision\nTransformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on\nthe idea of model adversarial augmentation, the first ensemble-based attack\nmethod tailored for ViTs to the best of our knowledge. Our approach generates\naugmented models for each surrogate ViT using three strategies: Multi-head\ndropping, Attention score scaling, and MLP feature mixing, with the associated\nparameters optimized by Bayesian optimization. These adversarially augmented\nmodels are ensembled to generate adversarial examples. Furthermore, we\nintroduce Automatic Reweighting and Step Size Enlargement modules to boost\ntransferability. Extensive experiments demonstrate that ViT-EnsembleAttack\nsignificantly enhances the adversarial transferability of ensemble-based\nattacks on ViTs, outperforming existing methods by a substantial margin. Code\nis available at https://github.com/Trustworthy-AI-Group/TransferAttack."
    },
    {
        "date": "2025-08",
        "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols",
        "author": "Yixuan Yang, Daoyuan Wu, and Yufan Chen",
        "link": "http://arxiv.org/abs/2508.13220v1",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, and attack scripts to evaluate these attacks across three major MCP\nproviders. Our benchmark is modular and extensible, allowing researchers to\nincorporate custom implementations of clients, servers, and transport protocols\nfor systematic security assessment. Experimental results show that over 85% of\nthe identified attacks successfully compromise at least one platform, with core\nvulnerabilities universally affecting Claude, OpenAI, and Cursor, while\nprompt-based and tool-centric attacks exhibit considerable variability across\ndifferent hosts and models. Overall, MCPSecBench standardizes the evaluation of\nMCP security and enables rigorous testing across all MCP layers."
    },
    {
        "date": "2025-08",
        "title": "Adjustable AprilTags For Identity Secured Tasks",
        "author": "Hao Li",
        "link": "http://arxiv.org/abs/2508.12304v1",
        "abstract": "Special tags such as AprilTags that facilitate image processing and pattern\nrecognition are useful in practical applications. In close and private\nenvironments, identity security is unlikely to be an issue because all involved\nAprilTags can be completely regulated. However, in open and public\nenvironments, identity security is no longer an issue that can be neglected. To\nhandle potential harm caused by adversarial attacks, this note advocates\nutilization of adjustable AprilTags instead of fixed ones."
    },
    {
        "date": "2025-08",
        "title": "HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization",
        "author": "Hyebin Ahn, Kangwook Jang, and Hoirin Kim",
        "link": "http://arxiv.org/abs/2508.12292v1",
        "abstract": "Noise robustness in speech foundation models (SFMs) has been a critical\nchallenge, as most models are primarily trained on clean data and experience\nperformance degradation when the models are exposed to noisy speech. To address\nthis issue, we propose HuBERT-VIC, a noise-robust SFM with variance,\nin-variance, and covariance regularization (VICReg) objectives. These\nobjectives adjust the statistics of noisy speech representations, enabling the\nmodel to capture diverse acoustic characteristics and improving the\ngeneralization ability across different types of noise. When applied to HuBERT,\nour model shows relative performance improvements of 23.3% on LibriSpeech\ntest-clean and 13.2% on test-other, compared to the baseline model pre-trained\non noisy speech."
    },
    {
        "date": "2025-08",
        "title": "WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions",
        "author": "Quan Chen, Xiong Yang, Rongfeng Lu, Qianyu Zhang, Yu Liu, Xiaofei Zhou, and Bolun Zheng",
        "link": "http://arxiv.org/abs/2508.12250v1",
        "abstract": "Salient object detection (SOD) in complex environments remains a challenging\nresearch topic. Most existing methods perform well in natural scenes with\nnegligible noise, and tend to leverage multi-modal information (e.g., depth and\ninfrared) to enhance accuracy. However, few studies are concerned with the\ndamage of weather noise on SOD performance due to the lack of dataset with\npixel-wise annotations. To bridge this gap, this paper introduces a novel\nWeather-eXtended Salient Object Detection (WXSOD) dataset. It consists of\n14,945 RGB images with diverse weather noise, along with the corresponding\nground truth annotations and weather labels. To verify algorithm\ngeneralization, WXSOD contains two test sets, i.e., a synthesized test set and\na real test set. The former is generated by adding weather noise to clean\nimages, while the latter contains real-world weather noise. Based on WXSOD, we\npropose an efficient baseline, termed Weather-aware Feature Aggregation Network\n(WFANet), which adopts a fully supervised two-branch architecture.\nSpecifically, the weather prediction branch mines weather-related deep\nfeatures, while the saliency detection branch fuses semantic features extracted\nfrom the backbone with weather features for SOD. Comprehensive comparisons\nagainst 17 SOD methods shows that our WFANet achieves superior performance on\nWXSOD. The code and benchmark results will be made publicly available at\nhttps://github.com/C-water/WXSOD"
    },
    {
        "date": "2025-08",
        "title": "CAN Networks Security in Smart Grids Communication Technologies",
        "author": "Ayman W. Baharia, Khaled T. Naga, Hesham S. Abdelfattah, Shady A. Maged, and Sherif A. Hammad",
        "link": "http://arxiv.org/abs/2508.12181v1",
        "abstract": "The rapid evolution of smart grids requires effective communication protocols\nto transfer data reliably and securely. Controller Area Network (CAN) is one of\nthe most recognized protocols that offer reliable data transmission in smart\ngrids due to its robustness, real-time capabilities, and relatively low initial\ncost of its required hardware. However, as a smart city becomes more\ninterconnected, it also becomes more vulnerable to cyber-attacks. As there are\nmany mechanisms to secure the CAN nodes from attacks, most of those mechanisms\nhave computational overhead, resulting in more delay in the network. We\nimplemented a solution that requires almost no overhead to any CAN node\nconnected to the network. It depends on a single node responsible for securing\nthe CAN network. This approach seeks to augment network security while reducing\nsecurity mechanisms overhead to all CAN network nodes. The methodology and\ncomprehensive test results will be presented in detail during a subsequent\ndiscussion. The used software for development is Code Composer Studio, and the\nused microcontroller evaluation boards (EVB) are TM4C 1294."
    },
    {
        "date": "2025-08",
        "title": "Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous",
        "author": "Ben Nassi, Stav Cohen, and Or Yair",
        "link": "http://arxiv.org/abs/2508.12175v1",
        "abstract": "The growing integration of LLMs into applications has introduced new security\nrisks, notably known as Promptware - maliciously engineered prompts designed to\nmanipulate LLMs to compromise the CIA triad of these applications. While prior\nresearch warned about a potential shift in the threat landscape for LLM-powered\napplications, the risk posed by Promptware is frequently perceived as low. In\nthis paper, we investigate the risk Promptware poses to users of Gemini-powered\nassistants (web application, mobile application, and Google Assistant). We\npropose a novel Threat Analysis and Risk Assessment (TARA) framework to assess\nPromptware risks for end users. Our analysis focuses on a new variant of\nPromptware called Targeted Promptware Attacks, which leverage indirect prompt\ninjection via common user interactions such as emails, calendar invitations,\nand shared documents. We demonstrate 14 attack scenarios applied against\nGemini-powered assistants across five identified threat classes: Short-term\nContext Poisoning, Permanent Memory Poisoning, Tool Misuse, Automatic Agent\nInvocation, and Automatic App Invocation. These attacks highlight both digital\nand physical consequences, including spamming, phishing, disinformation\ncampaigns, data exfiltration, unapproved user video streaming, and control of\nhome automation devices. We reveal Promptware's potential for on-device lateral\nmovement, escaping the boundaries of the LLM-powered application, to trigger\nmalicious actions using a device's applications. Our TARA reveals that 73% of\nthe analyzed threats pose High-Critical risk to end users. We discuss\nmitigations and reassess the risk (in response to deployed mitigations) and\nshow that the risk could be reduced significantly to Very Low-Medium. We\ndisclosed our findings to Google, which deployed dedicated mitigations."
    },
    {
        "date": "2025-08",
        "title": "Attack Graph Generation on HPC Clusters",
        "author": "Ming Li, and John Hale",
        "link": "http://arxiv.org/abs/2508.12161v1",
        "abstract": "Attack graphs (AGs) are graphical tools to analyze the security of computer\nnetworks. By connecting the exploitation of individual vulnerabilities, AGs\nexpose possible multi-step attacks against target networks, allowing system\nadministrators to take preventive measures to enhance their network's security.\nAs powerful analytical tools, however, AGs are both time- and memory-consuming\nto be generated. As the numbers of network assets, interconnections between\ndevices, as well as vulnerabilities increase, the size and volume of the\nresulting AGs grow at a much higher rate, leading to the well-known state-space\nexplosion. In this paper, we propose the use of high performance computing\n(HPC) clusters to implement AG generators. We evaluate the performance through\nexperiments and provide insights into how cluster environments can help resolve\nthe issues of slow speed and high memory demands in AG generation in a balanced\nway."
    },
    {
        "date": "2025-08",
        "title": "TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks",
        "author": "Amira Guesmi, Bassem Ouni, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2508.12132v1",
        "abstract": "Quantized Neural Networks (QNNs) are increasingly deployed in edge and\nresource-constrained environments due to their efficiency in computation and\nmemory usage. While shown to distort the gradient landscape and weaken\nconventional pixel-level attacks, it provides limited robustness against\npatch-based adversarial attacks-localized, high-saliency perturbations that\nremain surprisingly transferable across bit-widths. Existing defenses either\noverfit to fixed quantization settings or fail to address this cross-bit\ngeneralization vulnerability. We introduce \\textbf{TriQDef}, a tri-level\nquantization-aware defense framework designed to disrupt the transferability of\npatch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature\nDisalignment Penalty (FDP) that enforces semantic inconsistency by penalizing\nperceptual similarity in intermediate representations; (2) a Gradient\nPerceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients\nacross bit-widths by minimizing structural and directional agreement via Edge\nIoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training\nProtocol that unifies these penalties within a shared-weight training scheme\nacross multiple quantization levels. Extensive experiments on CIFAR-10 and\nImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over\n40\\% on unseen patch and quantization combinations, while preserving high clean\naccuracy. Our findings underscore the importance of disrupting both semantic\nand perceptual gradient alignment to mitigate patch transferability in QNNs."
    },
    {
        "date": "2025-08",
        "title": "Ethereum Crypto Wallets under Address Poisoning: How Usable and Secure Are They?",
        "author": "Shixuan Guan, and Kai Li",
        "link": "http://arxiv.org/abs/2508.12107v1",
        "abstract": "Blockchain address poisoning is an emerging phishing attack that crafts\n\"similar-looking\" transfer records in the victim's transaction history, which\naims to deceive victims and lure them into mistakenly transferring funds to the\nattacker. Recent works have shown that millions of Ethereum users were targeted\nand lost over 100 million US dollars.\n  Ethereum crypto wallets, serving users in browsing transaction history and\ninitiating transactions to transfer funds, play a central role in deploying\ncountermeasures to mitigate the address poisoning attack. However, whether they\nhave done so remains an open question. To fill the research void, in this\npaper, we design experiments to simulate address poisoning attacks and\nsystematically evaluate the usability and security of 53 popular Ethereum\ncrypto wallets. Our evaluation shows that there exist communication failures\nbetween 12 wallets and their transaction activity provider, which renders them\nunable to download the users' transaction history. Besides, our evaluation also\nshows that 16 wallets pose a high risk to their users due to displaying fake\ntoken phishing transfers. Moreover, our further analysis suggests that most\nwallets rely on transaction activity providers to filter out phishing\ntransfers. However, their phishing detection capability varies. Finally, we\nfound that only three wallets throw an explicit warning message when users\nattempt to transfer to the phishing address, implying a significant gap within\nthe broader Ethereum crypto wallet community in protecting users from address\npoisoning attacks.\n  Overall, our work shows that more efforts are needed by the Ethereum crypto\nwallet developer community to achieve the highest usability and security\nstandard. Our bug reports have been acknowledged by the developer community,\nwho are currently developing mitigation solutions."
    },
    {
        "date": "2025-08",
        "title": "Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network",
        "author": "Nilay Kushawaha, Carlo Alessi, Lorenzo Fruzzetti, and Egidio Falotico",
        "link": "http://arxiv.org/abs/2508.14100v1",
        "abstract": "Deep learning provides a powerful method for modeling the dynamics of soft\nrobots, offering advantages over traditional analytical approaches that require\nprecise knowledge of the robot's structure, material properties, and other\nphysical characteristics. Given the inherent complexity and non-linearity of\nthese systems, extracting such details can be challenging. The mappings learned\nin one domain cannot be directly transferred to another domain with different\nphysical properties. This challenge is particularly relevant for soft robots,\nas their materials gradually degrade over time. In this paper, we introduce a\ndomain translation framework based on a conditional cycle generative\nadversarial network (CCGAN) to enable knowledge transfer from a source domain\nto a target domain. Specifically, we employ a dynamic learning approach to\nadapt a pose controller trained in a standard simulation environment to a\ndomain with tenfold increased viscosity. Our model learns from input pressure\nsignals conditioned on corresponding end-effector positions and orientations in\nboth domains. We evaluate our approach through trajectory-tracking experiments\nacross five distinct shapes and further assess its robustness under noise\nperturbations and periodicity tests. The results demonstrate that CCGAN-GP\neffectively facilitates cross-domain skill transfer, paving the way for more\nadaptable and generalizable soft robotic controllers."
    },
    {
        "date": "2025-08",
        "title": "Robust Data Fusion via Subsampling",
        "author": "Jing Wang, HaiYing Wang, and Kun Chen",
        "link": "http://arxiv.org/abs/2508.12048v1",
        "abstract": "Data fusion and transfer learning are rapidly growing fields that enhance\nmodel performance for a target population by leveraging other related data\nsources or tasks. The challenges lie in the various potential heterogeneities\nbetween the target and external data, as well as various practical concerns\nthat prevent a na\\\"ive data integration. We consider a realistic scenario where\nthe target data is limited in size while the external data is large but\ncontaminated with outliers; such data contamination, along with other\ncomputational and operational constraints, necessitates proper selection or\nsubsampling of the external data for transfer learning. To our\nknowledge,transfer learning and subsampling under data contamination have not\nbeen thoroughly investigated. We address this gap by studying various transfer\nlearning methods with subsamples of the external data, accounting for outliers\ndeviating from the underlying true model due to arbitrary mean shifts. Two\nsubsampling strategies are investigated: one aimed at reducing biases and the\nother at minimizing variances. Approaches to combine these strategies are also\nintroduced to enhance the performance of the estimators. We provide\nnon-asymptotic error bounds for the transfer learning estimators, clarifying\nthe roles of sample sizes, signal strength, sampling rates, magnitude of\noutliers, and tail behaviors of model error distributions, among other factors.\nExtensive simulations show the superior performance of the proposed methods.\nAdditionally, we apply our methods to analyze the risk of hard landings in A380\nairplanes by utilizing data from other airplane types,demonstrating that robust\ntransfer learning can improve estimation efficiency for relatively rare\nairplane types with the help of data from other types of airplanes."
    },
    {
        "date": "2025-08",
        "title": "DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects",
        "author": "Tingbang Liang, Yixin Zeng, Jiatong Xie, and Boyu Zhou",
        "link": "http://arxiv.org/abs/2508.11950v1",
        "abstract": "We present DynamicPose, a retraining-free 6D pose tracking framework that\nimproves tracking robustness in fast-moving camera and object scenarios.\nPrevious work is mainly applicable to static or quasi-static scenes, and its\nperformance significantly deteriorates when both the object and the camera move\nrapidly. To overcome these challenges, we propose three synergistic components:\n(1) A visual-inertial odometry compensates for the shift in the Region of\nInterest (ROI) caused by camera motion; (2) A depth-informed 2D tracker\ncorrects ROI deviations caused by large object translation; (3) A VIO-guided\nKalman filter predicts object rotation, generates multiple candidate poses, and\nthen obtains the final pose by hierarchical refinement. The 6D pose tracking\nresults guide subsequent 2D tracking and Kalman filter updates, forming a\nclosed-loop system that ensures accurate pose initialization and precise pose\ntracking. Simulation and real-world experiments demonstrate the effectiveness\nof our method, achieving real-time and robust 6D pose tracking for fast-moving\ncameras and objects."
    },
    {
        "date": "2025-08",
        "title": "Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware",
        "author": "Yuannuo Feng, Wenyong Zhou, Yuexi Lyu, Yixiang Zhang, Zhengwu Liu, Ngai Wong, and Wang Kang",
        "link": "http://arxiv.org/abs/2508.11940v1",
        "abstract": "Analog Compute-In-Memory (CIM) architectures promise significant energy\nefficiency gains for neural network inference, but suffer from complex\nhardware-induced noise that poses major challenges for deployment. While\nnoise-aware training methods have been proposed to address this issue, they\ntypically rely on idealized and differentiable noise models that fail to\ncapture the full complexity of analog CIM hardware variations. Motivated by the\nStraight-Through Estimator (STE) framework in quantization, we decouple forward\nnoise simulation from backward gradient computation, enabling noise-aware\ntraining with more accurate but computationally intractable noise modeling in\nanalog CIM systems. We provide theoretical analysis demonstrating that our\napproach preserves essential gradient directional information while maintaining\ncomputational tractability and optimization stability. Extensive experiments\nshow that our extended STE framework achieves up to 5.3% accuracy improvement\non image classification, 0.72 perplexity reduction on text generation,\n2.2$\\times$ speedup in training time, and 37.9% lower peak memory usage\ncompared to standard noise-aware training methods."
    },
    {
        "date": "2025-08",
        "title": "HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware",
        "author": "Yuannuo Feng, Wenyong Zhou, Yuexi Lyu, Hanjie Liu, Zhengwu Liu, Ngai Wong, and Wang Kang",
        "link": "http://arxiv.org/abs/2508.11935v1",
        "abstract": "State Space Models (SSMs) are efficient alternatives to traditional sequence\nmodels, excelling at processing long sequences with lower computational\ncomplexity. Their reliance on matrix multiplications makes them ideal for\ncompute-in-memory (CIM) architectures, which improve energy efficiency by\ncomputing within memory arrays. However, device non-idealities in CIM introduce\nweight perturbations that can degrade inference accuracy. In this paper, we\nsystematically analyze the robustness of SSMs under noisy conditions,\nidentifying that the final block and output projection layers are more\nsusceptible to perturbations compared to other components. Building on these\ninsights, we propose HPD, a Hybrid Projection Decomposition strategy for the\nlast output projection layer. We replace the original weight matrix with the\nmultiplication of U and {\\Sigma} in its SVD to ensure compatibility with\nexisting hardware architectures, while offloading V> to digital hardware for\nprecise and robust correction. Comprehensive tests on Mamba models show that\nour method reduces perplexity by up to 99.57% under various noise conditions\ncompared to baseline models, with accuracy gains of up to 96.67% on the PIQA\nbenchmark for commonsense reasoning."
    },
    {
        "date": "2025-08",
        "title": "An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction",
        "author": "Tim van Erven, Jack Mayo, Julia Olkhovskaya, and Chen-Yu Wei",
        "link": "http://arxiv.org/abs/2508.11931v1",
        "abstract": "We present an efficient algorithm for linear contextual bandits with\nadversarial losses and stochastic action sets. Our approach reduces this\nsetting to misspecification-robust adversarial linear bandits with fixed action\nsets. Without knowledge of the context distribution or access to a context\nsimulator, the algorithm achieves $\\tilde{O}(\\min\\{d^2\\sqrt{T}, \\sqrt{d^3T\\log\nK}\\})$ regret and runs in $\\text{poly}(d,C,T)$ time, where $d$ is the feature\ndimension, $C$ is an upper bound on the number of linear constraints defining\nthe action set in each round, $K$ is an upper bound on the number of actions in\neach round, and $T$ is number of rounds. This resolves the open question by Liu\net al. (2023) on whether one can obtain $\\text{poly}(d)\\sqrt{T}$ regret in\npolynomial time independent of the number of actions. For the important class\nof combinatorial bandits with adversarial losses and stochastic action sets\nwhere the action sets can be described by a polynomial number of linear\nconstraints, our algorithm is the first to achieve $\\text{poly}(d)\\sqrt{T}$\nregret in polynomial time, while no prior algorithm achieves even $o(T)$ regret\nin polynomial time to our knowledge. When a simulator is available, the regret\nbound can be improved to $\\tilde{O}(d\\sqrt{L^\\star})$, where $L^\\star$ is the\ncumulative loss of the best policy."
    },
    {
        "date": "2025-08",
        "title": "Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning",
        "author": "Xiaojin Zhang, Mingcong Xu, Yiming Li, Wei Chen, and Qiang Yang",
        "link": "http://arxiv.org/abs/2508.11907v1",
        "abstract": "Federated learning (FL) offers a promising paradigm for collaborative model\ntraining while preserving data privacy. However, its susceptibility to gradient\ninversion attacks poses a significant challenge, necessitating robust privacy\nprotection mechanisms. This paper introduces a novel theoretical framework to\ndecipher the intricate interplay between attack and protection complexities in\nprivacy-preserving FL. We formally define \"Attack Complexity\" as the minimum\ncomputational and data resources an adversary requires to reconstruct private\ndata below a given error threshold, and \"Protection Complexity\" as the expected\ndistortion introduced by privacy mechanisms. Leveraging Maximum Bayesian\nPrivacy (MBP), we derive tight theoretical bounds for protection complexity,\ndemonstrating its scaling with model dimensionality and privacy budget.\nFurthermore, we establish comprehensive bounds for attack complexity, revealing\nits dependence on privacy leakage, gradient distortion, model dimension, and\nthe chosen privacy level. Our findings quantitatively illuminate the\nfundamental trade-offs between privacy guarantees, system utility, and the\neffort required for both attacking and defending. This framework provides\ncritical insights for designing more secure and efficient federated learning\nsystems."
    },
    {
        "date": "2025-08",
        "title": "ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages",
        "author": "Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haorang Wang, Matthew Lau, Wenke Lee, Wilian Lunardi, Martin Andreoni, and Polo Chau",
        "link": "http://arxiv.org/abs/2508.11854v1",
        "abstract": "As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks\nfor efficient novel-view synthesis from static images, how might an adversary\ntamper images to cause harm? We introduce ComplicitSplat, the first attack that\nexploits standard 3DGS shading methods to create viewpoint-specific camouflage\n- colors and textures that change with viewing angle - to embed adversarial\ncontent in scene objects that are visible only from specific viewpoints and\nwithout requiring access to model architecture or weights. Our extensive\nexperiments show that ComplicitSplat generalizes to successfully attack a\nvariety of popular detector - both single-stage, multi-stage, and\ntransformer-based models on both real-world capture of physical objects and\nsynthetic scenes. To our knowledge, this is the first black-box attack on\ndownstream object detectors using 3DGS, exposing a novel safety risk for\napplications like autonomous navigation and other mission-critical robotic\nsystems."
    },
    {
        "date": "2025-08",
        "title": "Adversarial Robustness in Distributed Quantum Machine Learning",
        "author": "Pouya Kananian, and Hans-Arno Jacobsen",
        "link": "http://arxiv.org/abs/2508.11848v1",
        "abstract": "Studying adversarial robustness of quantum machine learning (QML) models is\nessential in order to understand their potential advantages over classical\nmodels and build trustworthy systems. Distributing QML models allows leveraging\nmultiple quantum processors to overcome the limitations of individual devices\nand build scalable systems. However, this distribution can affect their\nadversarial robustness, potentially making them more vulnerable to new attacks.\nKey paradigms in distributed QML include federated learning, which, similar to\nclassical models, involves training a shared model on local data and sending\nonly the model updates, as well as circuit distribution methods inherent to\nquantum computing, such as circuit cutting and teleportation-based techniques.\nThese quantum-specific methods enable the distributed execution of quantum\ncircuits across multiple devices. This work reviews the differences between\nthese distribution methods, summarizes existing approaches on the adversarial\nrobustness of QML models when distributed using each paradigm, and discusses\nopen questions in this area."
    },
    {
        "date": "2025-08",
        "title": "Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering",
        "author": "Tyler Schroder, and Sohee Kim Park",
        "link": "http://arxiv.org/abs/2508.11812v1",
        "abstract": "The advancement of computing equipment and the advances in services over the\nInternet has allowed corporations, higher education, and many other\norganizations to pursue the shared computing network environment. A requirement\nfor shared computing environments is a centralized identity system to\nauthenticate and authorize user access. An organization's digital identity\nplane is a prime target for cyber threat actors. When compromised, identities\ncan be exploited to steal credentials, create unauthorized accounts, and\nmanipulate permissions-enabling attackers to gain control of the network and\nundermine its confidentiality, availability, and integrity. Cybercrime losses\nreached a record of 16.6 B in the United States in 2024. For organizations\nusing Microsoft software, Active Directory is the on-premises identity system\nof choice. In this article, we examine the challenge of security compromises in\nActive Directory (AD) environments and present effective strategies to prevent\ncredential theft and limit lateral movement by threat actors. Our proposed\napproaches aim to confine the movement of compromised credentials, preventing\nsignificant privilege escalation and theft. We argue that through our\nillustration of real-world scenarios, tiering can halt lateral movement and\nadvanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap\nin existing literature by combining technical guidelines with theoretical\narguments in support of tiering, positioning it as a vital component of modern\ncybersecurity strategy even though it cannot function in isolation. As the\nhardware advances and the cloud sourced services along with AI is advancing\nwith unprecedented speed, we think it is important for security experts and the\nbusiness to work together and start designing and developing software and\nframeworks to classify devices automatically and accurately within the tiered\nstructure."
    },
    {
        "date": "2025-08",
        "title": "Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach",
        "author": "Minhao Jin, Hongyu He, and Maria Apostolaki",
        "link": "http://arxiv.org/abs/2508.11742v1",
        "abstract": "Current synthetic traffic generators (SynNetGens) promise privacy but lack\ncomprehensive guarantees or empirical validation, even as their fidelity\nsteadily improves. We introduce the first attack-grounded benchmark for\nassessing the privacy of SynNetGens directly from the traffic they produce. We\nframe privacy as membership inference at the traffic-source level--a realistic\nand actionable threat for data holders. To this end, we present TraceBleed, the\nfirst attack that exploits behavioral fingerprints across flows using\ncontrastive learning and temporal chunking, outperforming prior membership\ninference baselines by 172%. Our large-scale study across GAN-, diffusion-, and\nGPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level\ninformation; (ii) differential privacy either fails to stop these attacks or\nseverely degrades fidelity; and (iii) sharing more synthetic data amplifies\nleakage by 59% on average. Finally, we introduce TracePatch, the first\nSynNetGen-agnostic defense that combines adversarial ML with SMT constraints to\nmitigate leakage while preserving fidelity."
    },
    {
        "date": "2025-08",
        "title": "Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks",
        "author": "Nathaniel Moyer, Charalampos Papamanthou, and Evgenios Kornaropoulos",
        "link": "http://arxiv.org/abs/2508.11563v1",
        "abstract": "Searchable encryption (SE) is the most scalable cryptographic primitive for\nsearching on encrypted data. Typical SE constructions often allow\naccess-pattern leakage, revealing which encrypted records are retrieved in the\nserver's responses. All the known generic cryptanalyses assume either that the\nqueries are issued uniformly at random or that the attacker observes the\nsearch-pattern leakage. It remains unclear what can be reconstructed when using\nonly the access-pattern leakage and knowledge of the query distribution. In\nthis work, we focus on the cryptanalytic technique of frequency analysis in the\ncontext of leakage-abuse attacks on schemes that support encrypted range\nqueries. Frequency analysis matches the frequency of retrieval of an encrypted\nrecord with a plaintext value based on its probability of retrieval that\nfollows from the knowledge of the query distribution. We generalize this\nunderexplored cryptanalytic technique and introduce a generic attack framework\ncalled Leakage-Abuse via Matching (LAMA) that works even on high-dimensional\nencrypted data. We identify a parameterization of LAMA that brings frequency\nanalysis to its limit -- that is, we prove that there is no additional\nfrequency matching that an attacker can perform to refine the result.\nFurthermore, we show that our results hold for any class of convex queries, and\nnot just axis-aligned rectangles, which is the assumption in all other attacks\non range schemes. Using these results, we identify query distributions that\nmake frequency analysis challenging for the attacker and, thus, can act as a\nmitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,\nfor the first time, plaintext data from encrypted range queries spanning up to\nfour dimensions."
    },
    {
        "date": "2025-08",
        "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning",
        "author": "Yang Wang, Yaxin Zhao, Xinyu Jiao, Sihan Xu, Xiangrui Cai, Ying Zhang, and Xiaojie Yuan",
        "link": "http://arxiv.org/abs/2508.11472v1",
        "abstract": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection."
    },
    {
        "date": "2025-08",
        "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization",
        "author": "Muhammad Zakwan, Liang Xu, and Giancarlo Ferrari-Trecate",
        "link": "http://arxiv.org/abs/2508.11432v1",
        "abstract": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks."
    },
    {
        "date": "2025-08",
        "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
        "author": "Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, and Oleg Somov",
        "link": "http://arxiv.org/abs/2508.11383v1",
        "abstract": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters."
    },
    {
        "date": "2025-08",
        "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models",
        "author": "Katarzyna Filus, and Jorge M. Cruz-Duarte",
        "link": "http://arxiv.org/abs/2508.11341v1",
        "abstract": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets."
    },
    {
        "date": "2025-08",
        "title": "Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks",
        "author": "Georgios Michail Makrakis, Jeroen Pijpker, Remco Hassing, Rob Loves, and Stephen McCombie",
        "link": "http://arxiv.org/abs/2508.11325v1",
        "abstract": "Cyber threats against the maritime industry have increased notably in recent\nyears, highlighting the need for innovative cybersecurity approaches. Ships, as\ncritical assets, possess highly specialized and interconnected network\ninfrastructures, where their legacy systems and operational constraints further\nexacerbate their vulnerability to cyberattacks. To better understand this\nevolving threat landscape, we propose the use of cyber-deception techniques and\nin particular honeynets, as a means to gather valuable insights into ongoing\nattack campaigns targeting the maritime sector.\n  In this paper we present Salty Seagull, a honeynet conceived to simulate a\nVSAT system for ships. This environment mimics the operations of a functional\nVSAT system onboard and, at the same time, enables a user to interact with it\nthrough a Web dashboard and a CLI environment. Furthermore, based on existing\nvulnerabilities, we purposefully integrate them into our system to increase\nattacker engagement. We exposed our honeynet for 30 days to the Internet to\nassess its capability and measured the received interaction. Results show that\nwhile numerous generic attacks have been attempted, only one curious attacker\nwith knowledge of the nature of the system and its vulnerabilities managed to\naccess it, without however exploring its full potential."
    },
    {
        "date": "2025-08",
        "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking",
        "author": "Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, and Zheng Yang",
        "link": "http://arxiv.org/abs/2508.11323v1",
        "abstract": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively."
    },
    {
        "date": "2025-08",
        "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble",
        "author": "Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, and Yi Zeng",
        "link": "http://arxiv.org/abs/2508.11279v1",
        "abstract": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models."
    },
    {
        "date": "2025-08",
        "title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories",
        "author": "William Alemanni, Arianna Burzacchi, Davide Colombi, and Elena Giarratano",
        "link": "http://arxiv.org/abs/2508.11235v1",
        "abstract": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios."
    },
    {
        "date": "2025-08",
        "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
        "author": "Abhinav Kumar, Yuliang Guo, Zhihao Zhang, Xinyu Huang, Liu Ren, and Xiaoming Liu",
        "link": "http://arxiv.org/abs/2508.11185v1",
        "abstract": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R"
    },
    {
        "date": "2025-08",
        "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME",
        "author": "Sam Chauhan, Estelle Duguet, Karthik Ramakrishnan, Hugh Van Deventer, Jack Kruger, and Ranjan Subbaraman",
        "link": "http://arxiv.org/abs/2508.11053v1",
        "abstract": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable\ninsights into black-box classifiers and are increasingly used to assess model\nbiases and generalizability. However, these methods are vulnerable to\nadversarial manipulation, potentially concealing harmful biases. Building on\nthe work of Slack et al. (2020), we investigate the susceptibility of LIME and\nSHAP to biased models and evaluate strategies for improving robustness. We\nfirst replicate the original COMPAS experiment to validate prior findings and\nestablish a baseline. We then introduce a modular testing framework enabling\nsystematic evaluation of augmented and ensemble explanation approaches across\nclassifiers of varying performance. Using this framework, we assess multiple\nLIME/SHAP ensemble configurations on out-of-distribution models, comparing\ntheir resistance to bias concealment against the original methods. Our results\nidentify configurations that substantially improve bias detection, highlighting\ntheir potential for enhancing transparency in the deployment of high-stakes\nmachine learning systems."
    },
    {
        "date": "2025-08",
        "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications",
        "author": "Wenpeng Xing, Zhonghao Qi, Yupeng Qin, Yilin Li, Caini Chang, Jiahui Yu, Changting Lin, Zhenzhen Xie, and Meng Han",
        "link": "http://arxiv.org/abs/2508.10991v2",
        "abstract": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems."
    },
    {
        "date": "2025-08",
        "title": "Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations",
        "author": "Benjamin Murphy, and Twm Stone",
        "link": "http://arxiv.org/abs/2508.15808v1",
        "abstract": "Advances in AI are widely understood to have implications for cybersecurity.\nArticles have emphasized the effect of AI on the cyber offense-defense balance,\nand commentators can be found arguing either that cyber will privilege\nattackers or defenders. For defenders, arguments are often made that AI will\nenable solutions like formal verification of all software--and for some\nwell-equipped companies, this may be true. This conversation, however, does not\nmatch the reality for most companies. \"Trailing-edge organizations,\" as we term\nthem, rely heavily on legacy software, poorly staff security roles, and\nstruggle to implement best practices like rapid deployment of security patches.\nThese decisions may be the result of corporate inertia, but may also be the\nresult of a seemingly-rational calculation that attackers may not bother\ntargeting a firm due to lack of economic incentives, and as a result,\nunderinvestment in defense will not be punished.\n  This approach to security may have been sufficient prior to the development\nof AI systems, but it is unlikely to remain viable in the near future. We argue\nthat continuing improvements in AI's capabilities poses additional risks on two\nfronts: First, increased usage of AI will alter the economics of the marginal\ncyberattack and expose these trailing-edge organizations to more attackers,\nmore frequently. Second, AI's advances will enable attackers to develop\nexploits and launch attacks earlier than they can today--meaning that it is\ninsufficient for these companies to attain parity with today's leading\ndefenders, but must instead aim for faster remediation timelines and more\nresilient software. The situation today portends a dramatically increased\nnumber of attacks in the near future. Moving forward, we offer a range of\nsolutions for both organizations and governments to improve the defensive\nposture of firms which lag behind their peers today."
    },
    {
        "date": "2025-08",
        "title": "Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios",
        "author": "Zhanwen Liu, Yujing Sun, Yang Wang, Nan Yang, Shengbo Eben Li, and Xiangmo Zhao",
        "link": "http://arxiv.org/abs/2508.10704v1",
        "abstract": "The dynamic range limitation of conventional RGB cameras reduces global\ncontrast and causes loss of high-frequency details such as textures and edges\nin complex traffic environments (e.g., nighttime driving, tunnels), hindering\ndiscriminative feature extraction and degrading frame-based object detection.\nTo address this, we integrate a bio-inspired event camera with an RGB camera to\nprovide high dynamic range information and propose a motion cue fusion network\n(MCFNet), which achieves optimal spatiotemporal alignment and adaptive\ncross-modal feature fusion under challenging lighting. Specifically, an event\ncorrection module (ECM) temporally aligns asynchronous event streams with image\nframes via optical-flow-based warping, jointly optimized with the detection\nnetwork to learn task-aware event representations. The event dynamic upsampling\nmodule (EDUM) enhances spatial resolution of event frames to match image\nstructures, ensuring precise spatiotemporal alignment. The cross-modal mamba\nfusion module (CMM) uses adaptive feature fusion with a novel interlaced\nscanning mechanism, effectively integrating complementary information for\nrobust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD\ndatasets demonstrate that MCFNet significantly outperforms existing methods in\nvarious poor lighting and fast moving traffic scenarios. Notably, on the\nDSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best\nexisting methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The\ncode is available at https://github.com/Charm11492/MCFNet."
    },
    {
        "date": "2025-08",
        "title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping",
        "author": "Busra Bulut, Maik Dannecker, Thomas Sanchez, Sara Neves Silva, Vladyslav Zalevskyi, Steven Jia, Jean-Baptiste Ledoux, Guillaume Auzias, Fran\u00e7ois Rousseau, Jana Hutter, Daniel Rueckert, and Meritxell Bach Cuadra",
        "link": "http://arxiv.org/abs/2508.10680v1",
        "abstract": "T2 mapping in fetal brain MRI has the potential to improve characterization\nof the developing brain, especially at mid-field (0.55T), where T2 decay is\nslower. However, this is challenging as fetal MRI acquisition relies on\nmultiple motion-corrupted stacks of thick slices, requiring slice-to-volume\nreconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,\nT2 mapping involves repeated acquisitions of these stacks at each echo time\n(TE), leading to long scan times and high sensitivity to motion. We tackle this\nchallenge with a method that jointly reconstructs data across TEs, addressing\nsevere motion. Our approach combines implicit neural representations with a\nphysics-informed regularization that models T2 decay, enabling information\nsharing across TEs while preserving anatomical and quantitative T2 fidelity. We\ndemonstrate state-of-the-art performance on simulated fetal brain and in vivo\nadult datasets with fetal-like motion. We also present the first in vivo fetal\nT2 mapping results at 0.55T. Our study shows potential for reducing the number\nof stacks per TE in T2 mapping by leveraging anatomical redundancy."
    },
    {
        "date": "2025-08",
        "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression",
        "author": "Mengjie Li, and William J. Song",
        "link": "http://arxiv.org/abs/2508.15806v1",
        "abstract": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations"
    },
    {
        "date": "2025-08",
        "title": "MirGuard: Towards a Robust Provenance-based Intrusion Detection System Against Graph Manipulation Attacks",
        "author": "Anyuan Sang, Lu Zhou, Li Yang, Junbo Jia, Huipeng Yang, Pengbin Feng, and Jianfeng Ma",
        "link": "http://arxiv.org/abs/2508.10639v1",
        "abstract": "Learning-based Provenance-based Intrusion Detection Systems (PIDSes) have\nbecome essential tools for anomaly detection in host systems due to their\nability to capture rich contextual and structural information, as well as their\npotential to detect unknown attacks. However, recent studies have shown that\nthese systems are vulnerable to graph manipulation attacks, where attackers\nmanipulate the graph structure to evade detection. While some previous\napproaches have discussed this type of attack, none have fully addressed it\nwith a robust detection solution, limiting the practical applicability of\nPIDSes.\n  To address this challenge, we propose MirGuard, a robust anomaly detection\nframework that combines logic-aware multi-view augmentation with contrastive\nrepresentation learning. Rather than applying arbitrary structural\nperturbations, MirGuard introduces Logic-Aware Noise Injection (LNI) to\ngenerate semantically valid graph views, ensuring that all augmentations\npreserve the underlying causal semantics of the provenance data. These views\nare then used in a Logic-Preserving Contrastive Learning framework, which\nencourages the model to learn representations that are invariant to benign\ntransformations but sensitive to adversarial inconsistencies. Comprehensive\nevaluations on multiple provenance datasets demonstrate that MirGuard\nsignificantly outperforms state-of-the-art detectors in robustness against\nvarious graph manipulation attacks without sacrificing detection performance\nand efficiency. Our work represents the first targeted study to enhance PIDS\nagainst such adversarial threats, providing a robust and effective solution to\nmodern cybersecurity challenges."
    },
    {
        "date": "2025-08",
        "title": "A Transformer-Based Approach for DDoS Attack Detection in IoT Networks",
        "author": "Sandipan Dey, Payal Santosh Kate, Vatsala Upadhyay, and Abhishek Vaish",
        "link": "http://arxiv.org/abs/2508.10636v1",
        "abstract": "DDoS attacks have become a major threat to the security of IoT devices and\ncan cause severe damage to the network infrastructure. IoT devices suffer from\nthe inherent problem of resource constraints and are therefore susceptible to\nsuch resource-exhausting attacks. Traditional methods for detecting DDoS\nattacks are not efficient enough to cope with the dynamic nature of IoT\nnetworks, as well as the scalability of the attacks, diversity of protocols,\nhigh volume of traffic, and variability in device behavior, and variability of\nprotocols like MQTT, CoAP, making it hard to implement security across all the\nprotocols. In this paper, we propose a novel approach, i.e., the use of\nTransformer models, which have shown remarkable performance in natural language\nprocessing tasks, for detecting DDoS attacks on IoT devices. The proposed model\nextracts features from network traffic data and processes them using a\nself-attention mechanism. Experiments conducted on a real-world dataset\ndemonstrate that the proposed approach outperforms traditional machine learning\ntechniques, which can be validated by comparing both approaches' accuracy,\nprecision, recall, and F1-score. The results of this study show that the\nTransformer models can be an effective solution for detecting DDoS attacks on\nIoT devices and have the potential to be deployed in real-world IoT\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving",
        "author": "Yuxin Cao, Yedi Zhang, Wentao He, Yifan Liao, Yan Xiao, Chang Li, Zhiyong Huang, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2508.10600v1",
        "abstract": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics."
    },
    {
        "date": "2025-08",
        "title": "Oops!... They Stole it Again: Attacks on Split Learning",
        "author": "Tanveer Khan, and Antonis Michalas",
        "link": "http://arxiv.org/abs/2508.10598v1",
        "abstract": "Split Learning (SL) is a collaborative learning approach that improves\nprivacy by keeping data on the client-side while sharing only the intermediate\noutput with a server. However, the distributed nature of SL introduces new\nsecurity challenges, necessitating a comprehensive exploration of potential\nattacks. This paper systematically reviews various attacks on SL, classifying\nthem based on factors such as the attacker's role, the type of privacy risks,\nwhen data leaks occur, and where vulnerabilities exist. We also analyze\nexisting defense methods, including cryptographic methods, data modification\napproaches, distributed techniques, and hybrid solutions. Our findings reveal\nsecurity gaps, highlighting the effectiveness and limitations of existing\ndefenses. By identifying open challenges and future directions, this work\nprovides valuable information to improve SL privacy issues and guide further\nresearch."
    },
    {
        "date": "2025-08",
        "title": "Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer",
        "author": "Xuanhao Mu, G\u00f6khan Demirel, Yuzhe Zhang, Jianlei Liu, Thorsten Schlachter, and Veit Hagenmeyer",
        "link": "http://arxiv.org/abs/2508.10587v1",
        "abstract": "To bridge the temporal granularity gap in energy network design and operation\nbased on Energy System Models, resampling of time series is required. While\nconventional upsampling methods are computationally efficient, they often\nresult in significant information loss or increased noise. Advanced models such\nas time series generation models, Super-Resolution models and imputation models\nshow potential, but also face fundamental challenges. The goal of time series\ngenerative models is to learn the distribution of the original data to generate\nhigh-resolution series with similar statistical characteristics. This is not\nentirely consistent with the definition of upsampling. Time series\nSuper-Resolution models or imputation models can degrade the accuracy of\nupsampling because the input low-resolution time series are sparse and may have\ninsufficient context. Moreover, such models usually rely on supervised learning\nparadigms. This presents a fundamental application paradox: their training\nrequires the high-resolution time series that is intrinsically absent in\nupsampling application scenarios. To address the mentioned upsampling issue,\nthis paper introduces a new method utilizing Generative Adversarial\nTransformers (GATs), which can be trained without access to any ground-truth\nhigh-resolution data. Compared with conventional interpolation methods, the\nintroduced method can reduce the root mean square error (RMSE) of upsampling\ntasks by 9%, and the accuracy of a model predictive control (MPC) application\nscenario is improved by 13%."
    },
    {
        "date": "2025-08",
        "title": "Contrastive ECOC: Learning Output Codes for Adversarial Defense",
        "author": "Che-Yu Chou, and Hung-Hsuan Chen",
        "link": "http://arxiv.org/abs/2508.10491v1",
        "abstract": "Although one-hot encoding is commonly used for multiclass classification, it\nis not always the most effective encoding mechanism. Error Correcting Output\nCodes (ECOC) address multiclass classification by mapping each class to a\nunique codeword used as a label. Traditional ECOC methods rely on manually\ndesigned or randomly generated codebooks, which are labor-intensive and may\nyield suboptimal, dataset-agnostic results. This paper introduces three models\nfor automated codebook learning based on contrastive learning, allowing\ncodebooks to be learned directly and adaptively from data. Across four\ndatasets, our proposed models demonstrate superior robustness to adversarial\nattacks compared to two baselines. The source is available at\nhttps://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique."
    },
    {
        "date": "2025-08",
        "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based Side-Channel Attacks on Fully Associative Randomized Caches",
        "author": "Chris Cao, and Gururaj Saileshwar",
        "link": "http://arxiv.org/abs/2508.10431v2",
        "abstract": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
    },
    {
        "date": "2025-08",
        "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation",
        "author": "Yan Ting Chok, Soyon Park, Seungheun Baek, Hajung Kim, Junhyun Lee, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2508.10425v1",
        "abstract": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings."
    },
    {
        "date": "2025-08",
        "title": "Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks",
        "author": "Irash Perera, Hiranya Abeyrathne, Sanjeewa Malalgoda, and Arshardh Ifthikar",
        "link": "http://arxiv.org/abs/2508.11711v1",
        "abstract": "GraphQL's flexibility, while beneficial for efficient data fetching,\nintroduces unique security vulnerabilities that traditional API security\nmechanisms often fail to address. Malicious GraphQL queries can exploit the\nlanguage's dynamic nature, leading to denial-of-service attacks, data\nexfiltration through injection, and other exploits. Existing solutions, such as\nstatic analysis, rate limiting, and general-purpose Web Application Firewalls,\noffer limited protection against sophisticated, context-aware attacks. This\npaper presents a novel, AI-driven approach for real-time detection of malicious\nGraphQL queries. Our method combines static analysis with machine learning\ntechniques, including Large Language Models (LLMs) for dynamic schema-based\nconfiguration, Sentence Transformers (SBERT and Doc2Vec) for contextual\nembedding of query payloads, and Convolutional Neural Networks (CNNs), Random\nForests, and Multilayer Perceptrons for classification. We detail the system\narchitecture, implementation strategies optimized for production environments\n(including ONNX Runtime optimization and parallel processing), and evaluate the\nperformance of our detection models and the overall system under load. Results\ndemonstrate high accuracy in detecting various threats, including SQL\ninjection, OS command injection, and XSS exploits, alongside effective\nmitigation of DoS and SSRF attempts. This research contributes a robust and\nadaptable solution for enhancing GraphQL API security."
    },
    {
        "date": "2025-08",
        "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation",
        "author": "Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, and Zhuo Li",
        "link": "http://arxiv.org/abs/2508.10404v1",
        "abstract": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated."
    },
    {
        "date": "2025-08",
        "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise",
        "author": "Yechan Kim, Dongho Yoon, Younkwan Lee, Unse Fatima, Hong Kook Kim, Songjae Lee, Sanga Park, Jeong Ho Park, Seonjong Kang, and Moongu Jeon",
        "link": "http://arxiv.org/abs/2508.10383v2",
        "abstract": "While previous studies on image segmentation focus on handling severe (or\nexplicit) label noise, real-world datasets also exhibit subtle (or implicit)\nlabel imperfections. These arise from inherent challenges, such as ambiguous\nobject boundaries and annotator variability. Although not explicitly present,\nsuch mild and latent noise can still impair model performance. Typical data\naugmentation methods, which apply identical transformations to the image and\nits label, risk amplifying these subtle imperfections and limiting the model's\ngeneralization capacity. In this paper, we introduce NSegment+, a novel\naugmentation framework that decouples image and label transformations to\naddress such realistic noise for semantic segmentation. By introducing\ncontrolled elastic deformations only to segmentation labels while preserving\nthe original images, our method encourages models to focus on learning robust\nrepresentations of object structures despite minor label inconsistencies.\nExtensive experiments demonstrate that NSegment+ consistently improves\nperformance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in\naverage on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even\nwithout bells and whistles, highlighting the importance of addressing implicit\nlabel noise. These gains can be further amplified when combined with other\ntraining tricks, including CutMix and Label Smoothing."
    },
    {
        "date": "2025-08",
        "title": "A Hierarchical IDS for Zero-Day Attack Detection in Internet of Medical Things Networks",
        "author": "Md Ashraf Uddin, Nam H. Chu, and Reza Rafeh",
        "link": "http://arxiv.org/abs/2508.10346v1",
        "abstract": "The Internet of Medical Things (IoMT) is driving a healthcare revolution but\nremains vulnerable to cyberattacks such as denial of service, ransomware, data\nhijacking, and spoofing. These networks comprise resource constrained,\nheterogeneous devices (e.g., wearable sensors, smart pills, implantables),\nmaking traditional centralized Intrusion Detection Systems (IDSs) unsuitable\ndue to response delays, privacy risks, and added vulnerabilities. Centralized\nIDSs require all sensors to transmit data to a central server, causing delays\nor network disruptions in dense environments. Running IDSs locally on IoMT\ndevices is often infeasible due to limited computation, and even lightweight\nIDS components remain at risk if updated models are delayed leaving them\nexposed to zero-day attacks that threaten patient health and data security. We\npropose a multi level IoMT IDS framework capable of detecting zero day attacks\nand distinguishing between known and unknown threats. The first layer (near\nEdge) filters traffic at a coarse level (attack or not) using meta-learning or\nOne Class Classification (OCC) with the usfAD algorithm. Subsequent layers (far\nEdge, Cloud) identify attack type and novelty. Experiments on the CICIoMT2024\ndataset show 99.77 percentage accuracy and 97.8 percentage F1-score. The first\nlayer detects zero-day attacks with high accuracy without needing new datasets,\nensuring strong applicability in IoMT environments. Additionally, the\nmeta-learning approach achieves high."
    },
    {
        "date": "2025-08",
        "title": "A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning",
        "author": "Keke Gai, Dongjue Wang, Jing Yu, Liehuang Zhu, and Qi Wu",
        "link": "http://arxiv.org/abs/2508.10315v1",
        "abstract": "Existing backdoor defense methods in Federated Learning (FL) rely on the\nassumption of homogeneous client data distributions or the availability of a\nclean serve dataset, which limits the practicality and effectiveness. Defending\nagainst backdoor attacks under heterogeneous client data distributions while\npreserving model performance remains a significant challenge. In this paper, we\npropose a FL backdoor defense framework named CLIP-Fed, which leverages the\nzero-shot learning capabilities of vision-language pre-training models. By\nintegrating both pre-aggregation and post-aggregation defense strategies,\nCLIP-Fed overcomes the limitations of Non-IID imposed on defense effectiveness.\nTo address privacy concerns and enhance the coverage of the dataset against\ndiverse triggers, we construct and augment the server dataset using the\nmultimodal large language model and frequency analysis without any client\nsamples. To address class prototype deviations caused by backdoor samples and\neliminate the correlation between trigger patterns and target labels, CLIP-Fed\naligns the knowledge of the global model and CLIP on the augmented dataset\nusing prototype contrastive loss and Kullback-Leibler divergence. Extensive\nexperiments on representative datasets validate the effectiveness of CLIP-Fed.\nCompared to state-of-the-art methods, CLIP-Fed achieves an average reduction in\nASR, i.e., 2.03\\% on CIFAR-10 and 1.35\\% on CIFAR-10-LT, while improving\naverage MA by 7.92\\% and 0.48\\%, respectively."
    },
    {
        "date": "2025-08",
        "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones",
        "author": "Yujie Zhao, Jiabei Zeng, and Shiguang Shan",
        "link": "http://arxiv.org/abs/2508.10268v1",
        "abstract": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page."
    },
    {
        "date": "2025-08",
        "title": "DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy",
        "author": "Soorena Salari, Catherine Spino, Laurie-Anne Pharand, Fabienne Lathuiliere, Hassan Rivaz, Silvain Beriault, and Yiming Xiao",
        "link": "http://arxiv.org/abs/2508.10260v1",
        "abstract": "Accurate tissue motion tracking is critical to ensure treatment outcome and\nsafety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by\nregistration of sequential images, but existing methods often face challenges\nwith large misalignments and lack of interpretability. In this paper, we\nintroduce DINOMotion, a novel deep learning framework based on DINOv2 with\nLow-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable\nmotion tracking. DINOMotion automatically detects corresponding landmarks to\nderive optimal image registration, enhancing interpretability by providing\nexplicit visual correspondences between sequential images. The integration of\nLoRA layers reduces trainable parameters, improving training efficiency, while\nDINOv2's powerful feature representations offer robustness against large\nmisalignments. Unlike iterative optimization-based methods, DINOMotion directly\ncomputes image registration at test time. Our experiments on volunteer and\npatient datasets demonstrate its effectiveness in estimating both linear and\nnonlinear transformations, achieving Dice scores of 92.07% for the kidney,\n90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff\ndistances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes\neach scan in approximately 30ms and consistently outperforms state-of-the-art\nmethods, particularly in handling large misalignments. These results highlight\nits potential as a robust and interpretable solution for real-time motion\ntracking in 2D-Cine MRI-guided radiotherapy."
    },
    {
        "date": "2025-08",
        "title": "Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on Transformer Models",
        "author": "Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, and Xiangwei Zhou",
        "link": "http://arxiv.org/abs/2508.10243v1",
        "abstract": "Transformer models have demonstrated exceptional performance and have become\nindispensable in computer vision (CV) and natural language processing (NLP)\ntasks. However, recent studies reveal that transformers are susceptible to\nbackdoor attacks. Prior backdoor attack methods typically rely on retraining\nwith clean data or altering the model architecture, both of which can be\nresource-intensive and intrusive. In this paper, we propose Head-wise Pruning\nand Malicious Injection (HPMI), a novel retraining-free backdoor attack on\ntransformers that does not alter the model's architecture. Our approach\nrequires only a small subset of the original data and basic knowledge of the\nmodel architecture, eliminating the need for retraining the target transformer.\nTechnically, HPMI works by pruning the least important head and injecting a\npre-trained malicious head to establish the backdoor. We provide a rigorous\ntheoretical justification demonstrating that the implanted backdoor resists\ndetection and removal by state-of-the-art defense techniques, under reasonable\nassumptions. Experimental evaluations across multiple datasets further validate\nthe effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy\nloss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four\nadvanced defense mechanisms. Additionally, relative to state-of-the-art\nretraining-dependent attacks, HPMI achieves greater concealment and robustness\nagainst diverse defense strategies, while maintaining minimal impact on clean\naccuracy."
    },
    {
        "date": "2025-08",
        "title": "Detecting Untargeted Attacks and Mitigating Unreliable Updates in Federated Learning for Underground Mining Operations",
        "author": "Md Sazedur Rahman, Mohamed Elmahallawy, Sanjay Madria, and Samuel Frimpong",
        "link": "http://arxiv.org/abs/2508.10212v1",
        "abstract": "Underground mining operations rely on distributed sensor networks to collect\ncritical data daily, including mine temperature, toxic gas concentrations, and\nminer movements for hazard detection and operational decision-making. However,\ntransmitting raw sensor data to a central server for training deep learning\nmodels introduces significant privacy risks, potentially exposing sensitive\nmine-specific information. Federated Learning (FL) offers a transformative\nsolution by enabling collaborative model training while ensuring that raw data\nremains localized at each mine. Despite its advantages, FL in underground\nmining faces key challenges: (i) An attacker may compromise a mine's local\nmodel by employing techniques such as sign-flipping attacks or additive noise,\nleading to erroneous predictions; (ii) Low-quality (yet potentially valuable)\ndata, caused by poor lighting conditions or sensor inaccuracies in mines may\ndegrade the FL training process. In response, this paper proposes MineDetect, a\ndefense FL framework that detects and isolates the attacked models while\nmitigating the impact of mines with low-quality data. MineDetect introduces two\nkey innovations: (i) Detecting attacked models (maliciously manipulated) by\ndeveloping a history-aware mechanism that leverages local and global averages\nof gradient updates; (ii) Identifying and eliminating adversarial influences\nfrom unreliable models (generated by clients with poor data quality) on the FL\ntraining process. Comprehensive simulations across diverse datasets demonstrate\nthat MineDetect outperforms existing methods in both robustness and accuracy,\neven in challenging non-IID data scenarios. Its ability to counter adversarial\ninfluences while maintaining lower computational efficiency makes it a vital\nadvancement for improving safety and operational effectiveness in underground\nmining."
    },
    {
        "date": "2025-08",
        "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model",
        "author": "Sushrut Patwardhan, Raghavendra Ramachandra, and Sushma Venkatesh",
        "link": "http://arxiv.org/abs/2508.10110v1",
        "abstract": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums."
    },
    {
        "date": "2025-08",
        "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development",
        "author": "Sattvik Sahai, Prasoon Goyal, Michael Johnston, Anna Gottardi, Yao Lu, Lucy Hu, Luke Dai, Shaohua Liu, Samyuth Sagi, Hangjie Shi, Desheng Zhang, Lavina Vaz, Leslie Ball, Maureen Murray, Rahul Gupta, and Shankar Ananthakrishna",
        "link": "http://arxiv.org/abs/2508.10108v1",
        "abstract": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety."
    },
    {
        "date": "2025-08",
        "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training",
        "author": "Wonho Lee, Hyunsik Na, Jisu Lee, and Daeseon Choi",
        "link": "http://arxiv.org/abs/2508.10946v1",
        "abstract": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments."
    },
    {
        "date": "2025-08",
        "title": "Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving by AWorld",
        "author": "Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, and Jinjie Gu",
        "link": "http://arxiv.org/abs/2508.09889v2",
        "abstract": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems."
    },
    {
        "date": "2025-08",
        "title": "On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators",
        "author": "Jasmin Frkatovic, Akash Malemath, Ivan Kankeu, Yannick Werner, Matthias Tsch\u00f6pe, Vitor Fortes Rey, Sungho Suh, Paul Lukowicz, Nikolaos Palaiodimopoulos, and Maximilian Kiefer-Emmanouilidis",
        "link": "http://arxiv.org/abs/2508.09844v1",
        "abstract": "We investigate the capabilities of Quantum Generative Adversarial Networks\n(QGANs) in image generations tasks. Our analysis centers on fully quantum\nimplementations of both the generator and discriminator. Through extensive\nnumerical testing of current main architectures, we find that QGANs struggle to\ngeneralize across datasets, converging on merely the average representation of\nthe training data. When the output of the generator is a pure-state, we\nanalytically derive a lower bound for the discriminator quality given by the\nfidelity between the pure-state output of the generator and the target data\ndistribution, thereby providing a theoretical explanation for the limitations\nobserved in current models. Our findings reveal fundamental challenges in the\ngeneralization capabilities of existing quantum generative models. While our\nanalysis focuses on QGANs, the results carry broader implications for the\nperformance of related quantum generative models."
    },
    {
        "date": "2025-08",
        "title": "Robustness analysis of Deep Sky Objects detection models on HPC",
        "author": "Olivier Parisot, and Diogo Ramalho Fernandes",
        "link": "http://arxiv.org/abs/2508.09831v1",
        "abstract": "Astronomical surveys and the growing involvement of amateur astronomers are\nproducing more sky images than ever before, and this calls for automated\nprocessing methods that are accurate and robust. Detecting Deep Sky Objects --\nsuch as galaxies, nebulae, and star clusters -- remains challenging because of\ntheir faint signals and complex backgrounds. Advances in Computer Vision and\nDeep Learning now make it possible to improve and automate this process. In\nthis paper, we present the training and comparison of different detection\nmodels (YOLO, RET-DETR) on smart telescope images, using High-Performance\nComputing (HPC) to parallelise computations, in particular for robustness\ntesting."
    },
    {
        "date": "2025-08",
        "title": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research",
        "author": "Klaudia Krawiecka, and Christian Schroeder de Witt",
        "link": "http://arxiv.org/abs/2508.09815v1",
        "abstract": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments."
    },
    {
        "date": "2025-08",
        "title": "Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning",
        "author": "Carlos Franzreb, Arnab Das, Tim Polzehl, and Sebastian M\u00f6ller",
        "link": "http://arxiv.org/abs/2508.09803v1",
        "abstract": "The current privacy evaluation for speaker anonymization often overestimates\nprivacy when a same-gender target selection algorithm (TSA) is used, although\nthis TSA leaks the speaker's gender and should hence be more vulnerable. We\nhypothesize that this occurs because the evaluation does not account for the\nfact that anonymized speech contains information from both the source and\ntarget speakers. To address this, we propose to add a target classifier that\nmeasures the influence of target speaker information in the evaluation, which\ncan also be removed with adversarial learning. Experiments demonstrate that\nthis approach is effective for multiple anonymizers, particularly when using a\nsame-gender TSA, leading to a more reliable assessment."
    },
    {
        "date": "2025-08",
        "title": "Perfect message authentication codes are robust to small deviations from uniform key distributions",
        "author": "Boris Ryabko",
        "link": "http://arxiv.org/abs/2508.09783v1",
        "abstract": "We investigate the impact of (possible) deviations of the probability\ndistribution of key values from a uniform distribution for the\ninformation-theoretic strong, or perfect, message authentication code. We found\na simple expression for the decrease in security as a function of the\nstatistical distance between the real key probability distribution and the\nuniform one. In a sense, a perfect message authentication code is robust to\nsmall deviations from a uniform key distribution."
    },
    {
        "date": "2025-08",
        "title": "Succinct Oblivious Tensor Evaluation and Applications: Adaptively-Secure Laconic Function Evaluation and Trapdoor Hashing for All Circuits",
        "author": "Damiano Abram, Giulio Malavolta, and Lawrence Roy",
        "link": "http://arxiv.org/abs/2508.09673v1",
        "abstract": "We propose the notion of succinct oblivious tensor evaluation (OTE), where\ntwo parties compute an additive secret sharing of a tensor product of two\nvectors $\\mathbf{x} \\otimes \\mathbf{y}$, exchanging two simultaneous messages.\nCrucially, the size of both messages and of the CRS is independent of the\ndimension of $\\mathbf{x}$.\n  We present a construction of OTE with optimal complexity from the standard\nlearning with errors (LWE) problem. Then we show how this new technical tool\nenables a host of cryptographic primitives, all with security reducible to LWE,\nsuch as:\n  * Adaptively secure laconic function evaluation for depth-$D$ functions\n$f:\\{0, 1\\}^m\\rightarrow\\{0, 1\\}^\\ell$ with communication $m+\\ell+D\\cdot\n\\mathrm{poly}(\\lambda)$.\n  * A trapdoor hash function for all functions.\n  * An (optimally) succinct homomorphic secret sharing for all functions.\n  * A rate-$1/2$ laconic oblivious transfer for batch messages, which is best\npossible.\n  In particular, we obtain the first laconic function evaluation scheme that is\nadaptively secure from the standard LWE assumption, improving upon Quach, Wee,\nand Wichs (FOCS 2018).\n  As a key technical ingredient, we introduce a new notion of \\emph{adaptive\nlattice encodings}, which may be of independent interest."
    },
    {
        "date": "2025-08",
        "title": "Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging",
        "author": "Lianfang Wang, Kuilin Qin, Xueying Liu, Huibin Chang, Yong Wang, and Yuping Duan",
        "link": "http://arxiv.org/abs/2508.09655v1",
        "abstract": "Computational imaging, especially non-line-of-sight (NLOS) imaging, the\nextraction of information from obscured or hidden scenes is achieved through\nthe utilization of indirect light signals resulting from multiple reflections\nor scattering. The inherently weak nature of these signals, coupled with their\nsusceptibility to noise, necessitates the integration of physical processes to\nensure accurate reconstruction. This paper presents a parameterized inverse\nproblem framework tailored for large-scale linear problems in 3D imaging\nreconstruction. Initially, a noise estimation module is employed to adaptively\nassess the noise levels present in transient data. Subsequently, a\nparameterized neural operator is developed to approximate the inverse mapping,\nfacilitating end-to-end rapid image reconstruction. Our 3D image reconstruction\nframework, grounded in operator learning, is constructed through deep algorithm\nunfolding, which not only provides commendable model interpretability but also\nenables dynamic adaptation to varying noise levels in the acquired data,\nthereby ensuring consistently robust and accurate reconstruction outcomes.\nFurthermore, we introduce a novel method for the fusion of global and local\nspatiotemporal data features. By integrating structural and detailed\ninformation, this method significantly enhances both accuracy and robustness.\nComprehensive numerical experiments conducted on both simulated and real\ndatasets substantiate the efficacy of the proposed method. It demonstrates\nremarkable performance with fast scanning data and sparse illumination point\ndata, offering a viable solution for NLOS imaging in complex scenarios."
    },
    {
        "date": "2025-08",
        "title": "TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos",
        "author": "Hao Xu, Arbind Agrahari Baniya, Sam Wells, Mohamed Reda Bouadjenek, Richard Dazely, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2508.09650v1",
        "abstract": "Robust ball tracking under occlusion remains a key challenge in sports video\nanalysis, affecting tasks like event detection and officiating. We present\nTOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,\nvisibility-weighted loss, and occlusion augmentation to improve performance\nunder partial and full occlusions. Developed in collaboration with Paralympics\nAustralia, TOTNet is designed for real-world sports analytics. We introduce\nTTA, a new occlusion-rich table tennis dataset collected from\nprofessional-level Paralympic matches, comprising 9,159 samples with 1,996\nocclusion cases. Evaluated on four datasets across tennis, badminton, and table\ntennis, TOTNet significantly outperforms prior state-of-the-art methods,\nreducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded\nframes from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for\noffline sports analytics in fast-paced scenarios. Code and data\naccess:\\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}."
    },
    {
        "date": "2025-08",
        "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion",
        "author": "Jiwon Kim, Pureum Kim, SeonHwa Kim, Soobin Park, Eunju Cha, and Kyong Hwan Jin",
        "link": "http://arxiv.org/abs/2508.09575v1",
        "abstract": "Recent advancements in controllable text-to-image (T2I) diffusion models,\nsuch as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance\ncontrol without requiring auxiliary module training. However, these models\noften struggle to accurately preserve spatial structures and fail to capture\nfine-grained conditions related to object poses and scene layouts. To address\nthese challenges, we propose a training-free Dual Recursive Feedback (DRF)\nsystem that properly reflects control conditions in controllable T2I models.\nThe proposed DRF consists of appearance feedback and generation feedback that\nrecursively refines the intermediate latents to better reflect the given\nappearance information and the user's intent. This dual-update mechanism guides\nlatent representations toward reliable manifolds, effectively integrating\nstructural and appearance attributes. Our approach enables fine-grained\ngeneration even between class-invariant structure-appearance fusion, such as\ntransferring human motion onto a tiger's form. Extensive experiments\ndemonstrate the efficacy of our method in producing high-quality, semantically\ncoherent, and structurally consistent image generations. Our source code is\navailable at https://github.com/jwonkm/DRF."
    },
    {
        "date": "2025-08",
        "title": "Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems",
        "author": "Arun Vignesh Malarkkan, Haoyue Bai, Dongjie Wang, and Yanjie Fu",
        "link": "http://arxiv.org/abs/2508.09504v1",
        "abstract": "With the growing complexity of cyberattacks targeting critical\ninfrastructures such as water treatment networks, there is a pressing need for\nrobust anomaly detection strategies that account for both system\nvulnerabilities and evolving attack patterns. Traditional methods --\nstatistical, density-based, and graph-based models struggle with distribution\nshifts and class imbalance in multivariate time series, often leading to high\nfalse positive rates. To address these challenges, we propose CGAD, a Causal\nGraph-based Anomaly Detection framework designed for reliable cyberattack\ndetection in public infrastructure systems. CGAD follows a two-phase supervised\nframework -- causal profiling and anomaly scoring. First, it learns causal\ninvariant graph structures representing the system's behavior under \"Normal\"\nand \"Attack\" states using Dynamic Bayesian Networks. Second, it employs\nstructural divergence to detect anomalies via causal graph comparison by\nevaluating topological deviations in causal graphs over time. By leveraging\ncausal structures, CGAD achieves superior adaptability and accuracy in\nnon-stationary and imbalanced time series environments compared to conventional\nmachine learning approaches. By uncovering causal structures beneath volatile\nsensor data, our framework not only detects cyberattacks with markedly higher\nprecision but also redefines robustness in anomaly detection, proving\nresilience where traditional models falter under imbalance and drift. Our\nframework achieves substantial gains in F1 and ROC-AUC scores over\nbest-performing baselines across four industrial datasets, demonstrating robust\ndetection of delayed and structurally complex anomalies."
    },
    {
        "date": "2025-08",
        "title": "Event-driven Robust Fitting on Neuromorphic Hardware",
        "author": "Tam Ngoc-Bang Nguyen, Anh-Dzung Doan, Zhipeng Cai, and Tat-Jun Chin",
        "link": "http://arxiv.org/abs/2508.09466v1",
        "abstract": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy."
    },
    {
        "date": "2025-08",
        "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
        "author": "Junxian Li, Beining Xu, and Di Zhang",
        "link": "http://arxiv.org/abs/2508.09456v1",
        "abstract": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack."
    },
    {
        "date": "2025-08",
        "title": "Security Analysis of ChatGPT: Threats and Privacy Risks",
        "author": "Yushan Xiang, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2508.09426v1",
        "abstract": "As artificial intelligence technology continues to advance, chatbots are\nbecoming increasingly powerful. Among them, ChatGPT, launched by OpenAI, has\ngarnered widespread attention globally due to its powerful natural language\nprocessing capabilities based on the GPT model, which enables it to engage in\nnatural conversations with users, understand various forms of linguistic\nexpressions, and generate useful information and suggestions. However, as its\napplication scope expands, user demand grows, and malicious attacks related to\nit become increasingly frequent, the security threats and privacy risks faced\nby ChatGPT are gradually coming to the forefront. In this paper, the security\nof ChatGPT is mainly studied from two aspects, security threats and privacy\nrisks. The article systematically analyzes various types of vulnerabilities\ninvolved in the above two types of problems and their causes. Briefly, we\ndiscuss the controversies that ChatGPT may cause at the ethical and moral\nlevels. In addition, this paper reproduces several network attack and defense\ntest scenarios by simulating the attacker's perspective and methodology.\nSimultaneously, it explores the feasibility of using ChatGPT for security\nvulnerability detection and security tool generation from the defender's\nperspective."
    },
    {
        "date": "2025-08",
        "title": "A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy",
        "author": "Maxime Heuillet, Rishika Bhagwatkar, Jonas Ngnaw\u00e9, Yann Pequignot, Alexandre Larouche, Christian Gagn\u00e9, Irina Rish, Ola Ahmad, and Audrey Durand",
        "link": "http://arxiv.org/abs/2508.14079v1",
        "abstract": "Deep learning models operating in the image domain are vulnerable to small\ninput perturbations. For years, robustness to such perturbations was pursued by\ntraining models from scratch (i.e., with random initializations) using\nspecialized loss objectives. Recently, robust fine-tuning has emerged as a more\nefficient alternative: instead of training from scratch, pretrained models are\nadapted to maximize predictive performance and robustness. To conduct robust\nfine-tuning, practitioners design an optimization strategy that includes the\nmodel update protocol (e.g., full or partial) and the specialized loss\nobjective. Additional design choices include the architecture type and size,\nand the pretrained representation. These design choices affect robust\ngeneralization, which is the model's ability to maintain performance when\nexposed to new and unseen perturbations at test time. Understanding how these\ndesign choices influence generalization remains an open question with\nsignificant practical implications. In response, we present an empirical study\nspanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3\nadaptation protocols, yielding 1,440 training configurations and 7,200\nrobustness measurements across five perturbation types. To our knowledge, this\nis the most diverse and comprehensive benchmark of robust fine-tuning to date.\nWhile attention-based architectures and robust pretrained representations are\nincreasingly popular, we find that convolutional neural networks pretrained in\na supervised manner on large datasets often perform best. Our analysis both\nconfirms and challenges prior design assumptions, highlighting promising\nresearch directions and offering practical guidance."
    },
    {
        "date": "2025-08",
        "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs",
        "author": "Aayush Gupta",
        "link": "http://arxiv.org/abs/2508.09288v2",
        "abstract": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research."
    },
    {
        "date": "2025-08",
        "title": "Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning",
        "author": "Amine Andam, Jamal Bentahar, and Mustapha Hedabou",
        "link": "http://arxiv.org/abs/2508.09275v1",
        "abstract": "Collaborative multi-agent reinforcement learning (c-MARL) has rapidly\nevolved, offering state-of-the-art algorithms for real-world applications,\nincluding sensitive domains. However, a key challenge to its widespread\nadoption is the lack of a thorough investigation into its vulnerabilities to\nadversarial attacks. Existing work predominantly focuses on training-time\nattacks or unrealistic scenarios, such as access to policy weights or the\nability to train surrogate policies. In this paper, we investigate new\nvulnerabilities under more realistic and constrained conditions, assuming an\nadversary can only collect and perturb the observations of deployed agents. We\nalso consider scenarios where the adversary has no access at all. We propose\nsimple yet highly effective algorithms for generating adversarial perturbations\ndesigned to misalign how victim agents perceive their environment. Our approach\nis empirically validated on three benchmarks and 22 environments, demonstrating\nits effectiveness across diverse algorithms and environments. Furthermore, we\nshow that our algorithm is sample-efficient, requiring only 1,000 samples\ncompared to the millions needed by previous methods."
    },
    {
        "date": "2025-08",
        "title": "Deep Learning Models for Robust Facial Liveness Detection",
        "author": "Oleksandr Kuznetsov, Emanuele Frontoni, Luca Romeo, Riccardo Rosati, Andrea Maranesi, and Alessandro Muscatello",
        "link": "http://arxiv.org/abs/2508.09094v1",
        "abstract": "In the rapidly evolving landscape of digital security, biometric\nauthentication systems, particularly facial recognition, have emerged as\nintegral components of various security protocols. However, the reliability of\nthese systems is compromised by sophisticated spoofing attacks, where imposters\ngain unauthorized access by falsifying biometric traits. Current literature\nreveals a concerning gap: existing liveness detection methodologies - designed\nto counteract these breaches - fall short against advanced spoofing tactics\nemploying deepfakes and other artificial intelligence-driven manipulations.\nThis study introduces a robust solution through novel deep learning models\naddressing the deficiencies in contemporary anti-spoofing techniques. By\ninnovatively integrating texture analysis and reflective properties associated\nwith genuine human traits, our models distinguish authentic presence from\nreplicas with remarkable precision. Extensive evaluations were conducted across\nfive diverse datasets, encompassing a wide range of attack vectors and\nenvironmental conditions. Results demonstrate substantial advancement over\nexisting systems, with our best model (AttackNet V2.2) achieving 99.9% average\naccuracy when trained on combined data. Moreover, our research unveils critical\ninsights into the behavioral patterns of impostor attacks, contributing to a\nmore nuanced understanding of their evolving nature. The implications are\nprofound: our models do not merely fortify the authentication processes but\nalso instill confidence in biometric systems across various sectors reliant on\nsecure access."
    },
    {
        "date": "2025-08",
        "title": "NetMoniAI: An Agentic AI Framework for Network Security & Monitoring",
        "author": "Pallavi Zambare, Venkata Nikhil Thanikella, Nikhil Padmanabh Kottur, Sree Akhil Akula, and Ying Liu",
        "link": "http://arxiv.org/abs/2508.10052v1",
        "abstract": "In this paper, we present NetMoniAI, an agentic AI framework for automatic\nnetwork monitoring and security that integrates decentralized analysis with\nlightweight centralized coordination. The framework consists of two layers:\nautonomous micro-agents at each node perform local traffic analysis and anomaly\ndetection. A central controller then aggregates insights across nodes to detect\ncoordinated attacks and maintain system-wide situational awareness. We\nevaluated NetMoniAI on a local micro-testbed and through NS-3 simulations.\nResults confirm that the two-tier agentic-AI design scales under resource\nconstraints, reduces redundancy, and improves response time without\ncompromising accuracy. To facilitate broader adoption and reproducibility, the\ncomplete framework is available as open source. This enables researchers and\npractitioners to replicate, validate, and extend it across diverse network\nenvironments and threat scenarios. Github link:\nhttps://github.com/pzambare3/NetMoniAI"
    },
    {
        "date": "2025-08",
        "title": "Attacks and Defenses Against LLM Fingerprinting",
        "author": "Kevin Kurian, Ethan Holland, and Sean Oesch",
        "link": "http://arxiv.org/abs/2508.09021v1",
        "abstract": "As large language models are increasingly deployed in sensitive environments,\nfingerprinting attacks pose significant privacy and security risks. We present\na study of LLM fingerprinting from both offensive and defensive perspectives.\nOur attack methodology uses reinforcement learning to automatically optimize\nquery selection, achieving better fingerprinting accuracy with only 3 queries\ncompared to randomly selecting 3 queries from the same pool. Our defensive\napproach employs semantic-preserving output filtering through a secondary LLM\nto obfuscate model identity while maintaining semantic integrity. The defensive\nmethod reduces fingerprinting accuracy across tested models while preserving\noutput quality. These contributions show the potential to improve\nfingerprinting tools capabilities while providing practical mitigation\nstrategies against fingerprinting attacks."
    },
    {
        "date": "2025-08",
        "title": "Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss",
        "author": "Naifu Feng, Lixing Chen, Junhua Tang, Hua Ding, Jianhua Li, and Yang Bai",
        "link": "http://arxiv.org/abs/2508.08955v1",
        "abstract": "Transformer-based models have made significant progress in time series\nforecasting. However, a key limitation of deep learning models is their\nsusceptibility to adversarial attacks, which has not been studied enough in the\ncontext of time series prediction. In contrast to areas such as computer\nvision, where adversarial robustness has been extensively studied, frequency\ndomain features of time series data play an important role in the prediction\ntask but have not been sufficiently explored in terms of adversarial attacks.\nThis paper proposes a time series prediction attack algorithm based on\nfrequency domain loss. Specifically, we adapt an attack method originally\ndesigned for classification tasks to the prediction field and optimize the\nadversarial samples using both time-domain and frequency-domain losses. To the\nbest of our knowledge, there is no relevant research on using frequency\ninformation for time-series adversarial attacks. Our experimental results show\nthat these current time series prediction models are vulnerable to adversarial\nattacks, and our approach achieves excellent performance on major time series\nforecasting datasets."
    },
    {
        "date": "2025-08",
        "title": "Load-Altering Attacks Against Power Grids: A Case Study Using the GB-36 Bus System Open Dataset",
        "author": "Syed Irtiza Maksud, and Subhash Lakshminarayana",
        "link": "http://arxiv.org/abs/2508.08945v1",
        "abstract": "The growing digitalization and the rapid adoption of high-powered\nInternet-of-Things (IoT)-enabled devices (e.g., EV charging stations) have\nincreased the vulnerability of power grids to cyber threats. In particular, the\nso-called Load Altering Attacks (LAAs) can trigger rapid frequency fluctuations\nand potentially destabilize the power grid. This paper aims to bridge the gap\nbetween academic research and practical application by using open-source\ndatasets released by grid operators. It investigates various LAA scenarios on a\nreal-world transmission network, namely the Great Britain (GB)-36 Zone model\nreleased by the UK's National Electricity System Operator (NESO). It evaluates\nthe threshold of LAA severity that the grid can tolerate before triggering\ncascading effects. Additionally, it explores how Battery Energy Storage Systems\n(BESS) based fast frequency response services can mitigate or prevent such\nimpacts. Simulations are conducted using DIgSILENT PowerFactory to ensure\nrealistic system representation. The analysis provides several useful insights\nto grid operators on the LAA impact, such as the influence of the relative\nlocations of BESS and LAA, as well as how delays in attack execution can\ninfluence the overall system response."
    },
    {
        "date": "2025-08",
        "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation",
        "author": "Eduarda Caldeira, Fadi Boutros, and Naser Damer",
        "link": "http://arxiv.org/abs/2508.08939v1",
        "abstract": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering."
    },
    {
        "date": "2025-08",
        "title": "EGGCodec: A Robust Neural Encodec Framework for EGG Reconstruction and F0 Extraction",
        "author": "Rui Feng, Yuang Chen, Yu Hu, Jun Du, and Jiahong Yuan",
        "link": "http://arxiv.org/abs/2508.08924v1",
        "abstract": "This letter introduces EGGCodec, a robust neural Encodec framework engineered\nfor electroglottography (EGG) signal reconstruction and F0 extraction. We\npropose a multi-scale frequency-domain loss function to capture the nuanced\nrelationship between original and reconstructed EGG signals, complemented by a\ntime-domain correlation loss to improve generalization and accuracy. Unlike\nconventional Encodec models that extract F0 directly from features, EGGCodec\nleverages reconstructed EGG signals, which more closely correspond to F0. By\nremoving the conventional GAN discriminator, we streamline EGGCodec's training\nprocess without compromising efficiency, incurring only negligible performance\ndegradation. Trained on a widely used EGG-inclusive dataset, extensive\nevaluations demonstrate that EGGCodec outperforms state-of-the-art F0\nextraction schemes, reducing mean absolute error (MAE) from 14.14 Hz to 13.69\nHz, and improving voicing decision error (VDE) by 38.2\\%. Moreover, extensive\nablation experiments validate the contribution of each component of EGGCodec."
    },
    {
        "date": "2025-08",
        "title": "Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning",
        "author": "Jungwoo Kim, and Jong-Seok Lee",
        "link": "http://arxiv.org/abs/2508.08920v1",
        "abstract": "Class-incremental continual learning addresses catastrophic forgetting by\nenabling classification models to preserve knowledge of previously learned\nclasses while acquiring new ones. However, the vulnerability of the models\nagainst adversarial attacks during this process has not been investigated\nsufficiently. In this paper, we present the first exploration of vulnerability\nto stage-transferred attacks, i.e., an adversarial example generated using the\nmodel in an earlier stage is used to attack the model in a later stage. Our\nfindings reveal that continual learning methods are highly susceptible to these\nattacks, raising a serious security issue. We explain this phenomenon through\nmodel similarity between stages and gradual robustness degradation.\nAdditionally, we find that existing adversarial training-based defense methods\nare not sufficiently effective to stage-transferred attacks. Codes are\navailable at https://github.com/mcml-official/CSAT."
    },
    {
        "date": "2025-08",
        "title": "A Robust Epipolar-Domain Regularization Algorithm for Light Field Depth Estimation",
        "author": "Noor Islam S. Mohammad",
        "link": "http://arxiv.org/abs/2508.08900v1",
        "abstract": "Robust depth estimation in light field imaging remains a critical challenge\nfor pattern recognition applications such as augmented reality, biomedical\nimaging, and scene reconstruction. While existing approaches often rely heavily\non deep convolutional neural networks, they tend to incur high computational\ncosts and struggle in noisy real-world environments. This paper proposes a\nnovel lightweight depth estimation pipeline that integrates light field-based\ndisparity information with a directed random walk refinement algorithm. Unlike\ntraditional CNN-based methods, our approach enhances depth map consistency\nwithout requiring extensive training or large-scale datasets. The proposed\nmethod was evaluated on the 4D Light Field Benchmark dataset and a diverse set\nof real-world images. Experimental results indicate that while performance\nslightly declines under uncontrolled conditions, the algorithm consistently\nmaintains low computational complexity and competitive accuracy compared to\nstate-of-the-art deep learning models. These findings highlight the potential\nof our method as a robust and efficient alternative for depth estimation and\nsegmentation in light field imaging. The work provides insights into practical\nalgorithm design for light field-based pattern recognition and opens new\ndirections for integrating probabilistic graph models with depth sensing\nframeworks."
    },
    {
        "date": "2025-08",
        "title": "Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks",
        "author": "Eric Seng, Hugh O'Connor, Adam Boyce, Josh J. Bailey, and Anton van Beek",
        "link": "http://arxiv.org/abs/2508.08863v1",
        "abstract": "Generative machine learning has emerged as a powerful tool for design\nrepresentation and exploration. However, its application is often constrained\nby the need for large datasets of existing designs and the lack of\ninterpretability about what features drive optimality. To address these\nchallenges, we introduce a systematic framework for constructing training\ndatasets tailored to generative models and demonstrate how these models can be\nleveraged for interpretable design. The novelty of this work is twofold: (i) we\npresent a systematic framework for generating archetypes with internally\nhomogeneous but mutually heterogeneous inputs that can be used to generate a\ntraining dataset, and (ii) we show how integrating generative models with\nBayesian optimization can enhance the interpretability of the latent space of\nadmissible designs. These findings are validated by using the framework to\ndesign a flow battery manifold, demonstrating that it effectively captures the\nspace of feasible designs, including novel configurations while enabling\nefficient exploration. This work broadens the applicability of generative\nmachine-learning models in system designs by enhancing quality and reliability."
    },
    {
        "date": "2025-08",
        "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems",
        "author": "Yuren Hao, Xiang Wan, and Chengxiang Zhai",
        "link": "http://arxiv.org/abs/2508.08833v1",
        "abstract": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities."
    },
    {
        "date": "2025-08",
        "title": "Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem",
        "author": "Michael Li, Eric Bae, Christopher Haberland, and Natasha Jaques",
        "link": "http://arxiv.org/abs/2508.08718v1",
        "abstract": "The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial\noptimization task with numerous practical applications. Classic heuristic\nsolvers can attain near-optimal performance for small problem instances, but\nbecome computationally intractable for larger problems. Real-world logistics\nproblems such as dynamically re-routing last-mile deliveries demand a solver\nwith fast inference time, which has led researchers to investigate specialized\nneural network solvers. However, neural networks struggle to generalize beyond\nthe synthetic data they were trained on. In particular, we show that there\nexist TSP distributions that are realistic in practice, which also consistently\nlead to poor worst-case performance for existing neural approaches. To address\nthis issue of distribution robustness, we present Combinatorial Optimization\nwith Generative Sampling (COGS), where training data is sampled from a\ngenerative TSP model. We show that COGS provides better data coverage and\ninterpolation in the space of TSP training distributions. We also present\nTSPLib50, a dataset of realistically distributed TSP samples, which tests\nreal-world generalization ability without conflating this issue with instance\nsize. We evaluate our method on various synthetic datasets as well as TSPLib50,\nand compare to state-of-the-art neural baselines. We demonstrate that COGS\nimproves distribution robustness, with most performance gains coming from\nworst-case scenarios."
    },
    {
        "date": "2025-08",
        "title": "Evasive Ransomware Attacks Using Low-level Behavioral Adversarial Examples",
        "author": "Manabu Hirano, and Ryotaro Kobayashi",
        "link": "http://arxiv.org/abs/2508.08656v1",
        "abstract": "Protecting state-of-the-art AI-based cybersecurity defense systems from cyber\nattacks is crucial. Attackers create adversarial examples by adding small\nchanges (i.e., perturbations) to the attack features to evade or fool the deep\nlearning model. This paper introduces the concept of low-level behavioral\nadversarial examples and its threat model of evasive ransomware. We formulate\nthe method and the threat model to generate the optimal source code of evasive\nmalware. We then examine the method using the leaked source code of Conti\nransomware with the micro-behavior control function. The micro-behavior control\nfunction is our test component to simulate changing source code in ransomware;\nransomware's behavior can be changed by specifying the number of threads, file\nencryption ratio, and delay after file encryption at the boot time. We\nevaluated how much an attacker can control the behavioral features of\nransomware using the micro-behavior control function to decrease the detection\nrate of a ransomware detector."
    },
    {
        "date": "2025-08",
        "title": "AME: Aligned Manifold Entropy for Robust Vision-Language Distillation",
        "author": "Guiming Cao, and Yuming Ou",
        "link": "http://arxiv.org/abs/2508.08644v1",
        "abstract": "Knowledge distillation is a long-established technique for knowledge\ntransfer, and has regained attention in the context of the recent emergence of\nlarge vision-language models (VLMs). However, vision-language knowledge\ndistillation often requires sufficient training data to achieve robust\ngeneralization on amples with ambiguous or boundary-adjacent representations,\nwhich are associated with high predictive uncertainty. Critically, collecting\nsuch large-scale, task-specific data for training is often impractical in\nreal-world scenarios. To address this major challenge arising from the\nentanglement of uncertainty and cross-modal feature representation, we propose\nAligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming\nto achieve robust generalization under real-world conditions. AME applies\nentropy minimization over a reconfigured shared manifold, where multi-modal\ndata (i.e., image and text) are bridged through a pair of projection functions,\nconducive to structural compression for cross-modal feature representations.\nThis enables robust knowledge distillation under low-data regimes, while\nrequiring no architectural modifications to the backbone. As a result, it can\nserve as a plug-and-play module compatible with a wide range of vision-language\ndistillation frameworks. Notably, our theoretical analysis reveals that\nintegrating knowledge distillation with entropy minimization over the shared\nmanifold leads to a tighter generalization error bound. Extensive experiments\nacross diverse distillation architectures and training settings demonstrate\nthat AME consistently facilitates robust knowledge distillation, resulting in\nsuperior generalization performance across a wide spectrum of downstream tasks."
    },
    {
        "date": "2025-08",
        "title": "Securing Educational LLMs: A Generalised Taxonomy of Attacks on LLMs and DREAD Risk Assessment",
        "author": "Farzana Zahid, Anjalika Sewwandi, Lee Brandon, Vimal Kumar, and Roopak Sinha",
        "link": "http://arxiv.org/abs/2508.08629v1",
        "abstract": "Due to perceptions of efficiency and significant productivity gains, various\norganisations, including in education, are adopting Large Language Models\n(LLMs) into their workflows. Educator-facing, learner-facing, and\ninstitution-facing LLMs, collectively, Educational Large Language Models\n(eLLMs), complement and enhance the effectiveness of teaching, learning, and\nacademic operations. However, their integration into an educational setting\nraises significant cybersecurity concerns. A comprehensive landscape of\ncontemporary attacks on LLMs and their impact on the educational environment is\nmissing. This study presents a generalised taxonomy of fifty attacks on LLMs,\nwhich are categorized as attacks targeting either models or their\ninfrastructure. The severity of these attacks is evaluated in the educational\nsector using the DREAD risk assessment framework. Our risk assessment indicates\nthat token smuggling, adversarial prompts, direct injection, and multi-step\njailbreak are critical attacks on eLLMs. The proposed taxonomy, its application\nin the educational environment, and our risk assessment will help academic and\nindustrial practitioners to build resilient solutions that protect learners and\ninstitutions."
    },
    {
        "date": "2025-08",
        "title": "Special-Character Adversarial Attacks on Open-Source Language Model",
        "author": "Ephraiem Sarabamoun",
        "link": "http://arxiv.org/abs/2508.14070v1",
        "abstract": "Large language models (LLMs) have achieved remarkable performance across\ndiverse natural language processing tasks, yet their vulnerability to\ncharacter-level adversarial manipulations presents significant security\nchallenges for real-world deployments."
    },
    {
        "date": "2025-08",
        "title": "AI Security Map: Holistic Organization of AI Security Technologies and Impacts on Stakeholders",
        "author": "Hiroya Kato, Kentaro Kita, Kento Hasegawa, and Seira Hidano",
        "link": "http://arxiv.org/abs/2508.08583v1",
        "abstract": "As the social implementation of AI has been steadily progressing, research\nand development related to AI security has also been increasing. However,\nexisting studies have been limited to organizing related techniques, attacks,\ndefenses, and risks in terms of specific domains or AI elements. Thus, it\nextremely difficult to understand the relationships among them and how negative\nimpacts on stakeholders are brought about. In this paper, we argue that the\nknowledge, technologies, and social impacts related to AI security should be\nholistically organized to help understand relationships among them. To this\nend, we first develop an AI security map that holistically organizes\ninterrelationships among elements related to AI security as well as negative\nimpacts on information systems and stakeholders. This map consists of the two\naspects, namely the information system aspect (ISA) and the external influence\naspect (EIA). The elements that AI should fulfill within information systems\nare classified under the ISA. The EIA includes elements that affect\nstakeholders as a result of AI being attacked or misused. For each element,\ncorresponding negative impacts are identified. By referring to the AI security\nmap, one can understand the potential negative impacts, along with their causes\nand countermeasures. Additionally, our map helps clarify how the negative\nimpacts on AI-based systems relate to those on stakeholders. We show some\nfindings newly obtained by referring to our map. We also provide several\nrecommendations and open problems to guide future AI security communities."
    },
    {
        "date": "2025-08",
        "title": "Multi-Target Backdoor Attacks Against Speaker Recognition",
        "author": "Alexandrine Fortier, Sonal Joshi, Thomas Thebaud, Jesus Villalba Lopez, Najim Dehak, and Patrick Cardinal",
        "link": "http://arxiv.org/abs/2508.08559v2",
        "abstract": "In this work, we propose a multi-target backdoor attack against speaker\nidentification using position-independent clicking sounds as triggers. Unlike\nprevious single-target approaches, our method targets up to 50 speakers\nsimultaneously, achieving success rates of up to 95.04%. To simulate more\nrealistic attack conditions, we vary the signal-to-noise ratio between speech\nand trigger, demonstrating a trade-off between stealth and effectiveness. We\nfurther extend the attack to the speaker verification task by selecting the\nmost similar training speaker - based on cosine similarity - as a proxy target.\nThe attack is most effective when target and enrolled speaker pairs are highly\nsimilar, reaching success rates of up to 90% in such cases."
    },
    {
        "date": "2025-08",
        "title": "Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System",
        "author": "Pallavi Zambare, Venkata Nikhil Thanikella, and Ying Liu",
        "link": "http://arxiv.org/abs/2508.10043v1",
        "abstract": "When combining Large Language Models (LLMs) with autonomous agents, used in\nnetwork monitoring and decision-making systems, this will create serious\nsecurity issues. In this research, the MAESTRO framework consisting of the\nseven layers threat modeling architecture in the system was used to expose,\nevaluate, and eliminate vulnerabilities of agentic AI. The prototype agent\nsystem was constructed and implemented, using Python, LangChain, and telemetry\nin WebSockets, and deployed with inference, memory, parameter tuning, and\nanomaly detection modules. Two practical threat cases were confirmed as\nfollows: (i) resource denial of service by traffic replay denial-of-service,\nand (ii) memory poisoning by tampering with the historical log file maintained\nby the agent. These situations resulted in measurable levels of performance\ndegradation, i.e. telemetry updates were delayed, and computational loads were\nincreased, as a result of poor system adaptations. It was suggested to use a\nmultilayered defense-in-depth approach with memory isolation, validation of\nplanners and anomaly response systems in real-time. These findings verify that\nMAESTRO is viable in operational threat mapping, prospective risk scoring, and\nthe basis of the resilient system design. The authors bring attention to the\nimportance of the enforcement of memory integrity, paying attention to the\nadaptation logic monitoring, and cross-layer communication protection that\nguarantee the agentic AI reliability in adversarial settings."
    },
    {
        "date": "2025-08",
        "title": "FIDELIS: Blockchain-Enabled Protection Against Poisoning Attacks in Federated Learning",
        "author": "Jane Carney, Kushal Upreti, Gaby G. Dagher, and Tim Andersen",
        "link": "http://arxiv.org/abs/2508.10042v1",
        "abstract": "Federated learning enhances traditional deep learning by enabling the joint\ntraining of a model with the use of IoT device's private data. It ensures\nprivacy for clients, but is susceptible to data poisoning attacks during\ntraining that degrade model performance and integrity. Current poisoning\ndetection methods in federated learning lack a standardized detection method or\ntake significant liberties with trust. In this paper, we present \\Sys, a novel\nblockchain-enabled poison detection framework in federated learning. The\nframework decentralizes the role of the global server across participating\nclients. We introduce a judge model used to detect data poisoning in model\nupdates. The judge model is produced by each client and verified to reach\nconsensus on a single judge model. We implement our solution to show \\Sys is\nrobust against data poisoning attacks and the creation of our judge model is\nscalable."
    },
    {
        "date": "2025-08",
        "title": "Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers",
        "author": "Amirhossein Taherpour, Abbas Taherpour, and Tamer Khattab",
        "link": "http://arxiv.org/abs/2508.08206v1",
        "abstract": "We propose a joint learning framework for Byzantine-resilient spectrum\nsensing and secure intelligent reflecting surface (IRS)--assisted opportunistic\naccess under channel state information (CSI) uncertainty. The sensing stage\nperforms logit-domain Bayesian updates with trimmed aggregation and\nattention-weighted consensus, and the base station (BS) fuses network beliefs\nwith a conservative minimum rule, preserving detection accuracy under a bounded\nnumber of Byzantine users. Conditioned on the sensing outcome, we pose downlink\ndesign as sum mean-squared error (MSE) minimization under transmit-power and\nsignal-leakage constraints and jointly optimize the BS precoder, IRS phase\nshifts, and user equalizers. With partial (or known) CSI, we develop an\naugmented-Lagrangian alternating algorithm with projected updates and provide\nprovable sublinear convergence, with accelerated rates under mild local\ncurvature. With unknown CSI, we perform constrained Bayesian optimization (BO)\nin a geometry-aware low-dimensional latent space using Gaussian process (GP)\nsurrogates; we prove regret bounds for a constrained upper confidence bound\n(UCB) variant of the BO module, and demonstrate strong empirical performance of\nthe implemented procedure. Simulations across diverse network conditions show\nhigher detection probability at fixed false-alarm rate under adversarial\nattacks, large reductions in sum MSE for honest users, strong suppression of\neavesdropper signal power, and fast convergence. The framework offers a\npractical path to secure opportunistic communication that adapts to CSI\navailability while coherently coordinating sensing and transmission through\njoint learning."
    },
    {
        "date": "2025-08",
        "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization",
        "author": "Nicholas Klein, Hemlata Tak, James Fullwood, Krishna Regmi, Leonidas Spinoulas, Ganesh Sivaraman, Tianxiang Chen, and Elie Khoury",
        "link": "http://arxiv.org/abs/2508.08141v1",
        "abstract": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset."
    },
    {
        "date": "2025-08",
        "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks",
        "author": "Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, and Xin Wang",
        "link": "http://arxiv.org/abs/2508.08127v1",
        "abstract": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard."
    },
    {
        "date": "2025-08",
        "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning",
        "author": "Jiayao Wang, Yang Song, Zhendong Zhao, Jiale Zhang, Qilin Wu, Junwu Zhu, and Dongfang Zhao",
        "link": "http://arxiv.org/abs/2508.08031v1",
        "abstract": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms."
    },
    {
        "date": "2025-08",
        "title": "Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks",
        "author": "Thusitha Dayaratne, Ngoc Duy Pham, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, and Carsten Rudolph",
        "link": "http://arxiv.org/abs/2508.08029v1",
        "abstract": "The introduction of 5G and the Open Radio Access Network (O-RAN) architecture\nhas enabled more flexible and intelligent network deployments. However, the\nincreased complexity and openness of these architectures also introduce novel\nsecurity challenges, such as data manipulation attacks on the semi-standardised\nShared Data Layer (SDL) within the O-RAN platform through malicious xApps. In\nparticular, malicious xApps can exploit this vulnerability by introducing\nsubtle Unicode-wise alterations (hypoglyphs) into the data that are being used\nby traditional machine learning (ML)-based anomaly detection methods. These\nUnicode-wise manipulations can potentially bypass detection and cause failures\nin anomaly detection systems based on traditional ML, such as AutoEncoders,\nwhich are unable to process hypoglyphed data without crashing. We investigate\nthe use of Large Language Models (LLMs) for anomaly detection within the O-RAN\narchitecture to address this challenge. We demonstrate that LLM-based xApps\nmaintain robust operational performance and are capable of processing\nmanipulated messages without crashing. While initial detection accuracy\nrequires further improvements, our results highlight the robustness of LLMs to\nadversarial attacks such as hypoglyphs in input data. There is potential to use\ntheir adaptability through prompt engineering to further improve the accuracy,\nalthough this requires further research. Additionally, we show that LLMs\nachieve low detection latency (under 0.07 seconds), making them suitable for\nNear-Real-Time (Near-RT) RIC deployments."
    },
    {
        "date": "2025-08",
        "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking",
        "author": "Tony Danjun Wang, Christian Heiliger, Nassir Navab, and Lennart Bastian",
        "link": "http://arxiv.org/abs/2508.07968v1",
        "abstract": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support."
    },
    {
        "date": "2025-08",
        "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security",
        "author": "Ajnas Muhammed, Iurri Medvedev, and Nuno Gon\u00e7alves",
        "link": "http://arxiv.org/abs/2508.07960v1",
        "abstract": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace"
    },
    {
        "date": "2025-08",
        "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake",
        "author": "Hongrui Zheng, Yuezun Li, Liejun Wang, Yunfeng Diao, and Zhiqing Guo",
        "link": "http://arxiv.org/abs/2508.07795v2",
        "abstract": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF."
    },
    {
        "date": "2025-08",
        "title": "Best-Effort Policies for Robust Markov Decision Processes",
        "author": "Alessandro Abate, Thom Badings, Giuseppe De Giacomo, and Francesco Fabiano",
        "link": "http://arxiv.org/abs/2508.07790v1",
        "abstract": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach."
    },
    {
        "date": "2025-08",
        "title": "Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations",
        "author": "Pietro Talli, Federico Mason, Federico Chiariotti, and Andrea Zanella",
        "link": "http://arxiv.org/abs/2508.07722v1",
        "abstract": "In this work, we address the problem of training Reinforcement Learning (RL)\nagents over communication networks. The RL paradigm requires the agent to\ninstantaneously perceive the state evolution to infer the effects of its\nactions on the environment. This is impossible if the agent receives state\nupdates over lossy or delayed wireless systems and thus operates with partial\nand intermittent information. In recent years, numerous frameworks have been\nproposed to manage RL with imperfect feedback; however, they often offer\nspecific solutions with a substantial computational burden. To address these\nlimits, we propose a novel architecture, named Homomorphic Robust Remote\nReinforcement Learning (HR3L), that enables the training of remote RL agents\nexchanging observations across a non-ideal wireless channel. HR3L considers two\nunits: the transmitter, which encodes meaningful representations of the\nenvironment, and the receiver, which decodes these messages and performs\nactions to maximize a reward signal. Importantly, HR3L does not require the\nexchange of gradient information across the wireless channel, allowing for\nquicker training and a lower communication overhead than state-of-the-art\nsolutions. Experimental results demonstrate that HR3L significantly outperforms\nbaseline methods in terms of sample efficiency and adapts to different\ncommunication scenarios, including packet losses, delayed transmissions, and\ncapacity limitations."
    },
    {
        "date": "2025-08",
        "title": "AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition",
        "author": "Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Xinyi Yin, Danlei Huang, and Fei Yu",
        "link": "http://arxiv.org/abs/2508.07608v1",
        "abstract": "Audio-visual speech recognition (AVSR) combines audio-visual modalities to\nimprove speech recognition, especially in noisy environments. However, most\nexisting methods deploy the unidirectional enhancement or symmetric fusion\nmanner, which limits their capability to capture heterogeneous and\ncomplementary correlations of audio-visual data-especially under asymmetric\ninformation conditions. To tackle these gaps, we introduce a new AVSR framework\ntermed AD-AVSR based on bidirectional modality enhancement. Specifically, we\nfirst introduce the audio dual-stream encoding strategy to enrich audio\nrepresentations from multiple perspectives and intentionally establish\nasymmetry to support subsequent cross-modal interactions. The enhancement\nprocess involves two key components, Audio-aware Visual Refinement Module for\nenhanced visual representations under audio guidance, and Cross-modal Noise\nSuppression Masking Module which refines audio representations using visual\ncues, collaboratively leading to the closed-loop and bidirectional information\nflow. To further enhance correlation robustness, we adopt a threshold-based\nselection mechanism to filter out irrelevant or weakly correlated audio-visual\npairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate\nthat our AD-AVSR consistently surpasses SOTA methods in both performance and\nnoise robustness, highlighting the effectiveness of our model design."
    },
    {
        "date": "2025-08",
        "title": "Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks",
        "author": "Kun Ming Goh",
        "link": "http://arxiv.org/abs/2508.09209v2",
        "abstract": "Generative adversarial networks (GANs) have emerged as a powerful paradigm\nfor producing high-fidelity data samples, yet their performance is constrained\nby the quality of latent representations, typically sampled from classical\nnoise distributions. This study investigates hybrid quantum-classical GANs\n(HQCGANs) in which a quantum generator, implemented via parameterised quantum\ncircuits, produces latent vectors for a classical discriminator. We evaluate a\nclassical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using\nQiskit's AerSimulator with realistic noise models to emulate near-term quantum\ndevices. The binary MNIST dataset (digits 0 and 1) is used to align with the\nlow-dimensional latent spaces imposed by current quantum hardware. Models are\ntrained for 150 epochs and assessed with Frechet Inception Distance (FID) and\nKernel Inception Distance (KID). Results show that while the classical GAN\nachieved the best scores, the 7-qubit HQCGAN produced competitive performance,\nnarrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier\nconvergence limitations. Efficiency analysis indicates only moderate training\ntime increases despite quantum sampling overhead. These findings validate the\nfeasibility of noisy quantum circuits as latent priors in GAN architectures,\nhighlighting their potential to enhance generative modelling within the\nconstraints of the noisy intermediate-scale quantum (NISQ) era."
    },
    {
        "date": "2025-08",
        "title": "ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack",
        "author": "Rongxuan Peng, Shunquan Tan, Chenqi Kong, Anwei Luo, Alex C. Kot, and Jiwu Huang",
        "link": "http://arxiv.org/abs/2508.07402v2",
        "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for\nadapting large vision foundation models, such as the Segment Anything Model\n(SAM) and LLaVA, to downstream tasks like image forgery detection and\nlocalization (IFDL). However, existing PEFT-based approaches overlook their\nvulnerability to adversarial attacks. In this paper, we show that highly\ntransferable adversarial images can be crafted solely via the upstream model,\nwithout accessing the downstream model or training data, significantly\ndegrading the IFDL performance. To address this, we propose ForensicsSAM, a\nunified IFDL framework with built-in adversarial robustness. Our design is\nguided by three key ideas: (1) To compensate for the lack of forgery-relevant\nknowledge in the frozen image encoder, we inject forgery experts into each\ntransformer block to enhance its ability to capture forgery artifacts. These\nforgery experts are always activated and shared across any input images. (2) To\ndetect adversarial images, we design an light-weight adversary detector that\nlearns to capture structured, task-specific artifact in RGB domain, enabling\nreliable discrimination across various attack methods. (3) To resist\nadversarial attacks, we inject adversary experts into the global attention\nlayers and MLP modules to progressively correct feature shifts induced by\nadversarial noise. These adversary experts are adaptively activated by the\nadversary detector, thereby avoiding unnecessary interference with clean\nimages. Extensive experiments across multiple benchmarks demonstrate that\nForensicsSAM achieves superior resistance to various adversarial attack\nmethods, while also delivering state-of-the-art performance in image-level\nforgery detection and pixel-level forgery localization. The resource is\navailable at https://github.com/siriusPRX/ForensicsSAM."
    },
    {
        "date": "2025-08",
        "title": "Multi-task Adversarial Attacks against Black-box Model with Few-shot Queries",
        "author": "Wenqiang Wang, Yan Xiao, Hao Lin, Yangshijie Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2508.10039v1",
        "abstract": "Current multi-task adversarial text attacks rely on abundant access to shared\ninternal features and numerous queries, often limited to a single task type. As\na result, these attacks are less effective against practical scenarios\ninvolving black-box feedback APIs, limited queries, or multiple task types. To\nbridge this gap, we propose \\textbf{C}luster and \\textbf{E}nsemble\n\\textbf{M}ulti-task Text Adversarial \\textbf{A}ttack (\\textbf{CEMA}), an\neffective black-box attack that exploits the transferability of adversarial\ntexts across different tasks. CEMA simplifies complex multi-task scenarios by\nusing a \\textit{deep-level substitute model} trained in a\n\\textit{plug-and-play} manner for text classification, enabling attacks without\nmimicking the victim model. This approach requires only a few queries for\ntraining, converting multi-task attacks into classification attacks and\nallowing attacks across various tasks.\n  CEMA generates multiple adversarial candidates using different text\nclassification methods and selects the one that most effectively attacks\nsubstitute models.\n  In experiments involving multi-task models with two, three, or six\ntasks--spanning classification, translation, summarization, and text-to-image\ngeneration--CEMA demonstrates significant attack success with as few as 100\nqueries. Furthermore, CEMA can target commercial APIs (e.g., Baidu and Google\nTranslate), large language models (e.g., ChatGPT 4o), and image-generation\nmodels (e.g., Stable Diffusion V2), showcasing its versatility and\neffectiveness in real-world applications."
    },
    {
        "date": "2025-08",
        "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering",
        "author": "Shubhra Ghosh, Abhilekh Borah, Aditya Kumar Guru, and Kripabandhu Ghosh",
        "link": "http://arxiv.org/abs/2508.07321v1",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available."
    },
    {
        "date": "2025-08",
        "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems",
        "author": "Qingyuan Zeng, Shu Jiang, Jiajing Lin, Zhenzhong Wang, Kay Chen Tan, and Min Jiang",
        "link": "http://arxiv.org/abs/2508.07263v1",
        "abstract": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital\nwatermarking techniques, embedding either 1D bitstreams or 2D images, are used\nfor copyright protection. However, the robustness of these watermarking\ntechniques against potential attacks remains underexplored. This paper\nintroduces the first universal black-box attack framework, the Group-based\nMulti-objective Evolutionary Attack (GMEA), designed to challenge these\nwatermarking systems. We formulate the attack as a large-scale multi-objective\noptimization problem, balancing watermark removal with visual quality. In a\nblack-box setting, we introduce an indirect objective function that blinds the\nwatermark detector by minimizing the standard deviation of features extracted\nby a convolutional network, thus rendering the feature maps uninformative. To\nmanage the vast search space of 3DGS models, we employ a group-based\noptimization strategy to partition the model into multiple, independent\nsub-optimization problems. Experiments demonstrate that our framework\neffectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking\nmethods while maintaining high visual fidelity. This work reveals critical\nvulnerabilities in existing 3DGS copyright protection schemes and calls for the\ndevelopment of more robust watermarking systems."
    },
    {
        "date": "2025-08",
        "title": "Certifiably robust malware detectors by design",
        "author": "Pierre-Francois Gimenez, Sarath Sivaprasad, and Mario Fritz",
        "link": "http://arxiv.org/abs/2508.10038v1",
        "abstract": "Malware analysis involves analyzing suspicious software to detect malicious\npayloads. Static malware analysis, which does not require software execution,\nrelies increasingly on machine learning techniques to achieve scalability.\nAlthough such techniques obtain very high detection accuracy, they can be\neasily evaded with adversarial examples where a few modifications of the sample\ncan dupe the detector without modifying the behavior of the software. Unlike\nother domains, such as computer vision, creating an adversarial example of\nmalware without altering its functionality requires specific transformations.\nWe propose a new model architecture for certifiably robust malware detection by\ndesign. In addition, we show that every robust detector can be decomposed into\na specific structure, which can be applied to learn empirically robust malware\ndetectors, even on fragile features. Our framework ERDALT is based on this\nstructure. We compare and validate these approaches with machine-learning-based\nmalware detection methods, allowing for robust detection with limited reduction\nof detection performance."
    },
    {
        "date": "2025-08",
        "title": "Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools",
        "author": "Prashant Sharma",
        "link": "http://arxiv.org/abs/2508.07203v1",
        "abstract": "Current digital government literature focuses on professional in-house IT\nteams, specialized digital service teams, vendor-developed systems, or\nproprietary low-code/no-code tools. Almost no scholarship addresses a growing\nmiddle ground: technically skilled civil servants outside formal IT roles who\ncan write real code but lack a sanctioned, secure path to deploy their work.\nThis paper introduces a limits-aware, open-source and replicable platform that\nenables such public servants to develop, peer review, and deploy small-scale,\ndomain-specific applications within government networks via a sandboxed,\nauditable workflow. By combining Jupyter Notebooks, preapproved open-source\nlibraries, and lightweight governance, the platform works within institutional\nconstraints such as procurement rules and IT security policies while avoiding\nvendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil\nservants' programming skills, keeping them technically competitive with their\nprivate-sector peers. This contribution fills a critical gap, offering a\nreplicable model for public-sector skill retention, resilience, and bottom-up\ndigital transformation."
    },
    {
        "date": "2025-08",
        "title": "A Real-Time, Self-Tuning Moderator Framework for Adversarial Prompt Detection",
        "author": "Ivan Zhang",
        "link": "http://arxiv.org/abs/2508.07139v1",
        "abstract": "Ensuring LLM alignment is critical to information security as AI models\nbecome increasingly widespread and integrated in society. Unfortunately, many\ndefenses against adversarial attacks and jailbreaking on LLMs cannot adapt\nquickly to new attacks, degrade model responses to benign prompts, or introduce\nsignificant barriers to scalable implementation. To mitigate these challenges,\nwe introduce a real-time, self-tuning (RTST) moderator framework to defend\nagainst adversarial attacks while maintaining a lightweight training footprint.\nWe empirically evaluate its effectiveness using Google's Gemini models against\nmodern, effective jailbreaks. Our results demonstrate the advantages of an\nadaptive, minimally intrusive framework for jailbreak defense over traditional\nfine-tuning or classifier models."
    },
    {
        "date": "2025-08",
        "title": "Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models",
        "author": "Antonino Greco, Marco D'Alessandro, Karl J. Friston, Giovanni Pezzulo, and Markus Siegel",
        "link": "http://arxiv.org/abs/2508.07115v1",
        "abstract": "Biological systems leverage top-down feedback for visual processing, yet most\nartificial vision models succeed in image classification using purely\nfeedforward or recurrent architectures, calling into question the functional\nsignificance of descending cortical pathways. Here, we trained convolutional\nrecurrent neural networks (ConvRNN) on image classification in the presence or\nabsence of top-down feedback projections to elucidate the specific\ncomputational contributions of those feedback pathways. We found that ConvRNNs\nwith top-down feedback exhibited remarkable speed-accuracy trade-off and\nrobustness to noise perturbations and adversarial attacks, but only when they\nwere trained with stochastic neural variability, simulated by randomly\nsilencing single units via dropout. By performing detailed analyses to identify\nthe reasons for such benefits, we observed that feedback information\nsubstantially shaped the representational geometry of the post-integration\nlayer, combining the bottom-up and top-down streams, and this effect was\namplified by dropout. Moreover, feedback signals coupled with dropout optimally\nconstrained network activity onto a low-dimensional manifold and encoded object\ninformation more efficiently in out-of-distribution regimes, with top-down\ninformation stabilizing the representational dynamics at the population level.\nTogether, these findings uncover a dual mechanism for resilient sensory coding.\nOn the one hand, neural stochasticity prevents unit-level co-adaptation albeit\nat the cost of more chaotic dynamics. On the other hand, top-down feedback\nharnesses high-level information to stabilize network activity on compact\nlow-dimensional manifolds."
    },
    {
        "date": "2025-08",
        "title": "ScamDetect: Towards a Robust, Agnostic Framework to Uncover Threats in Smart Contracts",
        "author": "Pasquale De Rosa, Pascal Felber, and Valerio Schiavoni",
        "link": "http://arxiv.org/abs/2508.07094v2",
        "abstract": "Smart contracts have transformed decentralized finance by enabling\nprogrammable, trustless transactions. However, their widespread adoption and\ngrowing financial significance have attracted persistent and sophisticated\nthreats, such as phishing campaigns and contract-level exploits. Traditional\ntransaction-based threat detection methods often expose sensitive user data and\ninteractions, raising privacy and security concerns. In response, static\nbytecode analysis has emerged as a proactive mitigation strategy, identifying\nmalicious contracts before they execute harmful actions. Building on this\napproach, we introduced PhishingHook, the first machine-learning-based\nframework for detecting phishing activities in smart contracts via static\nbytecode and opcode analysis, achieving approximately 90% detection accuracy.\nNevertheless, two pressing challenges remain: (1) the increasing use of\nsophisticated bytecode obfuscation techniques designed to evade static\nanalysis, and (2) the heterogeneity of blockchain environments requiring\nplatform-agnostic solutions. This paper presents a vision for ScamDetect (Smart\nContract Agnostic Malware Detector), a robust, modular, and platform-agnostic\nframework for smart contract malware detection. Over the next 2.5 years,\nScamDetect will evolve in two stages: first, by tackling obfuscated Ethereum\nVirtual Machine (EVM) bytecode through graph neural network (GNN) analysis of\ncontrol flow graphs (CFGs), leveraging GNNs' ability to capture complex\nstructural patterns beyond opcode sequences; and second, by generalizing\ndetection capabilities to emerging runtimes such as WASM. ScamDetect aims to\nenable proactive, scalable security for the future of decentralized ecosystems."
    },
    {
        "date": "2025-08",
        "title": "Neural Network-Based Detection and Multi-Class Classification of FDI Attacks in Smart Grid Home Energy Systems",
        "author": "Varsha Sen, and Biswash Basnet",
        "link": "http://arxiv.org/abs/2508.10035v1",
        "abstract": "False Data Injection Attacks (FDIAs) pose a significant threat to smart grid\ninfrastructures, particularly Home Area Networks (HANs), where real-time\nmonitoring and control are highly adopted. Owing to the comparatively less\nstringent security controls and widespread availability of HANs, attackers view\nthem as an attractive entry point to manipulate aggregated demand patterns,\nwhich can ultimately propagate and disrupt broader grid operations. These\nattacks undermine the integrity of smart meter data, enabling malicious actors\nto manipulate consumption values without activating conventional alarms,\nthereby creating serious vulnerabilities across both residential and\nutility-scale infrastructures. This paper presents a machine learning-based\nframework for both the detection and classification of FDIAs using residential\nenergy data. A real-time detection is provided by the lightweight Artificial\nNeural Network (ANN), which works by using the most vital features of energy\nconsumption, cost, and time context. For the classification of different attack\ntypes, a Bidirectional LSTM is trained to recognize normal, trapezoidal, and\nsigmoid attack shapes through learning sequential dependencies in the data. A\nsynthetic time-series dataset was generated to emulate realistic household\nbehaviour. Experimental results demonstrate that the proposed models are\neffective in identifying and classifying FDIAs, offering a scalable solution\nfor enhancing grid resilience at the edge. This work contributes toward\nbuilding intelligent, data-driven defence mechanisms that strengthen smart grid\ncybersecurity from residential endpoints."
    }
]