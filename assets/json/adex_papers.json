[
    {
        "date": "2025-09",
        "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt",
        "author": "Saket S. Chaturvedi, Gaurav Bagwe, Lan Zhang, and Xiaoyong Yuan",
        "link": "http://arxiv.org/abs/2509.15159v1",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources to improve factual accuracy\nand verifiability. However, this reliance introduces new attack surfaces within\nthe retrieval pipeline, beyond the LLM itself. While prior RAG attacks have\nexposed such vulnerabilities, they largely rely on manipulating user queries,\nwhich is often infeasible in practice due to fixed or protected user inputs.\nThis narrow focus overlooks a more realistic and stealthy vector: instructional\nprompts, which are widely reused, publicly shared, and rarely audited. Their\nimplicit trust makes them a compelling target for adversaries to manipulate RAG\nbehavior covertly.\n  We introduce a novel attack for Adversarial Instructional Prompt (AIP) that\nexploits adversarial instructional prompts to manipulate RAG outputs by subtly\naltering retrieval behavior. By shifting the attack surface to the\ninstructional prompts, AIP reveals how trusted yet seemingly benign interface\ncomponents can be weaponized to degrade system integrity. The attack is crafted\nto achieve three goals: (1) naturalness, to evade user detection; (2) utility,\nto encourage use of prompts; and (3) robustness, to remain effective across\ndiverse query variations. We propose a diverse query generation strategy that\nsimulates realistic linguistic variation in user queries, enabling the\ndiscovery of prompts that generalize across paraphrases and rephrasings.\nBuilding on this, a genetic algorithm-based joint optimization is developed to\nevolve adversarial prompts by balancing attack success, clean-task utility, and\nstealthiness. Experimental results show that AIP achieves up to 95.23% ASR\nwhile preserving benign functionality. These findings uncover a critical and\npreviously overlooked vulnerability in RAG systems, emphasizing the need to\nreassess the shared instructional prompts."
    },
    {
        "date": "2025-09",
        "title": "Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression",
        "author": "Yigit E. Yildirim, Samet Demir, and Zafer Dogan",
        "link": "http://arxiv.org/abs/2509.15141v1",
        "abstract": "Empirical Risk Minimization (ERM) is a foundational framework for supervised\nlearning but primarily optimizes average-case performance, often neglecting\nfairness and robustness considerations. Tilted Empirical Risk Minimization\n(TERM) extends ERM by introducing an exponential tilt hyperparameter $t$ to\nbalance average-case accuracy with worst-case fairness and robustness. However,\nin online or streaming settings where data arrive one sample at a time, the\nclassical TERM objective degenerates to standard ERM, losing tilt sensitivity.\nWe address this limitation by proposing an online TERM formulation that removes\nthe logarithm from the classical objective, preserving tilt effects without\nadditional computational or memory overhead. This formulation enables a\ncontinuous trade-off controlled by $t$, smoothly interpolating between ERM ($t\n\\to 0$), fairness emphasis ($t > 0$), and robustness to outliers ($t < 0$). We\nempirically validate online TERM on two representative streaming tasks: robust\nlinear regression with adversarial outliers and minority-class detection in\nbinary classification. Our results demonstrate that negative tilting\neffectively suppresses outlier influence, while positive tilting improves\nrecall with minimal impact on precision, all at per-sample computational cost\nequivalent to ERM. Online TERM thus recovers the full robustness-fairness\nspectrum of classical TERM in an efficient single-sample learning regime."
    },
    {
        "date": "2025-09",
        "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
        "author": "Ju Dong, Lei Zhang, Liding Zhang, Yao Ling, Yu Fu, Kaixin Bai, Zolt\u00e1n-Csaba M\u00e1rton, Zhenshan Bing, Zhaopeng Chen, Alois Christian Knoll, and Jianwei Zhang",
        "link": "http://arxiv.org/abs/2509.14980v1",
        "abstract": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser."
    },
    {
        "date": "2025-09",
        "title": "Discrete optimal transport is a strong audio adversarial attack",
        "author": "Anton Selitskiy, Akib Shahriyar, and Jishnuraj Prakasan",
        "link": "http://arxiv.org/abs/2509.14959v1",
        "abstract": "In this paper, we show that discrete optimal transport (DOT) is an effective\nblack-box adversarial attack against modern audio anti-spoofing countermeasures\n(CMs). Our attack operates as a post-processing, distribution-alignment step:\nframe-level WavLM embeddings of generated speech are aligned to an unpaired\nbona fide pool via entropic OT and a top-$k$ barycentric projection, then\ndecoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with\nAASIST baselines, DOT yields consistently high equal error rate (EER) across\ndatasets and remains competitive after CM fine-tuning, outperforming several\nconventional attacks in cross-dataset transfer. Ablation analysis highlights\nthe practical impact of vocoder overlap. Results indicate that\ndistribution-level alignment is a powerful and stable attack surface for\ndeployed CMs."
    },
    {
        "date": "2025-09",
        "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems",
        "author": "Diego Gosmar, and Deborah A. Dahl",
        "link": "http://arxiv.org/abs/2509.14956v1",
        "abstract": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time."
    },
    {
        "date": "2025-09",
        "title": "Robust Barycenters of Persistence Diagrams",
        "author": "Keanu Sisouk, Eloi Tanguy, Julie Delon, and Julien Tierny",
        "link": "http://arxiv.org/abs/2509.14904v1",
        "abstract": "This short paper presents a general approach for computing robust Wasserstein\nbarycenters of persistence diagrams. The classical method consists in computing\nassignment arithmetic means after finding the optimal transport plans between\nthe barycenter and the persistence diagrams. However, this procedure only works\nfor the transportation cost related to the $q$-Wasserstein distance $W_q$ when\n$q=2$. We adapt an alternative fixed-point method to compute a barycenter\ndiagram for generic transportation costs ($q > 1$), in particular those robust\nto outliers, $q \\in (1,2)$. We show the utility of our work in two\napplications: \\emph{(i)} the clustering of persistence diagrams on their metric\nspace and \\emph{(ii)} the dictionary encoding of persistence diagrams. In both\nscenarios, we demonstrate the added robustness to outliers provided by our\ngeneralized framework. Our Python implementation is available at this address:\nhttps://github.com/Keanu-Sisouk/RobustBarycenter ."
    },
    {
        "date": "2025-09",
        "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis",
        "author": "Hoang-Son Nguyen, and Hoi-To Wai",
        "link": "http://arxiv.org/abs/2509.14887v1",
        "abstract": "Learning the graph underlying a networked system from nodal signals is\ncrucial to downstream tasks in graph signal processing and machine learning.\nThe presence of hidden nodes whose signals are not observable might corrupt the\nestimated graph. While existing works proposed various robustifications of\nvanilla graph learning objectives by explicitly accounting for the presence of\nthese hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic\napproaches is still underexplored. This work demonstrates that vanilla graph\ntopology learning methods are implicitly robust to partial observations of\nlow-pass filtered graph signals. We achieve this theoretical result through\nextending the restricted isometry property (RIP) to the Dirichlet energy\nfunction used in graph learning objectives. We show that smoothness-based graph\nlearning formulation (e.g., the GL-SigRep method) on partial observations can\nrecover the ground truth graph topology corresponding to the observed nodes.\nSynthetic and real data experiments corroborate our findings."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration",
        "author": "Letian Zhang, Guanghao Meng, Xudong Ren, Yiming Wang, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2509.14750v1",
        "abstract": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains."
    },
    {
        "date": "2025-09",
        "title": "Security Analysis of Web Applications Based on Gruyere",
        "author": "Yonghao Ni, Zhongwen Li, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2509.14706v1",
        "abstract": "With the rapid development of Internet technologies, web systems have become\nessential infrastructures for modern information exchange and business\noperations. However, alongside their expansion, numerous security\nvulnerabilities have emerged, making web security a critical research focus\nwithin the broader field of cybersecurity. These issues are closely related to\ndata protection, privacy preservation, and business continuity, and systematic\nresearch on web security is crucial for mitigating malicious attacks and\nenhancing the reliability and robustness of network systems. This paper first\nreviews the OWASP Top 10, summarizing the types, causes, and impacts of common\nweb vulnerabilities, and illustrates their exploitation mechanisms through\nrepresentative cases. Building upon this, the Gruyere platform is adopted as an\nexperimental subject for analyzing known vulnerabilities. The study presents\ndetailed reproduction steps for specific vulnerabilities, proposes\ncomprehensive remediation strategies, and further compares Gruyere's\nvulnerabilities with contemporary real-world cases. The findings suggest that,\nalthough Gruyere's vulnerabilities are relatively outdated, their underlying\nprinciples remain highly relevant for explaining a wide range of modern\nsecurity flaws. Overall, this research demonstrates that web system security\nanalysis based on Gruyere not only deepens the understanding of vulnerability\nmechanisms but also provides practical support for technological innovation and\nsecurity defense."
    },
    {
        "date": "2025-09",
        "title": "Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework",
        "author": "Sergio Benlloch-Lopez, Miquel Viel-Vazquez, Javier Naranjo-Alcazar, Jordi Grau-Haro, and Pedro Zuccarello",
        "link": "http://arxiv.org/abs/2509.14657v1",
        "abstract": "The rapid proliferation of IoT nodes equipped with microphones and capable of\nperforming on-device audio classification exposes highly sensitive data while\noperating under tight resource constraints. To protect against this, we present\na defence-in-depth architecture comprising a security protocol that treats the\nedge device, cellular network and cloud backend as three separate trust\ndomains, linked by TPM-based remote attestation and mutually authenticated TLS\n1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At\nstartup, each boot stage is measured into TPM PCRs. The node can only decrypt\nits LUKS-sealed partitions after the cloud has verified a TPM quote and\nreleased a one-time unlock key. This ensures that rogue or tampered devices\nremain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber\nand Dilithium to provide post-quantum resilience. Meanwhile, end-to-end\nencryption and integrity hashes safeguard extracted audio features. Signed,\nrollback-protected AI models and tamper-responsive sensors harden firmware and\nhardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive\nsealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum\ncipher and an encrypted cloud replica. Finally, we set out a plan for\nevaluating the physical and logical security of the proposed protocol."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection",
        "author": "Yihao Guo, Haocheng Bian, Liutong Zhou, Ze Wang, Zhaoyi Zhang, Francois Kawala, Milan Dean, Ian Fischer, Yuantao Peng, Noyan Tokgozoglu, Ivan Barrientos, Riyaaz Shaik, Rachel Li, Chandru Venkataraman, Reza Shifteh Far, Moses Pawar, Venkat Sundaranatha, Michael Xu, and Frank Chu",
        "link": "http://arxiv.org/abs/2509.14622v1",
        "abstract": "With the deployment of Large Language Models (LLMs) in interactive\napplications, online malicious intent detection has become increasingly\ncritical. However, existing approaches fall short of handling diverse and\ncomplex user queries in real time. To address these challenges, we introduce\nADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework\nfor robust and efficient online malicious intent detection. In the training\nstage, a high-capacity teacher model is trained on adversarially perturbed,\nretrieval-augmented inputs to learn robust decision boundaries over diverse and\ncomplex user queries. In the inference stage, a distillation scheduler\ntransfers the teacher's knowledge into a compact student model, with a\ncontinually updated knowledge base collected online. At deployment, the compact\nstudent model leverages top-K similar safety exemplars retrieved from the\nonline-updated knowledge base to enable both online and real-time malicious\nquery detection. Evaluations across ten safety benchmarks demonstrate that\nADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's\nperformance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on\nout-of-distribution detection, while simultaneously delivering up to 5.6x lower\nlatency at 300 queries per second (QPS) in real-time applications."
    },
    {
        "date": "2025-09",
        "title": "Threats and Security Strategies for IoMT Infusion Pumps",
        "author": "Ramazan Yener, Muhammad Hassan, and Masooda Bashir",
        "link": "http://arxiv.org/abs/2509.14604v1",
        "abstract": "The integration of the Internet of Medical Things (IoMT) into healthcare\nsystems has transformed patient care by enabling real-time monitoring, enhanced\ndiagnostics, and enhanced operational efficiency. However, this increased\nconnectivity has also expanded the attack surface for cybercriminals, raising\nsignificant cybersecurity and privacy concerns. This study focuses on the\ncybersecurity vulnerabilities of IoMT infusion pumps, which are critical\ndevices in modern healthcare. Through a targeted literature review of the past\nfive years, we analyzed seven current studies from a pool of 132 papers to\nidentify security vulnerabilities. Our findings indicate that infusion pumps\nface vulnerabilities such as device-level flaws, authentication and access\ncontrol issues, network and communication weaknesses, data security and privacy\nrisks, and operational or organizational challenges that can expose them to\nlateral attacks within healthcare networks. Our analysis synthesizes findings\nfrom seven recent studies to clarify how and why infusion pumps remain\nvulnerable in each of these areas. By categorizing the security gaps, we\nhighlight critical risk patterns and their implications. This work underscores\nthe scope of the issue and provides a structured understanding that is valuable\nfor healthcare IT professionals and device manufacturers. Ultimately, the\nfindings can inform the development of targeted, proactive security strategies\nto better safeguard infusion pumps and protect patient well-being."
    },
    {
        "date": "2025-09",
        "title": "What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System",
        "author": "Johnny So, Michael Ferdman, and Nick Nikiforakis",
        "link": "http://arxiv.org/abs/2509.14583v1",
        "abstract": "The web continues to grow, but dependency-monitoring tools and standards for\nresource integrity lag behind. Currently, there exists no robust method to\nverify the integrity of web resources, much less in a generalizable yet\nperformant manner, and supply chains remain one of the most targeted parts of\nthe attack surface of web applications.\n  In this paper, we present the design of LiMS, a transparent system to\nbootstrap link integrity guarantees in web browsing sessions with minimal\noverhead. At its core, LiMS uses a set of customizable integrity policies to\ndeclare the (un)expected properties of resources, verifies these policies, and\nenforces them for website visitors. We discuss how basic integrity policies can\nserve as building blocks for a comprehensive set of integrity policies, while\nproviding guarantees that would be sufficient to defend against recent supply\nchain attacks detailed by security industry reports. Finally, we evaluate our\nopen-sourced prototype by simulating deployments on a representative sample of\n450 domains that are diverse in ranking and category. We find that our proposal\noffers the ability to bootstrap marked security improvements with an overall\noverhead of hundreds of milliseconds on initial page loads, and negligible\noverhead on reloads, regardless of network speeds. In addition, from examining\narchived data for the sample sites, we find that several of the proposed policy\nbuilding blocks suit their dependency usage patterns, and would incur minimal\nadministrative overhead."
    },
    {
        "date": "2025-09",
        "title": "VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models",
        "author": "Huanchen Wang, Wencheng Zhang, Zhiqiang Wang, Zhicong Lu, and Yuxin Ma",
        "link": "http://arxiv.org/abs/2509.14571v1",
        "abstract": "Vision-language (VL) models have shown transformative potential across\nvarious critical domains due to their capability to comprehend multi-modal\ninformation. However, their performance frequently degrades under distribution\nshifts, making it crucial to assess and improve robustness against real-world\ndata corruption encountered in practical applications. While advancements in VL\nbenchmark datasets and data augmentation (DA) have contributed to robustness\nevaluation and improvement, there remain challenges due to a lack of in-depth\ncomprehension of model behavior as well as the need for expertise and iterative\nefforts to explore data patterns. Given the achievement of visualization in\nexplaining complex models and exploring large-scale data, understanding the\nimpact of various data corruption on VL models aligns naturally with a visual\nanalytics approach. To address these challenges, we introduce VisMoDAl, a\nvisual analytics framework designed to evaluate VL model robustness against\nvarious corruption types and identify underperformed samples to guide the\ndevelopment of effective DA strategies. Grounded in the literature review and\nexpert discussions, VisMoDAl supports multi-level analysis, ranging from\nexamining performance under specific corruptions to task-driven inspection of\nmodel behavior and corresponding data slice. Unlike conventional works,\nVisMoDAl enables users to reason about the effects of corruption on VL models,\nfacilitating both model behavior understanding and DA strategy formulation. The\nutility of our system is demonstrated through case studies and quantitative\nevaluations focused on corruption robustness in the image captioning task."
    },
    {
        "date": "2025-09",
        "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings",
        "author": "Yuhong Lu",
        "link": "http://arxiv.org/abs/2509.14383v1",
        "abstract": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings."
    },
    {
        "date": "2025-09",
        "title": "Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics",
        "author": "Benjamin Sterling, Yousef El-Laham, and M\u00f3nica F. Bugallo",
        "link": "http://arxiv.org/abs/2509.14225v1",
        "abstract": "Recent advances in generative artificial intelligence applications have\nraised new data security concerns. This paper focuses on defending diffusion\nmodels against membership inference attacks. This type of attack occurs when\nthe attacker can determine if a certain data point was used to train the model.\nAlthough diffusion models are intrinsically more resistant to membership\ninference attacks than other generative models, they are still susceptible. The\ndefense proposed here utilizes critically-damped higher-order Langevin\ndynamics, which introduces several auxiliary variables and a joint diffusion\nprocess along these variables. The idea is that the presence of auxiliary\nvariables mixes external randomness that helps to corrupt sensitive input data\nearlier on in the diffusion process. This concept is theoretically investigated\nand validated on a toy dataset and a speech dataset using the Area Under the\nReceiver Operating Characteristic (AUROC) curves and the FID metric."
    },
    {
        "date": "2025-09",
        "title": "Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain",
        "author": "Shengbo Wang, and Nian Si",
        "link": "http://arxiv.org/abs/2509.14203v1",
        "abstract": "Learning and optimal control under robust Markov decision processes (MDPs)\nhave received increasing attention, yet most existing theory, algorithms, and\napplications focus on finite-horizon or discounted models. The average-reward\nformulation, while natural in many operations research and management contexts,\nremains underexplored. This is primarily because the dynamic programming\nfoundations are technically challenging and only partially understood, with\nseveral fundamental questions remaining open. This paper steps toward a general\nframework for average-reward robust MDPs by analyzing the constant-gain\nsetting. We study the average-reward robust control problem with possible\ninformation asymmetries between the controller and an S-rectangular adversary.\nOur analysis centers on the constant-gain robust Bellman equation, examining\nboth the existence of solutions and their relationship to the optimal average\nreward. Specifically, we identify when solutions to the robust Bellman equation\ncharacterize the optimal average reward and stationary policies, and we provide\nsufficient conditions ensuring solutions' existence. These findings expand the\ndynamic programming theory for average-reward robust MDPs and lay a foundation\nfor robust dynamic decision making under long-run average criteria in\noperational environments."
    },
    {
        "date": "2025-09",
        "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning",
        "author": "Ziyuan Chen, Zhenghui Zhao, Zhangye Han, Miancan Liu, Xianhang Ye, Yiqing Li, Hongbo Min, Jinkui Ren, Xiantao Zhang, and Guitao Cao",
        "link": "http://arxiv.org/abs/2509.14172v1",
        "abstract": "With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps."
    },
    {
        "date": "2025-09",
        "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
        "author": "V\u00edctor Mayoral-Vilches",
        "link": "http://arxiv.org/abs/2509.14139v1",
        "abstract": "We present a systematic security assessment of the Unitree G1 humanoid\nshowing it operates simultaneously as a covert surveillance node and can be\npurposed as an active cyber operations platform. Partial reverse engineering of\nUnitree's proprietary FMX encryption reveal a static Blowfish-ECB layer and a\npredictable LCG mask-enabled inspection of the system's otherwise sophisticated\nsecurity architecture, the most mature we have observed in commercial robotics.\nTwo empirical case studies expose the critical risk of this humanoid robot: (a)\nthe robot functions as a trojan horse, continuously exfiltrating multi-modal\nsensor and service-state telemetry to 43.175.228.18:17883 and\n43.175.229.18:17883 every 300 seconds without operator notice, creating\nviolations of GDPR Articles 6 and 13; (b) a resident Cybersecurity AI (CAI)\nagent can pivot from reconnaissance to offensive preparation against any\ntarget, such as the manufacturer's cloud control plane, demonstrating\nescalation from passive monitoring to active counter-operations. These findings\nargue for adaptive CAI-powered defenses as humanoids move into critical\ninfrastructure, contributing the empirical evidence needed to shape future\nsecurity standards for physical-cyber convergence systems."
    },
    {
        "date": "2025-09",
        "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection",
        "author": "Sara Concas, Simone Maurizio La Cava, Andrea Panzino, Ester Masala, Giulia Orr\u00f9, and Gian Luca Marcialis",
        "link": "http://arxiv.org/abs/2509.14120v1",
        "abstract": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations."
    },
    {
        "date": "2025-09",
        "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments",
        "author": "Tamara R. Lenhard, Andreas Weinmann, and Tobias Koch",
        "link": "http://arxiv.org/abs/2509.14012v1",
        "abstract": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline."
    },
    {
        "date": "2025-09",
        "title": "Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response",
        "author": "Ozer Ozturk, Busra Buyuktanir, Gozde Karatas Baydogmus, and Kazim Yildiz",
        "link": "http://arxiv.org/abs/2509.13987v1",
        "abstract": "Machine learning models used for distributed architectures consisting of\nservers and clients require large amounts of data to achieve high accuracy.\nData obtained from clients are collected on a central server for model\ntraining. However, storing data on a central server raises concerns about\nsecurity and privacy. To address this issue, a federated learning architecture\nhas been proposed. In federated learning, each client trains a local model\nusing its own data. The trained models are periodically transmitted to the\ncentral server. The server then combines the received models using federated\naggregation algorithms to obtain a global model. This global model is\ndistributed back to the clients, and the process continues in a cyclical\nmanner. Although preventing data from leaving the clients enhances security,\ncertain concerns still remain. Attackers can perform inference attacks on the\nobtained models to approximate the training dataset, potentially causing data\nleakage. In this study, differential privacy was applied to address the\naforementioned security vulnerability, and a performance analysis was\nconducted. The Data-Unaware Classification Based on Association (duCBA)\nalgorithm was used as the federated aggregation method. Differential privacy\nwas implemented on the data using the Randomized Response technique, and the\ntrade-off between security and performance was examined under different epsilon\nvalues. As the epsilon value decreased, the model accuracy declined, and class\nprediction imbalances were observed. This indicates that higher levels of\nprivacy do not always lead to practical outcomes and that the balance between\nsecurity and performance must be carefully considered."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification",
        "author": "Wenkui Yang, Jie Cao, Junxian Duan, and Ran He",
        "link": "http://arxiv.org/abs/2509.13922v1",
        "abstract": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow."
    },
    {
        "date": "2025-09",
        "title": "A Survey and Evaluation Framework for Secure DNS Resolution",
        "author": "Ali Sadeghi Jahromi, AbdelRahman Abdou, and Paul C. van Oorschot",
        "link": "http://arxiv.org/abs/2509.13797v1",
        "abstract": "Since security was not among the original design goals of the Domain Name\nSystem (herein called Vanilla DNS), many secure DNS schemes have been proposed\nto enhance the security and privacy of the DNS resolution process. Some\nproposed schemes aim to replace the existing DNS infrastructure entirely, but\nnone have succeeded in doing so. In parallel, numerous schemes focus on\nimproving DNS security without modifying its fundamental two-stage structure.\nThese efforts highlight the feasibility of addressing DNS security as two\ndistinct but compatible stages. We survey DNS resolution process attacks and\nthreats and develop a comprehensive threat model and attack taxonomy for their\nsystematic categorization. This analysis results in the formulation of 14\ndesirable security, privacy, and availability properties to mitigate the\nidentified threats. Using these properties, we develop an objective evaluation\nframework and apply it to comparatively analyze 12 secure DNS schemes surveyed\nin this work that aim to augment the properties of the DNS resolution process.\nOur evaluation reveals that no single scheme provides ideal protection across\nthe entire resolution path. Instead, the schemes tend to address a subset of\nproperties specific to individual stages. Since these schemes targeting\ndifferent stages of DNS resolution are complementary and can operate together,\ncombining compatible schemes offers a practical and effective approach to\nachieving comprehensive security in the DNS resolution process."
    },
    {
        "date": "2025-09",
        "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation",
        "author": "Inder Pal Singh, Nidhal Eddine Chenni, Abd El Rahman Shabayek, Arunkumar Rathinam, and Djamila Aouada",
        "link": "http://arxiv.org/abs/2509.13792v1",
        "abstract": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments."
    },
    {
        "date": "2025-09",
        "title": "GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?",
        "author": "Amena Amro, and Manar H. Alalfi",
        "link": "http://arxiv.org/abs/2509.13650v1",
        "abstract": "As software development practices increasingly adopt AI-powered tools,\nensuring that such tools can support secure coding has become critical. This\nstudy evaluates the effectiveness of GitHub Copilot's recently introduced code\nreview feature in detecting security vulnerabilities. Using a curated set of\nlabeled vulnerable code samples drawn from diverse open-source projects\nspanning multiple programming languages and application domains, we\nsystematically assessed Copilot's ability to identify and provide feedback on\ncommon security flaws. Contrary to expectations, our results reveal that\nCopilot's code review frequently fails to detect critical vulnerabilities such\nas SQL injection, cross-site scripting (XSS), and insecure deserialization.\nInstead, its feedback primarily addresses low-severity issues, such as coding\nstyle and typographical errors. These findings expose a significant gap between\nthe perceived capabilities of AI-assisted code review and its actual\neffectiveness in supporting secure development practices. Our results highlight\nthe continued necessity of dedicated security tools and manual code audits to\nensure robust software security."
    },
    {
        "date": "2025-09",
        "title": "Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs",
        "author": "Md Bokhtiar Al Zami, Md Raihan Uddin, and Dinh C. Nguyen",
        "link": "http://arxiv.org/abs/2509.13634v1",
        "abstract": "Federated learning (FL) has gained popularity as a privacy-preserving method\nof training machine learning models on decentralized networks. However to\nensure reliable operation of UAV-assisted FL systems, issues like as excessive\nenergy consumption, communication inefficiencies, and security vulnerabilities\nmust be solved. This paper proposes an innovative framework that integrates\nDigital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to\ntackle these challenges. UAVs act as mobile base stations, allowing scattered\ndevices to train FL models locally and upload model updates for aggregation. By\nincorporating DT technology, our approach enables real-time system monitoring\nand predictive maintenance, improving UAV network efficiency. Additionally,\nZero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification\nwithout exposing sensitive data. To optimize energy efficiency and resource\nmanagement, we introduce a dynamic allocation strategy that adjusts UAV flight\npaths, transmission power, and processing rates based on network conditions.\nUsing block coordinate descent and convex optimization techniques, our method\nsignificantly reduces system energy consumption by up to 29.6% compared to\nconventional FL approaches. Simulation results demonstrate improved learning\nperformance, security, and scalability, positioning this framework as a\npromising solution for next-generation UAV-based intelligent networks."
    },
    {
        "date": "2025-09",
        "title": "SAMIR, an efficient registration framework via robust feature learning from SAM",
        "author": "Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, and Xiang Chen",
        "link": "http://arxiv.org/abs/2509.13629v1",
        "abstract": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper."
    },
    {
        "date": "2025-09",
        "title": "Secure, Scalable and Privacy Aware Data Strategy in Cloud",
        "author": "Vijay Kumar Butte, and Sujata Butte",
        "link": "http://arxiv.org/abs/2509.13627v1",
        "abstract": "The enterprises today are faced with the tough challenge of processing,\nstoring large amounts of data in a secure, scalable manner and enabling\ndecision makers to make quick, informed data driven decisions. This paper\naddresses this challenge and develops an effective enterprise data strategy in\nthe cloud. Various components of an effective data strategy are discussed and\narchitectures addressing security, scalability and privacy aspects are\nprovided."
    },
    {
        "date": "2025-09",
        "title": "Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents",
        "author": "Abhishek Goswami",
        "link": "http://arxiv.org/abs/2509.13597v1",
        "abstract": "Autonomous LLM agents can issue thousands of API calls per hour without human\noversight. OAuth 2.0 assumes deterministic clients, but in agentic settings\nstochastic reasoning, prompt injection, or multi-agent orchestration can\nsilently expand privileges.\n  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each\nagent's action to verifiable user intent and, optionally, to a specific\nworkflow step. A-JWT carries an agent's identity as a one-way checksum hash\nderived from its prompt, tools and configuration, and a chained delegation\nassertion to prove which downstream agent may execute a given task, and\nper-agent proof-of-possession keys to prevent replay and in-process\nimpersonation. We define a new authorization mechanism and add a lightweight\nclient shim library that self-verifies code at run time, mints intent tokens,\ntracks workflow steps and derives keys, thus enabling secure agent identity and\nseparation even within a single process.\n  We illustrate a comprehensive threat model for agentic applications,\nimplement a Python proof-of-concept and show functional blocking of\nscope-violating requests, replay, impersonation, and prompt-injection pathways\nwith sub-millisecond overhead on commodity hardware. The design aligns with\nongoing OAuth agent discussions and offers a drop-in path toward zero-trust\nguarantees for agentic applications. A comprehensive performance and security\nevaluation with experimental results will appear in our forthcoming journal\npublication"
    },
    {
        "date": "2025-09",
        "title": "GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle",
        "author": "Mengxiao Wang, and Guofei Gu",
        "link": "http://arxiv.org/abs/2509.13561v1",
        "abstract": "Progressive Web App (PWA) installation is critical for integrating web and\nmobile app functionalities, offering a seamless user experience. However,\nensuring the security of the PWA installation lifecycle is essential for\nmaintaining user trust and privacy. This paper introduces the GUARDIANPWA\nframework, a comprehensive approach to analyzing the PWA installation mechanism\nbased on the CIA security principles (Confidentiality, Integrity, and\nAvailability) and identifying areas where browser vendors fail to comply with\nthese principles. Our study revealed 203 instances of non-compliance with\nsecurity principles, highlighting how these irregularities in the PWA\ninstallation lifecycle can lead to potential violations of user privacy. For\ninstance, in Firefox, PWAs installed in private mode incorrectly appear in\nnormal mode, risking user confidentiality. Additionally, 29,465 PWAs are at\nrisk because Samsung Internet does not display origins when PWAs navigate to\nthird-party websites, undermining integrity. These findings were reported to\nbrowser vendors, leading to Firefox acknowledging four issues, resolving one,\nand planning to resolve two others. GUARDIANPWA supports developers by\nanalyzing PWA manifest files for syntactic and semantic correctness, offering\nactionable recommendations, and helping to create PWAs that align with security\nbest practices. By using GUARDIANPWA, developers and users can address critical\nsecurity gaps and enhance compliance with CIA principles throughout the PWA\ninstallation lifecycle."
    },
    {
        "date": "2025-09",
        "title": "AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering",
        "author": "Onat Gungor, Roshan Sood, Harold Wang, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2509.13514v1",
        "abstract": "Large Language Models (LLMs) have recently demonstrated strong potential for\ncybersecurity question answering (QA), supporting decision-making in real-time\nthreat detection and response workflows. However, their substantial\ncomputational demands pose significant challenges for deployment on\nresource-constrained edge devices. Quantization, a widely adopted model\ncompression technique, can alleviate these constraints. Nevertheless,\nquantization may degrade model accuracy and increase susceptibility to\nadversarial attacks. Fine-tuning offers a potential means to mitigate these\nlimitations, but its effectiveness when combined with quantization remains\ninsufficiently explored. Hence, it is essential to understand the trade-offs\namong accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation\nframework designed to benchmark several state-of-the-art small LLMs under four\ndistinct configurations: base, quantized-only, fine-tuned, and fine-tuned\ncombined with quantization, specifically for cybersecurity QA. Our results\ndemonstrate that quantization alone yields the lowest accuracy and robustness\ndespite improving efficiency. In contrast, combining quantization with\nfine-tuning enhances both LLM robustness and predictive performance, achieving\nan optimal balance of accuracy, robustness, and efficiency. These findings\nhighlight the critical need for quantization-aware, robustness-preserving\nfine-tuning methodologies to enable the robust and efficient deployment of LLMs\nfor cybersecurity QA."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving",
        "author": "Artem Savkin, Thomas Lapotre, Kevin Strauss, Uzair Akbar, and Federico Tombari",
        "link": "http://arxiv.org/abs/2509.13507v1",
        "abstract": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation."
    },
    {
        "date": "2025-09",
        "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization",
        "author": "Yujia Lin, and Nicholas Evans",
        "link": "http://arxiv.org/abs/2509.13474v1",
        "abstract": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods."
    },
    {
        "date": "2025-09",
        "title": "A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks",
        "author": "S M Asif Hossain, Ruksat Khan Shayoni, Mohd Ruhul Ameen, Akif Islam, M. F. Mridha, and Jungpil Shin",
        "link": "http://arxiv.org/abs/2509.14285v1",
        "abstract": "Prompt injection attacks represent a major vulnerability in Large Language\nModel (LLM) deployments, where malicious instructions embedded in user inputs\ncan override system prompts and induce unintended behaviors. This paper\npresents a novel multi-agent defense framework that employs specialized LLM\nagents in coordinated pipelines to detect and neutralize prompt injection\nattacks in real-time. We evaluate our approach using two distinct\narchitectures: a sequential chain-of-agents pipeline and a hierarchical\ncoordinator-based system. Our comprehensive evaluation on 55 unique prompt\ninjection attacks, grouped into 8 categories and totaling 400 attack instances\nacross two LLM platforms (ChatGLM and Llama2), demonstrates significant\nsecurity improvements. Without defense mechanisms, baseline Attack Success\nRates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent\npipeline achieved 100% mitigation, reducing ASR to 0% across all tested\nscenarios. The framework demonstrates robustness across multiple attack\ncategories including direct overrides, code execution attempts, data\nexfiltration, and obfuscation techniques, while maintaining system\nfunctionality for legitimate queries."
    },
    {
        "date": "2025-09",
        "title": "LIGHT-HIDS: A Lightweight and Effective Machine Learning-Based Framework for Robust Host Intrusion Detection",
        "author": "Onat Gungor, Ishaan Kale, Jiasheng Zhou, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2509.13464v1",
        "abstract": "The expansion of edge computing has increased the attack surface, creating an\nurgent need for robust, real-time machine learning (ML)-based host intrusion\ndetection systems (HIDS) that balance accuracy and efficiency. In such\nsettings, inference latency poses a critical security risk, as delays may\nprovide exploitable opportunities for attackers. However, many state-of-the-art\nML-based HIDS solutions rely on computationally intensive architectures with\nhigh inference costs, limiting their practical deployment. This paper proposes\nLIGHT-HIDS, a lightweight machine learning framework that combines a compressed\nneural network feature extractor trained via Deep Support Vector Data\nDescription (DeepSVDD) with an efficient novelty detection model. This hybrid\napproach enables the learning of compact, meaningful representations of normal\nsystem call behavior for accurate anomaly detection. Experimental results on\nmultiple datasets demonstrate that LIGHT-HIDS consistently enhances detection\naccuracy while reducing inference time by up to 75x compared to\nstate-of-the-art methods. These findings highlight its effectiveness and\nscalability as a machine learning-based solution for real-time host intrusion\ndetection."
    },
    {
        "date": "2025-09",
        "title": "Defining Security in Quantum Key Distribution",
        "author": "Carla Ferradini, Martin Sandfuchs, Ramona Wolf, and Renato Renner",
        "link": "http://arxiv.org/abs/2509.13405v1",
        "abstract": "The security of quantum key distribution (QKD) is quantified by a parameter\n$\\varepsilon>0$, which -- under well-defined physical assumptions -- can be\nbounded explicitly. This contrasts with computationally secure schemes, where\nsecurity claims are only asymptotic (i.e., under standard complexity\nassumptions, one only knows that $\\varepsilon \\to 0$ as the key size grows, but\nhas no explicit bound). Here we explain the definition and interpretation of\n$\\varepsilon$-security. Adopting an axiomatic approach, we show that\n$\\varepsilon$ can be understood as the maximum probability of a security\nfailure. Finally, we review and address several criticisms of this definition\nthat have appeared in the literature."
    },
    {
        "date": "2025-09",
        "title": "JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks",
        "author": "Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, and Zhenghao Tang",
        "link": "http://arxiv.org/abs/2509.13266v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance across\nvarious applications, yet they are vulnerable to sophisticated adversarial\nattacks, particularly node injection attacks. The success of such attacks\nheavily relies on their stealthiness, the ability to blend in with the original\ngraph and evade detection. However, existing methods often achieve stealthiness\nby relying on indirect proxy metrics, lacking consideration for the fundamental\ncharacteristics of the injected content, or focusing only on imitating local\nstructures, which leads to the problem of local myopia. To overcome these\nlimitations, we propose a dual-constraint stealthy node injection framework,\ncalled Joint Alignment of Nodal and Universal Structures (JANUS). At the local\nlevel, we introduce a local feature manifold alignment strategy to achieve\ngeometric consistency in the feature space. At the global level, we incorporate\nstructured latent variables and maximize the mutual information with the\ngenerated structures, ensuring the injected structures are consistent with the\nsemantic patterns of the original graph. We model the injection attack as a\nsequential decision process, which is optimized by a reinforcement learning\nagent. Experiments on multiple standard datasets demonstrate that the JANUS\nframework significantly outperforms existing methods in terms of both attack\neffectiveness and stealthiness."
    },
    {
        "date": "2025-09",
        "title": "On the Out-of-Distribution Backdoor Attack for Federated Learning",
        "author": "Jiahao Xu, Zikai Zhang, and Rui Hu",
        "link": "http://arxiv.org/abs/2509.13219v1",
        "abstract": "Traditional backdoor attacks in federated learning (FL) operate within\nconstrained attack scenarios, as they depend on visible triggers and require\nphysical modifications to the target object, which limits their practicality.\nTo address this limitation, we introduce a novel backdoor attack prototype for\nFL called the out-of-distribution (OOD) backdoor attack ($\\mathtt{OBA}$), which\nuses OOD data as both poisoned samples and triggers simultaneously. Our\napproach significantly broadens the scope of backdoor attack scenarios in FL.\nTo improve the stealthiness of $\\mathtt{OBA}$, we propose $\\mathtt{SoDa}$,\nwhich regularizes both the magnitude and direction of malicious local models\nduring local training, aligning them closely with their benign versions to\nevade detection. Empirical results demonstrate that $\\mathtt{OBA}$ effectively\ncircumvents state-of-the-art defenses while maintaining high accuracy on the\nmain task.\n  To address this security vulnerability in the FL system, we introduce\n$\\mathtt{BNGuard}$, a new server-side defense method tailored against\n$\\mathtt{SoDa}$. $\\mathtt{BNGuard}$ leverages the observation that OOD data\ncauses significant deviations in the running statistics of batch normalization\nlayers. This allows $\\mathtt{BNGuard}$ to identify malicious model updates and\nexclude them from aggregation, thereby enhancing the backdoor robustness of FL.\nExtensive experiments across various settings show the effectiveness of\n$\\mathtt{BNGuard}$ on defending against $\\mathtt{SoDa}$. The code is available\nat https://github.com/JiiahaoXU/SoDa-BNGuard."
    },
    {
        "date": "2025-09",
        "title": "FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data",
        "author": "J. Cha, J. Lee, J. Cho, and J. Shin",
        "link": "http://arxiv.org/abs/2509.13218v1",
        "abstract": "Imbalanced and small data regimes are pervasive in domains such as rare\ndisease imaging, genomics, and disaster response, where labeled samples are\nscarce and naive augmentation often introduces artifacts. Existing solutions\nsuch as oversampling, focal loss, or meta-weighting address isolated aspects of\nthis challenge but remain fragile or complex. We introduce FOSSIL (Flexible\nOptimization via Sample Sensitive Importance Learning), a unified weighting\nframework that seamlessly integrates class imbalance correction,\ndifficulty-aware curricula, augmentation penalties, and warmup dynamics into a\nsingle interpretable formula. Unlike prior heuristics, the proposed framework\nprovides regret-based theoretical guarantees and achieves consistent empirical\ngains over ERM, curriculum, and meta-weighting baselines on synthetic and\nreal-world datasets, while requiring no architectural changes."
    },
    {
        "date": "2025-09",
        "title": "Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning",
        "author": "Ali Al-kuwari, Noureldin Mohamed, Saif Al-kuwari, Ahmed Farouk, and Bikash K. Behera",
        "link": "http://arxiv.org/abs/2509.14282v1",
        "abstract": "The emergence of quantum computing poses significant risks to the security of\nmodern communication networks as it breaks today's public-key cryptographic\nalgorithms. Quantum Key Distribution (QKD) offers a promising solution by\nharnessing the principles of quantum mechanics to establish secure keys.\nHowever, practical QKD implementations remain vulnerable to hardware\nimperfections and advanced attacks such as Photon Number Splitting and\nTrojan-Horse attacks. In this work, we investigate the potential of using\nquantum machine learning (QML) to detect popular QKD attacks. In particular, we\npropose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the\ndetection of common QKD attacks. By combining quantum-enhanced learning with\nclassical deep learning, the model captures complex temporal patterns in QKD\ndata, improving detection accuracy. To evaluate the proposed model, we\nintroduce a realistic QKD dataset simulating normal QKD operations along with\nseven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),\nTrojan-Horse attacks Random Number Generator (RNG), Detector Blinding,\nWavelength-dependent Trojan Horse, and Combined attacks. The dataset includes\nquantum security metrics such as Quantum Bit Error Rate (QBER), measurement\nentropy, signal and decoy loss rates, and time-based metrics, ensuring an\naccurate representation of real-world conditions. Our results demonstrate\npromising performance of the quantum machine learning approach compared to\ntraditional classical machine learning models, highlighting the potential of\nhybrid techniques to enhance the security of future quantum communication\nnetworks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\\%\nafter 50 training epochs, outperforming classical deep learning models such as\nLSTM, and CNN."
    },
    {
        "date": "2025-09",
        "title": "Digital Sovereignty Control Framework for Military AI-based Cyber Security",
        "author": "Clara Maathuis, and Kasper Cools",
        "link": "http://arxiv.org/abs/2509.13072v1",
        "abstract": "In today's evolving threat landscape, ensuring digital sovereignty has become\nmandatory for military organizations, especially given their increased\ndevelopment and investment in AI-driven cyber security solutions. To this end,\na multi-angled framework is proposed in this article in order to define and\nassess digital sovereign control of data and AI-based models for military cyber\nsecurity. This framework focuses on aspects such as context, autonomy,\nstakeholder involvement, and mitigation of risks in this domain. Grounded on\nthe concepts of digital sovereignty and data sovereignty, the framework aims to\nprotect sensitive defence assets against threats such as unauthorized access,\nransomware, and supply-chain attacks. This approach reflects the multifaceted\nnature of digital sovereignty by preserving operational autonomy, assuring\nsecurity and safety, securing privacy, and fostering ethical compliance of both\nmilitary systems and decision-makers. At the same time, the framework addresses\ninteroperability challenges among allied forces, strategic and legal\nconsiderations, and the integration of emerging technologies by considering a\nmultidisciplinary approach that enhances the resilience and preservation of\ncontrol over (critical) digital assets. This is done by adopting a design\noriented research where systematic literature review is merged with critical\nthinking and analysis of field incidents in order to assure the effectivity and\nrealism of the framework proposed."
    },
    {
        "date": "2025-09",
        "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation",
        "author": "Qianqi Lu, Yuxiang Xie, Jing Zhang, Shiwei Zou, Yan Chen, and Xidao Luan",
        "link": "http://arxiv.org/abs/2509.13070v1",
        "abstract": "Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages."
    },
    {
        "date": "2025-09",
        "title": "MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data",
        "author": "Eyal German, Daniel Samira, Yuval Elovici, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2509.13046v1",
        "abstract": "Synthetic data generation plays an important role in enabling data sharing,\nparticularly in sensitive domains like healthcare and finance. Recent advances\nin diffusion models have made it possible to generate realistic, high-quality\ntabular data, but they may also memorize training records and leak sensitive\ninformation. Membership inference attacks (MIAs) exploit this vulnerability by\ndetermining whether a record was used in training. While MIAs have been studied\nin images and text, their use against tabular diffusion models remains\nunderexplored despite the unique risks of structured attributes and limited\nrecord diversity. In this paper, we introduce MIAEPT, Membership Inference\nAttack via Error Prediction for Tabular Data, a novel black-box attack\nspecifically designed to target tabular diffusion models. MIA-EPT constructs\nerrorbased feature vectors by masking and reconstructing attributes of target\nrecords, disclosing membership signals based on how well these attributes are\npredicted. MIA-EPT operates without access to the internal components of the\ngenerative model, relying only on its synthetic data output, and was shown to\ngeneralize across multiple state-of-the-art diffusion models. We validate\nMIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up\nto 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST\n2025 competition conditions, MIA-EPT achieved second place in the Black-box\nMulti-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our\nmethod can uncover substantial membership leakage in synthetic tabular data,\nchallenging the assumption that synthetic data is inherently\nprivacy-preserving. Our code is publicly available at\nhttps://github.com/eyalgerman/MIA-EPT."
    },
    {
        "date": "2025-09",
        "title": "BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning",
        "author": "Honghong Zeng, Jiong Lou, Zhe Wang, Hefeng Zhou, Chentao Wu, Wei Zhao, and Jie Li",
        "link": "http://arxiv.org/abs/2509.12964v1",
        "abstract": "Prototype-based federated learning (PFL) has emerged as a promising paradigm\nto address data heterogeneity problems in federated learning, as it leverages\nmean feature vectors as prototypes to enhance model generalization. However,\nits robustness against backdoor attacks remains largely unexplored. In this\npaper, we identify that PFL is inherently resistant to existing backdoor\nattacks due to its unique prototype learning mechanism and local data\nheterogeneity. To further explore the security of PFL, we propose BAPFL, the\nfirst backdoor attack method specifically designed for PFL frameworks. BAPFL\nintegrates a prototype poisoning strategy with a trigger optimization\nmechanism. The prototype poisoning strategy manipulates the trajectories of\nglobal prototypes to mislead the prototype training of benign clients, pushing\ntheir local prototypes of clean samples away from the prototypes of\ntrigger-embedded samples. Meanwhile, the trigger optimization mechanism learns\na unique and stealthy trigger for each potential target label, and guides the\nprototypes of trigger-embedded samples to align closely with the global\nprototype of the target label. Experimental results across multiple datasets\nand PFL variants demonstrate that BAPFL achieves a 35\\%-75\\% improvement in\nattack success rate compared to traditional backdoor attacks, while preserving\nmain task accuracy. These results highlight the effectiveness, stealthiness,\nand adaptability of BAPFL in PFL."
    },
    {
        "date": "2025-09",
        "title": "Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization",
        "author": "Robert Tjarko Lange, Qi Sun, Aaditya Prasad, Maxence Faldor, Yujin Tang, and David Ha",
        "link": "http://arxiv.org/abs/2509.14279v1",
        "abstract": "Recent advances in large language models (LLMs) demonstrate their\neffectiveness in scaling test-time compute for software engineering tasks.\nHowever, these approaches often focus on high-level solutions, with limited\nattention to optimizing low-level CUDA kernel implementations. Additionally,\nexisting kernel generation benchmarks suffer from exploitable loopholes and\ninsufficient diversity in testing conditions, hindering true generalization\nassessment. To address these limitations, we introduce robust-kbench, a new\nbenchmark for rigorous evaluation of kernel performance and correctness across\nvaried scenarios. Furthermore, we present a comprehensive agentic framework\nthat automates CUDA kernel discovery, verification, and optimization. This\npipeline enables frontier LLMs to translate torch code to CUDA kernels and\niteratively improve their runtime within our robust evaluation setting. Our\nsequential workflow first translates PyTorch code into equivalent CUDA kernels.\nIt then optimizes their runtime using a novel evolutionary meta-generation\nprocedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for\ncorrectness and efficient filtering. Evaluated on robust-kbench, our approach\nproduces CUDA kernels outperforming torch implementations for practical\napplications, including forward and backward passes. It can fuse operations and\ndeploy various runtime optimization strategies. The verifier workflow\naccurately classifies incorrect kernels, enhancing hardware verification\nefficiency."
    },
    {
        "date": "2025-09",
        "title": "Sy-FAR: Symmetry-based Fair Adversarial Robustness",
        "author": "Haneen Najjar, Eyal Ronen, and Mahmood Sharif",
        "link": "http://arxiv.org/abs/2509.12939v1",
        "abstract": "Security-critical machine-learning (ML) systems, such as face-recognition\nsystems, are susceptible to adversarial examples, including real-world\nphysically realizable attacks. Various means to boost ML's adversarial\nrobustness have been proposed; however, they typically induce unfair\nrobustness: It is often easier to attack from certain classes or groups than\nfrom others. Several techniques have been developed to improve adversarial\nrobustness while seeking perfect fairness between classes. Yet, prior work has\nfocused on settings where security and fairness are less critical. Our insight\nis that achieving perfect parity in realistic fairness-critical tasks, such as\nface recognition, is often infeasible -- some classes may be highly similar,\nleading to more misclassifications between them. Instead, we suggest that\nseeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful\nas from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable\nbecause class resemblance is a symmetric relation in most domains.\nAdditionally, as we prove theoretically, symmetry between individuals induces\nsymmetry between any set of sub-groups, in contrast to other fairness notions\nwhere group-fairness is often elusive. We develop Sy-FAR, a technique to\nencourage symmetry while also optimizing adversarial robustness and extensively\nevaluate it using five datasets, with three model architectures, including\nagainst targeted and untargeted realistic attacks. The results show Sy-FAR\nsignificantly improves fair adversarial robustness compared to state-of-the-art\nmethods. Moreover, we find that Sy-FAR is faster and more consistent across\nruns. Notably, Sy-FAR also ameliorates another type of unfairness we discover\nin this work -- target classes that adversarial examples are likely to be\nclassified into become significantly less vulnerable after inducing symmetry."
    },
    {
        "date": "2025-09",
        "title": "A Graph-Based Approach to Alert Contextualisation in Security Operations Centres",
        "author": "Magnus Wiik Eckhoff, Peter Marius Flydal, Siem Peters, Martin Eian, Jonas Halvorsen, Vasileios Mavroeidis, and Gudmund Grov",
        "link": "http://arxiv.org/abs/2509.12923v2",
        "abstract": "Interpreting the massive volume of security alerts is a significant challenge\nin Security Operations Centres (SOCs). Effective contextualisation is\nimportant, enabling quick distinction between genuine threats and benign\nactivity to prioritise what needs further analysis. This paper proposes a\ngraph-based approach to enhance alert contextualisation in a SOC by aggregating\nalerts into graph-based alert groups, where nodes represent alerts and edges\ndenote relationships within defined time-windows. By grouping related alerts,\nwe enable analysis at a higher abstraction level, capturing attack steps more\neffectively than individual alerts. Furthermore, to show that our format is\nwell suited for downstream machine learning methods, we employ Graph Matching\nNetworks (GMNs) to correlate incoming alert groups with historical incidents,\nproviding analysts with additional insights."
    },
    {
        "date": "2025-09",
        "title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking",
        "author": "Hojat Ardi, Amir Jahanshahi, and Ali Diba",
        "link": "http://arxiv.org/abs/2509.12913v1",
        "abstract": "Aerial object tracking remains a challenging task due to scale variations,\ndynamic backgrounds, clutter, and frequent occlusions. While most existing\ntrackers emphasize spatial cues, they often overlook temporal dependencies,\nresulting in limited robustness in long-term tracking and under occlusion.\nFurthermore, correlation-based Siamese trackers are inherently constrained by\nthe linear nature of correlation operations, making them ineffective against\ncomplex, non-linear appearance changes. To address these limitations, we\nintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends\nthe SiamTPN architecture with explicit temporal modeling. Our approach\nincorporates temporal feature fusion and attention-based interactions,\nstrengthening temporal consistency and enabling richer feature representations.\nThese enhancements yield significant improvements over the baseline and achieve\nperformance competitive with state-of-the-art trackers. Crucially, despite the\nadded temporal modules, T-SiamTPN preserves computational efficiency. Deployed\non the resource-constrained Jetson Nano, the tracker runs in real time at 7.1\nFPS, demonstrating its suitability for real-world embedded applications without\nnotable runtime overhead. Experimental results highlight substantial gains:\ncompared to the baseline, T-SiamTPN improves success rate by 13.7% and\nprecision by 14.7%. These findings underscore the importance of temporal\nmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and\nefficient solution for aerial object tracking. Code is available at:\nhttps://github.com/to/be/released"
    },
    {
        "date": "2025-09",
        "title": "MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos",
        "author": "Damola Agbelese, Krishna Chaitanya, Pushpak Pati, Chaitanya Parmar, Pooya Mobadersany, Shreyas Fadnavis, Lindsey Surace, Shadi Yarandi, Louis R. Ghanem, Molly Lucas, Tommaso Mansi, Oana Gabriela Cula, Pablo F. Damasceno, and Kristopher Standish",
        "link": "http://arxiv.org/abs/2509.12772v1",
        "abstract": "Reliable uncertainty quantification (UQ) is essential in medical AI.\nEvidential Deep Learning (EDL) offers a computationally efficient way to\nquantify model uncertainty alongside predictions, unlike traditional methods\nsuch as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these\nmethods often rely on a single expert's annotations as ground truth for model\ntraining, overlooking the inter-rater variability in healthcare. To address\nthis issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates\nuncertainty estimates and predictions from multiple AI experts via EDL models\ntrained with diverse ground truths and modeling strategies. MEGAN's gating\nnetwork optimally combines predictions and uncertainties from each EDL model,\nenhancing overall prediction confidence and calibration. We extensively\nbenchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease\nseverity estimation, assessed by visual labeling of Mayo Endoscopic Subscore\n(MES), where inter-rater variability is prevalent. In large-scale prospective\nUC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%\nreduction in Expected Calibration Error (ECE) compared to existing methods.\nFurthermore, MEGAN facilitated uncertainty-guided sample stratification,\nreducing the annotation burden and potentially increasing efficiency and\nconsistency in UC trials."
    },
    {
        "date": "2025-09",
        "title": "Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models",
        "author": "Yunhan Zhao, Xiang Zheng, and Xingjun Ma",
        "link": "http://arxiv.org/abs/2509.12724v1",
        "abstract": "Despite their superb capabilities, Vision-Language Models (VLMs) have been\nshown to be vulnerable to jailbreak attacks. While recent jailbreaks have\nachieved notable progress, their effectiveness and efficiency can still be\nimproved. In this work, we reveal an interesting phenomenon: incorporating weak\ndefense into the attack pipeline can significantly enhance both the\neffectiveness and the efficiency of jailbreaks on VLMs. Building on this\ninsight, we propose Defense2Attack, a novel jailbreak method that bypasses the\nsafety guardrails of VLMs by leveraging defensive patterns to guide jailbreak\nprompt design. Specifically, Defense2Attack consists of three key components:\n(1) a visual optimizer that embeds universal adversarial perturbations with\naffirmative and encouraging semantics; (2) a textual optimizer that refines the\ninput using a defense-styled prompt; and (3) a red-team suffix generator that\nenhances the jailbreak through reinforcement fine-tuning. We empirically\nevaluate our method on four VLMs and four safety benchmarks. The results\ndemonstrate that Defense2Attack achieves superior jailbreak performance in a\nsingle attempt, outperforming state-of-the-art attack methods that often\nrequire multiple tries. Our work offers a new perspective on jailbreaking VLMs."
    },
    {
        "date": "2025-09",
        "title": "A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs",
        "author": "Kiho Lee, Jungkon Kim, Doowon Kim, and Hyoungshick Kim",
        "link": "http://arxiv.org/abs/2509.12649v1",
        "abstract": "Code-generating Large Language Models (LLMs) significantly accelerate\nsoftware development. However, their frequent generation of insecure code\npresents serious risks. We present a comprehensive evaluation of seven\nparameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial\ngains in secure code generation without compromising functionality. Our\nresearch identifies prompt-tuning as the most effective PEFT method, achieving\nan 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over\nthe 67.28% baseline. Optimizing decoding strategies through sampling\ntemperature further elevated security to 87.65%. This equates to a reduction of\napproximately 203,700 vulnerable code snippets per million generated. Moreover,\nprompt and prefix tuning increase robustness against poisoning attacks in our\nTrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502\nattack vectors. Our findings generalize across Python and Java, confirming\nprompt-tuning's consistent effectiveness. This study provides essential\ninsights and practical guidance for building more resilient software systems\nwith LLMs."
    },
    {
        "date": "2025-09",
        "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation",
        "author": "Liming Lu, Shuchao Pang, Xu Zheng, Xiang Gu, Anan Du, Yunhuai Liu, and Yongbin Zhou",
        "link": "http://arxiv.org/abs/2509.12633v1",
        "abstract": "Adversarial robustness distillation (ARD) aims to transfer both performance\nand robustness from teacher model to lightweight student model, enabling\nresilient performance on resource-constrained scenarios. Though existing ARD\napproaches enhance student model's robustness, the inevitable by-product leads\nto the degraded performance on clean examples. We summarize the causes of this\nproblem inherent in existing methods with dual-teacher framework as: 1. The\ndivergent optimization objectives of dual-teacher models, i.e., the clean and\nrobust teachers, impede effective knowledge transfer to the student model, and\n2. The iteratively generated adversarial examples during training lead to\nperformance deterioration of the robust teacher model. To address these\nchallenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key\ninnovations: a. A multi-teacher framework with contrastive push-loss alignment\nto resolve conflicts in dual-teacher optimization objectives, and b. Continuous\nadversarial retraining to maintain dynamic teacher robustness against\nperformance degradation from the varying adversarial examples. Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD\nachieves remarkable performance with an average 3.53 improvement in adversarial\ndefense rates across various attack scenarios and a 5.87 increase in clean\nsample accuracy, establishing a new benchmark for balancing model robustness\nand generalization. Our code is available at https://github.com/eminentgu/CIARD"
    },
    {
        "date": "2025-09",
        "title": "DisorientLiDAR: Physical Attacks on LiDAR-based Localization",
        "author": "Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, and Wanpeng Shao",
        "link": "http://arxiv.org/abs/2509.12595v1",
        "abstract": "Deep learning models have been shown to be susceptible to adversarial attacks\nwith visually imperceptible perturbations. Even this poses a serious security\nchallenge for the localization of self-driving cars, there has been very little\nexploration of attack on it, as most of adversarial attacks have been applied\nto 3D perception. In this work, we propose a novel adversarial attack framework\ncalled DisorientLiDAR targeting LiDAR-based localization. By\nreverse-engineering localization models (e.g., feature extraction networks),\nadversaries can identify critical keypoints and strategically remove them,\nthereby disrupting LiDAR-based localization. Our proposal is first evaluated on\nthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, and\nGeoTransformer) using the KITTI dataset. Experimental results demonstrate that\nremoving regions containing Top-K keypoints significantly degrades their\nregistration accuracy. We further validate the attack's impact on the Autoware\nautonomous driving platform, where hiding merely a few critical regions induces\nnoticeable localization drift. Finally, we extended our attacks to the physical\nworld by hiding critical regions with near-infrared absorptive materials,\nthereby successfully replicate the attack effects observed in KITTI data. This\nstep has been closer toward the realistic physical-world attack that\ndemonstrate the veracity and generality of our proposal."
    },
    {
        "date": "2025-09",
        "title": "Secure and Efficient Out-of-band Call Metadata Transmission",
        "author": "David Adei, Varun Madathil, Nithin Shyam S., and Bradley Reaves",
        "link": "http://arxiv.org/abs/2509.12582v1",
        "abstract": "The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,\nCanada, and France to combat pervasive telephone abuse has not achieved its\ngoals, partly because legacy non-VoIP infrastructure could not participate. The\nindustry solution to extend S/S broadcasts sensitive metadata of every non-VoIP\ncall in plaintext to every third party required to facilitate the system. It\nhas no mechanism to determine whether a provider's request for call data is\nappropriate, nor can it ensure that every copy of that call data is unavailable\nafter its specified expiration. It threatens subscriber privacy and provider\nconfidentiality.\n  In this paper, we present Sidecar, a distributed, privacy-preserving system\nwith tunable decentralization that securely extends S/S across all telephone\nnetwork technologies. We introduce the notion of secure out-of-band signaling\nfor telephony and formalize its system and security requirements. We then\ndesign novel, scalable protocols that realize these requirements and prove\ntheir security within the Universal Composability framework. Finally, we\ndemonstrate Sidecar's efficiency with our open-sourced reference\nimplementation. Compared to the current solution, Sidecar 1) protects the\nconfidentiality of subscriber identity and provider trade secrets, 2)\nguarantees record expiration as long as a single node handling a record is\nhonest, 3) reduces resource requirements while providing virtually identical\ncall-setup times and equivalent or better uptimes, and 4) enables secure\npay-per-use billing and integrates mechanisms to mitigate and detect\nmisbehavior. Moreover, Sidecar can be extended to provide the same security\nguarantees for arbitrary call metadata. Not only is Sidecar a superior\napproach, it is also a transformative tool to retrofit fragmented global\ntelephony and enable future improvements, such as stronger call authentication\nand Branded Calling."
    },
    {
        "date": "2025-09",
        "title": "Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design",
        "author": "Sanjeda Akter, Ibne Farabi Shihab, and Anuj Sharma",
        "link": "http://arxiv.org/abs/2509.12527v1",
        "abstract": "Large language models often produce plausible but incorrect outputs. Existing\nheuristics such as HallBayes lack formal guarantees. We develop the first\ncomprehensive theory of \\emph{information-lift certificates} under selective\nclassification. Our contributions are: (i) a PAC-Bayes \\emph{sub-gamma}\nanalysis extending beyond standard Bernstein bounds; (ii) explicit skeleton\nsensitivity theorems quantifying robustness to misspecification; (iii)\nfailure-mode guarantees under assumption violations; and (iv) a principled\nvariational method for skeleton construction. Across six datasets and multiple\nmodel families, we validate assumptions empirically, reduce abstention by\n12--15\\% at the same risk, and maintain runtime overhead below 20\\% (further\nreduced via batching)."
    },
    {
        "date": "2025-09",
        "title": "Evaluating Robustness of Vision-Language Models Under Noisy Conditions",
        "author": "Purushoth, and Alireza",
        "link": "http://arxiv.org/abs/2509.12492v1",
        "abstract": "Vision-Language Models (VLMs) have attained exceptional success across\nmultimodal tasks such as image captioning and visual question answering.\nHowever, their robustness under noisy conditions remains unfamiliar. In this\nstudy, we present a comprehensive evaluation framework to evaluate the\nperformance of several state-of-the-art VLMs under controlled perturbations,\nincluding lighting variation, motion blur, and compression artifacts. We used\nboth lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based\nsimilarity measures using sentence embeddings to quantify semantic alignment.\nOur experiments span diverse datasets, revealing key insights: (1)\ndescriptiveness of ground-truth captions significantly influences model\nperformance; (2) larger models like LLaVA excel in semantic understanding but\ndo not universally outperform smaller models; and (3) certain noise types, such\nas JPEG compression and motion blur, dramatically degrade performance across\nmodels. Our findings highlight the nuanced trade-offs between model size,\ndataset characteristics, and noise resilience, offering a standardized\nbenchmark for future robust multimodal learning."
    },
    {
        "date": "2025-09",
        "title": "Redefining Website Fingerprinting Attacks With Multiagent LLMs",
        "author": "Chuxu Song, Dheekshith Dev Manohar Mekala, Hao Wang, and Richard Martin",
        "link": "http://arxiv.org/abs/2509.12462v1",
        "abstract": "Website Fingerprinting (WFP) uses deep learning models to classify encrypted\nnetwork traffic to infer visited websites. While historically effective, prior\nmethods fail to generalize to modern web environments. Single-page applications\n(SPAs) eliminate the paradigm of websites as sets of discrete pages,\nundermining page-based classification, and traffic from scripted browsers lacks\nthe behavioral richness seen in real user sessions. Our study reveals that\nusers exhibit highly diverse behaviors even on the same website, producing\ntraffic patterns that vary significantly across individuals. This behavioral\nentropy makes WFP a harder problem than previously assumed and highlights the\nneed for larger, more diverse, and representative datasets to achieve robust\nperformance. To address this, we propose a new paradigm: we drop\nsession-boundaries in favor of contiguous traffic segments and develop a\nscalable data generation pipeline using large language models (LLM) agents.\nThese multi-agent systems coordinate decision-making and browser interaction to\nsimulate realistic, persona-driven browsing behavior at 3--5x lower cost than\nhuman collection. We evaluate nine state-of-the-art WFP models on traffic from\n20 modern websites browsed by 30 real users, and compare training performance\nacross human, scripted, and LLM-generated datasets. All models achieve under\n10\\% accuracy when trained on scripted traffic and tested on human data. In\ncontrast, LLM-generated traffic boosts accuracy into the 80\\% range,\ndemonstrating strong generalization to real-world traces. Our findings indicate\nthat for modern WFP, model performance is increasingly bottlenecked by data\nquality, and that scalable, semantically grounded synthetic traffic is\nessential for capturing the complexity of real user behavior."
    },
    {
        "date": "2025-09",
        "title": "Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks",
        "author": "Asim Waheed, Vasisht Duddu, Rui Zhang, Sebastian Szyller, and N. Asokan",
        "link": "http://arxiv.org/abs/2509.12386v1",
        "abstract": "ML models are susceptible to risks to security, privacy, and fairness.\nSeveral defenses are designed to protect against their intended risks, but can\ninadvertently affect susceptibility to other unrelated risks, known as\nunintended interactions. Several jurisdictions are preparing ML regulatory\nframeworks that require ML practitioners to assess the susceptibility of ML\nmodels to different risks. A library for valuating unintended interactions that\ncan be used by (a) practitioners to evaluate unintended interactions at scale\nprior to model deployment and (b) researchers to design defenses which do not\nsuffer from an unintended increase in unrelated risks. Ideally, such a library\nshould be i) comprehensive by including representative attacks, defenses and\nmetrics for different risks, ii) extensible to new modules due to its modular\ndesign, iii) consistent with a user-friendly API template for inputs and\noutputs, iv) applicable to evaluate previously unexplored unintended\ninteractions. We present AMULET, a Python library that covers risks to\nsecurity, privacy, and fairness, which satisfies all these requirements. AMULET\ncan be used to evaluate unexplored unintended interactions, compare\neffectiveness between defenses or attacks, and include new attacks and\ndefenses."
    },
    {
        "date": "2025-09",
        "title": "Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models",
        "author": "Gustavo Sandoval, Denys Fenchenko, and Junyao Chen",
        "link": "http://arxiv.org/abs/2509.14271v1",
        "abstract": "This paper documents early research conducted in 2022 on defending against\nprompt injection attacks in large language models, providing historical context\nfor the evolution of this critical security domain. This research focuses on\ntwo adversarial attacks against Large Language Models (LLMs): prompt injection\nand goal hijacking. We examine how to construct these attacks, test them on\nvarious LLMs, and compare their effectiveness. We propose and evaluate a novel\ndefense technique called Adversarial Fine-Tuning. Our results show that,\nwithout this defense, the attacks succeeded 31\\% of the time on GPT-3 series\nmodels. When using our Adversarial Fine-Tuning approach, attack success rates\nwere reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),\nthough we note that subsequent research has revealed limitations of\nfine-tuning-based defenses. We also find that more flexible models exhibit\ngreater vulnerability to these attacks. Consequently, large models such as\nGPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the\nspecific models tested are now superseded, the core methodology and empirical\nfindings contributed to the foundation of modern prompt injection defense\nresearch, including instruction hierarchy systems and constitutional AI\napproaches."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture",
        "author": "Ritesh Janga, and Rushit Dave",
        "link": "http://arxiv.org/abs/2509.12363v1",
        "abstract": "The agricultural sector is undergoing a transformation with the integration\nof advanced technologies, particularly in data-driven decision-making. This\nwork proposes a federated learning framework for smart farming, aiming to\ndevelop a scalable, efficient, and secure solution for crop disease detection\ntailored to the environmental and operational conditions of Minnesota farms. By\nmaintaining sensitive farm data locally and enabling collaborative model\nupdates, our proposed framework seeks to achieve high accuracy in crop disease\nclassification without compromising data privacy. We outline a methodology\ninvolving data collection from Minnesota farms, application of local deep\nlearning algorithms, transfer learning, and a central aggregation server for\nmodel refinement, aiming to achieve improved accuracy in disease detection,\ngood generalization across agricultural scenarios, lower costs in communication\nand training time, and earlier identification and intervention against diseases\nin future implementations. We outline a methodology and anticipated outcomes,\nsetting the stage for empirical validation in subsequent studies. This work\ncomes in a context where more and more demand for data-driven interpretations\nin agriculture has to be weighed with concerns about privacy from farms that\nare hesitant to share their operational data. This will be important to provide\na secure and efficient disease detection method that can finally revolutionize\nsmart farming systems and solve local agricultural problems with data\nconfidentiality. In doing so, this paper bridges the gap between advanced\nmachine learning techniques and the practical, privacy-sensitive needs of\nfarmers in Minnesota and beyond, leveraging the benefits of federated learning."
    },
    {
        "date": "2025-09",
        "title": "From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning",
        "author": "Collin Guo",
        "link": "http://arxiv.org/abs/2509.12176v1",
        "abstract": "Human face synthesis and manipulation are increasingly important in\nentertainment and AI, with a growing demand for highly realistic,\nidentity-preserving images even when only unpaired, unaligned datasets are\navailable. We study unpaired face manipulation via adversarial learning, moving\nfrom autoencoder baselines to a robust, guided CycleGAN framework. While\nautoencoders capture coarse identity, they often miss fine details. Our\napproach integrates spectral normalization for stable training, identity- and\nperceptual-guided losses to preserve subject identity and high-level structure,\nand landmark-weighted cycle constraints to maintain facial geometry across pose\nand illumination changes. Experiments show that our adversarial trained\nCycleGAN improves realism (FID), perceptual quality (LPIPS), and identity\npreservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction\nSSIM and practical inference times, which achieved high quality without paired\ndatasets and approaching pix2pix on curated paired subsets. These results\ndemonstrate that guided, spectrally normalized CycleGANs provide a practical\npath from autoencoders to robust unpaired face manipulation."
    },
    {
        "date": "2025-09",
        "title": "Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation",
        "author": "Sebastian Diaz, Benjamin Billot, Neel Dey, Molin Zhang, Esra Abaci Turk, P. Ellen Grant, Polina Golland, and Elfar Adalsteinsson",
        "link": "http://arxiv.org/abs/2509.12062v1",
        "abstract": "Fetal motion is a critical indicator of neurological development and\nintrauterine health, yet its quantification remains challenging, particularly\nat earlier gestational ages (GA). Current methods track fetal motion by\npredicting the location of annotated landmarks on 3D echo planar imaging (EPI)\ntime-series, primarily in third-trimester fetuses. The predicted landmarks\nenable simplification of the fetal body for downstream analysis. While these\nmethods perform well within their training age distribution, they consistently\nfail to generalize to early GAs due to significant anatomical changes in both\nmother and fetus across gestation, as well as the difficulty of obtaining\nannotated early GA EPI data. In this work, we develop a cross-population data\naugmentation framework that enables pose estimation models to robustly\ngeneralize to younger GA clinical cohorts using only annotated images from\nolder GA cohorts. Specifically, we introduce a fetal-specific augmentation\nstrategy that simulates the distinct intrauterine environment and fetal\npositioning of early GAs. Our experiments find that cross-population\naugmentation yields reduced variability and significant improvements across\nboth older GA and challenging early GA cases. By enabling more reliable pose\nestimation across gestation, our work potentially facilitates early clinical\ndetection and intervention in challenging 4D fetal imaging settings. Code is\navailable at https://github.com/sebodiaz/cross-population-pose."
    },
    {
        "date": "2025-09",
        "title": "RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration",
        "author": "Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, and Chongyi Li",
        "link": "http://arxiv.org/abs/2509.12039v1",
        "abstract": "This work presents Robust Representation Learning via Adaptive Mask (RAM++),\na two-stage framework for all-in-one image restoration. RAM++ integrates\nhigh-level semantic understanding with low-level texture generation to achieve\ncontent-oriented robust restoration. It addresses the limitations of existing\ndegradation-oriented methods in extreme scenarios (e.g., degradations strongly\ncoupled with image structures). RAM++ also mitigates common challenges such as\nunbalanced performance across tasks, overfitting to seen degradations, and weak\ngeneralization to unseen ones through three key designs: 1) Adaptive\nSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level\nmasks to semantically rich and textured regions. This design enables the\nnetwork to learn both generative priors and image content priors from various\ndegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning\nstrategy that adjusts the layers with higher contributions to bridge the\nintegrity gap between masked pretraining and full-image fine-tuning while\nretaining learned priors. 3) Robust Feature Regularization (RFR): a strategy\nthat leverages DINOv2's semantically consistent and degradation-invariant\nrepresentations, together with efficient feature fusion, to achieve faithful\nand semantically coherent restoration. With these designs, RAM++ achieves\nrobust, well-balanced, and state-of-the-art performance across seen, unseen,\nextreme, and mixed degradations. Our code and model will be released at\nhttps://github.com/DragonisCV/RAM"
    },
    {
        "date": "2025-09",
        "title": "Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness",
        "author": "Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2509.12024v1",
        "abstract": "Diffusion models have achieved unprecedented success in image generation but\npose increasing risks in terms of privacy, fairness, and security. A growing\ndemand exists to \\emph{erase} sensitive or harmful concepts (e.g., NSFW\ncontent, private individuals, artistic styles) from these models while\npreserving their overall generative capabilities. We introduce \\textbf{SCORE}\n(Secure and Concept-Oriented Robust Erasure), a novel framework for robust\nconcept removal in diffusion models. SCORE formulates concept erasure as an\n\\emph{adversarial independence} problem, theoretically guaranteeing that the\nmodel's outputs become statistically independent of the erased concept. Unlike\nprior heuristic methods, SCORE minimizes the mutual information between a\ntarget concept and generated outputs, yielding provable erasure guarantees. We\nprovide formal proofs establishing convergence properties and derive upper\nbounds on residual concept leakage. Empirically, we evaluate SCORE on Stable\nDiffusion and FLUX across four challenging benchmarks: object erasure, NSFW\nremoval, celebrity face suppression, and artistic style unlearning. SCORE\nconsistently outperforms state-of-the-art methods including EraseAnything, ANT,\nMACE, ESD, and UCE, achieving up to \\textbf{12.5\\%} higher erasure efficacy\nwhile maintaining comparable or superior image quality. By integrating\nadversarial optimization, trajectory consistency, and saliency-driven\nfine-tuning, SCORE sets a new standard for secure and robust concept erasure in\ndiffusion models."
    },
    {
        "date": "2025-09",
        "title": "Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study",
        "author": "James C. Ward, Alex Bott, Connor York, and Edmund R. Hunt",
        "link": "http://arxiv.org/abs/2509.11971v1",
        "abstract": "Simulating hostile attacks of physical autonomous systems can be a useful\ntool to examine their robustness to attack and inform vulnerability-aware\ndesign. In this work, we examine this through the lens of multi-robot patrol,\nby presenting a machine learning-based adversary model that observes robot\npatrol behavior in order to attempt to gain undetected access to a secure\nenvironment within a limited time duration. Such a model allows for evaluation\nof a patrol system against a realistic potential adversary, offering insight\ninto future patrol strategy design. We show that our new model outperforms\nexisting baselines, thus providing a more stringent test, and examine its\nperformance against multiple leading decentralized multi-robot patrol\nstrategies."
    },
    {
        "date": "2025-09",
        "title": "NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition",
        "author": "Zilin Li, Weiwei Xu, Xuanqi Zhao, and Yiran Zhu",
        "link": "http://arxiv.org/abs/2509.11916v1",
        "abstract": "Facial emotion recognition (FER) models trained only on pixels often fail to\ngeneralize across datasets because facial appearance is an indirect and biased\nproxy for underlying affect. We present NeuroGaze-Distill, a cross-modal\ndistillation framework that transfers brain-informed priors into an image-only\nFER student via static Valence/Arousal (V/A) prototypes and a\ndepression-inspired geometric prior (D-Geo). A teacher trained on EEG\ntopographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a\nconsolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face\npairing and no non-visual signals at deployment are required. The student\n(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two\nlightweight regularizers: (i) Proto-KD (cosine) aligns student features to the\nstatic prototypes; (ii) D-Geo softly shapes the embedding geometry in line with\naffective findings often reported in depression research (e.g., anhedonia-like\ncontraction in high-valence regions). We evaluate both within-domain (FERPlus\nvalidation) and cross-dataset protocols (AffectNet-mini; optional CK+),\nreporting standard 8-way scores alongside present-only Macro-F1 and balanced\naccuracy to fairly handle label-set mismatch. Ablations attribute consistent\ngains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.\nThe method is simple, deployable, and improves robustness without architectural\ncomplexity."
    },
    {
        "date": "2025-09",
        "title": "PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma",
        "author": "L. Zimmer, J. Weidner, M. Balcerak, F. Kofler, I. Ezhov, B. Menze, and B. Wiestler",
        "link": "http://arxiv.org/abs/2509.13360v1",
        "abstract": "Glioblastoma is the most prevalent primary brain malignancy, distinguished by\nits highly invasive behavior and exceptionally high rates of recurrence.\nConventional radiation therapy, which employs uniform treatment margins, fails\nto account for patient-specific anatomical and biological factors that\ncritically influence tumor cell migration. To address this limitation, numerous\ncomputational models of glioblastoma growth have been developed, enabling\ngeneration of tumor cell distribution maps extending beyond radiographically\nvisible regions and thus informing more precise treatment strategies. However,\ndespite encouraging preliminary findings, the clinical adoption of these growth\nmodels remains limited. To bridge this translational gap and accelerate both\nmodel development and clinical validation, we introduce PREDICT-GBM, a\ncomprehensive integrated pipeline and dataset for modeling and evaluation. This\nplatform enables systematic benchmarking of state-of-the-art tumor growth\nmodels using an expert-curated clinical dataset comprising 255 subjects with\ncomplete tumor segmentations and tissue characterization maps. Our analysis\ndemonstrates that personalized radiation treatment plans derived from tumor\ngrowth predictions achieved superior recurrence coverage compared to\nconventional uniform margin approaches for two of the evaluated models. This\nwork establishes a robust platform for advancing and systematically evaluating\ncutting-edge tumor growth modeling approaches, with the ultimate goal of\nfacilitating clinical translation and improving patient outcomes."
    },
    {
        "date": "2025-09",
        "title": "Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression",
        "author": "Xian Qin, Xue Yang, and Xiaohu Tang",
        "link": "http://arxiv.org/abs/2509.11870v1",
        "abstract": "Federated Learning (FL) allows collaborative model training across\ndistributed clients without sharing raw data, thus preserving privacy. However,\nthe system remains vulnerable to privacy leakage from gradient updates and\nByzantine attacks from malicious clients. Existing solutions face a critical\ntrade-off among privacy preservation, Byzantine robustness, and computational\nefficiency. We propose a novel scheme that effectively balances these competing\nobjectives by integrating homomorphic encryption with dimension compression\nbased on the Johnson-Lindenstrauss transformation. Our approach employs a\ndual-server architecture that enables secure Byzantine defense in the\nciphertext domain while dramatically reducing computational overhead through\ngradient compression. The dimension compression technique preserves the\ngeometric relationships necessary for Byzantine defence while reducing\ncomputation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where\n$k \\ll d$. Extensive experiments across diverse datasets demonstrate that our\napproach maintains model accuracy comparable to non-private FL while\neffectively defending against Byzantine clients comprising up to $40\\%$ of the\nnetwork."
    },
    {
        "date": "2025-09",
        "title": "Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer",
        "author": "Travis Davies, Yiqi Huang, Yunxin Liu, Xiang Chen, Huxian Liu, and Luhui Hu",
        "link": "http://arxiv.org/abs/2509.11865v1",
        "abstract": "Scaling Transformer policies and diffusion models has advanced robotic\nmanipulation, yet combining these techniques in lightweight, cross-embodiment\nlearning settings remains challenging. We study design choices that most affect\nstability and performance for diffusion-transformer policies trained on\nheterogeneous, multimodal robot data, and introduce Tenma, a lightweight\ndiffusion-transformer for bi-manual arm control. Tenma integrates multiview\nRGB, proprioception, and language via a cross-embodiment normalizer that maps\ndisparate state/action spaces into a shared latent space; a Joint State-Time\nencoder for temporally aligned observation learning with inference speed\nboosts; and a diffusion action decoder optimized for training stability and\nlearning capacity. Across benchmarks and under matched compute, Tenma achieves\nan average success rate of 88.95% in-distribution and maintains strong\nperformance under object and scene shifts, substantially exceeding baseline\npolicies whose best in-distribution average is 18.12%. Despite using moderate\ndata scale, Tenma delivers robust manipulation and generalization, indicating\nthe great potential for multimodal and cross-embodiment learning strategies for\nfurther augmenting the capacity of transformer-based imitation learning\npolicies."
    },
    {
        "date": "2025-09",
        "title": "NeuroStrike: Neuron-Level Attacks on Aligned LLMs",
        "author": "Lichao Wu, Sasha Behrouzi, Mohamadreza Rostami, Maximilian Thang, Stjepan Picek, and Ahmad-Reza Sadeghi",
        "link": "http://arxiv.org/abs/2509.11864v1",
        "abstract": "Safety alignment is critical for the ethical deployment of large language\nmodels (LLMs), guiding them to avoid generating harmful or unethical content.\nCurrent alignment techniques, such as supervised fine-tuning and reinforcement\nlearning from human feedback, remain fragile and can be bypassed by carefully\ncrafted adversarial prompts. Unfortunately, such attacks rely on trial and\nerror, lack generalizability across models, and are constrained by scalability\nand reliability.\n  This paper presents NeuroStrike, a novel and generalizable attack framework\nthat exploits a fundamental vulnerability introduced by alignment techniques:\nthe reliance on sparse, specialized safety neurons responsible for detecting\nand suppressing harmful inputs. We apply NeuroStrike to both white-box and\nblack-box settings: In the white-box setting, NeuroStrike identifies safety\nneurons through feedforward activation analysis and prunes them during\ninference to disable safety mechanisms. In the black-box setting, we propose\nthe first LLM profiling attack, which leverages safety neuron transferability\nby training adversarial prompt generators on open-weight surrogate models and\nthen deploying them against black-box and proprietary targets. We evaluate\nNeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing\nless than 0.6% of neurons in targeted layers, NeuroStrike achieves an average\nattack success rate (ASR) of 76.9% using only vanilla malicious prompts.\nMoreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on\nunsafe image inputs. Safety neurons transfer effectively across architectures,\nraising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled\nmodels. The black-box LLM profiling attack achieves an average ASR of 63.7%\nacross five black-box models, including the Google Gemini family."
    },
    {
        "date": "2025-09",
        "title": "Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network",
        "author": "Navid Hashemi, Samuel Sasaki, Diego Manzanas Lopez, Ipek Oguz, Meiyi Ma, and Taylor T. Johnson",
        "link": "http://arxiv.org/abs/2509.11838v1",
        "abstract": "Semantic segmentation networks (SSNs) play a critical role in domains such as\nmedical imaging, autonomous driving, and environmental monitoring, where safety\nhinges on reliable model behavior under uncertainty. Yet, existing\nprobabilistic verification approaches struggle to scale with the complexity and\ndimensionality of modern segmentation tasks, often yielding guarantees that are\ntoo conservative to be practical. We introduce a probabilistic verification\nframework that is both architecture-agnostic and scalable to high-dimensional\noutputs. Our approach combines sampling-based reachability analysis with\nconformal inference (CI) to deliver provable guarantees while avoiding the\nexcessive conservatism of prior methods. To counteract CI's limitations in\nhigh-dimensional settings, we propose novel strategies that reduce conservatism\nwithout compromising rigor. Empirical evaluation on large-scale segmentation\nmodels across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates\nthat our method provides reliable safety guarantees while substantially\ntightening bounds compared to SOTA. We also provide a toolbox implementing this\ntechnique, available on Github."
    },
    {
        "date": "2025-09",
        "title": "A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers",
        "author": "Kai Tan, Dongyang Zhan, Lin Ye, Hongli Zhang, and Binxing Fang",
        "link": "http://arxiv.org/abs/2509.11836v1",
        "abstract": "Sequence-based deep learning models (e.g., RNNs), can detect malware by\nanalyzing its behavioral sequences. Meanwhile, these models are susceptible to\nadversarial attacks. Attackers can create adversarial samples that alter the\nsequence characteristics of behavior sequences to deceive malware classifiers.\nThe existing methods for generating adversarial samples typically involve\ndeleting or replacing crucial behaviors in the original data sequences, or\ninserting benign behaviors that may violate the behavior constraints. However,\nthese methods that directly manipulate sequences make adversarial samples\ndifficult to implement or apply in practice. In this paper, we propose an\nadversarial attack approach based on Deep Q-Network and a heuristic\nbacktracking search strategy, which can generate perturbation sequences that\nsatisfy practical conditions for successful attacks. Subsequently, we utilize a\nnovel transformation approach that maps modifications back to the source code,\nthereby avoiding the need to directly modify the behavior log sequences. We\nconduct an evaluation of our approach, and the results confirm its\neffectiveness in generating adversarial samples from real-world malware\nbehavior sequences, which have a high success rate in evading anomaly detection\nmodels. Furthermore, our approach is practical and can generate adversarial\nsamples while maintaining the functionality of the modified software."
    },
    {
        "date": "2025-09",
        "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
        "author": "Filip Sondej, and Yushi Yang",
        "link": "http://arxiv.org/abs/2509.11816v1",
        "abstract": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact."
    },
    {
        "date": "2025-09",
        "title": "DetectAnyLLM: Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models",
        "author": "Jiachen Fu, Chun-Le Guo, and Chongyi Li",
        "link": "http://arxiv.org/abs/2509.14268v1",
        "abstract": "The rapid advancement of large language models (LLMs) has drawn urgent\nattention to the task of machine-generated text detection (MGTD). However,\nexisting approaches struggle in complex real-world scenarios: zero-shot\ndetectors rely heavily on scoring model's output distribution while\ntraining-based detectors are often constrained by overfitting to the training\ndata, limiting generalization. We found that the performance bottleneck of\ntraining-based detectors stems from the misalignment between training objective\nand task needs. To address this, we propose Direct Discrepancy Learning (DDL),\na novel optimization strategy that directly optimizes the detector with\ntask-oriented knowledge. DDL enables the detector to better capture the core\nsemantics of the detection task, thereby enhancing both robustness and\ngeneralization. Built upon this, we introduce DetectAnyLLM, a unified detection\nframework that achieves state-of-the-art MGTD performance across diverse LLMs.\nTo ensure a reliable evaluation, we construct MIRAGE, the most diverse\nmulti-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora\nacross 5 text-domains, which are then re-generated or revised using 17\ncutting-edge LLMs, covering a wide spectrum of proprietary models and textual\nstyles. Extensive experiments on MIRAGE reveal the limitations of existing\nmethods in complex environment. In contrast, DetectAnyLLM consistently\noutperforms them, achieving over a 70% performance improvement under the same\ntraining data and base scoring model, underscoring the effectiveness of our\nDDL. Project page: {https://fjc2005.github.io/detectanyllm}."
    },
    {
        "date": "2025-09",
        "title": "Removal Attack and Defense on AI-generated Content Latent-based Watermarking",
        "author": "De Zhang Lee, Han Fang, Hanyi Wang, and Ee-Chien Chang",
        "link": "http://arxiv.org/abs/2509.11745v2",
        "abstract": "Digital watermarks can be embedded into AI-generated content (AIGC) by\ninitializing the generation process with starting points sampled from a secret\ndistribution. When combined with pseudorandom error-correcting codes, such\nwatermarked outputs can remain indistinguishable from unwatermarked objects,\nwhile maintaining robustness under whitenoise. In this paper, we go beyond\nindistinguishability and investigate security under removal attacks. We\ndemonstrate that indistinguishability alone does not necessarily guarantee\nresistance to adversarial removal. Specifically, we propose a novel attack that\nexploits boundary information leaked by the locations of watermarked objects.\nThis attack significantly reduces the distortion required to remove watermarks\n-- by up to a factor of $15 \\times$ compared to a baseline whitenoise attack\nunder certain settings. To mitigate such attacks, we introduce a defense\nmechanism that applies a secret transformation to hide the boundary, and prove\nthat the secret transformation effectively rendering any attacker's\nperturbations equivalent to those of a naive whitenoise adversary. Our\nempirical evaluations, conducted on multiple versions of Stable Diffusion,\nvalidate the effectiveness of both the attack and the proposed defense,\nhighlighting the importance of addressing boundary leakage in latent-based\nwatermarking schemes."
    },
    {
        "date": "2025-09",
        "title": "DRAG: Data Reconstruction Attack using Guided Diffusion",
        "author": "Wa-Kin Lei, Jun-Cheng Chen, and Shang-Tse Chen",
        "link": "http://arxiv.org/abs/2509.11724v1",
        "abstract": "With the rise of large foundation models, split inference (SI) has emerged as\na popular computational paradigm for deploying models across lightweight edge\ndevices and cloud servers, addressing data privacy and computational cost\nconcerns. However, most existing data reconstruction attacks have focused on\nsmaller CNN classification models, leaving the privacy risks of foundation\nmodels in SI settings largely unexplored. To address this gap, we propose a\nnovel data reconstruction attack based on guided diffusion, which leverages the\nrich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on\na large-scale dataset. Our method performs iterative reconstruction on the\nLDM's learned image prior, effectively generating high-fidelity images\nresembling the original data from their intermediate representations (IR).\nExtensive experiments demonstrate that our approach significantly outperforms\nstate-of-the-art methods, both qualitatively and quantitatively, in\nreconstructing data from deep-layer IRs of the vision foundation model. The\nresults highlight the urgent need for more robust privacy protection mechanisms\nfor large models in SI scenarios. Code is available at:\nhttps://github.com/ntuaislab/DRAG."
    },
    {
        "date": "2025-09",
        "title": "A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience",
        "author": "Mohammad Olid Ali Akash, and Priyangana Saha",
        "link": "http://arxiv.org/abs/2509.11712v1",
        "abstract": "In the modern, fast-moving world of e-commerce, many Android apps face\nchallenges in providing a simple and secure shopping experience. Many of these\napps, often enough, have complicated designs that prevent users from finding\nwhat they want quickly, thus frustrating them and wasting their precious time.\nAnother major issue is that of security; with the limitation of payment options\nand weak authentication mechanisms, users' sensitive information can be\ncompromised. This research presents a new e-commerce platform that responds to\nthe above challenges with an intuitive interface and strong security measures.\nThe platform makes online shopping easy with well-organized categories of\nproducts and a fast, efficient checkout process. It also gives priority to\nsecurity by incorporating features such as Google authentication and\nSSL-secured payment gateways to protect user data and ensure secure\ntransactions. This paper discusses how a focus on user-friendliness, security,\nand personalization steps up the game for e-commerce platforms, providing\nworkable frameworks that match modern user needs and expectations. The findings\nshow the e-commerce user experience can be remodelled by the platform, hence\nopening ways for future developments in that respect."
    },
    {
        "date": "2025-09",
        "title": "An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks",
        "author": "Sawera Shahid, Umara Noor, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11683v1",
        "abstract": "Cyber attacks are rapidly increasing with the advancement of technology and\nthere is no protection for our information. To prevent future cyberattacks it\nis critical to promptly recognize cyberattacks and establish strong defense\nmechanisms against them. To respond to cybersecurity threats immediately, it is\nessential to examine the attackers skills, knowledge, and behaviors with the\ngoal of evaluating their impact on the system and comprehending the traits\nassociated with these attacks. Creating a profile of cyber threat actors based\non their traits or patterns of behavior can help to create effective defenses\nagainst cyberattacks in advance. In the current literature, multiple supervised\nmachine learning based approaches considered a smaller number of features for\nattacker profiling that are reported in textual cyber threat incident documents\nalthough these profiles have been developed based on the security experts own\nperception, we cannot rely on them. Supervised machine learning approaches\nstrictly depend upon the structure data set. This usually leads to a two step\nprocess where we first have to establish a structured data set before we can\nanalyze it and then employ it to construct defense mechanisms, which takes\ntime. In this paper, an unsupervised efficient agglomerative hierarchal\nclustering technique is proposed for profiling cybercriminal groups based on\ntheir comprehensive contextual threat information in order to address the\naforementioned issues. The main objective of this report is to identify the\nrelationship between cyber threat actors based on their common features,\naggregate them, and also profile cyber criminal groups."
    },
    {
        "date": "2025-09",
        "title": "Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight",
        "author": "Jonas C. Ditz, Veronika Lazar, Elmar Lichtme\u00df, Carola Plesch, Matthias Heck, Kevin Baum, and Markus Langer",
        "link": "http://arxiv.org/abs/2509.12290v1",
        "abstract": "Human oversight of AI is promoted as a safeguard against risks such as\ninaccurate outputs, system malfunctions, or violations of fundamental rights,\nand is mandated in regulation like the European AI Act. Yet debates on human\noversight have largely focused on its effectiveness, while overlooking a\ncritical dimension: the security of human oversight. We argue that human\noversight creates a new attack surface within the safety, security, and\naccountability architecture of AI operations. Drawing on cybersecurity\nperspectives, we analyze attack vectors that threaten the requirements of\neffective human oversight, thereby undermining the safety of AI operations.\nSuch attacks may target the AI system, its communication with oversight\npersonnel, or the personnel themselves. We then outline hardening strategies to\nmitigate these risks. Our contributions are: (1) introducing a security\nperspective on human oversight, and (2) providing an overview of attack vectors\nand hardening strategies to enable secure human oversight of AI."
    },
    {
        "date": "2025-09",
        "title": "Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing",
        "author": "Fizza Khurshid, Umara Noor, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11668v1",
        "abstract": "Innovative solutions to cyber security issues are shaped by the ever-changing\nlandscape of cyber threats. Automating the mitigation of these threats can be\nachieved through a new methodology that addresses the domain of mitigation\nautomation, which is often overlooked. This literature overview emphasizes the\nlack of scholarly work focusing specifically on automated cyber threat\nmitigation, particularly in addressing challenges beyond detection. The\nproposed methodology comprise of the development of an automatic cyber threat\nmitigation framework tailored for Distributed Denial-of-Service (DDoS) attacks.\nThis framework adopts a multi-layer security approach, utilizing smart devices\nat the device layer, and leveraging fog network and cloud computing layers for\ndeeper understanding and technological adaptability. Initially, firewall\nrule-based packet inspection is conducted on simulated attack traffic to filter\nout DoS packets, forwarding legitimate packets to the fog. The methodology\nemphasizes the integration of fog detection through statistical and behavioral\nanalysis, specification-based detection, and deep packet inspection, resulting\nin a comprehensive cyber protection system. Furthermore, cloud-level inspection\nis performed to confirm and mitigate attacks using firewalls, enhancing\nstrategic defense and increasing robustness against cyber threats. These\nenhancements enhance understanding of the research framework's practical\nimplementation and assessment strategies, substantiating its importance in\naddressing current cyber security challenges and shaping future automation\nmitigation approaches."
    },
    {
        "date": "2025-09",
        "title": "Task-Agnostic Learnable Weighted-Knowledge Base Scheme for Robust Semantic Communications",
        "author": "Shiyao Jiang, Jian Jiao, Xingjian Zhang, Ye Wang, Dusit Niyato, and Qinyu Zhang",
        "link": "http://arxiv.org/abs/2509.11636v1",
        "abstract": "With the emergence of diverse and massive data in the upcoming\nsixth-generation (6G) networks, the task-agnostic semantic communication system\nis regarded to provide robust intelligent services. In this paper, we propose a\ntask-agnostic learnable weighted-knowledge base semantic communication (TALSC)\nframework for robust image transmission to address the real-world heterogeneous\ndata bias in KB, including label flipping noise and class imbalance. The TALSC\nframework incorporates a sample confidence module (SCM) as meta-learner and the\nsemantic coding networks as learners. The learners are updated based on the\nempirical knowledge provided by the learnable weighted-KB (LW-KB). Meanwhile,\nthe meta-learner evaluates the significance of samples according to the task\nloss feedback, and adjusts the update strategy of learners to enhance the\nrobustness in semantic recovery for unknown tasks. To strike a balance between\nSCM parameters and precision of significance evaluation, we design an SCM-grid\nextension (SCM-GE) approach by embedding the Kolmogorov-Arnold networks (KAN)\nwithin SCM, which leverages the concept of spline refinement in KAN and enables\nscalable SCM with customizable granularity without retraining. Simulations\ndemonstrate that the TALSC framework effectively mitigates the effects of\nflipping noise and class imbalance in task-agnostic image semantic\ncommunication, achieving at least 12% higher semantic recovery accuracy (SRA)\nand multi-scale structural similarity (MS-SSIM) compared to state-of-the-art\nmethods."
    },
    {
        "date": "2025-09",
        "title": "Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check",
        "author": "Chentao Cao, Xiaojun Xu, Bo Han, and Hang Li",
        "link": "http://arxiv.org/abs/2509.11629v1",
        "abstract": "As large language models (LLMs) continue to advance in capabilities, ensuring\ntheir safety against jailbreak attacks remains a critical challenge. In this\npaper, we introduce a novel safety alignment approach called Answer-Then-Check,\nwhich enhances LLM robustness against malicious prompts by applying thinking\nability to mitigate jailbreaking problems before producing a final answer to\nthe user. Our method enables models to directly answer the question in their\nthought and then critically evaluate its safety before deciding whether to\nprovide it. To implement this approach, we construct the Reasoned Safety\nAlignment (ReSA) dataset, comprising 80K examples that teach models to reason\nthrough direct responses and then analyze their safety. Experimental results\ndemonstrate that our approach achieves the Pareto frontier with superior safety\ncapability while decreasing over-refusal rates on over-refusal benchmarks.\nNotably, the model fine-tuned with ReSA maintains general reasoning\ncapabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our\nmethod equips models with the ability to perform safe completion. Unlike\npost-hoc methods that can only reject harmful queries, our model can provide\nhelpful and safe alternative responses for sensitive topics (e.g., self-harm).\nFurthermore, we discover that training on a small subset of just 500 examples\ncan achieve comparable performance to using the full dataset, suggesting that\nsafety alignment may require less data than previously assumed."
    },
    {
        "date": "2025-09",
        "title": "Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution",
        "author": "Rimsha Kanwal, Umara Noor, Zafar Iqbal, and Zahid Rashid",
        "link": "http://arxiv.org/abs/2509.11615v1",
        "abstract": "With the ever-changing landscape of cyber threats, identifying their origin\nhas become paramount, surpassing the simple task of attack classification.\nCyber threat attribution gives security analysts the insights they need to\ndevice effective threat mitigation strategies. Such strategies empower\nenterprises to proactively detect and defend against future cyber-attacks.\nHowever, existing approaches exhibit limitations in accurately identifying\nthreat actors, leading to low precision and a significant occurrence of false\npositives. Machine learning offers the potential to automate certain aspects of\ncyber threat attribution. The distributed nature of information regarding cyber\nthreat actors and their intricate attack methodologies has hindered substantial\nprogress in this domain. Cybersecurity analysts deal with an ever-expanding\ncollection of cyber threat intelligence documents. While these documents hold\nvaluable insights, their sheer volume challenges efficient organization and\nretrieval of pertinent information. To assist the cybersecurity analyst\nactivities, we propose a machine learning based approach featuring visually\ninteractive analytics tool named the Cyber-Attack Pattern Explorer (CAPE),\ndesigned to facilitate efficient information discovery by employing interactive\nvisualization and mining techniques. In the proposed system, a non-parametric\nmining technique is proposed to create a dataset for identifying the attack\npatterns within cyber threat intelligence documents. These attack patterns\nalign semantically with commonly employed themes ensuring ease of\ninterpretation. The extracted dataset is used for training of proposed machine\nlearning algorithms that enables the attribution of cyber threats with\nrespective to the actors."
    },
    {
        "date": "2025-09",
        "title": "E-ROBOT: a dimension-free method for robust statistics and machine learning via Schr\u00f6dinger bridge",
        "author": "Davide La Vecchia, and Hang Liu",
        "link": "http://arxiv.org/abs/2509.11532v1",
        "abstract": "We propose the Entropic-regularized Robust Optimal Transport (E-ROBOT)\nframework, a novel method that combines the robustness of ROBOT with the\ncomputational and statistical benefits of entropic regularization. We show\nthat, rooted in the Schr\\\"{o}dinger bridge problem theory, E-ROBOT defines the\nrobust Sinkhorn divergence $\\overline{W}_{\\varepsilon,\\lambda}$, where the\nparameter $\\lambda$ controls robustness and $\\varepsilon$ governs the\nregularization strength. Letting $n\\in \\mathbb{N}$ denote the sample size, a\ncentral theoretical contribution is establishing that the sample complexity of\n$\\overline{W}_{\\varepsilon,\\lambda}$ is $\\mathcal{O}(n^{-1/2})$, thereby\navoiding the curse of dimensionality that plagues standard ROBOT. This\ndimension-free property unlocks the use of $\\overline{W}_{\\varepsilon,\\lambda}$\nas a loss function in large-dimensional statistical and machine learning tasks.\nWith this regard, we demonstrate its utility through four applications:\ngoodness-of-fit testing; computation of barycenters for corrupted 2D and 3D\nshapes; definition of gradient flows; and image colour transfer. From the\ncomputation standpoint, a perk of our novel method is that it can be easily\nimplemented by modifying existing (\\texttt{Python}) routines. From the\ntheoretical standpoint, our work opens the door to many research directions in\nstatistics and machine learning: we discuss some of them."
    },
    {
        "date": "2025-09",
        "title": "DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks",
        "author": "Jing Zou, Shungeng Zhang, Meikang Qiu, and Chong Li",
        "link": "http://arxiv.org/abs/2509.11525v1",
        "abstract": "Deep learning models are vulnerable to adversarial examples, posing critical\nsecurity challenges in real-world applications. While Adversarial Training (AT\n) is a widely adopted defense mechanism to enhance robustness, it often incurs\na trade-off by degrading performance on unperturbed, natural data. Recent\nefforts have highlighted that larger models exhibit enhanced robustness over\ntheir smaller counterparts. In this paper, we empirically demonstrate that such\nrobustness can be systematically distilled from large teacher models into\ncompact student models. To achieve better performance, we introduce Dice\nAdversarial Robustness Distillation (DARD), a novel method designed to transfer\nrobustness through a tailored knowledge distillation paradigm. Additionally, we\npropose Dice Projected Gradient Descent (DPGD), an adversarial example\ngeneralization method optimized for effective attack. Our extensive experiments\ndemonstrate that the DARD approach consistently outperforms adversarially\ntrained networks with the same architecture, achieving superior robustness and\nstandard accuracy."
    },
    {
        "date": "2025-09",
        "title": "MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder",
        "author": "Ayhan Can Erdur, Christian Beischl, Daniel Scholz, Jiazhen Pan, Benedikt Wiestler, Daniel Rueckert, and Jan C Peeken",
        "link": "http://arxiv.org/abs/2509.11442v1",
        "abstract": "Missing input sequences are common in medical imaging data, posing a\nchallenge for deep learning models reliant on complete input data. In this\nwork, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm\nfor multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our\nmethod treats each MRI sequence as a separate input modality, leveraging a\nlate-fusion-style transformer encoder to integrate multi-sequence information\n(multi-modal) and individual decoder streams for each modality for multi-task\nreconstruction. This pretraining strategy guides the model to learn rich\nrepresentations per modality while also equipping it to handle missing inputs\nthrough cross-sequence reasoning. The result is a flexible and generalizable\nencoder for brain MRIs that infers missing sequences from available inputs and\ncan be adapted to various downstream applications. We demonstrate the\nperformance and robustness of our method against an MAE-ViT baseline in\ndownstream segmentation and classification tasks, showing absolute improvement\nof $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing\ninput sequences. Our experiments demonstrate the strength of this pretraining\nstrategy. The implementation is made available."
    },
    {
        "date": "2025-09",
        "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications",
        "author": "Aadil Gani Ganie",
        "link": "http://arxiv.org/abs/2509.11431v1",
        "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations."
    },
    {
        "date": "2025-09",
        "title": "Pulse-to-Circuit Characterization of Stealthy Crosstalk Attack on Multi-Tenant Superconducting Quantum Hardware",
        "author": "Syed Emad Uddin Shubha, and Tasnuva Farheen",
        "link": "http://arxiv.org/abs/2509.11407v1",
        "abstract": "Hardware crosstalk in multi-tenant superconducting quantum computers\nconstitutes a significant security threat, enabling adversaries to inject\ntargeted errors across tenant boundaries. We present the first end-to-end\nframework for mapping physical pulse-level attacks to interpretable logical\nerror channels, integrating density-matrix simulation, quantum process\ntomography (QPT), and a novel isometry-based circuit extraction method. Our\npipeline reconstructs the complete induced error channel and fits an effective\nlogical circuit model, revealing a fundamentally asymmetric attack mechanism:\none adversarial qubit acts as a driver to set the induced logical rotation,\nwhile a second, the catalyst, refines the attack's coherence. Demonstrated on a\nlinear three-qubit system, our approach shows that such attacks can\nsignificantly disrupt diverse quantum protocols, sometimes reducing accuracy to\nrandom guessing, while remaining effective and stealthy even under realistic\nhardware parameter variations. We further propose a protocol-level detection\nstrategy based on observable attack signatures, showing that stealthy attacks\ncan be exposed through targeted monitoring and providing a foundation for\nfuture defense-in-depth in quantum cloud platforms."
    },
    {
        "date": "2025-09",
        "title": "Some Robustness Properties of Label Cleaning",
        "author": "Chen Cheng, and John Duchi",
        "link": "http://arxiv.org/abs/2509.11379v1",
        "abstract": "We demonstrate that learning procedures that rely on aggregated labels, e.g.,\nlabel information distilled from noisy responses, enjoy robustness properties\nimpossible without data cleaning. This robustness appears in several ways. In\nthe context of risk consistency -- when one takes the standard approach in\nmachine learning of minimizing a surrogate (typically convex) loss in place of\na desired task loss (such as the zero-one mis-classification error) --\nprocedures using label aggregation obtain stronger consistency guarantees than\nthose even possible using raw labels. And while classical statistical scenarios\nof fitting perfectly-specified models suggest that incorporating all possible\ninformation -- modeling uncertainty in labels -- is statistically efficient,\nconsistency fails for ``standard'' approaches as soon as a loss to be minimized\nis even slightly mis-specified. Yet procedures leveraging aggregated\ninformation still converge to optimal classifiers, highlighting how\nincorporating a fuller view of the data analysis pipeline, from collection to\nmodel-fitting to prediction time, can yield a more robust methodology by\nrefining noisy signals."
    },
    {
        "date": "2025-09",
        "title": "Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness",
        "author": "Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, and KC santosh",
        "link": "http://arxiv.org/abs/2509.11355v1",
        "abstract": "Convolutional Neural Networks (CNNs) excel at image classification but remain\nvulnerable to common corruptions that humans handle with ease. A key reason for\nthis fragility is their reliance on local texture cues rather than global\nobject shapes -- a stark contrast to human perception. To address this, we\npropose two complementary regularization strategies designed to encourage\nshape-biased representations and enhance robustness. The first introduces an\nauxiliary loss that enforces feature consistency between original and\nlow-frequency filtered inputs, discouraging dependence on high-frequency\ntextures. The second incorporates supervised contrastive learning to structure\nthe feature space around class-consistent, shape-relevant representations.\nEvaluated on the CIFAR-10-C benchmark, both methods improve corruption\nrobustness without degrading clean accuracy. Our results suggest that\nloss-level regularization can effectively steer CNNs toward more shape-aware,\nresilient representations."
    },
    {
        "date": "2025-09",
        "title": "On the Escaping Efficiency of Distributed Adversarial Training Algorithms",
        "author": "Ying Cao, Kun Yuan, and Ali H. Sayed",
        "link": "http://arxiv.org/abs/2509.11337v1",
        "abstract": "Adversarial training has been widely studied in recent years due to its role\nin improving model robustness against adversarial attacks. This paper focuses\non comparing different distributed adversarial training algorithms--including\ncentralized and decentralized strategies--within multi-agent learning\nenvironments. Previous studies have highlighted the importance of model\nflatness in determining robustness. To this end, we develop a general\ntheoretical framework to study the escaping efficiency of these algorithms from\nlocal minima, which is closely related to the flatness of the resulting models.\nWe show that when the perturbation bound is sufficiently small (i.e., when the\nattack strength is relatively mild) and a large batch size is used,\ndecentralized adversarial training algorithms--including consensus and\ndiffusion--are guaranteed to escape faster from local minima than the\ncentralized strategy, thereby favoring flatter minima. However, as the\nperturbation bound increases, this trend may no longer hold. In the simulation\nresults, we illustrate our theoretical findings and systematically compare the\nperformance of models obtained through decentralized and centralized\nadversarial training algorithms. The results highlight the potential of\ndecentralized strategies to enhance the robustness of models in distributed\nsettings."
    },
    {
        "date": "2025-09",
        "title": "SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing",
        "author": "Qiuhao Liu, Ling Li, Yao Lu, Qi Xuan, Zhaowei Zhu, and Jiaheng Wei",
        "link": "http://arxiv.org/abs/2509.11265v1",
        "abstract": "Deep neural networks tend to memorize noisy labels, severely degrading their\ngeneralization performance. Although Mixup has demonstrated effectiveness in\nimproving generalization and robustness, existing Mixup-based methods typically\nperform indiscriminate mixing without principled guidance on sample selection\nand mixing strategy, inadvertently propagating noisy supervision. To overcome\nthese limitations, we propose SelectMix, a confidence-guided mixing framework\nexplicitly tailored for noisy labels. SelectMix first identifies potentially\nnoisy or ambiguous samples through confidence based mismatch analysis using\nK-fold cross-validation, then selectively blends identified uncertain samples\nwith confidently predicted peers from their potential classes. Furthermore,\nSelectMix employs soft labels derived from all classes involved in the mixing\nprocess, ensuring the labels accurately represent the composition of the mixed\nsamples, thus aligning supervision signals closely with the actual mixed\ninputs. Through extensive theoretical analysis and empirical evaluations on\nmultiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world\nbenchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that\nSelectMix consistently outperforms strong baseline methods, validating its\neffectiveness and robustness in learning with noisy labels."
    },
    {
        "date": "2025-09",
        "title": "Realistic Environmental Injection Attacks on GUI Agents",
        "author": "Yitong Zhang, Ximo Li, Liyi Cai, and Jia Li",
        "link": "http://arxiv.org/abs/2509.11250v1",
        "abstract": "GUI agents built on LVLMs are increasingly used to interact with websites.\nHowever, their exposure to open-world content makes them vulnerable to\nEnvironmental Injection Attacks (EIAs) that hijack agent behavior via webpage\nelements. Many recent studies assume the attacker to be a regular user who can\nonly upload a single trigger image, which is more realistic than earlier\nassumptions of website-level administrative control. However, these works still\nfall short of realism: (1) the trigger's position and surrounding context\nremain largely fixed between training and testing, failing to capture the\ndynamic nature of real webpages and (2) the trigger often occupies an\nunrealistically large area, whereas real-world images are typically small. To\nbetter reflect real-world scenarios, we introduce a more realistic threat model\nwhere the attacker is a regular user and the trigger image is small and\nembedded within a dynamically changing environment. As a result, existing\nattacks prove largely ineffective under this threat model.\n  To better expose the vulnerabilities of GUI agents, we propose Chameleon, an\nattack framework with two main novelties. The first is LLM-Driven Environment\nSimulation, which automatically generates diverse and high-fidelity webpage\nsimulations. The second is Attention Black Hole, which transforms attention\nweights into explicit supervisory signals that guide the agent's focus toward\nthe trigger region. We evaluate Chameleon on 6 realistic websites and 4\nrepresentative LVLM-powered GUI agents, where it significantly outperforms\nexisting methods. Ablation studies confirm that both novelties are critical to\nperformance. Our findings reveal underexplored vulnerabilities in modern GUI\nagents and establish a robust foundation for future research on defense in\nopen-world GUI agent systems. The code is publicly available at\nhttps://github.com/zhangyitonggg/attack2gui."
    },
    {
        "date": "2025-09",
        "title": "Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers",
        "author": "Zhaofeng Yu, Dongyang Zhan, Lin Ye, Haining Yu, Hongli Zhang, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2509.11242v1",
        "abstract": "Recently, the WebAssembly (or Wasm) technology has been rapidly evolving,\nwith many runtimes actively under development, providing cross-platform secure\nsandboxes for Wasm modules to run as portable containers. Compared with Docker,\nwhich isolates applications at the operating system level, Wasm runtimes\nprovide more security mechanisms, such as linear memory, type checking, and\nprotected call stacks. Although Wasm is designed with security in mind and\nconsidered to be a more secure container runtime, various security challenges\nhave arisen, and researchers have focused on the security of Wasm runtimes,\nsuch as discovering vulnerabilities or proposing new security mechanisms to\nachieve robust isolation. However, we have observed that the resource isolation\nis not well protected by the current Wasm runtimes, and attackers can exhaust\nthe host's resources to interfere with the execution of other container\ninstances by exploiting the WASI/WASIX interfaces. And the attack surface has\nnot been well explored and measured. In this paper, we explore the resource\nisolation attack surface of Wasm runtimes systematically by proposing several\nstatic Wasm runtime analysis approaches. Based on the analysis results, we\npropose several exploitation strategies to break the resource isolation of Wasm\nruntimes. The experimental results show that malicious Wasm instances can not\nonly consume large amounts of system resources on their own but also introduce\nhigh workloads into other components of the underlying operating system,\nleading to a substantial performance degradation of the whole system. In\naddition, the mitigation approaches have also been discussed."
    },
    {
        "date": "2025-09",
        "title": "ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification",
        "author": "Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, and Vu N. Duong",
        "link": "http://arxiv.org/abs/2509.11220v1",
        "abstract": "Few-Shot Learning (FSL), which involves learning to generalize using only a\nfew data samples, has demonstrated promising and superior performances to\nordinary CNN methods. While Bayesian based estimation approaches using\nKullback-Leibler (KL) divergence have shown improvements, they remain\nvulnerable to adversarial attacks and natural noises. We introduce\nANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation\nNetwork that significantly advances the state-of-the-art in FSL robustness and\nperformance. Our approach implements an adversarially and naturally robust\nHellinger distance-based feature class aggregation scheme, demonstrating\nresilience to adversarial perturbations up to $\\epsilon=0.30$ and Gaussian\nnoise up to $\\sigma=0.30$. The network achieves substantial improvements across\nbenchmark datasets, including gains of 1.20\\% and 1.40\\% for 1-shot and 5-shot\nscenarios on miniImageNet respectively. We introduce a novel Hellinger\nSimilarity contrastive loss function that generalizes cosine similarity\ncontrastive loss for variational few-shot inference scenarios. Our approach\nalso achieves superior image reconstruction quality with a FID score of 2.75,\noutperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive\nexperiments conducted on four few-shot benchmarked datasets verify that\nANROT-HELANet's combination of Hellinger distance-based feature aggregation,\nattention mechanisms, and our novel loss function establishes new\nstate-of-the-art performance while maintaining robustness against both\nadversarial and natural perturbations. Our code repository will be available at\nhttps://github.com/GreedYLearner1146/ANROT-HELANet/tree/main."
    },
    {
        "date": "2025-09",
        "title": "DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations",
        "author": "Doan Minh Trung, Tien Duc Anh Hao, Luong Hoang Minh, Nghi Hoang Khoa, Nguyen Tan Cam, Van-Hau Pham, and Phan The Duy",
        "link": "http://arxiv.org/abs/2509.11187v1",
        "abstract": "In recent years, learning-based Android malware detection has seen\nsignificant advancements, with detectors generally falling into three\ncategories: string-based, image-based, and graph-based approaches. While these\nmethods have shown strong detection performance, they often struggle to sustain\nrobustness in real-world settings, particularly when facing code obfuscation\nand adversarial examples (AEs). Deep multimodal learning has emerged as a\npromising solution, leveraging the strengths of multiple feature types to\nenhance robustness and generalization. However, a systematic investigation of\nmultimodal fusion for both accuracy and resilience remains underexplored. In\nthis study, we propose DMLDroid, an Android malware detection based on\nmultimodal fusion that leverages three different representations of malware\nfeatures, including permissions & intents (tabular-based), DEX file\nrepresentations (image-based), and API calls (graph-derived sequence-based). We\nconduct exhaustive experiments independently on each feature, as well as in\ncombination, using different fusion strategies. Experimental results on the\nCICMalDroid 2020 dataset demonstrate that our multimodal approach with the\ndynamic weighted fusion mechanism achieves high performance, reaching 97.98%\naccuracy and 98.67% F1-score on original malware detection. Notably, the\nproposed method maintains strong robustness, sustaining over 98% accuracy and\n98% F1-score under both obfuscation and adversarial attack scenarios. Our\nfindings highlight the benefits of multimodal fusion in improving both\ndetection accuracy and robustness against evolving Android malware threats."
    },
    {
        "date": "2025-09",
        "title": "UDFS: Lightweight Representation-Driven Robust Network Traffic Classification",
        "author": "Youquan Xian, Xueying Zeng, Mei Huang, Aoxiang Zhou, Xiaoyu Cui, Peng Liu, and Lei Cui",
        "link": "http://arxiv.org/abs/2509.11157v1",
        "abstract": "In recent years, sequence features such as packet length have received\nconsiderable attention due to their central role in encrypted traffic analysis.\nExisting sequence modeling approaches can be broadly categorized into\nflow-level and trace-level methods: the former suffer from high feature\nredundancy, limiting their discriminative power, whereas the latter preserve\ncomplete information but incur substantial computational and storage overhead.\nTo address these limitations, we propose the \\textbf{U}p-\\textbf{D}own\n\\textbf{F}low \\textbf{S}equence (\\textbf{UDFS}) representation, which\ncompresses an entire trace into a two-dimensional sequence and characterizes\neach flow by the aggregate of its upstream and downstream traffic, reducing\ncomplexity while maintaining high discriminability. Furthermore, to address the\nchallenge of class-specific discriminability differences, we propose an\nadaptive threshold mechanism that dynamically adjusts training weights and\nrejection boundaries, enhancing the model's classification performance.\nExperimental results demonstrate that the proposed method achieves superior\nclassification performance and robustness on both coarse-grained and\nfine-grained datasets, as well as under concept drift and open-world scenarios.\nCode and Dataset are available at https://github.com/kid1999/UDFS."
    },
    {
        "date": "2025-09",
        "title": "RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations",
        "author": "Mintae Kim, Jiaze Cai, and Koushil Sreenath",
        "link": "http://arxiv.org/abs/2509.11149v1",
        "abstract": "Designing robust controllers for precise, arbitrary trajectory tracking with\nquadrotors is challenging due to nonlinear dynamics and underactuation, and\nbecomes harder with flexible cable-suspended payloads that introduce extra\ndegrees of freedom and hybridness. Classical model-based methods offer\nstability guarantees but require extensive tuning and often do not adapt when\nthe configuration changes, such as when a payload is added or removed, or when\nthe payload mass or cable length varies. We present RoVerFly, a unified\nlearning-based control framework in which a reinforcement learning (RL) policy\nserves as a robust and versatile tracking controller for standard quadrotors\nand for cable-suspended payload systems across a range of configurations.\nTrained with task and domain randomization, the controller is resilient to\ndisturbances and varying dynamics. It achieves strong zero-shot generalization\nacross payload settings, including no payload as well as varying mass and cable\nlength, without controller switching or re-tuning, while retaining the\ninterpretability and structure of a feedback tracking controller. Code and\nsupplementary materials are available at\nhttps://github.com/mintaeshkim/roverfly"
    },
    {
        "date": "2025-09",
        "title": "SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats",
        "author": "Qingzhao Zhang, Shaocheng Luo, Z. Morley Mao, Miroslav Pajic, and Michael K. Reiter",
        "link": "http://arxiv.org/abs/2509.11120v3",
        "abstract": "Autonomous vehicles, including self-driving cars, robotic ground vehicles,\nand drones, rely on complex sensor pipelines to ensure safe and reliable\noperation. However, these safety-critical systems remain vulnerable to\nadversarial sensor attacks that can compromise their performance and mission\nsuccess. While extensive research has demonstrated various sensor attack\ntechniques, critical gaps remain in understanding their feasibility in\nreal-world, end-to-end systems. This gap largely stems from the lack of a\nsystematic perspective on how sensor errors propagate through interconnected\nmodules in autonomous systems when autonomous vehicles interact with the\nphysical world.\n  To bridge this gap, we present a comprehensive survey of autonomous vehicle\nsensor attacks across platforms, sensor modalities, and attack methods. Central\nto our analysis is the System Error Propagation Graph (SEPG), a structured\ndemonstration tool that illustrates how sensor attacks propagate through system\npipelines, exposing the conditions and dependencies that determine attack\nfeasibility. With the aid of SEPG, our study distills seven key findings that\nhighlight the feasibility challenges of sensor attacks and uncovers eleven\npreviously overlooked attack vectors exploiting inter-module interactions,\nseveral of which we validate through proof-of-concept experiments.\nAdditionally, we demonstrate how large language models (LLMs) can automate\naspects of SEPG construction and cross-validate expert analysis, showcasing the\npromise of AI-assisted security evaluation."
    },
    {
        "date": "2025-09",
        "title": "Membership Inference Attacks on Recommender System: A Survey",
        "author": "Jiajie He, Yuechun Gu, Keke Chen, and Xintong Chen",
        "link": "http://arxiv.org/abs/2509.11080v1",
        "abstract": "Recommender systems (RecSys) have been widely applied to various\napplications, including E-commerce, finance, healthcare, social media and have\nbecome increasingly influential in shaping user behavior and decision-making,\nhighlighting their growing impact in various domains. However, recent studies\nhave shown that RecSys are vulnerable to membership inference attacks (MIAs),\nwhich aim to infer whether user interaction record was used to train a target\nmodel or not. MIAs on RecSys models can directly lead to a privacy breach. For\nexample, via identifying the fact that a purchase record that has been used to\ntrain a RecSys associated with a specific user, an attacker can infer that\nuser's special quirks. In recent years, MIAs have been shown to be effective on\nother ML tasks, e.g., classification models and natural language processing.\nHowever, traditional MIAs are ill-suited for RecSys due to the unseen posterior\nprobability. Although MIAs on RecSys form a newly emerging and rapidly growing\nresearch area, there has been no systematic survey on this topic yet. In this\narticle, we conduct the first comprehensive survey on RecSys MIAs. This survey\noffers a comprehensive review of the latest advancements in RecSys MIAs,\nexploring the design principles, challenges, attack and defense associated with\nthis emerging field. We provide a unified taxonomy that categorizes different\nRecSys MIAs based on their characterizations and discuss their pros and cons.\nBased on the limitations and gaps identified in this survey, we point out\nseveral promising future research directions to inspire the researchers who\nwish to follow this area. This survey not only serves as a reference for the\nresearch community but also provides a clear description for researchers\noutside this research domain."
    },
    {
        "date": "2025-09",
        "title": "Large Language Models for Security Operations Centers: A Comprehensive Survey",
        "author": "Ali Habibzadeh, Farid Feyzi, and Reza Ebrahimi Atani",
        "link": "http://arxiv.org/abs/2509.10858v1",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of\nunderstanding and generating human-like text, offering transformative potential\nacross diverse domains. The Security Operations Center (SOC), responsible for\nsafeguarding digital infrastructure, represents one of these domains. SOCs\nserve as the frontline of defense in cybersecurity, tasked with continuous\nmonitoring, detection, and response to incidents. However, SOCs face persistent\nchallenges such as high alert volumes, limited resources, high demand for\nexperts with advanced knowledge, delayed response times, and difficulties in\nleveraging threat intelligence effectively. In this context, LLMs can offer\npromising solutions by automating log analysis, streamlining triage, improving\ndetection accuracy, and providing the required knowledge in less time. This\nsurvey systematically explores the integration of generative AI and more\nspecifically LLMs into SOC workflow, providing a structured perspective on its\ncapabilities, challenges, and future directions. We believe that this survey\noffers researchers and SOC managers a broad overview of the current state of\nLLM integration within academic study. To the best of our knowledge, this is\nthe first comprehensive study to examine LLM applications in SOCs in details."
    },
    {
        "date": "2025-09",
        "title": "ORQ: Complex Analytics on Private Data with Strong Security Guarantees",
        "author": "Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, and John Liagouris",
        "link": "http://arxiv.org/abs/2509.10793v1",
        "abstract": "We present ORQ, a system that enables collaborative analysis of large private\ndatasets using cryptographically secure multi-party computation (MPC). ORQ\nprotects data against semi-honest or malicious parties and can efficiently\nevaluate relational queries with multi-way joins and aggregations that have\nbeen considered notoriously expensive under MPC. To do so, ORQ eliminates the\nquadratic cost of secure joins by leveraging the fact that, in practice, the\nstructure of many real queries allows us to join records and apply the\naggregations \"on the fly\" while keeping the result size bounded. On the system\nside, ORQ contributes generic oblivious operators, a data-parallel vectorized\nquery engine, a communication layer that amortizes MPC network costs, and a\ndataflow API for expressing relational analytics -- all built from the ground\nup.\n  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,\nincluding complex queries with multiple joins and custom aggregations. When\ncompared to state-of-the-art solutions, ORQ significantly reduces MPC execution\ntimes and can process one order of magnitude larger datasets. For our most\nchallenging workload, the full TPC-H benchmark, we report results entirely\nunder MPC with Scale Factor 10 -- a scale that had previously been achieved\nonly with information leakage or the use of trusted third parties."
    },
    {
        "date": "2025-09",
        "title": "GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research",
        "author": "Luke Howard",
        "link": "http://arxiv.org/abs/2509.10790v1",
        "abstract": "Transformers have become the foundation for a wide range of\nstate--of--the--art models across natural language processing, computer vision,\nand other machine learning domains. Despite their widespread deployment, the\nrobustness of these models under fault conditions remains underexplored. We\npresent GoldenTransformer, a modular and extensible fault injection framework\ndesigned to evaluate the resiliency of Large Language Models to induced\nhardware faults. GoldenTransformer offers a unified Python-based platform for\ninjecting diverse classes of faults--such as weight corruption, activation\ninjections, and attention--level disruptions--into pretrained\ntransformer--based models. Inspired by the GoldenEye simulator for DNNs, our\nframework focuses on the unique challenges of working with large transformer\narchitectures, including considerations such as structural complexity, latent\ndependencies, and nonuniform layer definitions. GoldenTransformer is built atop\nPyTorch and HuggingFace Transformers, and it supports experiment\nreproducibility, metric logging, and visualization out of the box. We detail\nthe technical design and use of GoldenTransformer and demonstrate through\nseveral example experiments on classification and generation tasks. By enabling\ncontrolled injection of faults at multiple logical and structural points in a\ntransformer, GoldenTransformer offers researchers and practitioners a valuable\ntool for model robustness analysis and for guiding dependable system design in\nreal-world LLM applications."
    },
    {
        "date": "2025-09",
        "title": "Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations",
        "author": "Zhongtang Luo, Jianting Zhang, Akshat Neerati, and Aniket Kate",
        "link": "http://arxiv.org/abs/2509.10755v1",
        "abstract": "The Tor network offers network anonymity to its users by routing their\ntraffic through a sequence of relays. A group of nine directory authorities\nmaintains information about all available relay nodes using a distributed\ndirectory protocol. We observe that the current protocol makes a steep\nsynchrony assumption, which makes it vulnerable to natural as well as\nadversarial non-synchronous communication scenarios over the Internet. In this\npaper, we show that it is possible to cause a failure in the Tor directory\nprotocol by targeting a majority of the authorities for only five minutes using\na well-executed distributed denial-of-service (DDoS) attack. We demonstrate\nthis attack in a controlled environment and show that it is cost-effective for\nas little as \\$53.28 per month to disrupt the protocol and to effectively bring\ndown the entire Tor network. To mitigate this problem, we consider the popular\npartial synchrony assumption for the Tor directory protocol that ensures that\nthe protocol security is hampered even when the network delays are large and\nunknown. We design a new Tor directory protocol that leverages any standard\npartial-synchronous consensus protocol to solve this problem, while also\nproving its security. We have implemented a prototype in Rust, demonstrating\ncomparable performance to the current protocol while resisting similar attacks."
    },
    {
        "date": "2025-09",
        "title": "Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip",
        "author": "Luigi Logrippo",
        "link": "http://arxiv.org/abs/2509.10727v1",
        "abstract": "The multi level Bell La Padula model for secure data access and data flow\ncontrol, formulated in the 1970s, was based on the theory of partial orders.\nSince then, another model, based on lattice theory, has prevailed. We present\nreasons why the partial order model is more appropriate. We also show, by\nexample, how non lattice data flow networks can be easily implemented by using\nAttribute-based access control (ABAC)."
    },
    {
        "date": "2025-09",
        "title": "Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential",
        "author": "Charankumar Akiri, Harrison Simpson, Kshitiz Aryal, Aarav Khanna, and Maanak Gupta",
        "link": "http://arxiv.org/abs/2509.10655v1",
        "abstract": "While the widespread deployment of Large Language Models (LLMs) holds great\npotential for society, their vulnerabilities to adversarial manipulation and\nexploitation can pose serious safety, security, and ethical risks. As new\nthreats continue to emerge, it becomes critically necessary to assess the\nlandscape of LLMs' safety and security against evolving adversarial prompt\ntechniques. To understand the behavior of LLMs, this research provides an\nempirical analysis and risk profile of nine prominent LLMs, Claude Opus 4,\nDeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3,\nLlama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and\nsafety categories. These LLMs are evaluated on their ability to produce harmful\nresponses for adversarially crafted prompts (dataset has been made public) for\na broad range of safety and security topics, such as promotion of violent\ncriminal behavior, promotion of non-violent criminal activity, societal harms\nrelated to safety, illegal sexual content, dangerous code generation, and\ncybersecurity threats beyond code. Our study introduces the Risk Severity Index\n(RSI), an agile and scalable evaluation score, to quantify and compare the\nsecurity posture and creating a risk profile of LLMs. As the LLM development\nlandscape progresses, the RSI is intended to be a valuable metric for comparing\nthe risks of LLMs across evolving threats. This research finds widespread\nvulnerabilities in the safety filters of the LLMs tested and highlights the\nurgent need for stronger alignment, responsible deployment practices, and model\ngovernance, particularly for open-access and rapidly iterated models."
    },
    {
        "date": "2025-09",
        "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention",
        "author": "Matteo Trippodo, Federico Becattini, and Lorenzo Seidenari",
        "link": "http://arxiv.org/abs/2509.10359v1",
        "abstract": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible."
    },
    {
        "date": "2025-09",
        "title": "SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs",
        "author": "Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, and Leandros Maglaras",
        "link": "http://arxiv.org/abs/2509.10594v1",
        "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping\ntoday's business practices, however, their adoption within small and\nmedium-sized enterprises (SMEs) raises significant technical, ethical and trust\nissues. This paper proposes a structured, multi-phased framework designed to\nembed trust and ethical principles throughout the AI lifecycle for their secure\nand responsible use in SMEs. Structured around four pillars, i.e., Data,\nAlgorithms, Human oversight, and Model Architecture, the framework bridges\ntheoretical ethical principles with operational practice, enhancing AI\ncapabilities in diverse SME applications. Ultimately, this paper offers a\nstructured roadmap for responsible AI adoption, framing trust and ethics as a\ncatalyst for resilience, competitiveness, and sustainable innovation in SMEs."
    },
    {
        "date": "2025-09",
        "title": "Innovating Augmented Reality Security: Recent E2E Encryption Approaches",
        "author": "Hamish Alsop, Leandros Maglaras, Helge Janicke, Iqbal H. Sarker, and Mohamed Amine Ferrag",
        "link": "http://arxiv.org/abs/2509.10313v1",
        "abstract": "End-to-end encryption (E2EE) has emerged as a fundamental element of modern\ndigital communication, protecting data from unauthorized access during\ntransmission. By design, E2EE ensures that only the intended recipient can\ndecrypt the information, making it inaccessible even to service providers. Yet,\nthis powerful safeguard of individual privacy and digital trust also introduces\na paradox: it can simultaneously prevent law enforcement efforts by hiding\npotential malicious activities. This paper examines the dual role of E2EE, its\ncritical importance to privacy, the challenges it"
    },
    {
        "date": "2025-09",
        "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks",
        "author": "Laith Nayal, Mahmoud Mousatat, and Bader Rasheed",
        "link": "http://arxiv.org/abs/2509.10298v1",
        "abstract": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules."
    },
    {
        "date": "2025-09",
        "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case",
        "author": "Salih Toprak, and Muge Erel-Ozcevik",
        "link": "http://arxiv.org/abs/2509.10291v1",
        "abstract": "In disaster scenarios where conventional energy infrastructure is\ncompromised, secure and traceable energy trading between solar-powered\nhouseholds and mobile charging units becomes a necessity. To ensure the\nintegrity of such transactions over a blockchain network, robust and\nunpredictable nonce generation is vital. This study proposes an SDN-enabled\narchitecture where machine learning regressors are leveraged not for their\naccuracy, but for their potential to generate randomized values suitable as\nnonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN\nallows flexible control over data flows and energy routing policies even in\nfragmented or degraded networks, ensuring adaptive response during emergencies.\nUsing a 9000-sample dataset, we evaluate five AutoML-selected regression models\n- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest\nNeighbors - not by their prediction accuracy, but by their ability to produce\ndiverse and non-deterministic outputs across shuffled data inputs. Randomness\nanalysis reveals that Random Forest and Extra Trees regressors exhibit complete\ndependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and\nLightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and\n99.9%, respectively). These findings highlight that certain machine learning\nmodels, particularly tree-based ensembles, may serve as effective and\nlightweight nonce generators within blockchain-secured, SDN-based energy\ntrading infrastructures resilient to disaster conditions."
    },
    {
        "date": "2025-09",
        "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI",
        "author": "Ema Masterl, Tina Vipotnik Vesnaver, and \u017diga \u0160piclin",
        "link": "http://arxiv.org/abs/2509.10257v1",
        "abstract": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability."
    },
    {
        "date": "2025-09",
        "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications",
        "author": "Janis Keuper",
        "link": "http://arxiv.org/abs/2509.10248v2",
        "abstract": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
    },
    {
        "date": "2025-09",
        "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach",
        "author": "Tamir Shazman, Idan Lev-Yehudi, Ron Benchetit, and Vadim Indelman",
        "link": "http://arxiv.org/abs/2509.10162v1",
        "abstract": "Online planning in Markov Decision Processes (MDPs) enables agents to make\nsequential decisions by simulating future trajectories from the current state,\nmaking it well-suited for large-scale or dynamic environments. Sample-based\nmethods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely\nadopted for their ability to approximate optimal actions using a generative\nmodel. However, in practical settings, the generative model is often learned\nfrom limited data, introducing approximation errors that can degrade\nperformance or lead to unsafe behaviors. To address these challenges, Robust\nMDPs (RMDPs) offer a principled framework for planning under model uncertainty,\nyet existing approaches are typically computationally intensive and not suited\nfor real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the\nfirst online planning algorithm for RMDPs with finite-sample theoretical\nperformance guarantees. Unlike Sparse Sampling, which estimates the nominal\nvalue function, RSS computes a robust value function by leveraging the\nefficiency and theoretical properties of Sample Average Approximation (SAA),\nenabling tractable robust policy computation in online settings. RSS is\napplicable to infinite or continuous state spaces, and its sample and\ncomputational complexities are independent of the state space size. We provide\ntheoretical performance guarantees and empirically show that RSS outperforms\nstandard Sparse Sampling in environments with uncertain dynamics."
    },
    {
        "date": "2025-09",
        "title": "Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching",
        "author": "Seyed Moein Abtahi, and Akramul Azim",
        "link": "http://arxiv.org/abs/2509.09970v1",
        "abstract": "Large Language Models (LLMs) show promise in generating firmware for embedded\nsystems, but often introduce security flaws and fail to meet real-time\nperformance constraints. This paper proposes a three-phase methodology that\ncombines LLM-based firmware generation with automated security validation and\niterative refinement in a virtualized environment. Using structured prompts,\nmodels like GPT-4 generate firmware for networking and control tasks, deployed\non FreeRTOS via QEMU. These implementations are tested using fuzzing, static\nanalysis, and runtime monitoring to detect vulnerabilities such as buffer\noverflows (CWE-120), race conditions (CWE-362), and denial-of-service threats\n(CWE-400). Specialized AI agents for Threat Detection, Performance\nOptimization, and Compliance Verification collaborate to improve detection and\nremediation. Identified issues are categorized using CWE, then used to prompt\ntargeted LLM-generated patches in an iterative loop. Experiments show a 92.4\\%\nVulnerability Remediation Rate (37.3\\% improvement), 95.8\\% Threat Model\nCompliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms\nworst-case execution time and 195{\\mu}s jitter. This process enhances firmware\nsecurity and performance while contributing an open-source dataset for future\nresearch."
    },
    {
        "date": "2025-09",
        "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation",
        "author": "Vu-Minh Le, Thao-Anh Tran, Duc Huy Do, Xuan Canh Do, Huong Ninh, and Hai Tran",
        "link": "http://arxiv.org/abs/2509.09946v1",
        "abstract": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard."
    },
    {
        "date": "2025-09",
        "title": "SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization",
        "author": "Lei Yu, Jingyuan Zhang, Xin Wang, Jiajia Ma, Li Yang, and Fengjun Zhang",
        "link": "http://arxiv.org/abs/2509.09942v1",
        "abstract": "Smart contracts automate the management of high-value assets, where\nvulnerabilities can lead to catastrophic financial losses. This challenge is\namplified in Large Language Models (LLMs) by two interconnected failures: they\noperate as unauditable \"black boxes\" lacking a transparent reasoning process,\nand consequently, generate code riddled with critical security vulnerabilities.\nTo address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a\nnovel framework for secure and explainable smart contract generation. It begins\nwith Continual Pre-training (CPT) to specialize the model. We then apply Long\nChain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated\nreasoning-and-code samples to train the model to emulate human security\nanalysis. Finally, to directly mitigate vulnerabilities, we employ\nSecurity-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement\nlearning phase that refines the generation policy by optimizing a weighted\nreward signal for compilation success, security compliance, and format\ncorrectness. Evaluated against 17 baselines on a benchmark of 756 real-world\nfunctions, SmartCoder-R1 establishes a new state of the art, achieving top\nperformance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a\nSafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This\nFullRate marks a 45.79% relative improvement over the strongest baseline,\nDeepSeek-R1. Crucially, its generated reasoning also excels in human\nevaluations, achieving high-quality ratings for Functionality (82.7%), Security\n(85.3%), and Clarity (90.7%)."
    },
    {
        "date": "2025-09",
        "title": "NISQ Security and Complexity via Simple Classical Reasoning",
        "author": "Alexandru Cojocaru, Juan Garay, Qipeng Liu, and Fang Song",
        "link": "http://arxiv.org/abs/2509.09900v1",
        "abstract": "We give novel lifting theorems for security games in the quantum random\noracle model (QROM) in Noisy Intermediate-Scale Quantum (NISQ) settings such as\nthe hybrid query model, the noisy oracle and the bounded-depth models. We\nprovide, for the first time, a hybrid lifting theorem for hybrid algorithms\nthat can perform both quantum and classical queries, as well as a lifting\ntheorem for quantum algorithms with access to noisy oracles or bounded quantum\ndepth.\n  At the core of our results lies a novel measure-and-reprogram framework,\ncalled hybrid coherent measure-and-reprogramming, tailored specifically for\nhybrid algorithms. Equipped with the lifting theorem, we are able to prove\ndirectly NISQ security and complexity results by calculating a single\ncombinatorial quantity, relying solely on classical reasoning.\n  As applications, we derive the first direct product theorems in the average\ncase, in the hybrid setting-i.e., an enabling tool to determine the hybrid\nhardness of solving multi-instance security games. This allows us to derive in\na straightforward manner the NISQ hardness of various security games, such as\n(i) the non-uniform hardness of salted games, (ii) the hardness of specific\ncryptographic tasks such as the multiple instance version of one-wayness and\ncollision-resistance, and (iii) uniform or non-uniform hardness of many other\ngames."
    },
    {
        "date": "2025-09",
        "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration",
        "author": "Yihao Liu, Junyu Chen, Lianrui Zuo, Shuwen Wei, Brian D. Boyd, Carmen Andreescu, Olusola Ajilore, Warren D. Taylor, Aaron Carass, and Bennett A. Landman",
        "link": "http://arxiv.org/abs/2509.09869v1",
        "abstract": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios."
    },
    {
        "date": "2025-09",
        "title": "Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things",
        "author": "Prokash Barman, Ratul Chowdhury, and Banani Saha",
        "link": "http://arxiv.org/abs/2509.10581v1",
        "abstract": "In modern smart systems, the convergence of the Internet of Things (IoT) and\nWireless of Things (WoT) have been revolutionized by offering a broad level of\nwireless connectivity and communication among various devices. Hitherto, this\ngreater interconnectivity poses important security problems, including the\nquestion of how to securely interconnect different networks, preserve secure\ncommunication channels, and maintain data integrity. However, the traditional\ncryptographic method and frequency hopping technique, although they provide\nsome protection, are not sufficient to defend against Man-In-The-Middle,\njamming, and replay attacks. In addition, synchronization issues in\nmulti-channel communication systems result in increased latency and energy\nconsumption, which make them unsuitable for resource-constrained IoT and WoT\ndevices. This work presents the Multi-Channel Secure Communication (MCSC)\nframework, which integrates advanced cryptographic protocols with dynamic\nchannel-hopping strategies to enhance security with reduced synchronization\noverhead. The MCSC framework maximizes the critical performance metrics, such\nas packet delivery ratio, latency, throughput, and energy efficiency, and\nfulfills the specific requirements of the IoT and WoT networks. A comprehensive\ncomparison of MCSC with well-established methods, including Frequency Hop\nSpread Spectrum, single channel Advanced Encryption Standard, and various\nElliptic Curve Cryptography-based schemes, indicates that MCSC has lower error\nrates and is more resilient to a wider range of cyber attacks. The efficiency\nof the proposed solution to secure IoT and WoT networks without compromising\nthe operational performance is validated under various interference conditions."
    },
    {
        "date": "2025-09",
        "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
        "author": "Tim Broedermannn, Christos Sakaridis, Luigi Piccinelli, Wim Abbeloos, and Luc Van Gool",
        "link": "http://arxiv.org/abs/2509.09828v1",
        "abstract": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion"
    },
    {
        "date": "2025-09",
        "title": "ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)",
        "author": "Nojan Sheybani, Alessandro Pegoraro, Jonathan Knauer, Phillip Rieger, Elissa Mollakuqe, Farinaz Koushanfar, and Ahmad-Reza Sadeghi",
        "link": "http://arxiv.org/abs/2509.09787v1",
        "abstract": "Split Learning (SL) is a distributed learning approach that enables\nresource-constrained clients to collaboratively train deep neural networks\n(DNNs) by offloading most layers to a central server while keeping in- and\noutput layers on the client-side. This setup enables SL to leverage server\ncomputation capacities without sharing data, making it highly effective in\nresource-constrained environments dealing with sensitive data. However, the\ndistributed nature enables malicious clients to manipulate the training\nprocess. By sending poisoned intermediate gradients, they can inject backdoors\ninto the shared DNN. Existing defenses are limited by often focusing on\nserver-side protection and introducing additional overhead for the server. A\nsignificant challenge for client-side defenses is enforcing malicious clients\nto correctly execute the defense algorithm.\n  We present ZORRO, a private, verifiable, and robust SL defense scheme.\nThrough our novel design and application of interactive zero-knowledge proofs\n(ZKPs), clients prove their correct execution of a client-located defense\nalgorithm, resulting in proofs of computational integrity attesting to the\nbenign nature of locally trained DNN portions. Leveraging the frequency\nrepresentation of model partitions enables ZORRO to conduct an in-depth\ninspection of the locally trained models in an untrusted environment, ensuring\nthat each client forwards a benign checkpoint to its succeeding client. In our\nextensive evaluation, covering different model architectures as well as various\nattack strategies and data scenarios, we show ZORRO's effectiveness, as it\nreduces the attack success rate to less than 6\\% while causing even for models\nstoring \\numprint{1000000} parameters on the client-side an overhead of less\nthan 10 seconds."
    },
    {
        "date": "2025-09",
        "title": "The Coding Limits of Robust Watermarking for Generative Models",
        "author": "Danilo Francati, Yevin Nikhel Goonatilake, Shubham Pawar, Daniele Venturi, and Giuseppe Ateniese",
        "link": "http://arxiv.org/abs/2509.10577v1",
        "abstract": "We prove a sharp threshold for the robustness of cryptographic watermarking\nfor generative models. This is achieved by introducing a coding abstraction,\nwhich we call messageless secret-key codes, that formalizes sufficient and\nnecessary requirements of robust watermarking: soundness, tamper detection, and\npseudorandomness. Thus, we establish that robustness has a precise limit: For\nbinary outputs no scheme can survive if more than half of the encoded bits are\nmodified, and for an alphabet of size q the corresponding threshold is\n$(1-1/q)$ of the symbols.\n  Complementing this impossibility, we give explicit constructions that meet\nthe bound up to a constant slack. For every ${\\delta} > 0$, assuming\npseudorandom functions and access to a public counter, we build linear-time\ncodes that tolerate up to $(1/2)(1-{\\delta})$ errors in the binary case and\n$(1-1/q)(1-{\\delta})$ errors in the $q$-ary case. Together with the lower\nbound, these yield the maximum robustness achievable under standard\ncryptographic assumptions.\n  We then test experimentally whether this limit appears in practice by looking\nat the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We\nshow that a simple crop and resize operation reliably flipped about half of the\nlatent signs and consistently prevented belief-propagation decoding from\nrecovering the codeword, erasing the watermark while leaving the image visually\nintact.\n  These results provide a complete characterization of robust watermarking,\nidentifying the threshold at which robustness fails, constructions that achieve\nit, and an experimental confirmation that the threshold is already reached in\npractice."
    },
    {
        "date": "2025-09",
        "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust Neural Audio Compression at Low Bit-rates",
        "author": "Harry Julian, Rachel Beeson, Lohith Konathala, Johanna Ulin, and Jiameng Gao",
        "link": "http://arxiv.org/abs/2509.09550v2",
        "abstract": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel."
    },
    {
        "date": "2025-09",
        "title": "ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning",
        "author": "Sena Ergisi, Luis Ma\u00dfny, and Rawad Bitar",
        "link": "http://arxiv.org/abs/2509.09534v1",
        "abstract": "Federated Learning (FL) emerged as a widely studied paradigm for distributed\nlearning. Despite its many advantages, FL remains vulnerable to adversarial\nattacks, especially under data heterogeneity. We propose a new Byzantine-robust\nFL algorithm called ProDiGy. The key novelty lies in evaluating the client\ngradients using a joint dual scoring system based on the gradients' proximity\nand dissimilarity. We demonstrate through extensive numerical experiments that\nProDiGy outperforms existing defenses in various scenarios. In particular, when\nthe clients' data do not follow an IID distribution, while other defense\nmechanisms fail, ProDiGy maintains strong defense capabilities and model\naccuracy. These findings highlight the effectiveness of a dual perspective\napproach that promotes natural similarity among honest clients while detecting\nsuspicious uniformity as a potential indicator of an attack."
    },
    {
        "date": "2025-09",
        "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language Models",
        "author": "Zhiyu He, Maojiang Wang, Xinwen Gao, Yuchuan Luo, Lin Liu, and Shaojing Fu",
        "link": "http://arxiv.org/abs/2509.09424v1",
        "abstract": "Secure inference enables privacy-preserving machine learning by leveraging\ncryptographic protocols that support computations on sensitive user data\nwithout exposing it. However, integrating cryptographic protocols with large\nlanguage models (LLMs) presents significant challenges, as the inherent\ncomplexity of these protocols, together with LLMs' massive parameter scale and\nsophisticated architectures, severely limits practical usability. In this work,\nwe propose ENSI, a novel non-interactive secure inference framework for LLMs,\nbased on the principle of co-designing the cryptographic protocols and LLM\narchitecture. ENSI employs an optimized encoding strategy that seamlessly\nintegrates CKKS scheme with a lightweight LLM variant, BitNet, significantly\nreducing the computational complexity of encrypted matrix multiplications. In\nresponse to the prohibitive computational demands of softmax under homomorphic\nencryption (HE), we pioneer the integration of the sigmoid attention mechanism\nwith HE as a seamless, retraining-free alternative. Furthermore, by embedding\nthe Bootstrapping operation within the RMSNorm process, we efficiently refresh\nciphertexts while markedly decreasing the frequency of costly bootstrapping\ninvocations. Experimental evaluations demonstrate that ENSI achieves\napproximately an 8x acceleration in matrix multiplications and a 2.6x speedup\nin softmax inference on CPU compared to state-of-the-art method, with the\nproportion of bootstrapping is reduced to just 1%."
    },
    {
        "date": "2025-09",
        "title": "Robust Non-Linear Correlations via Polynomial Regression",
        "author": "Luca Giuliani, and Michele Lombardi",
        "link": "http://arxiv.org/abs/2509.09380v1",
        "abstract": "The Hirschfeld-Gebelein-R\\'enyi (HGR) correlation coefficient is an extension\nof Pearson's correlation that is not limited to linear correlations, with\npotential applications in algorithmic fairness, scientific analysis, and causal\ndiscovery. Recently, novel algorithms to estimate HGR in a differentiable\nmanner have been proposed to facilitate its use as a loss regularizer in\nconstrained machine learning applications. However, the inherent\nuncomputability of HGR requires a bias-variance trade-off, which can possibly\ncompromise the robustness of the proposed methods, hence raising technical\nconcerns if applied in real-world scenarios. We introduce a novel computational\napproach for HGR that relies on user-configurable polynomial kernels, offering\ngreater robustness compared to previous methods and featuring a faster yet\nalmost equally effective restriction. Our approach provides significant\nadvantages in terms of robustness and determinism, making it a more reliable\noption for real-world applications. Moreover, we present a brief experimental\nanalysis to validate the applicability of our approach within a constrained\nmachine learning framework, showing that its computation yields an insightful\nsubgradient that can serve as a loss regularizer."
    },
    {
        "date": "2025-09",
        "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework",
        "author": "Zitao Wang, Nian Si, and Molei Liu",
        "link": "http://arxiv.org/abs/2509.09371v1",
        "abstract": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a\nnovel framework for Wasserstein distributionally robust learning that accounts\nfor predictive representations when guarding against distributional shifts.\nUnlike classical approaches that treat all feature perturbations equally, READ\nembeds a multidimensional alignment parameter into the transport cost, allowing\nthe model to differentially discourage perturbations along directions\nassociated with informative representations. This yields robustness to feature\nvariation while preserving invariant structure. Our first contribution is a\ntheoretical foundation: we show that seminorm regularizations for linear\nregression and binary classification arise as Wasserstein distributionally\nrobust objectives, thereby providing tractable reformulations of READ and\nunifying a broad class of regularized estimators under the DRO lens. Second, we\nadopt a principled procedure for selecting the Wasserstein radius using the\ntechniques of robust Wasserstein profile inference. This further enables the\nconstruction of valid, representation-aware confidence regions for model\nparameters with distinct geometric features. Finally, we analyze the geometry\nof READ estimators as the alignment parameters vary and propose an optimization\nalgorithm to estimate the projection of the global optimum onto this solution\nsurface. This procedure selects among equally robust estimators while optimally\nconstructing a representation structure. We conclude by demonstrating the\neffectiveness of our framework through extensive simulations and a real-world\nstudy, providing a powerful robust estimation grounded in learning\nrepresentation."
    },
    {
        "date": "2025-09",
        "title": "[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future",
        "author": "Harshini Sri Ramulu, Helen Schmitt, Bogdan Rerich, Rachel Gonzalez Rodriguez, Tadayoshi Kohno, and Yasemin Acar",
        "link": "http://arxiv.org/abs/2509.09351v1",
        "abstract": "Ethical questions are discussed regularly in computer security. Still,\nresearchers in computer security lack clear guidance on how to make, document,\nand assess ethical decisions in research when what is morally right or\nacceptable is not clear-cut. In this work, we give an overview of the\ndiscussion of ethical implications in current published work in computer\nsecurity by reviewing all 1154 top-tier security papers published in 2024,\nfinding inconsistent levels of ethics reporting with a strong focus of\nreporting institutional or ethics board approval, human subjects protection,\nand responsible disclosure, and a lack of discussion of balancing harms and\nbenefits. We further report on the results of a semi-structured interview study\nwith 24 computer security and privacy researchers (among whom were also:\nreviewers, ethics committee members, and/or program chairs) and their ethical\ndecision-making both as authors and during peer review, finding a strong desire\nfor ethical research, but a lack of consistency in considered values, ethical\nframeworks (if articulated), decision-making, and outcomes. We present an\noverview of the current state of the discussion of ethics and current de-facto\nstandards in computer security research, and contribute suggestions to improve\nthe state of ethics in computer security research."
    },
    {
        "date": "2025-09",
        "title": "On the Security of SSH Client Signatures",
        "author": "Fabian B\u00e4umer, Marcus Brinkmann, Maximilian Radoy, J\u00f6rg Schwenk, and Juraj Somorovsky",
        "link": "http://arxiv.org/abs/2509.09331v1",
        "abstract": "Administrators and developers use SSH client keys and signatures for\nauthentication, for example, to access internet backbone servers or to commit\nnew code on platforms like GitHub. However, unlike servers, SSH clients cannot\nbe measured through internet scans. We close this gap in two steps. First, we\ncollect SSH client public keys. Such keys are regularly published by their\nowners on open development platforms like GitHub and GitLab. We systematize\nprevious non-academic work by subjecting these keys to various security tests\nin a longitudinal study. Second, in a series of black-box lab experiments, we\nanalyze the implementations of algorithms for SSH client signatures in 24\npopular SSH clients for Linux, Windows, and macOS.\n  We extracted 31,622,338 keys from three public sources in two scans. Compared\nto previous work, we see a clear tendency to abandon RSA signatures in favor of\nEdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139\nkeys generated from weak randomness, and 149 keys with common or small\nfactors-the large majority of the retrieved keys exposed no weakness.\n  Weak randomness can not only compromise a secret key through its public key,\nbut also through signatures. It is well-known that a bias in random nonces in\nECDSA can reveal the secret key through public signatures. For the first time,\nwe show that the use of deterministic nonces in ECDSA can also be dangerous:\nThe private signing key of a PuTTY client can be recovered from just 58 valid\nsignatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our\nfinding in CVE-2024-31497, and they subsequently replaced the nonce generation\nalgorithm."
    },
    {
        "date": "2025-09",
        "title": "Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology",
        "author": "Dylan Peek, Matthew P. Skerritt, and Stephan Chalup",
        "link": "http://arxiv.org/abs/2509.09140v1",
        "abstract": "Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer\ncontrasting approaches to inferring topological structure from data. In this\nstudy, we examine the noise robustness of a supervised neural network trained\nto predict Betti numbers in 2D binary images. We compare an ANN approach\nagainst a PH pipeline based on cubical complexes and the Signed Euclidean\nDistance Transform (SEDT), which is a widely adopted strategy for noise-robust\ntopological analysis. Using one synthetic and two real-world datasets, we show\nthat ANNs can outperform this PH approach under noise, likely due to their\ncapacity to learn contextual and geometric priors from training data. Though\nstill emerging, the use of ANNs for topology estimation offers a compelling\nalternative to PH under structural noise."
    },
    {
        "date": "2025-09",
        "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
        "author": "Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, and Qichuan Ding",
        "link": "http://arxiv.org/abs/2509.09118v1",
        "abstract": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks."
    },
    {
        "date": "2025-09",
        "title": "CryptGNN: Enabling Secure Inference for Graph Neural Networks",
        "author": "Pritam Sen, Yao Ma, and Cristian Borcea",
        "link": "http://arxiv.org/abs/2509.09107v1",
        "abstract": "We present CryptGNN, a secure and effective inference solution for\nthird-party graph neural network (GNN) models in the cloud, which are accessed\nby clients as ML as a service (MLaaS). The main novelty of CryptGNN is its\nsecure message passing and feature transformation layers using distributed\nsecure multi-party computation (SMPC) techniques. CryptGNN protects the\nclient's input data and graph structure from the cloud provider and the\nthird-party model owner, and it protects the model parameters from the cloud\nprovider and the clients. CryptGNN works with any number of SMPC parties, does\nnot require a trusted server, and is provably secure even if P-1 out of P\nparties in the cloud collude. Theoretical analysis and empirical experiments\ndemonstrate the security and efficiency of CryptGNN."
    },
    {
        "date": "2025-09",
        "title": "When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning",
        "author": "Sichen Zhu, Hoyeung Leung, Xiaoyi Wang, Jia Wei, and Honghui Xu",
        "link": "http://arxiv.org/abs/2509.08995v1",
        "abstract": "The integration of Large Language Models (LLMs) into financial technology\n(FinTech) has revolutionized the analysis and processing of complex financial\ndata, driving advancements in real-time decision-making and analytics. With the\ngrowing trend of deploying AI models on edge devices for financial\napplications, ensuring the privacy of sensitive financial data has become a\nsignificant challenge. To address this, we propose DPFinLLM, a\nprivacy-enhanced, lightweight LLM specifically designed for on-device financial\napplications. DPFinLLM combines a robust differential privacy mechanism with a\nstreamlined architecture inspired by state-of-the-art models, enabling secure\nand efficient processing of financial data. This proposed DPFinLLM can not only\nsafeguard user data from privacy breaches but also ensure high performance\nacross diverse financial tasks. Extensive experiments on multiple financial\nsentiment datasets validate the effectiveness of DPFinLLM, demonstrating its\nability to achieve performance comparable to fully fine-tuned models, even\nunder strict privacy constraints."
    },
    {
        "date": "2025-09",
        "title": "Cross-Service Token: Finding Attacks in 5G Core Networks",
        "author": "Anqi Chen, Riccardo Preatoni, Alessandro Brighente, Mauro Conti, and Cristina Nita-Rotaru",
        "link": "http://arxiv.org/abs/2509.08992v1",
        "abstract": "5G marks a major departure from previous cellular architectures, by\ntransitioning from a monolithic design of the core network to a Service-Based\nArchitecture (SBA) where services are modularized as Network Functions (NFs)\nwhich communicate with each other via standard-defined HTTP-based APIs called\nService-Based Interfaces (SBIs). These NFs are deployed in private and public\ncloud infrastructure, and an access control framework based on OAuth restricts\nhow they communicate with each other and obtain access to resources. Given the\nincreased vulnerabilities of clouds to insiders, it is important to study the\nsecurity of the 5G Core services for vulnerabilities that allow attackers to\nuse compromised NFs to obtain unauthorized access to resources.\n  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover\nsecurity flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from\n3GPP API specifications to generate malformed, unexpected, or semantically\ninconsistent inputs, and it integrates automated bug detection with manual\nvalidation and root-cause analysis. We evaluate our approach on free5GC, the\nonly open-source 5G core implementing Release 17-compliant SBIs with an access\ncontrol mechanism. Using FivGeeFuzz, we discovered 8 previously unknown\nvulnerabilities in free5GC, leading to runtime crashes, improper error\nhandling, and unauthorized access to resources, including a very severe attack\nwe call Cross-Service Token Attack. All bugs were confirmed by the free5GC\nteam, 7 have already been patched, and the remaining one has a patch under\ndevelopment."
    },
    {
        "date": "2025-09",
        "title": "Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples",
        "author": "Le Duc Hieu",
        "link": "http://arxiv.org/abs/2509.08954v1",
        "abstract": "We study when the \\emph{optimization curve} of first-order methods -- the\nsequence \\${f(x\\_n)}*{n\\ge0}\\$ produced by constant-stepsize iterations -- is\nconvex, equivalently when the forward differences \\$f(x\\_n)-f(x*{n+1})\\$ are\nnonincreasing. For gradient descent (GD) on convex \\$L\\$-smooth functions, the\ncurve is convex for all stepsizes \\$\\eta \\le 1.75/L\\$, and this threshold is\ntight. Moreover, gradient norms are nonincreasing for all \\$\\eta \\le 2/L\\$, and\nin continuous time (gradient flow) the curve is always convex. These results\ncomplement and refine the classical smooth convex optimization toolbox,\nconnecting discrete and continuous dynamics as well as worst-case analyses."
    },
    {
        "date": "2025-09",
        "title": "Quantum Error Correction in Adversarial Regimes",
        "author": "Rahul Arvind, Nikhil Bansal, Dax Enshan Koh, Tobias Haug, and Kishor Bharti",
        "link": "http://arxiv.org/abs/2509.08943v1",
        "abstract": "In adversarial settings, where attackers can deliberately and strategically\ncorrupt quantum data, standard quantum error correction reaches its limits. It\ncan only correct up to half the code distance and must output a unique answer.\nQuantum list decoding offers a promising alternative. By allowing the decoder\nto output a short list of possible errors, it becomes possible to tolerate far\nmore errors, even under worst-case noise. But two fundamental questions remain:\nwhich quantum codes support list decoding, and can we design decoding schemes\nthat are secure against efficient, computationally bounded adversaries? In this\nwork, we answer both. To identify which codes are list-decodable, we provide a\ngeneralized version of the Knill-Laflamme conditions. Then, using tools from\nquantum cryptography, we build an unambiguous list decoding protocol based on\npseudorandom unitaries. Our scheme is secure against any quantum\npolynomial-time adversary, even across multiple decoding attempts, in contrast\nto previous schemes. Our approach connects coding theory with complexity-based\nquantum cryptography, paving the way for secure quantum information processing\nin adversarial settings."
    },
    {
        "date": "2025-09",
        "title": "Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty",
        "author": "Xenia Konti, Yi Shen, Zifan Wang, Karl Henrik Johansson, Michael J. Pencina, Nicoleta J. Economou-Zavlanos, and Michael M. Zavlanos",
        "link": "http://arxiv.org/abs/2509.08942v1",
        "abstract": "The performance of machine learning (ML) models critically depends on the\nquality and representativeness of the training data. In applications with\nmultiple heterogeneous data generating sources, standard ML methods often learn\nspurious correlations that perform well on average but degrade performance for\natypical or underrepresented groups. Prior work addresses this issue by\noptimizing the worst-group performance. However, these approaches typically\nassume that the underlying data distributions for each group can be accurately\nestimated using the training data, a condition that is frequently violated in\nnoisy, non-stationary, and evolving environments. In this work, we propose a\nnovel framework that relies on Wasserstein-based distributionally robust\noptimization (DRO) to account for the distributional uncertainty within each\ngroup, while simultaneously preserving the objective of improving the\nworst-group performance. We develop a gradient descent-ascent algorithm to\nsolve the proposed DRO problem and provide convergence results. Finally, we\nvalidate the effectiveness of our method on real-world data."
    },
    {
        "date": "2025-09",
        "title": "Securing Cryptographic Software via Typed Assembly Language (Extended Version)",
        "author": "Shixin Song, Tingzhen Dong, Kosi Nwabueze, Julian Zanders, Andres Erbsen, Adam Chlipala, and Mengjia Yan",
        "link": "http://arxiv.org/abs/2509.08727v1",
        "abstract": "Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$."
    },
    {
        "date": "2025-09",
        "title": "Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing",
        "author": "Shun Takagi, and Satoshi Hasegawa",
        "link": "http://arxiv.org/abs/2509.08709v2",
        "abstract": "In cross-device private federated learning, differentially private\nfollow-the-regularized-leader (DP-FTRL) has emerged as a promising\nprivacy-preserving method. However, existing approaches assume a semi-honest\nserver and have not addressed the challenge of securely removing this\nassumption. This is due to its statefulness, which becomes particularly\nproblematic in practical settings where clients can drop out or be corrupted.\nWhile trusted execution environments (TEEs) might seem like an obvious\nsolution, a straightforward implementation can introduce forking attacks or\navailability issues due to state management. To address this problem, our paper\nintroduces a novel server extension that acts as a trusted computing base (TCB)\nto realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral\nTEE module on the server side to produce verifiable proofs of server actions.\nSome clients, upon being selected, participate in auditing these proofs with\nsmall additional communication and computational demands. This extension\nsolution reduces the size of the TCB while maintaining the system's scalability\nand liveness. We provide formal proofs based on interactive differential\nprivacy, demonstrating privacy guarantee in malicious settings. Finally, we\nexperimentally show that our framework adds small constant overhead to clients\nin several realistic settings."
    },
    {
        "date": "2025-09",
        "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework",
        "author": "Zhen Tian, Christos Anagnostopoulos, Qiyuan Wang, and Zhiwei Gao",
        "link": "http://arxiv.org/abs/2509.08694v1",
        "abstract": "Coastal water segmentation from satellite imagery presents unique challenges\ndue to complex spectral characteristics and irregular boundary patterns.\nTraditional RGB-based approaches often suffer from training instability and\npoor generalization in diverse maritime environments. This paper introduces a\nsystematic robust enhancement framework, referred to as Robust U-Net, that\nleverages HSV color space supervision and multi-modal constraints for improved\ncoastal water segmentation. Our approach integrates five synergistic\ncomponents: HSV-guided color supervision, gradient-based coastline\noptimization, morphological post-processing, sea area cleanup, and connectivity\ncontrol. Through comprehensive ablation studies, we demonstrate that HSV\nsupervision provides the highest impact (0.85 influence score), while the\ncomplete framework achieves superior training stability (84\\% variance\nreduction) and enhanced segmentation quality. Our method shows consistent\nimprovements across multiple evaluation metrics while maintaining computational\nefficiency. For reproducibility, our training configurations and code are\navailable here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet."
    },
    {
        "date": "2025-09",
        "title": "Perfectly-Private Analog Secure Aggregation in Federated Learning",
        "author": "Delio Jaramillo-Velez, Charul Rajput, Ragnar Freij-Hollanti, Camilla Hollanti, and Alexandre Graell i Amat",
        "link": "http://arxiv.org/abs/2509.08683v2",
        "abstract": "In federated learning, multiple parties train models locally and share their\nparameters with a central server, which aggregates them to update a global\nmodel. To address the risk of exposing sensitive data through local models,\nsecure aggregation via secure multiparty computation has been proposed to\nenhance privacy. At the same time, perfect privacy can only be achieved by a\nuniform distribution of the masked local models to be aggregated. This raises a\nproblem when working with real valued data, as there is no measure on the reals\nthat is invariant under the masking operation, and hence information leakage is\nbound to occur. Shifting the data to a finite field circumvents this problem,\nbut as a downside runs into an inherent accuracy complexity tradeoff issue due\nto fixed point modular arithmetic as opposed to floating point numbers that can\nsimultaneously handle numbers of varying magnitudes. In this paper, a novel\nsecure parameter aggregation method is proposed that employs the torus rather\nthan a finite field. This approach guarantees perfect privacy for each party's\ndata by utilizing the uniform distribution on the torus, while avoiding\naccuracy losses. Experimental results show that the new protocol performs\nsimilarly to the model without secure aggregation while maintaining perfect\nprivacy. Compared to the finite field secure aggregation, the torus-based\nprotocol can in some cases significantly outperform it in terms of model\naccuracy and cosine similarity, hence making it a safer choice."
    },
    {
        "date": "2025-09",
        "title": "Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions",
        "author": "Amirhossein Taherpour, Abbas Taherpour, and Tamer Khattab",
        "link": "http://arxiv.org/abs/2509.08654v1",
        "abstract": "This paper presents a feature-based Partially Observable Markov Decision\nProcess (POMDP) framework for quantum network routing, combining belief-state\nplanning with Graph Neural Networks (GNNs) to address partial observability,\ndecoherence, and scalability challenges in dynamic quantum systems. Our\napproach encodes complex quantum network dynamics, including entanglement\ndegradation and time-varying channel noise, into a low-dimensional feature\nspace, enabling efficient belief updates and scalable policy learning. The core\nof our framework is a hybrid GNN-POMDP architecture that processes\ngraph-structured representations of entangled links to learn routing policies,\ncoupled with a noise-adaptive mechanism that fuses POMDP belief updates with\nGNN outputs for robust decision making. We provide a theoretical analysis\nestablishing guarantees for belief convergence, policy improvement, and\nrobustness to noise. Experiments on simulated quantum networks with up to 100\nnodes demonstrate significant improvements in routing fidelity and entanglement\ndelivery rates compared to state-of-the-art baselines, particularly under high\ndecoherence and nonstationary conditions."
    },
    {
        "date": "2025-09",
        "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations",
        "author": "Ron F. Del Rosario, Klaudia Krawiecka, and Christian Schroeder de Witt",
        "link": "http://arxiv.org/abs/2509.08646v1",
        "abstract": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents."
    },
    {
        "date": "2025-09",
        "title": "Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition",
        "author": "Jing-Tong Tzeng, Carlos Busso, and Chi-Chun Lee",
        "link": "http://arxiv.org/abs/2509.08470v1",
        "abstract": "Speech emotion recognition (SER) plays a critical role in building\nemotion-aware speech systems, but its performance degrades significantly under\nnoisy conditions. Although speech enhancement (SE) can improve robustness, it\noften introduces artifacts that obscure emotional cues and adds computational\noverhead to the pipeline. Multi-task learning (MTL) offers an alternative by\njointly optimizing SE and SER tasks. However, conventional shared-backbone\nmodels frequently suffer from gradient interference and representational\nconflicts between tasks. To address these challenges, we propose the Sparse\nMixture-of-Experts Representation Integration Technique (Sparse MERIT), a\nflexible MTL framework that applies frame-wise expert routing over\nself-supervised speech representations. Sparse MERIT incorporates task-specific\ngating networks that dynamically select from a shared pool of experts for each\nframe, enabling parameter-efficient and task-adaptive representation learning.\nExperiments on the MSP-Podcast corpus show that Sparse MERIT consistently\noutperforms baseline models on both SER and SE tasks. Under the most\nchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT\nimproves SER F1-macro by an average of 12.0% over a baseline relying on a SE\npre-processing strategy, and by 3.4% over a naive MTL baseline, with\nstatistical significance on unseen noise conditions. For SE, Sparse MERIT\nimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and\nby 20.0% over the naive MTL baseline. These results demonstrate that Sparse\nMERIT provides robust and generalizable performance for both emotion\nrecognition and enhancement tasks in noisy environments."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey",
        "author": "Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, and Quan Z. Sheng",
        "link": "http://arxiv.org/abs/2509.08463v1",
        "abstract": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy."
    },
    {
        "date": "2025-09",
        "title": "DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation",
        "author": "Charuka Herath, Yogachandran Rahulamathavan, Varuna De Silva, and Sangarapillai Lambotharan",
        "link": "http://arxiv.org/abs/2509.08449v1",
        "abstract": "Federated Learning (FL) enables decentralized model training without sharing\nraw data, offering strong privacy guarantees. However, existing FL protocols\nstruggle to defend against Byzantine participants, maintain model utility under\nnon-independent and identically distributed (non-IID) data, and remain\nlightweight for edge devices. Prior work either assumes trusted hardware, uses\nexpensive cryptographic tools, or fails to address privacy and robustness\nsimultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated\nLearning framework that addresses these limitations using a group-based secure\naggregation approach. Unlike LSFL, which assumes non-colluding semi-honest\nservers, DSFL removes this dependency by revealing a key vulnerability: privacy\nleakage through client-server collusion. DSFL introduces three key innovations:\n(1) a dual-server secure aggregation protocol that protects updates without\nencryption or key exchange, (2) a group-wise credit-based filtering mechanism\nto isolate Byzantine clients based on deviation scores, and (3) a dynamic\nreward-penalty system for enforcing fair participation. DSFL is evaluated on\nMNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in\nboth IID and non-IID settings. It consistently outperforms existing baselines,\nincluding LSFL, homomorphic encryption methods, and differential privacy\napproaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and\n68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar\nthreats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB\ncommunication per round."
    },
    {
        "date": "2025-09",
        "title": "Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset",
        "author": "Mohammed Yacoubi, Omar Moussaoui, and C. Drocourt",
        "link": "http://arxiv.org/abs/2509.10563v1",
        "abstract": "Explainable Artificial Intelligence (XAI) enhances the transparency and\ninterpretability of AI models, addressing their inherent opacity. In\ncybersecurity, particularly within the Internet of Medical Things (IoMT), the\nblack-box nature of AI-driven threat detection poses a significant challenge.\nCybersecurity professionals must not only detect attacks but also understand\nthe reasoning behind AI decisions to ensure trust and accountability. The rapid\nincrease in cyberattacks targeting connected medical devices threatens patient\nsafety and data privacy, necessitating advanced AI-driven solutions. This study\ncompares two ensemble learning techniques, bagging and boosting, for\ncyber-attack classification in IoMT environments. We selected Random Forest for\nbagging and CatBoost for boosting. Random Forest helps reduce variance, while\nCatBoost improves bias by combining weak classifiers into a strong ensemble\nmodel, making them effective for detecting sophisticated attacks. However,\ntheir complexity often reduces transparency, making it difficult for\ncybersecurity professionals to interpret and trust their decisions. To address\nthis issue, we apply XAI models to generate local and global explanations,\nproviding insights into AI decision-making. Using techniques like SHAP (Shapley\nAdditive Explanations) and LIME (Local Interpretable Model-agnostic\nExplanations), we highlight feature importance to help stakeholders understand\nthe key factors driving cyber threat detection."
    },
    {
        "date": "2025-09",
        "title": "Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records",
        "author": "Abdou-Essamad Jabri, C. Drocourt, Mostafa Azizi, and Gil Utard",
        "link": "http://arxiv.org/abs/2509.08402v1",
        "abstract": "The integration of the Internet of Things (IoT) in healthcare has\nrevolutionized patient monitoring and data collection, allowing real-time\ntracking of vital signs, remote diagnostics, and automated medical responses.\nHowever, the transmission and storage of sensitive medical data introduce\nsignificant security and privacy challenges. To address these concerns,\nblockchain technology provides a decentralized and immutable ledger that\nensures data integrity, , and transparency. Unlike public blockchains, private\nblockchains are permissioned; the access is granted only to authorized\nparticipants; they are more suitable for handling confidential healthcare data.\nAlthough blockchain ensures security and trust, it lacks built-in mechanisms to\nsupport flexible and controlled data sharing; This is where Proxy Re-Encryption\n(PRE) comes into play. PRE is a cryptographic technique that allows encrypted\ndata to be re-encrypted for a new recipient without exposing it to\nintermediaries. We propose an architecture integrating private blockchain and\nPRE to enable secure, traceable, and privacy-preserving data sharing in\nIoT-based healthcare systems. Blockchain guarantees tamper proof\nrecord-keeping, while PRE enables fine-grained access control, allowing medical\nprofessionals to securely share patient data without compromising\nconfidentiality. This combination creates a robust security framework that\nenhances trust and efficiency in digital healthcare ecosystems."
    },
    {
        "date": "2025-09",
        "title": "Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate Solution",
        "author": "Aduma Rishith, Aditya Kulkarni, Tamal Das, and Vivek Balachandran",
        "link": "http://arxiv.org/abs/2509.08364v1",
        "abstract": "The Domain Name System (DNS) serves as the backbone of the Internet,\nprimarily translating domain names to IP addresses. Over time, various\nenhancements have been introduced to strengthen the integrity of DNS. Among\nthese, DNSSEC stands out as a leading cryptographic solution. It protects\nagainst attacks (such as DNS spoofing) by establishing a chain of trust\nthroughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is\ncompromised when there is a break in this chain, resulting in \"Islands of\nSecurity\", where domains can authenticate locally but not across hierarchical\nlevels, leading to a loss of trust and validation between them. Leading\napproaches to addressing these issues were centralized, with a single authority\nmaintaining some kind of bulletin board. This approach requires significantly\nmore infrastructure and places excessive trust in the entity responsible for\nmanaging it properly. In this paper, we propose a decentralized approach to\naddressing gaps in DNSSEC's chain of trust, commonly referred to as \"Islands of\nSecurity\". We leverage TLS and IP-based certificates to enable end-to-end\nauthentication between hierarchical levels, eliminating the need for uniform\nDNSSEC deployment across every level of the DNS hierarchy. This approach\nenhances the overall integrity of DNSSEC, while reducing dependence on\nregistrars for maintaining signature records to verify the child nameserver's\nauthenticity. By offering a more flexible and efficient solution, our method\nstrengthens DNS security and streamlines deployment across diverse\nenvironments."
    },
    {
        "date": "2025-09",
        "title": "GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation",
        "author": "Seongho Kim, Sejong Ryu, Hyoukjun You, and Je Hyeong Hong",
        "link": "http://arxiv.org/abs/2509.08232v1",
        "abstract": "Recent advancements in video anomaly detection (VAD) have enabled\nidentification of various criminal activities in surveillance videos, but\ndetecting fatal incidents such as shootings and stabbings remains difficult due\nto their rarity and ethical issues in data collection. Recognizing this\nlimitation, we introduce GTA-Crime, a fatal video anomaly dataset and\ngeneration framework using Grand Theft Auto 5 (GTA5). Our dataset contains\nfatal situations such as shootings and stabbings, captured from CCTV multiview\nperspectives under diverse conditions including action types, weather, time of\nday, and viewpoints. To address the rarity of such scenarios, we also release a\nframework for generating these types of videos. Additionally, we propose a\nsnippet-level domain adaptation strategy using Wasserstein adversarial training\nto bridge the gap between synthetic GTA-Crime features and real-world features\nlike UCF-Crime. Experimental results validate our GTA-Crime dataset and\ndemonstrate that incorporating GTA-Crime with our domain adaptation strategy\nconsistently enhances real world fatal violence detection accuracy. Our dataset\nand the data generation framework are publicly available at\nhttps://github.com/ta-ho/GTA-Crime."
    },
    {
        "date": "2025-09",
        "title": "Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection",
        "author": "Jingjing Liu, Yinchao Han, Xianchao Xiu, Jianhua Zhang, and Wanquan Liu",
        "link": "http://arxiv.org/abs/2509.08205v1",
        "abstract": "Infrared small target detection (ISTD) is one of the key techniques in image\nprocessing. Although deep unfolding networks (DUNs) have demonstrated promising\nperformance in ISTD due to their model interpretability and data adaptability,\nexisting methods still face significant challenges in parameter lightweightness\nand noise robustness. In this regard, we propose a highly lightweight framework\nbased on robust principal component analysis (RPCA) called L-RPCANet.\nTechnically, a hierarchical bottleneck structure is constructed to reduce and\nincrease the channel dimension in the single-channel input infrared image to\nachieve channel-wise feature refinement, with bottleneck layers designed in\neach module to extract features. This reduces the number of channels in feature\nextraction and improves the lightweightness of network parameters. Furthermore,\na noise reduction module is embedded to enhance the robustness against complex\nnoise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a\nchannel attention mechanism to focus on the varying importance of different\nfeatures across channels, thereby achieving excellent performance while\nmaintaining both lightweightness and robustness. Extensive experiments on the\nISTD datasets validate the superiority of our proposed method compared with\nstate-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code\nwill be available at https://github.com/xianchaoxiu/L-RPCANet."
    },
    {
        "date": "2025-09",
        "title": "Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data",
        "author": "Kai Yang",
        "link": "http://arxiv.org/abs/2509.08155v1",
        "abstract": "A ubiquitous feature of data of our era is their extra-large sizes and\ndimensions. Analyzing such high-dimensional data poses significant challenges,\nsince the feature dimension is often much larger than the sample size. This\nthesis introduces robust and computationally efficient methods to address\nseveral common challenges associated with high-dimensional data. In my first\nmanuscript, I propose a coherent approach to variable screening that\naccommodates nonlinear associations. I develop a novel variable screening\nmethod that transcends traditional linear assumptions by leveraging mutual\ninformation, with an intended application in neuroimaging data. This approach\nallows for accurate identification of important variables by capturing\nnonlinear as well as linear relationships between the outcome and covariates.\nBuilding on this foundation, I develop new optimization methods for sparse\nestimation using nonconvex penalties in my second manuscript. These methods\naddress notable challenges in current statistical computing practices,\nfacilitating computationally efficient and robust analyses of complex datasets.\nThe proposed method can be applied to a general class of optimization problems.\nIn my third manuscript, I contribute to robust modeling of high-dimensional\ncorrelated observations by developing a mixed-effects model based on Tsallis\npower-law entropy maximization and discussed the theoretical properties of such\ndistribution. This model surpasses the constraints of conventional Gaussian\nmodels by accommodating a broader class of distributions with enhanced\nrobustness to outliers. Additionally, I develop a proximal nonlinear conjugate\ngradient algorithm that accelerates convergence while maintaining numerical\nstability, along with rigorous statistical properties for the proposed\nframework."
    },
    {
        "date": "2025-09",
        "title": "APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction",
        "author": "Sasan Sharifipour, Constantino \u00c1lvarez Casado, Mohammad Sabokrou, and Miguel Bordallo L\u00f3pez",
        "link": "http://arxiv.org/abs/2509.08104v1",
        "abstract": "Training deep learning models for point cloud prediction tasks such as shape\ncompletion and generation depends critically on loss functions that measure\ndiscrepancies between predicted and ground-truth point sets. Commonly used\nfunctions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on\nnearest-neighbor assignments, which often induce many-to-one correspondences,\nleading to point congestion in dense regions and poor coverage in sparse\nregions. These losses also involve non-differentiable operations due to index\nselection, which may affect gradient-based optimization. Earth Mover Distance\n(EMD) enforces one-to-one correspondences and captures structural similarity\nmore effectively, but its cubic computational complexity limits its practical\nuse. We propose the Adaptive Probabilistic Matching Loss (APML), a fully\ndifferentiable approximation of one-to-one matching that leverages Sinkhorn\niterations on a temperature-scaled similarity matrix derived from pairwise\ndistances. We analytically compute the temperature to guarantee a minimum\nassignment probability, eliminating manual tuning. APML achieves near-quadratic\nruntime, comparable to Chamfer-based losses, and avoids non-differentiable\noperations. When integrated into state-of-the-art architectures (PoinTr, PCN,\nFoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)\nthat generates 3D human point clouds from WiFi CSI measurements, APM loss\nyields faster convergence, superior spatial distribution, especially in\nlow-density regions, and improved or on-par quantitative performance without\nadditional hyperparameter search. The code is available at:\nhttps://github.com/apm-loss/apml."
    },
    {
        "date": "2025-09",
        "title": "SAGE: Sample-Aware Guarding Engine for Robust Intrusion Detection Against Adversarial Attacks",
        "author": "Jing Chen, Onat Gungor, Zhengli Shang, and Tajana Rosing",
        "link": "http://arxiv.org/abs/2509.08091v1",
        "abstract": "The rapid proliferation of the Internet of Things (IoT) continues to expose\ncritical security vulnerabilities, necessitating the development of efficient\nand robust intrusion detection systems (IDS). Machine learning-based intrusion\ndetection systems (ML-IDS) have significantly improved threat detection\ncapabilities; however, they remain highly susceptible to adversarial attacks.\nWhile numerous defense mechanisms have been proposed to enhance ML-IDS\nresilience, a systematic approach for selecting the most effective defense\nagainst a specific adversarial attack remains absent. To address this\nchallenge, we previously proposed DYNAMITE, a dynamic defense selection\napproach that identifies the most suitable defense against adversarial attacks\nthrough an ML-driven selection mechanism. Building on this foundation, we\npropose SAGE (Sample-Aware Guarding Engine), a substantially improved defense\nalgorithm that integrates active learning with targeted data reduction. It\nemploys an active learning mechanism to selectively identify the most\ninformative input samples and their corresponding optimal defense labels, which\nare then used to train a second-level learner responsible for selecting the\nmost effective defense. This targeted sampling improves computational\nefficiency, exposes the model to diverse adversarial strategies during\ntraining, and enhances robustness, stability, and generalizability. As a\nresult, SAGE demonstrates strong predictive performance across multiple\nintrusion detection datasets, achieving an average F1-score improvement of 201%\nover the state-of-the-art defenses. Notably, SAGE narrows the performance gap\nto the Oracle to just 3.8%, while reducing computational overhead by up to 29x."
    },
    {
        "date": "2025-09",
        "title": "Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning",
        "author": "Lucas Fenaux, Zheng Wang, Jacob Yan, Nathan Chung, and Florian Kerschbaum",
        "link": "http://arxiv.org/abs/2509.08089v1",
        "abstract": "Federated Learning is a distributed learning technique in which multiple\nclients cooperate to train a machine learning model. Distributed settings\nfacilitate backdoor attacks by malicious clients, who can embed malicious\nbehaviors into the model during their participation in the training process.\nThese malicious behaviors are activated during inference by a specific trigger.\nNo defense against backdoor attacks has stood the test of time, especially\nagainst adaptive attackers, a powerful but not fully explored category of\nattackers. In this work, we first devise a new adaptive adversary that\nsurpasses existing adversaries in capabilities, yielding attacks that only\nrequire one or two malicious clients out of 20 to break existing\nstate-of-the-art defenses. Then, we present Hammer and Anvil, a principled\ndefense approach that combines two defenses orthogonal in their underlying\nprinciple to produce a combined defense that, given the right set of\nparameters, must succeed against any attack. We show that our best combined\ndefense, Krum+, is successful against our new adaptive adversary and\nstate-of-the-art attacks."
    },
    {
        "date": "2025-09",
        "title": "Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations",
        "author": "Laurie Williams, and Sammy Migues",
        "link": "http://arxiv.org/abs/2509.08083v1",
        "abstract": "Software supply chain attacks have increased exponentially since 2020. The\nprimary attack vectors for supply chain attacks are through: (1) software\ncomponents; (2) the build infrastructure; and (3) humans (a.k.a software\npractitioners). Software supply chain risk management frameworks provide a list\nof tasks that an organization can adopt to reduce software supply chain risk.\nExhaustively adopting all the tasks of these frameworks is infeasible,\nnecessitating the prioritized adoption of tasks. Software organizations can\nbenefit from being guided in this prioritization by learning what tasks other\nteams have adopted. The goal of this study is to aid software development\norganizations in understanding the adoption of security tasks that reduce\nsoftware supply chain risk through an interview study of software practitioners\nengaged in software supply chain risk management efforts. An interview study\nwas conducted with 61 practitioners at nine software development organizations\nthat have focused efforts on reducing software supply chain risk. The results\nof the interviews indicate that organizations had implemented the most adopted\nsoftware tasks before the focus on software supply chain security. Therefore,\ntheir implementation in organizations is more mature. The tasks that mitigate\nthe novel attack vectors through software components and the build\ninfrastructure are in the early stages of adoption. Adoption of these tasks\nshould be prioritized."
    },
    {
        "date": "2025-09",
        "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness",
        "author": "Pratik Jayarao, Himanshu Gupta, Neeraj Varshney, and Chaitanya Dwivedi",
        "link": "http://arxiv.org/abs/2509.13332v1",
        "abstract": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness."
    },
    {
        "date": "2025-09",
        "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees",
        "author": "Katsuaki Nakano, Reza Feyyazi, Shanchieh Jay Yang, and Michael Zuzak",
        "link": "http://arxiv.org/abs/2509.07939v1",
        "abstract": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments"
    },
    {
        "date": "2025-09",
        "title": "Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering",
        "author": "Shivam Sharma, Supreeth Mysore Venkatesh, and Pushkin Kachroo",
        "link": "http://arxiv.org/abs/2509.07766v2",
        "abstract": "Clustering financial assets based on return correlations is a fundamental\ntask in portfolio optimization and statistical arbitrage. However, classical\nclustering methods often fall short when dealing with signed correlation\nstructures, typically requiring lossy transformations and heuristic assumptions\nsuch as a fixed number of clusters. In this work, we apply the Graph-based\nCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,\nweighted graphs without relying on such transformations. GCS-Q formulates each\npartitioning step as a QUBO problem, enabling it to leverage quantum annealing\nfor efficient exploration of exponentially large solution spaces. We validate\nour approach on both synthetic and real-world financial data, benchmarking\nagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Our\nexperiments demonstrate that GCS-Q consistently achieves higher clustering\nquality, as measured by Adjusted Rand Index and structural balance penalties,\nwhile dynamically determining the number of clusters. These results highlight\nthe practical utility of near-term quantum computing for graph-based\nunsupervised learning in financial applications."
    },
    {
        "date": "2025-09",
        "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents",
        "author": "Haitao Hu, Peng Chen, Yanpeng Zhao, and Yuqi Chen",
        "link": "http://arxiv.org/abs/2509.07764v1",
        "abstract": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses."
    },
    {
        "date": "2025-09",
        "title": "Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection",
        "author": "Nils Bars, Lukas Bernhard, Moritz Schloegel, and Thorsten Holz",
        "link": "http://arxiv.org/abs/2509.07757v2",
        "abstract": "We use browsers daily to access all sorts of information. Because browsers\nroutinely process scripts, media, and executable code from unknown sources,\nthey form a critical security boundary between users and adversaries. A common\nattack vector is JavaScript, which exposes a large attack surface due to the\nsheer complexity of modern JavaScript engines. To mitigate these threats,\nmodern engines increasingly adopt software-based fault isolation (SFI). A\nprominent example is Google's V8 heap sandbox, which represents the most widely\ndeployed SFI mechanism, protecting billions of users across all Chromium-based\nbrowsers and countless applications built on Node$.$js and Electron. The heap\nsandbox splits the address space into two parts: one part containing trusted,\nsecurity-sensitive metadata, and a sandboxed heap containing memory accessible\nto untrusted code. On a technical level, the sandbox enforces isolation by\nremoving raw pointers and using translation tables to resolve references to\ntrusted objects. Consequently, an attacker cannot corrupt trusted data even\nwith full control of the sandboxed data, unless there is a bug in how code\nhandles data from the sandboxed heap. Despite their widespread use, such SFI\nmechanisms have seen little security testing.\n  In this work, we propose a new testing technique that models the security\nboundary of modern SFI implementations. Following the SFI threat model, we\nassume a powerful attacker who fully controls the sandbox's memory. We\nimplement this by instrumenting memory loads originating in the trusted domain\nand accessing untrusted, attacker-controlled sandbox memory. We then inject\nfaults into the loaded data, aiming to trigger memory corruption in the trusted\ndomain. In a comprehensive evaluation, we identify 19 security bugs in V8 that\nenable an attacker to bypass the sandbox."
    },
    {
        "date": "2025-09",
        "title": "Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems",
        "author": "Kamel Kamel, Hridoy Sankar Dutta, Keshav Sood, and Sunil Aryal",
        "link": "http://arxiv.org/abs/2509.07677v1",
        "abstract": "Voice Authentication Systems (VAS) use unique vocal characteristics for\nverification. They are increasingly integrated into high-security sectors such\nas banking and healthcare. Despite their improvements using deep learning, they\nface severe vulnerabilities from sophisticated threats like deepfakes and\nadversarial attacks. The emergence of realistic voice cloning complicates\ndetection, as systems struggle to distinguish authentic from synthetic audio.\nWhile anti-spoofing countermeasures (CMs) exist to mitigate these risks, many\nrely on static detection models that can be bypassed by novel adversarial\nmethods, leaving a critical security gap. To demonstrate this vulnerability, we\npropose the Spectral Masking and Interpolation Attack (SMIA), a novel method\nthat strategically manipulates inaudible frequency regions of AI-generated\naudio. By altering the voice in imperceptible zones to the human ear, SMIA\ncreates adversarial samples that sound authentic while deceiving CMs. We\nconducted a comprehensive evaluation of our attack against state-of-the-art\n(SOTA) models across multiple tasks, under simulated real-world conditions.\nSMIA achieved a strong attack success rate (ASR) of at least 82% against\ncombined VAS/CM systems, at least 97.5% against standalone speaker verification\nsystems, and 100% against countermeasures. These findings conclusively\ndemonstrate that current security postures are insufficient against adaptive\nadversarial attacks. This work highlights the urgent need for a paradigm shift\ntoward next-generation defenses that employ dynamic, context-aware frameworks\ncapable of evolving with the threat landscape."
    },
    {
        "date": "2025-09",
        "title": "Nearest Neighbor Projection Removal Adversarial Training",
        "author": "Himanshu Singh, A. V. Subramanyam, Shivank Rajput, and Mohan Kankanhalli",
        "link": "http://arxiv.org/abs/2509.07673v2",
        "abstract": "Deep neural networks have exhibited impressive performance in image\nclassification tasks but remain vulnerable to adversarial examples. Standard\nadversarial training enhances robustness but typically fails to explicitly\naddress inter-class feature overlap, a significant contributor to adversarial\nsusceptibility. In this work, we introduce a novel adversarial training\nframework that actively mitigates inter-class proximity by projecting out\ninter-class dependencies from adversarial and clean samples in the feature\nspace. Specifically, our approach first identifies the nearest inter-class\nneighbors for each adversarial sample and subsequently removes projections onto\nthese neighbors to enforce stronger feature separability. Theoretically, we\ndemonstrate that our proposed logits correction reduces the Lipschitz constant\nof neural networks, thereby lowering the Rademacher complexity, which directly\ncontributes to improved generalization and robustness. Extensive experiments\nacross standard benchmarks including CIFAR-10, CIFAR-100, and SVHN show that\nour method demonstrates strong performance that is competitive with leading\nadversarial training techniques, highlighting significant achievements in both\nrobust and clean accuracy. Our findings reveal the importance of addressing\ninter-class feature proximity explicitly to bolster adversarial robustness in\nDNNs."
    },
    {
        "date": "2025-09",
        "title": "Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity",
        "author": "Sung Ju Lee, and Nam Ik Cho",
        "link": "http://arxiv.org/abs/2509.07647v1",
        "abstract": "Semantic watermarking techniques for latent diffusion models (LDMs) are\nrobust against regeneration attacks, but often suffer from detection\nperformance degradation due to the loss of frequency integrity. To tackle this\nproblem, we propose a novel embedding method called Hermitian Symmetric Fourier\nWatermarking (SFW), which maintains frequency integrity by enforcing Hermitian\nsymmetry. Additionally, we introduce a center-aware embedding strategy that\nreduces the vulnerability of semantic watermarking due to cropping attacks by\nensuring robust information retention. To validate our approach, we apply these\ntechniques to existing semantic watermarking schemes, enhancing their\nfrequency-domain structures for better robustness and retrieval accuracy.\nExtensive experiments demonstrate that our methods achieve state-of-the-art\nverification and identification performance, surpassing previous approaches\nacross various attack scenarios. Ablation studies confirm the impact of SFW on\ndetection capabilities, the effectiveness of the center-aware embedding against\ncropping, and how message capacity influences identification accuracy. Notably,\nour method achieves the highest detection accuracy while maintaining superior\nimage fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed\nSFW is shown to be an effective framework for balancing robustness and image\nfidelity, addressing the inherent trade-offs in semantic watermarking. Code\navailable at https://github.com/thomas11809/SFWMark"
    },
    {
        "date": "2025-09",
        "title": "RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection",
        "author": "Jad Yehya, Mansour Benbakoura, C\u00e9dric Allain, Beno\u00eet Malezieux, Matthieu Kowalski, and Thomas Moreau",
        "link": "http://arxiv.org/abs/2509.07523v3",
        "abstract": "Identifying recurring patterns and rare events in large-scale signals is a\nfundamental challenge in fields such as astronomy, physical simulations, and\nbiomedical science. Convolutional Dictionary Learning (CDL) offers a powerful\nframework for modeling local structures in signals, but its use for detecting\nrare or anomalous events remains largely unexplored. In particular, CDL faces\ntwo key challenges in this setting: high computational cost and sensitivity to\nartifacts and outliers. In this paper, we introduce RoseCDL, a scalable and\nrobust CDL algorithm designed for unsupervised rare event detection in long\nsignals. RoseCDL combines stochastic windowing for efficient training on large\ndatasets with inline outlier detection to enhance robustness and isolate\nanomalous patterns. This reframes CDL as a practical tool for event discovery\nand characterization in real-world signals, extending its role beyond\ntraditional tasks like compression or denoising."
    },
    {
        "date": "2025-09",
        "title": "Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios",
        "author": "Simon Cremer, Lydia Jehmlich, and Rainer Lenz",
        "link": "http://arxiv.org/abs/2509.07505v1",
        "abstract": "Spatial data are gaining increasing importance in many areas of research.\nParticularly spatial health data are becoming increasingly important for\nmedical research, for example, to better understand relationships between\nenvironmental factors and disease patterns. However, their use is often\nrestricted by legal data protection regulations, since georeferenced personal\ninformation carries a high risk of re-identification of individuals. To address\nthis issue, what are called geomasking methods are applied to guarantee data\nprotection through targeted displacement of individual data points, while\nsimultaneously maintaining analytical validity within a tolerable range. In the\ncurrent literature the degree of anonymity of such anonymized georeferenced\ndatasets is often measured by the so-called metric of spatial k-anonymity.\nHowever, this metric has considerable shortcomings, particularly regarding its\nresilience against realistic data attack scenarios. This article classifies the\npotential data attack scenarios in the context of anonymized georeferenced\nmicrodata and introduces appropriate metrics that enable a comprehensive\nassessment of anonymity adapted to potential data attack scenarios."
    },
    {
        "date": "2025-09",
        "title": "Backdoor Attacks and Defenses in Computer Vision Domain: A Survey",
        "author": "Bilal Hussain Abbasi, Yanjun Zhang, Leo Zhang, and Shang Gao",
        "link": "http://arxiv.org/abs/2509.07504v1",
        "abstract": "Backdoor (trojan) attacks embed hidden, controllable behaviors into\nmachine-learning models so that models behave normally on benign inputs but\nproduce attacker-chosen outputs when a trigger is present. This survey reviews\nthe rapidly growing literature on backdoor attacks and defenses in the\ncomputer-vision domain. We introduce a multi-dimensional taxonomy that\norganizes attacks and defenses by injection stage (dataset poisoning,\nmodel/parameter modification, inference-time injection), trigger type (patch,\nblended/frequency, semantic, transformation), labeling strategy (dirty-label\nvs. clean-label / feature-collision), representation stage (instance-specific,\nmanifold/class-level, neuron/parameter hijacking, distributed encodings), and\ntarget task (classification, detection, segmentation, video, multimodal). For\neach axis we summarize representative methods, highlight evaluation practices,\nand discuss where defenses succeed or fail. For example, many classical\nsanitization and reverse-engineering tools are effective against reusable patch\nattacks but struggle with input-aware, sample-specific, or parameter-space\nbackdoors and with transfer via compromised pre-trained encoders or hardware\nbit-flips. We synthesize trends, identify persistent gaps (supply-chain and\nhardware threats, certifiable defenses, cross-task benchmarks), and propose\npractical guidelines for threat-aware evaluation and layered defenses. This\nsurvey aims to orient researchers and practitioners to the current threat\nlandscape and pressing research directions in secure computer vision."
    },
    {
        "date": "2025-09",
        "title": "Generating Transferrable Adversarial Examples via Local Mixing and Logits Optimization for Remote Sensing Object Recognition",
        "author": "Chun Liu, Hailong Wang, Bingqian Zhu, Panpan Ding, Zheng Zheng, Tao Xu, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2509.07495v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, posing\nsignificant security threats to their deployment in remote sensing\napplications. Research on adversarial attacks not only reveals model\nvulnerabilities but also provides critical insights for enhancing robustness.\nAlthough current mixing-based strategies have been proposed to increase the\ntransferability of adversarial examples, they either perform global blending or\ndirectly exchange a region in the images, which may destroy global semantic\nfeatures and mislead the optimization of adversarial examples. Furthermore,\ntheir reliance on cross-entropy loss for perturbation optimization leads to\ngradient diminishing during iterative updates, compromising adversarial example\nquality. To address these limitations, we focus on non-targeted attacks and\npropose a novel framework via local mixing and logits optimization. First, we\npresent a local mixing strategy to generate diverse yet semantically consistent\ninputs. Different from MixUp, which globally blends two images, and MixCut,\nwhich stitches images together, our method merely blends local regions to\npreserve global semantic information. Second, we adapt the logit loss from\ntargeted attacks to non-targeted scenarios, mitigating the gradient vanishing\nproblem of cross-entropy loss. Third, a perturbation smoothing loss is applied\nto suppress high-frequency noise and enhance transferability. Extensive\nexperiments on FGSCR-42 and MTARSI datasets demonstrate superior performance\nover 12 state-of-the-art methods across 6 surrogate models. Notably, with\nResNet as the surrogate on MTARSI, our method achieves a 17.28% average\nimprovement in black-box attack success rate."
    },
    {
        "date": "2025-09",
        "title": "DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis",
        "author": "Sven Kirchner, Nils Purschke, Ross Greer, and Alois C. Knoll",
        "link": "http://arxiv.org/abs/2509.07463v1",
        "abstract": "Ensuring reliable robot operation when visual input is degraded or\ninsufficient remains a central challenge in robotics. This letter introduces\nDepthVision, a framework for multimodal scene understanding designed to address\nthis problem. Unlike existing Vision-Language Models (VLMs), which use only\ncamera-based visual input alongside language, DepthVision synthesizes RGB\nimages from sparse LiDAR point clouds using a conditional generative\nadversarial network (GAN) with an integrated refiner network. These synthetic\nviews are then combined with real RGB data using a Luminance-Aware Modality\nAdaptation (LAMA), which blends the two types of data dynamically based on\nambient lighting conditions. This approach compensates for sensor degradation,\nsuch as darkness or motion blur, without requiring any fine-tuning of\ndownstream vision-language models. We evaluate DepthVision on real and\nsimulated datasets across various models and tasks, with particular attention\nto safety-critical tasks. The results demonstrate that our approach improves\nperformance in low-light conditions, achieving substantial gains over RGB-only\nbaselines while preserving compatibility with frozen VLMs. This work highlights\nthe potential of LiDAR-guided RGB synthesis for achieving robust robot\noperation in real-world environments."
    },
    {
        "date": "2025-09",
        "title": "EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise",
        "author": "Arslan Majal, Aamir Hussain Chughtai, and Muhammad Tahir",
        "link": "http://arxiv.org/abs/2509.07415v1",
        "abstract": "We present a learning-based outlier-robust filter for a general setup where\nthe measurement noise can be correlated. Since it is an enhanced version of\nEM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is\nequipped with an additional powerful feature to learn the outlier\ncharacteristics during inference along with outlier-detection, EMORF-II has\nimproved outlier-mitigation capability. Numerical experiments confirm\nperformance gains as compared to the state-of-the-art methods in terms of\naccuracy with an increased computational overhead. However, thankfully the\ncomputational complexity order remains at par with other practical methods\nmaking it a useful choice for diverse applications."
    },
    {
        "date": "2025-09",
        "title": "Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness",
        "author": "Ziang Yin, Hongjian Zhou, Chetan Choppali Sudarshan, Vidya Chhabria, and Jiaqi Gu",
        "link": "http://arxiv.org/abs/2509.07396v1",
        "abstract": "The relentless growth of large-scale artificial intelligence (AI) has created\nunprecedented demand for computational power, straining the energy, bandwidth,\nand scaling limits of conventional electronic platforms. Electronic-photonic\nintegrated circuits (EPICs) have emerged as a compelling platform for\nnext-generation AI systems, offering inherent advantages in ultra-high\nbandwidth, low latency, and energy efficiency for computing and\ninterconnection. Beyond performance, EPICs also hold unique promises for\nsustainability. Fabricated in relaxed process nodes with fewer metal layers and\nlower defect densities, photonic devices naturally reduce embodied carbon\nfootprint (CFP) compared to advanced digital electronic integrated circuits,\nwhile delivering orders-of-magnitude higher computing performance and\ninterconnect bandwidth. To further advance the sustainability of photonic AI\nsystems, we explore how electronic-photonic design automation (EPDA) and\ncross-layer co-design methodologies can amplify these inherent benefits. We\npresent how advanced EPDA tools enable more compact layout generation, reducing\nboth chip area and metal layer usage. We will also demonstrate how cross-layer\ndevice-circuit-architecture co-design unlocks new sustainability gains for\nphotonic hardware: ultra-compact photonic circuit designs that minimize chip\narea cost, reconfigurable hardware topology that adapts to evolving AI\nworkloads, and intelligent resilience mechanisms that prolong lifetime by\ntolerating variations and faults. By uniting intrinsic photonic efficiency with\nEPDA- and co-design-driven gains in area efficiency, reconfigurability, and\nrobustness, we outline a vision for lifelong-sustainable electronic-photonic AI\nsystems. This perspective highlights how EPIC AI systems can simultaneously\nmeet the performance demands of modern AI and the urgent imperative for\nsustainable computing."
    },
    {
        "date": "2025-09",
        "title": "When Fine-Tuning is Not Enough: Lessons from HSAD on Hybrid and Adversarial Audio Spoof Detection",
        "author": "Bin Hu, Kunyang Huang, Daehan Kwak, Meng Xu, and Kuan Huang",
        "link": "http://arxiv.org/abs/2509.07323v1",
        "abstract": "The rapid advancement of AI has enabled highly realistic speech synthesis and\nvoice cloning, posing serious risks to voice authentication, smart assistants,\nand telecom security. While most prior work frames spoof detection as a binary\ntask, real-world attacks often involve hybrid utterances that mix genuine and\nsynthetic speech, making detection substantially more challenging. To address\nthis gap, we introduce the Hybrid Spoofed Audio Dataset (HSAD), a benchmark\ncontaining 1,248 clean and 41,044 degraded utterances across four classes:\nhuman, cloned, zero-shot AI-generated, and hybrid audio. Each sample is\nannotated with spoofing method, speaker identity, and degradation metadata to\nenable fine-grained analysis. We evaluate six transformer-based models,\nincluding spectrogram encoders (MIT-AST, MattyB95-AST) and self-supervised\nwaveform models (Wav2Vec2, HuBERT). Results reveal critical lessons: pretrained\nmodels overgeneralize and collapse under hybrid conditions; spoof-specific\nfine-tuning improves separability but struggles with unseen compositions; and\ndataset-specific adaptation on HSAD yields large performance gains (AST greater\nthan 97 percent and F1 score is approximately 99 percent), though residual\nerrors persist for complex hybrids. These findings demonstrate that fine-tuning\nalone is not sufficient-robust hybrid-aware benchmarks like HSAD are essential\nto expose calibration failures, model biases, and factors affecting spoof\ndetection in adversarial environments. HSAD thus provides both a dataset and an\nanalytic framework for building resilient and trustworthy voice authentication\nsystems."
    },
    {
        "date": "2025-09",
        "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection",
        "author": "David Oprea, and Sam Powers",
        "link": "http://arxiv.org/abs/2509.07308v1",
        "abstract": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies."
    },
    {
        "date": "2025-09",
        "title": "Adversarial Attacks on Audio Deepfake Detection: A Benchmark and Comparative Study",
        "author": "Kutub Uddin, Muhammad Umar Farooq, Awais Khan, and Khalid Mahmood Malik",
        "link": "http://arxiv.org/abs/2509.07132v1",
        "abstract": "The widespread use of generative AI has shown remarkable success in producing\nhighly realistic deepfakes, posing a serious threat to various voice biometric\napplications, including speaker verification, voice biometrics, audio\nconferencing, and criminal investigations. To counteract this, several\nstate-of-the-art (SoTA) audio deepfake detection (ADD) methods have been\nproposed to identify generative AI signatures to distinguish between real and\ndeepfake audio. However, the effectiveness of these methods is severely\nundermined by anti-forensic (AF) attacks that conceal generative signatures.\nThese AF attacks span a wide range of techniques, including statistical\nmodifications (e.g., pitch shifting, filtering, noise addition, and\nquantization) and optimization-based attacks (e.g., FGSM, PGD, C \\& W, and\nDeepFool). In this paper, we investigate the SoTA ADD methods and provide a\ncomparative analysis to highlight their effectiveness in exposing deepfake\nsignatures, as well as their vulnerabilities under adversarial conditions. We\nconducted an extensive evaluation of ADD methods on five deepfake benchmark\ndatasets using two categories: raw and spectrogram-based approaches. This\ncomparative analysis enables a deeper understanding of the strengths and\nlimitations of SoTA ADD methods against diverse AF attacks. It does not only\nhighlight vulnerabilities of ADD methods, but also informs the design of more\nrobust and generalized detectors for real-world voice biometrics. It will\nfurther guide future research in developing adaptive defense strategies that\ncan effectively counter evolving AF techniques."
    },
    {
        "date": "2025-09",
        "title": "SoK: Security and Privacy of AI Agents for Blockchain",
        "author": "Nicol\u00f2 Romandini, Carlo Mazzocca, Kai Otsuki, and Rebecca Montanari",
        "link": "http://arxiv.org/abs/2509.07131v1",
        "abstract": "Blockchain and smart contracts have garnered significant interest in recent\nyears as the foundation of a decentralized, trustless digital ecosystem,\nthereby eliminating the need for traditional centralized authorities. Despite\ntheir central role in powering Web3, their complexity still presents\nsignificant barriers for non-expert users. To bridge this gap, Artificial\nIntelligence (AI)-based agents have emerged as valuable tools for interacting\nwith blockchain environments, supporting a range of tasks, from analyzing\non-chain data and optimizing transaction strategies to detecting\nvulnerabilities within smart contracts. While interest in applying AI to\nblockchain is growing, the literature still lacks a comprehensive survey that\nfocuses specifically on the intersection with AI agents. Most of the related\nwork only provides general considerations, without focusing on any specific\ndomain. This paper addresses this gap by presenting the first Systematization\nof Knowledge dedicated to AI-driven systems for blockchain, with a special\nfocus on their security and privacy dimensions, shedding light on their\napplications, limitations, and future research directions."
    },
    {
        "date": "2025-09",
        "title": "Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry",
        "author": "Soruya Saha, Md Nurul Absur, and Saptarshi Debroy",
        "link": "http://arxiv.org/abs/2509.07130v1",
        "abstract": "Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by\nfusing camera and Inertial Measurement Unit (IMU) data for real-time pose.\nHowever, current trend of offloading VIO to edge servers can lead server-side\nthreat surface where subtle pose spoofing can accumulate into substantial\ndrift, while evading heuristic checks. In this paper, we study this threat and\npresent an unsupervised, label-free detection and recovery mechanism. The\nproposed model is trained on attack-free sessions to learn temporal\nregularities of motion to detect runtime deviations and initiate recovery to\nrestore pose consistency. We evaluate the approach in a realistic offloaded-VIO\nenvironment using ILLIXR testbed across multiple spoofing intensities.\nExperimental results in terms of well-known performance metrics show\nsubstantial reductions in trajectory and pose error compared to a no-defense\nbaseline."
    },
    {
        "date": "2025-09",
        "title": "Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition",
        "author": "Tarhib Al Azad, and Shahana Ibrahim",
        "link": "http://arxiv.org/abs/2509.06918v1",
        "abstract": "Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings."
    },
    {
        "date": "2025-09",
        "title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition",
        "author": "Behnoud Shafiezadeh, Amir Mashmool, Farshad Eshghi, and Manoochehr Kelarestaghi",
        "link": "http://arxiv.org/abs/2509.06868v1",
        "abstract": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput."
    },
    {
        "date": "2025-09",
        "title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset",
        "author": "Nabeyou Tadessa, Balaji Iyangar, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2509.06835v1",
        "abstract": "Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems."
    },
    {
        "date": "2025-09",
        "title": "Imitative Membership Inference Attack",
        "author": "Yuntao Du, Yuetian Chen, Hanshen Xiao, Bruno Ribeiro, and Ninghui Li",
        "link": "http://arxiv.org/abs/2509.06796v1",
        "abstract": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches."
    },
    {
        "date": "2025-09",
        "title": "When Secure Isn't: Assessing the Security of Machine Learning Model Sharing",
        "author": "Gabriele Digregorio, Marco Di Gennaro, Stefano Zanero, Stefano Longari, and Michele Carminati",
        "link": "http://arxiv.org/abs/2509.06703v1",
        "abstract": "The rise of model-sharing through frameworks and dedicated hubs makes Machine\nLearning significantly more accessible. Despite their benefits, these tools\nexpose users to underexplored security risks, while security awareness remains\nlimited among both practitioners and developers. To enable a more\nsecurity-conscious culture in Machine Learning model sharing, in this paper we\nevaluate the security posture of frameworks and hubs, assess whether\nsecurity-oriented mechanisms offer real protection, and survey how users\nperceive the security narratives surrounding model sharing. Our evaluation\nshows that most frameworks and hubs address security risks partially at best,\noften by shifting responsibility to the user. More concerningly, our analysis\nof frameworks advertising security-oriented settings and complete model sharing\nuncovered six 0-day vulnerabilities enabling arbitrary code execution. Through\nthis analysis, we debunk the misconceptions that the model-sharing problem is\nlargely solved and that its security can be guaranteed by the file format used\nfor sharing. As expected, our survey shows that the surrounding security\nnarrative leads users to consider security-oriented settings as trustworthy,\ndespite the weaknesses shown in this work. From this, we derive takeaways and\nsuggestions to strengthen the security of model-sharing ecosystems."
    },
    {
        "date": "2025-09",
        "title": "Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives",
        "author": "Yuanyuan Wu, Zhenlin Qin, Leizhen Wang, Xiaolei Ma, and Zhenliang Ma",
        "link": "http://arxiv.org/abs/2509.06656v1",
        "abstract": "Understanding and modeling individual travel behavior responses is crucial\nfor urban mobility regulation and policy evaluation. The Markov decision\nprocess (MDP) provides a structured framework for dynamic travel behavior\nmodeling at the individual level. However, solving an MDP in this context is\nhighly data-intensive and faces challenges of data quantity, spatial-temporal\ncoverage, and situational diversity. To address these, we propose a\ngroup-effect-enhanced generative adversarial imitation learning (gcGAIL) model\nthat improves the individual behavior modeling efficiency by leveraging shared\nbehavioral patterns among passenger groups. We validate the gcGAIL model using\na public transport fare-discount case study and compare against\nstate-of-the-art benchmarks, including adversarial inverse reinforcement\nlearning (AIRL), baseline GAIL, and conditional GAIL. Experimental results\ndemonstrate that gcGAIL outperforms these methods in learning individual travel\nbehavior responses to incentives over time in terms of accuracy,\ngeneralization, and pattern demonstration efficiency. Notably, gcGAIL is robust\nto spatial variation, data sparsity, and behavioral diversity, maintaining\nstrong performance even with partial expert demonstrations and underrepresented\npassenger groups. The gcGAIL model predicts the individual behavior response at\nany time, providing the basis for personalized incentives to induce sustainable\nbehavior changes (better timing of incentive injections)."
    },
    {
        "date": "2025-09",
        "title": "Network-level Censorship Attacks in the InterPlanetary File System",
        "author": "Jan Matter, and Muoi Tran",
        "link": "http://arxiv.org/abs/2509.06626v1",
        "abstract": "The InterPlanetary File System (IPFS) has been successfully established as\nthe de facto standard for decentralized data storage in the emerging Web3.\nDespite its decentralized nature, IPFS nodes, as well as IPFS content\nproviders, have converged to centralization in large public clouds.\nCentralization introduces BGP routing-based attacks, such as passive\ninterception and BGP hijacking, as potential threats. Although this attack\nvector has been investigated for many other Web3 protocols, such as Bitcoin and\nEthereum, to the best of our knowledge, it has not been analyzed for the IPFS\nnetwork. In our work, we bridge this gap and demonstrate that BGP routing\nattacks can be effectively leveraged to censor content in IPFS. For the\nanalysis, we collected 3,000 content blocks called CIDs and conducted a\nsimulation of BGP hijacking and passive interception against them. We find that\na single malicious AS can censor 75% of the IPFS content for more than 57% of\nall requester nodes. Furthermore, we show that even with a small set of only 62\nhijacked prefixes, 70% of the full attack effectiveness can already be reached.\nWe further propose and validate countermeasures based on global collaborative\ncontent replication among all nodes in the IPFS network, together with\nadditional robust backup content provider nodes that are well-hardened against\nBGP hijacking. We hope this work raises awareness about the threat BGP\nrouting-based attacks pose to IPFS and triggers further efforts to harden the\nlive IPFS network against them."
    },
    {
        "date": "2025-09",
        "title": "A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)",
        "author": "Margarita Capretto, Mart\u00edn Ceresa, Antonio Fern\u00e1ndez Anta, Pedro Moreno-Sanchez, and C\u00e9sar S\u00e1nchez",
        "link": "http://arxiv.org/abs/2509.06614v2",
        "abstract": "Blockchains face a scalability limitation, partly due to the throughput\nlimitations of consensus protocols, especially when aiming to obtain a high\ndegree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to\nconventional blockchains. L2s perform most computations offchain using\nminimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer\nis a service that receives offchain L2 transaction requests, batches these\ntransactions, and commits compressed or hashed batches to L1. Using hashing\nneeds less L1 space, which is beneficial for gas cost, but requires a data\navailability committee (DAC) service to translate hashes into their\ncorresponding batches of transaction requests. The behavior of sequencers and\nDACs influence the evolution of the L2 blockchain, presenting a potential\nsecurity threat and delaying L2 adoption. We propose in this paper fraud-proof\nmechanisms, arbitrated by L1 contracts, to detect and generate evidence of\ndishonest behavior of the sequencer and DAC. We study how these fraud-proofs\nlimit the power of adversaries that control different number of sequencer and\nDACs members, and provide incentives for their honest behavior. We designed\nthese fraud-proof mechanisms as two player games. Unlike the generic\nfraud-proofs in current L2s (designed to guarantee the correct execution of\ntransactions), our fraud-proofs are over pred-etermined algorithms that verify\nthe properties that determine the correctness of the DAC. Arbitrating over\nconcrete algorithms makes our fraud-proofs more efficient, easier to\nunderstand, and simpler to prove correct. We provide as an artifact a\nmechanization in LEAN4 of our fraud-proof games, including (1) the verified\nstrategies that honest players should play to win all games as well as (2)\nmechanisms to detect dishonest claims."
    },
    {
        "date": "2025-09",
        "title": "Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination",
        "author": "Yian Huang, Yang Feng, and Zhiliang Ying",
        "link": "http://arxiv.org/abs/2509.06575v1",
        "abstract": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination."
    },
    {
        "date": "2025-09",
        "title": "Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem",
        "author": "Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, and Zhi Xue",
        "link": "http://arxiv.org/abs/2509.06572v1",
        "abstract": "Large language models (LLMs) are increasingly integrated with external\nsystems through the Model Context Protocol (MCP), which standardizes tool\ninvocation and has rapidly become a backbone for LLM-powered applications.\nWhile this paradigm enhances functionality, it also introduces a fundamental\nsecurity shift: LLMs transition from passive information processors to\nautonomous orchestrators of task-oriented toolchains, expanding the attack\nsurface, elevating adversarial goals from manipulating single outputs to\nhijacking entire execution flows. In this paper, we reveal a new class of\nattacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy\nDisclosure (MCP-UPD). These attacks require no direct victim interaction;\ninstead, adversaries embed malicious instructions into external data sources\nthat LLMs access during legitimate tasks. The malicious logic infiltrates the\ntoolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection,\nand Privacy Disclosure, culminating in stealthy exfiltration of private data.\nOur root cause analysis reveals that MCP lacks both context-tool isolation and\nleast-privilege enforcement, enabling adversarial instructions to propagate\nunchecked into sensitive tool invocations. To assess the severity, we design\nMCP-SEC and conduct the first large-scale security census of the MCP ecosystem,\nanalyzing 12,230 tools across 1,360 servers. Our findings show that the MCP\necosystem is rife with exploitable gadgets and diverse attack methods,\nunderscoring systemic risks in MCP platforms and the urgent need for defense\nmechanisms in LLM-integrated environments."
    },
    {
        "date": "2025-09",
        "title": "Robustness and accuracy of mean opinion scores with hard and soft outlier detection",
        "author": "Dietmar Saupe, and Tim Bleile",
        "link": "http://arxiv.org/abs/2509.06554v1",
        "abstract": "In subjective assessment of image and video quality, observers rate or\ncompare selected stimuli. Before calculating the mean opinion scores (MOS) for\nthese stimuli from the ratings, it is recommended to identify and deal with\noutliers that may have given unreliable ratings. Several methods are available\nfor this purpose, some of which have been standardized. These methods are\ntypically based on statistics and sometimes tested by introducing synthetic\nratings from artificial outliers, such as random clickers. However, a reliable\nand comprehensive approach is lacking for comparative performance analysis of\noutlier detection methods. To fill this gap, this work proposes and applies an\nempirical worst-case analysis as a general solution. Our method involves\nevolutionary optimization of an adversarial black-box attack on outlier\ndetection algorithms, where the adversary maximizes the distortion of scale\nvalues with respect to ground truth. We apply our analysis to several hard and\nsoft outlier detection methods for absolute category ratings and show their\ndiffering performance in this stress test. In addition, we propose two new\noutlier detection methods with low complexity and excellent worst-case\nperformance. Software for adversarial attacks and data analysis is available."
    },
    {
        "date": "2025-09",
        "title": "When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation",
        "author": "Hailong Chang, Guozhu Meng, Shuhui Xiao, Kai Chen, Kun Sun, and Yilin Li",
        "link": "http://arxiv.org/abs/2509.06504v1",
        "abstract": "With the growing demand for cross-language codebase migration, evaluating\nLLMs' security implications in translation tasks has become critical. Existing\nevaluations primarily focus on syntactic or functional correctness at the\nfunction level, neglecting the critical dimension of security.\n  To enable security evaluation, we construct STED (Security-centric\nTranslation Evaluation Dataset), the first dataset specifically designed for\nevaluating the security implications of LLM-based code translation. It\ncomprises 720 security-related code samples across five programming languages\nand nine high-impact CWE categories, sourced from CVE/NVD and manually verified\nfor translation tasks. Our evaluation framework consists of two independent\nassessment modules: (1) rigorous evaluation by security researchers, and (2)\nautomated analysis via LLM-as-a-judge. Together they evaluate three critical\naspects: functional correctness, vulnerability preservation, and vulnerability\nintroduction rates.\n  Our large-scale evaluation of five state-of-the-art LLMs across 6,000\ntranslation instances reveals significant security degradation, with 28.6-45%\nof translations introducing new vulnerabilities--particularly for web-related\nflaws like input validation, where LLMs show consistent weaknesses.\nFurthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation\nstrategy that reduces translation-induced vulnerabilities by 32.8%, showing the\npotential of knowledge-enhanced prompting."
    },
    {
        "date": "2025-09",
        "title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks",
        "author": "Sebastian-Vasile Echim, Andrei-Alexandru Preda, Dumitru-Clementin Cercel, and Florin Pop",
        "link": "http://arxiv.org/abs/2509.06459v1",
        "abstract": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation."
    },
    {
        "date": "2025-09",
        "title": "CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup",
        "author": "Xudong Mou, Rui Wang, Tiejun Wang, Renyu Yang, Shiru Chen, Jie Sun, Tianyu Wo, and Xudong Liu",
        "link": "http://arxiv.org/abs/2509.06419v1",
        "abstract": "Time series anomaly detection (TSAD) is a vital yet challenging task,\nparticularly in scenarios where labeled anomalies are scarce and temporal\ndependencies are complex. Recent anomaly assumption (AA) approaches alleviate\nthe lack of anomalies by injecting synthetic samples and training\ndiscriminative models. Despite promising results, these methods often suffer\nfrom two fundamental limitations: patchy generation, where scattered anomaly\nknowledge leads to overly simplistic or incoherent anomaly injection, and\nAnomaly Shift, where synthetic anomalies either resemble normal data too\nclosely or diverge unrealistically from real anomalies, thereby distorting\nclassification boundaries. In this paper, we propose CAPMix, a controllable\nanomaly augmentation framework that addresses both issues. First, we design a\nCutAddPaste mechanism to inject diverse and complex anomalies in a targeted\nmanner, avoiding patchy generation. Second, we introduce a label revision\nstrategy to adaptively refine anomaly labels, reducing the risk of anomaly\nshift. Finally, we employ dual-space mixup within a temporal convolutional\nnetwork to enforce smoother and more robust decision boundaries. Extensive\nexperiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and\nESA, demonstrate that CAPMix achieves significant improvements over\nstate-of-the-art baselines, with enhanced robustness against contaminated\ntraining data. The code is available at https://github.com/alsike22/CAPMix."
    },
    {
        "date": "2025-09",
        "title": "Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection",
        "author": "Hyungjoon Soh, Dongha Lee, Vipul Periwal, and Junghyo Jo",
        "link": "http://arxiv.org/abs/2509.06383v1",
        "abstract": "Selecting key variables from high-dimensional data is increasingly important\nin the era of big data. Sparse regression serves as a powerful tool for this\npurpose by promoting model simplicity and explainability. In this work, we\nrevisit a valuable yet underutilized method, the statistical physics-based\nVariational Garrote (VG), which introduces explicit feature selection spin\nvariables and leverages variational inference to derive a tractable loss\nfunction. We enhance VG by incorporating modern automatic differentiation\ntechniques, enabling scalable and efficient optimization. We evaluate VG on\nboth fully controllable synthetic datasets and complex real-world datasets. Our\nresults demonstrate that VG performs especially well in highly sparse regimes,\noffering more consistent and robust variable selection than Ridge and LASSO\nregression across varying levels of sparsity. We also uncover a sharp\ntransition: as superfluous variables are admitted, generalization degrades\nabruptly and the uncertainty of the selection variables increases. This\ntransition point provides a practical signal for estimating the correct number\nof relevant variables, an insight we successfully apply to identify key\npredictors in real-world data. We expect that VG offers strong potential for\nsparse modeling across a wide range of applications, including compressed\nsensing and model pruning in machine learning."
    },
    {
        "date": "2025-09",
        "title": "From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)",
        "author": "Kunlin Cai, Jinghuai Zhang, Ying Li, Zhiyuan Wang, Xun Chen, Tianshi Li, and Yuan Tian",
        "link": "http://arxiv.org/abs/2509.06368v1",
        "abstract": "The immersive nature of XR introduces a fundamentally different set of\nsecurity and privacy (S&P) challenges due to the unprecedented user\ninteractions and data collection that traditional paradigms struggle to\nmitigate. As the primary architects of XR applications, developers play a\ncritical role in addressing novel threats. However, to effectively support\ndevelopers, we must first understand how they perceive and respond to different\nthreats. Despite the growing importance of this issue, there is a lack of\nin-depth, threat-aware studies that examine XR S&P from the developers'\nperspective. To fill this gap, we interviewed 23 professional XR developers\nwith a focus on emerging threats in XR. Our study addresses two research\nquestions aiming to uncover existing problems in XR development and identify\nactionable paths forward.\n  By examining developers' perceptions of S&P threats, we found that: (1) XR\ndevelopment decisions (e.g., rich sensor data collection, user-generated\ncontent interfaces) are closely tied to and can amplify S&P threats, yet\ndevelopers are often unaware of these risks, resulting in cognitive biases in\nthreat perception; and (2) limitations in existing mitigation methods, combined\nwith insufficient strategic, technical, and communication support, undermine\ndevelopers' motivation, awareness, and ability to effectively address these\nthreats. Based on these findings, we propose actionable and stakeholder-aware\nrecommendations to improve XR S&P throughout the XR development process. This\nwork represents the first effort to undertake a threat-aware,\ndeveloper-centered study in the XR domain -- an area where the immersive,\ndata-rich nature of the XR technology introduces distinctive challenges."
    },
    {
        "date": "2025-09",
        "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?",
        "author": "Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, and Xiangzheng Zhang",
        "link": "http://arxiv.org/abs/2509.06350v1",
        "abstract": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks."
    },
    {
        "date": "2025-09",
        "title": "Schrodinger's Toolbox: Exploring the Quantum Rowhammer Attack",
        "author": "Devon Campbell",
        "link": "http://arxiv.org/abs/2509.06318v1",
        "abstract": "Residual cross-talk in superconducting qubit devices creates a security\nvulnerability for emerging quantum cloud services. We demonstrate a\nClifford-only Quantum Rowhammer attack-using just X and CNOT gates-that injects\nfaults on IBM's 127-qubit Eagle processors without requiring pulse-level\naccess. Experiments show that targeted hammering induces localized errors\nconfined to the attack cycle and primarily manifests as phase noise, as\nconfirmed by near 50% flip rates under Hadamard-basis probing. A full lattice\nsweep maps QR's spatial and temporal behavior, revealing reproducible\ncorruption limited to qubits within two coupling hops and rapid recovery in\nsubsequent benign cycles. Finally, we leverage these properties to outline a\nprime-and-probe covert channel, demonstrating that the clear separability\nbetween hammered and benign rounds enables highly reliable signaling without\nerror correction. These findings underscore the need for hardware-level\nisolation and scheduler-aware defenses as multi-tenant quantum computing\nbecomes standard."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition",
        "author": "Guangyu Lei, Tianhao Liang, Yuqi Ping, Xinglin Chen, Longyu Zhou, Junwei Wu, Xiyuan Zhang, Huahao Ding, Xingjian Zhang, Weijie Yuan, Tingting Zhang, and Qinyu Zhang",
        "link": "http://arxiv.org/abs/2509.06312v1",
        "abstract": "The rapid development of the low-altitude economy emphasizes the critical\nneed for effective perception and intent recognition of non-cooperative\nunmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities\nof multimodal large language models (MLLMs) present a promising approach in\nsuch tasks. In this paper, we focus on the combination of UAV intent\nrecognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV\nintent recognition architecture, where the multimodal perception system is\nutilized to obtain real-time payload and motion information of UAVs, generating\nstructured input information, and MLLM outputs intent recognition results by\nincorporating environmental information, prior knowledge, and tactical\npreferences. Subsequently, we review the related work and demonstrate their\nprogress within the proposed architecture. Then, a use case for low-altitude\nconfrontation is conducted to demonstrate the feasibility of our architecture\nand offer valuable insights for practical system design. Finally, the future\nchallenges are discussed, followed by corresponding strategic recommendations\nfor further applications."
    },
    {
        "date": "2025-09",
        "title": "Robust Analysis for Resilient AI System",
        "author": "Yu Wang, Ran Jin, and Lulu Kang",
        "link": "http://arxiv.org/abs/2509.06172v1",
        "abstract": "Operational hazards in Manufacturing Industrial Internet (MII) systems\ngenerate severe data outliers that cripple traditional statistical analysis.\nThis paper proposes a novel robust regression method, DPD-Lasso, which\nintegrates Density Power Divergence with Lasso regularization to analyze\ncontaminated data from AI resilience experiments. We develop an efficient\niterative algorithm to overcome previous computational bottlenecks. Applied to\nan MII testbed for Aerosol Jet Printing, DPD-Lasso provides reliable, stable\nperformance on both clean and outlier-contaminated data, accurately quantifying\nhazard impacts. This work establishes robust regression as an essential tool\nfor developing and validating resilient industrial AI systems."
    },
    {
        "date": "2025-09",
        "title": "Additive Distributionally Robust Ranking and Selection",
        "author": "Zaile Li, Yuchen Wan, and L. Jeff Hong",
        "link": "http://arxiv.org/abs/2509.06147v1",
        "abstract": "Ranking and selection (R&S) aims to identify the alternative with the best\nmean performance among $k$ simulated alternatives. The practical value of R&S\ndepends on accurate simulation input modeling, which often suffers from the\ncurse of input uncertainty due to limited data. Distributionally robust ranking\nand selection (DRR&S) addresses this challenge by modeling input uncertainty\nvia an ambiguity set of $m > 1$ plausible input distributions, resulting in\n$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:\nadditivity in budget allocation is essential for efficiency. However, existing\njustifications are heuristic, and fundamental properties such as consistency\nand the precise allocation pattern induced by additivity remain poorly\nunderstood. In this paper, we propose a simple additive allocation (AA)\nprocedure that aims to exclusively sample the $k + m - 1$ previously\nhypothesized critical scenarios. Leveraging boundary-crossing arguments, we\nestablish a lower bound on the probability of correct selection and\ncharacterize the procedure's budget allocation behavior. We then prove that AA\nis consistent and, surprisingly, achieves additivity in the strongest sense: as\nthe total budget increases, only $k + m - 1$ scenarios are sampled infinitely\noften. Notably, the worst-case scenarios of non-best alternatives may not be\namong them, challenging prior beliefs about their criticality. These results\noffer new and counterintuitive insights into the additive structure of DRR&S.\nTo improve practical performance while preserving this structure, we introduce\na general additive allocation (GAA) framework that flexibly incorporates\nsampling rules from traditional R&S procedures in a modular fashion. Numerical\nexperiments support our theoretical findings and demonstrate the competitive\nperformance of the proposed GAA procedures."
    },
    {
        "date": "2025-09",
        "title": "Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving",
        "author": "Yang Lou, Haibo Hu, Qun Song, Qian Xu, Yi Zhu, Rui Tan, Wei-Bin Lee, and Jianping Wang",
        "link": "http://arxiv.org/abs/2509.06071v1",
        "abstract": "High-definition maps provide precise environmental information essential for\nprediction and planning in autonomous driving systems. Due to the high cost of\nlabeling and maintenance, recent research has turned to online HD map\nconstruction using onboard sensor data, offering wider coverage and more timely\nupdates for autonomous vehicles. However, the robustness of online map\nconstruction under adversarial conditions remains underexplored. In this paper,\nwe present a systematic vulnerability analysis of online map construction\nmodels, which reveals that these models exhibit an inherent bias toward\npredicting symmetric road structures. In asymmetric scenes like forks or\nmerges, this bias often causes the model to mistakenly predict a straight\nboundary that mirrors the opposite side. We demonstrate that this vulnerability\npersists in the real-world and can be reliably triggered by obstruction or\ntargeted interference. Leveraging this vulnerability, we propose a novel\ntwo-stage attack framework capable of manipulating online constructed maps.\nFirst, our method identifies vulnerable asymmetric scenes along the victim AV's\npotential route. Then, we optimize the location and pattern of camera-blinding\nattacks and adversarial patch attacks. Evaluations on a public AD dataset\ndemonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render\nup to 44% of targeted routes unreachable, and increase unsafe planned\ntrajectory rates, colliding with real-world road boundaries, by up to 27%.\nThese attacks are also validated on a real-world testbed vehicle. We further\nanalyze root causes of the symmetry bias, attributing them to training data\nimbalance, model architecture, and map element representation. To the best of\nour knowledge, this study presents the first vulnerability assessment of online\nmap construction models and introduces the first digital and physical attack\nagainst them."
    }
]