[
    {
        "date": "2025-02",
        "title": "Learning atomic forces from uncertainty-calibrated adversarial attacks",
        "author": "Henrique Musseli Cezar, Tilmann Bodenstein, Henrik Andersen Sveinsson, Morten Ledum, Simen Reine, and Sigbj\u00f8rn L\u00f8land Bore",
        "link": "http://arxiv.org/abs/2502.18314v1",
        "abstract": "Adversarial approaches, which intentionally challenge machine learning models\nby generating difficult examples, are increasingly being adopted to improve\nmachine learning interatomic potentials (MLIPs). While already providing great\npractical value, little is known about the actual prediction errors of MLIPs on\nadversarial structures and whether these errors can be controlled. We propose\nthe Calibrated Adversarial Geometry Optimization (CAGO) algorithm to discover\nadversarial structures with user-assigned errors. Through uncertainty\ncalibration, the estimated uncertainty of MLIPs is unified with real errors. By\nperforming geometry optimization for calibrated uncertainty, we reach\nadversarial structures with the user-assigned target MLIP prediction error.\nIntegrating with active learning pipelines, we benchmark CAGO, demonstrating\nstable MLIPs that systematically converge structural, dynamical, and\nthermodynamical properties for liquid water and water adsorption in a\nmetal-organic framework within only hundreds of training structures, where\npreviously many thousands were typically required."
    },
    {
        "date": "2025-02",
        "title": "Experimental Analysis of Efficiency of the Messaging Layer Security for Multiple Delivery Services",
        "author": "David Soler, Carlos Dafonte, Manuel Fern\u00e1ndez-Veiga, Ana Fern\u00e1ndez Vilas, and Francisco J. N\u00f3voa",
        "link": "http://arxiv.org/abs/2502.18303v1",
        "abstract": "Messaging Layer security (MLS) and its underlying Continuous Group Key\nAgreement (CGKA) protocol allows a group of users to share a cryptographic\nsecret in a dynamic manner, such that the secret is modified in member\ninsertions and deletions. One of the most relevant contributions of MLS is its\nefficiency, as its communication cost scales logarithmically with the number of\nmembers. However, this claim has only been analysed in theoretical models and\nthus it is unclear how efficient MLS is in real-world scenarios. Furthermore,\npractical decisions such as the chosen Delivery Service and paradigm can also\ninfluence the efficiency and evolution of an MLS group. In this work we analyse\nMLS from an empirical viewpoint: we provide real-world measurements for metrics\nsuch as commit generation and processing times and message sizes under\ndifferent conditions. In order to obtain these results we have developed a\nhighly configurable environment for empirical evaluations of MLS through the\nsimulation of MLS clients. Among other findings, our results show that\ncomputation costs scale linearly in practical scenarios even in the best-case\nscenario."
    },
    {
        "date": "2025-02",
        "title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models",
        "author": "Zhaoyi Liu, and Huan Zhang",
        "link": "http://arxiv.org/abs/2502.18290v1",
        "abstract": "Self-supervised learning (SSL) vision encoders learn high-quality image\nrepresentations and thus have become a vital part of developing vision modality\nof large vision language models (LVLMs). Due to the high cost of training such\nencoders, pre-trained encoders are widely shared and deployed into many LVLMs,\nwhich are security-critical or bear societal significance. Under this practical\nscenario, we reveal a new backdoor threat that significant visual\nhallucinations can be induced into these LVLMs by merely compromising vision\nencoders. Because of the sharing and reuse of these encoders, many downstream\nLVLMs may inherit backdoor behaviors from encoders, leading to widespread\nbackdoors. In this work, we propose BadVision, the first method to exploit this\nvulnerability in SSL vision encoders for LVLMs with novel trigger optimization\nand backdoor learning techniques. We evaluate BadVision on two types of SSL\nencoders and LVLMs across eight benchmarks. We show that BadVision effectively\ndrives the LVLMs to attacker-chosen hallucination with over 99% attack success\nrate, causing a 77.6% relative visual understanding error while maintaining the\nstealthiness. SoTA backdoor detection methods cannot detect our attack\neffectively."
    },
    {
        "date": "2025-02",
        "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
        "author": "Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, and Xueqi Cheng",
        "link": "http://arxiv.org/abs/2502.18176v1",
        "abstract": "In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure."
    },
    {
        "date": "2025-02",
        "title": "The Built-In Robustness of Decentralized Federated Averaging to Bad Data",
        "author": "Samuele Sabella, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, and Marco Conti",
        "link": "http://arxiv.org/abs/2502.18097v1",
        "abstract": "Decentralized federated learning (DFL) enables devices to collaboratively\ntrain models over complex network topologies without relying on a central\ncontroller. In this setting, local data remains private, but its quality and\nquantity can vary significantly across nodes. The extent to which a fully\ndecentralized system is vulnerable to poor-quality or corrupted data remains\nunclear, but several factors could contribute to potential risks. Without a\ncentral authority, there can be no unified mechanism to detect or correct\nerrors, and each node operates with a localized view of the data distribution,\nmaking it difficult for the node to assess whether its perspective aligns with\nthe true distribution. Moreover, models trained on low-quality data can\npropagate through the network, amplifying errors. To explore the impact of\nlow-quality data on DFL, we simulate two scenarios with degraded data quality\n-- one where the corrupted data is evenly distributed in a subset of nodes and\none where it is concentrated on a single node -- using a decentralized\nimplementation of FedAvg. Our results reveal that averaging-based decentralized\nlearning is remarkably robust to localized bad data, even when the corrupted\ndata resides in the most influential nodes of the network. Counterintuitively,\nthis robustness is further enhanced when the corrupted data is concentrated on\na single node, regardless of its centrality in the communication network\ntopology. This phenomenon is explained by the averaging process, which ensures\nthat no single node -- however central -- can disproportionately influence the\noverall learning process."
    },
    {
        "date": "2025-02",
        "title": "Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network Representation",
        "author": "Guang Lin, Duc Thien Nguyen, Zerui Tao, Konstantinos Slavakis, Toshihisa Tanaka, and Qibin Zhao",
        "link": "http://arxiv.org/abs/2502.17972v1",
        "abstract": "Deep neural networks are known to be vulnerable to well-designed adversarial\nattacks. Although numerous defense strategies have been proposed, many are\ntailored to the specific attacks or tasks and often fail to generalize across\ndiverse scenarios. In this paper, we propose Tensor Network Purification (TNP),\na novel model-free adversarial purification method by a specially designed\ntensor network decomposition algorithm. TNP depends neither on the pre-trained\ngenerative model nor the specific dataset, resulting in strong robustness\nacross diverse adversarial scenarios. To this end, the key challenge lies in\nrelaxing Gaussian-noise assumptions of classical decompositions and\naccommodating the unknown distribution of adversarial perturbations. Unlike the\nlow-rank representation of classical decompositions, TNP aims to reconstruct\nthe unobserved clean examples from an adversarial example. Specifically, TNP\nleverages progressive downsampling and introduces a novel adversarial\noptimization objective to address the challenge of minimizing reconstruction\nerror but without inadvertently restoring adversarial perturbations. Extensive\nexperiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our\nmethod generalizes effectively across various norm threats, attack types, and\ntasks, providing a versatile and promising adversarial purification technique."
    },
    {
        "date": "2025-02",
        "title": "Robust Polyp Detection and Diagnosis through Compositional Prompt-Guided Diffusion Models",
        "author": "Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Junbo Huang, Quanlin Li, Pinghong Zhou, Zhihua Wang, Fei Wu, Shuo Wang, and Xian Yang",
        "link": "http://arxiv.org/abs/2502.17951v1",
        "abstract": "Colorectal cancer (CRC) is a significant global health concern, and early\ndetection through screening plays a critical role in reducing mortality. While\ndeep learning models have shown promise in improving polyp detection,\nclassification, and segmentation, their generalization across diverse clinical\nenvironments, particularly with out-of-distribution (OOD) data, remains a\nchallenge. Multi-center datasets like PolypGen have been developed to address\nthese issues, but their collection is costly and time-consuming. Traditional\ndata augmentation techniques provide limited variability, failing to capture\nthe complexity of medical images. Diffusion models have emerged as a promising\nsolution for generating synthetic polyp images, but the image generation\nprocess in current models mainly relies on segmentation masks as the condition,\nlimiting their ability to capture the full clinical context. To overcome these\nlimitations, we propose a Progressive Spectrum Diffusion Model (PSDM) that\nintegrates diverse clinical annotations-such as segmentation masks, bounding\nboxes, and colonoscopy reports-by transforming them into compositional prompts.\nThese prompts are organized into coarse and fine components, allowing the model\nto capture both broad spatial structures and fine details, generating\nclinically accurate synthetic images. By augmenting training data with\nPSDM-generated samples, our model significantly improves polyp detection,\nclassification, and segmentation. For instance, on the PolypGen dataset, PSDM\nincreases the F1 score by 2.12% and the mean average precision by 3.09%,\ndemonstrating superior performance in OOD scenarios and enhanced\ngeneralization."
    },
    {
        "date": "2025-02",
        "title": "VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution",
        "author": "Rui Lu, Bihai Zhang, and Dan Wang",
        "link": "http://arxiv.org/abs/2502.17880v1",
        "abstract": "With the popularity of 3D volumetric video applications, such as Autonomous\nDriving, Virtual Reality, and Mixed Reality, current developers have turned to\ndeep learning for compressing volumetric video frames, i.e., point clouds for\nvideo upstreaming. The latest deep learning-based solutions offer higher\nefficiency, lower distortion, and better hardware support compared to\ntraditional ones like MPEG and JPEG. However, privacy threats arise, especially\nreconstruction attacks targeting to recover the original input point cloud from\nthe intermediate results. In this paper, we design VVRec, to the best of our\nknowledge, which is the first targeting DL-based Volumetric Video\nReconstruction attack scheme. VVRec demonstrates the ability to reconstruct\nhigh-quality point clouds from intercepted transmission intermediate results\nusing four well-trained neural network modules we design. Leveraging the latest\nlatent diffusion models with Gamma distribution and a refinement algorithm,\nVVRec excels in reconstruction quality, color recovery, and surpasses existing\ndefenses. We evaluate VVRec using three volumetric video datasets. The results\ndemonstrate that VVRec achieves 64.70dB reconstruction accuracy, with an\nimpressive 46.39% reduction of distortion over baselines."
    },
    {
        "date": "2025-02",
        "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
        "author": "Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios, Saikrishna Sanniboina, Nanyun Peng, Kai-wei Chang, Daniel Kang, and Heng Ji",
        "link": "http://arxiv.org/abs/2502.17832v1",
        "abstract": "Multimodal large language models (MLLMs) equipped with Retrieval Augmented\nGeneration (RAG) leverage both their rich parametric knowledge and the dynamic,\nexternal knowledge to excel in tasks such as Question Answering. While RAG\nenhances MLLMs by grounding responses in query-relevant external knowledge,\nthis reliance poses a critical yet underexplored safety risk: knowledge\npoisoning attacks, where misinformation or irrelevant knowledge is\nintentionally injected into external knowledge bases to manipulate model\noutputs to be incorrect and even harmful. To expose such vulnerabilities in\nmultimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack\nframework with two attack strategies: Localized Poisoning Attack (LPA), which\ninjects query-specific misinformation in both text and images for targeted\nmanipulation, and Globalized Poisoning Attack (GPA) to provide false guidance\nduring MLLM generation to elicit nonsensical responses across all queries. We\nevaluate our attacks across multiple tasks, models, and access settings,\ndemonstrating that LPA successfully manipulates the MLLM to generate\nattacker-controlled answers, with a success rate of up to 56% on MultiModalQA.\nMoreover, GPA completely disrupts model generation to 0% accuracy with just a\nsingle irrelevant knowledge injection. Our results highlight the urgent need\nfor robust defenses against knowledge poisoning to safeguard multimodal RAG\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17801v1",
        "abstract": "Cloud computing environments are increasingly vulnerable to security threats\nsuch as distributed denial-of-service (DDoS) attacks and SQL injection.\nTraditional security mechanisms, based on rule matching and feature\nrecognition, struggle to adapt to evolving attack strategies. This paper\nproposes an adaptive security protection framework leveraging deep learning to\nconstruct a multi-layered defense architecture. The proposed system is\nevaluated in a real-world business environment, achieving a detection accuracy\nof 97.3%, an average response time of 18 ms, and an availability rate of\n99.999%. Experimental results demonstrate that the proposed method\nsignificantly enhances detection accuracy, response efficiency, and resource\nutilization, offering a novel and effective approach to cloud computing\nsecurity."
    },
    {
        "date": "2025-02",
        "title": "Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM",
        "author": "Yuqing Wang, and Xiao Yang",
        "link": "http://arxiv.org/abs/2502.17763v1",
        "abstract": "Traditional security protection methods struggle to address sophisticated\nattack vectors in large-scale distributed systems, particularly when balancing\ndetection accuracy with data privacy concerns. This paper presents a novel\ndistributed security threat detection system that integrates federated learning\nwith multimodal large language models (LLMs). Our system leverages federated\nlearning to ensure data privacy while employing multimodal LLMs to process\nheterogeneous data sources including network traffic, system logs, images, and\nsensor data. Experimental evaluation on a 10TB distributed dataset demonstrates\nthat our approach achieves 96.4% detection accuracy, outperforming traditional\nbaseline models by 4.1 percentage points. The system reduces both false\npositive and false negative rates by 1.8 and 2.4 percentage points\nrespectively. Performance analysis shows that our system maintains efficient\nprocessing capabilities in distributed environments, requiring 180 seconds for\nmodel training and 3.8 seconds for threat detection across the distributed\nnetwork. These results demonstrate significant improvements in detection\naccuracy and computational efficiency while preserving data privacy, suggesting\nstrong potential for real-world deployment in large-scale security systems."
    },
    {
        "date": "2025-02",
        "title": "Robust and Efficient Deep Hedging via Linearized Objective Neural Network",
        "author": "Lei Zhao, and Lin Cai",
        "link": "http://arxiv.org/abs/2502.17757v1",
        "abstract": "Deep hedging represents a cutting-edge approach to risk management for\nfinancial derivatives by leveraging the power of deep learning. However,\nexisting methods often face challenges related to computational inefficiency,\nsensitivity to noisy data, and optimization complexity, limiting their\npractical applicability in dynamic and volatile markets. To address these\nlimitations, we propose Deep Hedging with Linearized-objective Neural Network\n(DHLNN), a robust and generalizable framework that enhances the training\nprocedure of deep learning models. By integrating a periodic fixed-gradient\noptimization method with linearized training dynamics, DHLNN stabilizes the\ntraining process, accelerates convergence, and improves robustness to noisy\nfinancial data. The framework incorporates trajectory-wide optimization and\nBlack-Scholes Delta anchoring, ensuring alignment with established financial\ntheory while maintaining flexibility to adapt to real-world market conditions.\nExtensive experiments on synthetic and real market data validate the\neffectiveness of DHLNN, demonstrating its ability to achieve faster\nconvergence, improved stability, and superior hedging performance across\ndiverse market scenarios."
    },
    {
        "date": "2025-02",
        "title": "The Cyber Immune System: Harnessing Adversarial Forces for Security Resilience",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2502.17698v1",
        "abstract": "Both parasites in biological systems and adversarial forces in cybersecurity\nare often perceived as threats: disruptive elements that must be eliminated.\nHowever, these entities play a critical role in revealing systemic weaknesses,\ndriving adaptation, and ultimately strengthening resilience. This paper draws\nfrom environmental epidemiology and cybersecurity to reframe parasites and\ncyber exploiters as essential stress-testers of complex systems, exposing\nhidden vulnerabilities and pushing defensive innovations forward. By examining\nhow biological and digital systems evolve in response to persistent threats, we\nhighlight the necessity of adversarial engagement in fortifying security\nframeworks. The recent breach of the DOGE website serves as a timely case\nstudy, illustrating how adversarial forces, whether biological or digital,\ncompel systems to reassess and reinforce their defenses."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning with Global Sensitivity Estimation for Financial Risk Management",
        "author": "Lei Zhao, Lin Cai, and Wu-Sheng Lu",
        "link": "http://arxiv.org/abs/2502.17694v1",
        "abstract": "In decentralized financial systems, robust and efficient Federated Learning\n(FL) is promising to handle diverse client environments and ensure resilience\nto systemic risks. We propose Federated Risk-Aware Learning with Central\nSensitivity Estimation (FRAL-CSE), an innovative FL framework designed to\nenhance scalability, stability, and robustness in collaborative financial\ndecision-making. The framework's core innovation lies in a central acceleration\nmechanism, guided by a quadratic sensitivity-based approximation of global\nmodel dynamics. By leveraging local sensitivity information derived from robust\nrisk measurements, FRAL-CSE performs a curvature-informed global update that\nefficiently incorporates second-order information without requiring repeated\nlocal re-evaluations, thereby enhancing training efficiency and improving\noptimization stability. Additionally, distortion risk measures are embedded\ninto the training objectives to capture tail risks and ensure robustness\nagainst extreme scenarios. Extensive experiments validate the effectiveness of\nFRAL-CSE in accelerating convergence and improving resilience across\nheterogeneous datasets compared to state-of-the-art baselines."
    },
    {
        "date": "2025-02",
        "title": "THOR: A Non-Speculative Value Dependent Timing Side Channel Attack Exploiting Intel AMX",
        "author": "Farshad Dizani, Azam Ghanbari, Joshua Kalyanapu, Darsh Asher, and Samira Mirbagher Ajorpaz",
        "link": "http://arxiv.org/abs/2502.17658v1",
        "abstract": "The rise of on-chip accelerators signifies a major shift in computing, driven\nby the growing demands of artificial intelligence (AI) and specialized\napplications. These accelerators have gained popularity due to their ability to\nsubstantially boost performance, cut energy usage, lower total cost of\nownership (TCO), and promote sustainability. Intel's Advanced Matrix Extensions\n(AMX) is one such on-chip accelerator, specifically designed for handling tasks\ninvolving large matrix multiplications commonly used in machine learning (ML)\nmodels, image processing, and other computational-heavy operations. In this\npaper, we introduce a novel value-dependent timing side-channel vulnerability\nin Intel AMX. By exploiting this weakness, we demonstrate a software-based,\nvalue-dependent timing side-channel attack capable of inferring the sparsity of\nneural network weights without requiring any knowledge of the confidence score,\nprivileged access or physical proximity. Our attack method can fully recover\nthe sparsity of weights assigned to 64 input elements within 50 minutes, which\nis 631% faster than the maximum leakage rate achieved in the Hertzbleed attack."
    },
    {
        "date": "2025-02",
        "title": "Formally-verified Security against Forgery of Remote Attestation using SSProve",
        "author": "Sara Zain, Jannik M\u00e4hn, Stefan K\u00f6psell, and Sebastian Ertel",
        "link": "http://arxiv.org/abs/2502.17653v1",
        "abstract": "Remote attestation (RA) is the foundation for trusted execution environments\nin the cloud and trusted device driver onboarding in operating systems.\nHowever, RA misses a rigorous mechanized definition of its security properties\nin one of the strongest models, i.e., the semantic model. Such a mechanization\nrequires the concept of State-Separating Proofs (SSP). However, SSP was only\nrecently implemented as a foundational framework in the Rocq Prover. Based on\nthis framework, this paper presents the first mechanized formalization of the\nfundamental security properties of RA. Our Rocq Prover development first\ndefines digital signatures and formally verifies security against forgery in\nthe strong existential attack model. Based on these results, we define RA and\nreduce the security of RA to the security of digital signatures. Our\ndevelopment provides evidence that the RA protocol is secure against forgery.\nAdditionally, we extend our reasoning to the primitives of RA and reduce their\nsecurity to the security of the primitives of the digital signatures. Finally,\nwe found that proving the security of the primitives for digital signatures was\nnot feasible. This observation contrasts textbook formalizations and sparks a\ndiscussion on reasoning about the security of libraries in SSP-based\nframeworks."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust Legal Reasoning: Harnessing Logical LLMs in Law",
        "author": "Manuj Kant, Sareh Nabi, Manav Kant, Roland Scharrer, Megan Ma, and Marzieh Nabi",
        "link": "http://arxiv.org/abs/2502.17638v1",
        "abstract": "Legal services rely heavily on text processing. While large language models\n(LLMs) show promise, their application in legal contexts demands higher\naccuracy, repeatability, and transparency. Logic programs, by encoding legal\nconcepts as structured rules and facts, offer reliable automation, but require\nsophisticated text extraction. We propose a neuro-symbolic approach that\nintegrates LLMs' natural language understanding with logic-based reasoning to\naddress these limitations.\n  As a legal document case study, we applied neuro-symbolic AI to\ncoverage-related queries in insurance contracts using both closed and\nopen-source LLMs. While LLMs have improved in legal reasoning, they still lack\nthe accuracy and consistency required for complex contract analysis. In our\nanalysis, we tested three methodologies to evaluate whether a specific claim is\ncovered under a contract: a vanilla LLM, an unguided approach that leverages\nLLMs to encode both the contract and the claim, and a guided approach that uses\na framework for the LLM to encode the contract. We demonstrated the promising\ncapabilities of LLM + Logic in the guided approach."
    },
    {
        "date": "2025-02",
        "title": "A stochastic smoothing framework for nonconvex-nonconcave min-sum-max problems with applications to Wasserstein distributionally robust optimization",
        "author": "Wei Liu, Muhammad Khan, Gabriel Mancino-Ball, and Yangyang Xu",
        "link": "http://arxiv.org/abs/2502.17602v1",
        "abstract": "Applications such as adversarially robust training and Wasserstein\nDistributionally Robust Optimization (WDRO) can be naturally formulated as\nmin-sum-max optimization problems. While this formulation can be rewritten as\nan equivalent min-max problem, the summation of max terms introduces\ncomputational challenges, including increased complexity and memory demands,\nwhich must be addressed. These challenges are particularly evident in WDRO,\nwhere existing tractable algorithms often rely on restrictive assumptions on\nthe objective function, limiting their applicability to state-of-the-art\nmachine learning problems such as the training of deep neural networks. This\nstudy introduces a novel stochastic smoothing framework based on the\n\\mbox{log-sum-exp} function, efficiently approximating the max operator in\nmin-sum-max problems. By leveraging the Clarke regularity of the max operator,\nwe develop an iterative smoothing algorithm that addresses these computational\ndifficulties and guarantees almost surely convergence to a Clarke/directional\nstationary point. We further prove that the proposed algorithm finds an\n$\\epsilon$-scaled Clarke stationary point of the original problem, with a\nworst-case iteration complexity of $\\widetilde{O}(\\epsilon^{-3})$. Our\nnumerical experiments demonstrate that our approach outperforms or is\ncompetitive with state-of-the-art methods in solving the newsvendor problem,\ndeep learning regression, and adversarially robust deep learning. The results\nhighlight that our method yields more accurate and robust solutions in these\nchallenging problem settings."
    },
    {
        "date": "2025-02",
        "title": "Robust Confinement State Classification with Uncertainty Quantification through Ensembled Data-Driven Methods",
        "author": "Yoeri Poels, Cristina Venturini, Alessandro Pau, Olivier Sauter, Vlado Menkovski, the TCV team, and the WPTE team",
        "link": "http://arxiv.org/abs/2502.17397v1",
        "abstract": "Maximizing fusion performance in tokamaks relies on high energy confinement,\noften achieved through distinct operating regimes. The automated labeling of\nthese confinement states is crucial to enable large-scale analyses or for\nreal-time control applications. While this task becomes difficult to automate\nnear state transitions or in marginal scenarios, much success has been achieved\nwith data-driven models. However, these methods generally provide predictions\nas point estimates, and cannot adequately deal with missing and/or broken input\nsignals. To enable wide-range applicability, we develop methods for confinement\nstate classification with uncertainty quantification and model robustness. We\nfocus on off-line analysis for TCV discharges, distinguishing L-mode, H-mode,\nand an in-between dithering phase (D). We propose ensembling data-driven\nmethods on two axes: model formulations and feature sets. The former considers\na dynamic formulation based on a recurrent Fourier Neural Operator-architecture\nand a static formulation based on gradient-boosted decision trees. These models\nare trained using multiple feature groupings categorized by diagnostic system\nor physical quantity. A dataset of 302 TCV discharges is fully labeled, and\nwill be publicly released. We evaluate our method quantitatively using Cohen's\nkappa coefficient for predictive performance and the Expected Calibration Error\nfor the uncertainty calibration. Furthermore, we discuss performance using a\nvariety of common and alternative scenarios, the performance of individual\ncomponents, out-of-distribution performance, cases of broken or missing\nsignals, and evaluate conditionally-averaged behavior around different state\ntransitions. Overall, the proposed method can distinguish L, D and H-mode with\nhigh performance, can cope with missing or broken signals, and provides\nmeaningful uncertainty estimates."
    },
    {
        "date": "2025-02",
        "title": "Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences",
        "author": "Yangshijie Zhang",
        "link": "http://arxiv.org/abs/2502.17392v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success in the field of\nnatural language processing (NLP), leading to widely recognized applications\nsuch as ChatGPT. However, the vulnerability of these models to adversarial\nattacks remains a significant concern. Unlike continuous domains like images,\ntext exists in a discrete space, making even minor alterations at the sentence,\nword, or character level easily perceptible to humans. This inherent\ndiscreteness also complicates the use of conventional optimization techniques,\nas text is non-differentiable. Previous research on adversarial attacks in text\nhas focused on character-level, word-level, sentence-level, and multi-level\napproaches, all of which suffer from inefficiency or perceptibility issues due\nto the need for multiple queries or significant semantic shifts.\n  In this work, we introduce a novel adversarial attack method, Emoji-Attack,\nwhich leverages the manipulation of emojis to create subtle, yet effective,\nperturbations. Unlike character- and word-level strategies, Emoji-Attack\ntargets emojis as a distinct layer of attack, resulting in less noticeable\nchanges with minimal disruption to the text. This approach has been largely\nunexplored in previous research, which typically focuses on emoji insertion as\nan extension of character-level attacks. Our experiments demonstrate that\nEmoji-Attack achieves strong attack performance on both large and small models,\nmaking it a promising technique for enhancing adversarial robustness in NLP\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Unveiling ECC Vulnerabilities: LSTM Networks for Operation Recognition in Side-Channel Attacks",
        "author": "Alberto Battistello, Guido Bertoni, Michele Corrias, Lorenzo Nava, Davide Rusconi, Matteo Zoia, Fabio Pierazzi, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2502.17330v1",
        "abstract": "We propose a novel approach for performing side-channel attacks on elliptic\ncurve cryptography. Unlike previous approaches and inspired by the ``activity\ndetection'' literature, we adopt a long-short-term memory (LSTM) neural network\nto analyze a power trace and identify patterns of operation in the scalar\nmultiplication algorithm performed during an ECDSA signature, that allows us to\nrecover bits of the ephemeral key, and thus retrieve the signer's private key.\nOur approach is based on the fact that modular reductions are conditionally\nperformed by micro-ecc and depend on key bits.\n  We evaluated the feasibility and reproducibility of our attack through\nexperiments in both simulated and real implementations. We demonstrate the\neffectiveness of our attack by implementing it on a real target device, an\nSTM32F415 with the micro-ecc library, and successfully compromise it.\nFurthermore, we show that current countermeasures, specifically the coordinate\nrandomization technique, are not sufficient to protect against side channels.\nFinally, we suggest other approaches that may be implemented to thwart our\nattack."
    },
    {
        "date": "2025-02",
        "title": "Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach",
        "author": "Yanmeng Wang, Wenkai Ji, Jian Zhou, Fu Xiao, and Tsung-Hui Chang",
        "link": "http://arxiv.org/abs/2502.17260v1",
        "abstract": "Federated learning (FL) has emerged as a promising distributed learning\nparadigm for training deep neural networks (DNNs) at the wireless edge, but its\nperformance can be severely hindered by unreliable wireless transmission and\ninherent data heterogeneity among clients. Existing solutions primarily address\nthese challenges by incorporating wireless resource optimization strategies,\noften focusing on uplink resource allocation across clients under the\nassumption of homogeneous client-server network standards. However, these\napproaches overlooked the fact that mobile clients may connect to the server\nvia diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized\nconfigurations, limiting the flexibility of server-side modifications and\nrestricting applicability in real-world commercial networks. This paper\npresents a novel theoretical analysis about how transmission failures in\nunreliable networks distort the effective label distributions of local samples,\ncausing deviations from the global data distribution and introducing\nconvergence bias in FL. Our analysis reveals that a carefully designed client\nselection strategy can mitigate biases induced by network unreliability and\ndata heterogeneity. Motivated by this insight, we propose FedCote, a client\nselection approach that optimizes client selection probabilities without\nrelying on wireless resource scheduling. Experimental results demonstrate the\nrobustness of FedCote in DNN-based classification tasks under unreliable\nnetworks with frequent transmission failures."
    },
    {
        "date": "2025-02",
        "title": "REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective",
        "author": "Simon Geisler, Tom Wollschl\u00e4ger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2502.17254v1",
        "abstract": "To circumvent the alignment of large language models (LLMs), current\noptimization-based adversarial attacks usually craft adversarial prompts by\nmaximizing the likelihood of a so-called affirmative response. An affirmative\nresponse is a manually designed start of a harmful answer to an inappropriate\nrequest. While it is often easy to craft prompts that yield a substantial\nlikelihood for the affirmative response, the attacked model frequently does not\ncomplete the response in a harmful manner. Moreover, the affirmative objective\nis usually not adapted to model-specific preferences and essentially ignores\nthe fact that LLMs output a distribution over responses. If low attack success\nunder such an objective is taken as a measure of robustness, the true\nrobustness might be grossly overestimated. To alleviate these flaws, we propose\nan adaptive and semantic optimization problem over the population of responses.\nWe derive a generally applicable objective via the REINFORCE policy-gradient\nformalism and demonstrate its efficacy with the state-of-the-art jailbreak\nalgorithms Greedy Coordinate Gradient (GCG) and Projected Gradient Descent\n(PGD). For example, our objective doubles the attack success rate (ASR) on\nLlama3 and increases the ASR from 2% to 50% with circuit breaker defense."
    },
    {
        "date": "2025-02",
        "title": "CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping",
        "author": "Yufei Lu, Yuetao Li, Zhizhou Jia, Qun Hao, and Shaohui Zhang",
        "link": "http://arxiv.org/abs/2502.17249v1",
        "abstract": "In this letter, we propose a color-assisted robust framework for accurate\nLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both the\nLiDAR and the camera, the framework utilizes the color information from the\ncamera images to colorize the LiDAR point clouds and then performs iterative\npose optimization. For each LiDAR scan, the edge and planar features are\nextracted and colored using the corresponding image and then matched to a\nglobal map. Specifically, we adopt a perceptually uniform color difference\nweighting strategy to exclude color correspondence outliers and a robust error\nmetric based on the Welsch's function to mitigate the impact of positional\ncorrespondence outliers during the pose optimization process. As a result, the\nsystem achieves accurate localization and reconstructs dense, accurate, colored\nand three-dimensional (3D) maps of the environment. Thorough experiments with\nchallenging scenarios, including complex forests and a campus, show that our\nmethod provides higher robustness and accuracy compared with current\nstate-of-the-art methods."
    },
    {
        "date": "2025-02",
        "title": "Motion-Robust T2* Quantification from Gradient Echo MRI with Physics-Informed Deep Learning",
        "author": "Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Lina Felsner, Kilian Weiss, Christine Preibisch, and Julia A. Schnabel",
        "link": "http://arxiv.org/abs/2502.17209v1",
        "abstract": "Purpose: T2* quantification from gradient echo magnetic resonance imaging is\nparticularly affected by subject motion due to the high sensitivity to magnetic\nfield inhomogeneities, which are influenced by motion and might cause signal\nloss. Thus, motion correction is crucial to obtain high-quality T2* maps.\n  Methods: We extend our previously introduced learning-based physics-informed\nmotion correction method, PHIMO, by utilizing acquisition knowledge to enhance\nthe reconstruction performance for challenging motion patterns and increase\nPHIMO's robustness to varying strengths of magnetic field inhomogeneities\nacross the brain. We perform comprehensive evaluations regarding motion\ndetection accuracy and image quality for data with simulated and real motion.\n  Results: Our extended version of PHIMO outperforms the learning-based\nbaseline methods both qualitatively and quantitatively with respect to line\ndetection and image quality. Moreover, PHIMO performs on-par with a\nconventional state-of-the-art motion correction method for T2* quantification\nfrom gradient echo MRI, which relies on redundant data acquisition.\n  Conclusion: PHIMO's competitive motion correction performance, combined with\na reduction in acquisition time by over 40% compared to the state-of-the-art\nmethod, make it a promising solution for motion-robust T2* quantification in\nresearch settings and clinical routine."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Training for Defense Against Label Poisoning Attacks",
        "author": "Melis Ilayda Bal, Volkan Cevher, and Michael Muehlebach",
        "link": "http://arxiv.org/abs/2502.17121v1",
        "abstract": "As machine learning models grow in complexity and increasingly rely on\npublicly sourced data, such as the human-annotated labels used in training\nlarge language models, they become more vulnerable to label poisoning attacks.\nThese attacks, in which adversaries subtly alter the labels within a training\ndataset, can severely degrade model performance, posing significant risks in\ncritical applications. In this paper, we propose FLORAL, a novel adversarial\ntraining defense strategy based on support vector machines (SVMs) to counter\nthese threats. Utilizing a bilevel optimization framework, we cast the training\nprocess as a non-zero-sum Stackelberg game between an attacker, who\nstrategically poisons critical training labels, and the model, which seeks to\nrecover from such attacks. Our approach accommodates various model\narchitectures and employs a projected gradient descent algorithm with kernel\nSVMs for adversarial training. We provide a theoretical analysis of our\nalgorithm's convergence properties and empirically evaluate FLORAL's\neffectiveness across diverse classification tasks. Compared to robust baselines\nand foundation models such as RoBERTa, FLORAL consistently achieves higher\nrobust accuracy under increasing attacker budgets. These results underscore the\npotential of FLORAL to enhance the resilience of machine learning models\nagainst label poisoning threats, thereby ensuring robust classification in\nadversarial settings."
    },
    {
        "date": "2025-02",
        "title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness",
        "author": "Zekun Wang, Mingyang Yi, Shuchen Xue, Zhenguo Li, Ming Liu, Bing Qin, and Zhi-Ming Ma",
        "link": "http://arxiv.org/abs/2502.17099v1",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved significant success in\ngenerative tasks. However, their training and sampling processes suffer from\nthe issue of distribution mismatch. During the denoising process, the input\ndata distributions differ between the training and inference stages,\npotentially leading to inaccurate data generation. To obviate this, we analyze\nthe training objective of DPMs and theoretically demonstrate that this mismatch\ncan be alleviated through Distributionally Robust Optimization (DRO), which is\nequivalent to performing robustness-driven Adversarial Training (AT) on DPMs.\nFurthermore, for the recently proposed Consistency Model (CM), which distills\nthe inference process of the DPM, we prove that its training objective also\nencounters the mismatch issue. Fortunately, this issue can be mitigated by AT\nas well. Based on these insights, we propose to conduct efficient AT on both\nDPM and CM. Finally, extensive empirical studies validate the effectiveness of\nAT in diffusion-based models. The code is available at\nhttps://github.com/kugwzk/AT_Diff."
    },
    {
        "date": "2025-02",
        "title": "Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation",
        "author": "Wenyuan Wu, Zheng Liu, Yong Chen, Chao Su, Dezhong Peng, and Xu Wang",
        "link": "http://arxiv.org/abs/2502.17003v1",
        "abstract": "In recent years, the rapid development of deep neural networks has brought\nincreased attention to the security and robustness of these models. While\nexisting adversarial attack algorithms have demonstrated success in improving\nadversarial transferability, their performance remains suboptimal due to a lack\nof consideration for the discrepancies between target and source models. To\naddress this limitation, we propose a novel method, Inverse Knowledge\nDistillation (IKD), designed to enhance adversarial transferability\neffectively. IKD introduces a distillation-inspired loss function that\nseamlessly integrates with gradient-based attack methods, promoting diversity\nin attack gradients and mitigating overfitting to specific model architectures.\nBy diversifying gradients, IKD enables the generation of adversarial samples\nwith superior generalization capabilities across different models,\nsignificantly enhancing their effectiveness in black-box attack scenarios.\nExtensive experiments on the ImageNet dataset validate the effectiveness of our\napproach, demonstrating substantial improvements in the transferability and\nattack success rates of adversarial samples across a wide range of models."
    },
    {
        "date": "2025-02",
        "title": "FedSV: Byzantine-Robust Federated Learning via Shapley Value",
        "author": "Khaoula Otmani, Rachid Elazouzi, and Vincent Labatut",
        "link": "http://arxiv.org/abs/2502.17526v1",
        "abstract": "In Federated Learning (FL), several clients jointly learn a machine learning\nmodel: each client maintains a local model for its local learning dataset,\nwhile a master server maintains a global model by aggregating the local models\nof the client devices. However, the repetitive communication between server and\nclients leaves room for attacks aimed at compromising the integrity of the\nglobal model, causing errors in its targeted predictions. In response to such\nthreats on FL, various defense measures have been proposed in the literature.\nIn this paper, we present a powerful defense against malicious clients in FL,\ncalled FedSV, using the Shapley Value (SV), which has been proposed recently to\nmeasure user contribution in FL by computing the marginal increase of average\naccuracy of the model due to the addition of local data of a user. Our approach\nmakes the identification of malicious clients more robust, since during the\nlearning phase, it estimates the contribution of each client according to the\ndifferent groups to which the target client belongs. FedSV's effectiveness is\ndemonstrated by extensive experiments on MNIST datasets in a cross-silo context\nunder various attacks."
    },
    {
        "date": "2025-02",
        "title": "Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs",
        "author": "Himanshu Beniwal, Sailesh Panda, and Mayank Singh",
        "link": "http://arxiv.org/abs/2502.16901v1",
        "abstract": "We explore Cross-lingual Backdoor ATtacks (X-BAT) in multilingual Large\nLanguage Models (mLLMs), revealing how backdoors inserted in one language can\nautomatically transfer to others through shared embedding spaces. Using\ntoxicity classification as a case study, we demonstrate that attackers can\ncompromise multilingual systems by poisoning data in a single language, with\nrare tokens serving as specific effective triggers. Our findings expose a\ncritical vulnerability in the fundamental architecture that enables\ncross-lingual transfer in these models. Our code and data are publicly\navailable at https://github.com/himanshubeniwal/X-BAT."
    },
    {
        "date": "2025-02",
        "title": "Distributionally Robust Active Learning for Gaussian Process Regression",
        "author": "Shion Takeno, Yoshito Okura, Yu Inatsu, Aoyama Tatsuya, Tomonari Tanaka, Akahane Satoshi, Hiroyuki Hanada, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2502.16870v1",
        "abstract": "Gaussian process regression (GPR) or kernel ridge regression is a widely used\nand powerful tool for nonlinear prediction. Therefore, active learning (AL) for\nGPR, which actively collects data labels to achieve an accurate prediction with\nfewer data labels, is an important problem. However, existing AL methods do not\ntheoretically guarantee prediction accuracy for target distribution.\nFurthermore, as discussed in the distributionally robust learning literature,\nspecifying the target distribution is often difficult. Thus, this paper\nproposes two AL methods that effectively reduce the worst-case expected error\nfor GPR, which is the worst-case expectation in target distribution candidates.\nWe show an upper bound of the worst-case expected squared error, which suggests\nthat the error will be arbitrarily small by a finite number of data labels\nunder mild conditions. Finally, we demonstrate the effectiveness of the\nproposed methods through synthetic and real-world datasets."
    },
    {
        "date": "2025-02",
        "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning",
        "author": "Yang Xu, Washim Uddin Mondal, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16816v1",
        "abstract": "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory."
    },
    {
        "date": "2025-02",
        "title": "VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning",
        "author": "Yang Chen, and Bin Zhou",
        "link": "http://arxiv.org/abs/2502.16793v1",
        "abstract": "Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability."
    },
    {
        "date": "2025-02",
        "title": "The Robustness of Structural Features in Species Interaction Networks",
        "author": "Sanaz Hasanzadeh Fard, and Emily Dolson",
        "link": "http://arxiv.org/abs/2502.16778v1",
        "abstract": "Species interaction networks are a powerful tool for describing ecological\ncommunities; they typically contain nodes representing species, and edges\nrepresenting interactions between those species. For the purposes of drawing\nabstract inferences about groups of similar networks, ecologists often use\ngraph topology metrics to summarize structural features. However, gathering the\ndata that underlies these networks is challenging, which can lead to some\ninteractions being missed. Thus, it is important to understand how much\ndifferent structural metrics are affected by missing data. To address this\nquestion, we analyzed a database of 148 real-world bipartite networks\nrepresenting four different types of species interactions (pollination,\nhost-parasite, plant-ant, and seed-dispersal). For each network, we measured\nsix different topological properties: number of connected components, variance\nin node betweenness, variance in node PageRank, largest Eigenvalue, the number\nof non-zero Eigenvalues, and community detection as determined by four\ndifferent algorithms. We then tested how these properties change as additional\nedges -- representing data that may have been missed -- are added to the\nnetworks. We found substantial variation in how robust different properties\nwere to the missing data. For example, the Clauset-Newman-Moore and Louvain\ncommunity detection algorithms showed much more gradual change as edges were\nadded than the label propagation and Girvan-Newman algorithms did, suggesting\nthat the former are more robust. Robustness also varied for some metrics based\non interaction type. These results provide a foundation for selecting network\nproperties to use when analyzing messy ecological network data."
    },
    {
        "date": "2025-02",
        "title": "Order-Optimal Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization",
        "author": "Yiyang Lu, Mohammad Pedramfar, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2502.16744v1",
        "abstract": "Projection-based algorithms for constrained Online Convex Optimization (COCO)\nface scalability challenges in high-dimensional settings due to the\ncomputational complexity of projecting iterates onto constraint sets. This\npaper introduces a projection-free algorithm for COCO that achieves\nstate-of-the-art performance guarantees while eliminating the need for\nprojections. By integrating a separation oracle with adaptive Online Gradient\nDescent (OGD) and employing a Lyapunov-driven surrogate function, while\ndynamically adjusting step sizes using gradient norms, our method jointly\noptimizes the regret and cumulative constraint violation (CCV). We also use a\nblocked version of OGD that helps achieve tradeoffs betweeen the regret and CCV\nwith the number of calls to the separation oracle. For convex cost functions,\nour algorithm attains an optimal regret of $\\mathcal{O}(\\sqrt{T})$ and a CCV of\n$\\mathcal{O}(\\sqrt{T} \\log T)$, matching the best-known projection-based\nresults, while only using $\\tilde{\\mathcal{O}}({T})$ calls to the separation\noracle. The results also demonstrate a tradeoff where lower calls to the\nseparation oracle increase the regret and the CCV. In the strongly convex\nsetting, we further achieve a regret of $\\mathcal{O}(\\log T)$ and a CCV of\n$\\mathcal{O}(\\sqrt{T\\log T} )$, while requiring ${\\mathcal{O}}({T}^2)$ calls to\nthe separation oracle. Further, tradeoff with the decreasing oracle calls is\nstudied. These results close the gap between projection-free and\nprojection-based approaches, demonstrating that projection-free methods can\nachieve performance comparable to projection-based counterparts."
    },
    {
        "date": "2025-02",
        "title": "Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning",
        "author": "Avinandan Bose, Laurent Lessard, Maryam Fazel, and Krishnamurthy Dj Dvijotham",
        "link": "http://arxiv.org/abs/2502.16737v1",
        "abstract": "The rise of foundation models fine-tuned on human feedback from potentially\nuntrusted users has increased the risk of adversarial data poisoning,\nnecessitating the study of robustness of learning algorithms against such\nattacks. Existing research on provable certified robustness against data\npoisoning attacks primarily focuses on certifying robustness for static\nadversaries who modify a fraction of the dataset used to train the model before\nthe training algorithm is applied. In practice, particularly when learning from\nhuman feedback in an online sense, adversaries can observe and react to the\nlearning process and inject poisoned samples that optimize adversarial\nobjectives better than when they are restricted to poisoning a static dataset\nonce, before the learning algorithm is applied. Indeed, it has been shown in\nprior work that online dynamic adversaries can be significantly more powerful\nthan static ones. We present a novel framework for computing certified bounds\non the impact of dynamic poisoning, and use these certificates to design robust\nlearning algorithms. We give an illustration of the framework for the mean\nestimation and binary classification problems and outline directions for\nextending this in further work. The code to implement our certificates and\nreplicate our results is available at\nhttps://github.com/Avinandan22/Certified-Robustness."
    },
    {
        "date": "2025-02",
        "title": "Towards Optimal Adversarial Robust Reinforcement Learning with Infinity Measurement Error",
        "author": "Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Jiayu Lv, Tiande Guo, and Yudong Hu",
        "link": "http://arxiv.org/abs/2502.16734v1",
        "abstract": "Ensuring the robustness of deep reinforcement learning (DRL) agents against\nadversarial attacks is critical for their trustworthy deployment. Recent\nresearch highlights the challenges of achieving state-adversarial robustness\nand suggests that an optimal robust policy (ORP) does not always exist,\ncomplicating the enforcement of strict robustness constraints. In this paper,\nwe further explore the concept of ORP. We first introduce the Intrinsic\nState-adversarial Markov Decision Process (ISA-MDP), a novel formulation where\nadversaries cannot fundamentally alter the intrinsic nature of state\nobservations. ISA-MDP, supported by empirical and theoretical evidence,\nuniversally characterizes decision-making under state-adversarial paradigms. We\nrigorously prove that within ISA-MDP, a deterministic and stationary ORP\nexists, aligning with the Bellman optimal policy. Our findings theoretically\nreveal that improving DRL robustness does not necessarily compromise\nperformance in natural environments. Furthermore, we demonstrate the necessity\nof infinity measurement error (IME) in both $Q$-function and probability spaces\nto achieve ORP, unveiling vulnerabilities of previous DRL algorithms that rely\non $1$-measurement errors. Motivated by these insights, we develop the\nConsistent Adversarial Robust Reinforcement Learning (CAR-RL) framework, which\noptimizes surrogates of IME. We apply CAR-RL to both value-based and\npolicy-based DRL algorithms, achieving superior performance and validating our\ntheoretical analysis."
    },
    {
        "date": "2025-02",
        "title": "The Popularity Hypothesis in Software Security: A Large-Scale Replication with PHP Packages",
        "author": "Jukka Ruohonen, and Qusai Ramadan",
        "link": "http://arxiv.org/abs/2502.16670v1",
        "abstract": "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security."
    },
    {
        "date": "2025-02",
        "title": "Security Analysis of 5G NR Device-to-Device Sidelink Communications",
        "author": "Evangelos Bitsikas, and Aanjhan Ranganathan",
        "link": "http://arxiv.org/abs/2502.16650v1",
        "abstract": "5G NR sidelink communication enables new possibilities for direct\ndevice-to-device interactions, supporting applications from\nvehicle-to-everything (V2X) systems to public safety, industrial automation,\nand drone networks. However, these advancements come with significant security\nchallenges due to the decentralized trust model and increased reliance on User\nEquipment (UE) for critical functions like synchronization, resource\nallocation, and authorization. This paper presents the first comprehensive\nsecurity analysis of NR V2X sidelink. We identify vulnerabilities across\ncritical procedures and demonstrate plausible attack, including attacks that\nmanipulate data integrity feedback and block resources, ultimately undermining\nthe reliability and privacy of sidelink communications. Our analysis reveals\nthat NR operational modes are vulnerable, with the ones relying on autonomous\nresource management (without network supervision) particularly exposed. To\naddress these issues, we propose mitigation strategies to enhance the security\nof 5G sidelink communications. This work establishes a foundation for future\nefforts to strengthen 5G device-to-device sidelink communications, ensuring its\nsafe deployment in critical applications."
    },
    {
        "date": "2025-02",
        "title": "AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs",
        "author": "Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, and Fons van der Sommen",
        "link": "http://arxiv.org/abs/2502.16610v1",
        "abstract": "Ensuring the quality and integrity of medical images is crucial for\nmaintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosis\nand Computer-Aided Detection (CAD) systems. Covariate shifts are subtle\nvariations in the data distribution caused by different imaging devices or\nsettings and can severely degrade model performance, similar to the effects of\nadversarial attacks. Therefore, it is vital to have a lightweight and fast\nmethod to assess the quality of these images prior to using CAD models.\nAdverX-Ray addresses this need by serving as an image-quality assessment layer,\ndesigned to detect covariate shifts effectively. This Adversarial Variational\nAutoencoder prioritizes the discriminator's role, using the suboptimal outputs\nof the generator as negative samples to fine-tune the discriminator's ability\nto identify high-frequency artifacts. Images generated by adversarial networks\noften exhibit severe high-frequency artifacts, guiding the discriminator to\nfocus excessively on these components. This makes the discriminator ideal for\nthis approach. Trained on patches from X-ray images of specific machine models,\nAdverX-Ray can evaluate whether a scan matches the training distribution, or if\na scan from the same machine is captured under different settings. Extensive\ncomparisons with various OOD detection methods show that AdverX-Ray\nsignificantly outperforms existing techniques, achieving a 96.2% average AUROC\nusing only 64 random patches from an X-ray. Its lightweight and fast\narchitecture makes it suitable for real-time applications, enhancing the\nreliability of medical imaging systems. The code and pretrained models are\npublicly available."
    },
    {
        "date": "2025-02",
        "title": "Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images",
        "author": "Yubo Wang, Jianting Tang, Chaohu Liu, and Linli Xu",
        "link": "http://arxiv.org/abs/2502.16593v1",
        "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable image\nunderstanding and dialogue capabilities, allowing them to handle a variety of\nvisual question answering tasks. However, their widespread availability raises\nconcerns about unauthorized usage and copyright infringement, where users or\nindividuals can develop their own LVLMs by fine-tuning published models. In\nthis paper, we propose a novel method called Parameter Learning Attack (PLA)\nfor tracking the copyright of LVLMs without modifying the original model.\nSpecifically, we construct adversarial images through targeted attacks against\nthe original model, enabling it to generate specific outputs. To ensure these\nattacks remain effective on potential fine-tuned models to trigger copyright\ntracking, we allow the original model to learn the trigger images by updating\nparameters in the opposite direction during the adversarial attack process.\nNotably, the proposed method can be applied after the release of the original\nmodel, thus not affecting the model's performance and behavior. To simulate\nreal-world applications, we fine-tune the original model using various\nstrategies across diverse datasets, creating a range of models for copyright\nverification. Extensive experiments demonstrate that our method can more\neffectively identify the original copyright of fine-tuned models compared to\nbaseline methods. Therefore, this work provides a powerful tool for tracking\ncopyrights and detecting unlicensed usage of LVLMs."
    },
    {
        "date": "2025-02",
        "title": "UNCA: A Neutrosophic-Based Framework for Robust Clustering and Enhanced Data Interpretation",
        "author": "D. Dhinakaran, S. Edwin Raja, S. Gopalakrishnan, D. Selvaraj, and S. D. Lalitha",
        "link": "http://arxiv.org/abs/2502.17523v1",
        "abstract": "Accurately representing the complex linkages and inherent uncertainties\nincluded in huge datasets is still a major difficulty in the field of data\nclustering. We address these issues with our proposed Unified Neutrosophic\nClustering Algorithm (UNCA), which combines a multifaceted strategy with\nNeutrosophic logic to improve clustering performance. UNCA starts with a\nfull-fledged similarity examination via a {\\lambda}-cutting matrix that filters\nmeaningful relationships between each two points of data. Then, we initialize\ncentroids for Neutrosophic K-Means clustering, where the membership values are\nbased on their degrees of truth, indeterminacy and falsity. The algorithm then\nintegrates with a dynamic network visualization and MST (Minimum Spanning Tree)\nso that a visual interpretation of the relationships between the clusters can\nbe clearly represented. UNCA employs SingleValued Neutrosophic Sets (SVNSs) to\nrefine cluster assignments, and after fuzzifying similarity measures,\nguarantees a precise clustering result. The final step involves solidifying the\nclustering results through defuzzification methods, offering definitive cluster\nassignments. According to the performance evaluation results, UNCA outperforms\nconventional approaches in several metrics: it achieved a Silhouette Score of\n0.89 on the Iris Dataset, a Davies-Bouldin Index of 0.59 on the Wine Dataset,\nan Adjusted Rand Index (ARI) of 0.76 on the Digits Dataset, and a Normalized\nMutual Information (NMI) of 0.80 on the Customer Segmentation Dataset. These\nresults demonstrate how UNCA enhances interpretability and resilience in\naddition to improving clustering accuracy when contrasted with Fuzzy C-Means\n(FCM), Neutrosophic C-Means (NCM), as well as Kernel Neutrosophic C-Means\n(KNCM). This makes UNCA a useful tool for complex data processing tasks"
    },
    {
        "date": "2025-02",
        "title": "Can Indirect Prompt Injection Attacks Be Detected and Removed?",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yufei He, Yue Liu, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2502.16580v1",
        "abstract": "Prompt injection attacks manipulate large language models (LLMs) by\nmisleading them to deviate from the original input instructions and execute\nmaliciously injected instructions, because of their instruction-following\ncapabilities and inability to distinguish between the original input\ninstructions and maliciously injected instructions. To defend against such\nattacks, recent studies have developed various detection mechanisms. While\nsignificant efforts have focused on detecting direct prompt injection attacks,\nwhere injected instructions are directly from the attacker who is also the\nuser, limited attention has been given to indirect prompt injection attacks,\nwhere injected instructions are indirectly from external tools, such as a\nsearch engine. Moreover, current works mainly investigate injection detection\nmethods and pay less attention to the post-processing method that aims to\nmitigate the injection after detection. In this paper, we investigate the\nfeasibility of detecting and removing indirect prompt injection attacks, and we\nconstruct a benchmark dataset for evaluation. For detection, we assess the\nperformance of existing LLMs and open-source detection models, and we further\ntrain detection models using our crafted training datasets. For removal, we\nevaluate two intuitive methods: (1) the segmentation removal method, which\nsegments the injected document and removes parts containing injected\ninstructions, and (2) the extraction removal method, which trains an extraction\nmodel to identify and remove injected instructions."
    },
    {
        "date": "2025-02",
        "title": "Multi-Target Federated Backdoor Attack Based on Feature Aggregation",
        "author": "Lingguag Hao, Kuangrong Hao, Bing Wei, and Xue-song Tang",
        "link": "http://arxiv.org/abs/2502.16545v1",
        "abstract": "Current federated backdoor attacks focus on collaboratively training backdoor\ntriggers, where multiple compromised clients train their local trigger patches\nand then merge them into a global trigger during the inference phase. However,\nthese methods require careful design of the shape and position of trigger\npatches and lack the feature interactions between trigger patches during\ntraining, resulting in poor backdoor attack success rates. Moreover, the pixels\nof the patches remain untruncated, thereby making abrupt areas in backdoor\nexamples easily detectable by the detection algorithm. To this end, we propose\na novel benchmark for the federated backdoor attack based on feature\naggregation. Specifically, we align the dimensions of triggers with images,\ndelimit the trigger's pixel boundaries, and facilitate feature interaction\namong local triggers trained by each compromised client. Furthermore,\nleveraging the intra-class attack strategy, we propose the simultaneous\ngeneration of backdoor triggers for all target classes, significantly reducing\nthe overall production time for triggers across all target classes and\nincreasing the risk of the federated model being attacked. Experiments\ndemonstrate that our method can not only bypass the detection of defense\nmethods while patch-based methods fail, but also achieve a zero-shot backdoor\nattack with a success rate of 77.39%. To the best of our knowledge, our work is\nthe first to implement such a zero-shot attack in federated learning. Finally,\nwe evaluate attack performance by varying the trigger's training factors,\nincluding poison location, ratio, pixel bound, and trigger training duration\n(local epochs and communication rounds)."
    },
    {
        "date": "2025-02",
        "title": "Rebalancing the Scales: A Systematic Mapping Study of Generative Adversarial Networks (GANs) in Addressing Data Imbalance",
        "author": "Pankaj Yadav, Gulshan Sihag, and Vivek Vijay",
        "link": "http://arxiv.org/abs/2502.16535v1",
        "abstract": "Machine learning algorithms are used in diverse domains, many of which face\nsignificant challenges due to data imbalance. Studies have explored various\napproaches to address the issue, like data preprocessing, cost-sensitive\nlearning, and ensemble methods. Generative Adversarial Networks (GANs) showed\nimmense potential as a data preprocessing technique that generates good quality\nsynthetic data. This study employs a systematic mapping methodology to analyze\n3041 papers on GAN-based sampling techniques for imbalanced data sourced from\nfour digital libraries. A filtering process identified 100 key studies spanning\ndomains such as healthcare, finance, and cybersecurity. Through comprehensive\nquantitative analysis, this research introduces three categorization mappings\nas application domains, GAN techniques, and GAN variants used to handle the\nimbalanced nature of the data. GAN-based over-sampling emerges as an effective\npreprocessing method. Advanced architectures and tailored frameworks helped\nGANs to improve further in the case of data imbalance. GAN variants like\nvanilla GAN, CTGAN, and CGAN show great adaptability in structured imbalanced\ndata cases. Interest in GANs for imbalanced data has grown tremendously,\ntouching a peak in recent years, with journals and conferences playing crucial\nroles in transmitting foundational theories and practical applications. While\nwith these advances, none of the reviewed studies explicitly explore hybridized\nGAN frameworks with diffusion models or reinforcement learning techniques. This\ngap leads to a future research idea develop innovative approaches for\neffectively handling data imbalance."
    },
    {
        "date": "2025-02",
        "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension",
        "author": "Yulong Wu, Viktor Schlegel, and Riza Batista-Navarro",
        "link": "http://arxiv.org/abs/2502.16523v1",
        "abstract": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data."
    },
    {
        "date": "2025-02",
        "title": "Auxiliary Discrminator Sequence Generative Adversarial Networks (ADSeqGAN) for Few Sample Molecule Generation",
        "author": "Haocheng Tang, Jing Long, and Junmei Wang",
        "link": "http://arxiv.org/abs/2502.16446v1",
        "abstract": "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work."
    },
    {
        "date": "2025-02",
        "title": "Unified Prompt Attack Against Text-to-Image Generation Models",
        "author": "Duo Peng, Qiuhong Ke, Mark He Huang, Ping Hu, and Jun Liu",
        "link": "http://arxiv.org/abs/2502.16423v1",
        "abstract": "Text-to-Image (T2I) models have advanced significantly, but their growing\npopularity raises security concerns due to their potential to generate harmful\nimages. To address these issues, we propose UPAM, a novel framework to evaluate\nthe robustness of T2I models from an attack perspective. Unlike prior methods\nthat focus solely on textual defenses, UPAM unifies the attack on both textual\nand visual defenses. Additionally, it enables gradient-based optimization,\novercoming reliance on enumeration for improved efficiency and effectiveness.\nTo handle cases where T2I models block image outputs due to defenses, we\nintroduce Sphere-Probing Learning (SPL) to enable optimization even without\nimage results. Following SPL, our model bypasses defenses, inducing the\ngeneration of harmful content. To ensure semantic alignment with attacker\nintent, we propose Semantic-Enhancing Learning (SEL) for precise semantic\ncontrol. UPAM also prioritizes the naturalness of adversarial prompts using\nIn-context Naturalness Enhancement (INE), making them harder for human\nexaminers to detect. Additionally, we address the issue of iterative\nqueries--common in prior methods and easily detectable by API defenders--by\nintroducing Transferable Attack Learning (TAL), allowing effective attacks with\nminimal queries. Extensive experiments validate UPAM's superiority in\neffectiveness, efficiency, naturalness, and low query detection rates."
    },
    {
        "date": "2025-02",
        "title": "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications",
        "author": "Feng Ma, Xiu-min Wang, Chen Chen, Xiao-bin Xu, and Xin-ping Yan",
        "link": "http://arxiv.org/abs/2502.16402v1",
        "abstract": "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications."
    },
    {
        "date": "2025-02",
        "title": "Efficient Semantic-aware Encryption for Secure Communications in Intelligent Connected Vehicles",
        "author": "Bizhu Wang, Zhiqiang Bian, Yue Chen, Xiaodong Xu, Chen Sun, Wenqi Zhang, and Ping Zhang",
        "link": "http://arxiv.org/abs/2502.16400v1",
        "abstract": "Semantic communication (SemCom) significantly improves inter-vehicle\ninteractions in intelligent connected vehicles (ICVs) within limited wireless\nspectrum. However, the open nature of wireless communications introduces\neavesdropping risks. To mitigate this, we propose the Efficient Semantic-aware\nEncryption (ESAE) mechanism, integrating cryptography into SemCom to secure\nsemantic transmission without complex key management. ESAE leverages semantic\nreciprocity between source and reconstructed information from past\ncommunications to independently generate session keys at both ends, reducing\nkey transmission costs and associated security risks. Additionally, ESAE\nintroduces a semantic-aware key pre-processing method (SA-KP) using the\nYOLO-v10 model to extract consistent semantics from bit-level diverse yet\nsemantically identical content, ensuring key consistency. Experimental results\nvalidate ESAE's effectiveness and feasibility under various wireless\nconditions, with key performance factors discussed."
    },
    {
        "date": "2025-02",
        "title": "Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness",
        "author": "Sangil Han, Kyoowon Kim, and Sungkyu Jung",
        "link": "http://arxiv.org/abs/2502.16391v1",
        "abstract": "In this paper, we explore the theoretical properties of subspace recovery\nusing Winsorized Principal Component Analysis (WPCA), utilizing a common data\ntransformation technique that caps extreme values to mitigate the impact of\noutliers. Despite the widespread use of winsorization in various tasks of\nmultivariate analysis, its theoretical properties, particularly for subspace\nrecovery, have received limited attention. We provide a detailed analysis of\nthe accuracy of WPCA, showing that increasing the number of samples while\ndecreasing the proportion of outliers guarantees the consistency of the sample\nsubspaces from WPCA with respect to the true population subspace. Furthermore,\nwe establish perturbation bounds that ensure the WPCA subspace obtained from\ncontaminated data remains close to the subspace recovered from pure data.\nAdditionally, we extend the classical notion of breakdown points to\nsubspace-valued statistics and derive lower bounds for the breakdown points of\nWPCA. Our analysis demonstrates that WPCA exhibits strong robustness to\noutliers while maintaining consistency under mild assumptions. A toy example is\nprovided to numerically illustrate the behavior of the upper bounds for\nperturbation bounds and breakdown points, emphasizing winsorization's utility\nin subspace recovery."
    },
    {
        "date": "2025-02",
        "title": "Personhood Credentials: Human-Centered Design Recommendation Balancing Security, Usability, and Trust",
        "author": "Ayae Ide, and Tanusree Sharma",
        "link": "http://arxiv.org/abs/2502.16375v1",
        "abstract": "Building on related concepts, like, decentralized identifiers (DIDs), proof\nof personhood, anonymous credentials, personhood credentials (PHCs) emerged as\nan alternative approach, enabling individuals to verify to digital service\nproviders that they are a person without disclosing additional information.\nHowever, new technologies might introduce some friction due to users\nmisunderstandings and mismatched expectations. Despite their growing\nimportance, limited research has been done on users perceptions and preferences\nregarding PHCs. To address this gap, we conducted competitive analysis, and\nsemi-structured online user interviews with 23 participants from US and EU to\nprovide concrete design recommendations for PHCs that incorporate user needs,\nadoption rules, and preferences. Our study -- (a)surfaces how people reason\nabout unknown privacy and security guarantees of PHCs compared to current\nverification methods -- (b) presents the impact of several factors on how\npeople would like to onboard and manage PHCs, including, trusted issuers (e.g.\ngov), ground truth data to issue PHC (e.g biometrics, physical id), and\nissuance system (e.g. centralized vs decentralized). In a think-aloud\nconceptual design session, participants recommended -- conceptualized design,\nsuch as periodic biometrics verification, time-bound credentials, visually\ninteractive human-check, and supervision of government for issuance system. We\npropose actionable designs reflecting users preferences."
    },
    {
        "date": "2025-02",
        "title": "Revealing Microscopic Objects in Fluorescence Live Imaging by Video-to-video Translation Based on A Spatial-temporal Generative Adversarial Network",
        "author": "Yang Jiao, Mei Yang, and Mo Weng",
        "link": "http://arxiv.org/abs/2502.16342v1",
        "abstract": "In spite of being a valuable tool to simultaneously visualize multiple types\nof subcellular structures using spectrally distinct fluorescent labels, a\nstandard fluoresce microscope is only able to identify a few microscopic\nobjects; such a limit is largely imposed by the number of fluorescent labels\navailable to the sample. In order to simultaneously visualize more objects, in\nthis paper, we propose to use video-to-video translation that mimics the\ndevelopment process of microscopic objects. In essence, we use a microscopy\nvideo-to-video translation framework namely Spatial-temporal Generative\nAdversarial Network (STGAN) to reveal the spatial and temporal relationships\nbetween the microscopic objects, after which a microscopy video of one object\ncan be translated to another object in a different domain. The experimental\nresults confirm that the proposed STGAN is effective in microscopy\nvideo-to-video translation that mitigates the spectral conflicts caused by the\nlimited fluorescent labels, allowing multiple microscopic objects be\nsimultaneously visualized."
    },
    {
        "date": "2025-02",
        "title": "Verification of Bit-Flip Attacks against Quantized Neural Networks",
        "author": "Yedi Zhang, Lei Huang, Pengfei Gao, Fu Song, Jun Sun, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2502.16286v1",
        "abstract": "In the rapidly evolving landscape of neural network security, the resilience\nof neural networks against bit-flip attacks (i.e., an attacker maliciously\nflips an extremely small amount of bits within its parameter storage memory\nsystem to induce harmful behavior), has emerged as a relevant area of research.\nExisting studies suggest that quantization may serve as a viable defense\nagainst such attacks. Recognizing the documented susceptibility of real-valued\nneural networks to such attacks and the comparative robustness of quantized\nneural networks (QNNs), in this work, we introduce BFAVerifier, the first\nverification framework designed to formally verify the absence of bit-flip\nattacks or to identify all vulnerable parameters in a sound and rigorous\nmanner. BFAVerifier comprises two integral components: an abstraction-based\nmethod and an MILP-based method. Specifically, we first conduct a reachability\nanalysis with respect to symbolic parameters that represent the potential\nbit-flip attacks, based on a novel abstract domain with a sound guarantee. If\nthe reachability analysis fails to prove the resilience of such attacks, then\nwe encode this verification problem into an equivalent MILP problem which can\nbe solved by off-the-shelf solvers. Therefore, BFAVerifier is sound, complete,\nand reasonably efficient. We conduct extensive experiments, which demonstrate\nits effectiveness and efficiency across various network architectures,\nquantization bit-widths, and adversary capabilities."
    },
    {
        "date": "2025-02",
        "title": "Beyond Trusting Trust: Multi-Model Validation for Robust Code Generation",
        "author": "Bradley McDanel",
        "link": "http://arxiv.org/abs/2502.16279v1",
        "abstract": "This paper explores the parallels between Thompson's \"Reflections on Trusting\nTrust\" and modern challenges in LLM-based code generation. We examine how\nThompson's insights about compiler backdoors take on new relevance in the era\nof large language models, where the mechanisms for potential exploitation are\neven more opaque and difficult to analyze. Building on this analogy, we discuss\nhow the statistical nature of LLMs creates novel security challenges in code\ngeneration pipelines. As a potential direction forward, we propose an\nensemble-based validation approach that leverages multiple independent models\nto detect anomalous code patterns through cross-model consensus. This\nperspective piece aims to spark discussion about trust and validation in\nAI-assisted software development."
    },
    {
        "date": "2025-02",
        "title": "Robustness and Cybersecurity in the EU Artificial Intelligence Act",
        "author": "Henrik Nolte, Miriam Rateike, and Mich\u00e8le Finck",
        "link": "http://arxiv.org/abs/2502.16184v1",
        "abstract": "The EU Artificial Intelligence Act (AIA) establishes different legal\nprinciples for different types of AI systems. While prior work has sought to\nclarify some of these principles, little attention has been paid to robustness\nand cybersecurity. This paper aims to fill this gap. We identify legal\nchallenges and shortcomings in provisions related to robustness and\ncybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI\nmodels (Art. 55 AIA). We show that robustness and cybersecurity demand\nresilience against performance disruptions. Furthermore, we assess potential\nchallenges in implementing these provisions in light of recent advancements in\nthe machine learning (ML) literature. Our analysis informs efforts to develop\nharmonized standards, guidelines by the European Commission, as well as\nbenchmarks and measurement methodologies under Art. 15(2) AIA. With this, we\nseek to bridge the gap between legal terminology and ML research, fostering a\nbetter alignment between research and implementation efforts."
    },
    {
        "date": "2025-02",
        "title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models",
        "author": "Xinwei Liu, Xiaojun Jia, Yuan Xun, Hua Zhang, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2502.16167v1",
        "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in\ntext-to-image (T2I) synthesis. However, the widespread use of personalized\ngenerative models raises significant concerns regarding privacy violations and\ncopyright infringement. To address these issues, researchers have proposed\nadversarial perturbation-based protection techniques. However, these methods\nhave notable limitations, including insufficient robustness against data\ntransformations and the inability to fully eliminate identifiable features of\nprotected objects in the generated output. In this paper, we introduce\nPersGuard, a novel backdoor-based approach that prevents malicious\npersonalization of specific images. Unlike traditional adversarial perturbation\nmethods, PersGuard implant backdoor triggers into pre-trained T2I models,\npreventing the generation of customized outputs for designated protected images\nwhile allowing normal personalization for unprotected ones. Unfortunately,\nexisting backdoor methods for T2I diffusion models fail to be applied to\npersonalization scenarios due to the different backdoor objectives and the\npotential backdoor elimination during downstream fine-tuning processes. To\naddress these, we propose three novel backdoor objectives specifically designed\nfor personalization scenarios, coupled with backdoor retention loss engineered\nto resist downstream fine-tuning. These components are integrated into a\nunified optimization framework. Extensive experimental evaluations demonstrate\nPersGuard's effectiveness in preserving data privacy, even under challenging\nconditions including gray-box settings, multi-object protection, and facial\nidentity scenarios. Our method significantly outperforms existing techniques,\noffering a more robust solution for privacy and copyright protection."
    },
    {
        "date": "2025-02",
        "title": "Robust Dynamic Facial Expression Recognition",
        "author": "Feng Liu, Hanyang Wang, and Siyuan Shen",
        "link": "http://arxiv.org/abs/2502.16129v1",
        "abstract": "The study of Dynamic Facial Expression Recognition (DFER) is a nascent field\nof research that involves the automated recognition of facial expressions in\nvideo data. Although existing research has primarily focused on learning\nrepresentations under noisy and hard samples, the issue of the coexistence of\nboth types of samples remains unresolved. In order to overcome this challenge,\nthis paper proposes a robust method of distinguishing between hard and noisy\nsamples. This is achieved by evaluating the prediction agreement of the model\non different sampled clips of the video. Subsequently, methodologies that\nreinforce the learning of hard samples and mitigate the impact of noisy samples\ncan be employed. Moreover, to identify the principal expression in a video and\nenhance the model's capacity for representation learning, comprising a key\nexpression re-sampling framework and a dual-stream hierarchical network is\nproposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The key\nexpression re-sampling framework is designed to identify the key expression,\nthereby mitigating the potential confusion caused by non-target expressions.\nRDFER employs two sequence models with the objective of disentangling\nshort-term facial movements and long-term emotional changes. The proposed\nmethod has been shown to outperform current State-Of-The-Art approaches in DFER\nthrough extensive experimentation on benchmark datasets such as DFEW and\nFERV39K. A comprehensive analysis provides valuable insights and observations\nregarding the proposed agreement. This work has significant implications for\nthe field of dynamic facial expression recognition and promotes the further\ndevelopment of the field of noise-consistent robust learning in dynamic facial\nexpression recognition. The code is available from\n[https://github.com/Cross-Innovation-Lab/RDFER]."
    },
    {
        "date": "2025-02",
        "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals",
        "author": "Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, and Yi Zhang",
        "link": "http://arxiv.org/abs/2502.16101v1",
        "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in\nmitigating hallucinations in large language models (LLMs). However, LLMs\nstruggle to handle misleading retrievals and often fail to maintain their own\nreasoning when exposed to conflicting or selectively-framed evidence, making\nthem vulnerable to real-world misinformation. In such real-world retrieval\nscenarios, misleading and conflicting information is rampant, particularly in\nthe political domain, where evidence is often selectively framed, incomplete,\nor polarized. However, existing RAG benchmarks largely assume a clean retrieval\nsetting, where models succeed by accurately retrieving and generating answers\nfrom gold-standard documents. This assumption fails to align with real-world\nconditions, leading to an overestimation of RAG system performance. To bridge\nthis gap, we introduce RAGuard, a fact-checking dataset designed to evaluate\nthe robustness of RAG systems against misleading retrievals. Unlike prior\nbenchmarks that rely on synthetic noise, our dataset constructs its retrieval\ncorpus from Reddit discussions, capturing naturally occurring misinformation.\nIt categorizes retrieved evidence into three types: supporting, misleading, and\nirrelevant, providing a realistic and challenging testbed for assessing how\nwell RAG systems navigate different retrieval information. Our benchmark\nexperiments reveal that when exposed to misleading retrievals, all tested\nLLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no\nretrieval at all), highlighting their susceptibility to noisy environments. To\nthe best of our knowledge, RAGuard is the first benchmark to systematically\nassess RAG robustness against misleading evidence. We expect this benchmark\nwill drive future research toward improving RAG systems beyond idealized\ndatasets, making them more reliable for real-world applications."
    },
    {
        "date": "2025-02",
        "title": "Stealing Training Data from Large Language Models in Decentralized Training through Activation Inversion Attack",
        "author": "Chenxi Dai, Lin Lu, and Pan Zhou",
        "link": "http://arxiv.org/abs/2502.16086v1",
        "abstract": "Decentralized training has become a resource-efficient framework to\ndemocratize the training of large language models (LLMs). However, the privacy\nrisks associated with this framework, particularly due to the potential\ninclusion of sensitive data in training datasets, remain unexplored. This paper\nidentifies a novel and realistic attack surface: the privacy leakage from\ntraining data in decentralized training, and proposes \\textit{activation\ninversion attack} (AIA) for the first time. AIA first constructs a shadow\ndataset comprising text labels and corresponding activations using public\ndatasets. Leveraging this dataset, an attack model can be trained to\nreconstruct the training data from activations in victim decentralized\ntraining. We conduct extensive experiments on various LLMs and publicly\navailable datasets to demonstrate the susceptibility of decentralized training\nto AIA. These findings highlight the urgent need to enhance security measures\nin decentralized training to mitigate privacy risks in training LLMs."
    },
    {
        "date": "2025-02",
        "title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing Environments",
        "author": "Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, and Yushun Dong",
        "link": "http://arxiv.org/abs/2502.16065v1",
        "abstract": "Model Extraction Attacks (MEAs) threaten modern machine learning systems by\nenabling adversaries to steal models, exposing intellectual property and\ntraining data. With the increasing deployment of machine learning models in\ndistributed computing environments, including cloud, edge, and federated\nlearning settings, each paradigm introduces distinct vulnerabilities and\nchallenges. Without a unified perspective on MEAs across these distributed\nenvironments, organizations risk fragmented defenses, inadequate risk\nassessments, and substantial economic and privacy losses. This survey is\nmotivated by the urgent need to understand how the unique characteristics of\ncloud, edge, and federated deployments shape attack vectors and defense\nrequirements. We systematically examine the evolution of attack methodologies\nand defense mechanisms across these environments, demonstrating how\nenvironmental factors influence security strategies in critical sectors such as\nautonomous vehicles, healthcare, and financial services. By synthesizing recent\nadvances in MEAs research and discussing the limitations of current evaluation\npractices, this survey provides essential insights for developing robust and\nadaptive defense strategies. Our comprehensive approach highlights the\nimportance of integrating protective measures across the entire distributed\ncomputing landscape to ensure the secure deployment of machine learning models."
    },
    {
        "date": "2025-02",
        "title": "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven Deep Reinforcement Learning",
        "author": "Zahra Aref, Sheng Wei, and Narayan B. Mandayam",
        "link": "http://arxiv.org/abs/2502.16054v1",
        "abstract": "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms."
    },
    {
        "date": "2025-02",
        "title": "A Multi-Scale Isolation Forest Approach for Real-Time Detection and Filtering of FGSM Adversarial Attacks in Video Streams of Autonomous Vehicles",
        "author": "Richard Abhulimhen, Negash Begashaw, Gurcan Comert, Chunheng Zhao, and Pierluigi Pisu",
        "link": "http://arxiv.org/abs/2502.16044v1",
        "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable success across a\nwide range of tasks, particularly in fields such as image classification.\nHowever, DNNs are highly susceptible to adversarial attacks, where subtle\nperturbations are introduced to input images, leading to erroneous model\noutputs. In today's digital era, ensuring the security and integrity of images\nprocessed by DNNs is of critical importance. One of the most prominent\nadversarial attack methods is the Fast Gradient Sign Method (FGSM), which\nperturbs images in the direction of the loss gradient to deceive the model.\n  This paper presents a novel approach for detecting and filtering FGSM\nadversarial attacks in image processing tasks. Our proposed method evaluates\n10,000 images, each subjected to five different levels of perturbation,\ncharacterized by $\\epsilon$ values of 0.01, 0.02, 0.05, 0.1, and 0.2. These\nperturbations are applied in the direction of the loss gradient. We demonstrate\nthat our approach effectively filters adversarially perturbed images,\nmitigating the impact of FGSM attacks.\n  The method is implemented in Python, and the source code is publicly\navailable on GitHub for reproducibility and further research."
    },
    {
        "date": "2025-02",
        "title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving",
        "author": "Prashant Shekhar, Bidur Devkota, Dumindu Samaraweera, Laxima Niure Kandel, and Manoj Babu",
        "link": "http://arxiv.org/abs/2502.16012v1",
        "abstract": "Adversarial attacks pose a significant threat to deep learning models,\nparticularly in safety-critical applications like healthcare and autonomous\ndriving. Recently, patch based attacks have demonstrated effectiveness in\nreal-time inference scenarios owing to their 'drag and drop' nature. Following\nthis idea for Semantic Segmentation (SS), here we propose a novel Expectation\nOver Transformation (EOT) based adversarial patch attack that is more realistic\nfor autonomous vehicles. To effectively train this attack we also propose a\n'simplified' loss function that is easy to analyze and implement. Using this\nattack as our basis, we investigate whether adversarial patches once optimized\non a specific SS model, can fool other models or architectures. We conduct a\ncomprehensive cross-model transferability analysis of adversarial patches\ntrained on SOTA Convolutional Neural Network (CNN) models such PIDNet-S,\nPIDNet-M and PIDNet-L, among others. Additionally, we also include the\nSegformer model to study transferability to Vision Transformers (ViTs). All of\nour analysis is conducted on the widely used Cityscapes dataset. Our study\nreveals key insights into how model architectures (CNN vs CNN or CNN vs.\nTransformer-based) influence attack susceptibility. In particular, we conclude\nthat although the transferability (effectiveness) of attacks on unseen images\nof any dimension is really high, the attacks trained against one particular\nmodel are minimally effective on other models. And this was found to be true\nfor both ViT and CNN based models. Additionally our results also indicate that\nfor CNN-based models, the repercussions of patch attacks are local, unlike\nViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less\nmisclassification than others. The code for the project is available at:\nhttps://github.com/p-shekhar/adversarial-patch-transferability"
    },
    {
        "date": "2025-02",
        "title": "IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector",
        "author": "Zheng Chen, Yushi Feng, Changyang He, Yue Deng, Hongxi Pu, and Bo Li",
        "link": "http://arxiv.org/abs/2502.15902v1",
        "abstract": "Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinguishing between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide explainable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and a Distinguisher that examines how\nwell the input texts align with the predicted prompts. We develop and examine\ntwo versions of Distinguishers. Empirical evaluations demonstrate that both\nDistinguishers perform significantly better than the baseline methods, with\nversion2 outperforming baselines by 9.73% on in-distribution data (F1-score)\nand 12.65% on OOD data (AUROC). Furthermore, a user study is conducted to\nillustrate that IPAD enhances the AI detection trustworthiness by allowing\nusers to directly examine the decision-making evidence, which provides\ninterpretable support for its state-of-the-art detection results."
    },
    {
        "date": "2025-02",
        "title": "Directional Gradient Projection for Robust Fine-Tuning of Foundation Models",
        "author": "Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira",
        "link": "http://arxiv.org/abs/2502.15895v1",
        "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks\nwhile preserving their robustness to distribution shifts. Existing methods\nprimarily focus on constraining and projecting current model towards the\npre-trained initialization based on the magnitudes between fine-tuned and\npre-trained weights, which often require extensive hyper-parameter tuning and\ncan sometimes result in underfitting. In this work, we propose Directional\nGradient Projection (DiGraP), a novel layer-wise trainable method that\nincorporates directional information from gradients to bridge regularization\nand multi-objective optimization. Besides demonstrating our method on image\nclassification, as another contribution we generalize this area to the\nmulti-modal evaluation settings for robust fine-tuning. Specifically, we first\nbridge the uni-modal and multi-modal gap by performing analysis on Image\nClassification reformulated Visual Question Answering (VQA) benchmarks and\nfurther categorize ten out-of-distribution (OOD) VQA datasets by distribution\nshift types and degree (i.e. near versus far OOD). Experimental results show\nthat DiGraP consistently outperforms existing baselines across Image\nClassfication and VQA tasks with discriminative and generative backbones,\nimproving both in-distribution (ID) generalization and OOD robustness."
    },
    {
        "date": "2025-02",
        "title": "Blockchain-based Trust Management in Security Credential Management System for Vehicular Network",
        "author": "SangHyun Byun, Arijet Sarker, Sang-Yoon Chang, and Jugal Kalita",
        "link": "http://arxiv.org/abs/2502.15653v1",
        "abstract": "Cellular networking is advancing as a wireless technology to support diverse\napplications in vehicular communication, enabling vehicles to interact with\nvarious applications to enhance the driving experience, even when managed by\ndifferent authorities. Security Credential Management System (SCMS) is the\nPublic Key Infrastructure (PKI) for vehicular networking and the\nstate-of-the-art distributed PKI to protect the privacy-preserving vehicular\nnetworking against an honest-but-curious authority using multiple authorities\nand to decentralize the trust management. We build a Blockchain-Based Trust\nManagement (BBTM) to provide even greater decentralization and security.\nSpecifically, BBTM uses the blockchain to 1) replace the existing Policy\nGenerator (PG), 2) manage the policy of each authority in SCMS, 3) aggregate\nthe Global Certificate Chain File (GCCF), and 4) provide greater accountability\nand transparency on the aforementioned functionalities. We implement BBTM on\nHyperledger Fabric using a smart contract for experimentation and analyses. Our\nexperiments show that BBTM is lightweight in processing, efficient management\nin the certificate chain and ledger size, supports a bandwidth of multiple\ntransactions per second, and provides validated end-entities."
    },
    {
        "date": "2025-02",
        "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
        "author": "Tianle Li, Chenyang Zhang, Xingwu Chen, Yuan Cao, and Difan Zou",
        "link": "http://arxiv.org/abs/2502.15609v1",
        "abstract": "Transformer-based Large Language Models (LLMs) have demonstrated powerful\nin-context learning capabilities. However, their predictions can be disrupted\nby factually correct context, a phenomenon known as context hijacking,\nrevealing a significant robustness issue. To understand this phenomenon\ntheoretically, we explore an in-context linear classification problem based on\nrecent advances in linear transformers. In our setup, context tokens are\ndesigned as factually correct query-answer pairs, where the queries are similar\nto the final query but have opposite labels. Then, we develop a general\ntheoretical analysis on the robustness of the linear transformers, which is\nformulated as a function of the model depth, training context lengths, and\nnumber of hijacking context tokens. A key finding is that a well-trained deeper\ntransformer can achieve higher robustness, which aligns with empirical\nobservations. We show that this improvement arises because deeper layers enable\nmore fine-grained optimization steps, effectively mitigating interference from\ncontext hijacking. This is also well supported by our numerical experiments.\nOur findings provide theoretical insights into the benefits of deeper\narchitectures and contribute to enhancing the understanding of transformer\narchitectures."
    },
    {
        "date": "2025-02",
        "title": "FLARE: Fault Attack Leveraging Address Reconfiguration Exploits in Multi-Tenant FPGAs",
        "author": "Jayeeta Chaudhuri, Hassan Nassar, Dennis R. E. Gnad, Jorg Henkel, Mehdi B. Tahoori, and Krishnendu Chakrabarty",
        "link": "http://arxiv.org/abs/2502.15578v1",
        "abstract": "Modern FPGAs are increasingly supporting multi-tenancy to enable dynamic\nreconfiguration of user modules. While multi-tenant FPGAs improve utilization\nand flexibility, this paradigm introduces critical security threats. In this\npaper, we present FLARE, a fault attack that exploits vulnerabilities in the\npartial reconfiguration process, specifically while a user bitstream is being\nuploaded to the FPGA by a reconfiguration manager. Unlike traditional fault\nattacks that operate during module runtime, FLARE injects faults in the\nbitstream during its reconfiguration, altering the configuration address and\nredirecting it to unintended partial reconfigurable regions (PRRs). This\nenables the overwriting of pre-configured co-tenant modules, disrupting their\nfunctionality. FLARE leverages power-wasters that activate briefly during the\nreconfiguration process, making the attack stealthy and more challenging to\ndetect with existing countermeasures. Experimental results on a Xilinx Pynq\nFPGA demonstrate the effectiveness of FLARE in compromising multiple user\nbitstreams during the reconfiguration process."
    },
    {
        "date": "2025-02",
        "title": "Context-Aware Doubly-Robust Semi-Supervised Learning",
        "author": "Clement Ruah, Houssem Sifaou, Osvaldo Simeone, and Bashir Al-Hashimi",
        "link": "http://arxiv.org/abs/2502.15577v1",
        "abstract": "The widespread adoption of artificial intelligence (AI) in next-generation\ncommunication systems is challenged by the heterogeneity of traffic and network\nconditions, which call for the use of highly contextual, site-specific, data. A\npromising solution is to rely not only on real-world data, but also on\nsynthetic pseudo-data generated by a network digital twin (NDT). However, the\neffectiveness of this approach hinges on the accuracy of the NDT, which can\nvary widely across different contexts. To address this problem, this paper\nintroduces context-aware doubly-robust (CDR) learning, a novel semi-supervised\nscheme that adapts its reliance on the pseudo-data to the different levels of\nfidelity of the NDT across contexts. CDR is evaluated on the task of downlink\nbeamforming, showing superior performance compared to previous state-of-the-art\nsemi-supervised approaches."
    },
    {
        "date": "2025-02",
        "title": "Model Privacy: A Unified Framework to Understand Model Stealing Attacks and Defenses",
        "author": "Ganghua Wang, Yuhong Yang, and Jie Ding",
        "link": "http://arxiv.org/abs/2502.15567v1",
        "abstract": "The use of machine learning (ML) has become increasingly prevalent in various\ndomains, highlighting the importance of understanding and ensuring its safety.\nOne pressing concern is the vulnerability of ML applications to model stealing\nattacks. These attacks involve adversaries attempting to recover a learned\nmodel through limited query-response interactions, such as those found in\ncloud-based services or on-chip artificial intelligence interfaces. While\nexisting literature proposes various attack and defense strategies, these often\nlack a theoretical foundation and standardized evaluation criteria. In\nresponse, this work presents a framework called ``Model Privacy'', providing a\nfoundation for comprehensively analyzing model stealing attacks and defenses.\nWe establish a rigorous formulation for the threat model and objectives,\npropose methods to quantify the goodness of attack and defense strategies, and\nanalyze the fundamental tradeoffs between utility and privacy in ML models. Our\ndeveloped theory offers valuable insights into enhancing the security of ML\nmodels, especially highlighting the importance of the attack-specific structure\nof perturbations for effective defenses. We demonstrate the application of\nmodel privacy from the defender's perspective through various learning\nscenarios. Extensive experiments corroborate the insights and the effectiveness\nof defense mechanisms developed under the proposed framework."
    },
    {
        "date": "2025-02",
        "title": "A Defensive Framework Against Adversarial Attacks on Machine Learning-Based Network Intrusion Detection Systems",
        "author": "Benyamin Tafreshian, and Shengzhi Zhang",
        "link": "http://arxiv.org/abs/2502.15561v1",
        "abstract": "As cyberattacks become increasingly sophisticated, advanced Network Intrusion\nDetection Systems (NIDS) are critical for modern network security. Traditional\nsignature-based NIDS are inadequate against zero-day and evolving attacks. In\nresponse, machine learning (ML)-based NIDS have emerged as promising solutions;\nhowever, they are vulnerable to adversarial evasion attacks that subtly\nmanipulate network traffic to bypass detection. To address this vulnerability,\nwe propose a novel defensive framework that enhances the robustness of ML-based\nNIDS by simultaneously integrating adversarial training, dataset balancing\ntechniques, advanced feature engineering, ensemble learning, and extensive\nmodel fine-tuning. We validate our framework using the NSL-KDD and UNSW-NB15\ndatasets. Experimental results show, on average, a 35% increase in detection\naccuracy and a 12.5% reduction in false positives compared to baseline models,\nparticularly under adversarial conditions. The proposed defense against\nadversarial attacks significantly advances the practical deployment of robust\nML-based NIDS in real-world networks."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs",
        "author": "Giulio Zizzo, Giandomenico Cornacchia, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Beat Buesser, Mark Purcell, Pin-Yu Chen, Prasanna Sattigeri, and Kush Varshney",
        "link": "http://arxiv.org/abs/2502.15427v1",
        "abstract": "As large language models (LLMs) become integrated into everyday applications,\nensuring their robustness and security is increasingly critical. In particular,\nLLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks.\nThe variety of jailbreak styles is growing, necessitating the use of external\ndefences known as guardrails. While many jailbreak defences have been proposed,\nnot all defences are able to handle new out-of-distribution attacks due to the\nnarrow segment of jailbreaks used to align them. Moreover, the lack of\nsystematisation around defences has created significant gaps in their practical\napplication. In this work, we perform systematic benchmarking across 15\ndifferent defences, considering a broad swathe of malicious and benign\ndatasets. We find that there is significant performance variation depending on\nthe style of jailbreak a defence is subject to. Additionally, we show that\nbased on current datasets available for evaluation, simple baselines can\ndisplay competitive out-of-distribution performance compared to many\nstate-of-the-art defences. Code is available at\nhttps://github.com/IBM/Adversarial-Prompt-Evaluation."
    },
    {
        "date": "2025-02",
        "title": "On the (In)Security of Non-resettable Device Identifiers in Custom Android Systems",
        "author": "Zikan Dong, Liu Wang, Guoai Xu, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2502.15270v1",
        "abstract": "User tracking is critical in the mobile ecosystem, which relies on device\nidentifiers to build clear user profiles. In earlier ages, Android allowed easy\naccess to non-resettable device identifiers like device serial numbers and IMEI\nby third-party apps for user tracking. As privacy concerns grew, Google has\ntightened restrictions on these identifiers in native Android. Despite this,\nstakeholders in custom Android systems seek consistent and stable user tracking\ncapabilities across different system and device models, and they have\nintroduced covert channels (e.g., system properties and settings) in customized\nsystems to access identifiers, which undoubtedly increases the risk of user\nprivacy breaches. This paper examines the introduction of non-resettable\nidentifiers through system customization and their vulnerability due to poor\naccess control. We present IDRadar, a scalable and accurate approach for\nidentifying vulnerable properties and settings on custom Android ROMs. Applying\nour approach to 1,814 custom ROMs, we have identified 8,192 system properties\nand 3,620 settings that store non-resettable identifiers, with 3,477 properties\nand 1,336 settings lacking adequate access control, which can be abused by\nthird-party apps to track users without permissions. Our large-scale analysis\ncan identify a large number of security issues which are two orders of\nmagnitude greater than existing techniques. We further investigate the root\ncauses of these access control deficiencies. Validation on 32 devices through\nthe remote testing service confirmed our results. Additionally, we observe that\nthe vulnerable properties and settings occur in devices of the same OEMs. We\nhave reported our findings to the vendors and received positive confirmations.\nOur work underscores the need for greater scrutiny of covert access channels to\ndevice identifiers and better solutions to safeguard user privacy."
    },
    {
        "date": "2025-02",
        "title": "Interpreting Adversarial Attacks and Defences using Architectures with Enhanced Interpretability",
        "author": "Akshay G Rao, Chandrashekhar Lakshminarayanan, and Arun Rajkumar",
        "link": "http://arxiv.org/abs/2502.15017v1",
        "abstract": "Adversarial attacks in deep learning represent a significant threat to the\nintegrity and reliability of machine learning models. Adversarial training has\nbeen a popular defence technique against these adversarial attacks. In this\nwork, we capitalize on a network architecture, namely Deep Linearly Gated\nNetworks (DLGN), which has better interpretation capabilities than regular deep\nnetwork architectures. Using this architecture, we interpret robust models\ntrained using PGD adversarial training and compare them with standard training.\nFeature networks in DLGN act as feature extractors, making them the only medium\nthrough which an adversary can attack the model. We analyze the feature network\nof DLGN with fully connected layers with respect to properties like alignment\nof the hyperplanes, hyperplane relation with PCA, and sub-network overlap among\nclasses and compare these properties between robust and standard models. We\nalso consider this architecture having CNN layers wherein we qualitatively\n(using visualizations) and quantitatively contrast gating patterns between\nrobust and standard models. We uncover insights into hyperplanes resembling\nprincipal components in PGD-AT and STD-TR models, with PGD-AT hyperplanes\naligned farther from the data points. We use path activity analysis to show\nthat PGD-AT models create diverse, non-overlapping active subnetworks across\nclasses, preventing attack-induced gating overlaps. Our visualization ideas\nshow the nature of representations learnt by PGD-AT and STD-TR models."
    },
    {
        "date": "2025-02",
        "title": "EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models",
        "author": "Nastaran Darabi, Devashri Naik, Sina Tayebati, Dinithi Jayasuriya, Ranganath Krishnan, and Amit Ranjan Trivedi",
        "link": "http://arxiv.org/abs/2502.14976v1",
        "abstract": "Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large\nLanguage Models (LLMs), which are further exacerbated by their multimodal\nnature. Existing defenses, including adversarial training, input\ntransformations, and heuristic detection, are computationally expensive,\narchitecture-dependent, and fragile against adaptive attacks. We introduce\nEigenShield, an inference-time defense leveraging Random Matrix Theory to\nquantify adversarial disruptions in high-dimensional VLM representations.\nUnlike prior methods that rely on empirical heuristics, EigenShield employs the\nspiked covariance model to detect structured spectral deviations. Using a\nRobustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it\nseparates causal eigenvectors, which encode semantic information, from\ncorrelational eigenvectors that are susceptible to adversarial artifacts. By\nprojecting embeddings onto the causal subspace, EigenShield filters adversarial\nnoise without modifying model parameters or requiring adversarial training.\nThis architecture-independent, attack-agnostic approach significantly reduces\nthe attack success rate, establishing spectral analysis as a principled\nalternative to conventional defenses. Our results demonstrate that EigenShield\nconsistently outperforms all existing defenses, including adversarial training,\nUNIGUARD, and CIDER."
    },
    {
        "date": "2025-02",
        "title": "CyberSentinel: An Emergent Threat Detection System for AI Security",
        "author": "Krti Tallam",
        "link": "http://arxiv.org/abs/2502.14966v1",
        "abstract": "The rapid advancement of artificial intelligence (AI) has significantly\nexpanded the attack surface for AI-driven cybersecurity threats, necessitating\nadaptive defense strategies. This paper introduces CyberSentinel, a unified,\nsingle-agent system for emergent threat detection, designed to identify and\nmitigate novel security risks in real time. CyberSentinel integrates: (1)\nBrute-force attack detection through SSH log analysis, (2) Phishing threat\nassessment using domain blacklists and heuristic URL scoring, and (3) Emergent\nthreat detection via machine learning-based anomaly detection. By continuously\nadapting to evolving adversarial tactics, CyberSentinel strengthens proactive\ncybersecurity defense, addressing critical vulnerabilities in AI security."
    },
    {
        "date": "2025-02",
        "title": "Red-Teaming LLM Multi-Agent Systems via Communication Attacks",
        "author": "Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, and Hui Liu",
        "link": "http://arxiv.org/abs/2502.14847v1",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized\ncomplex problem-solving capability by enabling sophisticated agent\ncollaboration through message-based communications. While the communication\nframework is crucial for agent coordination, it also introduces a critical yet\nunexplored security vulnerability. In this work, we introduce\nAgent-in-the-Middle (AiTM), a novel attack that exploits the fundamental\ncommunication mechanisms in LLM-MAS by intercepting and manipulating\ninter-agent messages. Unlike existing attacks that compromise individual\nagents, AiTM demonstrates how an adversary can compromise entire multi-agent\nsystems by only manipulating the messages passing between agents. To enable the\nattack under the challenges of limited control and role-restricted\ncommunication format, we develop an LLM-powered adversarial agent with a\nreflection mechanism that generates contextually-aware malicious instructions.\nOur comprehensive evaluation across various frameworks, communication\nstructures, and real-world applications demonstrates that LLM-MAS is vulnerable\nto communication-based attacks, highlighting the need for robust security\nmeasures in multi-agent systems."
    },
    {
        "date": "2025-02",
        "title": "Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide",
        "author": "Xingyu Zhao",
        "link": "http://arxiv.org/abs/2502.14833v1",
        "abstract": "Deep learning (DL) has demonstrated significant potential across various\nsafety-critical applications, yet ensuring its robustness remains a key\nchallenge. While adversarial robustness has been extensively studied in\nworst-case scenarios, probabilistic robustness (PR) offers a more practical\nperspective by quantifying the likelihood of failures under stochastic\nperturbations. This paper provides a concise yet comprehensive overview of PR,\ncovering its formal definitions, evaluation and enhancement methods. We\nintroduce a reformulated ''min-max'' optimisation framework for adversarial\ntraining specifically designed to improve PR. Furthermore, we explore the\nintegration of PR verification evidence into system-level safety assurance,\naddressing challenges in translating DL model-level robustness to system-level\nclaims. Finally, we highlight open research questions, including benchmarking\nPR evaluation methods, extending PR to generative AI tasks, and developing\nrigorous methodologies and case studies for system-level integration."
    },
    {
        "date": "2025-02",
        "title": "An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces",
        "author": "Alexander Terenin, and Jeffrey Negrea",
        "link": "http://arxiv.org/abs/2502.14790v3",
        "abstract": "We develop an analysis of Thompson sampling for online learning under full\nfeedback - also known as prediction with expert advice - where the learner's\nprior is defined over the space of an adversary's future actions, rather than\nthe space of experts. We show regret decomposes into regret the learner\nexpected a priori, plus a prior-robustness-type term we call excess regret. In\nthe classical finite-expert setting, this recovers optimal rates. As an initial\nstep towards practical online learning in settings with a\npotentially-uncountably-infinite number of experts, we show that Thompson\nsampling with a certain Gaussian process prior widely-used in the Bayesian\noptimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate\nagainst a $\\beta$-bounded $\\lambda$-Lipschitz adversary."
    },
    {
        "date": "2025-02",
        "title": "Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination",
        "author": "Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, and Thanasis Pittas",
        "link": "http://arxiv.org/abs/2502.14772v1",
        "abstract": "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings."
    },
    {
        "date": "2025-02",
        "title": "Moshi Moshi? A Model Selection Hijacking Adversarial Attack",
        "author": "Riccardo Petrucci, Luca Pajola, Francesco Marchiori, Luca Pasa, and Mauro conti",
        "link": "http://arxiv.org/abs/2502.14586v1",
        "abstract": "Model selection is a fundamental task in Machine Learning~(ML), focusing on\nselecting the most suitable model from a pool of candidates by evaluating their\nperformance on specific metrics. This process ensures optimal performance,\ncomputational efficiency, and adaptability to diverse tasks and environments.\nDespite its critical role, its security from the perspective of adversarial ML\nremains unexplored. This risk is heightened in the\nMachine-Learning-as-a-Service model, where users delegate the training phase\nand the model selection process to third-party providers, supplying data and\ntraining strategies. Therefore, attacks on model selection could harm both the\nuser and the provider, undermining model performance and driving up operational\ncosts.\n  In this work, we present MOSHI (MOdel Selection HIjacking adversarial\nattack), the first adversarial attack specifically targeting model selection.\nOur novel approach manipulates model selection data to favor the adversary,\neven without prior knowledge of the system. Utilizing a framework based on\nVariational Auto Encoders, we provide evidence that an attacker can induce\ninefficiencies in ML deployment. We test our attack on diverse computer vision\nand speech recognition benchmark tasks and different settings, obtaining an\naverage attack success rate of 75.42%. In particular, our attack causes an\naverage 88.30% decrease in generalization capabilities, an 83.33% increase in\nlatency, and an increase of up to 105.85% in energy consumption. These results\nhighlight the significant vulnerabilities in model selection processes and\ntheir potential impact on real-world applications."
    },
    {
        "date": "2025-02",
        "title": "Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining",
        "author": "Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, and Sunghoon Im",
        "link": "http://arxiv.org/abs/2502.14573v1",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) aims to predict the dense\ndepth map of a monocular image, by learning depth from RGB image sequences,\neliminating the need for ground-truth depth labels. Although this approach\nsimplifies data acquisition compared to supervised methods, it struggles with\nreflective surfaces, as they violate the assumptions of Lambertian reflectance,\nleading to inaccurate training on such surfaces. To tackle this problem, we\npropose a novel training strategy for an SSMDE by leveraging triplet mining to\npinpoint reflective regions at the pixel level, guided by the camera geometry\nbetween different viewpoints. The proposed reflection-aware triplet mining loss\nspecifically penalizes the inappropriate photometric error minimization on the\nlocalized reflective regions while preserving depth accuracy in non-reflective\nareas. We also incorporate a reflection-aware knowledge distillation method\nthat enables a student model to selectively learn the pixel-level knowledge\nfrom reflective and non-reflective regions. This results in robust depth\nestimation across areas. Evaluation results on multiple datasets demonstrate\nthat our method effectively enhances depth quality on reflective surfaces and\noutperforms state-of-the-art SSMDE baselines."
    },
    {
        "date": "2025-02",
        "title": "FUIA: Model Inversion Attack against Federated Unlearning",
        "author": "Lei Zhou, and Youwen Zhu",
        "link": "http://arxiv.org/abs/2502.14558v1",
        "abstract": "With the introduction of regulations related to the ``right to be forgotten\",\nfederated learning (FL) is facing new privacy compliance challenges. To address\nthese challenges, researchers have proposed federated unlearning (FU). However,\nexisting FU research has primarily focused on improving the efficiency of\nunlearning, with less attention paid to the potential privacy vulnerabilities\ninherent in these methods. To address this gap, we draw inspiration from\ngradient inversion attacks in FL and propose the federated unlearning inversion\nattack (FUIA). The FUIA is specifically designed for the three types of FU\n(sample unlearning, client unlearning, and class unlearning), aiming to provide\na comprehensive analysis of the privacy leakage risks associated with FU. In\nFUIA, the server acts as an honest-but-curious attacker, recording and\nexploiting the model differences before and after unlearning to expose the\nfeatures and labels of forgotten data. FUIA significantly leaks the privacy of\nforgotten data and can target all types of FU. This attack contradicts the goal\nof FU to eliminate specific data influence, instead exploiting its\nvulnerabilities to recover forgotten data and expose its privacy flaws.\nExtensive experimental results show that FUIA can effectively reveal the\nprivate information of forgotten data. To mitigate this privacy leakage, we\nalso explore two potential defense methods, although these come at the cost of\nreduced unlearning effectiveness and the usability of the unlearned model."
    },
    {
        "date": "2025-02",
        "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
        "author": "Haokun Chen, Sebastian Szyller, Weilin Xu, and Nageen Himayat",
        "link": "http://arxiv.org/abs/2502.15836v1",
        "abstract": "Large language models (LLMs) have become increasingly popular. Their emergent\ncapabilities can be attributed to their massive training datasets. However,\nthese datasets often contain undesirable or inappropriate content, e.g.,\nharmful texts, personal information, and copyrighted material. This has\npromoted research into machine unlearning that aims to remove information from\ntrained models. In particular, approximate unlearning seeks to achieve\ninformation removal by strategically editing the model rather than complete\nmodel retraining.\n  Recent work has shown that soft token attacks (STA) can successfully extract\npurportedly unlearned information from LLMs, thereby exposing limitations in\ncurrent unlearning methodologies. In this work, we reveal that STAs are an\ninadequate tool for auditing unlearning. Through systematic evaluation on\ncommon unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate\nthat such attacks can elicit any information from the LLM, regardless of (1)\nthe deployed unlearning algorithm, and (2) whether the queried content was\noriginally present in the training corpus. Furthermore, we show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400-characters\nlong. Thus showing that STAs are too powerful, and misrepresent the\neffectiveness of the unlearning methods.\n  Our work highlights the need for better evaluation baselines, and more\nappropriate auditing tools for assessing the effectiveness of unlearning in\nLLMs."
    },
    {
        "date": "2025-02",
        "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
        "author": "Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, and Qing Guo",
        "link": "http://arxiv.org/abs/2502.14529v1",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba."
    },
    {
        "date": "2025-02",
        "title": "Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation",
        "author": "Austin A. Barr, Robert Rozman, and Eddie Guo",
        "link": "http://arxiv.org/abs/2502.14523v1",
        "abstract": "We propose a new framework for zero-shot generation of synthetic tabular\ndata. Using the large language model (LLM) GPT-4o and plain-language prompting,\nwe demonstrate the ability to generate high-fidelity tabular data without\ntask-specific fine-tuning or access to real-world data (RWD) for pre-training.\nTo benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated\nsynthetic data against data generated with the conditional tabular generative\nadversarial network (CTGAN), across three open-access datasets: Iris, Fish\nMeasurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o\noutperformed CTGAN in preserving means, 95% confidence intervals, bivariate\ncorrelations, and data privacy of RWD, even at amplified sample sizes. Notably,\ncorrelations between parameters were consistently preserved with appropriate\ndirection and strength. However, refinement is necessary to better retain\ndistributional characteristics. These findings highlight the potential of LLMs\nin tabular data synthesis, offering an accessible alternative to generative\nadversarial networks and variational autoencoders."
    },
    {
        "date": "2025-02",
        "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
        "author": "Zhuohang Long, Siyuan Wang, Shujun Liu, Yuhang Lai, Xuanjing Huang, and Zhongyu Wei",
        "link": "http://arxiv.org/abs/2502.14486v1",
        "abstract": "Jailbreak attacks, where harmful prompts bypass generative models' built-in\nsafety, raise serious concerns about model vulnerability. While many defense\nmethods have been proposed, the trade-offs between safety and helpfulness, and\ntheir application to Large Vision-Language Models (LVLMs), are not well\nunderstood. This paper systematically examines jailbreak defenses by reframing\nthe standard generation task as a binary classification problem to assess model\nrefusal tendencies for both harmful and benign queries. We identify two key\ndefense mechanisms: safety shift, which increases refusal rates across all\nqueries, and harmfulness discrimination, which improves the model's ability to\ndistinguish between harmful and benign inputs. Using these mechanisms, we\ndevelop two ensemble defense strategies-inter-mechanism ensembles and\nintra-mechanism ensembles-to balance safety and helpfulness. Experiments on the\nMM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these\nstrategies effectively improve model safety or optimize the trade-off between\nsafety and helpfulness."
    },
    {
        "date": "2025-02",
        "title": "Generalization Certificates for Adversarially Robust Bayesian Linear Regression",
        "author": "Mahalakshmi Sabanayagam, Russell Tsuchida, Cheng Soon Ong, and Debarghya Ghoshdastidar",
        "link": "http://arxiv.org/abs/2502.14298v1",
        "abstract": "Adversarial robustness of machine learning models is critical to ensuring\nreliable performance under data perturbations. Recent progress has been on\npoint estimators, and this paper considers distributional predictors. First,\nusing the link between exponential families and Bregman divergences, we\nformulate an adversarial Bregman divergence loss as an adversarial negative\nlog-likelihood. Using the geometric properties of Bregman divergences, we\ncompute the adversarial perturbation for such models in closed-form. Second,\nunder such losses, we introduce \\emph{adversarially robust posteriors}, by\nexploiting the optimization-centric view of generalized Bayesian inference.\nThird, we derive the \\emph{first} rigorous generalization certificates in the\ncontext of an adversarial extension of Bayesian linear regression by leveraging\nthe PAC-Bayesian framework. Finally, experiments on real and synthetic datasets\ndemonstrate the superior robustness of the derived adversarially robust\nposterior over Bayes posterior, and also validate our theoretical guarantees."
    },
    {
        "date": "2025-02",
        "title": "Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning",
        "author": "Ye Liu, Yuqing Niu, Chengyan Ma, Ruidong Han, Wei Ma, Yi Li, Debin Gao, and David Lo",
        "link": "http://arxiv.org/abs/2502.14215v1",
        "abstract": "Smart contracts are highly susceptible to manipulation attacks due to the\nleakage of sensitive information. Addressing manipulation vulnerabilities is\nparticularly challenging because they stem from inherent data confidentiality\nissues rather than straightforward implementation bugs. To tackle this by\npreventing sensitive information leakage, we present PartitionGPT, the first\nLLM-driven approach that combines static analysis with the in-context learning\ncapabilities of large language models (LLMs) to partition smart contracts into\nprivileged and normal codebases, guided by a few annotated sensitive data\nvariables. We evaluated PartitionGPT on 18 annotated smart contracts containing\n99 sensitive functions. The results demonstrate that PartitionGPT successfully\ngenerates compilable, and verified partitions for 78% of the sensitive\nfunctions while reducing approximately 30% code compared to function-level\npartitioning approach. Furthermore, we evaluated PartitionGPT on nine\nreal-world manipulation attacks that lead to a total loss of 25 million\ndollars, PartitionGPT effectively prevents eight cases, highlighting its\npotential for broad applicability and the necessity for secure program\npartitioning during smart contract development to diminish manipulation\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization",
        "author": "Keane Ong, Rui Mao, Deeksha Varshney, Erik Cambria, and Gianmarco Mengaldo",
        "link": "http://arxiv.org/abs/2502.15821v1",
        "abstract": "Sustainability reports are key for evaluating companies' environmental,\nsocial and governance, ESG performance, but their content is increasingly\nobscured by greenwashing - sustainability claims that are misleading,\nexaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack\nrobustness against greenwashing risks, often extracting insights that reflect\nmisleading or exaggerated sustainability claims rather than objective ESG\nperformance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis\nwith Cross-Category Generalization, as a novel dataset to improve the\nrobustness of ESG analysis amid the prevalence of greenwashing. By explicitly\nlinking sustainability aspects with their associated actions, A3CG facilitates\na more fine-grained and transparent evaluation of sustainability claims,\nensuring that insights are grounded in verifiable actions rather than vague or\nmisleading rhetoric. Additionally, A3CG emphasizes cross-category\ngeneralization. This ensures robust model performance in aspect-action analysis\neven when companies change their reports to selectively favor certain\nsustainability areas. Through experiments on A3CG, we analyze state-of-the-art\nsupervised models and LLMs, uncovering their limitations and outlining key\ndirections for future research."
    },
    {
        "date": "2025-02",
        "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
        "author": "Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, and Preetha Chatterjee",
        "link": "http://arxiv.org/abs/2502.14202v1",
        "abstract": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses."
    },
    {
        "date": "2025-02",
        "title": "Conformal Prediction under L\u00e9vy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations",
        "author": "Liviu Aolaritei, Michael I. Jordan, Youssef Marzouk, Zheyu Oliver Wang, and Julie Zhu",
        "link": "http://arxiv.org/abs/2502.14105v1",
        "abstract": "Conformal prediction provides a powerful framework for constructing\nprediction intervals with finite-sample guarantees, yet its robustness under\ndistribution shifts remains a significant challenge. This paper addresses this\nlimitation by modeling distribution shifts using L\\'evy-Prokhorov (LP)\nambiguity sets, which capture both local and global perturbations. We provide a\nself-contained overview of LP ambiguity sets and their connections to popular\nmetrics such as Wasserstein and Total Variation. We show that the link between\nconformal prediction and LP ambiguity sets is a natural one: by propagating the\nLP ambiguity set through the scoring function, we reduce complex\nhigh-dimensional distribution shifts to manageable one-dimensional distribution\nshifts, enabling exact quantification of worst-case quantiles and coverage.\nBuilding on this analysis, we construct robust conformal prediction intervals\nthat remain valid under distribution shifts, explicitly linking LP parameters\nto interval width and confidence levels. Experimental results on real-world\ndatasets demonstrate the effectiveness of the proposed approach."
    },
    {
        "date": "2025-02",
        "title": "Cyber security of OT networks: A tutorial and overview",
        "author": "Sumit Kumar, and Harsh Vardhan",
        "link": "http://arxiv.org/abs/2502.14017v1",
        "abstract": "This manuscript explores the cybersecurity challenges of Operational\nTechnology (OT) networks, focusing on their critical role in industrial\nenvironments such as manufacturing, energy, and utilities. As OT systems\nincreasingly integrate with Information Technology (IT) systems due to Industry\n4.0 initiatives, they become more vulnerable to cyberattacks, which pose risks\nnot only to data but also to physical infrastructure. The study examines key\ncomponents of OT systems, such as SCADA (Supervisory Control and Data\nAcquisition), PLCs (Programmable Logic Controllers), and RTUs (Remote Terminal\nUnits), and analyzes recent cyberattacks targeting OT environments.\nFurthermore, it highlights the security concerns arising from the convergence\nof IT and OT systems, examining attack vectors and the growing threats posed by\nmalware, ransomware, and nation-state actors. Finally, the paper discusses\nmodern approaches and tools used to secure these environments, providing\ninsights into improving the cybersecurity posture of OT networks."
    },
    {
        "date": "2025-02",
        "title": "The Round Complexity of Black-Box Post-Quantum Secure Computation",
        "author": "Rohit Chatterjee, Xiao Liang, Omkant Pandey, and Takashi Yamakawa",
        "link": "http://arxiv.org/abs/2502.13830v1",
        "abstract": "We study the round complexity of secure multi-party computation (MPC) in the\npost-quantum regime. Our focus is on the fully black-box setting, where both\nthe construction and security reduction are black-box. Chia, Chung, Liu, and\nYamakawa [FOCS'22] demonstrated the infeasibility of achieving standard\nsimulation-based security within constant rounds unless $\\mathbf{NP} \\subseteq\n\\mathbf{BQP}$. This leaves crucial feasibility questions unresolved.\nSpecifically, it remains unknown whether black-box constructions are achievable\nwithin polynomial rounds; also, the existence of constant-round constructions\nwith respect to $\\epsilon$-simulation, a relaxed yet useful alternative to\nstandard simulation, remains unestablished.\n  This work provides positive answers. We introduce the first black-box\nconstruction for PQ-MPC in polynomial rounds, from the minimal assumption of\npost-quantum semi-honest oblivious transfers. In the two-party scenario, our\nconstruction requires only $\\omega(1)$ rounds. These results have already been\napplied in the oracle separation between classical-communication quantum MPC\nand $\\mathbf{P} = \\mathbf{NP}$ in Kretschmer, Qian, and Tal [STOC'25].\n  As for $\\epsilon$-simulation, Chia, Chung, Liang, and Yamakawa [CRYPTO'22]\nresolved the issue for the two-party setting, leaving the multi-party case\nopen. We complete the picture by presenting the first black-box, constant-round\nconstruction in the multi-party setting, instantiable using various standard\npost-quantum primitives.\n  En route, we obtain a black-box, constant-round post-quantum commitment\nachieving a weaker version of 1-many non-malleability, from post-quantum\none-way functions. Besides its role in our MPC construction, this commitment\nalso reduces the assumption used in the quantum parallel repetition lower bound\nby Bostanci, Qian, Spooner, and Yuen [STOC'24]. We anticipate further\napplications in the future."
    },
    {
        "date": "2025-02",
        "title": "Display Field-Of-View Agnostic Robust CT Kernel Synthesis Using Model-Based Deep Learning",
        "author": "Hemant Kumar Aggarwal, Antony Jerald, Phaneendra K. Yalavarthy, Rajesh Langoju, and Bipul Das",
        "link": "http://arxiv.org/abs/2502.14920v1",
        "abstract": "In X-ray computed tomography (CT) imaging, the choice of reconstruction\nkernel is crucial as it significantly impacts the quality of clinical images.\nDifferent kernels influence spatial resolution, image noise, and contrast in\nvarious ways. Clinical applications involving lung imaging often require images\nreconstructed with both soft and sharp kernels. The reconstruction of images\nwith different kernels requires raw sinogram data and storing images for all\nkernels increases processing time and storage requirements. The Display\nField-of-View (DFOV) adds complexity to kernel synthesis, as data acquired at\ndifferent DFOVs exhibit varying levels of sharpness and details. This work\nintroduces an efficient, DFOV-agnostic solution for image-based kernel\nsynthesis using model-based deep learning. The proposed method explicitly\nintegrates CT kernel and DFOV characteristics into the forward model.\nExperimental results on clinical data, along with quantitative analysis of the\nestimated modulation transfer function using wire phantom data, clearly\ndemonstrate the utility of the proposed method in real-time. Additionally, a\ncomparative study with a direct learning network, that lacks forward model\ninformation, shows that the proposed method is more robust to DFOV variations."
    },
    {
        "date": "2025-02",
        "title": "RobustX: Robust Counterfactual Explanations Made Easy",
        "author": "Junqi Jiang, Luca Marzari, Aaryan Purohit, and Francesco Leofante",
        "link": "http://arxiv.org/abs/2502.13751v1",
        "abstract": "The increasing use of Machine Learning (ML) models to aid decision-making in\nhigh-stakes industries demands explainability to facilitate trust.\nCounterfactual Explanations (CEs) are ideally suited for this, as they can\noffer insights into the predictions of an ML model by illustrating how changes\nin its input data may lead to different outcomes. However, for CEs to realise\ntheir explanatory potential, significant challenges remain in ensuring their\nrobustness under slight changes in the scenario being explained. Despite the\nwidespread recognition of CEs' robustness as a fundamental requirement, a lack\nof standardised tools and benchmarks hinders a comprehensive and effective\ncomparison of robust CE generation methods. In this paper, we introduce\nRobustX, an open-source Python library implementing a collection of CE\ngeneration and evaluation methods, with a focus on the robustness property.\nRobustX provides interfaces to several existing methods from the literature,\nenabling streamlined access to state-of-the-art techniques. The library is also\neasily extensible, allowing fast prototyping of novel robust CE generation and\nevaluation methods."
    },
    {
        "date": "2025-02",
        "title": "Robust Counterfactual Inference in Markov Decision Processes",
        "author": "Jessica Lally, Milad Kazemi, and Nicola Paoletti",
        "link": "http://arxiv.org/abs/2502.13731v1",
        "abstract": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
    },
    {
        "date": "2025-02",
        "title": "Secure Federated Data Distillation",
        "author": "Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, and Antonino Nocera",
        "link": "http://arxiv.org/abs/2502.13728v1",
        "abstract": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications."
    },
    {
        "date": "2025-02",
        "title": "What Skills Do Cyber Security Professionals Need?",
        "author": "Faheem Ullah, Xiaohan Ye, Uswa Fatima, Zahid Akhtar, Yuxi Wu, and Hussain Ahmad",
        "link": "http://arxiv.org/abs/2502.13658v2",
        "abstract": "Purpose: The increasing number of cyber-attacks has elevated the importance\nof cybersecurity for organizations. This has also increased the demand for\nprofessionals with the necessary skills to protect these organizations. As a\nresult, many individuals are looking to enter the field of cybersecurity.\nHowever, there is a lack of clear understanding of the skills required for a\nsuccessful career in this field. In this paper, we identify the skills required\nfor cybersecurity professionals. We also determine how the demand for cyber\nskills relates to various cyber roles such as security analyst and security\narchitect. Furthermore, we identify the programming languages that are\nimportant for cybersecurity professionals. Design/Methodology: For this study,\nwe have collected and analyzed data from 12,161 job ads and 49,002 Stack\nOverflow posts. By examining this, we identified patterns and trends related to\nskill requirements, role-specific demands, and programming languages in\ncybersecurity. Findings: Our results reveal that (i) communication skills and\nproject management skills are the most important soft skills, (ii) as compared\nto soft skills, the demand for technical skills varies more across various\ncyber roles, and (iii) Java is the most commonly used programming language.\nOriginality: Our findings serve as a guideline for individuals aiming to get\ninto the field of cybersecurity. Moreover, our findings are useful in terms of\ninforming educational institutes to teach the correct set of skills to students\ndoing degrees in cybersecurity."
    },
    {
        "date": "2025-02",
        "title": "Toward Robust Non-Transferable Learning: A Survey and Benchmark",
        "author": "Ziming Hong, Yongli Xiang, and Tongliang Liu",
        "link": "http://arxiv.org/abs/2502.13593v1",
        "abstract": "Over the past decades, researchers have primarily focused on improving the\ngeneralization abilities of models, with limited attention given to regulating\nsuch generalization. However, the ability of models to generalize to unintended\ndata (e.g., harmful or unauthorized data) can be exploited by malicious\nadversaries in unforeseen ways, potentially resulting in violations of model\nethics. Non-transferable learning (NTL), a task aimed at reshaping the\ngeneralization abilities of deep learning models, was proposed to address these\nchallenges. While numerous methods have been proposed in this field, a\ncomprehensive review of existing progress and a thorough analysis of current\nlimitations remain lacking. In this paper, we bridge this gap by presenting the\nfirst comprehensive survey on NTL and introducing NTLBench, the first benchmark\nto evaluate NTL performance and robustness within a unified framework.\nSpecifically, we first introduce the task settings, general framework, and\ncriteria of NTL, followed by a summary of NTL approaches. Furthermore, we\nemphasize the often-overlooked issue of robustness against various attacks that\ncan destroy the non-transferable mechanism established by NTL. Experiments\nconducted via NTLBench verify the limitations of existing NTL methods in\nrobustness. Finally, we discuss the practical applications of NTL, along with\nits future directions and associated challenges."
    },
    {
        "date": "2025-02",
        "title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking",
        "author": "Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, and Lei Zou",
        "link": "http://arxiv.org/abs/2502.13527v1",
        "abstract": "The rise of Large Language Models (LLMs) has led to significant applications\nbut also introduced serious security threats, particularly from jailbreak\nattacks that manipulate output generation. These attacks utilize prompt\nengineering and logit manipulation to steer models toward harmful content,\nprompting LLM providers to implement filtering and safety alignment strategies.\nWe investigate LLMs' safety mechanisms and their recent applications, revealing\na new threat model targeting structured output interfaces, which enable\nattackers to manipulate the inner logit during LLM generation, requiring only\nAPI access permissions. To demonstrate this threat model, we introduce a\nblack-box attack framework called AttackPrefixTree (APT). APT exploits\nstructured output interfaces to dynamically construct attack patterns. By\nleveraging prefixes of models' safety refusal response and latent harmful\noutputs, APT effectively bypasses safety measures. Experiments on benchmark\ndatasets indicate that this approach achieves higher attack success rate than\nexisting methods. This work highlights the urgent need for LLM providers to\nenhance security protocols to address vulnerabilities arising from the\ninteraction between safety patterns and structured outputs."
    },
    {
        "date": "2025-02",
        "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
        "author": "Shangyu Wu, Hongchao Du, Ying Xiong, Shuai Chen, Tei-wei Kuo, Nan Guan, and Chun Jason Xue",
        "link": "http://arxiv.org/abs/2502.14910v1",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing\nstructured pruning methods address this issue by removing redundant structures\n(e.g., elements, channels, layers) from the model. However, these methods\nemploy a heuristic pruning strategy, which leads to suboptimal performance.\nBesides, they also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting structured pruning techniques, EvoP achieves the best performance\nwhile maintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications."
    },
    {
        "date": "2025-02",
        "title": "JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework",
        "author": "Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, and Yuantao Gu",
        "link": "http://arxiv.org/abs/2502.13407v1",
        "abstract": "Deep learning has achieved significant success in the field of remote sensing\nimage change detection (CD), yet two major challenges remain: the scarcity of\nsub-meter, all-inclusive open-source CD datasets, and the difficulty of\nachieving consistent and satisfactory detection results across images with\nvarying change areas. To address these issues, we introduce the JL1-CD dataset,\nwhich contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5\nto 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation\n(MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD\ndatasets demonstrate that the MTKD framework significantly improves the\nperformance of CD models with various network architectures and parameter\nsizes, achieving new state-of-the-art results. The code is available at\nhttps://github.com/circleLZY/MTKD-CD."
    },
    {
        "date": "2025-02",
        "title": "CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks",
        "author": "Ke Jiang, Sen Deng, Yinshuai Li, Shuai Wang, Tianwei Zhang, and Yinqian Zhang",
        "link": "http://arxiv.org/abs/2502.13401v1",
        "abstract": "Cryptographic implementations bolster security against timing side-channel\nattacks by integrating constant-time components. However, the new ciphertext\nside channels resulting from the deterministic memory encryption in Trusted\nExecution Environments (TEEs), enable ciphertexts to manifest identifiable\npatterns when being sequentially written to the same memory address. Attackers\nwith read access to encrypted memory in TEEs can potentially deduce plaintexts\nby analyzing these changing ciphertext patterns.\n  In this paper, we design CipherGuard, a compiler-aided mitigation methodology\nto counteract ciphertext side channels with high efficiency and security.\nCipherGuard is based on the LLVM ecosystem, and encompasses multiple mitigation\nstrategies, including software-based probabilistic encryption and secret-aware\nregister allocation. Through a comprehensive evaluation, we demonstrate that\nCipherGuard can strengthen the security of various cryptographic\nimplementations more efficiently than existing state-of-the-art defense\nmechanism, i.e., CipherFix."
    },
    {
        "date": "2025-02",
        "title": "KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models",
        "author": "Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, and Kathleen M. Carley",
        "link": "http://arxiv.org/abs/2502.14908v1",
        "abstract": "The robustness of large language models (LLMs) against knowledge conflicts in\nunimodal question answering systems has been well studied. However, the effect\nof conflicts in information sources on vision language models (VLMs) in\nmultimodal settings has not yet been explored. In this work, we propose\n\\segsub, a framework that applies targeted perturbations to image sources to\nstudy and improve the robustness of VLMs against three different types of\nknowledge conflicts, namely parametric, source, and counterfactual conflicts.\nContrary to prior findings that showed that LLMs are sensitive to parametric\nconflicts arising from textual perturbations, we find VLMs are largely robust\nto image perturbation. On the other hand, VLMs perform poorly on counterfactual\nexamples (<30% accuracy) and fail to reason over source conflicts (<1%\naccuracy). We also find a link between hallucinations and image context, with\nGPT-4o prone to hallucination when presented with highly contextualized\ncounterfactual examples. While challenges persist with source conflicts,\nfinetuning models significantly improves reasoning over counterfactual samples.\nOur findings highlight the need for VLM training methodologies that enhance\ntheir reasoning capabilities, particularly in addressing complex knowledge\nconflicts between multimodal sources."
    },
    {
        "date": "2025-02",
        "title": "Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios",
        "author": "Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, and Qi Wu",
        "link": "http://arxiv.org/abs/2502.13345v1",
        "abstract": "Latent diffusion models have exhibited considerable potential in generative\ntasks. Watermarking is considered to be an alternative to safeguard the\ncopyright of generative models and prevent their misuse. However, in the\ncontext of model distribution scenarios, the accessibility of models to large\nscale of model users brings new challenges to the security, efficiency and\nrobustness of existing watermark solutions. To address these issues, we propose\na secure and efficient watermarking solution. A new security mechanism is\ndesigned to prevent watermark leakage and watermark escape, which considers\nwatermark randomness and watermark-model association as two constraints for\nmandatory watermark injection. To reduce the time cost of training the security\nmodule, watermark injection and the security mechanism are decoupled, ensuring\nthat fine-tuning VAE only accomplishes the security mechanism without the\nburden of learning watermark patterns. A watermark distribution-based\nverification strategy is proposed to enhance the robustness against diverse\nattacks in the model distribution scenarios. Experimental results prove that\nour watermarking consistently outperforms existing six baselines on\neffectiveness and robustness against ten image processing attacks and\nadversarial attacks, while enhancing security in the distribution scenarios."
    },
    {
        "date": "2025-02",
        "title": "Pruning as a Defense: Reducing Memorization in Large Language Models",
        "author": "Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, and Sanjif Shanmugavelu",
        "link": "http://arxiv.org/abs/2502.15796v1",
        "abstract": "Large language models have been shown to memorize significant portions of\ntheir training data, which they can reproduce when appropriately prompted. This\nwork investigates the impact of simple pruning techniques on this behavior. Our\nfindings reveal that pruning effectively reduces the extent of memorization in\nLLMs, demonstrating its potential as a foundational approach for mitigating\nmembership inference attacks."
    },
    {
        "date": "2025-02",
        "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models",
        "author": "Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, and Weijie Zhao",
        "link": "http://arxiv.org/abs/2502.13141v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs."
    },
    {
        "date": "2025-02",
        "title": "The Role of GitHub Copilot on Software Development: A Perspec-tive on Productivity, Security, Best Practices and Future Directions",
        "author": "Suresh Babu Nettur, Shanthi Karpurapu, Unnati Nettur, Likhit Sagar Gajja, Sravanthy Myneni, and Akhil Dusi",
        "link": "http://arxiv.org/abs/2502.13199v1",
        "abstract": "GitHub Copilot is transforming software development by automating tasks and\nboosting productivity through AI-driven code generation. In this paper, we\ncon-duct a literature survey to synthesize insights on Copilot's impact on\nproductivity and security. We review academic journal databases, industry\nreports, and official docu-mentation to highlight key findings and challenges.\nWhile Copilot accelerates coding and prototyping, concerns over security\nvulnerabilities and intellectual property risks persist. Drawing from the\nliterature, we provide a perspective on best practices and future directions\nfor responsible AI adoption in software engineering, offering action-able\ninsights for developers and organizations to integrate Copilot effectively\nwhile maintaining high standards of quality and security."
    },
    {
        "date": "2025-02",
        "title": "RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection",
        "author": "Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, and Ming-Hsuan Yang",
        "link": "http://arxiv.org/abs/2502.13071v1",
        "abstract": "While recent low-cost radar-camera approaches have shown promising results in\nmulti-modal 3D object detection, both sensors face challenges from\nenvironmental and intrinsic disturbances. Poor lighting or adverse weather\nconditions degrade camera performance, while radar suffers from noise and\npositional ambiguity. Achieving robust radar-camera 3D object detection\nrequires consistent performance across varying conditions, a topic that has not\nyet been fully explored. In this work, we first conduct a systematic analysis\nof robustness in radar-camera detection on five kinds of noises and propose\nRobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D\nGaussian Expansion (3DGE) module to mitigate inaccuracies in radar points,\nincluding position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS\nand velocity priors to generate a deformable kernel map and variance for kernel\nsize adjustment and value distribution. Additionally, we introduce a\nweather-adaptive fusion module, which adaptively fuses radar and camera\nfeatures based on camera signal confidence. Extensive experiments on the\npopular benchmark, nuScenes, show that our model achieves competitive results\nin regular and noisy conditions."
    },
    {
        "date": "2025-02",
        "title": "Sublinear-Overhead Secure Linear Algebra on a Dishonest Server",
        "author": "Mark Braverman, and Stephen Newman",
        "link": "http://arxiv.org/abs/2502.13060v1",
        "abstract": "Most heavy computation occurs on servers owned by a second party. This\nreduces data privacy, resulting in interest in data-oblivious computation,\nwhich typically severely degrades performance. Secure and fast remote\ncomputation is particularly important for linear algebra, which comprises a\nlarge fraction of total computation and is best run on highly specialized\nhardware often only accessible through the cloud. We state the natural\nefficiency and security desiderata for fast, remote, and data-oblivious linear\nalgebra, conjecture the existence of matrix and vector families implying\nsatisfactory algorithms, and provide such an algorithm contingent on common\ncryptographic assumptions. We achieve sublinear overhead for the server,\ndramatically reduced computation cost for the client, and various other\npractical advantages over previous algorithms.\n  Keywords: Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic\nEncryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN,\nMatrix Multiplication."
    },
    {
        "date": "2025-02",
        "title": "Preventing the Popular Item Embedding Based Attack in Federated Recommendations",
        "author": "Jun Zhang, Huan Li, Dazhong Rong, Yan Zhao, Ke Chen, and Lidan Shou",
        "link": "http://arxiv.org/abs/2502.12958v1",
        "abstract": "Privacy concerns have led to the rise of federated recommender systems (FRS),\nwhich can create personalized models across distributed clients. However, FRS\nis vulnerable to poisoning attacks, where malicious users manipulate gradients\nto promote their target items intentionally. Existing attacks against FRS have\nlimitations, as they depend on specific models and prior knowledge, restricting\ntheir real-world applicability. In our exploration of practical FRS\nvulnerabilities, we devise a model-agnostic and prior-knowledge-free attack,\nnamed PIECK (Popular Item Embedding based Attack). The core module of PIECK is\npopular item mining, which leverages embedding changes during FRS training to\neffectively identify the popular items. Built upon the core module, PIECK\nbranches into two diverse solutions: The PIECKIPE solution employs an item\npopularity enhancement module, which aligns the embeddings of targeted items\nwith the mined popular items to increase item exposure. The PIECKUEA further\nenhances the robustness of the attack by using a user embedding approximation\nmodule, which approximates private user embeddings using mined popular items.\nUpon identifying PIECK, we evaluate existing federated defense methods and find\nthem ineffective against PIECK, as poisonous gradients inevitably overwhelm the\ncold target items. We then propose a novel defense method by introducing two\nregularization terms during user training, which constrain item popularity\nenhancement and user embedding approximation while preserving FRS performance.\nWe evaluate PIECK and its defense across two base models, three real datasets,\nfour top-tier attacks, and six general defense methods, affirming the efficacy\nof both PIECK and its defense."
    },
    {
        "date": "2025-02",
        "title": "Decentralized and Robust Privacy-Preserving Model Using Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises",
        "author": "Reza Fotohi, Fereidoon Shams Aliee, and Bahar Farahani",
        "link": "http://arxiv.org/abs/2502.17485v1",
        "abstract": "In Federated Deep Learning (FDL), multiple local enterprises are allowed to\ntrain a model jointly. Then, they submit their local updates to the central\nserver, and the server aggregates the updates to create a global model.\nHowever, trained models usually perform worse than centralized models,\nespecially when the training data distribution is non-independent and\nidentically distributed (nonIID). NonIID data harms the accuracy and\nperformance of the model. Additionally, due to the centrality of federated\nlearning (FL) and the untrustworthiness of enterprises, traditional FL\nsolutions are vulnerable to security and privacy attacks. To tackle this issue,\nwe propose FedAnil, a secure blockchain enabled Federated Deep Learning Model\nthat improves enterprise models decentralization, performance, and tamper proof\nproperties, incorporating two main phases. The first phase addresses the nonIID\nchallenge (label and feature distribution skew). The second phase addresses\nsecurity and privacy concerns against poisoning and inference attacks through\nthree steps. Extensive experiments were conducted using the Sent140,\nFashionMNIST, FEMNIST, and CIFAR10 new real world datasets to evaluate FedAnils\nrobustness and performance. The simulation results demonstrate that FedAnil\nsatisfies FDL privacy preserving requirements. In terms of convergence\nanalysis, the model parameter obtained with FedAnil converges to the optimum of\nthe model parameter. In addition, it performs better in terms of accuracy (more\nthan 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%)\ncompared with baseline approaches, namely ShieldFL, RVPFL, and RFA."
    },
    {
        "date": "2025-02",
        "title": "Strands Rocq: Why is a Security Protocol Correct, Mechanically?",
        "author": "Matteo Busi, Riccardo Focardi, and Flaminia L. Luccio",
        "link": "http://arxiv.org/abs/2502.12848v1",
        "abstract": "Strand spaces are a formal framework for symbolic protocol verification that\nallows for pen-and-paper proofs of security. While extremely insightful,\npen-and-paper proofs are error-prone, and it is hard to gain confidence on\ntheir correctness. To overcome this problem, we developed StrandsRocq, a full\nmechanization of the strand spaces in Coq (soon to be renamed Rocq). The\nmechanization was designed to be faithful to the original pen-and-paper\ndevelopment, and it was engineered to be modular and extensible. StrandsRocq\nincorporates new original proof techniques, a novel notion of maximal\npenetrator that enables protocol compositionality, and a set of Coq tactics\ntailored to the domain, facilitating proof automation and reuse, and\nsimplifying the work of protocol analysts. To demonstrate the versatility of\nour approach, we modelled and analyzed a family of authentication protocols,\ndrawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical\nNeedham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis\nfor a key management API. The analyses in StrandsRocq confirmed the high degree\nof proof reuse, and enabled us to distill the minimal requirements for protocol\nsecurity. Through mechanization, we identified and addressed several issues in\nthe original proofs and we were able to significantly improve the precision of\nthe static analysis for the key management API. Moreover, we were able to\nleverage the novel notion of maximal penetrator to provide a compositional\nproof of security for two simple authentication protocols."
    },
    {
        "date": "2025-02",
        "title": "Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training",
        "author": "Yuanfan Li, Zhaohan Zhang, Chengzhengxu Li, Chao Shen, and Xiaoming Liu",
        "link": "http://arxiv.org/abs/2502.12734v1",
        "abstract": "Machine-generated Text (MGT) detection is crucial for regulating and\nattributing online texts. While the existing MGT detectors achieve strong\nperformance, they remain vulnerable to simple perturbations and adversarial\nattacks. To build an effective defense against malicious perturbations, we view\nMGT detection from a threat modeling perspective, that is, analyzing the\nmodel's vulnerability from an adversary's point of view and exploring effective\nmitigations. To this end, we introduce an adversarial framework for training a\nrobust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The\nGREATER consists of two key components: an adversary GREATER-A and a detector\nGREATER-D. The GREATER-D learns to defend against the adversarial attack from\nGREATER-A and generalizes the defense to other attacks. GREATER-A identifies\nand perturbs the critical tokens in embedding space, along with greedy search\nand pruning to generate stealthy and disruptive adversarial examples. Besides,\nwe update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D\nto generalize its defense to different attacks and varying attack intensities.\nOur experimental results across 9 text perturbation strategies and 5\nadversarial attacks show that our GREATER-D reduces the Attack Success Rate\n(ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is\ndemonstrated to be more effective and efficient than SOTA attack approaches."
    },
    {
        "date": "2025-02",
        "title": "Chronus: Understanding and Securing the Cutting-Edge Industry Solutions to DRAM Read Disturbance",
        "author": "O\u011fuzhan Canpolat, A. Giray Ya\u011fl\u0131k\u00e7\u0131, Geraldo F. Oliveira, Ataberk Olgun, Nisa Bostanc\u0131, \u0130smail Emir Y\u00fcksel, Haocong Luo, O\u011fuz Ergin, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2502.12650v1",
        "abstract": "We 1) present the first rigorous security, performance, energy, and cost\nanalyses of the state-of-the-art on-DRAM-die read disturbance mitigation\nmethod, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new\nmechanism that addresses PRAC's two major weaknesses. Our analysis shows that\nPRAC's system performance overhead on benign applications is non-negligible for\nmodern DRAM chips and prohibitively large for future DRAM chips that are more\nvulnerable to read disturbance. We identify two weaknesses of PRAC that cause\nthese overheads. First, PRAC increases critical DRAM access latency parameters\ndue to the additional time required to increment activation counters. Second,\nPRAC performs a constant number of preventive refreshes at a time, making it\nvulnerable to an adversarial access pattern, known as the wave attack, and\nconsequently requiring it to be configured for significantly smaller activation\nthresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die\nRowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation\ncounters concurrently while serving accesses by separating counters from the\ndata and 2) prevents the wave attack by dynamically controlling the number of\npreventive refreshes performed. Our performance analysis shows that Chronus's\nsystem performance overhead is near-zero for modern DRAM chips and very low for\nfuture DRAM chips. Chronus outperforms three variants of PRAC and three other\nstate-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's\nimplications for future systems and foreshadow future research directions. To\naid future research, we open-source our Chronus implementation at\nhttps://github.com/CMU-SAFARI/Chronus."
    },
    {
        "date": "2025-02",
        "title": "Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach",
        "author": "Tvrtko Sternak, Davor Runje, Dorian Grano\u0161a, and Chi Wang",
        "link": "http://arxiv.org/abs/2502.12630v1",
        "abstract": "This paper presents a novel approach to evaluating the security of large\nlanguage models (LLMs) against prompt leakage-the exposure of system-level\nprompts or proprietary configurations. We define prompt leakage as a critical\nthreat to secure LLM deployment and introduce a framework for testing the\nrobustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we\nimplement a multi-agent system where cooperative agents are tasked with probing\nand exploiting the target LLM to elicit its prompt.\n  Guided by traditional definitions of security in cryptography, we further\ndefine a prompt leakage-safe system as one in which an attacker cannot\ndistinguish between two agents: one initialized with an original prompt and the\nother with a prompt stripped of all sensitive information. In a safe system,\nthe agents' outputs will be indistinguishable to the attacker, ensuring that\nsensitive information remains secure. This cryptographically inspired framework\nprovides a rigorous standard for evaluating and designing secure LLMs.\n  This work establishes a systematic methodology for adversarial testing of\nprompt leakage, bridging the gap between automated threat modeling and\npractical LLM security.\n  You can find the implementation of our prompt leakage probing on GitHub."
    },
    {
        "date": "2025-02",
        "title": "DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent",
        "author": "Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, and Sen Su",
        "link": "http://arxiv.org/abs/2502.12575v1",
        "abstract": "As LLM-based agents become increasingly prevalent, backdoors can be implanted\ninto agents through user queries or environment feedback, raising critical\nconcerns regarding safety vulnerabilities. However, backdoor attacks are\ntypically detectable by safety audits that analyze the reasoning process of\nagents. To this end, we propose a novel backdoor implantation strategy called\n\\textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}.\nSpecifically, we introduce dynamic encryption, which maps the backdoor into\nbenign content, effectively circumventing safety audits. To enhance\nstealthiness, we further decompose the backdoor into multiple sub-backdoor\nfragments. Based on these advancements, backdoors are allowed to bypass safety\naudits significantly. Additionally, we present AgentBackdoorEval, a dataset\ndesigned for the comprehensive evaluation of agent backdoor attacks.\nExperimental results across multiple datasets demonstrate that our method\nachieves an attack success rate nearing 100\\% while maintaining a detection\nrate of 0\\%, illustrating its effectiveness in evading safety audits. Our\nfindings highlight the limitations of existing safety mechanisms in detecting\nadvanced attacks, underscoring the urgent need for more robust defenses against\nbackdoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent."
    },
    {
        "date": "2025-02",
        "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
        "author": "Wenpeng Xing, Minghao Li, Mohan Li, and Meng Han",
        "link": "http://arxiv.org/abs/2502.13175v2",
        "abstract": "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI."
    },
    {
        "date": "2025-02",
        "title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation",
        "author": "Kounianhua Du, Hanjing Wang, Jianxing Liu, Jizheng Chen, Xinyi Dai, Yasheng Wang, Ruiming Tang, Yong Yu, Jun Wang, and Weinan Zhang",
        "link": "http://arxiv.org/abs/2502.12492v1",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, particularly in system 1 tasks, yet the intricacies of their\nproblem-solving mechanisms in system 2 tasks are not sufficiently explored.\nRecent research on System2-to-System1 methods surge, exploring the System 2\nreasoning knowledge via inference-time computation and compressing the explored\nknowledge into System 1 process. In this paper, we focus on code generation,\nwhich is a representative System 2 task, and identify two primary challenges:\n(1) the complex hidden reasoning processes and (2) the heterogeneous data\ndistributions that complicate the exploration and training of robust LLM\nsolvers. To tackle these issues, we propose a novel BDC framework that explores\ninsightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with\nmutual \\textbf{B}oosting, \\textbf{D}isentangles the heterogeneous training data\nfor composable LoRA-experts, and obtain \\textbf{C}ustomized problem solver for\neach data instance with an input-aware hypernetwork to weight over the\nLoRA-experts, offering effectiveness, flexibility, and robustness. This\nframework leverages multiple LLMs through mutual verification and boosting,\nintegrated into a Monte-Carlo Tree Search process enhanced by reflection-based\npruning and refinement. Additionally, we introduce the DisenLora algorithm,\nwhich clusters heterogeneous data to fine-tune LLMs into composable Lora\nexperts, enabling the adaptive generation of customized problem solvers through\nan input-aware hypernetwork. This work lays the groundwork for advancing LLM\ncapabilities in complex reasoning tasks, offering a novel System2-to-System1\nsolution."
    },
    {
        "date": "2025-02",
        "title": "PKE and ABE with Collusion-Resistant Secure Key Leasing",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2502.12491v2",
        "abstract": "Secure key leasing (SKL) is an advanced encryption functionality that allows\na secret key holder to generate a quantum decryption key and securely lease it\nto a user. Once the user returns the quantum decryption key (or provides a\nclassical certificate confirming its deletion), they lose their decryption\ncapability. Previous works on public key encryption with SKL (PKE-SKL) have\nonly considered the single-key security model, where the adversary receives at\nmost one quantum decryption key. However, this model does not accurately\nreflect real-world applications of PKE-SKL. To address this limitation, we\nintroduce collusion-resistant security for PKE-SKL (denoted as PKE-CR-SKL). In\nthis model, the adversary can adaptively obtain multiple quantum decryption\nkeys and access a verification oracle which validates the correctness of\nqueried quantum decryption keys. Importantly, the size of the public key and\nciphertexts must remain independent of the total number of generated quantum\ndecryption keys. We present the following constructions:\n  - A PKE-CR-SKL scheme based on the learning with errors (LWE) assumption.\n  - An attribute-based encryption scheme with collusion-resistant SKL\n(ABE-CR-SKL), also based on the LWE assumption.\n  - An ABE-CR-SKL scheme with classical certificates, relying on multi-input\nABE with polynomial arity."
    },
    {
        "date": "2025-02",
        "title": "Software Security in Software-Defined Networking: A Systematic Literature Review",
        "author": "Moustapha Awwalou Diouf, Samuel Ouya, Jacques Klein, and Tegawend\u00e9 F. Bissyand\u00e9",
        "link": "http://arxiv.org/abs/2502.13828v1",
        "abstract": "Software-defined networking (SDN) has shifted network management by\ndecoupling the data and control planes. This enables programmatic control via\nsoftware applications using open APIs. SDN's programmability has fueled its\npopularity but may have opened issues extending the attack surface by\nintroducing vulnerable software. Therefore, the research community needs to\nhave a deep and broad understanding of the risks posed by SDN to propose\nmitigating measures. The literature, however, lacks a comprehensive review of\nthe current state of research in this direction. This paper addresses this gap\nby providing a comprehensive overview of the state-of-the-art research in SDN\nsecurity focusing on the software (i.e., the controller, APIs, applications)\npart. We systematically reviewed 58 relevant publications to analyze trends,\nidentify key testing and analysis methodologies, and categorize studied\nvulnerabilities. We further explore areas where the research community can make\nsignificant contributions. This work offers the most extensive and in-depth\nanalysis of SDN software security to date."
    },
    {
        "date": "2025-02",
        "title": "Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning",
        "author": "Mengshi Qi, Changsheng Lv, and Huadong Ma",
        "link": "http://arxiv.org/abs/2502.12425v1",
        "abstract": "In this paper, we propose a new Robust Disentangled Counterfactual Learning\n(RDCL) approach for physical audiovisual commonsense reasoning. The task aims\nto infer objects' physics commonsense based on both video and audio input, with\nthe main challenge being how to imitate the reasoning ability of humans, even\nunder the scenario of missing modalities. Most of the current methods fail to\ntake full advantage of different characteristics in multi-modal data, and\nlacking causal reasoning ability in models impedes the progress of implicit\nphysical knowledge inferring. To address these issues, our proposed RDCL method\ndecouples videos into static (time-invariant) and dynamic (time-varying)\nfactors in the latent space by the disentangled sequential encoder, which\nadopts a variational autoencoder (VAE) to maximize the mutual information with\na contrastive loss function. Furthermore, we introduce a counterfactual\nlearning module to augment the model's reasoning ability by modeling physical\nknowledge relationships among different objects under counterfactual\nintervention. To alleviate the incomplete modality data issue, we introduce a\nrobust multimodal learning method to recover the missing data by decomposing\nthe shared features and model-specific features. Our proposed method is a\nplug-and-play module that can be incorporated into any baseline including VLMs.\nIn experiments, we show that our proposed method improves the reasoning\naccuracy and robustness of baseline methods and achieves the state-of-the-art\nperformance."
    },
    {
        "date": "2025-02",
        "title": "Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness",
        "author": "Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, and Meie Fang",
        "link": "http://arxiv.org/abs/2502.12418v1",
        "abstract": "Color constancy estimates illuminant chromaticity to correct color-biased\nimages. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models\nhave made substantial advancements. Nevertheless, the potential risks in DNNCC\ndue to the vulnerability of deep neural networks have not yet been explored. In\nthis paper, we conduct the first investigation into the impact of a key factor\nin color constancy-brightness-on DNNCC from a robustness perspective. Our\nevaluation reveals that several mainstream DNNCC models exhibit high\nsensitivity to brightness despite their focus on chromaticity estimation. This\nsheds light on a potential limitation of existing DNNCC models: their\nsensitivity to brightness may hinder performance given the widespread\nbrightness variations in real-world datasets. From the insights of our\nanalysis, we propose a simple yet effective brightness robustness enhancement\nstrategy for DNNCC models, termed BRE. The core of BRE is built upon the\nadaptive step-size adversarial brightness augmentation technique, which\nidentifies high-risk brightness variation and generates augmented images via\nexplicit brightness adjustment. Subsequently, BRE develops a\nbrightness-robustness-aware model optimization strategy that integrates\nadversarial brightness training and brightness contrastive loss, significantly\nbolstering the brightness robustness of DNNCC models. BRE is\nhyperparameter-free and can be integrated into existing DNNCC models, without\nincurring additional overhead during the testing phase. Experiments on two\npublic color constancy datasets-ColorChecker and Cube+-demonstrate that the\nproposed BRE consistently enhances the illuminant estimation performance of\nexisting DNNCC models, reducing the estimation error by an average of 5.04%\nacross six mainstream DNNCC models, underscoring the critical role of enhancing\nbrightness robustness in these models."
    },
    {
        "date": "2025-02",
        "title": "Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?",
        "author": "Blaine Hoak, Kunyang Li, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2502.12377v1",
        "abstract": "Representational alignment refers to the extent to which a model's internal\nrepresentations mirror biological vision, offering insights into both neural\nsimilarity and functional correspondence. Recently, some more aligned models\nhave demonstrated higher resiliency to adversarial examples, raising the\nquestion of whether more human-aligned models are inherently more secure. In\nthis work, we conduct a large-scale empirical analysis to systematically\ninvestigate the relationship between representational alignment and adversarial\nrobustness. We evaluate 118 models spanning diverse architectures and training\nparadigms, measuring their neural and behavioral alignment and engineering task\nperformance across 106 benchmarks as well as their adversarial robustness via\nAutoAttack. Our findings reveal that while average alignment and robustness\nexhibit a weak overall correlation, specific alignment benchmarks serve as\nstrong predictors of adversarial robustness, particularly those that measure\nselectivity towards texture or shape. These results suggest that different\nforms of alignment play distinct roles in model robustness, motivating further\ninvestigation into how alignment-driven approaches can be leveraged to build\nmore secure and perceptually-grounded vision models."
    },
    {
        "date": "2025-02",
        "title": "Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV",
        "author": "Allen M. Wang, Alessandro Pau, Cristina Rea, Oswin So, Charles Dawson, Olivier Sauter, Mark D. Boyer, Anna Vu, Cristian Galperti, Chuchu Fan, Antoine Merle, Yoeri Poels, Cristina Venturini, Stefano Marchioni, and the TCV Team",
        "link": "http://arxiv.org/abs/2502.12327v1",
        "abstract": "The rampdown in tokamak operations is a difficult to simulate phase during\nwhich the plasma is often pushed towards multiple instability limits. To\naddress this challenge, and reduce the risk of disrupting operations, we\nleverage recent advances in Scientific Machine Learning (SciML) to develop a\nneural state-space model (NSSM) that predicts plasma dynamics during Tokamak\n\\`a Configuration Variable (TCV) rampdowns. By integrating simple physics\nstructure and data-driven models, the NSSM efficiently learns plasma dynamics\nduring the rampdown from a modest dataset of 311 pulses with only five pulses\nin the reactor relevant high performance regime. The NSSM is parallelized\nacross uncertainties, and reinforcement learning (RL) is applied to design\ntrajectories that avoid multiple instability limits with high probability.\nExperiments at TCV ramping down high performance plasmas show statistically\nsignificant improvements in current and energy at plasma termination, with\nimprovements in speed through continuous re-training. A predict-first\nexperiment, increasing plasma current by 20\\% from baseline, demonstrates the\nNSSM's ability to make small extrapolations with sufficient accuracy to design\ntrajectories that successfully terminate the pulse. The developed approach\npaves the way for designing tokamak controls with robustness to considerable\nuncertainty, and demonstrates the relevance of the SciML approach to learning\nplasma dynamics for rapidly developing robust trajectories and controls during\nthe incremental campaigns of upcoming burning plasma tokamaks."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Debiasing for Unbiased Parameter Recovery",
        "author": "Luke C Sanford, Megan Ayers, Matthew Gordon, and Eliana Stone",
        "link": "http://arxiv.org/abs/2502.12323v1",
        "abstract": "Advances in machine learning and the increasing availability of\nhigh-dimensional data have led to the proliferation of social science research\nthat uses the predictions of machine learning models as proxies for measures of\nhuman activity or environmental outcomes. However, prediction errors from\nmachine learning models can lead to bias in the estimates of regression\ncoefficients. In this paper, we show how this bias can arise, propose a test\nfor detecting bias, and demonstrate the use of an adversarial machine learning\nalgorithm in order to de-bias predictions. These methods are applicable to any\nsetting where machine-learned predictions are the dependent variable in a\nregression. We conduct simulations and empirical exercises using ground truth\nand satellite data on forest cover in Africa. Using the predictions from a\nnaive machine learning model leads to biased parameter estimates, while the\npredictions from the adversarial model recover the true coefficients."
    },
    {
        "date": "2025-02",
        "title": "Robust 6DoF Pose Tracking Considering Contour and Interior Correspondence Uncertainty for AR Assembly Guidance",
        "author": "Jixiang Chen, Jing Chen, Kai Liu, Haochen Chang, Shanfeng Fu, and Jian Yang",
        "link": "http://arxiv.org/abs/2502.11971v1",
        "abstract": "Augmented reality assembly guidance is essential for intelligent\nmanufacturing and medical applications, requiring continuous measurement of the\n6DoF poses of manipulated objects. Although current tracking methods have made\nsignificant advancements in accuracy and efficiency, they still face challenges\nin robustness when dealing with cluttered backgrounds, rotationally symmetric\nobjects, and noisy sequences. In this paper, we first propose a robust\ncontour-based pose tracking method that addresses error-prone contour\ncorrespondences and improves noise tolerance. It utilizes a fan-shaped search\nstrategy to refine correspondences and models local contour shape and noise\nuncertainty as mixed probability distribution, resulting in a highly robust\ncontour energy function. Secondly, we introduce a CPU-only strategy to better\ntrack rotationally symmetric objects and assist the contour-based method in\novercoming local minima by exploring sparse interior correspondences. This is\nachieved by pre-sampling interior points from sparse viewpoint templates\noffline and using the DIS optical flow algorithm to compute their\ncorrespondences during tracking. Finally, we formulate a unified energy\nfunction to fuse contour and interior information, which is solvable using a\nre-weighted least squares algorithm. Experiments on public datasets and real\nscenarios demonstrate that our method significantly outperforms\nstate-of-the-art monocular tracking methods and can achieve more than 100 FPS\nusing only a CPU."
    },
    {
        "date": "2025-02",
        "title": "A limited technical background is sufficient for attack-defense tree acceptability",
        "author": "Nathan Daniel Schiele, and Olga Gadyatskaya",
        "link": "http://arxiv.org/abs/2502.11920v1",
        "abstract": "Attack-defense trees (ADTs) are a prominent graphical threat modeling method\nthat is highly recommended for analyzing and communicating security-related\ninformation. Despite this, existing empirical studies of attack trees have\nestablished their acceptability only for users with highly technical (computer\nscience) backgrounds while raising questions about their suitability for threat\nmodeling stakeholders with a limited technical background. Our research\naddresses this gap by investigating the impact of the users' technical\nbackground on ADT acceptability in an empirical study.\n  Our Method Evaluation Model-based study consisted of n = 102 participants (53\nwith a strong computer science background and 49 with a limited computer\nscience background) who were asked to complete a series of ADT-related tasks.\nBy analyzing their responses and comparing the results, we reveal that a very\nlimited technical background is sufficient for ADT acceptability. This finding\nunderscores attack trees' viability as a threat modeling method."
    },
    {
        "date": "2025-02",
        "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
        "author": "Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, and Xuan-Lam Pham",
        "link": "http://arxiv.org/abs/2502.11915v1",
        "abstract": "ChatGPT, an Artificial Intelligence model, has the potential to revolutionize\neducation. However, its effectiveness in solving non-English questions remains\nuncertain. This study evaluates ChatGPT's robustness using 586 Korean\nmathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering\n391 out of 586 questions. We also assess its ability to rate mathematics\nquestions based on eleven criteria and perform a topic analysis. Our findings\nshow that ChatGPT's ratings align with educational theory and test-taker\nperspectives. While ChatGPT performs well in question classification, it\nstruggles with non-English contexts, highlighting areas for improvement. Future\nresearch should address linguistic biases and enhance accuracy across diverse\nlanguages. Domain-specific optimizations and multilingual training could\nimprove ChatGPT's role in personalized education."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives",
        "author": "Leo Schwinn, Yan Scholten, Tom Wollschl\u00e4ger, Sophie Xhonneux, Stephen Casper, Stephan G\u00fcnnemann, and Gauthier Gidel",
        "link": "http://arxiv.org/abs/2502.11910v2",
        "abstract": "Misaligned research objectives have considerably hindered progress in\nadversarial robustness research over the past decade. For instance, an\nextensive focus on optimizing target metrics, while neglecting rigorous\nstandardized evaluation, has led researchers to pursue ad-hoc heuristic\ndefenses that were seemingly effective. Yet, most of these were exposed as\nflawed by subsequent evaluations, ultimately contributing little measurable\nprogress to the field. In this position paper, we illustrate that current\nresearch on the robustness of large language models (LLMs) risks repeating past\npatterns with potentially worsened real-world implications. To address this, we\nargue that realigned objectives are necessary for meaningful progress in\nadversarial alignment. To this end, we build on established cybersecurity\ntaxonomy to formally define differences between past and emerging threat models\nthat apply to LLMs. Using this framework, we illustrate that progress requires\ndisentangling adversarial alignment into addressable sub-problems and returning\nto core academic principles, such as measureability, reproducibility, and\ncomparability. Although the field presents significant challenges, the fresh\nstart on adversarial robustness offers the unique opportunity to build on past\nexperience while avoiding previous mistakes."
    },
    {
        "date": "2025-02",
        "title": "On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series",
        "author": "Kausik Lakkaraju, Rachneet Kaur, Parisa Zehtabi, Sunandita Patra, Siva Likitha Valluru, Zhen Zeng, Biplav Srivastava, and Marco Valtorta",
        "link": "http://arxiv.org/abs/2502.12226v1",
        "abstract": "Foundation Models (FMs) have improved time series forecasting in various\nsectors, such as finance, but their vulnerability to input disturbances can\nhinder their adoption by stakeholders, such as investors and analysts. To\naddress this, we propose a causally grounded rating framework to study the\nrobustness of Foundational Models for Time Series (FMTS) with respect to input\nperturbations. We evaluate our approach to the stock price prediction problem,\na well-studied problem with easily accessible public data, evaluating six\nstate-of-the-art (some multi-modal) FMTS across six prominent stocks spanning\nthree industries. The ratings proposed by our framework effectively assess the\nrobustness of FMTS and also offer actionable insights for model selection and\ndeployment. Within the scope of our study, we find that (1) multi-modal FMTS\nexhibit better robustness and accuracy compared to their uni-modal versions\nand, (2) FMTS pre-trained on time series forecasting task exhibit better\nrobustness and forecasting accuracy compared to general-purpose FMTS\npre-trained across diverse settings. Further, to validate our framework's\nusability, we conduct a user study showcasing FMTS prediction errors along with\nour computed ratings. The study confirmed that our ratings reduced the\ndifficulty for users in comparing the robustness of different systems."
    },
    {
        "date": "2025-02",
        "title": "FedEAT: A Robustness Optimization Framework for Federated LLMs",
        "author": "Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, and Hai Jin",
        "link": "http://arxiv.org/abs/2502.11863v1",
        "abstract": "Significant advancements have been made by Large Language Models (LLMs) in\nthe domains of natural language understanding and automated content creation.\nHowever, they still face persistent problems, including substantial\ncomputational costs and inadequate availability of training data. The\ncombination of Federated Learning (FL) and LLMs (federated LLMs) offers a\nsolution by leveraging distributed data while protecting privacy, which\npositions it as an ideal choice for sensitive domains. However, Federated LLMs\nstill suffer from robustness challenges, including data heterogeneity,\nmalicious clients, and adversarial attacks, which greatly hinder their\napplications. We first introduce the robustness problems in federated LLMs, to\naddress these challenges, we propose FedEAT (Federated Embedding space\nAdversarial Training), a novel framework that applies adversarial training in\nthe embedding space of client LLM and employs a robust aggregation approach,\nspecifically geometric median aggregation, to enhance the robustness of\nFederated LLMs. Our experiments demonstrate that FedEAT effectively improves\nthe robustness of Federated LLMs with minimal performance loss."
    },
    {
        "date": "2025-02",
        "title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives",
        "author": "Zeliang Zhang, Susan Liang, Daiki Shimada, and Chenliang Xu",
        "link": "http://arxiv.org/abs/2502.11858v2",
        "abstract": "While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency."
    },
    {
        "date": "2025-02",
        "title": "StructTransform: A Scalable Attack Surface for Safety-Aligned Large Language Models",
        "author": "Shehel Yoosuf, Temoor Ali, Ahmed Lekssays, Mashael AlSabah, and Issa Khalil",
        "link": "http://arxiv.org/abs/2502.11853v1",
        "abstract": "In this work, we present a series of structure transformation attacks on LLM\nalignment, where we encode natural language intent using diverse syntax spaces,\nranging from simple structure formats and basic query languages (e.g. SQL) to\nnew novel spaces and syntaxes created entirely by LLMs. Our extensive\nevaluation shows that our simplest attacks can achieve close to 90% success\nrate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment\nmechanisms. We improve the attack performance further by using an adaptive\nscheme that combines structure transformations along with existing\n\\textit{content transformations}, resulting in over 96% ASR with 0% refusals.\n  To generalize our attacks, we explore numerous structure formats, including\nsyntaxes purely generated by LLMs. Our results indicate that such novel\nsyntaxes are easy to generate and result in a high ASR, suggesting that\ndefending against our attacks is not a straightforward process. Finally, we\ndevelop a benchmark and evaluate existing safety-alignment defenses against it,\nshowing that most of them fail with 100% ASR. Our results show that existing\nsafety alignment mostly relies on token-level patterns without recognizing\nharmful concepts, highlighting and motivating the need for serious research\nefforts in this direction. As a case study, we demonstrate how attackers can\nuse our attack to easily generate a sample malware, and a corpus of fraudulent\nSMS messages, which perform well in bypassing detection."
    },
    {
        "date": "2025-02",
        "title": "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "author": "Mark Vero, Niels M\u00fcndler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanovi\u0107, Jingxuan He, and Martin Vechev",
        "link": "http://arxiv.org/abs/2502.11844v2",
        "abstract": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs."
    },
    {
        "date": "2025-02",
        "title": "Robust Partial-Label Learning by Leveraging Class Activation Values",
        "author": "Tobias Fuchs, and Florian Kalinke",
        "link": "http://arxiv.org/abs/2502.11743v1",
        "abstract": "Real-world training data is often noisy; for example, human annotators assign\nconflicting class labels to the same instances. Partial-label learning (PLL) is\na weakly supervised learning paradigm that allows training classifiers in this\ncontext without manual data cleaning. While state-of-the-art methods have good\npredictive performance, their predictions are sensitive to high noise levels,\nout-of-distribution data, and adversarial perturbations. We propose a novel PLL\nmethod based on subjective logic, which explicitly represents uncertainty by\nleveraging the magnitudes of the underlying neural network's class activation\nvalues. Thereby, we effectively incorporate prior knowledge about the class\nlabels by using a novel label weight re-distribution strategy that we prove to\nbe optimal. We empirically show that our method yields more robust predictions\nin terms of predictive performance under high PLL noise levels, handling\nout-of-distribution examples, and handling adversarial perturbations on the\ntest instances."
    },
    {
        "date": "2025-02",
        "title": "Adversarially Robust CLIP Models Can Induce Better (Robust) Perceptual Metrics",
        "author": "Francesco Croce, Christian Schlarmann, Naman Deep Singh, and Matthias Hein",
        "link": "http://arxiv.org/abs/2502.11725v1",
        "abstract": "Measuring perceptual similarity is a key tool in computer vision. In recent\nyears perceptual metrics based on features extracted from neural networks with\nlarge and diverse training sets, e.g. CLIP, have become popular. At the same\ntime, the metrics extracted from features of neural networks are not\nadversarially robust. In this paper we show that adversarially robust CLIP\nmodels, called R-CLIP$_\\textrm{F}$, obtained by unsupervised adversarial\nfine-tuning induce a better and adversarially robust perceptual metric that\noutperforms existing metrics in a zero-shot setting, and further matches the\nperformance of state-of-the-art metrics while being robust after fine-tuning.\nMoreover, our perceptual metric achieves strong performance on related tasks\nsuch as robust image-to-image retrieval, which becomes especially relevant when\napplied to \"Not Safe for Work\" (NSFW) content detection and dataset filtering.\nWhile standard perceptual metrics can be easily attacked by a small\nperturbation completely degrading NSFW detection, our robust perceptual metric\nmaintains high accuracy under an attack while having similar performance for\nunperturbed images. Finally, perceptual metrics induced by robust CLIP models\nhave higher interpretability: feature inversion can show which images are\nconsidered similar, while text inversion can find what images are associated to\na given prompt. This also allows us to visualize the very rich visual concepts\nlearned by a CLIP model, including memorized persons, paintings and complex\nqueries."
    },
    {
        "date": "2025-02",
        "title": "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning",
        "author": "Manaar Alam, Hithem Lamri, and Michail Maniatakos",
        "link": "http://arxiv.org/abs/2502.11687v1",
        "abstract": "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning."
    },
    {
        "date": "2025-02",
        "title": "DELMAN: Dynamic Defense Against Large Language Model Jailbreaking with Model Editing",
        "author": "Yi Wang, Fenghua Weng, Sibei Yang, Zhan Qin, Minlie Huang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2502.11647v1",
        "abstract": "Large Language Models (LLMs) are widely applied in decision making, but their\ndeployment is threatened by jailbreak attacks, where adversarial users\nmanipulate model behavior to bypass safety measures. Existing defense\nmechanisms, such as safety fine-tuning and model editing, either require\nextensive parameter modifications or lack precision, leading to performance\ndegradation on general tasks, which is unsuitable to post-deployment safety\nalignment. To address these challenges, we propose DELMAN (Dynamic Editing for\nLLMs JAilbreak DefeNse), a novel approach leveraging direct model editing for\nprecise, dynamic protection against jailbreak attacks. DELMAN directly updates\na minimal set of relevant parameters to neutralize harmful behaviors while\npreserving the model's utility. To avoid triggering a safe response in benign\ncontext, we incorporate KL-divergence regularization to ensure the updated\nmodel remains consistent with the original model when processing benign\nqueries. Experimental results demonstrate that DELMAN outperforms baseline\nmethods in mitigating jailbreak attacks while preserving the model's utility,\nand adapts seamlessly to new attack instances, providing a practical and\nefficient solution for post-deployment model protection."
    },
    {
        "date": "2025-02",
        "title": "Membership Inference Attacks for Face Images Against Fine-Tuned Latent Diffusion Models",
        "author": "Lauritz Christian Holme, Anton Mosquera Storgaard, and Siavash Arjomand Bigdeli",
        "link": "http://arxiv.org/abs/2502.11619v1",
        "abstract": "The rise of generative image models leads to privacy concerns when it comes\nto the huge datasets used to train such models. This paper investigates the\npossibility of inferring if a set of face images was used for fine-tuning a\nLatent Diffusion Model (LDM). A Membership Inference Attack (MIA) method is\npresented for this task. Using generated auxiliary data for the training of the\nattack model leads to significantly better performance, and so does the use of\nwatermarks. The guidance scale used for inference was found to have a\nsignificant influence. If a LDM is fine-tuned for long enough, the text prompt\nused for inference has no significant influence. The proposed MIA is found to\nbe viable in a realistic black-box setup against LDMs fine-tuned on\nface-images."
    },
    {
        "date": "2025-02",
        "title": "Trinity: A Scalable and Forward-Secure DSSE for Spatio-Temporal Range Query",
        "author": "Zhijun Li, Kuizhi Liu, Minghui Xu, Xiangyu Wang, Yinbin Miao, Jianfeng Ma, and Xiuzhen Cheng",
        "link": "http://arxiv.org/abs/2502.11550v1",
        "abstract": "Cloud-based outsourced Location-based services have profound impacts on\nvarious aspects of people's lives but bring security concerns. Existing\nspatio-temporal data secure retrieval schemes have significant shortcomings\nregarding dynamic updates, either compromising privacy through leakage during\nupdates (forward insecurity) or incurring excessively high update costs that\nhinder practical application. Under these circumstances, we first propose a\nbasic filter-based spatio-temporal range query scheme \\TrinityI that supports\nlow-cost dynamic updates and automatic expansion. Furthermore, to improve\nsecurity, reduce storage cost, and false positives, we propose a forward secure\nand verifiable scheme \\TrinityII that simultaneously minimizes storage\noverhead. A formal security analysis proves that \\TrinityI and \\TrinityII are\nIndistinguishable under Selective Chosen-Plaintext Attack (IND-SCPA). Finally,\nextensive experiments demonstrate that our design \\TrinityII significantly\nreduces storage requirements by 80\\%, enables data retrieval at the 1\nmillion-record level in just 0.01 seconds, and achieves 10 $\\times$ update\nefficiency than state-of-art."
    },
    {
        "date": "2025-02",
        "title": "Optimized detection of cyber-attacks on IoT networks via hybrid deep learning models",
        "author": "Ahmed Bensaoud, and Jugal Kalita",
        "link": "http://arxiv.org/abs/2502.11470v1",
        "abstract": "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies."
    },
    {
        "date": "2025-02",
        "title": "Semantically Robust Unsupervised Image Translation for Paired Remote Sensing Images",
        "author": "Sheng Fang, Kaiyu Li, Zhe Li, Jianli Zhao, and Xingli Zhang",
        "link": "http://arxiv.org/abs/2502.11468v1",
        "abstract": "Image translation for change detection or classification in bi-temporal\nremote sensing images is unique. Although it can acquire paired images, it is\nstill unsupervised. Moreover, strict semantic preservation in translation is\nalways needed instead of multimodal outputs. In response to these problems,\nthis paper proposes a new method, SRUIT (Semantically Robust Unsupervised\nImage-to-image Translation), which ensures semantically robust translation and\nproduces deterministic output. Inspired by previous works, the method explores\nthe underlying characteristics of bi-temporal Remote Sensing images and designs\nthe corresponding networks. Firstly, we assume that bi-temporal Remote Sensing\nimages share the same latent space, for they are always acquired from the same\nland location. So SRUIT makes the generators share their high-level layers, and\nthis constraint will compel two domain mapping to fall into the same latent\nspace. Secondly, considering land covers of bi-temporal images could evolve\ninto each other, SRUIT exploits the cross-cycle-consistent adversarial networks\nto translate from one to the other and recover them. Experimental results show\nthat constraints of sharing weights and cross-cycle consistency enable\ntranslated images with both good perceptual image quality and semantic\npreservation for significant differences."
    },
    {
        "date": "2025-02",
        "title": "Adversary-Aware DPO: Enhancing Safety Alignment in Vision Language Models via Adversarial Training",
        "author": "Fenghua Weng, Jian Lou, Jun Feng, Minlie Huang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2502.11455v1",
        "abstract": "Safety alignment is critical in pre-training large language models (LLMs) to\ngenerate responses aligned with human values and refuse harmful queries. Unlike\nLLM, the current safety alignment of VLMs is often achieved with post-hoc\nsafety fine-tuning. However, these methods are less effective to white-box\nattacks. To address this, we propose $\\textit{Adversary-aware DPO (ADPO)}$, a\nnovel training framework that explicitly considers adversarial.\n$\\textit{Adversary-aware DPO (ADPO)}$ integrates adversarial training into DPO\nto enhance the safety alignment of VLMs under worst-case adversarial\nperturbations. $\\textit{ADPO}$ introduces two key components: (1) an\nadversarial-trained reference model that generates human-preferred responses\nunder worst-case perturbations, and (2) an adversarial-aware DPO loss that\ngenerates winner-loser pairs accounting for adversarial distortions. By\ncombining these innovations, $\\textit{ADPO}$ ensures that VLMs remain robust\nand reliable even in the presence of sophisticated jailbreak attacks. Extensive\nexperiments demonstrate that $\\textit{ADPO}$ outperforms baselines in the\nsafety alignment and general utility of VLMs."
    },
    {
        "date": "2025-02",
        "title": "Learning Dexterous Bimanual Catch Skills through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning",
        "author": "Taewoo Kim, Youngwoo Yoon, and Jaehong Kim",
        "link": "http://arxiv.org/abs/2502.11437v1",
        "abstract": "Robotic catching has traditionally focused on single-handed systems, which\nare limited in their ability to handle larger or more complex objects. In\ncontrast, bimanual catching offers significant potential for improved dexterity\nand object handling but introduces new challenges in coordination and control.\nIn this paper, we propose a novel framework for learning dexterous bimanual\ncatching skills using Heterogeneous-Agent Reinforcement Learning (HARL). Our\napproach introduces an adversarial reward scheme, where a throw agent increases\nthe difficulty of throws-adjusting speed-while a catch agent learns to\ncoordinate both hands to catch objects under these evolving conditions. We\nevaluate the framework in simulated environments using 15 different objects,\ndemonstrating robustness and versatility in handling diverse objects. Our\nmethod achieved approximately a 2x increase in catching reward compared to\nsingle-agent baselines across 15 diverse objects."
    },
    {
        "date": "2025-02",
        "title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models",
        "author": "Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, and Jun Zhou",
        "link": "http://arxiv.org/abs/2502.11379v1",
        "abstract": "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted."
    },
    {
        "date": "2025-02",
        "title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System",
        "author": "Ziyou Jiang, Mingyang Li, Guowei Yang, Junjie Wang, Yuekai Huang, Zhiyuan Chang, and Qing Wang",
        "link": "http://arxiv.org/abs/2502.11358v1",
        "abstract": "Information theft attacks pose a significant risk to Large Language Model\n(LLM) tool-learning systems. Adversaries can inject malicious commands through\ncompromised tools, manipulating LLMs to send sensitive information to these\ntools, which leads to potential privacy breaches. However, existing attack\napproaches are black-box oriented and rely on static commands that cannot adapt\nflexibly to the changes in user queries and the invocation chain of tools. It\nmakes malicious commands more likely to be detected by LLM and leads to attack\nfailure. In this paper, we propose AutoCMD, a dynamic attack comment generation\napproach for information theft attacks in LLM tool-learning systems. Inspired\nby the concept of mimicking the familiar, AutoCMD is capable of inferring the\ninformation utilized by upstream tools in the toolchain through learning on\nopen-source systems and reinforcement with target system examples, thereby\ngenerating more targeted commands for information theft. The evaluation results\nshow that AutoCMD outperforms the baselines with +13.2% $ASR_{Theft}$, and can\nbe generalized to new tool-learning systems to expose their information leakage\nrisks. We also design four defense methods to effectively protect tool-learning\nsystems from the attack."
    },
    {
        "date": "2025-02",
        "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
        "author": "Cullen Anderson, and Jeff M. Phillips",
        "link": "http://arxiv.org/abs/2502.11324v1",
        "abstract": "Robust statistics aims to compute quantities to represent data where a\nfraction of it may be arbitrarily corrupted. The most essential statistic is\nthe mean, and in recent years, there has been a flurry of theoretical\nadvancement for efficiently estimating the mean in high dimensions on corrupted\ndata. While several algorithms have been proposed that achieve near-optimal\nerror, they all rely on large data size requirements as a function of\ndimension. In this paper, we perform an extensive experimentation over various\nmean estimation techniques where data size might not meet this requirement due\nto the high-dimensional setting."
    },
    {
        "date": "2025-02",
        "title": "ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation",
        "author": "Yiyi Chen, Qiongkai Xu, and Johannes Bjerva",
        "link": "http://arxiv.org/abs/2502.11308v2",
        "abstract": "With the growing popularity of Large Language Models (LLMs) and vector\ndatabases, private textual data is increasingly processed and stored as\nnumerical embeddings. However, recent studies have proven that such embeddings\nare vulnerable to inversion attacks, where original text is reconstructed to\nreveal sensitive information. Previous research has largely assumed access to\nmillions of sentences to train attack models, e.g., through data leakage or\nnearly unrestricted API access. With our method, a single data point is\nsufficient for a partially successful inversion attack. With as little as 1k\ndata samples, performance reaches an optimum across a range of black-box\nencoders, without training on leaked data. We present a Few-shot Textual\nEmbedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning\nvictim embeddings to the attack space and using a generative model to\nreconstruct text. We find that ALGEN attacks can be effectively transferred\nacross domains and languages, revealing key information. We further examine a\nvariety of defense mechanisms against ALGEN, and find that none are effective,\nhighlighting the vulnerabilities posed by inversion attacks. By significantly\nlowering the cost of inversion and proving that embedding spaces can be aligned\nthrough one-step optimization, we establish a new textual embedding inversion\nparadigm with broader applications for embedding alignment in NLP."
    },
    {
        "date": "2025-02",
        "title": "Game-Of-Goals: Using adversarial games to achieve strategic resilience",
        "author": "Aditya Ghose, and Asjad Khan",
        "link": "http://arxiv.org/abs/2502.11295v1",
        "abstract": "Our objective in this paper is to develop a machinery that makes a given\norganizational strategic plan resilient to the actions of competitor agents\n(adverse environmental actions). We assume that we are given a goal tree\nrepresenting strategic goals (can also be seen business requirements for a\nsoftware systems) with the assumption that competitor agents are behaving in a\nmaximally adversarial fashion(opposing actions against our sub goals or goals\nin general). We use game tree search methods (such as minimax) to select an\noptimal execution strategy(at a given point in time), such that it can maximize\nour chances of achieving our (high level) strategic goals. Our machinery helps\nus determine which path to follow(strategy selection) to achieve the best end\noutcome. This is done by comparing alternative execution strategies available\nto us via an evaluation function. Our evaluation function is based on the idea\nthat we want to make our execution plans defensible(future-proof) by selecting\nexecution strategies that make us least vulnerable to adversarial actions by\nthe competitor agents. i.e we want to select an execution strategy such that\nits leaves minimum room(or options) for the adversary to cause\nimpediment/damage to our business goals/plans."
    },
    {
        "date": "2025-02",
        "title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN",
        "author": "Jiayu Zhang, Zhiyu Zhu, Xinyi Wang, Silin Liao, Zhibo Jin, Flora D. Salim, and Huaming Chen",
        "link": "http://arxiv.org/abs/2502.12207v1",
        "abstract": "Deep neural networks have demonstrated remarkable performance across various\ndomains. However, they are vulnerable to adversarial examples, which can lead\nto erroneous predictions. Generative Adversarial Networks (GANs) can leverage\nthe generators and discriminators model to quickly produce high-quality\nadversarial examples. Since both modules train in a competitive and\nsimultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial\nexamples with better transferability compared to traditional methods. However,\nthe generation of perturbations is usually limited to a single iteration,\npreventing these examples from fully exploiting the potential of the methods.\nTo tackle this issue, we introduce a novel approach named Progressive\nAuto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive\niteration mechanism within a progressive generation network to craft\nadversarial examples with enhanced attack capability. We thoroughly evaluate\nour PAR-AdvGAN method with a large-scale experiment, demonstrating its superior\nperformance over various state-of-the-art black-box adversarial attacks, as\nwell as the original AdvGAN.Moreover, PAR-AdvGAN significantly accelerates the\nadversarial example generation, i.e., achieving the speeds of up to 335.5\nframes per second on Inception-v3 model, outperforming the gradient-based\ntransferable attack algorithms. Our code is available at:\nhttps://anonymous.4open.science/r/PAR-01BF/"
    },
    {
        "date": "2025-02",
        "title": "ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs",
        "author": "Ziyi Ni, Hao Wang, and Huacan Wang",
        "link": "http://arxiv.org/abs/2502.13162v1",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains but remain vulnerable to adversarial jailbreak attacks. Existing\nprompt-defense strategies, including parameter-modifying and parameter-free\napproaches, face limitations in adaptability, interpretability, and\ncustomization, constraining their effectiveness against evolving threats. To\naddress these challenges, we propose ShieldLearner, a novel paradigm that\nmimics human learning in defense. Through trial and error, it autonomously\ndistills attack signatures into a Pattern Atlas and synthesizes defense\nheuristics into a Meta-analysis Framework, enabling systematic and\ninterpretable threat detection. Furthermore, we introduce Adaptive Adversarial\nAugmentation to generate adversarial variations of successfully defended\nprompts, enabling continuous self-improvement without model retraining. In\naddition to standard benchmarks, we create a hard test set by curating\nadversarial prompts from the Wildjailbreak dataset, emphasizing more concealed\nmalicious intent. Experimental results show that ShieldLearner achieves a\nsignificantly higher defense success rate than existing baselines on both\nconventional and hard test sets, while also operating with lower computational\noverhead, making it a practical and efficient solution for real-world\nadversarial defense."
    },
    {
        "date": "2025-02",
        "title": "Logarithmic Width Suffices for Robust Memorization",
        "author": "Amitsour Egosi, Gilad Yehudai, and Ohad Shamir",
        "link": "http://arxiv.org/abs/2502.11162v1",
        "abstract": "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$)."
    },
    {
        "date": "2025-02",
        "title": "G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems",
        "author": "Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang",
        "link": "http://arxiv.org/abs/2502.11127v1",
        "abstract": "Large Language Model (LLM)-based Multi-agent Systems (MAS) have demonstrated\nremarkable capabilities in various complex tasks, ranging from collaborative\nproblem-solving to autonomous decision-making. However, as these systems become\nincreasingly integrated into critical applications, their vulnerability to\nadversarial attacks, misinformation propagation, and unintended behaviors have\nraised significant concerns. To address this challenge, we introduce\nG-Safeguard, a topology-guided security lens and treatment for robust LLM-MAS,\nwhich leverages graph neural networks to detect anomalies on the multi-agent\nutterance graph and employ topological intervention for attack remediation.\nExtensive experiments demonstrate that G-Safeguard: (I) exhibits significant\neffectiveness under various attack strategies, recovering over 40% of the\nperformance for prompt injection; (II) is highly adaptable to diverse LLM\nbackbones and large-scale MAS; (III) can seamlessly combine with mainstream MAS\nwith security guarantees. The code is available at\nhttps://github.com/wslong20/G-safeguard."
    },
    {
        "date": "2025-02",
        "title": "SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks",
        "author": "Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, and Junlan Feng",
        "link": "http://arxiv.org/abs/2502.11090v2",
        "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of\nLLMs has been a critical concern requiring precise assessment. Current\nbenchmarks primarily concentrate on single-turn dialogues or a single jailbreak\nattack method to assess the safety. Additionally, these benchmarks have not\ntaken into account the LLM's capability of identifying and handling unsafe\ninformation in detail. To address these issues, we propose a fine-grained\nbenchmark SafeDialBench for evaluating the safety of LLMs across various\njailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier\nhierarchical safety taxonomy that considers 6 safety dimensions and generates\nmore than 4000 multi-turn dialogues in both Chinese and English under 22\ndialogue scenarios. We employ 7 jailbreak attack strategies, such as reference\nattack and purpose reverse, to enhance the dataset quality for dialogue\ngeneration. Notably, we construct an innovative assessment framework of LLMs,\nmeasuring capabilities in detecting, and handling unsafe information and\nmaintaining consistency when facing jailbreak attacks. Experimental results\nacross 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior\nsafety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack",
        "author": "Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, and Baoyuan Wu",
        "link": "http://arxiv.org/abs/2502.12202v1",
        "abstract": "Longer thought, better performance: large language models with deep reasoning\ncapabilities, particularly o1-like models, have demonstrated remarkable\nperformance by generating extensive thought processes during inference. This\ntrade-off reveals a potential vulnerability: adversaries could compromise model\nperformance by forcing immediate responses without thought processes. To this\nend, in this paper, we introduce a novel attack scenario targeting the long\nthought processes of o1-like models and propose BoT (Break CoT), which can\nselectively break intrinsic reasoning mechanisms through backdoor attacks. BoT\nconstructs poisoned datasets with designed triggers and injects backdoor by\neither supervised fine-tuning or direct preference optimization. When\ntriggered, the model directly generates answers without thought processes,\nwhile maintaining normal reasoning capabilities for clean inputs. Extensive\nexperiments on open-source o1-like models, including recent DeepSeek-R1,\ndemonstrate that BoT nearly achieves high attack success rates while\nmaintaining clean accuracy, highlighting the critical safety risk in current\nmodels. Furthermore, the relationship between task difficulty and helpfulness\nreveals a potential application for good, enabling users to customize model\nbehavior based on task complexity. Code is available at\n\\href{https://github.com/zihao-ai/BoT}{https://github.com/zihao-ai/BoT}."
    },
    {
        "date": "2025-02",
        "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models",
        "author": "Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2502.11054v3",
        "abstract": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain."
    },
    {
        "date": "2025-02",
        "title": "Leveraging Large Language Models for Cybersecurity: Enhancing SMS Spam Detection with Robust and Context-Aware Text Classification",
        "author": "Mohsen Ahmadi, Matin Khajavi, Abbas Varmaghani, Ali Ala, Kasra Danesh, and Danial Javaheri",
        "link": "http://arxiv.org/abs/2502.11014v1",
        "abstract": "This study evaluates the effectiveness of different feature extraction\ntechniques and classification algorithms in detecting spam messages within SMS\ndata. We analyzed six classifiers Naive Bayes, K-Nearest Neighbors, Support\nVector Machines, Linear Discriminant Analysis, Decision Trees, and Deep Neural\nNetworks using two feature extraction methods: bag-of-words and TF-IDF. The\nprimary objective was to determine the most effective classifier-feature\ncombination for SMS spam detection. Our research offers two main contributions:\nfirst, by systematically examining various classifier and feature extraction\npairings, and second, by empirically evaluating their ability to distinguish\nspam messages. Our results demonstrate that the TF-IDF method consistently\noutperforms the bag-of-words approach across all six classifiers. Specifically,\nNaive Bayes with TF-IDF achieved the highest accuracy of 96.2%, with a\nprecision of 0.976 for non-spam and 0.754 for spam messages. Similarly, Support\nVector Machines with TF-IDF exhibited an accuracy of 94.5%, with a precision of\n0.926 for non-spam and 0.891 for spam. Deep Neural Networks using TF-IDF\nyielded an accuracy of 91.0%, with a recall of 0.991 for non-spam and 0.415 for\nspam messages. In contrast, classifiers such as K-Nearest Neighbors, Linear\nDiscriminant Analysis, and Decision Trees showed weaker performance, regardless\nof the feature extraction method employed. Furthermore, we observed substantial\nvariability in classifier effectiveness depending on the chosen feature\nextraction technique. Our findings emphasize the significance of feature\nselection in SMS spam detection and suggest that TF-IDF, when paired with Naive\nBayes, Support Vector Machines, or Deep Neural Networks, provides the most\nreliable performance. These insights provide a foundation for improving SMS\nspam detection through optimized feature extraction and classification methods."
    },
    {
        "date": "2025-02",
        "title": "FeaKM: Robust Collaborative Perception under Noisy Pose Conditions",
        "author": "Jiuwu Hao, Liguo Sun, Ti Xiang, Yuting Wan, Haolin Song, and Pin Lv",
        "link": "http://arxiv.org/abs/2502.11003v1",
        "abstract": "Collaborative perception is essential for networks of agents with limited\nsensing capabilities, enabling them to work together by exchanging information\nto achieve a robust and comprehensive understanding of their environment.\nHowever, localization inaccuracies often lead to significant spatial message\ndisplacement, which undermines the effectiveness of these collaborative\nefforts. To tackle this challenge, we introduce FeaKM, a novel method that\nemploys Feature-level Keypoints Matching to effectively correct pose\ndiscrepancies among collaborating agents. Our approach begins by utilizing a\nconfidence map to identify and extract salient points from intermediate feature\nrepresentations, allowing for the computation of their descriptors. This step\nensures that the system can focus on the most relevant information, enhancing\nthe matching process. We then implement a target-matching strategy that\ngenerates an assignment matrix, correlating the keypoints identified by\ndifferent agents. This is critical for establishing accurate correspondences,\nwhich are essential for effective collaboration. Finally, we employ a\nfine-grained transformation matrix to synchronize the features of all agents\nand ascertain their relative statuses, ensuring coherent communication among\nthem. Our experimental results demonstrate that FeaKM significantly outperforms\nexisting methods on the DAIR-V2X dataset, confirming its robustness even under\nsevere noise conditions. The code and implementation details are available at\nhttps://github.com/uestchjw/FeaKM."
    },
    {
        "date": "2025-02",
        "title": "SSVEP-BiMA: Bifocal Masking Attention Leveraging Native and Symmetric-Antisymmetric Components for Robust SSVEP Decoding",
        "author": "Yuxin Liu, Zhenxi Song, Guoyang Xu, Zirui Wang, Feng Wan, Yong Hu, Min Zhang, and Zhiguo Zhang",
        "link": "http://arxiv.org/abs/2502.10994v1",
        "abstract": "Brain-computer interface (BCI) based on steady-state visual evoked potentials\n(SSVEP) is a popular paradigm for its simplicity and high information transfer\nrate (ITR). Accurate and fast SSVEP decoding is crucial for reliable BCI\nperformance. However, conventional decoding methods demand longer time windows,\nand deep learning models typically require subject-specific fine-tuning,\nleaving challenges in achieving optimal performance in cross-subject settings.\nThis paper proposed a biofocal masking attention-based method (SSVEP-BiMA) that\nsynergistically leverages the native and symmetric-antisymmetric components for\ndecoding SSVEP. By utilizing multiple signal representations, the network is\nable to integrate features from a wider range of sample perspectives, leading\nto more generalized and comprehensive feature learning, which enhances both\nprediction accuracy and robustness. We performed experiments on two public\ndatasets, and the results demonstrate that our proposed method surpasses\nbaseline approaches in both accuracy and ITR. We believe that this work will\ncontribute to the development of more efficient SSVEP-based BCI systems."
    },
    {
        "date": "2025-02",
        "title": "RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization",
        "author": "Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, and Haoyu Wang",
        "link": "http://arxiv.org/abs/2502.10993v1",
        "abstract": "Large language models (LLMs) have achieved impressive performance but face\nhigh computational costs and latency, limiting their deployment in\nresource-constrained settings. In contrast, small-scale LLMs (SLMs) are more\nefficient yet struggle to capture evolving real-world knowledge.\nRetrieval-augmented generation (RAG) helps by integrating external knowledge,\nbut imperfect retrieval can introduce distracting noise that misleads SLMs. We\npropose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference\nOptimization. RoseRAG employs multi-turn prompting for detailed reasoning,\nrejection sampling for high-quality explanations, and contrastive preference\nselection to refine responses by maximizing the likelihood gap between\npreferred and non-preferred outputs. By integrating these components into a\nmargin-aware optimization process, RoseRAG robustly enhances the accuracy and\nreliability of SLMs for RAG applications. Extensive experiments on three\nopen-domain question answering benchmarks indicate that our innovative RoseRAG\nsurpasses state-of-the-art baselines significantly."
    },
    {
        "date": "2025-02",
        "title": "D-CIPHER: Dynamic Collaborative Intelligent Agents with Planning and Heterogeneous Execution for Enhanced Reasoning in Offensive Security",
        "author": "Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, and Muhammad Shafique",
        "link": "http://arxiv.org/abs/2502.10931v1",
        "abstract": "Large Language Models (LLMs) have been used in cybersecurity in many ways,\nincluding their recent use as intelligent agent systems for autonomous security\nanalysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing\nthe automated task-planning abilities of LLM agents across various\ncybersecurity skill sets. Early attempts to apply LLMs for solving CTF\nchallenges relied on single-agent systems, where feedback was restricted to a\nsingle reasoning-action loop. This approach proved inadequate for handling\ncomplex CTF tasks. Drawing inspiration from real-world CTF competitions, where\nteams of experts collaborate, we introduce the D-CIPHER multi-agent LLM\nframework for collaborative CTF challenge solving. D-CIPHER integrates agents\nwith distinct roles, enabling dynamic feedback loops to enhance reasoning on\nCTF challenges. It introduces the Planner-Executor agent system, consisting of\na Planner agent for overall problem-solving along with multiple heterogeneous\nExecutor agents for individual tasks, facilitating efficient allocation of\nresponsibilities among the LLMs. Additionally, D-CIPHER incorporates an\nAuto-prompter agent, which improves problem-solving by exploring the challenge\nenvironment and generating a highly relevant initial prompt. We evaluate\nD-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive\nstudies to highlight the impact of our enhancements. Our results demonstrate\nthat the multi-agent D-CIPHER system achieves a significant improvement in\nchallenges solved, setting a state-of-the-art performance on three benchmarks:\n22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox. D-CIPHER is\navailable at https://github.com/NYU-LLM-CTF/nyuctf_agents as the\nnyuctf_multiagent package."
    },
    {
        "date": "2025-02",
        "title": "A Closer Look at System Prompt Robustness",
        "author": "Norman Mu, Jonathan Lu, Michael Lavery, and David Wagner",
        "link": "http://arxiv.org/abs/2502.12197v1",
        "abstract": "System prompts have emerged as a critical control surface for specifying the\nbehavior of LLMs in chat and agent settings. Developers depend on system\nprompts to specify important context, output format, personalities, guardrails,\ncontent policies, and safety countermeasures, all of which require models to\nrobustly adhere to the system prompt, especially when facing conflicting or\nadversarial user inputs. In practice, models often forget to consider relevant\nguardrails or fail to resolve conflicting demands between the system and the\nuser. In this work, we study various methods for improving system prompt\nrobustness by creating realistic new evaluation and fine-tuning datasets based\non prompts collected from from OpenAI's GPT Store and HuggingFace's\nHuggingChat. Our experiments assessing models with a panel of new and existing\nbenchmarks show that performance can be considerably improved with realistic\nfine-tuning data, as well as inference-time interventions such as\nclassifier-free guidance. Finally, we analyze the results of recently released\nreasoning models from OpenAI and DeepSeek, which show exciting but uneven\nimprovements on the benchmarks we study. Overall, current techniques fall short\nof ensuring system prompt robustness and further study is warranted."
    },
    {
        "date": "2025-02",
        "title": "Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm",
        "author": "Nanyu Luo, and Feng Ji",
        "link": "http://arxiv.org/abs/2502.10650v1",
        "abstract": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. This study introduces Adversarial\nVariational Bayes (AVB) algorithms as an improvement to VAEs for IFA with\nimproved flexibility and accuracy. By bridging the strengths of VAEs and\nGenerative Adversarial Networks (GANs), AVB incorporates an auxiliary\ndiscriminator network to reframe the estimation process as a two-player\nadversarial game and removes the restrictive assumption of standard normal\ndistributions in the inference model. Theoretically, AVB can achieve similar or\nhigher likelihood compared to VAEs. A further enhanced algorithm,\nImportance-weighted Adversarial Variational Bayes (IWAVB) is proposed and\ncompared with Importance-weighted Autoencoders (IWAE). In an exploratory\nanalysis of real empirical data, IWAVB demonstrated superior expressiveness by\nachieving a higher likelihood compared to IWAE. In confirmatory studies with\nsimulated data, IWAVB achieved similar mean-square error results to IWAE while\nconsistently achieving higher likelihoods. Moreover, in simulations where\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE by\nproviding more accurate parameter estimates. With its innovative use of GANs,\nIWAVB is shown to have the potential to extend IFA to handle large-scale data,\nfacilitating the potential integration of psychometrics and multimodal data\nanalysis."
    },
    {
        "date": "2025-02",
        "title": "LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization",
        "author": "Erica Zhang, Ryunosuke Goto, Naomi Sagan, Jurik Mutter, Nick Phillips, Ash Alizadeh, Kangwook Lee, Jose Blanchet, Mert Pilanci, and Robert Tibshirani",
        "link": "http://arxiv.org/abs/2502.10648v2",
        "abstract": "We introduce LLM-Lasso, a novel framework that leverages large language\nmodels (LLMs) to guide feature selection in Lasso $\\ell_1$ regression. Unlike\ntraditional methods that rely solely on numerical data, LLM-Lasso incorporates\ndomain-specific knowledge extracted from natural language, enhanced through a\nretrieval-augmented generation (RAG) pipeline, to seamlessly integrate\ndata-driven modeling with contextual insights. Specifically, the LLM generates\npenalty factors for each feature, which are converted into weights for the\nLasso penalty using a simple, tunable model. Features identified as more\nrelevant by the LLM receive lower penalties, increasing their likelihood of\nbeing retained in the final model, while less relevant features are assigned\nhigher penalties, reducing their influence. Importantly, LLM-Lasso has an\ninternal validation step that determines how much to trust the contextual\nknowledge in our prediction pipeline. Hence it addresses key challenges in\nrobustness, making it suitable for mitigating potential inaccuracies or\nhallucinations from the LLM. In various biomedical case studies, LLM-Lasso\noutperforms standard Lasso and existing feature selection baselines, all while\nensuring the LLM operates without prior access to the datasets. To our\nknowledge, this is the first approach to effectively integrate conventional\nfeature selection techniques directly with LLM-based domain-specific reasoning."
    },
    {
        "date": "2025-02",
        "title": "Dark Deceptions in DHCP: Dismantling Network Defenses",
        "author": "Robert Dilworth",
        "link": "http://arxiv.org/abs/2502.10646v1",
        "abstract": "This paper explores vulnerabilities in the Dynamic Host Configuration\nProtocol (DHCP) and their implications on the Confidentiality, Integrity, and\nAvailability (CIA) triad. Through an analysis of various attacks, including\nDHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits,\nthe paper provides a taxonomic classification of threats, assesses risks, and\nproposes appropriate controls. The discussion also highlights the dangers of\nVPN decloaking through DHCP exploits and underscores the importance of\nsafeguarding network infrastructures. By bringing awareness to the TunnelVision\nexploit, this paper aims to mitigate risks associated with these prevalent\nvulnerabilities."
    },
    {
        "date": "2025-02",
        "title": "Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network",
        "author": "Arun Kunwar, Dibakar Raj Pant, Jukka Heikkonen, and Rajeev Kanth",
        "link": "http://arxiv.org/abs/2502.10334v1",
        "abstract": "The Convolutional Neural Network (CNN) has shown impressive performance in\nimage classification because of its strong learning capabilities. However, it\ndemands a substantial and balanced dataset for effective training. Otherwise,\nnetworks frequently exhibit over fitting and struggle to generalize to new\nexamples. Publicly available dataset of fundus images of ocular disease is\ninsufficient to train any classification model to achieve satisfactory\naccuracy. So, we propose Generative Adversarial Network(GAN) based data\ngeneration technique to synthesize dataset for training CNN based\nclassification model and later use original disease containing ocular images to\ntest the model. During testing the model classification accuracy with the\noriginal ocular image, the model achieves an accuracy rate of 78.6% for myopia,\n88.6% for glaucoma, and 84.6% for cataract, with an overall classification\naccuracy of 84.6%."
    },
    {
        "date": "2025-02",
        "title": "VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect",
        "author": "Qingyuan Fei, Wenjie Hou, Xuan Hai, and Xin Liu",
        "link": "http://arxiv.org/abs/2502.10329v1",
        "abstract": "The rapid advancements in AI voice cloning, fueled by machine learning, have\nsignificantly impacted text-to-speech (TTS) and voice conversion (VC) fields.\nWhile these developments have led to notable progress, they have also raised\nconcerns about the misuse of AI VC technology, causing economic losses and\nnegative public perceptions. To address this challenge, this study focuses on\ncreating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds\npseudo-timbre (jamming information) based on SFS into audio segments that are\nimperceptible to the human ear, thereby forming systematic fragments to prevent\nvoice cloning. This approach protects the voice without compromising its\nquality. In comparison to existing methods, such as adversarial noise\nincorporation, VocalCrypt significantly enhances robustness and real-time\nperformance, achieving a 500\\% increase in generation speed while maintaining\ninterference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our\nmethod offers preemptive defense, reducing implementation costs and enhancing\nfeasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets\nshow that our AI-cloned speech defense system performs excellently in automatic\nspeaker verification (ASV) tests while preserving the integrity of the\nprotected audio."
    },
    {
        "date": "2025-02",
        "title": "Adversarial Mixup Unlearning",
        "author": "Zhuoyi Peng, Yixuan Tang, and Yi Yang",
        "link": "http://arxiv.org/abs/2502.10288v1",
        "abstract": "Machine unlearning is a critical area of research aimed at safeguarding data\nprivacy by enabling the removal of sensitive information from machine learning\nmodels. One unique challenge in this field is catastrophic unlearning, where\nerasing specific data from a well-trained model unintentionally removes\nessential knowledge, causing the model to deviate significantly from a\nretrained one. To address this, we introduce a novel approach that regularizes\nthe unlearning process by utilizing synthesized mixup samples, which simulate\nthe data susceptible to catastrophic effects. At the core of our approach is a\ngenerator-unlearner framework, MixUnlearn, where a generator adversarially\nproduces challenging mixup examples, and the unlearner effectively forgets\ntarget information based on these synthesized data. Specifically, we first\nintroduce a novel contrastive objective to train the generator in an\nadversarial direction: generating examples that prompt the unlearner to reveal\ninformation that should be forgotten, while losing essential knowledge. Then\nthe unlearner, guided by two other contrastive loss terms, processes the\nsynthesized and real data jointly to ensure accurate unlearning without losing\ncritical knowledge, overcoming catastrophic effects. Extensive evaluations\nacross benchmark datasets demonstrate that our method significantly outperforms\nstate-of-the-art approaches, offering a robust solution to machine unlearning.\nThis work not only deepens understanding of unlearning mechanisms but also lays\nthe foundation for effective machine unlearning with mixup augmentation."
    },
    {
        "date": "2025-02",
        "title": "Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study",
        "author": "Sharjeel Imtiaz, Uljana Reinsalu, and Tara Ghasempouri",
        "link": "http://arxiv.org/abs/2502.10194v1",
        "abstract": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches."
    },
    {
        "date": "2025-02",
        "title": "A Robust Attack: Displacement Backdoor Attack",
        "author": "Yong Li, and Han Gao",
        "link": "http://arxiv.org/abs/2502.10490v1",
        "abstract": "As artificial intelligence becomes more prevalent in our lives, people are\nenjoying the convenience it brings, but they are also facing hidden threats,\nsuch as data poisoning and adversarial attacks. These threats can have\ndisastrous consequences for the application of artificial intelligence,\nespecially for some applications that take effect immediately, such as\nautonomous driving and medical fields. Among these threats, backdoor attacks\nhave left a deep impression on people with their concealment and simple\ndeployment, making them a threat that cannot be ignored, however, in the\nprocess of deploying the backdoor model, the backdoor attack often has some\nreasons that make it unsatisfactory in real-world applications, such as jitter\nand brightness changes. Based on this, we propose a highly robust backdoor\nattack that shifts the target sample and combines it with itself to form a\nbackdoor sample, the Displacement Backdoor Attack(DBA). Experimental results\nshow that the DBA attack can resist data augmentation that simulates real-world\ndifferences, such as rotation and cropping."
    },
    {
        "date": "2025-02",
        "title": "Fast Proxies for LLM Robustness Evaluation",
        "author": "Tim Beyer, Jan Schuchardt, Leo Schwinn, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2502.10487v1",
        "abstract": "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe\ndeployment, yet current red-teaming methods are often prohibitively expensive.\nWe compare the ability of fast proxy metrics to predict the real-world\nrobustness of an LLM against a simulated attacker ensemble. This allows us to\nestimate a model's robustness to computationally expensive attacks without\nrequiring runs of the attacks themselves. Specifically, we consider\ngradient-descent-based embedding-space attacks, prefilling attacks, and direct\nprompting. Even though direct prompting in particular does not achieve high\nASR, we find that it and embedding-space attacks can predict attack success\nrates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank)\ncorrelations with the full attack ensemble while reducing computational cost by\nthree orders of magnitude."
    },
    {
        "date": "2025-02",
        "title": "A Survey of Safety on Large Vision-Language Models: Attacks, Defenses and Evaluations",
        "author": "Mang Ye, Xuankun Rong, Wenke Huang, Bo Du, Nenghai Yu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2502.14881v1",
        "abstract": "With the rapid advancement of Large Vision-Language Models (LVLMs), ensuring\ntheir safety has emerged as a crucial area of research. This survey provides a\ncomprehensive analysis of LVLM safety, covering key aspects such as attacks,\ndefenses, and evaluation methods. We introduce a unified framework that\nintegrates these interrelated components, offering a holistic perspective on\nthe vulnerabilities of LVLMs and the corresponding mitigation strategies.\nThrough an analysis of the LVLM lifecycle, we introduce a classification\nframework that distinguishes between inference and training phases, with\nfurther subcategories to provide deeper insights. Furthermore, we highlight\nlimitations in existing research and outline future directions aimed at\nstrengthening the robustness of LVLMs. As part of our research, we conduct a\nset of safety evaluations on the latest LVLM, Deepseek Janus-Pro, and provide a\ntheoretical analysis of the results. Our findings provide strategic\nrecommendations for advancing LVLM safety and ensuring their secure and\nreliable deployment in high-stakes, real-world applications. This survey aims\nto serve as a cornerstone for future research, facilitating the development of\nmodels that not only push the boundaries of multimodal intelligence but also\nadhere to the highest standards of security and ethical integrity. Furthermore,\nto aid the growing research in this field, we have created a public repository\nto continuously compile and update the latest work on LVLM safety:\nhttps://github.com/XuankunRong/Awesome-LVLM-Safety ."
    },
    {
        "date": "2025-02",
        "title": "Robust Anomaly Detection via Tensor Chidori Pseudoskeleton Decomposition",
        "author": "Bowen Su",
        "link": "http://arxiv.org/abs/2502.09926v2",
        "abstract": "Anomaly detection plays a critical role in modern data-driven applications,\nfrom identifying fraudulent transactions and safeguarding network\ninfrastructure to monitoring sensor systems for irregular patterns. Traditional\napproaches, such as distance, density, or cluster-based methods, face\nsignificant challenges when applied to high dimensional tensor data, where\ncomplex interdependencies across dimensions amplify noise and computational\ncomplexity. To address these limitations, this paper leverages Tensor Chidori\npseudoskeleton decomposition within a tensor-robust principal component\nanalysis framework to extract low Tucker rank structure while isolating sparse\nanomalies, ensuring robustness to anomaly detection. We establish theoretical\nresults regarding convergence, and estimation error, demonstrating the\nstability and accuracy of the proposed approach. Numerical experiments on\nreal-world spatiotemporal data from New York City taxi trip records validate\nthe superiority of the proposed method in detecting anomalous urban events\ncompared to existing benchmark methods. The results underscore the potential of\nTensor Chidori pseudoskeleton decomposition to enhance anomaly detection for\nlarge-scale, high-dimensional data."
    },
    {
        "date": "2025-02",
        "title": "ChatIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation",
        "author": "Ye Dong, Yan Lin Aung, Sudipta Chattopadhyay, and Jianying Zhou",
        "link": "http://arxiv.org/abs/2502.09896v1",
        "abstract": "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone."
    },
    {
        "date": "2025-02",
        "title": "U Can Touch This! Microarchitectural Timing Attacks via Machine Clears",
        "author": "Billy Bob Brumley",
        "link": "http://arxiv.org/abs/2502.09864v1",
        "abstract": "Microarchitectural timing attacks exploit subtle timing variations caused by\nhardware behaviors to leak sensitive information. In this paper, we introduce\nMCHammer, a novel side-channel technique that leverages machine clears induced\nby self-modifying code detection mechanisms. Unlike most traditional\ntechniques, MCHammer does not require memory access or waiting periods, making\nit highly efficient. We compare MCHammer to the classical Flush+Reload\ntechnique, improving in terms of trace granularity, providing a powerful\nside-channel attack vector. Using MCHammer, we successfully recover keys from a\ndeployed implementation of a cryptographic tool. Our findings highlight the\npractical implications of MCHammer and its potential impact on real-world\nsystems."
    },
    {
        "date": "2025-02",
        "title": "Elastic Representation: Mitigating Spurious Correlations for Group Robustness",
        "author": "Tao Wen, Zihan Wang, Quan Zhang, and Qi Lei",
        "link": "http://arxiv.org/abs/2502.09850v1",
        "abstract": "Deep learning models can suffer from severe performance degradation when\nrelying on spurious correlations between input features and labels, making the\nmodels perform well on training data but have poor prediction accuracy for\nminority groups. This problem arises especially when training data are limited\nor imbalanced. While most prior work focuses on learning invariant features\n(with consistent correlations to y), it overlooks the potential harm of\nspurious correlations between features. We hereby propose Elastic\nRepresentation (ElRep) to learn features by imposing Nuclear- and\nFrobenius-norm penalties on the representation from the last layer of a neural\nnetwork. Similar to the elastic net, ElRep enjoys the benefits of learning\nimportant features without losing feature diversity. The proposed method is\nsimple yet effective. It can be integrated into many deep learning approaches\nto mitigate spurious correlations and improve group robustness. Moreover, we\ntheoretically show that ElRep has minimum negative impacts on in-distribution\npredictions. This is a remarkable advantage over approaches that prioritize\nminority groups at the cost of overall performance."
    },
    {
        "date": "2025-02",
        "title": "On the robustness of multimodal language model towards distractions",
        "author": "Ming Liu, Hao Chen, Jindong Wang, and Wensheng Zhang",
        "link": "http://arxiv.org/abs/2502.09818v1",
        "abstract": "Although vision-language models (VLMs) have achieved significant success in\nvarious applications such as visual question answering, their resilience to\nprompt variations remains an under-explored area. Understanding how\ndistractions affect VLMs is crucial for improving their real-world\napplicability, as inputs could have noisy and irrelevant information in many\npractical scenarios. This paper aims to assess the robustness of VLMs against\nboth visual and textual distractions in the context of science question\nanswering. Built on the ScienceQA dataset, we developed a new benchmark that\nintroduces distractions in both the visual and textual contexts to evaluate the\nreasoning capacity of VLMs amid these distractions. Our findings reveal that\nmost-of-the-art VLMs, including GPT-4, are vulnerable to various types of\ndistractions, experiencing noticeable degradation in reasoning capabilities\nwhen confronted with distractions. Notably, models such as InternVL2\ndemonstrate a higher degree of robustness to these distractions. We also found\nthat models exhibit greater sensitivity to textual distractions than visual\nones. Additionally, we explored various mitigation strategies, such as prompt\nengineering, to counteract the impact of distractions. While these strategies\nimproved solution accuracy, our analysis shows that there remain significant\nopportunities for improvement."
    },
    {
        "date": "2025-02",
        "title": "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from Sparse Matrix Decomposition",
        "author": "Yu Zheng, Qizhi Zhang, Lichun Li, Kai Zhou, and Shan Yin",
        "link": "http://arxiv.org/abs/2502.09808v1",
        "abstract": "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions."
    },
    {
        "date": "2025-02",
        "title": "Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models",
        "author": "Jin Hyun Park, Seyyed Ali Ayati, and Yichen Cai",
        "link": "http://arxiv.org/abs/2502.09782v3",
        "abstract": "The increasing prevalence of microphones in everyday devices and the growing\nreliance on online services have amplified the risk of acoustic side-channel\nattacks (ASCAs) targeting keyboards. This study explores deep learning\ntechniques, specifically vision transformers (VTs) and large language models\n(LLMs), to enhance the effectiveness and applicability of such attacks. We\npresent substantial improvements over prior research, with the CoAtNet model\nachieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement\nfor keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via\nZoom compared to previous benchmarks. We also evaluate transformer\narchitectures and language models, with the best VT model matching CoAtNet's\nperformance. A key advancement is the introduction of a noise mitigation method\nfor real-world scenarios. By using LLMs for contextual understanding, we detect\nand correct erroneous keystrokes in noisy environments, enhancing ASCA\nperformance. Additionally, fine-tuned lightweight language models with Low-Rank\nAdaptation (LoRA) deliver comparable performance to heavyweight models with 67X\nmore parameters. This integration of VTs and LLMs improves the practical\napplicability of ASCA mitigation, marking the first use of these technologies\nto address ASCAs and error correction in real-world scenarios."
    },
    {
        "date": "2025-02",
        "title": "SoK: Come Together -- Unifying Security, Information Theory, and Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework",
        "author": "Ali Teymourian, Andrew M. Webb, Taha Gharaibeh, Arushi Ghildiyal, and Ibrahim Baggili",
        "link": "http://arxiv.org/abs/2502.09763v1",
        "abstract": "We present a primary attack ontology and analysis framework for deception\nattacks in Mixed Reality (MR). This is achieved through multidisciplinary\nSystematization of Knowledge (SoK), integrating concepts from MR security,\ninformation theory, and cognition. While MR grows in popularity, it presents\nmany cybersecurity challenges, particularly concerning deception attacks and\ntheir effects on humans. In this paper, we use the Borden-Kopp model of\ndeception to develop a comprehensive ontology of MR deception attacks. Further,\nwe derive two models to assess impact of MR deception attacks on information\ncommunication and decision-making. The first, an information-theoretic model,\nmathematically formalizes the effects of attacks on information communication.\nThe second, a decision-making model, details the effects of attacks on\ninterlaced cognitive processes. Using our ontology and models, we establish the\nMR Deception Analysis Framework (DAF) to assess the effects of MR deception\nattacks on information channels, perception, and attention. Our SoK uncovers\nfive key findings for research and practice and identifies five research gaps\nto guide future work."
    },
    {
        "date": "2025-02",
        "title": "Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization",
        "author": "Amit Levi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, and Chaim Baskin",
        "link": "http://arxiv.org/abs/2502.09755v1",
        "abstract": "Jailbreak attacks aim to exploit large language models (LLMs) and pose a\nsignificant threat to their proper conduct; they seek to bypass models'\nsafeguards and often provoke transgressive behaviors. However, existing\nautomatic jailbreak attacks require extensive computational resources and are\nprone to converge on suboptimal solutions. In this work, we propose\n\\textbf{C}ompliance \\textbf{R}efusal \\textbf{I}nitialization (CRI), a novel,\nattack-agnostic framework that efficiently initializes the optimization in the\nproximity of the compliance subspace of harmful prompts. By narrowing the\ninitial gap to the adversarial objective, CRI substantially improves\nadversarial success rates (ASR) and drastically reduces computational overhead\n-- often requiring just a single optimization step. We evaluate CRI on the\nwidely-used AdvBench dataset over the standard jailbreak attacks of GCG and\nAutoDAN. Results show that CRI boosts ASR and decreases the median steps to\nsuccess by up to \\textbf{\\(\\times 60\\)}. The project page, along with the\nreference implementation, is publicly available at\n\\texttt{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}."
    },
    {
        "date": "2025-02",
        "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
        "author": "Abdullah Aydeger, Sanzida Hoque, Engin Zeydan, and Kapal Dev",
        "link": "http://arxiv.org/abs/2502.09726v1",
        "abstract": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
    },
    {
        "date": "2025-02",
        "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency",
        "author": "Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li",
        "link": "http://arxiv.org/abs/2502.09621v1",
        "abstract": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/"
    },
    {
        "date": "2025-02",
        "title": "SyntheticPop: Attacking Speaker Verification Systems With Synthetic VoicePops",
        "author": "Eshaq Jamdar, and Amith Kamath Belman",
        "link": "http://arxiv.org/abs/2502.09553v1",
        "abstract": "Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method."
    },
    {
        "date": "2025-02",
        "title": "Registration, Detection, and Deregistration: Analyzing DNS Abuse for Phishing Attacks",
        "author": "Kyungchan Lim, Kiho Lee, Raffaele Sommese, Mattis Jonker, Ricky Mok, kc claffy, and Doowon Kim",
        "link": "http://arxiv.org/abs/2502.09549v1",
        "abstract": "Phishing continues to pose a significant cybersecurity threat. While\nblocklists currently serve as a primary defense, due to their reactive, passive\nnature, these delayed responses leave phishing websites operational long enough\nto harm potential victims. It is essential to address this fundamental\nchallenge at the root, particularly in phishing domains. Domain registration\npresents a crucial intervention point, as domains serve as the primary gateway\nbetween users and websites. We conduct a comprehensive longitudinal analysis of\n690,502 unique phishing domains, spanning a 39 month period, to examine their\ncharacteristics and behavioral patterns throughout their lifecycle-from initial\nregistration to detection and eventual deregistration. We find that 66.1% of\nthe domains in our dataset are maliciously registered, leveraging\ncost-effective TLDs and targeting brands by mimicking their domain names under\nalternative TLDs (e.g., .top and .tk) instead of the TLDs under which the brand\ndomains are registered (e.g., .com and .ru). We also observe minimal\nimprovements in detection speed for maliciously registered domains compared to\ncompromised domains. Detection times vary widely across blocklists, and\nphishing domains remain accessible for an average of 11.5 days after detection,\nprolonging their potential impact. Our systematic investigation uncovers key\npatterns from registration through detection to deregistration, which could be\nleveraged to enhance anti-phishing active defenses at the DNS level."
    },
    {
        "date": "2025-02",
        "title": "Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based Security",
        "author": "Carlton Shepherd, and Elliot Hurley",
        "link": "http://arxiv.org/abs/2502.09535v3",
        "abstract": "Mobile sensor data has been proposed for security-critical applications such\nas device pairing, proximity detection, and continuous authentication. However,\nthe foundational assumption that these signals provide sufficient entropy\nremains under-explored. In this work, we systematically analyse the entropy of\nmobile sensor data across four diverse datasets spanning multiple application\ncontexts. Our findings reveal pervasive biases, with single-sensor mean\nmin-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite\nShannon entropy being several multiples higher. We further demonstrate that\ncorrelations between sensor modalities reduce the worst-case entropy of using\nmultiple sensors by up to approx. 75% compared to average-case Shannon entropy.\nThis brings joint min-entropy well below 10 bits in many cases and, in the best\ncase, yielding only approx. 24 bits of min-entropy when combining 20 sensor\nmodalities. These results call into question the widely held assumption that\nadding more sensors inherently yields higher security. We ultimately caution\nagainst relying on raw sensor data as a primary source of randomness."
    },
    {
        "date": "2025-02",
        "title": "Robust Learning of Multi-index Models via Iterative Subspace Approximation",
        "author": "Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, and Nikos Zarifis",
        "link": "http://arxiv.org/abs/2502.09525v1",
        "abstract": "We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$."
    },
    {
        "date": "2025-02",
        "title": "Variable Stiffness for Robust Locomotion through Reinforcement Learning",
        "author": "Dario Spoljaric, Yashuai Yan, and Dongheui Lee",
        "link": "http://arxiv.org/abs/2502.09436v1",
        "abstract": "Reinforcement-learned locomotion enables legged robots to perform highly\ndynamic motions but often accompanies time-consuming manual tuning of joint\nstiffness. This paper introduces a novel control paradigm that integrates\nvariable stiffness into the action space alongside joint positions, enabling\ngrouped stiffness control such as per-joint stiffness (PJS), per-leg stiffness\n(PLS) and hybrid joint-leg stiffness (HJLS). We show that variable stiffness\npolicies, with grouping in per-leg stiffness (PLS), outperform position-based\ncontrol in velocity tracking and push recovery. In contrast, HJLS excels in\nenergy efficiency. Furthermore, our method showcases robust walking behaviour\non diverse outdoor terrains by sim-to-real transfer, although the policy is\nsorely trained on a flat floor. Our approach simplifies design by eliminating\nper-joint stiffness tuning while keeping competitive results with various\nmetrics."
    },
    {
        "date": "2025-02",
        "title": "Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes",
        "author": "Navdeep Kumar, Adarsh Gupta, Maxence Mohamed Elfatihi, Giorgia Ramponi, Kfir Yehuda Levy, and Shie Mannor",
        "link": "http://arxiv.org/abs/2502.09432v1",
        "abstract": "We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs."
    },
    {
        "date": "2025-02",
        "title": "A hierarchical approach for assessing the vulnerability of tree-based classification models to membership inference attack",
        "author": "Richard J. Preen, and Jim Smith",
        "link": "http://arxiv.org/abs/2502.09396v1",
        "abstract": "Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy."
    },
    {
        "date": "2025-02",
        "title": "Wasserstein distributional adversarial training for deep neural networks",
        "author": "Xingjian Bai, Guangyi He, Yifan Jiang, and Jan Obloj",
        "link": "http://arxiv.org/abs/2502.09352v1",
        "abstract": "Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images)."
    },
    {
        "date": "2025-02",
        "title": "LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection",
        "author": "Wenlun Zhang, Enyan Dai, and Kentaro Yoshioka",
        "link": "http://arxiv.org/abs/2502.09271v3",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method."
    },
    {
        "date": "2025-02",
        "title": "GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation",
        "author": "Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang",
        "link": "http://arxiv.org/abs/2502.09268v2",
        "abstract": "With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks."
    },
    {
        "date": "2025-02",
        "title": "DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in Segmenting Hemorrhagic Lesions from Fundus Images",
        "author": "Zesheng Li, Minwen Liao, Haoran Chen, Yan Su, Chengchang Pan, and Honggang Qi",
        "link": "http://arxiv.org/abs/2502.09256v1",
        "abstract": "The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]"
    },
    {
        "date": "2025-02",
        "title": "In Specs we Trust? Conformance-Analysis of Implementation to Specifications in Node-RED and Associated Security Risks",
        "author": "Simon Schneider, Komal Kashish, Katja Tuma, and Riccardo Scandariato",
        "link": "http://arxiv.org/abs/2502.09117v1",
        "abstract": "Low-code development frameworks for IoT platforms offer a simple\ndrag-and-drop mechanism to create applications for the billions of existing IoT\ndevices without the need for extensive programming knowledge. The security of\nsuch software is crucial given the close integration of IoT devices in many\nhighly sensitive areas such as healthcare or home automation. Node-RED is such\na framework, where applications are built from nodes that are contributed by\nopen-source developers. Its reliance on unvetted open-source contributions and\nlack of security checks raises the concern that the applications could be\nvulnerable to attacks, thereby imposing a security risk to end users. The\nlow-code approach suggests, that many users could lack the technical knowledge\nto mitigate, understand, or even realize such security concerns. This paper\nfocuses on \"hidden\" information flows in Node-RED nodes, meaning flows that are\nnot captured by the specifications. They could (unknowingly or with malicious\nintent) cause leaks of sensitive information to unauthorized entities. We\nreport the results of a conformance analysis of all nodes in the Node-RED\nframework, for which we compared the numbers of specified inputs and outputs of\neach node against the number of sources and sinks detected with CodeQL. The\nresults show, that 55% of all nodes exhibit more possible flows than are\nspecified. A risk assessment of a subset of the nodes showed, that 28% of them\nare associated with a high severity and 36% with a medium severity rating."
    },
    {
        "date": "2025-02",
        "title": "Pulling Back the Curtain: Unsupervised Adversarial Detection via Contrastive Auxiliary Networks",
        "author": "Eylon Mizrahi, Raz Lapid, and Moshe Sipper",
        "link": "http://arxiv.org/abs/2502.09110v1",
        "abstract": "Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems."
    },
    {
        "date": "2025-02",
        "title": "PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration",
        "author": "Jinhui Guo, Lubin Fan, Bojian Wu, Jiaqi Gu, Shen Cao, and Jieping Ye",
        "link": "http://arxiv.org/abs/2502.09075v1",
        "abstract": "In this paper, we present PTZ-Calib, a robust two-stage PTZ camera\ncalibration method, that efficiently and accurately estimates camera parameters\nfor arbitrary viewpoints. Our method includes an offline and an online stage.\nIn the offline stage, we first uniformly select a set of reference images that\nsufficiently overlap to encompass a complete 360{\\deg} view. We then utilize\nthe novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to\nautomatically calibrate the cameras within a local coordinate system.\nAdditionally, for practical application, we can further optimize camera\nparameters and align them with the geographic coordinate system using extra\nglobal reference 3D information. In the online stage, we formulate the\ncalibration of any new viewpoints as a relocalization problem. Our approach\nbalances the accuracy and computational efficiency to meet real-world demands.\nExtensive evaluations demonstrate our robustness and superior performance over\nstate-of-the-art methods on various real and synthetic datasets. Datasets and\nsource code can be accessed online at https://github.com/gjgjh/PTZ-Calib"
    }
]