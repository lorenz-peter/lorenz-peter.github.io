[
    {
        "date": "2025-03",
        "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
        "author": "Moussa Kassem Sbeyti, Nadja Klein, Azarm Nowzad, Fikret Sivrikaya, and Sahin Albayrak",
        "link": "http://arxiv.org/abs/2503.18903v1",
        "abstract": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications."
    },
    {
        "date": "2025-03",
        "title": "Secure Edge Computing Reference Architecture for Data-driven Structural Health Monitoring: Lessons Learned from Implementation and Benchmarking",
        "author": "Sheikh Muhammad Farjad, Sandeep Reddy Patllola, Yonas Kassa, George Grispos, and Robin Gandhi",
        "link": "http://arxiv.org/abs/2503.18857v1",
        "abstract": "Structural Health Monitoring (SHM) plays a crucial role in maintaining aging\nand critical infrastructure, supporting applications such as smart cities and\ndigital twinning. These applications demand machine learning models capable of\nprocessing large volumes of real-time sensor data at the network edge. However,\nexisting approaches often neglect the challenges of deploying machine learning\nmodels at the edge or are constrained by vendor-specific platforms. This paper\nintroduces a scalable and secure edge-computing reference architecture tailored\nfor data-driven SHM. We share practical insights from deploying this\narchitecture at the Memorial Bridge in New Hampshire, US, referred to as the\nLiving Bridge project. Our solution integrates a commercial data acquisition\nsystem with off-the-shelf hardware running an open-source edge-computing\nplatform, remotely managed and scaled through cloud services. To support the\ndevelopment of data-driven SHM systems, we propose a resource consumption\nbenchmarking framework called edgeOps to evaluate the performance of machine\nlearning models on edge devices. We study this framework by collecting resource\nutilization data for machine learning models typically used in SHM applications\non two different edge computing hardware platforms. edgeOps was specifically\nstudied on off-the-shelf Linux and ARM-based edge devices. Our findings\ndemonstrate the impact of platform and model selection on system performance,\nproviding actionable guidance for edge-based SHM system design."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
        "author": "Wenxi Chen, Raymond A. Yeh, Shaoshuai Mou, and Yan Gu",
        "link": "http://arxiv.org/abs/2503.18784v1",
        "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles",
        "author": "Der-Hau Lee",
        "link": "http://arxiv.org/abs/2503.18752v1",
        "abstract": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers."
    },
    {
        "date": "2025-03",
        "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting",
        "author": "Lijiang Li, Jinglu Wang, Xiang Ming, and Yan Lu",
        "link": "http://arxiv.org/abs/2503.18718v1",
        "abstract": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time."
    },
    {
        "date": "2025-03",
        "title": "Robust face recognition based on the wing loss and the $\\ell_1$ regularization",
        "author": "Yaoyao Yun, and Jianwen Xu",
        "link": "http://arxiv.org/abs/2503.18652v1",
        "abstract": "In recent years, sparse sampling techniques based on regression analysis have\nwitnessed extensive applications in face recognition research. Presently,\nnumerous sparse sampling models based on regression analysis have been explored\nby various researchers. Nevertheless, the recognition rates of the majority of\nthese models would be significantly decreased when confronted with highly\noccluded and highly damaged face images. In this paper, a new wing-constrained\nsparse coding model(WCSC) and its weighted version(WWCSC) are introduced, so as\nto deal with the face recognition problem in complex circumstances, where the\nalternating direction method of multipliers (ADMM) algorithm is employed to\nsolve the corresponding minimization problems. In addition, performances of the\nproposed method are examined based on the four well-known facial databases,\nnamely the ORL facial database, the Yale facial database, the AR facial\ndatabase and the FERET facial database. Also, compared to the other methods in\nthe literatures, the WWCSC has a very high recognition rate even in complex\nsituations where face images have high occlusion or high damage, which\nillustrates the robustness of the WWCSC method in facial recognition."
    },
    {
        "date": "2025-03",
        "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling",
        "author": "Kunyang Li, and Ming Hou",
        "link": "http://arxiv.org/abs/2503.18631v1",
        "abstract": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions."
    },
    {
        "date": "2025-03",
        "title": "Anchor-based oversampling for imbalanced tabular data via contrastive and adversarial learning",
        "author": "Hadi Mohammadi, Ehsan Nazerfard, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2503.18569v1",
        "abstract": "Imbalanced data represent a distribution with more frequencies of one class\n(majority) than the other (minority). This phenomenon occurs across various\ndomains, such as security, medical care and human activity. In imbalanced\nlearning, classification algorithms are typically inclined to classify the\nmajority class accurately, resulting in artificially high accuracy rates. As a\nresult, many minority samples are mistakenly labelled as majority-class\ninstances, resulting in a bias that benefits the majority class. This study\npresents a framework based on boundary anchor samples to tackle the imbalance\nlearning challenge. First, we select and use anchor samples to train a\nmultilayer perceptron (MLP) classifier, which acts as a prior knowledge model\nand aids the adversarial and contrastive learning procedures. Then, we designed\na novel deep generative model called Anchor Stabilized Conditional Generative\nAdversarial Network or Anch-SCGAN in short. Anch-SCGAN is supported with two\ngenerators for the minority and majority classes and a discriminator\nincorporating additional class-specific information from the pre-trained\nfeature extractor MLP. In addition, we facilitate the generator's training\nprocedure in two ways. First, we define a new generator loss function based on\nreprocessed anchor samples and contrastive learning. Second, we apply a scoring\nstrategy to stabilize the adversarial training part in generators. We train\nAnch-SCGAN and further finetune it with anchor samples to improve the precision\nof the generated samples. Our experiments on 16 real-world imbalanced datasets\nillustrate that Anch-SCGAN outperforms the renowned methods in imbalanced\nlearning."
    },
    {
        "date": "2025-03",
        "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
        "author": "Erjian Guo, Zhen Zhao, Zicheng Wang, Tong Chen, Yunyi Liu, and Luping Zhou",
        "link": "http://arxiv.org/abs/2503.18536v1",
        "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module."
    },
    {
        "date": "2025-03",
        "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
        "author": "Jiate Li, Meng Pang, Yun Dong, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.18503v1",
        "abstract": "Graph neural networks (GNNs) are becoming the de facto method to learn on the\ngraph data and have achieved the state-of-the-art on node and graph\nclassification tasks. However, recent works show GNNs are vulnerable to\ntraining-time poisoning attacks -- marginally perturbing edges, nodes, or/and\nnode features of training graph(s) can largely degrade GNNs' testing\nperformance. Most previous defenses against graph poisoning attacks are\nempirical and are soon broken by adaptive / stronger ones. A few provable\ndefenses provide robustness guarantees, but have large gaps when applied in\npractice: 1) restrict the attacker on only one type of perturbation; 2) design\nfor a particular GNN architecture or task; and 3) robustness guarantees are not\n100\\% accurate.\n  In this work, we bridge all these gaps by developing PGNNCert, the first\ncertified defense of GNNs against poisoning attacks under arbitrary (edge,\nnode, and node feature) perturbations with deterministic robustness guarantees.\nExtensive evaluations on multiple node and graph classification datasets and\nGNNs demonstrate the effectiveness of PGNNCert to provably defend against\narbitrary poisoning perturbations. PGNNCert is also shown to significantly\noutperform the state-of-the-art certified defenses against edge perturbation or\nnode perturbation during GNN training."
    },
    {
        "date": "2025-03",
        "title": "Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study",
        "author": "Xinggong Zhang, Qingyang Li, Yunpeng Tan, Zongming Guo, Lei Zhang, and Yong Cui",
        "link": "http://arxiv.org/abs/2503.18487v1",
        "abstract": "Network attack detection is a pivotal technology to identify network anomaly\nand classify malicious traffic. Large Language Models (LLMs) are trained on a\nvast corpus of text, have amassed remarkable capabilities of\ncontext-understanding and commonsense knowledge. This has opened up a new door\nfor network threat detection. Researchers have already initiated discussions\nregarding the application of LLMs on specific cyber-security tasks.\nUnfortunately, there is still a lack of comprehensive elaboration how to mine\nLLMs' potentials in network threat detections, as well as the opportunities and\nchallenges. In this paper, we mainly focus on the classification of malicious\ntraffic from the perspective of LLMs' capability. We present a holistic view of\nthe architecture of LLM-powered network attack detection, including\nPre-training, Fine-tuning, and Detection. Especially, by exploring the\nknowledge and capabilities of LLM, we identify three distinct roles LLM can act\nin network attack detection: \\textit{Classifier, Encoder, and Predictor}. For\neach of them, the modeling paradigm, opportunities and challenges are\nelaborated. Finally, we present our design on LLM-powered DDoS detection as a\ncase study. The proposed framework attains accurate detection on carpet bombing\nDDoS by exploiting LLMs' capabilities in contextual mining. The evaluation\nshows its efficacy, exhibiting a nearly $35$\\% improvement compared to existing\nsystems."
    },
    {
        "date": "2025-03",
        "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness",
        "author": "Chenfei Liao, Kaiyu Lei, Xu Zheng, Junha Moon, Zhixiong Wang, Yixuan Wang, Danda Pani Paudel, Luc Van Gool, and Xuming Hu",
        "link": "http://arxiv.org/abs/2503.18445v1",
        "abstract": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "author": "Wen Bai, Yi Wong, Xiao Qiao, and Chin Pang Ho",
        "link": "http://arxiv.org/abs/2503.18436v1",
        "abstract": "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity."
    },
    {
        "date": "2025-03",
        "title": "RoCA: Robust Contrastive One-class Time Series Anomaly Detection with Contaminated Data",
        "author": "Xudong Mou, Rui Wang, Bo Li, Tianyu Wo, Jie Sun, Hui Wang, and Xudong Liu",
        "link": "http://arxiv.org/abs/2503.18385v1",
        "abstract": "The accumulation of time-series signals and the absence of labels make\ntime-series Anomaly Detection (AD) a self-supervised task of deep learning.\nMethods based on normality assumptions face the following three limitations:\n(1) A single assumption could hardly characterize the whole normality or lead\nto some deviation. (2) Some assumptions may go against the principle of AD. (3)\nTheir basic assumption is that the training data is uncontaminated (free of\nanomalies), which is unrealistic in practice, leading to a decline in\nrobustness. This paper proposes a novel robust approach, RoCA, which is the\nfirst to address all of the above three challenges, as far as we are aware. It\nfuses the separated assumptions of one-class classification and contrastive\nlearning in a single training process to characterize a more complete so-called\nnormality. Additionally, it monitors the training data and computes a carefully\ndesigned anomaly score throughout the training process. This score helps\nidentify latent anomalies, which are then used to define the classification\nboundary, inspired by the concept of outlier exposure. The performance on AIOps\ndatasets improved by 6% compared to when contamination was not considered\n(COCA). On two large and high-dimensional multivariate datasets, the\nperformance increased by 5% to 10%. RoCA achieves the highest average\nperformance on both univariate and multivariate datasets. The source code is\navailable at https://github.com/ruiking04/RoCA."
    },
    {
        "date": "2025-03",
        "title": "Attacking and Improving the Tor Directory Protocol",
        "author": "Zhongtang Luo, Adithya Bhat, Kartik Nayak, and Aniket Kate",
        "link": "http://arxiv.org/abs/2503.18345v1",
        "abstract": "The Tor network enhances clients' privacy by routing traffic through an\noverlay network of volunteered intermediate relays. Tor employs a distributed\nprotocol among nine hard-coded Directory Authority (DA) servers to securely\ndisseminate information about these relays to produce a new consensus document\nevery hour. With a straightforward voting mechanism to ensure consistency, the\nprotocol is expected to be secure even when a minority of those authorities get\ncompromised. However, the current consensus protocol is flawed: it allows an\nequivocation attack that enables only a single compromised authority to create\na valid consensus document with malicious relays. Importantly the vulnerability\nis not innocuous: We demonstrate that the compromised authority can effectively\ntrick a targeted client into using the equivocated consensus document in an\nundetectable manner. Moreover, even if we have archived Tor consensus documents\navailable since its beginning, we cannot be sure that no client was ever\ntricked.\n  We propose a two-stage solution to deal with this exploit. In the short term,\nwe have developed and deployed TorEq, a monitor to detect such exploits\nreactively: the Tor clients can refer to the monitor before updating the\nconsensus to ensure no equivocation. To solve the problem proactively, we first\ndefine the Tor DA consensus problem as the interactive consistency (IC) problem\nfrom the distributed computing literature. We then design DirCast, a novel\nsecure Byzantine Broadcast protocol that requires minimal code change from the\ncurrent Tor DA code base. Our protocol has near-optimal efficiency that uses\noptimistically five rounds and at most nine rounds to reach an agreement in the\ncurrent nine-authority system. We are communicating with the Tor security team\nto incorporate the solutions into the Tor project."
    },
    {
        "date": "2025-03",
        "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
        "author": "Kazuma Kitazawa, Takahito Aoto, Satoshi Ikehata, and Tsuyoshi Takatani",
        "link": "http://arxiv.org/abs/2503.18341v1",
        "abstract": "Recently, the energy-efficient photometric stereo method using an event\ncamera has been proposed to recover surface normals from events triggered by\nchanges in logarithmic Lambertian reflections under a moving directional light\nsource. However, EventPS treats each event interval independently, making it\nsensitive to noise, shadows, and non-Lambertian reflections. This paper\nproposes Photometric Stereo based on Event Interval Profile (PS-EIP), a robust\nmethod that recovers pixelwise surface normals from a time-series profile of\nevent intervals. By exploiting the continuity of the profile and introducing an\noutlier detection method based on profile shape, our approach enhances\nrobustness against outliers from shadows and specular reflections. Experiments\nusing real event data from 3D-printed objects demonstrate that PS-EIP\nsignificantly improves robustness to outliers compared to EventPS's\ndeep-learning variant, EventPS-FCN, without relying on deep learning."
    },
    {
        "date": "2025-03",
        "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD",
        "author": "Paul K. Mandal",
        "link": "http://arxiv.org/abs/2503.18290v1",
        "abstract": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences."
    },
    {
        "date": "2025-03",
        "title": "Enhance GNNs with Reliable Confidence Estimation via Adversarial Calibration Learning",
        "author": "Yilong Wang, Jiahao Zhang, Tianxiang Zhao, and Suhang Wang",
        "link": "http://arxiv.org/abs/2503.18235v1",
        "abstract": "Despite their impressive predictive performance, GNNs often exhibit poor\nconfidence calibration, i.e., their predicted confidence scores do not\naccurately reflect true correctness likelihood. This issue raises concerns\nabout their reliability in high-stakes domains such as fraud detection, and\nrisk assessment, where well-calibrated predictions are essential for\ndecision-making. To ensure trustworthy predictions, several GNN calibration\nmethods are proposed. Though they can improve global calibration, our\nexperiments reveal that they often fail to generalize across different node\ngroups, leading to inaccurate confidence in node groups with different degree\nlevels, classes, and local structures. In certain cases, they even degrade\ncalibration compared to the original uncalibrated GNN. To address this\nchallenge, we propose a novel AdvCali framework that adaptively enhances\ncalibration across different node groups. Our method leverages adversarial\ntraining to automatically identify mis-calibrated node groups and applies a\ndifferentiable Group Expected Calibration Error (ECE) loss term to refine\nconfidence estimation within these groups. This allows the model to dynamically\nadjust its calibration strategy without relying on dataset-specific prior\nknowledge about miscalibrated subgroups. Extensive experiments on real-world\ndatasets demonstrate that our approach not only improves global calibration but\nalso significantly enhances calibration within groups defined by feature\nsimilarity, topology, and connectivity, outperforming previous methods and\ndemonstrating its effectiveness in practical scenarios."
    },
    {
        "date": "2025-03",
        "title": "Literature Review: Cyber Security Monitoring in Maritime",
        "author": "Risto Vaarandi, Leonidas Tsiopoulos, Gabor Visky, Muaan Ur Rehman, and Hayretdin Bahsi",
        "link": "http://arxiv.org/abs/2503.18173v1",
        "abstract": "In recent years, many cyber incidents have happened in the maritime sector,\ntargeting the information technology (IT) and operational technology (OT)\ninfrastructure. Although several systematization-of-knowledge papers have been\npublished in the maritime field, none of the previous studies has focused on\ncyber security monitoring, which aims at timely detection of cyber attacks with\nautomated methods. The current article addresses this research gap and surveys\nthe methods, algorithms, tools and architectures used for cyber security\nmonitoring in the maritime sector. For the survey, a systematic literature\nreview of cyber security monitoring studies is conducted in this article,\nfollowing the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) protocol. The first contribution of this article is the\nbibliometric analysis of related literature and the identification of the main\nresearch themes in previous works. For that purpose, our article presents a\ntaxonomy for existing studies which highlights the main properties of maritime\ncyber security monitoring research. The second contribution of this article is\nan in-depth analysis of previous works and the identification of research gaps\nand limitations in existing literature. Based on our findings, we outline\nfuture research directions for cyber security monitoring in the maritime field."
    },
    {
        "date": "2025-03",
        "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models",
        "author": "Chenyu Zhang, Yiwen Ma, Lanjun Wang, Wenhui Li, Yi Tu, and An-An Liu",
        "link": "http://arxiv.org/abs/2503.17987v1",
        "abstract": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}"
    },
    {
        "date": "2025-03",
        "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
        "author": "Dong Zhao, Jinlong Li, Shuang Wang, Mengyao Wu, Qi Zang, Nicu Sebe, and Zhun Zhong",
        "link": "http://arxiv.org/abs/2503.17940v1",
        "abstract": "Vision Foundation Models (VFMs) excel in generalization due to large-scale\npretraining, but fine-tuning them for Domain Generalized Semantic Segmentation\n(DGSS) while maintaining this ability remains challenging. Existing approaches\neither selectively fine-tune parameters or freeze the VFMs and update only the\nadapters, both of which may underutilize the VFMs' full potential in DGSS\ntasks. We observe that domain-sensitive parameters in VFMs, arising from task\nand distribution differences, can hinder generalization. To address this, we\npropose \\textbf{FisherTune}, a robust fine-tuning method guided by the\nDomain-Related Fisher Information Matrix (DR-FIM). DR-FIM measures parameter\nsensitivity across tasks and domains, enabling selective updates that preserve\ngeneralization and enhance DGSS adaptability. FisherTune incorporates\nvariational inference to stabilize DR-FIM estimation, treating parameters as\nGaussian-distributed variables and leveraging pre-trained priors. Extensive\nexperiments show that FisherTune achieves superior cross-domain segmentation\nwhile maintaining generalization, outperforming selective-parameter and\nadapter-based methods."
    },
    {
        "date": "2025-03",
        "title": "Understanding and Mitigating Side and Covert Channel Vulnerabilities Introduced by RowHammer Defenses",
        "author": "F. Nisa Bostanc\u0131, O\u011fuzhan Canpolat, Ataberk Olgun, \u0130smail Emir Y\u00fcksel, Mohammad Sadrosadati, A. Giray Ya\u011fl\u0131k\u00e7\u0131, and Onur Mutlu",
        "link": "http://arxiv.org/abs/2503.17891v1",
        "abstract": "DRAM chips are vulnerable to read disturbance phenomena (e.g., RowHammer and\nRowPress), where repeatedly accessing or keeping open a DRAM row causes\nbitflips in nearby rows, due to DRAM density scaling. Attackers can leverage\nRowHammer bitflips in real systems to take over systems and leak data.\nConsequently, many prior works propose mitigations, including recent DDR\nspecifications introducing new mitigation frameworks (e.g., PRAC and RFM). For\nrobustness, it is timely and critical to analyze other security implications\nthat widely-adopted RowHammer mitigations can introduce. Unfortunately, no\nprior work analyzes the timing channel vulnerabilities introduced by RowHammer\nmitigations. In this work, we present the first analysis and evaluation of\ntiming channel vulnerabilities introduced by RowHammer mitigations. Our key\nobservation is that RowHammer mitigations' preventive actions have two features\nthat enable timing channels. First, preventive actions often reduce DRAM\nbandwidth availability because they block access to DRAM, thereby delaying\nregular memory requests and resulting in increased memory latencies. Second,\npreventive actions can be triggered on demand as they depend on memory access\npatterns. We systematically analyze two latest industry mitigations and\nintroduce LeakyHammer, a new class of attacks that leverage the RowHammer\nmitigation-induced memory latency differences to establish communication\nchannels between processes and leak secrets. First, we build two covert channel\nattacks exploiting two state-of-the-art RowHammer mitigations, providing 41.9\nKbps and 54.0 Kbps channel capacity. Second, we demonstrate a proof-of-concept\nwebsite fingerprinting attack that can identify visited websites based on the\nRowHammer mitigation behavior. We discuss 3 mitigations against LeakyHammer and\nshow that fundamentally mitigating LeakyHammer induces significant performance\noverheads."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Mitigating DDoS Attacks with AI: A Survey",
        "author": "Alexandru Apostu, Silviu Gheorghe, Andrei H\u00eeji, Nicolae Cleju, Andrei P\u0103tra\u015fcu, Cristian Rusu, Radu Ionescu, and Paul Irofti",
        "link": "http://arxiv.org/abs/2503.17867v1",
        "abstract": "Distributed Denial of Service attacks represent an active cybersecurity\nresearch problem. Recent research shifted from static rule-based defenses\ntowards AI-based detection and mitigation. This comprehensive survey covers\nseveral key topics. Preeminently, state-of-the-art AI detection methods are\ndiscussed. An in-depth taxonomy based on manual expert hierarchies and an\nAI-generated dendrogram are provided, thus settling DDoS categorization\nambiguities. An important discussion on available datasets follows, covering\ndata format options and their role in training AI detection methods together\nwith adversarial training and examples augmentation. Beyond detection, AI based\nmitigation techniques are surveyed as well. Finally, multiple open research\ndirections are proposed."
    },
    {
        "date": "2025-03",
        "title": "NVBleed: Covert and Side-Channel Attacks on NVIDIA Multi-GPU Interconnect",
        "author": "Yicheng Zhang, Ravan Nazaraliyev, Sankha Baran Dutta, Andres Marquez, Kevin Barker, and Nael Abu-Ghazaleh",
        "link": "http://arxiv.org/abs/2503.17847v1",
        "abstract": "Multi-GPU systems are becoming increasingly important in highperformance\ncomputing (HPC) and cloud infrastructure, providing acceleration for\ndata-intensive applications, including machine learning workloads. These\nsystems consist of multiple GPUs interconnected through high-speed networking\nlinks such as NVIDIA's NVLink. In this work, we explore whether the\ninterconnect on such systems can offer a novel source of leakage, enabling new\nforms of covert and side-channel attacks. Specifically, we reverse engineer the\noperations of NVlink and identify two primary sources of leakage: timing\nvariations due to contention and accessible performance counters that disclose\ncommunication patterns. The leakage is visible remotely and even across VM\ninstances in the cloud, enabling potentially dangerous attacks. Building on\nthese observations, we develop two types of covert-channel attacks across two\nGPUs, achieving a bandwidth of over 70 Kbps with an error rate of 4.78% for the\ncontention channel. We develop two end-to-end crossGPU side-channel attacks:\napplication fingerprinting (including 18 high-performance computing and deep\nlearning applications) and 3D graphics character identification within Blender,\na multi-GPU rendering application. These attacks are highly effective,\nachieving F1 scores of up to 97.78% and 91.56%, respectively. We also discover\nthat leakage surprisingly occurs across Virtual Machines on the Google Cloud\nPlatform (GCP) and demonstrate a side-channel attack on Blender, achieving F1\nscores exceeding 88%. We also explore potential defenses such as managing\naccess to counters and reducing the resolution of the clock to mitigate the two\nsources of leakage."
    },
    {
        "date": "2025-03",
        "title": "Connectedness: a dimension of security bug severity assessment for measuring uncertainty",
        "author": "Shue Long Chan",
        "link": "http://arxiv.org/abs/2503.17813v1",
        "abstract": "Current frameworks for evaluating security bug severity, such as the Common\nVulnerability Scoring System (CVSS), prioritize the ratio of exploitability to\nimpact. This paper suggests that the above approach measures the \"known knowns\"\nbut inadequately addresses the \"known unknowns\" especially when there exist\nmultiple possible exploit paths and side effects, which introduce significant\nuncertainty. This paper introduces the concept of connectedness, which measures\nhow strongly a security bug is connected with different entities, thereby\nreflecting the uncertainty of impact and the exploit potential. This work\nhighlights the critical but underappreciated role connectedness plays in\nseverity assessments."
    },
    {
        "date": "2025-03",
        "title": "Design and implementation of a novel cryptographically secure pseudorandom number generator",
        "author": "Juan Di Mauro, Eduardo Salazar, and Hugo D. Scolnik",
        "link": "http://arxiv.org/abs/2503.17767v1",
        "abstract": "The aim of this paper is to present a new design for a pseudorandom number\ngenerator (PRNG) that is cryptographically secure, passes all of the usual\nstatistical tests referenced in the literature and hence generates high quality\nrandom sequences, that is compact and easy to implement in practice, of\nportable design and offering reasonable execution times. Our procedure achieves\nthose objectives through the use of a sequence of modular exponentiations\nfollowed by the application of Feistel-like boxes that mix up bits using a\nnonlinear function. The results of extensive statistical tests on sequences of\nabout 2^40 bits in size generated by our algorithm are also presented."
    },
    {
        "date": "2025-03",
        "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "author": "Jie Zhang, Zhongqi Wang, Shiguang Shan, and Xilin Chen",
        "link": "http://arxiv.org/abs/2503.17724v1",
        "abstract": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA."
    },
    {
        "date": "2025-03",
        "title": "Measuring the Robustness of Audio Deepfake Detectors",
        "author": "Xiang Li, Pin-Yu Chen, and Wenqi Wei",
        "link": "http://arxiv.org/abs/2503.17577v1",
        "abstract": "Deepfakes have become a universal and rapidly intensifying concern of\ngenerative AI across various media types such as images, audio, and videos.\nAmong these, audio deepfakes have been of particular concern due to the ease of\nhigh-quality voice synthesis and distribution via platforms such as social\nmedia and robocalls. Consequently, detecting audio deepfakes plays a critical\nrole in combating the growing misuse of AI-synthesized speech. However,\nreal-world scenarios often introduce various audio corruptions, such as noise,\nmodification, and compression, that may significantly impact detection\nperformance. This work systematically evaluates the robustness of 10 audio\ndeepfake detection models against 16 common corruptions, categorized into noise\nperturbation, audio modification, and compression. Using both traditional deep\nlearning models and state-of-the-art foundation models, we make four unique\nobservations. First, our findings show that while most models demonstrate\nstrong robustness to noise, they are notably more vulnerable to modifications\nand compression, especially when neural codecs are applied. Second, speech\nfoundation models generally outperform traditional models across most\nscenarios, likely due to their self-supervised learning paradigm and\nlarge-scale pre-training. Third, our results show that increasing model size\nimproves robustness, albeit with diminishing returns. Fourth, we demonstrate\nhow targeted data augmentation during training can enhance model resilience to\nunseen perturbations. A case study on political speech deepfakes highlights the\neffectiveness of foundation models in achieving high accuracy under real-world\nconditions. These findings emphasize the importance of developing more robust\ndetection frameworks to ensure reliability in practical deployment settings."
    },
    {
        "date": "2025-03",
        "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography",
        "author": "Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, and Hassan Mansour",
        "link": "http://arxiv.org/abs/2503.17351v1",
        "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."
    },
    {
        "date": "2025-03",
        "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
        "author": "John Naulty, Eason Chen, Joy Wang, George Digkas, and Kostas Chalkias",
        "link": "http://arxiv.org/abs/2503.17302v1",
        "abstract": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
    },
    {
        "date": "2025-03",
        "title": "UAV Resilience Against Stealthy Attacks",
        "author": "Arthur Amorim, Max Taylor, Trevor Kann, Gary T. Leavens, William L. Harrison, and Lance Joneckis",
        "link": "http://arxiv.org/abs/2503.17298v1",
        "abstract": "Unmanned aerial vehicles (UAVs) depend on untrusted software components to\nautomate dangerous or critical missions, making them a desirable target for\nattacks. Some work has been done to prevent an attacker who has either\ncompromised a ground control station or parts of a UAV's software from\nsabotaging the vehicle, but not both. We present an architecture running a UAV\nsoftware stack with runtime monitoring and seL4-based software isolation that\nprevents attackers from both exploiting software bugs and utilizing stealthy\nattacks. Our architecture retrofits legacy UAVs and secures the popular MAVLink\nprotocol, making wide adoption possible."
    },
    {
        "date": "2025-03",
        "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators",
        "author": "Jan Rabenseifner, Sven Klaassen, Jannis Kueck, and Philipp Bach",
        "link": "http://arxiv.org/abs/2503.17290v1",
        "abstract": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
    },
    {
        "date": "2025-03",
        "title": "Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies",
        "author": "Ronan Mouchoux, and Fran\u00e7ois Moerman",
        "link": "http://arxiv.org/abs/2503.17219v1",
        "abstract": "This paper introduces a novel mathematical framework for analyzing cyber\nthreat campaigns through fractal geometry. By conceptualizing hierarchical\ntaxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics,\ntechniques, and sub-techniques forming concentric layers, we establish a\nrigorous method for campaign comparison using Hutchinson's Theorem and\nHausdorff distance metrics. Evaluation results confirm that our fractal\nrepresentation preserves hierarchical integrity while providing a\ndimensionality-based complexity assessment that correlates with campaign\ncomplexity. The proposed methodology bridges taxonomy-driven cyber threat\nanalysis and computational geometry, providing analysts with both mathematical\nrigor and interpretable visualizations for addressing the growing complexity of\nadversarial operations across multiple threat domains."
    },
    {
        "date": "2025-03",
        "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization",
        "author": "Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, and Wang Lu",
        "link": "http://arxiv.org/abs/2503.17211v1",
        "abstract": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels."
    },
    {
        "date": "2025-03",
        "title": "Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability",
        "author": "Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, and Ada Sedova",
        "link": "http://arxiv.org/abs/2503.17173v1",
        "abstract": "The ability of machine learning (ML) classification models to resist small,\ntargeted input perturbations - known as adversarial attacks - is a key measure\nof their safety and reliability. We show that floating-point non associativity\n(FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to\nresult in misclassification, without any perturbation to the input.\nAdditionally, we show this misclassification is particularly significant for\ninputs close to the decision boundary and that standard adversarial robustness\nresults may be overestimated up to 4.6% when not considering machine-level\ndetails. We first study a linear classifier, before focusing on standard Graph\nNeural Network (GNN) architectures and datasets. We present a novel black-box\nattack using Bayesian optimization to determine external workloads that bias\nthe output of reductions on GPUs and reliably lead to misclassification.\nMotivated by these results, we present a new learnable permutation (LP)\ngradient-based approach, to learn floating point operation orderings that lead\nto misclassifications, making the assumption that any reduction or permutation\nordering is possible. This LP approach provides a worst-case estimate in a\ncomputationally efficient manner, avoiding the need to run identical\nexperiments tens of thousands of times over a potentially large set of possible\nGPU states or architectures. Finally, we investigate parallel reduction\nordering across different GPU architectures for a reduction under three\nconditions: (1) executing external background workloads, (2) utilizing\nmulti-GPU virtualization, and (3) applying power capping. Our results\ndemonstrate that parallel reduction ordering varies significantly across\narchitectures under the first two conditions. The results and methods developed\nhere can help to include machine-level considerations into adversarial\nrobustness assessments."
    },
    {
        "date": "2025-03",
        "title": "Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers",
        "author": "Gaojie Jin, Tianjin Huang, Ronghui Mu, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2503.17172v1",
        "abstract": "Recent studies have identified a critical challenge in deep neural networks\n(DNNs) known as ``robust fairness\", where models exhibit significant\ndisparities in robust accuracy across different classes. While prior work has\nattempted to address this issue in adversarial robustness, the study of\nworst-class certified robustness for smoothed classifiers remains unexplored.\nOur work bridges this gap by developing a PAC-Bayesian bound for the\nworst-class error of smoothed classifiers. Through theoretical analysis, we\ndemonstrate that the largest eigenvalue of the smoothed confusion matrix\nfundamentally influences the worst-class error of smoothed classifiers. Based\non this insight, we introduce a regularization method that optimizes the\nlargest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy\nof the smoothed classifier and further improve its worst-class certified\nrobustness. We provide extensive experimental validation across multiple\ndatasets and model architectures to demonstrate the effectiveness of our\napproach."
    },
    {
        "date": "2025-03",
        "title": "Generative adversarial framework to calibrate excursion set models for the 3D morphology of all-solid-state battery cathodes",
        "author": "Orkun Furat, Sabrina Weber, Johannes Schubert, Ren\u00e9 Rekers, Maximilian Luczak, Erik Glatt, Andreas Wiegmann, J\u00fcrgen Janek, Anja Bielefeld, and Volker Schmidt",
        "link": "http://arxiv.org/abs/2503.17171v1",
        "abstract": "This paper presents a computational method for generating virtual 3D\nmorphologies of functional materials using low-parametric stochastic geometry\nmodels, i.e., digital twins, calibrated with 2D microscopy images. These\ndigital twins allow systematic parameter variations to simulate various\nmorphologies, that can be deployed for virtual materials testing by means of\nspatially resolved numerical simulations of macroscopic properties. Generative\nadversarial networks (GANs) have gained popularity for calibrating models to\ngenerate realistic 3D morphologies. However, GANs often comprise of numerous\nuninterpretable parameters make systematic variation of morphologies for\nvirtual materials testing challenging. In contrast, low-parametric stochastic\ngeometry models (e.g., based on Gaussian random fields) enable targeted\nvariation but may struggle to mimic complex morphologies. Combining GANs with\nadvanced stochastic geometry models (e.g., excursion sets of more general\nrandom fields) addresses these limitations, allowing model calibration solely\nfrom 2D image data. This approach is demonstrated by generating a digital twin\nof all-solid-state battery (ASSB) cathodes. Since the digital twins are\nparametric, they support systematic exploration of structural scenarios and\ntheir macroscopic properties. The proposed method facilitates simulation\nstudies for optimizing 3D morphologies, benefiting not only ASSB cathodes but\nalso other materials with similar structures."
    },
    {
        "date": "2025-03",
        "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving",
        "author": "Alexandra Arzberger, and Ramin Tavakoli Kolagari",
        "link": "http://arxiv.org/abs/2503.17168v1",
        "abstract": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults."
    },
    {
        "date": "2025-03",
        "title": "HiFi-Stream: Streaming Speech Enhancement with Generative Adversarial Networks",
        "author": "Ekaterina Dmitrieva, and Maksim Kaledin",
        "link": "http://arxiv.org/abs/2503.17141v1",
        "abstract": "Speech Enhancement techniques have become core technologies in mobile devices\nand voice software simplifying downstream speech tasks. Still, modern Deep\nLearning (DL) solutions often require high amount of computational resources\nwhat makes their usage on low-resource devices challenging. We present\nHiFi-Stream, an optimized version of recently published HiFi++ model. Our\nexperiments demonstrate that HiFiStream saves most of the qualities of the\noriginal model despite its size and computational complexity: the lightest\nversion has only around 490k parameters which is 3.5x reduction in comparison\nto the original HiFi++ making it one of the smallest and fastest models\navailable. The model is evaluated in streaming setting where it demonstrates\nits superior performance in comparison to modern baselines."
    },
    {
        "date": "2025-03",
        "title": "EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations",
        "author": "Tadeu Freitas, Erick Silva, Rehana Yasmin, Ali Shoker, Manuel E. Correia, Rolando Martins, and Paulo Esteves-Verissimo",
        "link": "http://arxiv.org/abs/2503.16984v1",
        "abstract": "Vehicle cybersecurity has emerged as a critical concern, driven by the\ninnovation in the automotive industry, e.g., automomous, electric, or\nconnnected vehicles. Current efforts to address these challenges are\nconstrained by the limited computational resources of vehicles and the reliance\non connected infrastructures. This motivated the foundation of Vehicle Security\nOperations Centers (VSOCs) that extend IT-based Security Operations Centers\n(SOCs) to cover the entire automotive ecosystem, both the in-vehicle and\noff-vehicle scopes. Security Orchestration, Automation, and Response (SOAR)\ntools are considered key for impelementing an effective cybersecurity solution.\nHowever, existing state-of-the-art solutions depend on infrastructure networks\nsuch as 4G, 5G, and WiFi, which often face scalability and congestion issues.\nTo address these limitations, we propose a novel SOAR architecture EVSOAR that\nleverages the EV charging stations for connectivity and computing to enhance\nvehicle cybersecurity. Our EV-specific SOAR architecture enables real-time\nanalysis and automated responses to cybersecurity threats closer to the EV,\nreducing the cellular latency, bandwidth, and interference limitations. Our\nexperimental results demonstrate a significant improvement in latency,\nstability, and scalability through the infrastructure and the capacity to\ndeploy computationally intensive applications, that are otherwise infeasible\nwithin the resource constraints of individual vehicles."
    },
    {
        "date": "2025-03",
        "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision",
        "author": "Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, and Hang Su",
        "link": "http://arxiv.org/abs/2503.16975v1",
        "abstract": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."
    },
    {
        "date": "2025-03",
        "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
        "author": "Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, and Yi Yang",
        "link": "http://arxiv.org/abs/2503.16964v1",
        "abstract": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery."
    },
    {
        "date": "2025-03",
        "title": "CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks",
        "author": "Lei Chong",
        "link": "http://arxiv.org/abs/2503.16950v1",
        "abstract": "Stack-based memory corruption vulnerabilities have\n  long been exploited by attackers to execute arbitrary code\n  or perform unauthorized memory operations. Various defense\n  mechanisms have been introduced to mitigate stack memory\n  errors, but they typically focus on specific attack types, incur\n  substantial performance overhead, or suffer from compatibility\n  limitations.In this paper, we present CleanStack, an efficient,\n  highly compatible, and comprehensive stack protection mech anism. CleanStack\nisolates stack objects influenced by external\n  input from other safe stack objects, thereby preventing attackers\n  from modifying return addresses via controlled stack objects.\n  Additionally, by randomizing the placement of tainted stack\n  objects within the Unclean Stack, CleanStack mitigates non control data\nattacks by preventing attackers from predicting the\n  stack layout.A key component of CleanStack is the identifica tion of tainted\nstack objects. We analyze both static program\n  analysis and heuristic methods for this purpose. To maximize\n  compatibility, we adopt a heuristic approach and implement\n  CleanStack within the LLVM compiler framework, applying it to\n  SPEC CPU2017 benchmarks and a real-world application.Our\n  security evaluation demonstrates that CleanStack significantly\n  reduces the exploitability of stack-based memory errors by\n  providing a dual-stack system with isolation and randomization.\n  Performance evaluation results indicate that CleanStack incurs\n  an execution overhead of only 1.73% on the SPEC CPU2017\n  benchmark while introducing a minimal memory overhead of\n  just 0.04%. Compared to existing stack protection techniques,\n  CleanStack achieves an optimal balance between protection\n  coverage, runtime overhead, and compatibility, making it one\n  of the most comprehensive and efficient stack security solutions\n  to date."
    },
    {
        "date": "2025-03",
        "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI",
        "author": "Shayne Longpre, Kevin Klyman, Ruth E. Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M. Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca S. Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, and Arvind Narayanan",
        "link": "http://arxiv.org/abs/2503.16861v2",
        "abstract": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
    },
    {
        "date": "2025-03",
        "title": "Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic",
        "author": "Yali Yuan, Qianqi Niu, and Yachao Yuan",
        "link": "http://arxiv.org/abs/2503.16847v1",
        "abstract": "Flow correlation attacks is an efficient network attacks, aiming to expose\nthose who use anonymous network services, such as Tor. Conducting such attacks\nduring the early stages of network communication is particularly critical for\nscenarios demanding rapid decision-making, such as cybercrime detection or\nfinancial fraud prevention. Although recent studies have made progress in flow\ncorrelation attacks techniques, research specifically addressing flow\ncorrelation with early network traffic flow remains limited. Moreover, due to\nfactors such as model complexity, training costs, and real-time requirements,\nexisting technologies cannot be directly applied to flow correlation with early\nnetwork traffic flow. In this paper, we propose flow correlation attack with\nearly network traffic, named Early-MFC, based on multi-view triplet networks.\nThe proposed approach extracts multi-view traffic features from the payload at\nthe transport layer and the Inter-Packet Delay. It then integrates multi-view\nflow information, converting the extracted features into shared embeddings. By\nleveraging techniques such as metric learning and contrastive learning, the\nmethod optimizes the embeddings space by ensuring that similar flows are mapped\ncloser together while dissimilar flows are positioned farther apart. Finally,\nBayesian decision theory is applied to determine flow correlation, enabling\nhigh-accuracy flow correlation with early network traffic flow. Furthermore, we\ninvestigate flow correlation attacks under extra-early network traffic flow\nconditions. To address this challenge, we propose Early-MFC+, which utilizes\npayload data to construct embedded feature representations, ensuring robust\nperformance even with minimal packet availability."
    },
    {
        "date": "2025-03",
        "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization",
        "author": "Massa Baali, Xiang Li, Hao Chen, Rita Singh, and Bhiksha Raj",
        "link": "http://arxiv.org/abs/2503.16718v1",
        "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released."
    },
    {
        "date": "2025-03",
        "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents",
        "author": "Mihaela-Larisa Clement, M\u00f3nika Farsang, Felix Resch, and Radu Grosu",
        "link": "http://arxiv.org/abs/2503.16711v1",
        "abstract": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task."
    },
    {
        "date": "2025-03",
        "title": "ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks",
        "author": "Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, and Yushun Dong",
        "link": "http://arxiv.org/abs/2503.16693v1",
        "abstract": "Graph Neural Networks (GNNs) have gained traction in Graph-based Machine\nLearning as a Service (GMLaaS) platforms, yet they remain vulnerable to\ngraph-based model extraction attacks (MEAs), where adversaries reconstruct\nsurrogate models by querying the victim model. Existing defense mechanisms,\nsuch as watermarking and fingerprinting, suffer from poor real-time\nperformance, susceptibility to evasion, or reliance on post-attack\nverification, making them inadequate for handling the dynamic characteristics\nof graph-based MEA variants. To address these limitations, we propose ATOM, a\nnovel real-time MEA detection framework tailored for GNNs. ATOM integrates\nsequential modeling and reinforcement learning to dynamically detect evolving\nattack patterns, while leveraging $k$-core embedding to capture the structural\nproperties, enhancing detection precision. Furthermore, we provide theoretical\nanalysis to characterize query behaviors and optimize detection strategies.\nExtensive experiments on multiple real-world datasets demonstrate that ATOM\noutperforms existing approaches in detection performance, maintaining stable\nacross different time steps, thereby offering a more effective defense\nmechanism for GMLaaS environments."
    },
    {
        "date": "2025-03",
        "title": "Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity",
        "author": "Qiankun Shi, Jie Peng, Kun Yuan, Xiao Wang, and Qing Ling",
        "link": "http://arxiv.org/abs/2503.16337v1",
        "abstract": "In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight."
    },
    {
        "date": "2025-03",
        "title": "Securing Satellite Communications: Real-Time Video Encryption Scheme on Satellite Payloads",
        "author": "Hanshuo Qiu, Jing Lian, Xiaoyuan Wang, and Jizhao Liu",
        "link": "http://arxiv.org/abs/2503.16287v1",
        "abstract": "The rapid development of low-Earth orbit (LEO) satellite constellations and\nsatellite communication systems has elevated the importance of secure video\ntransmission, which is the key to applications such as remote sensing, disaster\nrelief, and secure information exchange. In this context, three serious issues\narise concerning real-time encryption of videos on satellite embedded devices:\n(a) the challenge of achieving real-time performance; (b) the limitations posed\nby the constrained computing performance of satellite payloads; and (c) the\npotential for excessive power consumption leading to overheating, thereby\nescalating safety risks. To overcome these challenges, this study introduced a\nnovel approach for encrypting videos by employing two 1D chaotic maps, which\nwas deployed on a satellite for the first time. The experiment on the satellite\nconfirms that our scheme is suitable for complex satellite environments. In\naddition, the proposed chaotic maps were implemented on a Field Programmable\nGate Array (FPGA) platform, and simulation results showed consistency with\nthose obtained on a Raspberry Pi. Experiments on the Raspberry Pi 4B\ndemonstrate exceptional real-time performance and low power consumption,\nvalidating both the hardware feasibility and the stability of our design.\nRigorous statistical testing also confirms the scheme's resilience against a\nvariety of attacks, underscoring its potential for secure, real-time data\ntransmission in satellite communication systems."
    },
    {
        "date": "2025-03",
        "title": "Rethinking Robustness in Machine Learning: A Posterior Agreement Approach",
        "author": "Jo\u00e3o Borges S. Carvalho, Alessandro Torcinovich, Victor Jimenez Rodriguez, Antonio E. Cin\u00e0, Carlos Cotrini, Lea Sch\u00f6nherr, and Joachim M. Buhmann",
        "link": "http://arxiv.org/abs/2503.16271v1",
        "abstract": "The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations."
    },
    {
        "date": "2025-03",
        "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
        "author": "Ziang Li, Hongguang Zhang, Juan Wang, Meihui Chen, Hongxin Hu, Wenzhe Yi, Xiaoyang Xu, Mengda Yang, and Chenjun Ma",
        "link": "http://arxiv.org/abs/2503.16266v1",
        "abstract": "Model Inversion Attacks (MIAs) aim to reconstruct private training data from\nmodels, leading to privacy leakage, particularly in facial recognition systems.\nAlthough many studies have enhanced the effectiveness of white-box MIAs, less\nattention has been paid to improving efficiency and utility under limited\nattacker capabilities. Existing black-box MIAs necessitate an impractical\nnumber of queries, incurring significant overhead. Therefore, we analyze the\nlimitations of existing MIAs and introduce Surrogate Model-based Inversion with\nLong-tailed Enhancement (SMILE), a high-resolution oriented and query-efficient\nMIA for the black-box setting. We begin by analyzing the initialization of MIAs\nfrom a data distribution perspective and propose a long-tailed surrogate\ntraining method to obtain high-quality initial points. We then enhance the\nattack's effectiveness by employing the gradient-free black-box optimization\nalgorithm selected by NGOpt. Our experiments show that SMILE outperforms\nexisting state-of-the-art black-box MIAs while requiring only about 5% of the\nquery overhead."
    },
    {
        "date": "2025-03",
        "title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet",
        "author": "Atharv Singh Patlan, Peiyao Sheng, S. Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath",
        "link": "http://arxiv.org/abs/2503.16248v1",
        "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible."
    },
    {
        "date": "2025-03",
        "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training",
        "author": "Fatemeh Amerehi, and Patrick Healy",
        "link": "http://arxiv.org/abs/2503.16179v1",
        "abstract": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."
    },
    {
        "date": "2025-03",
        "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation",
        "author": "Alex-Razvan Ispas, Charles-Elie Simon, Fabien Caspani, and Vincent Guigue",
        "link": "http://arxiv.org/abs/2503.16161v1",
        "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."
    },
    {
        "date": "2025-03",
        "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge",
        "author": "Marek Wodzinski, and Henning M\u00fcller",
        "link": "http://arxiv.org/abs/2503.16075v1",
        "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."
    },
    {
        "date": "2025-03",
        "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic",
        "author": "Bisola Faith Kayode, Akinyemi Sadeeq Akintola, Oluwole Fagbohun, Egonna Anaesiuba-Bristol, Onyekachukwu Ojumah, Oluwagbade Odimayo, Toyese Oloyede, Aniema Inyang, Teslim Kazeem, Habeeb Alli, Udodirim Ibem Offia, and Prisca Chinazor Amajuoyi",
        "link": "http://arxiv.org/abs/2503.16047v2",
        "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
    },
    {
        "date": "2025-03",
        "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
        "author": "Zenghui Yuan, Jiawen Shi, Pan Zhou, Neil Zhenqiang Gong, and Lichao Sun",
        "link": "http://arxiv.org/abs/2503.16023v1",
        "abstract": "Multi-modal large language models (MLLMs) extend large language models (LLMs)\nto process multi-modal information, enabling them to generate responses to\nimage-text inputs. MLLMs have been incorporated into diverse multi-modal\napplications, such as autonomous driving and medical diagnosis, via\nplug-and-play without fine-tuning. This deployment paradigm increases the\nvulnerability of MLLMs to backdoor attacks. However, existing backdoor attacks\nagainst MLLMs achieve limited effectiveness and stealthiness. In this work, we\npropose BadToken, the first token-level backdoor attack to MLLMs. BadToken\nintroduces two novel backdoor behaviors: Token-substitution and Token-addition,\nwhich enable flexible and stealthy attacks by making token-level modifications\nto the original output for backdoored inputs. We formulate a general\noptimization problem that considers the two backdoor behaviors to maximize the\nattack effectiveness. We evaluate BadToken on two open-source MLLMs and various\ntasks. Our results show that our attack maintains the model's utility while\nachieving high attack success rates and stealthiness. We also show the\nreal-world threats of BadToken in two scenarios, i.e., autonomous driving and\nmedical diagnosis. Furthermore, we consider defenses including fine-tuning and\ninput purification. Our results highlight the threat of our attack."
    },
    {
        "date": "2025-03",
        "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
        "author": "Junsung Park, Hwijeong Lee, Inha Kang, and Hyunjung Shim",
        "link": "http://arxiv.org/abs/2503.15910v2",
        "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness."
    },
    {
        "date": "2025-03",
        "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
        "author": "Andy Zhou, Kevin Wu, Francesco Pinto, Zhaorun Chen, Yi Zeng, Yu Yang, Shuang Yang, Sanmi Koyejo, James Zou, and Bo Li",
        "link": "http://arxiv.org/abs/2503.15754v1",
        "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems."
    },
    {
        "date": "2025-03",
        "title": "Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat",
        "author": "Joseph Emmanuel DL Dayo, Michel Onasis S. Ogbinar, and Prospero C. Naval Jr",
        "link": "http://arxiv.org/abs/2503.15726v1",
        "abstract": "The objective of this study is to design and implement a reinforcement\nlearning (RL) environment using D\\&D 5E combat scenarios to challenge smaller\nRL agents through interaction with a robust adversarial agent controlled by\nadvanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research\nemploys Deep Q-Networks (DQN) for the smaller agents, creating a testbed for\nstrategic AI development that also serves as an educational tool by simulating\ndynamic and unpredictable combat scenarios. We successfully integrated\nsophisticated language models into the RL framework, enhancing strategic\ndecision-making processes. Our results indicate that while RL agents generally\noutperform LLM-controlled adversaries in standard metrics, the strategic depth\nprovided by LLMs significantly enhances the overall AI capabilities in this\ncomplex, rule-based setting. The novelty of our approach and its implications\nfor mastering intricate environments and developing adaptive strategies are\ndiscussed, alongside potential innovations in AI-driven interactive\nsimulations. This paper aims to demonstrate how integrating LLMs can create\nmore robust and adaptable AI systems, providing valuable insights for further\nresearch and educational applications."
    },
    {
        "date": "2025-03",
        "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
        "author": "Yuchen Ren, Zhengyu Zhao, Chenhao Lin, Bo Yang, Lu Zhou, Zhe Liu, and Chao Shen",
        "link": "http://arxiv.org/abs/2503.15404v1",
        "abstract": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."
    },
    {
        "date": "2025-03",
        "title": "Robustness of Nonlinear Representation Learning",
        "author": "Simon Buchholz, and Bernhard Sch\u00f6lkopf",
        "link": "http://arxiv.org/abs/2503.15355v1",
        "abstract": "We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes."
    },
    {
        "date": "2025-03",
        "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors",
        "author": "Dominik Macko, Robert Moro, and Ivan Srba",
        "link": "http://arxiv.org/abs/2503.15128v1",
        "abstract": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."
    },
    {
        "date": "2025-03",
        "title": "StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion",
        "author": "Le Ma, Ziyu Meng, Tengyu Liu, Yuhan Li, Ran Song, Wei Zhang, and Siyuan Huang",
        "link": "http://arxiv.org/abs/2503.15082v1",
        "abstract": "Humanoid robots are anticipated to acquire a wide range of locomotion\ncapabilities while ensuring natural movement across varying speeds and\nterrains. Existing methods encounter a fundamental dilemma in learning humanoid\nlocomotion: reinforcement learning with handcrafted rewards can achieve agile\nlocomotion but produces unnatural gaits, while Generative Adversarial Imitation\nLearning (GAIL) with motion capture data yields natural movements but suffers\nfrom unstable training processes and restricted agility. Integrating these\napproaches proves challenging due to the inherent heterogeneity between expert\npolicies and human motion datasets. To address this, we introduce StyleLoco, a\nnovel two-stage framework that bridges this gap through a Generative\nAdversarial Distillation (GAD) process. Our framework begins by training a\nteacher policy using reinforcement learning to achieve agile and dynamic\nlocomotion. It then employs a multi-discriminator architecture, where distinct\ndiscriminators concurrently extract skills from both the teacher policy and\nmotion capture data. This approach effectively combines the agility of\nreinforcement learning with the natural fluidity of human-like movements while\nmitigating the instability issues commonly associated with adversarial\ntraining. Through extensive simulation and real-world experiments, we\ndemonstrate that StyleLoco enables humanoid robots to perform diverse\nlocomotion tasks with the precision of expertly trained policies and the\nnatural aesthetics of human motion, successfully transferring styles across\ndifferent movement types while maintaining stable locomotion across a broad\nspectrum of command inputs."
    },
    {
        "date": "2025-03",
        "title": "A Semantic and Clean-label Backdoor Attack against Graph Convolutional Networks",
        "author": "Jiazhu Dai, and Haoyu Sun",
        "link": "http://arxiv.org/abs/2503.14922v1",
        "abstract": "Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples."
    },
    {
        "date": "2025-03",
        "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift",
        "author": "Jingyi Liao, Xun Xu, Yongyi Su, Rong-Cheng Tu, Yifan Liu, Dacheng Tao, and Xulei Yang",
        "link": "http://arxiv.org/abs/2503.14910v1",
        "abstract": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."
    },
    {
        "date": "2025-03",
        "title": "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders Decomposition",
        "author": "Seyed Mojtaba Mohasel, and Hamidreza Koosha",
        "link": "http://arxiv.org/abs/2503.14873v1",
        "abstract": "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available."
    },
    {
        "date": "2025-03",
        "title": "On the Robustness Tradeoff in Fine-Tuning",
        "author": "Kunyang Li, Jean-Charles Noirot Ferrand, Ryan Sheatsley, Blaine Hoak, Yohan Beugin, Eric Pauley, and Patrick McDaniel",
        "link": "http://arxiv.org/abs/2503.14836v1",
        "abstract": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."
    },
    {
        "date": "2025-03",
        "title": "Robust Transmission of Punctured Text with Large Language Model-based Recovery",
        "author": "Sojeong Park, Hyeonho Noh, and Hyun Jong Yang",
        "link": "http://arxiv.org/abs/2503.14831v1",
        "abstract": "With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions."
    },
    {
        "date": "2025-03",
        "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation",
        "author": "Shiyi Jiang, Farshad Firouzi, and Krishnendu Chakrabarty",
        "link": "http://arxiv.org/abs/2503.16542v1",
        "abstract": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data."
    },
    {
        "date": "2025-03",
        "title": "Temporal Context Awareness: A Defense Framework Against Multi-turn Manipulation Attacks on Large Language Models",
        "author": "Prashant Kulkarni, and Assaf Namer",
        "link": "http://arxiv.org/abs/2503.15560v1",
        "abstract": "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security."
    },
    {
        "date": "2025-03",
        "title": "LipShiFT: A Certifiably Robust Shift-based Vision Transformer",
        "author": "Rohan Menon, Nicola Franco, and Stephan G\u00fcnnemann",
        "link": "http://arxiv.org/abs/2503.14751v1",
        "abstract": "Deriving tight Lipschitz bounds for transformer-based architectures presents\na significant challenge. The large input sizes and high-dimensional attention\nmodules typically prove to be crucial bottlenecks during the training process\nand leads to sub-optimal results. Our research highlights practical constraints\nof these methods in vision tasks. We find that Lipschitz-based margin training\nacts as a strong regularizer while restricting weights in successive layers of\nthe model. Focusing on a Lipschitz continuous variant of the ShiftViT model, we\naddress significant training challenges for transformer-based architectures\nunder norm-constrained input setting. We provide an upper bound estimate for\nthe Lipschitz constants of this model using the $l_2$ norm on common image\nclassification datasets. Ultimately, we demonstrate that our method scales to\nlarger models and advances the state-of-the-art in certified robustness for\ntransformer-based architectures."
    },
    {
        "date": "2025-03",
        "title": "A Comprehensive Study of LLM Secure Code Generation",
        "author": "Shih-Chieh Dai, Jun Xu, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2503.15554v1",
        "abstract": "LLMs are widely used in software development. However, the code generated by\nLLMs often contains vulnerabilities. Several secure code generation methods\nhave been proposed to address this issue, but their current evaluation schemes\nleave several concerns unaddressed. Specifically, most existing studies\nevaluate security and functional correctness separately, using different\ndatasets. That is, they assess vulnerabilities using security-related code\ndatasets while validating functionality with general code datasets. In\naddition, prior research primarily relies on a single static analyzer, CodeQL,\nto detect vulnerabilities in generated code, which limits the scope of security\nevaluation.\n  In this work, we conduct a comprehensive study to systematically assess the\nimprovements introduced by four state-of-the-art secure code generation\ntechniques. Specifically, we apply both security inspection and functionality\nvalidation to the same generated code and evaluate these two aspects together.\nWe also employ three popular static analyzers and two LLMs to identify\npotential vulnerabilities in the generated code. Our study reveals that\nexisting techniques often compromise the functionality of generated code to\nenhance security. Their overall performance remains limited when evaluating\nsecurity and functionality together. In fact, many techniques even degrade the\nperformance of the base LLM. Our further inspection reveals that these\ntechniques often either remove vulnerable lines of code entirely or generate\n``garbage code'' that is unrelated to the intended task. Moreover, the commonly\nused static analyzer CodeQL fails to detect several vulnerabilities, further\nobscuring the actual security improvements achieved by existing techniques. Our\nstudy serves as a guideline for a more rigorous and comprehensive evaluation of\nsecure code generation performance in future work."
    },
    {
        "date": "2025-03",
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "author": "Tharindu Kumarage, Cameron Johnson, Jadie Adams, Lin Ai, Matthias Kirchner, Anthony Hoogs, Joshua Garland, Julia Hirschberg, Arslan Basharat, and Huan Liu",
        "link": "http://arxiv.org/abs/2503.15552v1",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts."
    },
    {
        "date": "2025-03",
        "title": "Anomaly-Flow: A Multi-domain Federated Generative Adversarial Network for Distributed Denial-of-Service Detection",
        "author": "Leonardo Henrique de Melo, Gustavo de Carvalho Bertoli, Michele Nogueira, Aldri Luiz dos Santos, and Louren\u00e7o Alves Pereira Junior",
        "link": "http://arxiv.org/abs/2503.14618v1",
        "abstract": "Distributed denial-of-service (DDoS) attacks remain a critical threat to\nInternet services, causing costly disruptions. While machine learning (ML) has\nshown promise in DDoS detection, current solutions struggle with multi-domain\nenvironments where attacks must be detected across heterogeneous networks and\norganizational boundaries. This limitation severely impacts the practical\ndeployment of ML-based defenses in real-world settings.\n  This paper introduces Anomaly-Flow, a novel framework that addresses this\ncritical gap by combining Federated Learning (FL) with Generative Adversarial\nNetworks (GANs) for privacy-preserving, multi-domain DDoS detection. Our\nproposal enables collaborative learning across diverse network domains while\npreserving data privacy through synthetic flow generation. Through extensive\nevaluation across three distinct network datasets, Anomaly-Flow achieves an\naverage F1-score of $0.747$, outperforming baseline models. Importantly, our\nframework enables organizations to share attack detection capabilities without\nexposing sensitive network data, making it particularly valuable for critical\ninfrastructure and privacy-sensitive sectors.\n  Beyond immediate technical contributions, this work provides insights into\nthe challenges and opportunities in multi-domain DDoS detection, establishing a\nfoundation for future research in collaborative network defense systems. Our\nfindings have important implications for academic research and industry\npractitioners working to deploy practical ML-based security solutions."
    },
    {
        "date": "2025-03",
        "title": "Doubly robust identification of treatment effects from multiple environments",
        "author": "Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, and Fanny Yang",
        "link": "http://arxiv.org/abs/2503.14459v1",
        "abstract": "Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods."
    },
    {
        "date": "2025-03",
        "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting Attack",
        "author": "Murong Yue, and Ziyu Yao",
        "link": "http://arxiv.org/abs/2503.15551v1",
        "abstract": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
    },
    {
        "date": "2025-03",
        "title": "Unveiling the Role of Randomization in Multiclass Adversarial Classification: Insights from Graph Theory",
        "author": "Lucas Gnecco-Heredia, Matteo Sammut, Muni Sreenivas Pydi, Rafael Pinot, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2503.14299v1",
        "abstract": "Randomization as a mean to improve the adversarial robustness of machine\nlearning models has recently attracted significant attention. Unfortunately,\nmuch of the theoretical analysis so far has focused on binary classification,\nproviding only limited insights into the more complex multiclass setting. In\nthis paper, we take a step toward closing this gap by drawing inspiration from\nthe field of graph theory. Our analysis focuses on discrete data distributions,\nallowing us to cast the adversarial risk minimization problems within the\nwell-established framework of set packing problems. By doing so, we are able to\nidentify three structural conditions on the support of the data distribution\nthat are necessary for randomization to improve robustness. Furthermore, we are\nable to construct several data distributions where (contrarily to binary\nclassification) switching from a deterministic to a randomized solution\nsignificantly reduces the optimal adversarial risk. These findings highlight\nthe crucial role randomization can play in enhancing robustness to adversarial\nattacks in multiclass classification."
    },
    {
        "date": "2025-03",
        "title": "Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation",
        "author": "Justus Westerhoff, Golzar Atefi, Mario Koddenbrock, Alexei Figueroa, Alexander L\u00f6ser, Erik Rodner, and Felix A. Gers",
        "link": "http://arxiv.org/abs/2503.14572v1",
        "abstract": "The capacity of a foundation model allows for adaptation to new downstream\ntasks. Weight imprinting is a universal and efficient method to fulfill this\npurpose. It has been reinvented several times, but it has not been\nsystematically studied. In this paper, we propose a framework for imprinting,\nidentifying three main components: generation, normalization, and aggregation.\nThis allows us to conduct an in-depth analysis of imprinting and a comparison\nof the existing work. We reveal the benefits of representing novel data with\nmultiple proxies in the generation step and show the importance of proper\nnormalization. We determine those proxies through clustering and propose a\nnovel variant of imprinting that outperforms previous work. We motivate this by\nthe neural collapse phenomenon -- an important connection that we can draw for\nthe first time. Our results show an increase of up to 4% in challenging\nscenarios with complex data distributions for new classes."
    },
    {
        "date": "2025-03",
        "title": "XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants",
        "author": "Adam \u0160torek, Mukur Gupta, Noopur Bhatt, Aditya Gupta, Janie Kim, Prashast Srivastava, and Suman Jana",
        "link": "http://arxiv.org/abs/2503.14281v1",
        "abstract": "AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools."
    },
    {
        "date": "2025-03",
        "title": "GeoFlow-SLAM: A Robust Tightly-Coupled RGBD-Inertial Fusion SLAM for Dynamic Legged Robotics",
        "author": "Tingyang Xiao, Xiaolin Zhou, Liu Liu, Wei Sui, Wei Feng, Jiaxiong Qiu, Xinjie Wang, and Zhizhong Su",
        "link": "http://arxiv.org/abs/2503.14247v1",
        "abstract": "This paper presents GeoFlow-SLAM, a robust and effective Tightly-Coupled\nRGBD-inertial SLAM for legged robots operating in highly dynamic\nenvironments.By integrating geometric consistency, legged odometry constraints,\nand dual-stream optical flow (GeoFlow), our method addresses three critical\nchallenges:feature matching and pose initialization failures during fast\nlocomotion and visual feature scarcity in texture-less scenes.Specifically, in\nrapid motion scenarios, feature matching is notably enhanced by leveraging\ndual-stream optical flow, which combines prior map points and poses.\nAdditionally, we propose a robust pose initialization method for fast\nlocomotion and IMU error in legged robots, integrating IMU/Legged odometry,\ninter-frame Perspective-n-Point (PnP), and Generalized Iterative Closest Point\n(GICP). Furthermore, a novel optimization framework that tightly couples\ndepth-to-map and GICP geometric constraints is first introduced to improve the\nrobustness and accuracy in long-duration, visually texture-less environments.\nThe proposed algorithms achieve state-of-the-art (SOTA) on collected legged\nrobots and open-source datasets. To further promote research and development,\nthe open-source datasets and code will be made publicly available at\nhttps://github.com/NSN-Hello/GeoFlow-SLAM"
    },
    {
        "date": "2025-03",
        "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
        "author": "Junjin Xiao, Qing Zhang, Yonewei Nie, Lei Zhu, and Wei-Shi Zheng",
        "link": "http://arxiv.org/abs/2503.14198v1",
        "abstract": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat."
    },
    {
        "date": "2025-03",
        "title": "Towards properties of adversarial image perturbations",
        "author": "Egor Kuznetsov, Kirill Aistov, and Maxim Koroteev",
        "link": "http://arxiv.org/abs/2503.14111v1",
        "abstract": "Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization."
    },
    {
        "date": "2025-03",
        "title": "Securing Automated Insulin Delivery Systems: A Review of Security Threats and Protectives Strategies",
        "author": "Yuchen Niu, and Siew-Kei Lam",
        "link": "http://arxiv.org/abs/2503.14006v1",
        "abstract": "Automated insulin delivery (AID) systems have emerged as a significant\ntechnological advancement in diabetes care. These systems integrate a\ncontinuous glucose monitor, an insulin pump, and control algorithms to automate\ninsulin delivery, reducing the burden of self-management and offering enhanced\nglucose control. However, the increasing reliance on wireless connectivity and\nsoftware control has exposed AID systems to critical security risks that could\nresult in life-threatening treatment errors. This review first presents a\ncomprehensive examination of the security landscape, covering technical\nvulnerabilities, legal frameworks, and commercial product considerations, and\nan analysis of existing research on attack vectors, defence mechanisms, as well\nas evaluation methods and resources for AID systems. Despite recent\nadvancements, several open challenges remain in achieving secure AID systems,\nparticularly in standardising security evaluation frameworks and developing\ncomprehensive, lightweight, and adaptive defence strategies. As one of the most\nwidely adopted and extensively studied physiologic closed-loop control systems,\nthis review serves as a valuable reference for understanding security\nchallenges and solutions applicable to analogous medical systems."
    },
    {
        "date": "2025-03",
        "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
        "author": "Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui",
        "link": "http://arxiv.org/abs/2503.13962v1",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."
    },
    {
        "date": "2025-03",
        "title": "Robust Machine Unlearning for Quantized Neural Networks via Adaptive Gradient Reweighting with Similar Labels",
        "author": "Yujia Tong, Yuze Wang, Jingling Yuan, and Chuang Hu",
        "link": "http://arxiv.org/abs/2503.13917v1",
        "abstract": "Model quantization enables efficient deployment of deep neural networks on\nedge devices through low-bit parameter representation, yet raises critical\nchallenges for implementing machine unlearning (MU) under data privacy\nregulations. Existing MU methods designed for full-precision models fail to\naddress two fundamental limitations in quantized networks: 1) Noise\namplification from label mismatch during data processing, and 2) Gradient\nimbalance between forgotten and retained data during training. These issues are\nexacerbated by quantized models' constrained parameter space and discrete\noptimization. We propose Q-MUL, the first dedicated unlearning framework for\nquantized models. Our method introduces two key innovations: 1) Similar Labels\nassignment replaces random labels with semantically consistent alternatives to\nminimize noise injection, and 2) Adaptive Gradient Reweighting dynamically\naligns parameter update contributions from forgotten and retained data. Through\nsystematic analysis of quantized model vulnerabilities, we establish\ntheoretical foundations for these mechanisms. Extensive evaluations on\nbenchmark datasets demonstrate Q-MUL's superiority over existing approaches."
    },
    {
        "date": "2025-03",
        "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation",
        "author": "Xinliang Zhang, Lei Zhu, Shuang Zeng, Hangzhou He, Ourui Fu, Zhengjian Yao, Zhaoheng Xie, and Yanye Lu",
        "link": "http://arxiv.org/abs/2503.13895v1",
        "abstract": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available."
    },
    {
        "date": "2025-03",
        "title": "Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception",
        "author": "Jinge Ma, Jiangpeng He, and Fengqing Zhu",
        "link": "http://arxiv.org/abs/2503.13869v1",
        "abstract": "3D perception plays a crucial role in real-world applications such as\nautonomous driving, robotics, and AR/VR. In practical scenarios, 3D perception\nmodels must continuously adapt to new data and emerging object categories, but\nretraining from scratch incurs prohibitive costs. Therefore, adopting\nclass-incremental learning (CIL) becomes particularly essential. However,\nreal-world 3D point cloud data often include corrupted samples, which poses\nsignificant challenges for existing CIL methods and leads to more severe\nforgetting on corrupted data. To address these challenges, we consider the\nscenario in which a CIL model can be updated using point clouds with unknown\ncorruption to better simulate real-world conditions. Inspired by Farthest Point\nSampling, we propose a novel exemplar selection strategy that effectively\npreserves intra-class diversity when selecting replay exemplars, mitigating\nforgetting induced by data corruption. Furthermore, we introduce a point cloud\ndownsampling-based replay method to utilize the limited replay buffer memory\nmore efficiently, thereby further enhancing the model's continual learning\nability. Extensive experiments demonstrate that our method improves the\nperformance of replay-based CIL baselines by 2% to 11%, proving its\neffectiveness and promising potential for real-world 3D applications."
    },
    {
        "date": "2025-03",
        "title": "Text-Guided Image Invariant Feature Learning for Robust Image Watermarking",
        "author": "Muhammad Ahtesham, and Xin Zhong",
        "link": "http://arxiv.org/abs/2503.13805v1",
        "abstract": "Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking."
    },
    {
        "date": "2025-03",
        "title": "Web Artifact Attacks Disrupt Vision Language Models",
        "author": "Maan Qraitem, Piotr Teterwak, Kate Saenko, and Bryan A. Plummer",
        "link": "http://arxiv.org/abs/2503.13652v1",
        "abstract": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness."
    },
    {
        "date": "2025-03",
        "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
        "author": "Johan Edstedt",
        "link": "http://arxiv.org/abs/2503.13433v1",
        "abstract": "The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1."
    },
    {
        "date": "2025-03",
        "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes",
        "author": "Nhi Pham, Bernt Schiele, Adam Kortylewski, and Jonas Fischer",
        "link": "http://arxiv.org/abs/2503.13429v1",
        "abstract": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness."
    },
    {
        "date": "2025-03",
        "title": "Securing Virtual Reality Experiences: Unveiling and Tackling Cybersickness Attacks with Explainable AI",
        "author": "Ripan Kumar Kundu, Matthew Denton, Genova Mongalo, Prasad Calyam, and Khaza Anuarul Hoque",
        "link": "http://arxiv.org/abs/2503.13419v1",
        "abstract": "The synergy between virtual reality (VR) and artificial intelligence (AI),\nspecifically deep learning (DL)-based cybersickness detection models, has\nushered in unprecedented advancements in immersive experiences by automatically\ndetecting cybersickness severity and adaptively various mitigation techniques,\noffering a smooth and comfortable VR experience. While this DL-enabled\ncybersickness detection method provides promising solutions for enhancing user\nexperiences, it also introduces new risks since these models are vulnerable to\nadversarial attacks; a small perturbation of the input data that is visually\nundetectable to human observers can fool the cybersickness detection model and\ntrigger unexpected mitigation, thus disrupting user immersive experiences (UIX)\nand even posing safety risks. In this paper, we present a new type of VR\nattack, i.e., a cybersickness attack, which successfully stops the triggering\nof cybersickness mitigation by fooling DL-based cybersickness detection models\nand dramatically hinders the UIX. Next, we propose a novel explainable\nartificial intelligence (XAI)-guided cybersickness attack detection framework\nto detect such attacks in VR to ensure UIX and a comfortable VR experience. We\nevaluate the proposed attack and the detection framework using two\nstate-of-the-art open-source VR cybersickness datasets: Simulation 2021 and\nGameplay dataset. Finally, to verify the effectiveness of our proposed method,\nwe implement the attack and the XAI-based detection using a testbed with a\ncustom-built VR roller coaster simulation with an HTC Vive Pro Eye headset and\nperform a user study. Our study shows that such an attack can dramatically\nhinder the UIX. However, our proposed XAI-guided cybersickness attack detection\ncan successfully detect cybersickness attacks and trigger the proper\nmitigation, effectively reducing VR cybersickness."
    },
    {
        "date": "2025-03",
        "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization Framework from a Deep-Learning Perspective",
        "author": "Dengyun Peng, Yuhang Zhou, Qiguang Chen, Jinhao Liu, Jingjing Chen, and Libo Qin",
        "link": "http://arxiv.org/abs/2503.13413v3",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
    },
    {
        "date": "2025-03",
        "title": "Follow-the-Regularized-Leader with Adversarial Constraints",
        "author": "Ricardo N. Ferreira, and Cl\u00e1udia Soares",
        "link": "http://arxiv.org/abs/2503.13366v1",
        "abstract": "Constrained Online Convex Optimization (COCO) can be seen as a generalization\nof the standard Online Convex Optimization (OCO) framework. At each round, a\ncost function and constraint function are revealed after a learner chooses an\naction. The goal is to minimize both the regret and cumulative constraint\nviolation (CCV) against an adaptive adversary. We show for the first time that\nis possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV,\nimproving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\~{O}\n\\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively."
    },
    {
        "date": "2025-03",
        "title": "RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall Downscaling",
        "author": "Marcello Iotti, Paolo Davini, Jost von Hardenberg, and Giuseppe Zappa",
        "link": "http://arxiv.org/abs/2503.13316v1",
        "abstract": "To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature."
    },
    {
        "date": "2025-03",
        "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
        "author": "Tianxing Fu, Jia Hu, Geyong Min, and Zi Wang",
        "link": "http://arxiv.org/abs/2503.13255v1",
        "abstract": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models while ensuring their data remains private and\nsecure. Blockchain technology further enhances FL by providing stronger\nsecurity, a transparent audit trail, and protection against data tampering and\nmodel manipulation. Most blockchain-secured FL systems rely on conventional\nconsensus mechanisms: Proof-of-Work (PoW) is computationally expensive, while\nProof-of-Stake (PoS) improves energy efficiency but risks centralization as it\ninherently favors participants with larger stakes. Recently, learning-based\nconsensus has emerged as an alternative by replacing cryptographic tasks with\nmodel training to save energy. However, this approach introduces potential\nprivacy vulnerabilities, as the training process may inadvertently expose\nsensitive information through gradient sharing and model updates. To address\nthese challenges, we propose a novel Zero-Knowledge Proof of Training (ZKPoT)\nconsensus mechanism. This method leverages the zero-knowledge succinct\nnon-interactive argument of knowledge proof (zk-SNARK) protocol to validate\nparticipants' contributions based on their model performance, effectively\neliminating the inefficiencies of traditional consensus methods and mitigating\nthe privacy risks posed by learning-based consensus. We analyze our system's\nsecurity, demonstrating its capacity to prevent the disclosure of sensitive\ninformation about local models or training data to untrusted parties during the\nentire FL process. Extensive experiments demonstrate that our system is robust\nagainst privacy and Byzantine attacks while maintaining accuracy and utility\nwithout trade-offs, scalable across various blockchain settings, and efficient\nin both computation and communication."
    },
    {
        "date": "2025-03",
        "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained Models Against Extraction",
        "author": "Tong Zhou, Shijin Duan, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Shaolei Ren, and Xiaolin Xu",
        "link": "http://arxiv.org/abs/2503.13224v1",
        "abstract": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security."
    },
    {
        "date": "2025-03",
        "title": "Robust Decision-Making Via Free Energy Minimization",
        "author": "Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, and Giovanni Russo",
        "link": "http://arxiv.org/abs/2503.13223v1",
        "abstract": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments."
    },
    {
        "date": "2025-03",
        "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization",
        "author": "Yiwei Xu, Yifei Yu, Wentian Gan, Tengfei Wang, Zongqian Zhan, Hao Cheng, and Xin Wang",
        "link": "http://arxiv.org/abs/2503.13086v1",
        "abstract": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction."
    },
    {
        "date": "2025-03",
        "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling",
        "author": "Robin Zbinden, Nina van Tiel, Gencer Sumbul, Chiara Vanalli, Benjamin Kellenberger, and Devis Tuia",
        "link": "http://arxiv.org/abs/2503.13057v1",
        "abstract": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications."
    },
    {
        "date": "2025-03",
        "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided Mirror Crafting",
        "author": "Rui Pu, Chaozhuo Li, Rui Ha, Litian Zhang, Lirong Qiu, and Xi Zhang",
        "link": "http://arxiv.org/abs/2503.12931v1",
        "abstract": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness."
    },
    {
        "date": "2025-03",
        "title": "MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements to RAG",
        "author": "Pingyu Wu, Daiheng Gao, Jing Tang, Huimin Chen, Wenbo Zhou, Weiming Zhang, and Nenghai Yu",
        "link": "http://arxiv.org/abs/2503.13563v1",
        "abstract": "Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https://github.com/wpydcr/MES-RAG."
    },
    {
        "date": "2025-03",
        "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "author": "Pengcheng Zhou, Yinglun Feng, and Zhongliang Yang",
        "link": "http://arxiv.org/abs/2503.15548v1",
        "abstract": "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures."
    },
    {
        "date": "2025-03",
        "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models",
        "author": "Xiaojun Jia, Sensen Gao, Simeng Qin, Ke Ma, Xinfeng Li, Yihao Huang, Wei Dong, Yang Liu, and Xiaochun Cao",
        "link": "http://arxiv.org/abs/2503.12874v2",
        "abstract": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT."
    },
    {
        "date": "2025-03",
        "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
        "author": "Chen Liu, Peike Li, Liying Yang, Dadong Wang, Lincheng Li, and Xin Yu",
        "link": "http://arxiv.org/abs/2503.12847v1",
        "abstract": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation."
    },
    {
        "date": "2025-03",
        "title": "CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting",
        "author": "Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, and Sangpil Kim",
        "link": "http://arxiv.org/abs/2503.12836v3",
        "abstract": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model."
    },
    {
        "date": "2025-03",
        "title": "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
        "author": "Md Farhamdur Reza, Richeng Jin, Tianfu Wu, and Huaiyu Dai",
        "link": "http://arxiv.org/abs/2503.12827v2",
        "abstract": "Existing score-based adversarial attacks mainly focus on crafting $top$-1\nadversarial examples against classifiers with single-label classification.\nTheir attack success rate and query efficiency are often less than\nsatisfactory, particularly under small perturbation requirements; moreover, the\nvulnerability of classifiers with multi-label learning is yet to be studied. In\nthis paper, we propose a comprehensive surrogate free score-based attack, named\n\\b geometric \\b score-based \\b black-box \\b attack (GSBA$^K$), to craft\nadversarial examples in an aggressive $top$-$K$ setting for both untargeted and\ntargeted attacks, where the goal is to change the $top$-$K$ predictions of the\ntarget classifier. We introduce novel gradient-based methods to find a good\ninitial boundary point to attack. Our iterative method employs novel gradient\nestimation techniques, particularly effective in $top$-$K$ setting, on the\ndecision boundary to effectively exploit the geometry of the decision boundary.\nAdditionally, GSBA$^K$ can be used to attack against classifiers with $top$-$K$\nmulti-label learning. Extensive experimental results on ImageNet and PASCAL VOC\ndatasets validate the effectiveness of GSBA$^K$ in crafting $top$-$K$\nadversarial examples."
    },
    {
        "date": "2025-03",
        "title": "BLIA: Detect model memorization in binary classification model through passive Label Inference attack",
        "author": "Mohammad Wahiduzzaman Khan, Sheng Chen, Ilya Mironov, Leizhen Zhang, and Rabib Noor",
        "link": "http://arxiv.org/abs/2503.12801v1",
        "abstract": "Model memorization has implications for both the generalization capacity of\nmachine learning models and the privacy of their training data. This paper\ninvestigates label memorization in binary classification models through two\nnovel passive label inference attacks (BLIA). These attacks operate passively,\nrelying solely on the outputs of pre-trained models, such as confidence scores\nand log-loss values, without interacting with or modifying the training\nprocess. By intentionally flipping 50% of the labels in controlled subsets,\ntermed \"canaries,\" we evaluate the extent of label memorization under two\nconditions: models trained without label differential privacy (Label-DP) and\nthose trained with randomized response-based Label-DP. Despite the application\nof varying degrees of Label-DP, the proposed attacks consistently achieve\nsuccess rates exceeding 50%, surpassing the baseline of random guessing and\nconclusively demonstrating that models memorize training labels, even when\nthese labels are deliberately uncorrelated with the features."
    },
    {
        "date": "2025-03",
        "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization",
        "author": "Yechao Zhang, Yingzhe Xu, Junyu Shi, Leo Yu Zhang, Shengshan Hu, Minghui Li, and Yanjun Zhang",
        "link": "http://arxiv.org/abs/2503.12793v2",
        "abstract": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%."
    },
    {
        "date": "2025-03",
        "title": "Algebraic Adversarial Attacks on Explainability Models",
        "author": "Lachlan Simpson, Federico Costanza, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, and Hong Gunn Chew",
        "link": "http://arxiv.org/abs/2503.12683v1",
        "abstract": "Classical adversarial attacks are phrased as a constrained optimisation\nproblem. Despite the efficacy of a constrained optimisation approach to\nadversarial attacks, one cannot trace how an adversarial point was generated.\nIn this work, we propose an algebraic approach to adversarial attacks and study\nthe conditions under which one can generate adversarial examples for post-hoc\nexplainability models. Phrasing neural networks in the framework of geometric\ndeep learning, algebraic adversarial attacks are constructed through analysis\nof the symmetry groups of neural networks. Algebraic adversarial examples\nprovide a mathematically tractable approach to adversarial examples. We\nvalidate our approach of algebraic adversarial examples on two well-known and\none real-world dataset."
    },
    {
        "date": "2025-03",
        "title": "SCOOP: CoSt-effective COngestiOn Attacks in Payment Channel Networks",
        "author": "Mohammed Ababneh, Kartick Kolachala, and Roopa Vishwanathan",
        "link": "http://arxiv.org/abs/2503.12625v1",
        "abstract": "Payment channel networks (PCNs) are a promising solution to address\nblockchain scalability and throughput challenges, However, the security of PCNs\nand their vulnerability to attacks are not sufficiently studied. In this paper,\nwe introduce SCOOP, a framework that includes two novel congestion attacks on\nPCNs. These attacks consider the minimum transferable amount along a path (path\ncapacity) and the number of channels involved (path length), formulated as\nlinear optimization problems. The first attack allocates the attacker's budget\nto achieve a specific congestion threshold, while the second maximizes\ncongestion under budget constraints. Simulation results show the effectiveness\nof the proposed attack formulations in comparison to other attack strategies.\nSpecifically, the results indicate that the first attack provides around a 40\\%\nimprovement in congestion performance, while the second attack offers\napproximately a 50\\% improvement in comparison to the state-of-the-art.\nMoreover, in terms of payment to congestion efficiency, the first attack is\nabout 60\\% more efficient, and the second attack is around 90\\% more efficient\nin comparison to state-of-the-art"
    },
    {
        "date": "2025-03",
        "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack",
        "author": "Abyad Enan, and Mashrur Chowdhury",
        "link": "http://arxiv.org/abs/2503.12567v1",
        "abstract": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel."
    },
    {
        "date": "2025-03",
        "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry",
        "author": "Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, and Dewen Hu",
        "link": "http://arxiv.org/abs/2503.12527v1",
        "abstract": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}."
    },
    {
        "date": "2025-03",
        "title": "Defense Against Model Stealing Based on Account-Aware Distribution Discrepancy",
        "author": "Jian-Ping Mei, Weibin Zhang, Jie Chen, Xuyun Zhang, and Tiantian Zhu",
        "link": "http://arxiv.org/abs/2503.12497v1",
        "abstract": "Malicious users attempt to replicate commercial models functionally at low\ncost by training a clone model with query responses. It is challenging to\ntimely prevent such model-stealing attacks to achieve strong protection and\nmaintain utility. In this paper, we propose a novel non-parametric detector\ncalled Account-aware Distribution Discrepancy (ADD) to recognize queries from\nmalicious users by leveraging account-wise local dependency. We formulate each\nclass as a Multivariate Normal distribution (MVN) in the feature space and\nmeasure the malicious score as the sum of weighted class-wise distribution\ndiscrepancy. The ADD detector is combined with random-based prediction\npoisoning to yield a plug-and-play defense module named D-ADD for image\nclassification models. Results of extensive experimental studies show that\nD-ADD achieves strong defense against different types of attacks with little\ninterference in serving benign users for both soft and hard-label settings."
    },
    {
        "date": "2025-03",
        "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation",
        "author": "Edgar Heinert, Thomas Gottwald, Annika M\u00fctze, and Matthias Rottmann",
        "link": "http://arxiv.org/abs/2503.12453v1",
        "abstract": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs."
    },
    {
        "date": "2025-03",
        "title": "Semi-Decision-Focused Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization",
        "author": "Juhyeong Kim",
        "link": "http://arxiv.org/abs/2503.13544v1",
        "abstract": "I propose Semi-Decision-Focused Learning, a practical adaptation of\nDecision-Focused Learning for portfolio optimization. Rather than directly\noptimizing complex financial metrics, I employ simple target portfolios\n(Max-Sortino or One-Hot) and train models with a convex, cross-entropy loss. I\nfurther incorporate Deep Ensemble methods to reduce variance and stabilize\nperformance. Experiments on two universes (one upward-trending and another\nrange-bound) show consistent outperformance over baseline portfolios,\ndemonstrating the effectiveness and robustness of my approach. Code is\navailable at https://github.com/sDFLwDE/sDFLwDE"
    },
    {
        "date": "2025-03",
        "title": "SCReedSolo: A Secure and Robust LSB Image Steganography Framework with Randomized Symmetric Encryption and Reed-Solomon Coding",
        "author": "Syed Rifat Raiyan, and Md. Hasanul Kabir",
        "link": "http://arxiv.org/abs/2503.12368v1",
        "abstract": "Image steganography is an information-hiding technique that involves the\nsurreptitious concealment of covert informational content within digital\nimages. In this paper, we introduce ${\\rm SCR{\\small EED}S{\\small OLO}}$, a\nnovel framework for concealing arbitrary binary data within images. Our\napproach synergistically leverages Random Shuffling, Fernet Symmetric\nEncryption, and Reed-Solomon Error Correction Codes to encode the secret\npayload, which is then discretely embedded into the carrier image using LSB\n(Least Significant Bit) Steganography. The combination of these methods\naddresses the vulnerability vectors of both security and resilience against\nbit-level corruption in the resultant stego-images. We show that our framework\nachieves a data payload of 3 bits per pixel for an RGB image, and\nmathematically assess the probability of successful transmission for the\namalgamated $n$ message bits and $k$ error correction bits. Additionally, we\nfind that ${\\rm SCR{\\small EED}S{\\small OLO}}$ yields good results upon being\nevaluated with multiple performance metrics, successfully eludes detection by\nvarious passive steganalysis tools, and is immune to simple active steganalysis\nattacks. Our code and data are available at\nhttps://github.com/Starscream-11813/SCReedSolo-Steganography."
    },
    {
        "date": "2025-03",
        "title": "Synthetic Data for Robust AI Model Development in Regulated Enterprises",
        "author": "Aditi Godbole",
        "link": "http://arxiv.org/abs/2503.12353v1",
        "abstract": "In today's business landscape, organizations need to find the right balance\nbetween using their customers' data ethically to power AI solutions and being\ncompliant regarding data privacy and data usage regulations. In this paper, we\ndiscuss synthetic data as a possible solution to this dilemma. Synthetic data\nis simulated data that mimics the real data. We explore how organizations in\nheavily regulated industries, such as financial institutions or healthcare\norganizations, can leverage synthetic data to build robust AI solutions while\nstaying compliant. We demonstrate that synthetic data offers two significant\nadvantages by allowing AI models to learn from more diverse data and by helping\norganizations stay compliant against data privacy laws with the use of\nsynthetic data instead of customer information. We discuss case studies to show\nhow synthetic data can be effectively used in the finance and healthcare sector\nwhile discussing the challenges of using synthetic data and some ethical\nquestions it raises. Our research finds that synthetic data could be a\ngame-changer for AI in regulated industries. The potential can be realized when\nindustry, academia, and regulators collaborate to build solutions. We aim to\ninitiate discussions on the use of synthetic data to build ethical,\nresponsible, and effective AI systems in regulated enterprise industries."
    },
    {
        "date": "2025-03",
        "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions",
        "author": "Wenqing Kuang, Xiongwei Zhao, Yehui Shen, Congcong Wen, Huimin Lu, Zongtan Zhou, and Xieyuanli Chen",
        "link": "http://arxiv.org/abs/2503.12350v1",
        "abstract": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR."
    },
    {
        "date": "2025-03",
        "title": "Augmented Adversarial Trigger Learning",
        "author": "Zhe Wang, and Yanjun Qi",
        "link": "http://arxiv.org/abs/2503.12339v1",
        "abstract": "Gradient optimization-based adversarial attack methods automate the learning\nof adversarial triggers to generate jailbreak prompts or leak system prompts.\nIn this work, we take a closer look at the optimization objective of\nadversarial trigger learning and propose ATLA: Adversarial Trigger Learning\nwith Augmented objectives. ATLA improves the negative log-likelihood loss used\nby previous studies into a weighted loss formulation that encourages the\nlearned adversarial triggers to optimize more towards response format tokens.\nThis enables ATLA to learn an adversarial trigger from just one query-response\npair and the learned trigger generalizes well to other similar queries. We\nfurther design a variation to augment trigger optimization with an auxiliary\nloss that suppresses evasive responses. We showcase how to use ATLA to learn\nadversarial suffixes jailbreaking LLMs and to extract hidden system prompts.\nEmpirically we demonstrate that ATLA consistently outperforms current\nstate-of-the-art techniques, achieving nearly 100% success in attacking while\nrequiring 80% fewer queries. ATLA learned jailbreak suffixes demonstrate high\ngeneralization to unseen queries and transfer well to new LLMs."
    },
    {
        "date": "2025-03",
        "title": "One Goal, Many Challenges: Robust Preference Optimization Amid Content-Aware and Multi-Source Noise",
        "author": "Amirabbas Afzali, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi, and Sanjay Lall",
        "link": "http://arxiv.org/abs/2503.12301v1",
        "abstract": "Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness."
    },
    {
        "date": "2025-03",
        "title": "FedTilt: Towards Multi-Level Fairness-Preserving and Robust Federated Learning",
        "author": "Binghui Zhang, Luis Mares De La Cruz, and Binghui Wang",
        "link": "http://arxiv.org/abs/2503.13537v1",
        "abstract": "Federated Learning (FL) is an emerging decentralized learning paradigm that\ncan partly address the privacy concern that cannot be handled by traditional\ncentralized and distributed learning. Further, to make FL practical, it is also\nnecessary to consider constraints such as fairness and robustness. However,\nexisting robust FL methods often produce unfair models, and existing fair FL\nmethods only consider one-level (client) fairness and are not robust to\npersistent outliers (i.e., injected outliers into each training round) that are\ncommon in real-world FL settings. We propose \\texttt{FedTilt}, a novel FL that\ncan preserve multi-level fairness and be robust to outliers. In particular, we\nconsider two common levels of fairness, i.e., \\emph{client fairness} --\nuniformity of performance across clients, and \\emph{client data fairness} --\nuniformity of performance across different classes of data within a client.\n\\texttt{FedTilt} is inspired by the recently proposed tilted empirical risk\nminimization, which introduces tilt hyperparameters that can be flexibly tuned.\nTheoretically, we show how tuning tilt values can achieve the two-level\nfairness and mitigate the persistent outliers, and derive the convergence\ncondition of \\texttt{FedTilt} as well. Empirically, our evaluation results on a\nsuite of realistic federated datasets in diverse settings show the\neffectiveness and flexibility of the \\texttt{FedTilt} framework and the\nsuperiority to the state-of-the-arts."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels",
        "author": "Chengxuan Qian, Kai Han, Siqi Ma, Chongwen Lyu, Zhenlong Yuan, Jun Chen, and Zhe Liu",
        "link": "http://arxiv.org/abs/2503.12218v1",
        "abstract": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods."
    },
    {
        "date": "2025-03",
        "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
        "author": "Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, and Jianfei Cai",
        "link": "http://arxiv.org/abs/2503.12150v1",
        "abstract": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
    },
    {
        "date": "2025-03",
        "title": "Robust Isolation Forest using Soft Sparse Random Projection and Valley Emphasis Method",
        "author": "Hun Kang, and Kyoungok Kim",
        "link": "http://arxiv.org/abs/2503.12125v1",
        "abstract": "Isolation Forest (iForest) is an unsupervised anomaly detection algorithm\ndesigned to effectively detect anomalies under the assumption that anomalies\nare ``few and different.\" Various studies have aimed to enhance iForest, but\nthe resulting algorithms often exhibited significant performance disparities\nacross datasets. Additionally, the challenge of isolating rare and widely\ndistributed anomalies persisted in research focused on improving splits. To\naddress these challenges, we introduce Robust iForest (RiForest). RiForest\nleverages both existing features and random hyperplanes obtained through soft\nsparse random projection to identify superior split features for anomaly\ndetection, independent of datasets. It utilizes the underutilized valley\nemphasis method for optimal split point determination and incorporates sparsity\nrandomization in soft sparse random projection for enhanced anomaly detection\nrobustness. Across 24 benchmark datasets, experiments demonstrate RiForest's\nconsistent outperformance of existing algorithms in anomaly detection,\nemphasizing stability and robustness to noise variables."
    },
    {
        "date": "2025-03",
        "title": "Robust Dataset Distillation by Matching Adversarial Trajectories",
        "author": "Wei Lai, Tianyu Ding, ren dongdong, Lei Wang, Jing Huo, Yang Gao, and Wenbin Li",
        "link": "http://arxiv.org/abs/2503.12069v1",
        "abstract": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Training-Inference Trigger Intensity in Backdoor Attacks",
        "author": "Chenhao Lin, Chenyang Zhao, Shiwei Wang, Longtian Wang, Chao Shen, and Zhengyu Zhao",
        "link": "http://arxiv.org/abs/2503.12058v1",
        "abstract": "Backdoor attacks typically place a specific trigger on certain training data,\nsuch that the model makes prediction errors on inputs with that trigger during\ninference. Despite the core role of the trigger, existing studies have commonly\nbelieved a perfect match between training-inference triggers is optimal. In\nthis paper, for the first time, we systematically explore the\ntraining-inference trigger relation, particularly focusing on their mismatch,\nbased on a Training-Inference Trigger Intensity Manipulation (TITIM) workflow.\nTITIM specifically investigates the training-inference trigger intensity, such\nas the size or the opacity of a trigger, and reveals new insights into trigger\ngeneralization and overfitting.\n  These new insights challenge the above common belief by demonstrating that\nthe training-inference trigger mismatch can facilitate attacks in two practical\nscenarios, posing more significant security threats than previously thought.\nFirst, when the inference trigger is fixed, using training triggers with mixed\nintensities leads to stronger attacks than using any single intensity. For\nexample, on CIFAR-10 with ResNet-18, mixing training triggers with 1.0 and 0.1\nopacities improves the worst-case attack success rate (ASR) (over different\ntesting opacities) of the best single-opacity attack from 10.61\\% to 92.77\\%.\nSecond, intentionally using certain mismatched training-inference triggers can\nimprove the attack stealthiness, i.e., better bypassing defenses. For example,\ncompared to the training/inference intensity of 1.0/1.0, using 1.0/0.7\ndecreases the area under the curve (AUC) of the Scale-Up defense from 0.96 to\n0.62, while maintaining a high attack ASR (99.65\\% vs. 91.62\\%). The above new\ninsights are validated to be generalizable across different backdoor attacks,\nmodels, datasets, tasks, and (digital/physical) domains."
    },
    {
        "date": "2025-03",
        "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training",
        "author": "Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.12030v1",
        "abstract": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt."
    },
    {
        "date": "2025-03",
        "title": "Mixed-feature Logistic Regression Robust to Distribution Shifts",
        "author": "Qingshi Sun, Nathan Justin, Andres Gomez, and Phebe Vayanos",
        "link": "http://arxiv.org/abs/2503.12012v1",
        "abstract": "Logistic regression models are widely used in the social and behavioral\nsciences and in high-stakes domains, due to their simplicity and\ninterpretability properties. At the same time, such domains are permeated by\ndistribution shifts, where the distribution generating the data changes between\ntraining and deployment. In this paper, we study a distributionally robust\nlogistic regression problem that seeks the model that will perform best against\nadversarial realizations of the data distribution drawn from a suitably\nconstructed Wasserstein ambiguity set. Our model and solution approach differ\nfrom prior work in that we can capture settings where the likelihood of\ndistribution shifts can vary across features, significantly broadening the\napplicability of our model relative to the state-of-the-art. We propose a\ngraph-based solution approach that can be integrated into off-the-shelf\noptimization solvers. We evaluate the performance of our model and algorithms\non numerous publicly available datasets. Our solution achieves a 408x speed-up\nrelative to the state-of-the-art. Additionally, compared to the\nstate-of-the-art, our model reduces average calibration error by up to 36.19%\nand worst-case calibration error by up to 41.70%, while increasing the average\narea under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to\n48.37%."
    },
    {
        "date": "2025-03",
        "title": "Winning the MIDST Challenge: New Membership Inference Attacks on Diffusion Models for Tabular Data Synthesis",
        "author": "Xiaoyu Wu, Yifei Pang, Terrance Liu, and Steven Wu",
        "link": "http://arxiv.org/abs/2503.12008v1",
        "abstract": "Tabular data synthesis using diffusion models has gained significant\nattention for its potential to balance data utility and privacy. However,\nexisting privacy evaluations often rely on heuristic metrics or weak membership\ninference attacks (MIA), leaving privacy risks inadequately assessed. In this\nwork, we conduct a rigorous MIA study on diffusion-based tabular synthesis,\nrevealing that state-of-the-art attacks designed for image models fail in this\nsetting. We identify noise initialization as a key factor influencing attack\nefficacy and propose a machine-learning-driven approach that leverages loss\nfeatures across different noises and time steps. Our method, implemented with a\nlightweight MLP, effectively learns membership signals, eliminating the need\nfor manual optimization. Experimental results from the MIDST Challenge @ SaTML\n2025 demonstrate the effectiveness of our approach, securing first place across\nall tracks. Code is available at\nhttps://github.com/Nicholas0228/Tartan_Federer_MIDST."
    },
    {
        "date": "2025-03",
        "title": "Internet of Things-Based Smart Precision Farming in Soilless Agriculture: Opportunities and Challenges for Global Food Security",
        "author": "Monica Dutta, Deepali Gupta, Sumegh Tharewal, Deepam Goyal, Jasminder Kaur Sandhu, Manjit Kaur, Ahmad Ali Alzubi, and Jazem Mutared Alanazi",
        "link": "http://arxiv.org/abs/2503.13528v1",
        "abstract": "The rapid growth of the global population and the continuous decline in\ncultivable land pose significant threats to food security. This challenge\nworsens as climate change further reduces the availability of farmland.\nSoilless agriculture, such as hydroponics, aeroponics, and aquaponics, offers a\nsustainable solution by enabling efficient crop cultivation in controlled\nenvironments. The integration of the Internet of Things (IoT) with smart\nprecision farming improves resource efficiency, automates environmental\ncontrol, and ensures stable and high-yield crop production. IoT-enabled smart\nfarming systems utilize real-time monitoring, data-driven decision-making, and\nautomation to optimize water and nutrient usage while minimizing human\nintervention. This paper explores the opportunities and challenges of IoT-based\nsoilless farming, highlighting its role in sustainable agriculture, urban\nfarming, and global food security. These advanced farming methods ensure\ngreater productivity, resource conservation, and year-round cultivation.\nHowever, they also face challenges such as high initial investment,\ntechnological dependency, and energy consumption. Through a comprehensive\nstudy, bibliometric analysis, and comparative analysis, this research\nhighlights current trends and research gaps. It also outlines future directions\nfor researchers, policymakers, and industry stakeholders to drive innovation\nand scalability in IoT-driven soilless agriculture. By emphasizing the benefits\nof vertical farming and Controlled Environment Agriculture (CEA)-enabled\nsoilless techniques, this paper supports informed decision-making to address\nfood security challenges and promote sustainable agricultural innovations."
    },
    {
        "date": "2025-03",
        "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder",
        "author": "Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, and Yanxia Zhang",
        "link": "http://arxiv.org/abs/2503.11937v1",
        "abstract": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model."
    },
    {
        "date": "2025-03",
        "title": "Trust Under Siege: Label Spoofing Attacks against Machine Learning for Android Malware Detection",
        "author": "Tianwei Lan, Luca Demetrio, Farid Nait-Abdesselam, Yufei Han, and Simone Aonzo",
        "link": "http://arxiv.org/abs/2503.11841v1",
        "abstract": "Machine learning (ML) malware detectors rely heavily on crowd-sourced\nAntiVirus (AV) labels, with platforms like VirusTotal serving as a trusted\nsource of malware annotations. But what if attackers could manipulate these\nlabels to classify benign software as malicious? We introduce label spoofing\nattacks, a new threat that contaminates crowd-sourced datasets by embedding\nminimal and undetectable malicious patterns into benign samples. These patterns\ncoerce AV engines into misclassifying legitimate files as harmful, enabling\npoisoning attacks against ML-based malware classifiers trained on those data.\nWe demonstrate this scenario by developing AndroVenom, a methodology for\npolluting realistic data sources, causing consequent poisoning attacks against\nML malware detectors. Experiments show that not only state-of-the-art feature\nextractors are unable to filter such injection, but also various ML models\nexperience Denial of Service already with 1% poisoned samples. Additionally,\nattackers can flip decisions of specific unaltered benign samples by modifying\nonly 0.015% of the training data, threatening their reputation and market share\nand being unable to be stopped by anomaly detectors on training data. We\nconclude our manuscript by raising the alarm on the trustworthiness of the\ntraining process based on AV annotations, requiring further investigation on\nhow to produce proper labels for ML malware detectors."
    },
    {
        "date": "2025-03",
        "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
        "author": "Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, and Leonid Karlinsky",
        "link": "http://arxiv.org/abs/2503.11790v1",
        "abstract": "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Resiliency of Sketch-based Security via LSB Sharing-based Dynamic Late Merging",
        "author": "Seungsam Yang, Seyed Mohammad Mehdi Mirnajafizadeh, Sian Kim, Rhongho Jang, and DaeHun Nyang",
        "link": "http://arxiv.org/abs/2503.11777v1",
        "abstract": "With the exponentially growing Internet traffic, sketch data structure with a\nprobabilistic algorithm has been expected to be an alternative solution for\nnon-compromised (non-selective) security monitoring. While facilitating\ncounting within a confined memory space, the sketch's memory efficiency and\naccuracy were further pushed to their limit through finer-grained and dynamic\ncontrol of constrained memory space to adapt to the data stream's inherent\nskewness (i.e., Zipf distribution), namely small counters with extensions. In\nthis paper, we unveil a vulnerable factor of the small counter design by\nintroducing a new sketch-oriented attack, which threatens a stream of\nstate-of-the-art sketches and their security applications. With the root cause\nanalyses, we propose Siamese Counter with enhanced adversarial resiliency and\nverified feasibility with extensive experimental and theoretical analyses.\nUnder a sketch pollution attack, Siamese Counter delivers 47% accurate results\nthan a state-of-the-art scheme, and demonstrates up to 82% more accurate\nestimation under normal measurement scenarios."
    },
    {
        "date": "2025-03",
        "title": "Centaur: Robust End-to-End Autonomous Driving with Test-Time Training",
        "author": "Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, and Jose M. Alvarez",
        "link": "http://arxiv.org/abs/2503.11650v1",
        "abstract": "How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels."
    },
    {
        "date": "2025-03",
        "title": "Are Deep Speech Denoising Models Robust to Adversarial Noise?",
        "author": "Will Schwarzer, Philip S. Thomas, Andrea Fanelli, and Xiaoyu Liu",
        "link": "http://arxiv.org/abs/2503.11627v1",
        "abstract": "Deep noise suppression (DNS) models enjoy widespread use throughout a variety\nof high-stakes speech applications. However, in this paper, we show that four\nrecent DNS models can each be reduced to outputting unintelligible gibberish\nthrough the addition of imperceptible adversarial noise. Furthermore, our\nresults show the near-term plausibility of targeted attacks, which could induce\nmodels to output arbitrary utterances, and over-the-air attacks. While the\nsuccess of these attacks varies by model and setting, and attacks appear to be\nstrongest when model-specific (i.e., white-box and non-transferable), our\nresults highlight a pressing need for practical countermeasures in DNS systems."
    },
    {
        "date": "2025-03",
        "title": "Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense",
        "author": "Shuyang Hao, Yiwei Wang, Bryan Hooi, Ming-Hsuan Yang, Jun Liu, Chengcheng Tang, Zi Huang, and Yujun Cai",
        "link": "http://arxiv.org/abs/2503.11619v1",
        "abstract": "Deploying large vision-language models (LVLMs) introduces a unique\nvulnerability: susceptibility to malicious attacks via visual inputs. However,\nexisting defense methods suffer from two key limitations: (1) They solely focus\non textual defenses, fail to directly address threats in the visual domain\nwhere attacks originate, and (2) the additional processing steps often incur\nsignificant computational overhead or compromise model performance on benign\ntasks. Building on these insights, we propose ESIII (Embedding Security\nInstructions Into Images), a novel methodology for transforming the visual\nspace from a source of vulnerability into an active defense mechanism.\nInitially, we embed security instructions into defensive images through\ngradient-based optimization, obtaining security instructions in the visual\ndimension. Subsequently, we integrate security instructions from visual and\ntextual dimensions with the input query. The collaboration between security\ninstructions from different dimensions ensures comprehensive security\nprotection. Extensive experiments demonstrate that our approach effectively\nfortifies the robustness of LVLMs against such attacks while preserving their\nperformance on standard benign tasks and incurring an imperceptible increase in\ntime costs."
    },
    {
        "date": "2025-03",
        "title": "Leveraging Angle of Arrival Estimation against Impersonation Attacks in Physical Layer Authentication",
        "author": "Thuy M. Pham, Linda Senigagliesi, Marco Baldi, Rafael F. Schaefer, Gerhard P. Fettweis, and Arsenia Chorti",
        "link": "http://arxiv.org/abs/2503.11508v1",
        "abstract": "In this paper, we investigate the utilization of the angle of arrival (AoA)\nas a feature for robust physical layer authentication (PLA). While most of the\nexisting approaches to PLA focus on common features of the physical layer of\ncommunication channels, such as channel frequency response, channel impulse\nresponse or received signal strength, the use of AoA in this domain has not yet\nbeen studied in depth, particularly regarding the ability to thwart\nimpersonation attacks. In this work, we demonstrate that an impersonation\nattack targeting AoA based PLA is only feasible under strict conditions on the\nattacker's location and hardware capabilities, which highlights the AoA's\npotential as a strong feature for PLA. We extend previous works considering a\nsingle-antenna attacker to the case of a multiple-antenna attacker, and we\ndevelop a theoretical characterization of the conditions in which a successful\nimpersonation attack can be mounted. Furthermore, we leverage extensive\nsimulations in support of theoretical analyses, to validate the robustness of\nAoA-based PLA."
    },
    {
        "date": "2025-03",
        "title": "Dynamic Obstacle Avoidance with Bounded Rationality Adversarial Reinforcement Learning",
        "author": "Jose-Luis Holgado-Alvarez, Aryaman Reddi, and Carlo D'Eramo",
        "link": "http://arxiv.org/abs/2503.11467v1",
        "abstract": "Reinforcement Learning (RL) has proven largely effective in obtaining stable\nlocomotion gaits for legged robots. However, designing control algorithms which\ncan robustly navigate unseen environments with obstacles remains an ongoing\nproblem within quadruped locomotion. To tackle this, it is convenient to solve\nnavigation tasks by means of a hierarchical approach with a low-level\nlocomotion policy and a high-level navigation policy. Crucially, the high-level\npolicy needs to be robust to dynamic obstacles along the path of the agent. In\nthis work, we propose a novel way to endow navigation policies with robustness\nby a training process that models obstacles as adversarial agents, following\nthe adversarial RL paradigm. Importantly, to improve the reliability of the\ntraining process, we bound the rationality of the adversarial agent resorting\nto quantal response equilibria, and place a curriculum over its rationality. We\ncalled this method Hierarchical policies via Quantal response Adversarial\nReinforcement Learning (Hi-QARL). We demonstrate the robustness of our method\nby benchmarking it in unseen randomized mazes with multiple obstacles. To prove\nits applicability in real scenarios, our method is applied on a Unitree GO1\nrobot in simulation."
    },
    {
        "date": "2025-03",
        "title": "In Shift and In Variance: Assessing the Robustness of HAR Deep Learning Models against Variability",
        "author": "Azhar Ali Khaked, Nobuyuki Oishi, Daniel Roggen, and Paula Lago",
        "link": "http://arxiv.org/abs/2503.11466v1",
        "abstract": "Human Activity Recognition (HAR) using wearable inertial measurement unit\n(IMU) sensors can revolutionize healthcare by enabling continual health\nmonitoring, disease prediction, and routine recognition. Despite the high\naccuracy of Deep Learning (DL) HAR models, their robustness to real-world\nvariabilities remains untested, as they have primarily been trained and tested\non limited lab-confined data. In this study, we isolate subject, device,\nposition, and orientation variability to determine their effect on DL HAR\nmodels and assess the robustness of these models in real-world conditions. We\nevaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a\ncomprehensive discussion on the impact of variability on data distribution\nshifts and changes in model performance. Our experiments measured shifts in\ndata distribution using Maximum Mean Discrepancy (MMD) and observed DL model\nperformance drops due to variability. We concur that studied variabilities\naffect DL HAR models differently, and there is an inverse relationship between\ndata distribution shifts and model performance. The compounding effect of\nvariability was analyzed, and the implications of variabilities in real-world\nscenarios were highlighted. MMD proved an effective metric for calculating data\ndistribution shifts and explained the drop in performance due to variabilities\nin HARVAR and REALDISP datasets. Combining our understanding of variability\nwith evaluating its effects will facilitate the development of more robust DL\nHAR models and optimal training techniques. Allowing Future models to not only\nbe assessed based on their maximum F1 score but also on their ability to\ngeneralize effectively"
    },
    {
        "date": "2025-03",
        "title": "BACE-RUL: A Bi-directional Adversarial Network with Covariate Encoding for Machine Remaining Useful Life Prediction",
        "author": "Zekai Zhang, Dan Li, Shunyu Wu, Junya Cai, Bo Zhang, See Kiong Ng, and Zibin Zheng",
        "link": "http://arxiv.org/abs/2503.11730v1",
        "abstract": "Prognostic and Health Management (PHM) are crucial ways to avoid unnecessary\nmaintenance for Cyber-Physical Systems (CPS) and improve system reliability.\nPredicting the Remaining Useful Life (RUL) is one of the most challenging tasks\nfor PHM. Existing methods require prior knowledge about the system, contrived\nassumptions, or temporal mining to model the life cycles of machine\nequipment/devices, resulting in diminished accuracy and limited applicability\nin real-world scenarios. This paper proposes a Bi-directional Adversarial\nnetwork with Covariate Encoding for machine Remaining Useful Life (BACE-RUL)\nprediction, which only adopts sensor measurements from the current life cycle\nto predict RUL rather than relying on previous consecutive cycle recordings.\nThe current sensor measurements of mechanical devices are encoded to a\nconditional space to better understand the implicit inner mechanical status.\nThe predictor is trained as a conditional generative network with the encoded\nsensor measurements as its conditions. Various experiments on several\nreal-world datasets, including the turbofan aircraft engine dataset and the\ndataset collected from degradation experiments of Li-Ion battery cells, show\nthat the proposed model is a general framework and outperforms state-of-the-art\nmethods."
    },
    {
        "date": "2025-03",
        "title": "Align in Depth: Defending Jailbreak Attacks via Progressive Answer Detoxification",
        "author": "Yingjie Zhang, Tong Liu, Zhe Zhao, Guozhu Meng, and Kai Chen",
        "link": "http://arxiv.org/abs/2503.11185v1",
        "abstract": "Large Language Models (LLMs) are vulnerable to jailbreak attacks, which use\ncrafted prompts to elicit toxic responses. These attacks exploit LLMs'\ndifficulty in dynamically detecting harmful intents during the generation\nprocess. Traditional safety alignment methods, often relying on the initial few\ngeneration steps, are ineffective due to limited computational budget. This\npaper proposes DEEPALIGN, a robust defense framework that fine-tunes LLMs to\nprogressively detoxify generated content, significantly improving both the\ncomputational budget and effectiveness of mitigating harmful generation. Our\napproach uses a hybrid loss function operating on hidden states to directly\nimprove LLMs' inherent awareness of toxity during generation. Furthermore, we\nredefine safe responses by generating semantically relevant answers to harmful\nqueries, thereby increasing robustness against representation-mutation attacks.\nEvaluations across multiple LLMs demonstrate state-of-the-art defense\nperformance against six different attack types, reducing Attack Success Rates\nby up to two orders of magnitude compared to previous state-of-the-art defense\nwhile preserving utility. This work advances LLM safety by addressing\nlimitations of conventional alignment through dynamic, context-aware\nmitigation."
    },
    {
        "date": "2025-03",
        "title": "DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation",
        "author": "Hongbin Lin, Zilu Guo, Yifan Zhang, Shuaicheng Niu, Yafeng Li, Ruimao Zhang, Shuguang Cui, and Zhen Li",
        "link": "http://arxiv.org/abs/2503.11122v1",
        "abstract": "In autonomous driving, vision-centric 3D detection aims to identify 3D\nobjects from images. However, high data collection costs and diverse real-world\nscenarios limit the scale of training data. Once distribution shifts occur\nbetween training and test data, existing methods often suffer from performance\ndegradation, known as Out-of-Distribution (OOD) problems. To address this,\ncontrollable Text-to-Image (T2I) diffusion offers a potential solution for\ntraining data enhancement, which is required to generate diverse OOD scenarios\nwith precise 3D object geometry. Nevertheless, existing controllable T2I\napproaches are restricted by the limited scale of training data or struggle to\npreserve all annotated 3D objects. In this paper, we present DriveGEN, a method\ndesigned to improve the robustness of 3D detectors in Driving via Training-Free\nControllable Text-to-Image Diffusion Generation. Without extra diffusion model\ntraining, DriveGEN consistently preserves objects with precise 3D geometry\nacross diverse OOD generations, consisting of 2 stages: 1) Self-Prototype\nExtraction: We empirically find that self-attention features are semantic-aware\nbut require accurate region selection for 3D objects. Thus, we extract precise\nobject features via layouts to capture 3D object geometry, termed\nself-prototypes. 2) Prototype-Guided Diffusion: To preserve objects across\nvarious OOD scenarios, we perform semantic-aware feature alignment and shallow\nfeature alignment during denoising. Extensive experiments demonstrate the\neffectiveness of DriveGEN in improving 3D detection. The code is available at\nhttps://github.com/Hongbin98/DriveGEN."
    },
    {
        "date": "2025-03",
        "title": "Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data",
        "author": "Lilin Zhang, Chengpei Wu, and Ning Yang",
        "link": "http://arxiv.org/abs/2503.11032v2",
        "abstract": "Existing adversarial training (AT) methods often suffer from incomplete\nperturbation, meaning that not all non-robust features are perturbed when\ngenerating adversarial examples (AEs). This results in residual correlations\nbetween non-robust features and labels, leading to suboptimal learning of\nrobust features. However, achieving complete perturbation, i.e., perturbing as\nmany non-robust features as possible, is challenging due to the difficulty in\ndistinguishing robust and non-robust features and the sparsity of labeled data.\nTo address these challenges, we propose a novel approach called Weakly\nSupervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete\nperturbation for improved learning of robust features by disrupting\ncorrelations between non-robust features and labels through complete AE\ngeneration over partially labeled data, grounded in information theory.\nExtensive theoretical analysis and comprehensive experiments on widely adopted\nbenchmarks validate the superiority of WSCAT. Our code is available at\nhttps://github.com/zhang-lilin/WSCAT."
    },
    {
        "date": "2025-03",
        "title": "Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching",
        "author": "Ruochen Hou, Mingzhang Zhu, Hyunwoo Nam, Gabriel I. Fernandez, and Dennis W. Hong",
        "link": "http://arxiv.org/abs/2503.11020v1",
        "abstract": "Accurate robot localization is essential for effective operation. Monte Carlo\nLocalization (MCL) is commonly used with known maps but is computationally\nexpensive due to landmark matching for each particle. Humanoid robots face\nadditional challenges, including sensor noise from locomotion vibrations and a\nlimited field of view (FOV) due to camera placement. This paper proposes a fast\nand robust localization method via iterative landmark matching (ILM) for\nhumanoid robots. The iterative matching process improves the accuracy of the\nlandmark association so that it does not need MCL to match landmarks to\nparticles. Pose estimation with the outlier removal process enhances its\nrobustness to measurement noise and faulty detections. Furthermore, an\nadditional filter can be utilized to fuse inertial data from the inertial\nmeasurement unit (IMU) and pose data from localization. We compared ILM with\nIterative Closest Point (ICP), which shows that ILM method is more robust\ntowards the error in the initial guess and easier to get a correct matching. We\nalso compared ILM with the Augmented Monte Carlo Localization (aMCL), which\nshows that ILM method is much faster than aMCL and even more accurate. The\nproposed method's effectiveness is thoroughly evaluated through experiments and\nvalidated on the humanoid robot ARTEMIS during RoboCup 2024 adult-sized soccer\ncompetition."
    },
    {
        "date": "2025-03",
        "title": "From Abstraction to Reality: DARPA's Vision for Robust Sim-to-Real Autonomy",
        "author": "Erfaun Noorani, Zachary Serlin, Ben Price, and Alvaro Velasquez",
        "link": "http://arxiv.org/abs/2503.11007v1",
        "abstract": "The DARPA Transfer from Imprecise and Abstract Models to Autonomous\nTechnologies (TIAMAT) program aims to address rapid and robust transfer of\nautonomy technologies across dynamic and complex environments, goals, and\nplatforms. Existing methods for simulation-to-reality (sim-to-real) transfer\noften rely on high-fidelity simulations and struggle with broad adaptation,\nparticularly in time-sensitive scenarios. Although many approaches have shown\nincredible performance at specific tasks, most techniques fall short when posed\nwith unforeseen, complex, and dynamic real-world scenarios due to the inherent\nlimitations of simulation. In contrast to current research that aims to bridge\nthe gap between simulation environments and the real world through increasingly\nsophisticated simulations and a combination of methods typically assuming a\nsmall sim-to-real gap -- such as domain randomization, domain adaptation,\nimitation learning, meta-learning, policy distillation, and dynamic\noptimization -- TIAMAT takes a different approach by instead emphasizing\ntransfer and adaptation of the autonomy stack directly to real-world\nenvironments by utilizing a breadth of low(er)-fidelity simulations to create\nbroadly effective sim-to-real transfers. By abstractly learning from multiple\nsimulation environments in reference to their shared semantics, TIAMAT's\napproaches aim to achieve abstract-to-real transfer for effective and rapid\nreal-world adaptation. Furthermore, this program endeavors to improve the\noverall autonomy pipeline by addressing the inherent challenges in translating\nsimulated behaviors into effective real-world performance."
    },
    {
        "date": "2025-03",
        "title": "ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with Multi-Modal Large Language Models and General Vision Models",
        "author": "Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch",
        "link": "http://arxiv.org/abs/2503.10937v1",
        "abstract": "Face Recognition Systems (FRS) are increasingly vulnerable to face-morphing\nattacks, prompting the development of Morphing Attack Detection (MAD)\nalgorithms. However, a key challenge in MAD lies in its limited\ngeneralizability to unseen data and its lack of explainability-critical for\npractical application environments such as enrolment stations and automated\nborder control systems. Recognizing that most existing MAD algorithms rely on\nsupervised learning paradigms, this work explores a novel approach to MAD using\nzero-shot learning leveraged on Large Language Models (LLMs). We propose two\ntypes of zero-shot MAD algorithms: one leveraging general vision models and the\nother utilizing multimodal LLMs. For general vision models, we address the MAD\ntask by computing the mean support embedding of an independent support set\nwithout using morphed images. For the LLM-based approach, we employ the\nstate-of-the-art GPT-4 Turbo API with carefully crafted prompts. To evaluate\nthe feasibility of zero-shot MAD and the effectiveness of the proposed methods,\nwe constructed a print-scan morph dataset featuring various unseen morphing\nalgorithms, simulating challenging real-world application scenarios.\nExperimental results demonstrated notable detection accuracy, validating the\napplicability of zero-shot learning for MAD tasks. Additionally, our\ninvestigation into LLM-based MAD revealed that multimodal LLMs, such as\nChatGPT, exhibit remarkable generalizability to untrained MAD tasks.\nFurthermore, they possess a unique ability to provide explanations and\nguidance, which can enhance transparency and usability for end-users in\npractical applications."
    },
    {
        "date": "2025-03",
        "title": "Attacking Multimodal OS Agents with Malicious Image Patches",
        "author": "Lukas Aichberger, Alasdair Paren, Yarin Gal, Philip Torr, and Adel Bibi",
        "link": "http://arxiv.org/abs/2503.10809v1",
        "abstract": "Recent advances in operating system (OS) agents enable vision-language models\nto interact directly with the graphical user interface of an OS. These\nmultimodal OS agents autonomously perform computer-based tasks in response to a\nsingle prompt via application programming interfaces (APIs). Such APIs\ntypically support low-level operations, including mouse clicks, keyboard\ninputs, and screenshot captures. We introduce a novel attack vector: malicious\nimage patches (MIPs) that have been adversarially perturbed so that, when\ncaptured in a screenshot, they cause an OS agent to perform harmful actions by\nexploiting specific APIs. For instance, MIPs embedded in desktop backgrounds or\nshared on social media can redirect an agent to a malicious website, enabling\nfurther exploitation. These MIPs generalise across different user requests and\nscreen layouts, and remain effective for multiple OS agents. The existence of\nsuch attacks highlights critical security vulnerabilities in OS agents, which\nshould be carefully addressed before their widespread adoption."
    },
    {
        "date": "2025-03",
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "author": "Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen",
        "link": "http://arxiv.org/abs/2503.10635v1",
        "abstract": "Despite promising performance on open-source large vision-language models\n(LVLMs), transfer-based targeted attacks often fail against black-box\ncommercial LVLMs. Analyzing failed adversarial perturbations reveals that the\nlearned perturbations typically originate from a uniform distribution and lack\nclear semantic details, resulting in unintended responses. This critical\nabsence of semantic information leads commercial LVLMs to either ignore the\nperturbation entirely or misinterpret its embedded semantics, thereby causing\nthe attack to fail. To overcome these issues, we notice that identifying core\nsemantic objects is a key objective for models trained with various datasets\nand methodologies. This insight motivates our approach that refines semantic\nclarity by encoding explicit semantic details within local regions, thus\nensuring interoperability and capturing finer-grained features, and by\nconcentrating modifications on semantically rich areas rather than applying\nthem uniformly. To achieve this, we propose a simple yet highly effective\nsolution: at each optimization step, the adversarial image is cropped randomly\nby a controlled aspect ratio and scale, resized, and then aligned with the\ntarget image in the embedding space. Experimental results confirm our\nhypothesis. Our adversarial examples crafted with local-aggregated\nperturbations focused on crucial regions exhibit surprisingly good\ntransferability to commercial LVLMs, including GPT-4.5, GPT-4o,\nGemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning\nmodels like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach\nachieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly\noutperforming all prior state-of-the-art attack methods. Our optimized\nadversarial examples under different configurations and training code are\navailable at https://github.com/VILA-Lab/M-Attack."
    },
    {
        "date": "2025-03",
        "title": "Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology",
        "author": "Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan",
        "link": "http://arxiv.org/abs/2503.10629v1",
        "abstract": "Adversarial attacks pose significant challenges for vision models in critical\nfields like healthcare, where reliability is essential. Although adversarial\ntraining has been well studied in natural images, its application to biomedical\nand microscopy data remains limited. Existing self-supervised adversarial\ntraining methods overlook the hierarchical structure of histopathology images,\nwhere patient-slide-patch relationships provide valuable discriminative\nsignals. To address this, we propose Hierarchical Self-Supervised Adversarial\nTraining (HSAT), which exploits these properties to craft adversarial examples\nusing multi-level contrastive learning and integrate it into adversarial\ntraining for enhanced robustness. We evaluate HSAT on multiclass histopathology\ndataset OpenSRH and the results show that HSAT outperforms existing methods\nfrom both biomedical and natural image domains. HSAT enhances robustness,\nachieving an average gain of 54.31% in the white-box setting and reducing\nperformance drops to 3-4% in the black-box setting, compared to 25-30% for the\nbaseline. These results set a new benchmark for adversarial training in this\ndomain, paving the way for more robust models. Our Code for training and\nevaluation is available at https://github.com/HashmatShadab/HSAT."
    },
    {
        "date": "2025-03",
        "title": "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity Analysis",
        "author": "Nannan Wu, Zengqiang Yan, Nong Sang, Li Yu, and Chang Wen Chen",
        "link": "http://arxiv.org/abs/2503.10567v1",
        "abstract": "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "MASQUE: A Text-Guided Diffusion-Based Framework for Localized and Customized Adversarial Makeup",
        "author": "Youngjin Kwon, and Xiao Zhang",
        "link": "http://arxiv.org/abs/2503.10549v1",
        "abstract": "As facial recognition is increasingly adopted for government and commercial\nservices, its potential misuse has raised serious concerns about privacy and\ncivil rights. To counteract, various anti-facial recognition techniques have\nbeen proposed for privacy protection by adversarially perturbing face images,\namong which generative makeup-based approaches are the most popular. However,\nthese methods, designed primarily to impersonate specific target identities,\ncan only achieve weak dodging success rates while increasing the risk of\ntargeted abuse. In addition, they often introduce global visual artifacts or a\nlack of adaptability to accommodate diverse makeup prompts, compromising user\nsatisfaction. To address the above limitations, we develop MASQUE, a novel\ndiffusion-based framework that generates localized adversarial makeups guided\nby user-defined text prompts. Built upon precise null-text inversion,\ncustomized cross-attention fusion with masking, and a pairwise adversarial\nguidance mechanism using images of the same individual, MASQUE achieves robust\ndodging performance without requiring any external identity. Comprehensive\nevaluations on open-source facial recognition models and commercial APIs\ndemonstrate that MASQUE significantly improves dodging success rates over all\nbaselines, along with higher perceptual fidelity and stronger adaptability to\nvarious text makeup prompts."
    },
    {
        "date": "2025-03",
        "title": "Whisper Speaker Identification: Leveraging Pre-Trained Multilingual Transformers for Robust Speaker Embeddings",
        "author": "Jakaria Islam Emon, Md Abu Salek, and Kazi Tamanna Alam",
        "link": "http://arxiv.org/abs/2503.10446v1",
        "abstract": "Speaker identification in multilingual settings presents unique challenges,\nparticularly when conventional models are predominantly trained on English\ndata. In this paper, we propose WSI (Whisper Speaker Identification), a\nframework that repurposes the encoder of the Whisper automatic speech\nrecognition model pre trained on extensive multilingual data to generate robust\nspeaker embeddings via a joint loss optimization strategy that leverages online\nhard triplet mining and self supervised Normalized Temperature-scaled Cross\nEntropy loss. By capitalizing on Whisper language-agnostic acoustic\nrepresentations, our approach effectively distinguishes speakers across diverse\nlanguages and recording conditions. Extensive evaluations on multiple corpora,\nincluding VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish,\nChinese, and Japanese), and Voxconverse (English), demonstrate that WSI\nconsistently outperforms state-of-the-art baselines, namely Pyannote Embedding,\nECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC\nscores. These results validate our hypothesis that a multilingual pre-trained\nASR encoder, combined with joint loss optimization, substantially improves\nspeaker identification performance in non-English languages."
    },
    {
        "date": "2025-03",
        "title": "HyperArm Bandit Optimization: A Novel approach to Hyperparameter Optimization and an Analysis of Bandit Algorithms in Stochastic and Adversarial Settings",
        "author": "Samih Karroum, and Saad Mazhar",
        "link": "http://arxiv.org/abs/2503.10282v1",
        "abstract": "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization."
    },
    {
        "date": "2025-03",
        "title": "Robust Learning-Based Sparse Recovery for Device Activity Detection in Grant-Free Random Access Cell-Free Massive MIMO: Enhancing Resilience to Impairments",
        "author": "Ali Elkeshawy, Haifa Fares, and Amor Nafkha",
        "link": "http://arxiv.org/abs/2503.10280v1",
        "abstract": "Massive MIMO is considered a key enabler to support massive machine-type\ncommunication (mMTC). While massive access schemes have been extensively\nanalyzed for co-located massive MIMO arrays, this paper explores activity\ndetection in grant-free random access for mMTC within the context of cell-free\nmassive MIMO systems, employing distributed antenna arrays. This sparse support\nrecovery of device activity status is performed by a finite cluster of access\npoints (APs) from a large number of geographically distributed APs\ncollaborating to serve a larger number of devices. Active devices transmit\nnon-orthogonal pilot sequences to APs, which forward the received signals to a\ncentral processing unit (CPU) for collaborative activity detection. This paper\nproposes a simple and efficient data-driven algorithm tailored for device\nactivity detection, implemented centrally at the CPU. Furthermore, the study\nassesses the algorithm's robustness to input perturbations and examines the\neffects of adopting fixed-point representation on its performance."
    },
    {
        "date": "2025-03",
        "title": "Numerically robust Gaussian state estimation with singular observation noise",
        "author": "Nicholas Kr\u00e4mer, and Filip Tronarp",
        "link": "http://arxiv.org/abs/2503.10279v1",
        "abstract": "This article proposes numerically robust algorithms for Gaussian state\nestimation with singular observation noise. Our approach combines a series of\nbasis changes with Bayes' rule, transforming the singular estimation problem\ninto a nonsingular one with reduced state dimension. In addition to ensuring\nlow runtime and numerical stability, our proposal facilitates\nmarginal-likelihood computations and Gauss-Markov representations of the\nposterior process. We analyse the proposed method's computational savings and\nnumerical robustness and validate our findings in a series of simulations."
    },
    {
        "date": "2025-03",
        "title": "Robustness Tokens: Towards Adversarial Robustness of Transformers",
        "author": "Brian Pulfer, Yury Belousov, and Slava Voloshynovskiy",
        "link": "http://arxiv.org/abs/2503.10191v1",
        "abstract": "Recently, large pre-trained foundation models have become widely adopted by\nmachine learning practitioners for a multitude of tasks. Given that such models\nare publicly available, relying on their use as backbone models for downstream\ntasks might result in high vulnerability to adversarial attacks crafted with\nthe same public model. In this work, we propose Robustness Tokens, a novel\napproach specific to the transformer architecture that fine-tunes a few\nadditional private tokens with low computational requirements instead of tuning\nmodel parameters as done in traditional adversarial training. We show that\nRobustness Tokens make Vision Transformer models significantly more robust to\nwhite-box adversarial attacks while also retaining the original downstream\nperformances."
    },
    {
        "date": "2025-03",
        "title": "Exploring the Vulnerabilities of Federated Learning: A Deep Dive into Gradient Inversion Attacks",
        "author": "Pengxin Guo, Runxi Wang, Shuang Zeng, Jinjing Zhu, Haoning Jiang, Yanran Wang, Yuyin Zhou, Feifei Wang, Hui Xiong, and Liangqiong Qu",
        "link": "http://arxiv.org/abs/2503.11514v1",
        "abstract": "Federated Learning (FL) has emerged as a promising privacy-preserving\ncollaborative model training paradigm without sharing raw data. However, recent\nstudies have revealed that private information can still be leaked through\nshared gradient information and attacked by Gradient Inversion Attacks (GIA).\nWhile many GIA methods have been proposed, a detailed analysis, evaluation, and\nsummary of these methods are still lacking. Although various survey papers\nsummarize existing privacy attacks in FL, few studies have conducted extensive\nexperiments to unveil the effectiveness of GIA and their associated limiting\nfactors in this context. To fill this gap, we first undertake a systematic\nreview of GIA and categorize existing methods into three types, i.e.,\n\\textit{optimization-based} GIA (OP-GIA), \\textit{generation-based} GIA\n(GEN-GIA), and \\textit{analytics-based} GIA (ANA-GIA). Then, we comprehensively\nanalyze and evaluate the three types of GIA in FL, providing insights into the\nfactors that influence their performance, practicality, and potential threats.\nOur findings indicate that OP-GIA is the most practical attack setting despite\nits unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA\nis easily detectable, making them both impractical. Finally, we offer a\nthree-stage defense pipeline to users when designing FL frameworks and\nprotocols for better privacy protection and share some future research\ndirections from the perspectives of attackers and defenders that we believe\nshould be pursued. We hope that our study can help researchers design more\nrobust FL frameworks to defend against these attacks."
    },
    {
        "date": "2025-03",
        "title": "Team NYCU at Defactify4: Robust Detection and Source Identification of AI-Generated Images Using CNN and CLIP-Based Models",
        "author": "Tsan-Tsung Yang, I-Wei Chen, Kuan-Ting Chen, Shang-Hsuan Chiang, and Wen-Chih Peng",
        "link": "http://arxiv.org/abs/2503.10718v1",
        "abstract": "With the rapid advancement of generative AI, AI-generated images have become\nincreasingly realistic, raising concerns about creativity, misinformation, and\ncontent authenticity. Detecting such images and identifying their source models\nhas become a critical challenge in ensuring the integrity of digital media.\nThis paper tackles the detection of AI-generated images and identifying their\nsource models using CNN and CLIP-ViT classifiers. For the CNN-based classifier,\nwe leverage EfficientNet-B0 as the backbone and feed with RGB channels,\nfrequency features, and reconstruction errors, while for CLIP-ViT, we adopt a\npretrained CLIP image encoder to extract image features and SVM to perform\nclassification. Evaluated on the Defactify 4 dataset, our methods demonstrate\nstrong performance in both tasks, with CLIP-ViT showing superior robustness to\nimage perturbations. Compared to baselines like AEROBLADE and OCC-CLIP, our\napproach achieves competitive results. Notably, our method ranked Top-3 overall\nin the Defactify 4 competition, highlighting its effectiveness and\ngeneralizability. All of our implementations can be found in\nhttps://github.com/uuugaga/Defactify_4"
    },
    {
        "date": "2025-03",
        "title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption",
        "author": "Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, and Sung-eui Yoon",
        "link": "http://arxiv.org/abs/2503.10081v1",
        "abstract": "The outstanding capability of diffusion models in generating high-quality\nimages poses significant threats when misused by adversaries. In particular, we\nassume malicious adversaries exploiting diffusion models for inpainting tasks,\nsuch as replacing a specific region with a celebrity. While existing methods\nfor protecting images from manipulation in diffusion-based generative models\nhave primarily focused on image-to-image and text-to-image tasks, the challenge\nof preventing unauthorized inpainting has been rarely addressed, often\nresulting in suboptimal protection performance. To mitigate inpainting abuses,\nwe propose ADVPAINT, a novel defensive framework that generates adversarial\nperturbations that effectively disrupt the adversary's inpainting tasks.\nADVPAINT targets the self- and cross-attention blocks in a target diffusion\ninpainting model to distract semantic understanding and prompt interactions\nduring image generation. ADVPAINT also employs a two-stage perturbation\nstrategy, dividing the perturbation region based on an enlarged bounding box\naround the object, enhancing robustness across diverse masks of varying shapes\nand sizes. Our experimental results demonstrate that ADVPAINT's perturbations\nare highly effective in disrupting the adversary's inpainting tasks,\noutperforming existing methods; ADVPAINT attains over a 100-point increase in\nFID and substantial decreases in precision."
    },
    {
        "date": "2025-03",
        "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's Latest ISA Extension",
        "author": "Taehun Kim, Hyerean Jang, and Youngjoo Shin",
        "link": "http://arxiv.org/abs/2503.10074v1",
        "abstract": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
    },
    {
        "date": "2025-03",
        "title": "Provably Secure Covert Messaging Using Image-based Diffusion Processes",
        "author": "Luke A. Bauer, Wenxuan Bao, and Vincent Bindschaedler",
        "link": "http://arxiv.org/abs/2503.10063v1",
        "abstract": "We consider the problem of securely and robustly embedding covert messages\ninto an image-based diffusion model's output. The sender and receiver want to\nexchange the maximum amount of information possible per diffusion sampled image\nwhile remaining undetected. The adversary wants to detect that such\ncommunication is taking place by identifying those diffusion samples that\ncontain covert messages. To maximize robustness to transformations of the\ndiffusion sample, a strategy is for the sender and the receiver to embed the\nmessage in the initial latents. We first show that prior work that attempted\nthis is easily broken because their embedding technique alters the latents'\ndistribution. We then propose a straightforward method to embed covert messages\nin the initial latent {\\em without} altering the distribution. We prove that\nour construction achieves indistinguishability to any probabilistic polynomial\ntime adversary. Finally, we discuss and analyze empirically the tradeoffs\nbetween embedding capacity, message recovery rates, and robustness. We find\nthat optimizing the inversion method for error correction is crucial for\nreliability."
    },
    {
        "date": "2025-03",
        "title": "Distributionally Robust Multi-Agent Reinforcement Learning for Dynamic Chute Mapping",
        "author": "Guangyi Liu, Suzan Iloglu, Michael Caldara, Joseph W. Durham, and Michael M. Zavlanos",
        "link": "http://arxiv.org/abs/2503.09755v1",
        "abstract": "In Amazon robotic warehouses, the destination-to-chute mapping problem is\ncrucial for efficient package sorting. Often, however, this problem is\ncomplicated by uncertain and dynamic package induction rates, which can lead to\nincreased package recirculation. To tackle this challenge, we introduce a\nDistributionally Robust Multi-Agent Reinforcement Learning (DRMARL) framework\nthat learns a destination-to-chute mapping policy that is resilient to\nadversarial variations in induction rates. Specifically, DRMARL relies on group\ndistributionally robust optimization (DRO) to learn a policy that performs well\nnot only on average but also on each individual subpopulation of induction\nrates within the group that capture, for example, different seasonality or\noperation modes of the system. This approach is then combined with a novel\ncontextual bandit-based predictor of the worst-case induction distribution for\neach state-action pair, significantly reducing the cost of exploration and\nthereby increasing the learning efficiency and scalability of our framework.\nExtensive simulations demonstrate that DRMARL achieves robust chute mapping in\nthe presence of varying induction distributions, reducing package recirculation\nby an average of 80\\% in the simulation scenario."
    },
    {
        "date": "2025-03",
        "title": "Enhancing Adversarial Example Detection Through Model Explanation",
        "author": "Qian Ma, and Ziping Ye",
        "link": "http://arxiv.org/abs/2503.09735v1",
        "abstract": "Adversarial examples are a major problem for machine learning models, leading\nto a continuous search for effective defenses. One promising direction is to\nleverage model explanations to better understand and defend against these\nattacks. We looked at AmI, a method proposed by a NeurIPS 2018 spotlight paper\nthat uses model explanations to detect adversarial examples. Our study shows\nthat while AmI is a promising idea, its performance is too dependent on\nspecific settings (e.g., hyperparameter) and external factors such as the\noperating system and the deep learning framework used, and such drawbacks limit\nAmI's practical usage. Our findings highlight the need for more robust defense\nmechanisms that are effective under various conditions. In addition, we\nadvocate for a comprehensive evaluation framework for defense techniques."
    },
    {
        "date": "2025-03",
        "title": "How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?",
        "author": "Mir Imtiaz Mostafiz, Imtiaz Karim, and Elisa Bertino",
        "link": "http://arxiv.org/abs/2503.09726v1",
        "abstract": "Graph Neural Networks (GNNs) are widely used and deployed for graph-based\nprediction tasks. However, as good as GNNs are for learning graph data, they\nalso come with the risk of privacy leakage. For instance, an attacker can run\ncarefully crafted queries on the GNNs and, from the responses, can infer the\nexistence of an edge between a pair of nodes. This attack, dubbed as a\n\"link-stealing\" attack, can jeopardize the user's privacy by leaking\npotentially sensitive information. To protect against this attack, we propose\nan approach called \"$(N)$ode $(A)$ugmentation for $(R)$estricting $(G)$raphs\nfrom $(I)$nsinuating their $(S)$tructure\" ($NARGIS$) and study its feasibility.\n$NARGIS$ is focused on reshaping the graph embedding space so that the\nposterior from the GNN model will still provide utility for the prediction task\nbut will introduce ambiguity for the link-stealing attackers. To this end,\n$NARGIS$ applies spectral clustering on the given graph to facilitate it being\naugmented with new nodes -- that have learned features instead of fixed ones.\nIt utilizes tri-level optimization for learning parameters for the GNN model,\nsurrogate attacker model, and our defense model (i.e. learnable node features).\nWe extensively evaluate $NARGIS$ on three benchmark citation datasets over\neight knowledge availability settings for the attackers. We also evaluate the\nmodel fidelity and defense performance on influence-based link inference\nattacks. Through our studies, we have figured out the best feature of $NARGIS$\n-- its superior fidelity-privacy performance trade-off in a significant number\nof cases. We also have discovered in which cases the model needs to be\nimproved, and proposed ways to integrate different schemes to make the model\nmore robust against link stealing attacks."
    },
    {
        "date": "2025-03",
        "title": "Revisiting Backdoor Attacks on Time Series Classification in the Frequency Domain",
        "author": "Yuanmin Huang, Mi Zhang, Zhaoxiang Wang, Wenxuan Li, and Min Yang",
        "link": "http://arxiv.org/abs/2503.09712v2",
        "abstract": "Time series classification (TSC) is a cornerstone of modern web applications,\npowering tasks such as financial data analysis, network traffic monitoring, and\nuser behavior analysis. In recent years, deep neural networks (DNNs) have\ngreatly enhanced the performance of TSC models in these critical domains.\nHowever, DNNs are vulnerable to backdoor attacks, where attackers can covertly\nimplant triggers into models to induce malicious outcomes. Existing backdoor\nattacks targeting DNN-based TSC models remain elementary. In particular, early\nmethods borrow trigger designs from computer vision, which are ineffective for\ntime series data. More recent approaches utilize generative models for trigger\ngeneration, but at the cost of significant computational complexity. In this\nwork, we analyze the limitations of existing attacks and introduce an enhanced\nmethod, FreqBack. Drawing inspiration from the fact that DNN models inherently\ncapture frequency domain features in time series data, we identify that\nimproper perturbations in the frequency domain are the root cause of\nineffective attacks. To address this, we propose to generate triggers both\neffectively and efficiently, guided by frequency analysis. FreqBack exhibits\nsubstantial performance across five models and eight datasets, achieving an\nimpressive attack success rate of over 90%, while maintaining less than a 3%\ndrop in model accuracy on clean data."
    },
    {
        "date": "2025-03",
        "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
        "author": "Sangwon Jang, June Suk Choi, Jaehyeong Jo, Kimin Lee, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2503.09669v1",
        "abstract": "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos."
    },
    {
        "date": "2025-03",
        "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for Trigger-Action IoT Environment",
        "author": "Md Morshed Alam, Lokesh Chandra Das, Sandip Roy, Sachin Shetty, and Weichao Wang",
        "link": "http://arxiv.org/abs/2503.09513v1",
        "abstract": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment",
        "author": "Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, and Donald E. Brown",
        "link": "http://arxiv.org/abs/2503.09498v1",
        "abstract": "Healthcare relies on multiple types of data, such as medical images, genetic\ninformation, and clinical records, to improve diagnosis and treatment. However,\nmissing data is a common challenge due to privacy restrictions, cost, and\ntechnical issues, making many existing multi-modal models unreliable. To\naddress this, we propose a new multi-model model called Mixture of Experts,\nSymmetric Aligning, and Reconstruction (MoSARe), a deep learning framework that\nhandles incomplete multimodal data while maintaining high accuracy. MoSARe\nintegrates expert selection, cross-modal attention, and contrastive learning to\nimprove feature representation and decision-making. Our results show that\nMoSARe outperforms existing models in situations when the data is complete.\nFurthermore, it provides reliable predictions even when some data are missing.\nThis makes it especially useful in real-world healthcare settings, including\nresource-limited environments. Our code is publicly available at\nhttps://github.com/NazaninMn/MoSARe."
    },
    {
        "date": "2025-03",
        "title": "Robust Multimodal Survival Prediction with the Latent Differentiation Conditional Variational AutoEncoder",
        "author": "Junjie Zhou, Jiao Tang, Yingli Zuo, Peng Wan, Daoqiang Zhang, and Wei Shao",
        "link": "http://arxiv.org/abs/2503.09496v2",
        "abstract": "The integrative analysis of histopathological images and genomic data has\nreceived increasing attention for survival prediction of human cancers.\nHowever, the existing studies always hold the assumption that full modalities\nare available. As a matter of fact, the cost for collecting genomic data is\nhigh, which sometimes makes genomic data unavailable in testing samples. A\ncommon way of tackling such incompleteness is to generate the genomic\nrepresentations from the pathology images. Nevertheless, such strategy still\nfaces the following two challenges: (1) The gigapixel whole slide images (WSIs)\nare huge and thus hard for representation. (2) It is difficult to generate the\ngenomic embeddings with diverse function categories in a unified generative\nframework. To address the above challenges, we propose a Conditional Latent\nDifferentiation Variational AutoEncoder (LD-CVAE) for robust multimodal\nsurvival prediction, even with missing genomic data. Specifically, a\nVariational Information Bottleneck Transformer (VIB-Trans) module is proposed\nto learn compressed pathological representations from the gigapixel WSIs. To\ngenerate different functional genomic features, we develop a novel Latent\nDifferentiation Variational AutoEncoder (LD-VAE) to learn the common and\nspecific posteriors for the genomic embeddings with diverse functions. Finally,\nwe use the product-of-experts technique to integrate the genomic common\nposterior and image posterior for the joint latent distribution estimation in\nLD-CVAE. We test the effectiveness of our method on five different cancer\ndatasets, and the experimental results demonstrate its superiority in both\ncomplete and missing modality scenarios."
    },
    {
        "date": "2025-03",
        "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
        "author": "Beier Zhu, Jiequan Cui, Hanwang Zhang, and Chi Zhang",
        "link": "http://arxiv.org/abs/2503.09487v2",
        "abstract": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
    },
    {
        "date": "2025-03",
        "title": "Automatic Association of Quality Requirements and Quantifiable Metrics for Cloud Security Certification",
        "author": "John Bianchi, Shuya Dong, Luca Petrillo, and Marinella Petrocchi",
        "link": "http://arxiv.org/abs/2503.09460v1",
        "abstract": "The European Cybersecurity Certification Scheme for Cloud Services (EUCS) is\none of the first cybersecurity schemes in Europe, defined by the European Union\nAgency for Cybersecurity (ENISA). It aims to encourage cloud providers to\nstrengthen their cybersecurity policies in order to receive an official seal of\napproval from European authorities. EUCS defines a set of security requirements\nthat the cloud provider must meet, in whole or in part, in order to achieve the\nsecurity certification. The requirements are written in natural language and\ncover every aspect of security in the cloud environment, from logging access to\nprotecting the system with anti-malware tools to training staff. Operationally,\neach requirement is associated with one or more evaluable metrics. For example,\na requirement to monitor access attempts to a service will have associated\nmetrics that take into account the number of accesses, the number of access\nattempts, who is accessing, and what resources are being used. Partners in the\nEuropean project Medina, which ended in October 2023, defined 163 metrics and\nmanually mapped them to 70 EUCS requirements. Manual mapping is intuitively a\nlong and costly process in terms of human resources. This paper proposes an\napproach based on Sentence Transformers to automatically associate requirements\nand metrics. In terms of correctness of associations, the proposed method\nachieves a Normalized Discounted Cumulative Gain of 0.640, improving a previous\nexperiment by 0.146 points."
    },
    {
        "date": "2025-03",
        "title": "Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization",
        "author": "Amit Attia, and Tomer Koren",
        "link": "http://arxiv.org/abs/2503.09411v1",
        "abstract": "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1/(2p+1)}/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios."
    },
    {
        "date": "2025-03",
        "title": "AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",
        "author": "Claudius Kienle, Benjamin Alt, Finn Schneider, Tobias Pertlwieser, Rainer J\u00e4kel, and Rania Rayyes",
        "link": "http://arxiv.org/abs/2503.09409v1",
        "abstract": "Despite the widespread adoption of industrial robots in automotive assembly,\nwire harness installation remains a largely manual process, as it requires\nprecise and flexible manipulation. To address this challenge, we design a novel\nAI-based framework that automates cable connector mating by integrating force\ncontrol with deep visuotactile learning. Our system optimizes\nsearch-and-insertion strategies using first-order optimization over a\nmultimodal transformer architecture trained on visual, tactile, and\nproprioceptive data. Additionally, we design a novel automated data collection\nand optimization pipeline that minimizes the need for machine learning\nexpertise. The framework optimizes robot programs that run natively on standard\nindustrial controllers, permitting human experts to audit and certify them.\nExperimental validations on a center console assembly task demonstrate\nsignificant improvements in cycle times and robustness compared to conventional\nrobot programming approaches. Videos are available under\nhttps://claudius-kienle.github.io/AppMuTT."
    },
    {
        "date": "2025-03",
        "title": "Membership Inference Attacks fueled by Few-Short Learning to detect privacy leakage tackling data integrity",
        "author": "Daniel Jim\u00e9nez-L\u00f3pez, Nuria Rodr\u00edguez-Barroso, M. Victoria Luz\u00f3n, and Francisco Herrera",
        "link": "http://arxiv.org/abs/2503.09365v1",
        "abstract": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information."
    },
    {
        "date": "2025-03",
        "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
        "author": "Hongyu Chen, and Seraphina Goldfarb-Tarrant",
        "link": "http://arxiv.org/abs/2503.09347v1",
        "abstract": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
    },
    {
        "date": "2025-03",
        "title": "Stealthy Patch-Wise Backdoor Attack in 3D Point Cloud via Curvature Awareness",
        "author": "Yu Feng, Dingxin Zhang, Runkai Zhao, Yong Xia, Heng Huang, and Weidong Cai",
        "link": "http://arxiv.org/abs/2503.09336v1",
        "abstract": "Backdoor attacks pose a severe threat to deep neural networks (DNN) by\nimplanting hidden backdoors that can be activated with predefined triggers to\nmanipulate model behaviors maliciously. Existing 3D point cloud backdoor\nattacks primarily rely on sample-wise global modifications, resulting in\nsuboptimal stealthiness. To address this limitation, we propose Stealthy\nPatch-Wise Backdoor Attack (SPBA), which employs the first patch-wise trigger\nfor 3D point clouds and restricts perturbations to local regions, significantly\nenhancing stealthiness. Specifically, SPBA decomposes point clouds into local\npatches and evaluates their geometric complexity using a curvature-based patch\nimperceptibility score, ensuring that the trigger remains less perceptible to\nthe human eye by strategically applying it across multiple geometrically\ncomplex patches with lower visual sensitivity. By leveraging the Graph Fourier\nTransform (GFT), SPBA optimizes a patch-wise spectral trigger that perturbs the\nspectral features of selected patches, enhancing attack effectiveness while\npreserving the global geometric structure of the point cloud. Extensive\nexperiments on ModelNet40 and ShapeNetPart demonstrate that SPBA consistently\nachieves an attack success rate (ASR) exceeding 96.5% across different models\nwhile achieving state-of-the-art imperceptibility compared to existing backdoor\nattack methods."
    },
    {
        "date": "2025-03",
        "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data",
        "author": "Adel ElZemity, Budi Arief, and Shujun Li",
        "link": "http://arxiv.org/abs/2503.09334v1",
        "abstract": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance."
    },
    {
        "date": "2025-03",
        "title": "Group-robust Machine Unlearning",
        "author": "Thomas De Min, Subhankar Roy, St\u00e9phane Lathuili\u00e8re, Elisa Ricci, and Massimiliano Mancini",
        "link": "http://arxiv.org/abs/2503.09330v1",
        "abstract": "Machine unlearning is an emerging paradigm to remove the influence of\nspecific training data (i.e., the forget set) from a model while preserving its\nknowledge of the rest of the data (i.e., the retain set). Previous approaches\nassume the forget data to be uniformly distributed from all training\ndatapoints. However, if the data to unlearn is dominant in one group, we\nempirically show that performance for this group degrades, leading to fairness\nissues. This work tackles the overlooked problem of non-uniformly distributed\nforget sets, which we call group-robust machine unlearning, by presenting a\nsimple, effective strategy that mitigates the performance loss in dominant\ngroups via sample distribution reweighting. Moreover, we present MIU (Mutual\nInformation-aware Machine Unlearning), the first approach for group robustness\nin approximate machine unlearning. MIU minimizes the mutual information between\nmodel features and group information, achieving unlearning while reducing\nperformance degradation in the dominant group of the forget set. Additionally,\nMIU exploits sample distribution reweighting and mutual information calibration\nwith the original model to preserve group robustness. We conduct experiments on\nthree datasets and show that MIU outperforms standard methods, achieving\nunlearning without compromising model robustness. Source code available at\nhttps://github.com/tdemin16/group-robust_machine_unlearning."
    },
    {
        "date": "2025-03",
        "title": "Towards Robust Model Evolution with Algorithmic Recourse",
        "author": "Hao-Tsung Yang, Jie Gao, Bo-Yi Liu, and Zhi-Xuan Liu",
        "link": "http://arxiv.org/abs/2503.09658v1",
        "abstract": "Algorithmic Recourse is a way for users to modify their attributes to align\nwith a model's expectations, thereby improving their outcomes after receiving\nunfavorable decisions. In real-world scenarios, users often need to\nstrategically adjust their attributes to compete for limited resources.\nHowever, such strategic behavior induces users to \"game\" algorithms, causing\nmodel collapse due to distribution shifts. These shifts arise from user\ncompetition, resource constraints, and adaptive user responses. While prior\nresearch on Algorithmic Recourse has explored its effects on both systems and\nusers, the impact of resource constraints and competition over time remains\nunderexplored. In this work, we develop a general framework to model user\nstrategic behaviors and their interactions with decision-making systems under\nresource constraints and competitive dynamics. Through theoretical analysis and\nempirical evaluation, we identify three key phenomena that arise consistently\nin both synthetic and real-world datasets: escalating decision boundaries,\nnon-robust model predictions, and inequitable recourse actions. Finally, we\ndiscuss the broader social implications of these findings and present two\nalgorithmic strategies aimed at mitigating these challenges."
    },
    {
        "date": "2025-03",
        "title": "Detecting and Preventing Data Poisoning Attacks on AI Models",
        "author": "Halima I. Kure, Pradipta Sarkar, Ahmed B. Ndanusa, and Augustine O. Nwajana",
        "link": "http://arxiv.org/abs/2503.09302v1",
        "abstract": "This paper investigates the critical issue of data poisoning attacks on AI\nmodels, a growing concern in the ever-evolving landscape of artificial\nintelligence and cybersecurity. As advanced technology systems become\nincreasingly prevalent across various sectors, the need for robust defence\nmechanisms against adversarial attacks becomes paramount. The study aims to\ndevelop and evaluate novel techniques for detecting and preventing data\npoisoning attacks, focusing on both theoretical frameworks and practical\napplications. Through a comprehensive literature review, experimental\nvalidation using the CIFAR-10 and Insurance Claims datasets, and the\ndevelopment of innovative algorithms, this paper seeks to enhance the\nresilience of AI models against malicious data manipulation. The study explores\nvarious methods, including anomaly detection, robust optimization strategies,\nand ensemble learning, to identify and mitigate the effects of poisoned data\nduring model training. Experimental results indicate that data poisoning\nsignificantly degrades model performance, reducing classification accuracy by\nup to 27% in image recognition tasks (CIFAR-10) and 22% in fraud detection\nmodels (Insurance Claims dataset). The proposed defence mechanisms, including\nstatistical anomaly detection and adversarial training, successfully mitigated\npoisoning effects, improving model robustness and restoring accuracy levels by\nan average of 15-20%. The findings further demonstrate that ensemble learning\ntechniques provide an additional layer of resilience, reducing false positives\nand false negatives caused by adversarial data injections."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inference Attack on Distributed Large Language Model Inference Frameworks",
        "author": "Xinjian Luo, Ting Yu, and Xiaokui Xiao",
        "link": "http://arxiv.org/abs/2503.09291v1",
        "abstract": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications."
    },
    {
        "date": "2025-03",
        "title": "In-Context Defense in Computer Agents: An Empirical Study",
        "author": "Pei Yang, Hai Ci, and Mike Zheng Shou",
        "link": "http://arxiv.org/abs/2503.09241v1",
        "abstract": "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior."
    },
    {
        "date": "2025-03",
        "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted Clients",
        "author": "Xiuwen Fang, Mang Ye, and Bo Du",
        "link": "http://arxiv.org/abs/2503.09206v1",
        "abstract": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL."
    },
    {
        "date": "2025-03",
        "title": "AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks",
        "author": "Jin Li, Ziqiang He, Anwei Luo, Jian-Fang Hu, Z. Jane Wang, and Xiangui Kang",
        "link": "http://arxiv.org/abs/2503.09124v1",
        "abstract": "Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible\nperturbation to the input data. Previous methods typically improve the\nimperceptibility of attacks by integrating common attack paradigms with\nspecifically designed perception-based losses or the capabilities of generative\nmodels. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a\nnovel modeling framework distinct from existing attack paradigms. AdvAD\ninnovatively conceptualizes attacking as a non-parametric diffusion process by\ntheoretically exploring basic modeling approach rather than using the denoising\nor generation abilities of regular diffusion models requiring neural networks.\nAt each step, much subtler yet effective adversarial guidance is crafted using\nonly the attacked model without any additional network, which gradually leads\nthe end of diffusion process from the original image to a desired imperceptible\nadversarial example. Grounded in a solid theoretical foundation of the proposed\nnon-parametric diffusion process, AdvAD achieves high attack efficacy and\nimperceptibility with intrinsically lower overall perturbation strength.\nAdditionally, an enhanced version AdvAD-X is proposed to evaluate the extreme\nof our novel framework under an ideal scenario. Extensive experiments\ndemonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with\nstate-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9$\\%$\n(+17.3$\\%$) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971\n(+0.0043) SSIM against four prevalent DNNs with three different architectures\non the ImageNet-compatible dataset. Code is available at\nhttps://github.com/XianguiKang/AdvAD."
    },
    {
        "date": "2025-03",
        "title": "C^2 ATTACK: Towards Representation Backdoor on CLIP via Concept Confusion",
        "author": "Lijie Hu, Junchi Liao, Weimin Lyu, Shaopeng Fu, Tianhao Huang, Shu Yang, Guimin Hu, and Di Wang",
        "link": "http://arxiv.org/abs/2503.09095v1",
        "abstract": "Backdoor attacks pose a significant threat to deep learning models, enabling\nadversaries to embed hidden triggers that manipulate the behavior of the model\nduring inference. Traditional backdoor attacks typically rely on inserting\nexplicit triggers (e.g., external patches, or perturbations) into input data,\nbut they often struggle to evade existing defense mechanisms. To address this\nlimitation, we investigate backdoor attacks through the lens of the reasoning\nprocess in deep learning systems, drawing insights from interpretable AI. We\nconceptualize backdoor activation as the manipulation of learned concepts\nwithin the model's latent representations. Thus, existing attacks can be seen\nas implicit manipulations of these activated concepts during inference. This\nraises interesting questions: why not manipulate the concepts explicitly? This\nidea leads to our novel backdoor attack framework, Concept Confusion Attack\n(C^2 ATTACK), which leverages internal concepts in the model's reasoning as\n\"triggers\" without introducing explicit external modifications. By avoiding the\nuse of real triggers and directly activating or deactivating specific concepts\nin latent spaces, our approach enhances stealth, making detection by existing\ndefenses significantly harder. Using CLIP as a case study, experimental results\ndemonstrate the effectiveness of C^2 ATTACK, achieving high attack success\nrates while maintaining robustness against advanced defenses."
    },
    {
        "date": "2025-03",
        "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and Manipulating Adversarial States",
        "author": "Xin Wei Chia, and Jonathan Pan",
        "link": "http://arxiv.org/abs/2503.09066v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level."
    },
    {
        "date": "2025-03",
        "title": "Mono2D: A Trainable Monogenic Layer for Robust Knee Cartilage Segmentation on Out-of-Distribution 2D Ultrasound Data",
        "author": "Alvin Kimbowa, Arjun Parmar, Maziar Badii, David Liu, Matthew Harkey, and Ilker Hacihaliloglu",
        "link": "http://arxiv.org/abs/2503.09050v2",
        "abstract": "Automated knee cartilage segmentation using point-of-care ultrasound devices\nand deep-learning networks has the potential to enhance the management of knee\nosteoarthritis. However, segmentation algorithms often struggle with domain\nshifts caused by variations in ultrasound devices and acquisition parameters,\nlimiting their generalizability. In this paper, we propose Mono2D, a monogenic\nlayer that extracts multi-scale, contrast- and intensity-invariant local phase\nfeatures using trainable bandpass quadrature filters. This layer mitigates\ndomain shifts, improving generalization to out-of-distribution domains. Mono2D\nis integrated before the first layer of a segmentation network, and its\nparameters jointly trained alongside the network's parameters. We evaluated\nMono2D on a multi-domain 2D ultrasound knee cartilage dataset for single-source\ndomain generalization (SSDG). Our results demonstrate that Mono2D outperforms\nother SSDG methods in terms of Dice score and mean average surface distance. To\nfurther assess its generalizability, we evaluate Mono2D on a multi-site\nprostate MRI dataset, where it continues to outperform other SSDG methods,\nhighlighting its potential to improve domain generalization in medical imaging.\nNevertheless, further evaluation on diverse datasets is still necessary to\nassess its clinical utility."
    },
    {
        "date": "2025-03",
        "title": "Adaptive Backdoor Attacks with Reasonable Constraints on Graph Neural Networks",
        "author": "Xuewen Dong, Jiachen Li, Shujun Li, Zhichao You, Qiang Qu, Yaroslav Kholodov, and Yulong Shen",
        "link": "http://arxiv.org/abs/2503.09049v1",
        "abstract": "Recent studies show that graph neural networks (GNNs) are vulnerable to\nbackdoor attacks. Existing backdoor attacks against GNNs use fixed-pattern\ntriggers and lack reasonable trigger constraints, overlooking individual graph\ncharacteristics and rendering insufficient evasiveness. To tackle the above\nissues, we propose ABARC, the first Adaptive Backdoor Attack with Reasonable\nConstraints, applying to both graph-level and node-level tasks in GNNs. For\ngraph-level tasks, we propose a subgraph backdoor attack independent of the\ngraph's topology. It dynamically selects trigger nodes for each target graph\nand modifies node features with constraints based on graph similarity, feature\nrange, and feature type. For node-level tasks, our attack begins with an\nanalysis of node features, followed by selecting and modifying trigger\nfeatures, which are then constrained by node similarity, feature range, and\nfeature type. Furthermore, an adaptive edge-pruning mechanism is designed to\nreduce the impact of neighbors on target nodes, ensuring a high attack success\nrate (ASR). Experimental results show that even with reasonable constraints for\nattack evasiveness, our attack achieves a high ASR while incurring a marginal\nclean accuracy drop (CAD). When combined with the state-of-the-art defense\nrandomized smoothing (RS) method, our attack maintains an ASR over 94%,\nsurpassing existing attacks by more than 7%."
    },
    {
        "date": "2025-03",
        "title": "Prompt Inversion Attack against Collaborative Inference of Large Language Models",
        "author": "Wenjie Qu, Yuguang Zhou, Yongji Wu, Tingsong Xiao, Binhang Yuan, Yiming Li, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2503.09022v2",
        "abstract": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
    },
    {
        "date": "2025-03",
        "title": "Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models",
        "author": "Shahnewaz Karim Sakib, Anindya Bijoy Das, and Shibbir Ahmed",
        "link": "http://arxiv.org/abs/2503.10690v1",
        "abstract": "Adversarial factuality refers to the deliberate insertion of misinformation\ninto input prompts by an adversary, characterized by varying levels of\nexpressed confidence. In this study, we systematically evaluate the performance\nof several open-source large language models (LLMs) when exposed to such\nadversarial inputs. Three tiers of adversarial confidence are considered:\nstrongly confident, moderately confident, and limited confidence. Our analysis\nencompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (3.8B), Qwen 2.5 (7B),\nDeepseek-v2 (16B), Gemma2 (9B), Falcon (7B), Mistrallite (7B), and LLaVA (7B).\nEmpirical results indicate that LLaMA 3.1 (8B) exhibits a robust capability in\ndetecting adversarial inputs, whereas Falcon (7B) shows comparatively lower\nperformance. Notably, for the majority of the models, detection success\nimproves as the adversary's confidence decreases; however, this trend is\nreversed for LLaMA 3.1 (8B) and Phi 3 (3.8B), where a reduction in adversarial\nconfidence corresponds with diminished detection performance. Further analysis\nof the queries that elicited the highest and lowest rates of successful attacks\nreveals that adversarial attacks are more effective when targeting less\ncommonly referenced or obscure information."
    },
    {
        "date": "2025-03",
        "title": "Not All Edges are Equally Robust: Evaluating the Robustness of Ranking-Based Federated Learning",
        "author": "Zirui Gong, Yanjun Zhang, Leo Yu Zhang, Zhaoxi Zhang, Yong Xiang, and Shirui Pan",
        "link": "http://arxiv.org/abs/2503.08976v1",
        "abstract": "Federated Ranking Learning (FRL) is a state-of-the-art FL framework that\nstands out for its communication efficiency and resilience to poisoning\nattacks. It diverges from the traditional FL framework in two ways: 1) it\nleverages discrete rankings instead of gradient updates, significantly reducing\ncommunication costs and limiting the potential space for malicious updates, and\n2) it uses majority voting on the server side to establish the global ranking,\nensuring that individual updates have minimal influence since each client\ncontributes only a single vote. These features enhance the system's scalability\nand position FRL as a promising paradigm for FL training.\n  However, our analysis reveals that FRL is not inherently robust, as certain\nedges are particularly vulnerable to poisoning attacks. Through a theoretical\ninvestigation, we prove the existence of these vulnerable edges and establish a\nlower bound and an upper bound for identifying them in each layer. Based on\nthis finding, we introduce a novel local model poisoning attack against FRL,\nnamely the Vulnerable Edge Manipulation (VEM) attack. The VEM attack focuses on\nidentifying and perturbing the most vulnerable edges in each layer and\nleveraging an optimization-based approach to maximize the attack's impact.\nThrough extensive experiments on benchmark datasets, we demonstrate that our\nattack achieves an overall 53.23% attack impact and is 3.7x more impactful than\nexisting methods. Our findings highlight significant vulnerabilities in\nranking-based FL systems and underline the urgency for the development of new\nrobust FL frameworks."
    },
    {
        "date": "2025-03",
        "title": "Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks",
        "author": "Idris Zakariyya, Ferheen Ayaz, Mounia Kharbouche-Harrari, Jeremy Singer, Sye Loong Keoh, Danilo Pau, and Jos\u00e9 Cano",
        "link": "http://arxiv.org/abs/2503.08973v1",
        "abstract": "Reducing the memory footprint of Machine Learning (ML) models, especially\nDeep Neural Networks (DNNs), is imperative to facilitate their deployment on\nresource-constrained edge devices. However, a notable drawback of DNN models\nlies in their susceptibility to adversarial attacks, wherein minor input\nperturbations can deceive them. A primary challenge revolves around the\ndevelopment of accurate, resilient, and compact DNN models suitable for\ndeployment on resource-constrained edge devices. This paper presents the\noutcomes of a compact DNN model that exhibits resilience against both black-box\nand white-box adversarial attacks. This work has achieved this resilience\nthrough training with the QKeras quantization-aware training framework. The\nstudy explores the potential of QKeras and an adversarial robustness technique,\nJacobian Regularization (JR), to co-optimize the DNN architecture through\nper-layer JR methodology. As a result, this paper has devised a DNN model\nemploying this co-optimization strategy based on Stochastic Ternary\nQuantization (STQ). Its performance was compared against existing DNN models in\nthe face of various white-box and black-box attacks. The experimental findings\nrevealed that, the proposed DNN model had small footprint and on average, it\nexhibited better performance than Quanos and DS-CNN MLCommons/TinyML (MLC/T)\nbenchmarks when challenged with white-box and black-box attacks, respectively,\non the CIFAR-10 image and Google Speech Commands audio datasets."
    },
    {
        "date": "2025-03",
        "title": "Leaky Batteries: A Novel Set of Side-Channel Attacks on Electric Vehicles",
        "author": "Francesco Marchiori, and Mauro Conti",
        "link": "http://arxiv.org/abs/2503.08956v1",
        "abstract": "Advancements in battery technology have accelerated the adoption of Electric\nVehicles (EVs) due to their environmental benefits. However, their growing\nsophistication introduces security and privacy challenges. Often seen as mere\noperational data, battery consumption patterns can unintentionally reveal\ncritical information exploitable for malicious purposes. These risks go beyond\nprivacy, impacting vehicle security and regulatory compliance. Despite these\nconcerns, current research has largely overlooked the broader implications of\nbattery consumption data exposure. As EVs integrate further into smart\ntransportation networks, addressing these gaps is crucial to ensure their\nsafety, reliability, and resilience. In this work, we introduce a novel class\nof side-channel attacks that exploit EV battery data to extract sensitive user\ninformation. Leveraging only battery consumption patterns, we demonstrate a\nmethodology to accurately identify the EV driver and their driving style,\ndetermine the number of occupants, and infer the vehicle's start and end\nlocations when user habits are known. We utilize several machine learning\nmodels and feature extraction techniques to analyze EV power consumption\npatterns, validating our approach on simulated and real-world datasets\ncollected from actual drivers. Our attacks achieve an average success rate of\n95.4% across all attack objectives. Our findings highlight the privacy risks\nassociated with EV battery data, emphasizing the need for stronger protections\nto safeguard user privacy and vehicle security."
    },
    {
        "date": "2025-03",
        "title": "Robust Unsupervised Fault Diagnosis For High-Dimensional Nonlinear Noisy Data",
        "author": "Dandan Zhao, Hongpeng Yin, Jintang Bian, and Han Zhou",
        "link": "http://arxiv.org/abs/2503.08916v1",
        "abstract": "Traditional fault diagnosis methods struggle to handle fault data, with\ncomplex data characteristics such as high dimensions and large noise. Deep\nlearning is a promising solution, which typically works well only when labeled\nfault data are available. To address these problems, a robust unsupervised\nfault diagnosis using machine learning is proposed in this paper. First, a\nspecial dimension reduction method for the high-dimensional fault data is\ndesigned. Second, the extracted features are enhanced by incorporating\nnonlinear information through the learning of a graph structure. Third, to\nalleviate the problem of reduced fault-diagnosis accuracy attributed to noise\nand outliers, $l_{2,1}$-norm and typicality-aware constraints are introduced\nfrom the perspective of model optimization, respectively. Finally, this paper\nprovides comprehensive theoretical and experimental evidence supporting the\neffectiveness and robustness of the proposed method. The experiments on both\nthe benchmark Tennessee-Eastman process and a real hot-steel milling process\nshow that the proposed method exhibits better robustness compared to other\nmethods, maintaining high diagnostic accuracy even in the presence of outliers\nor noise."
    },
    {
        "date": "2025-03",
        "title": "A Deep Bayesian Nonparametric Framework for Robust Mutual Information Estimation",
        "author": "Forough Fazeliasl, Michael Minyi Zhang, Bei Jiang, and Linglong Kong",
        "link": "http://arxiv.org/abs/2503.08902v1",
        "abstract": "Mutual Information (MI) is a crucial measure for capturing dependencies\nbetween variables, but exact computation is challenging in high dimensions with\nintractable likelihoods, impacting accuracy and robustness. One idea is to use\nan auxiliary neural network to train an MI estimator; however, methods based on\nthe empirical distribution function (EDF) can introduce sharp fluctuations in\nthe MI loss due to poor out-of-sample performance, destabilizing convergence.\nWe present a Bayesian nonparametric (BNP) solution for training an MI estimator\nby constructing the MI loss with a finite representation of the Dirichlet\nprocess posterior to incorporate regularization in the training process. With\nthis regularization, the MI loss integrates both prior knowledge and empirical\ndata to reduce the loss sensitivity to fluctuations and outliers in the sample\ndata, especially in small sample settings like mini-batches. This approach\naddresses the challenge of balancing accuracy and low variance by effectively\nreducing variance, leading to stabilized and robust MI loss gradients during\ntraining and enhancing the convergence of the MI approximation while offering\nstronger theoretical guarantees for convergence. We explore the application of\nour estimator in maximizing MI between the data space and the latent space of a\nvariational autoencoder. Experimental results demonstrate significant\nimprovements in convergence over EDF-based methods, with applications across\nsynthetic and real datasets, notably in 3D CT image generation, yielding\nenhanced structure discovery and reduced overfitting in data synthesis. While\nthis paper focuses on generative models in application, the proposed estimator\nis not restricted to this setting and can be applied more broadly in various\nBNP learning procedures."
    },
    {
        "date": "2025-03",
        "title": "Seal Your Backdoor with Variational Defense",
        "author": "Ivan Saboli\u0107, Matej Grci\u0107, and Sini\u0161a \u0160egvi\u0107",
        "link": "http://arxiv.org/abs/2503.08829v1",
        "abstract": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios."
    }
]