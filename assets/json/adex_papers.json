[
    {
        "date": "2025-06",
        "title": "ExtendAttack: Attacking Servers of LRMs via Extending Reasoning",
        "author": "Zhenhao Zhu, Yue Liu, Yingwei Ma, Hongcheng Gao, Nuo Chen, Yanpei Guo, Wenjie Qu, Huiying Xu, Xinzhong Zhu, and Jiaheng Zhang",
        "link": "http://arxiv.org/abs/2506.13737v1",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated promising performance in\ncomplex tasks. However, the resource-consuming reasoning processes may be\nexploited by attackers to maliciously occupy the resources of the servers,\nleading to a crash, like the DDoS attack in cyber. To this end, we propose a\nnovel attack method on LRMs termed ExtendAttack to maliciously occupy the\nresources of servers by stealthily extending the reasoning processes of LRMs.\nConcretely, we systematically obfuscate characters within a benign prompt,\ntransforming them into a complex, poly-base ASCII representation. This compels\nthe model to perform a series of computationally intensive decoding sub-tasks\nthat are deeply embedded within the semantic structure of the query itself.\nExtensive experiments demonstrate the effectiveness of our proposed\nExtendAttack. Remarkably, it increases the length of the model's response by\nover 2.5 times for the o3 model on the HumanEval benchmark. Besides, it\npreserves the original meaning of the query and achieves comparable answer\naccuracy, showing the stealthiness."
    },
    {
        "date": "2025-06",
        "title": "Weakest Link in the Chain: Security Vulnerabilities in Advanced Reasoning Models",
        "author": "Arjun Krishna, Aaditya Rastogi, and Erick Galinkin",
        "link": "http://arxiv.org/abs/2506.13726v1",
        "abstract": "The introduction of advanced reasoning capabilities have improved the\nproblem-solving performance of large language models, particularly on math and\ncoding benchmarks. However, it remains unclear whether these reasoning models\nare more or less vulnerable to adversarial prompt attacks than their\nnon-reasoning counterparts. In this work, we present a systematic evaluation of\nweaknesses in advanced reasoning models compared to similar non-reasoning\nmodels across a diverse set of prompt-based attack categories. Using\nexperimental data, we find that on average the reasoning-augmented models are\n\\emph{slightly more robust} than non-reasoning models (42.51\\% vs 45.53\\%\nattack success rate, lower is better). However, this overall trend masks\nsignificant category-specific differences: for certain attack types the\nreasoning models are substantially \\emph{more vulnerable} (e.g., up to 32\npercentage points worse on a tree-of-attacks prompt), while for others they are\nmarkedly \\emph{more robust} (e.g., 29.8 points better on cross-site scripting\ninjection). Our findings highlight the nuanced security implications of\nadvanced reasoning in language models and emphasize the importance of\nstress-testing safety across diverse adversarial techniques."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Disentanglement by Backpropagation with Physics-Informed Variational Autoencoder",
        "author": "Ioannis Christoforos Koune, and Alice Cicirello",
        "link": "http://arxiv.org/abs/2506.13658v1",
        "abstract": "Inference and prediction under partial knowledge of a physical system is\nchallenging, particularly when multiple confounding sources influence the\nmeasured response. Explicitly accounting for these influences in physics-based\nmodels is often infeasible due to epistemic uncertainty, cost, or time\nconstraints, resulting in models that fail to accurately describe the behavior\nof the system. On the other hand, data-driven machine learning models such as\nvariational autoencoders are not guaranteed to identify a parsimonious\nrepresentation. As a result, they can suffer from poor generalization\nperformance and reconstruction accuracy in the regime of limited and noisy\ndata. We propose a physics-informed variational autoencoder architecture that\ncombines the interpretability of physics-based models with the flexibility of\ndata-driven models. To promote disentanglement of the known physics and\nconfounding influences, the latent space is partitioned into physically\nmeaningful variables that parametrize a physics-based model, and data-driven\nvariables that capture variability in the domain and class of the physical\nsystem. The encoder is coupled with a decoder that integrates physics-based and\ndata-driven components, and constrained by an adversarial training objective\nthat prevents the data-driven components from overriding the known physics,\nensuring that the physics-grounded latent variables remain interpretable. We\ndemonstrate that the model is able to disentangle features of the input signal\nand separate the known physics from confounding influences using supervision in\nthe form of class and domain observables. The model is evaluated on a series of\nsynthetic case studies relevant to engineering structures, demonstrating the\nfeasibility of the proposed approach."
    },
    {
        "date": "2025-06",
        "title": "EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning",
        "author": "Zhiqiang Li, Haiyong Bao, Menghong Guan, Hao Pan, Cheng Huang, and Hong-Ning Dai",
        "link": "http://arxiv.org/abs/2506.13612v1",
        "abstract": "Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security."
    },
    {
        "date": "2025-06",
        "title": "Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks",
        "author": "Yali Yuan, Kai Xu, Ruolin Ma, and Yuchen Zhang",
        "link": "http://arxiv.org/abs/2506.13563v1",
        "abstract": "Website Fingerprinting (WF) is an effective tool for regulating and governing\nthe dark web. However, its performance can be significantly degraded by\nbackdoor poisoning attacks in practical deployments. This paper aims to address\nthe problem of hidden backdoor poisoning attacks faced by Website\nFingerprinting attack, and designs a feasible mothed that integrates unlearning\ntechnology to realize detection of automatic poisoned points and complete\nremoval of its destructive effects, requiring only a small number of known\npoisoned test points. Taking Tor onion routing as an example, our method\nevaluates the influence value of each training sample on these known poisoned\ntest points as the basis for judgment. We optimize the use of influence scores\nto identify poisoned samples within the training dataset. Furthermore, by\nquantifying the difference between the contribution of model parameters on the\ntaining data and the clean data, the target parameters are dynamically adjusted\nto eliminate the impact of the backdoor attacks. Experiments on public datasets\nunder the assumptions of closed-world (CW) and open-world (OW) verify the\neffectiveness of the proposed method. In complex scenes containing both clean\nwebsite fingerprinting features and backdoor triggers, the accuracy of the\nmodel on the poisoned dataset and the test dataset is stable at about 80%,\nsignificantly outperforming the traditional WF attack models. In addition, the\nproposed method achieves a 2-3 times speedup in runtime efficiency compared to\nbaseline methods. By incorporating machine unlearning, we realize a WF attack\nmodel that exhibits enhanced resistance to backdoor poisoning and faster\nexecution speeds in adversarial settings."
    },
    {
        "date": "2025-06",
        "title": "LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations",
        "author": "Lorenzo Bini, and Stephane Marchand-Maillet",
        "link": "http://arxiv.org/abs/2506.13344v1",
        "abstract": "Generating high-fidelity and biologically plausible synthetic single-cell RNA\nsequencing (scRNA-seq) data, especially with conditional control, is\nchallenging due to its high dimensionality, sparsity, and complex biological\nvariations. Existing generative models often struggle to capture these unique\ncharacteristics and ensure robustness to structural noise in cellular networks.\nWe introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model\nfor robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates\ngraph-based representations with a score-based diffusion model, enhanced by a\nnovel spectral adversarial perturbation mechanism on graph edge weights. Our\ncontributions are threefold: we leverage Laplacian Positional Encodings (LPEs)\nto enrich the latent space with crucial cellular relationship information; we\ndevelop a conditional score-based diffusion model for effective learning and\ngeneration from complex scRNA-seq distributions; and we employ a unique\nspectral adversarial training scheme on graph edge weights, boosting robustness\nagainst structural variations. Extensive experiments on diverse scRNA-seq\ndatasets demonstrate LapDDPM's superior performance, achieving high fidelity\nand generating biologically-plausible, cell-type-specific samples. LapDDPM sets\na new benchmark for conditional scRNA-seq data generation, offering a robust\ntool for various downstream biological applications."
    },
    {
        "date": "2025-06",
        "title": "Navigating the Black Box: Leveraging LLMs for Effective Text-Level Graph Injection Attacks",
        "author": "Yuefei Lyu, Chaozhuo Li, Xi Zhang, and Tianle Zhang",
        "link": "http://arxiv.org/abs/2506.13276v1",
        "abstract": "Text-attributed graphs (TAGs) integrate textual data with graph structures,\nproviding valuable insights in applications such as social network analysis and\nrecommendation systems. Graph Neural Networks (GNNs) effectively capture both\ntopological structure and textual information in TAGs but are vulnerable to\nadversarial attacks. Existing graph injection attack (GIA) methods assume that\nattackers can directly manipulate the embedding layer, producing\nnon-explainable node embeddings. Furthermore, the effectiveness of these\nattacks often relies on surrogate models with high training costs. Thus, this\npaper introduces ATAG-LLM, a novel black-box GIA framework tailored for TAGs.\nOur approach leverages large language models (LLMs) to generate interpretable\ntext-level node attributes directly, ensuring attacks remain feasible in\nreal-world scenarios. We design strategies for LLM prompting that balance\nexploration and reliability to guide text generation, and propose a similarity\nassessment method to evaluate attack text effectiveness in disrupting graph\nhomophily. This method efficiently perturbs the target node with minimal\ntraining costs in a strict black-box setting, ensuring a text-level graph\ninjection attack for TAGs. Experiments on real-world TAG datasets validate the\nsuperior performance of ATAG-LLM compared to state-of-the-art embedding-level\nand text-level attack methods."
    },
    {
        "date": "2025-06",
        "title": "Building Automotive Security on Internet Standards: An Integration of DNSSEC, DANE, and DANCE to Authenticate and Authorize In-Car Services",
        "author": "Timo Salomon, Mehmet Mueller, Philipp Meyer, and Thomas C. Schmidt",
        "link": "http://arxiv.org/abs/2506.13261v1",
        "abstract": "The automotive industry is undergoing a software-as-a-service transformation\nthat enables software-defined functions and post-sale updates via cloud and\nvehicle-to-everything communication. Connectivity in cars introduces\nsignificant security challenges, as remote attacks on vehicles have become\nincreasingly prevalent. Current automotive designs call for security solutions\nthat address the entire lifetime of a vehicle. In this paper, we propose to\nauthenticate and authorize in-vehicle services by integrating DNSSEC, DANE, and\nDANCE with automotive middleware. Our approach decouples the cryptographic\nauthentication of the service from that of the service deployment with the help\nof DNSSEC and thereby largely simplifies key management. We propose to\nauthenticate in-vehicle services by certificates that are solely generated by\nthe service suppliers but published on deployment via DNSSEC TLSA records\nsolely signed by the OEM. Building on well-established Internet standards\nensures interoperability with various current and future protocols, scalable\nmanagement of credentials for millions of connected vehicles at\nwell-established security levels. We back our design proposal by a security\nanalysis using the STRIDE threat model and by evaluations in a realistic\nin-vehicle setup that demonstrate its effectiveness."
    },
    {
        "date": "2025-06",
        "title": "No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!",
        "author": "Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Christian Kroer",
        "link": "http://arxiv.org/abs/2506.13244v2",
        "abstract": "We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan."
    },
    {
        "date": "2025-06",
        "title": "Using LLMs for Security Advisory Investigations: How Far Are We?",
        "author": "Bayu Fedra Abdullah, Yusuf Sulistyo Nugroho, Brittany Reid, Raula Gaikovina Kula, Kazumasa Shimari, and Kenichi Matsumoto",
        "link": "http://arxiv.org/abs/2506.13161v1",
        "abstract": "Large Language Models (LLMs) are increasingly used in software security, but\ntheir trustworthiness in generating accurate vulnerability advisories remains\nuncertain. This study investigates the ability of ChatGPT to (1) generate\nplausible security advisories from CVE-IDs, (2) differentiate real from fake\nCVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated\ndataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility\nand consistency of the model's outputs. The results show that ChatGPT generated\nplausible security advisories for 96% of given input real CVE-IDs and 97% of\ngiven input fake CVE-IDs, demonstrating a limitation in differentiating between\nreal and fake IDs. Furthermore, when these generated advisories were\nreintroduced to ChatGPT to identify their original CVE-ID, the model produced a\nfake CVE-ID in 6% of cases from real advisories. These findings highlight both\nthe strengths and limitations of ChatGPT in cybersecurity applications. While\nthe model demonstrates potential for automating advisory generation, its\ninability to reliably authenticate CVE-IDs or maintain consistency upon\nre-evaluation underscores the risks associated with its deployment in critical\nsecurity tasks. Our study emphasizes the importance of using LLMs with caution\nin cybersecurity workflows and suggests the need for further improvements in\ntheir design to improve reliability and applicability in security advisory\ngeneration."
    },
    {
        "date": "2025-06",
        "title": "Buy it Now, Track Me Later: Attacking User Privacy via Wi-Fi AP Online Auctions",
        "author": "Steven Su, Erik Rye, Robert Beverly, and Dave Levin",
        "link": "http://arxiv.org/abs/2506.13052v1",
        "abstract": "Static and hard-coded layer-two network identifiers are well known to present\nsecurity vulnerabilities and endanger user privacy. In this work, we introduce\na new privacy attack against Wi-Fi access points listed on secondhand\nmarketplaces. Specifically, we demonstrate the ability to remotely gather a\nlarge quantity of layer-two Wi-Fi identifiers by programmatically querying the\neBay marketplace and applying state-of-the-art computer vision techniques to\nextract IEEE 802.11 BSSIDs from the seller's posted images of the hardware. By\nleveraging data from a global Wi-Fi Positioning System (WPS) that geolocates\nBSSIDs, we obtain the physical locations of these devices both pre- and\npost-sale. In addition to validating the degree to which a seller's location\nmatches the location of the device, we examine cases of device movement -- once\nthe device is sold and then subsequently re-used in a new environment. Our work\nhighlights a previously unrecognized privacy vulnerability and suggests, yet\nagain, the strong need to protect layer-two network identifiers."
    },
    {
        "date": "2025-06",
        "title": "HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs",
        "author": "Zijian Zhang, Xuecheng Wu, Danlei Huang, Siyu Yan, Chong Peng, and Xuezhi Cao",
        "link": "http://arxiv.org/abs/2506.13038v2",
        "abstract": "Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains."
    },
    {
        "date": "2025-06",
        "title": "Position: Certified Robustness Does Not (Yet) Imply Model Security",
        "author": "Andrew C. Cullen, Paul Montague, Sarah M. Erfani, and Benjamin I. P. Rubinstein",
        "link": "http://arxiv.org/abs/2506.13024v1",
        "abstract": "While certified robustness is widely promoted as a solution to adversarial\nexamples in Artificial Intelligence systems, significant challenges remain\nbefore these techniques can be meaningfully deployed in real-world\napplications. We identify critical gaps in current research, including the\nparadox of detection without distinction, the lack of clear criteria for\npractitioners to evaluate certification schemes, and the potential security\nrisks arising from users' expectations surrounding ``guaranteed\" robustness\nclaims. This position paper is a call to arms for the certification research\ncommunity, proposing concrete steps to address these fundamental challenges and\nadvance the field toward practical applicability."
    },
    {
        "date": "2025-06",
        "title": "Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective",
        "author": "Nima Naderloui, Shenao Yan, Binghui Wang, Jie Fu, Wendy Hui Wang, Weiran Liu, and Yuan Hong",
        "link": "http://arxiv.org/abs/2506.13009v1",
        "abstract": "Machine unlearning focuses on efficiently removing specific data from trained\nmodels, addressing privacy and compliance concerns with reasonable costs.\nAlthough exact unlearning ensures complete data removal equivalent to\nretraining, it is impractical for large-scale models, leading to growing\ninterest in inexact unlearning methods. However, the lack of formal guarantees\nin these methods necessitates the need for robust evaluation frameworks to\nassess their privacy and effectiveness. In this work, we first identify several\nkey pitfalls of the existing unlearning evaluation frameworks, e.g., focusing\non average-case evaluation or targeting random samples for evaluation,\nincomplete comparisons with the retraining baseline. Then, we propose RULI\n(Rectified Unlearning Evaluation Framework via Likelihood Inference), a novel\nframework to address critical gaps in the evaluation of inexact unlearning\nmethods. RULI introduces a dual-objective attack to measure both unlearning\nefficacy and privacy risks at a per-sample granularity. Our findings reveal\nsignificant vulnerabilities in state-of-the-art unlearning methods, where RULI\nachieves higher attack success rates, exposing privacy risks underestimated by\nexisting methods. Built on a game-based foundation and validated through\nempirical evaluations on both image and text data (spanning tasks from\nclassification to generation), RULI provides a rigorous, scalable, and\nfine-grained methodology for evaluating unlearning techniques."
    },
    {
        "date": "2025-06",
        "title": "Open Source, Open Threats? Investigating Security Challenges in Open-Source Software",
        "author": "Seyed Ali Akhavani, Behzad Ousat, and Amin Kharraz",
        "link": "http://arxiv.org/abs/2506.12995v1",
        "abstract": "Open-source software (OSS) has become increasingly more popular across\ndifferent domains. However, this rapid development and widespread adoption come\nwith a security cost. The growing complexity and openness of OSS ecosystems\nhave led to increased exposure to vulnerabilities and attack surfaces. This\npaper investigates the trends and patterns of reported vulnerabilities within\nOSS platforms, focusing on the implications of these findings for security\npractices. To understand the dynamics of OSS vulnerabilities, we analyze a\ncomprehensive dataset comprising 31,267 unique vulnerability reports from\nGitHub's advisory database and Snyk.io, belonging to 14,675 packages across 10\nprogramming languages. Our analysis reveals a significant surge in reported\nvulnerabilities, increasing at an annual rate of 98%, far outpacing the 25%\naverage annual growth in the number of open-source software (OSS) packages.\nAdditionally, we observe an 85% increase in the average lifespan of\nvulnerabilities across ecosystems during the studied period, indicating a\npotential decline in security. We identify the most prevalent Common Weakness\nEnumerations (CWEs) across programming languages and find that, on average,\njust seven CWEs are responsible for over 50% of all reported vulnerabilities.\nWe further examine these commonly observed CWEs and highlight\necosystem-specific trends. Notably, we find that vulnerabilities associated\nwith intentionally malicious packages comprise 49% of reports in the NPM\necosystem and 14% in PyPI, an alarming indication of targeted attacks within\npackage repositories. We conclude with an in-depth discussion of the\ncharacteristics and attack vectors associated with these malicious packages."
    },
    {
        "date": "2025-06",
        "title": "Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks",
        "author": "Erica Cai, Xi Chen, Reagan Grey Keeney, Ethan Zuckerman, Brendan O'Connor, and Przemyslaw A. Grabowicz",
        "link": "http://arxiv.org/abs/2506.12925v1",
        "abstract": "Comparative studies of news coverage are challenging to conduct because\nmethods to identify news articles about the same event in different languages\nrequire expertise that is difficult to scale. We introduce an AI-powered method\nfor identifying news articles based on an event FINGERPRINT, which is a minimal\nset of metadata required to identify critical events. Our event coverage\nidentification method, FINGERPRINT TO ARTICLE MATCHING FOR EVENTS (FAME),\nefficiently identifies news articles about critical world events, specifically\nterrorist attacks and several types of natural disasters. FAME does not require\ntraining data and is able to automatically and efficiently identify news\narticles that discuss an event given its fingerprint: time, location, and class\n(such as storm or flood). The method achieves state-of-the-art performance and\nscales to massive databases of tens of millions of news articles and hundreds\nof events happening globally. We use FAME to identify 27,441 articles that\ncover 470 natural disaster and terrorist attack events that happened in 2020.\nTo this end, we use a massive database of news articles in three languages from\nMediaCloud, and three widely used, expert-curated databases of critical events:\nEM-DAT, USGS, and GTD. Our case study reveals patterns consistent with prior\nliterature: coverage of disasters and terrorist attacks correlates to death\ncounts, to the GDP of a country where the event occurs, and to trade volume\nbetween the reporting country and the country where the event occurred. We\nshare our NLP annotations and cross-country media attention data to support the\nefforts of researchers and media monitoring organizations."
    },
    {
        "date": "2025-06",
        "title": "Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs",
        "author": "Lu Chen, Han Yang, Hu Wang, Yuxin Cao, Shaofeng Li, and Yuan Luo",
        "link": "http://arxiv.org/abs/2506.12875v1",
        "abstract": "Adversarial examples have attracted significant attention over the years, yet\nunderstanding their frequency-based characteristics remains insufficient. In\nthis paper, we investigate the intriguing properties of adversarial examples in\nthe frequency domain for the image classification task, with the following key\nfindings. (1) As the high-frequency components increase, the performance gap\nbetween adversarial and natural examples becomes increasingly pronounced. (2)\nThe model performance against filtered adversarial examples initially increases\nto a peak and declines to its inherent robustness. (3) In Convolutional Neural\nNetworks, mid- and high-frequency components of adversarial examples exhibit\ntheir attack capabilities, while in Transformers, low- and mid-frequency\ncomponents of adversarial examples are particularly effective. These results\nsuggest that different network architectures have different frequency\npreferences and that differences in frequency components between adversarial\nand natural examples may directly influence model robustness. Based on our\nfindings, we further conclude with three useful proposals that serve as a\nvaluable reference to the AI model security community."
    },
    {
        "date": "2025-06",
        "title": "Active Adversarial Noise Suppression for Image Forgery Localization",
        "author": "Rongxuan Peng, Shunquan Tan, Xianbo Mo, Alex C. Kot, and Jiwu Huang",
        "link": "http://arxiv.org/abs/2506.12871v1",
        "abstract": "Recent advances in deep learning have significantly propelled the development\nof image forgery localization. However, existing models remain highly\nvulnerable to adversarial attacks: imperceptible noise added to forged images\ncan severely mislead these models. In this paper, we address this challenge\nwith an Adversarial Noise Suppression Module (ANSM) that generate a defensive\nperturbation to suppress the attack effect of adversarial noise. We observe\nthat forgery-relevant features extracted from adversarial and original forged\nimages exhibit distinct distributions. To bridge this gap, we introduce\nForgery-relevant Features Alignment (FFA) as a first-stage training strategy,\nwhich reduces distributional discrepancies by minimizing the channel-wise\nKullback-Leibler divergence between these features. To further refine the\ndefensive perturbation, we design a second-stage training strategy, termed\nMask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR\nensures that the perturbation remains effective for both adversarial and\noriginal forged images, recovering forgery localization accuracy to their\noriginal level. Extensive experiments across various attack algorithms\ndemonstrate that our method significantly restores the forgery localization\nmodel's performance on adversarial images. Notably, when ANSM is applied to\noriginal forged images, the performance remains nearly unaffected. To our best\nknowledge, this is the first report of adversarial defense in image forgery\nlocalization tasks. We have released the source code and anti-forensics\ndataset."
    },
    {
        "date": "2025-06",
        "title": "TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models",
        "author": "Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Xiaochun Cao, Shouling Ji, Jiaheng Zhang, Jincai Huang, and Li Shen",
        "link": "http://arxiv.org/abs/2506.12815v1",
        "abstract": "Recent advances in Trajectory Optimization (TO) models have achieved\nremarkable success in offline reinforcement learning. However, their\nvulnerabilities against backdoor attacks are poorly understood. We find that\nexisting backdoor attacks in reinforcement learning are based on reward\nmanipulation, which are largely ineffective against the TO model due to its\ninherent sequence modeling nature. Moreover, the complexities introduced by\nhigh-dimensional action spaces further compound the challenge of action\nmanipulation. To address these gaps, we propose TrojanTO, the first\naction-level backdoor attack against TO models. TrojanTO employs alternating\ntraining to enhance the connection between triggers and target actions for\nattack effectiveness. To improve attack stealth, it utilizes precise poisoning\nvia trajectory filtering for normal performance and batch poisoning for trigger\nconsistency. Extensive evaluations demonstrate that TrojanTO effectively\nimplants backdoor attacks across diverse tasks and attack objectives with a low\nattack budget (0.3\\% of trajectories). Furthermore, TrojanTO exhibits broad\napplicability to DT, GDT, and DC, underscoring its scalability across diverse\nTO model architectures."
    },
    {
        "date": "2025-06",
        "title": "Federated Neuroevolution O-RAN: Enhancing the Robustness of Deep Reinforcement Learning xApps",
        "author": "Mohammadreza Kouchaki, Aly Sabri Abdalla, and Vuk Marojevic",
        "link": "http://arxiv.org/abs/2506.12812v1",
        "abstract": "The open radio access network (O-RAN) architecture introduces RAN intelligent\ncontrollers (RICs) to facilitate the management and optimization of the\ndisaggregated RAN. Reinforcement learning (RL) and its advanced form, deep RL\n(DRL), are increasingly employed for designing intelligent controllers, or\nxApps, to be deployed in the near-real time (near-RT) RIC. These models often\nencounter local optima, which raise concerns about their reliability for RAN\nintelligent control. We therefore introduce Federated O-RAN enabled\nNeuroevolution (NE)-enhanced DRL (F-ONRL) that deploys an NE-based optimizer\nxApp in parallel to the RAN controller xApps. This NE-DRL xApp framework\nenables effective exploration and exploitation in the near-RT RIC without\ndisrupting RAN operations. We implement the NE xApp along with a DRL xApp and\ndeploy them on Open AI Cellular (OAIC) platform and present numerical results\nthat demonstrate the improved robustness of xApps while effectively balancing\nthe additional computational load."
    },
    {
        "date": "2025-06",
        "title": "Predicting Genetic Mutations from Single-Cell Bone Marrow Images in Acute Myeloid Leukemia Using Noise-Robust Deep Learning Models",
        "author": "Garima Jain, Ravi Kant Gupta, Priyansh Jain, Abhijeet Patil, Ardhendu Sekhar, Gajendra Smeeta, Sanghamitra Pati, and Amit Sethi",
        "link": "http://arxiv.org/abs/2506.12798v1",
        "abstract": "In this study, we propose a robust methodology for identification of myeloid\nblasts followed by prediction of genetic mutation in single-cell images of\nblasts, tackling challenges associated with label accuracy and data noise. We\ntrained an initial binary classifier to distinguish between leukemic (blasts)\nand non-leukemic cells images, achieving 90 percent accuracy. To evaluate the\nmodels generalization, we applied this model to a separate large unlabeled\ndataset and validated the predictions with two haemato-pathologists, finding an\napproximate error rate of 20 percent in the leukemic and non-leukemic labels.\nAssuming this level of label noise, we further trained a four-class model on\nimages predicted as blasts to classify specific mutations. The mutation labels\nwere known for only a bag of cell images extracted from a single slide. Despite\nthe tumor label noise, our mutation classification model achieved 85 percent\naccuracy across four mutation classes, demonstrating resilience to label\ninconsistencies. This study highlights the capability of machine learning\nmodels to work with noisy labels effectively while providing accurate,\nclinically relevant mutation predictions, which is promising for diagnostic\napplications in areas such as haemato-pathology."
    },
    {
        "date": "2025-06",
        "title": "Unconstrained Robust Online Convex Optimization",
        "author": "Jiujia Zhang, and Ashok Cutkosky",
        "link": "http://arxiv.org/abs/2506.12781v1",
        "abstract": "This paper addresses online learning with ``corrupted'' feedback. Our learner\nis provided with potentially corrupted gradients $\\tilde g_t$ instead of the\n``true'' gradients $g_t$. We make no assumptions about how the corruptions\narise: they could be the result of outliers, mislabeled data, or even malicious\ninterference. We focus on the difficult ``unconstrained'' setting in which our\nalgorithm must maintain low regret with respect to any comparison point $u \\in\n\\mathbb{R}^d$. The unconstrained setting is significantly more challenging as\nexisting algorithms suffer extremely high regret even with very tiny amounts of\ncorruption (which is not true in the case of a bounded domain). Our algorithms\nguarantee regret $ \\|u\\|G (\\sqrt{T} + k) $ when $G \\ge \\max_t \\|g_t\\|$ is\nknown, where $k$ is a measure of the total amount of corruption. When $G$ is\nunknown we incur an extra additive penalty of $(\\|u\\|^2+G^2) k$."
    },
    {
        "date": "2025-06",
        "title": "Base3: a simple interpolation-based ensemble method for robust dynamic link prediction",
        "author": "Kondrup Emma",
        "link": "http://arxiv.org/abs/2506.12764v1",
        "abstract": "Dynamic link prediction remains a central challenge in temporal graph\nlearning, particularly in designing models that are both effective and\npractical for real-world deployment. Existing approaches often rely on complex\nneural architectures, which are computationally intensive and difficult to\ninterpret.\n  In this work, we build on the strong recurrence-based foundation of the\nEdgeBank baseline, by supplementing it with inductive capabilities. We do so by\nleveraging the predictive power of non-learnable signals from two complementary\nperspectives: historical edge recurrence, as captured by EdgeBank, and global\nnode popularity, as introduced in the PopTrack model. We propose t-CoMem, a\nlightweight memory module that tracks temporal co-occurrence patterns and\nneighborhood activity. Building on this, we introduce Base3, an\ninterpolation-based model that fuses EdgeBank, PopTrack, and t-CoMem into a\nunified scoring framework. This combination effectively bridges local and\nglobal temporal dynamics -- repetition, popularity, and context -- without\nrelying on training. Evaluated on the Temporal Graph Benchmark, Base3 achieves\nperformance competitive with state-of-the-art deep models, even outperforming\nthem on some datasets. Importantly, it considerably improves on existing\nbaselines' performance under more realistic and challenging negative sampling\nstrategies -- offering a simple yet robust alternative for temporal graph\nlearning."
    },
    {
        "date": "2025-06",
        "title": "Learning to Fuse: Modality-Aware Adaptive Scheduling for Robust Multimodal Foundation Models",
        "author": "Liam Bennett, Mason Clark, Lucas Anderson, Hana Satou, and Olivia Martinez",
        "link": "http://arxiv.org/abs/2506.12733v1",
        "abstract": "Multimodal foundation models have achieved impressive progress across a wide\nrange of vision-language tasks. However, existing approaches often adopt fixed\nor task-specific fusion strategies, neglecting the intrinsic variability of\nmodality reliability and sample complexity. In this paper, we propose\nModality-Aware Adaptive Fusion Scheduling (MA-AFS), a general framework that\nlearns to dynamically modulate the contribution of each modality on a\nper-instance basis. MA-AFS introduces a lightweight neural scheduler that\npredicts modality fusion weights by integrating visual and textual entropy\nsignals along with cross-modal agreement cues. This enables the model to\nadaptively emphasize more reliable modalities, especially under noisy, missing,\nor misaligned inputs. We formulate the fusion process as a differentiable\nscheduling mechanism, analyze its theoretical consistency and regularization\neffect, and demonstrate that it improves robustness without increasing model\ncapacity significantly. Extensive experiments on image-text retrieval,\ncaptioning, and visual question answering show that MA-AFS achieves consistent\nperformance gains over strong baselines such as CLIP, ALBEF, and BLIP.\nMoreover, MA-AFS exhibits improved robustness under modality corruption and\nenhanced generalization under domain shifts. Our work highlights the importance\nof adaptive fusion and opens a promising direction toward reliable and\nuncertainty-aware multimodal learning."
    },
    {
        "date": "2025-06",
        "title": "SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression",
        "author": "Yucheng Li, Surin Ahn, Huiqiang Jiang, Amir H. Abdi, Yuqing Yang, and Lili Qiu",
        "link": "http://arxiv.org/abs/2506.12707v1",
        "abstract": "Large language models (LLMs) have achieved widespread adoption across\nnumerous applications. However, many LLMs are vulnerable to malicious attacks\neven after safety alignment. These attacks typically bypass LLMs' safety\nguardrails by wrapping the original malicious instructions inside adversarial\njailbreaks prompts. Previous research has proposed methods such as adversarial\ntraining and prompt rephrasing to mitigate these safety vulnerabilities, but\nthese methods often reduce the utility of LLMs or lead to significant\ncomputational overhead and online latency. In this paper, we propose\nSecurityLingua, an effective and efficient approach to defend LLMs against\njailbreak attacks via security-oriented prompt compression. Specifically, we\ntrain a prompt compressor designed to discern the \"true intention\" of the input\nprompt, with a particular focus on detecting the malicious intentions of\nadversarial prompts. Then, in addition to the original prompt, the intention is\npassed via the system prompt to the target LLM to help it identify the true\nintention of the request. SecurityLingua ensures a consistent user experience\nby leaving the original input prompt intact while revealing the user's\npotentially malicious intention and stimulating the built-in safety guardrails\nof the LLM. Moreover, thanks to prompt compression, SecurityLingua incurs only\na negligible overhead and extra token cost compared to all existing defense\nmethods, making it an especially practical solution for LLM defense.\nExperimental results demonstrate that SecurityLingua can effectively defend\nagainst malicious attacks and maintain utility of the LLM with negligible\ncompute and latency overhead. Our code is available at\nhttps://aka.ms/SecurityLingua."
    },
    {
        "date": "2025-06",
        "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models",
        "author": "Jiaming Zhang, Xin Wang, Xingjun Ma, Lingyu Qiu, Yu-Gang Jiang, and Jitao Sang",
        "link": "http://arxiv.org/abs/2506.12706v1",
        "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable\ncapabilities in understanding relationships between visual and textual data\nthrough joint embedding spaces. Despite their effectiveness, these models\nremain vulnerable to adversarial attacks, particularly in the image modality,\nposing significant security concerns. Building upon our previous work on\nAdversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to\nenhance adversarial robustness in VLMs without extensive parameter training, we\npresent a significant extension by introducing the Neural Augmentor framework\nfor Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations\ninclude: (1) extending AdvPT from text-only to multi-modal prompting across\nboth text and visual modalities, (2) expanding from single-layer to multi-layer\nprompt architectures, and (3) proposing a novel architecture-level redesign\nthrough our Neural Augmentor approach, which implements feature purification to\ndirectly address the distortions introduced by adversarial attacks in feature\nspace. Our NAP-Tuning approach incorporates token refiners that learn to\nreconstruct purified features through residual connections, allowing for\nmodality-specific and layer-specific feature correction.Comprehensive\nexperiments demonstrate that NAP-Tuning significantly outperforms existing\nmethods across various datasets and attack types. Notably, our approach shows\nsignificant improvements over the strongest baselines under the challenging\nAutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on\nViT-B32 architectures while maintaining competitive clean accuracy."
    },
    {
        "date": "2025-06",
        "title": "DR-SAC: Distributionally Robust Soft Actor-Critic for Reinforcement Learning under Uncertainty",
        "author": "Mingxuan Cui, Duo Zhou, Yuxuan Han, Grani A. Hanasusanto, Qiong Wang, Huan Zhang, and Zhengyuan Zhou",
        "link": "http://arxiv.org/abs/2506.12622v1",
        "abstract": "Deep reinforcement learning (RL) has achieved significant success, yet its\napplication in real-world scenarios is often hindered by a lack of robustness\nto environmental uncertainties. To solve this challenge, some robust RL\nalgorithms have been proposed, but most are limited to tabular settings. In\nthis work, we propose Distributionally Robust Soft Actor-Critic (DR-SAC), a\nnovel algorithm designed to enhance the robustness of the state-of-the-art Soft\nActor-Critic (SAC) algorithm. DR-SAC aims to maximize the expected value with\nentropy against the worst possible transition model lying in an uncertainty\nset. A distributionally robust version of the soft policy iteration is derived\nwith a convergence guarantee. For settings where nominal distributions are\nunknown, such as offline RL, a generative modeling approach is proposed to\nestimate the required nominal distributions from data. Furthermore,\nexperimental results on a range of continuous control benchmark tasks\ndemonstrate our algorithm achieves up to $9.8$ times the average reward of the\nSAC baseline under common perturbations. Additionally, compared with existing\nrobust reinforcement learning algorithms, DR-SAC significantly improves\ncomputing efficiency and applicability to large-scale problems."
    },
    {
        "date": "2025-06",
        "title": "Existence of Adversarial Examples for Random Convolutional Networks via Isoperimetric Inequalities on $\\mathbb{so}(d)$",
        "author": "Amit Daniely",
        "link": "http://arxiv.org/abs/2506.12613v1",
        "abstract": "We show that adversarial examples exist for various random convolutional\nnetworks, and furthermore, that this is a relatively simple consequence of the\nisoperimetric inequality on the special orthogonal group $\\mathbb{so}(d)$. This\nextends and simplifies a recent line of work which shows similar results for\nrandom fully connected networks."
    },
    {
        "date": "2025-06",
        "title": "Deep Fusion of Ultra-Low-Resolution Thermal Camera and Gyroscope Data for Lighting-Robust and Compute-Efficient Rotational Odometry",
        "author": "Farida Mohsen, and Ali Safa",
        "link": "http://arxiv.org/abs/2506.12536v1",
        "abstract": "Accurate rotational odometry is crucial for autonomous robotic systems,\nparticularly for small, power-constrained platforms such as drones and mobile\nrobots. This study introduces thermal-gyro fusion, a novel sensor fusion\napproach that integrates ultra-low-resolution thermal imaging with gyroscope\nreadings for rotational odometry. Unlike RGB cameras, thermal imaging is\ninvariant to lighting conditions and, when fused with gyroscopic data,\nmitigates drift which is a common limitation of inertial sensors. We first\ndevelop a multimodal data acquisition system to collect synchronized thermal\nand gyroscope data, along with rotational speed labels, across diverse\nenvironments. Subsequently, we design and train a lightweight Convolutional\nNeural Network (CNN) that fuses both modalities for rotational speed\nestimation. Our analysis demonstrates that thermal-gyro fusion enables a\nsignificant reduction in thermal camera resolution without significantly\ncompromising accuracy, thereby improving computational efficiency and memory\nutilization. These advantages make our approach well-suited for real-time\ndeployment in resource-constrained robotic systems. Finally, to facilitate\nfurther research, we publicly release our dataset as supplementary material."
    },
    {
        "date": "2025-06",
        "title": "Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning",
        "author": "Sara Rajaram, R. James Cotton, and Fabian H. Sinz",
        "link": "http://arxiv.org/abs/2506.12529v1",
        "abstract": "Preference-based Reinforcement Learning (PbRL) entails a variety of\napproaches for aligning models with human intent to alleviate the burden of\nreward engineering. However, most previous PbRL work has not investigated the\nrobustness to labeler errors, inevitable with labelers who are non-experts or\noperate under time constraints. Additionally, PbRL algorithms often target very\nspecific settings (e.g. pairwise ranked preferences or purely offline\nlearning). We introduce Similarity as Reward Alignment (SARA), a simple\ncontrastive framework that is both resilient to noisy labels and adaptable to\ndiverse feedback formats and training paradigms. SARA learns a latent\nrepresentation of preferred samples and computes rewards as similarities to the\nlearned latent. We demonstrate strong performance compared to baselines on\ncontinuous control offline RL benchmarks. We further demonstrate SARA's\nversatility in applications such as trajectory filtering for downstream tasks,\ncross-task preference transfer, and reward shaping in online learning."
    },
    {
        "date": "2025-06",
        "title": "When Forgetting Triggers Backdoors: A Clean Unlearning Attack",
        "author": "Marco Arazzi, Antonino Nocera, and Vinod P",
        "link": "http://arxiv.org/abs/2506.12522v1",
        "abstract": "Machine unlearning has emerged as a key component in ensuring ``Right to be\nForgotten'', enabling the removal of specific data points from trained models.\nHowever, even when the unlearning is performed without poisoning the forget-set\n(clean unlearning), it can be exploited for stealthy attacks that existing\ndefenses struggle to detect. In this paper, we propose a novel {\\em clean}\nbackdoor attack that exploits both the model learning phase and the subsequent\nunlearning requests. Unlike traditional backdoor methods, during the first\nphase, our approach injects a weak, distributed malicious signal across\nmultiple classes. The real attack is then activated and amplified by\nselectively unlearning {\\em non-poisoned} samples. This strategy results in a\npowerful and stealthy novel attack that is hard to detect or mitigate,\nhighlighting critical vulnerabilities in current unlearning mechanisms and\nhighlighting the need for more robust defenses."
    },
    {
        "date": "2025-06",
        "title": "Exploiting AI for Attacks: On the Interplay between Adversarial AI and Offensive AI",
        "author": "Saskia Laura Schr\u00f6er, Luca Pajola, Alberto Castagnaro, Giovanni Apruzzese, and Mauro Conti",
        "link": "http://arxiv.org/abs/2506.12519v1",
        "abstract": "As Artificial Intelligence (AI) continues to evolve, it has transitioned from\na research-focused discipline to a widely adopted technology, enabling\nintelligent solutions across various sectors. In security, AI's role in\nstrengthening organizational resilience has been studied for over two decades.\nWhile much attention has focused on AI's constructive applications, the\nincreasing maturity and integration of AI have also exposed its darker\npotentials. This article explores two emerging AI-related threats and the\ninterplay between them: AI as a target of attacks (`Adversarial AI') and AI as\na means to launch attacks on any target (`Offensive AI') -- potentially even on\nanother AI. By cutting through the confusion and explaining these threats in\nplain terms, we introduce the complex and often misunderstood interplay between\nAdversarial AI and Offensive AI, offering a clear and accessible introduction\nto the challenges posed by these threats."
    },
    {
        "date": "2025-06",
        "title": "Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization",
        "author": "Filip Sondej, Yushi Yang, Miko\u0142aj Kniejski, and Marcel Windys",
        "link": "http://arxiv.org/abs/2506.12484v1",
        "abstract": "Language models can retain dangerous knowledge and skills even after\nextensive safety fine-tuning, posing both misuse and misalignment risks. Recent\nstudies show that even specialized unlearning methods can be easily reversed.\nTo address this, we systematically evaluate many existing and novel components\nof unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating\nweights, where the signs of the unlearning gradient and the retaining gradient\nare the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients,\nand also confirm the usefulness of meta-learning. We combine these insights\ninto MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and\nvalidate its effectiveness at preventing the recovery of dangerous\ncapabilities. MUDMAN outperforms the prior TAR method by 40\\%, setting a new\nstate-of-the-art for robust unlearning."
    },
    {
        "date": "2025-06",
        "title": "Towards Safety and Security Testing of Cyberphysical Power Systems by Shape Validation",
        "author": "Alexander Geiger, Immanuel Hacker, \u00d6mer Sen, and Andreas Ulbig",
        "link": "http://arxiv.org/abs/2506.12466v1",
        "abstract": "The increasing complexity of cyberphysical power systems leads to larger\nattack surfaces to be exploited by malicious actors and a higher risk of faults\nthrough misconfiguration. We propose to meet those risks with a declarative\napproach to describe cyberphysical power systems and to automatically evaluate\nsecurity and safety controls. We leverage Semantic Web technologies as a\nwell-standardized framework, providing languages to specify ontologies, rules\nand shape constraints. We model infrastructure through an ontology which\ncombines external ontologies, architecture and data models for sufficient\nexpressivity and interoperability with external systems. The ontology can\nenrich itself through rules defined in SPARQL, allowing for the inference of\nknowledge that is not explicitly stated. Through the evaluation of SHACL shape\nconstraints we can then validate the data and verify safety and security\nconstraints. We demonstrate this concept with two use cases and illustrate how\nthis solution can be developed further in a community-driven fashion."
    },
    {
        "date": "2025-06",
        "title": "Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates",
        "author": "Chengqing Yu, Fei Wang, Chuanguang Yang, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, and Yongjun Xu",
        "link": "http://arxiv.org/abs/2506.12459v1",
        "abstract": "Multivariate Time Series Forecasting (MTSF) involves predicting future values\nof multiple interrelated time series. Recently, deep learning-based MTSF models\nhave gained significant attention for their promising ability to mine semantics\n(global and local information) within MTS data. However, these models are\npervasively susceptible to missing values caused by malfunctioning data\ncollectors. These missing values not only disrupt the semantics of MTS, but\ntheir distribution also changes over time. Nevertheless, existing models lack\nrobustness to such issues, leading to suboptimal forecasting performance. To\nthis end, in this paper, we propose Multi-View Representation Learning\n(Merlin), which can help existing models achieve semantic alignment between\nincomplete observations with different missing rates and complete observations\nin MTS. Specifically, Merlin consists of two key modules: offline knowledge\ndistillation and multi-view contrastive learning. The former utilizes a teacher\nmodel to guide a student model in mining semantics from incomplete\nobservations, similar to those obtainable from complete observations. The\nlatter improves the student model's robustness by learning from\npositive/negative data pairs constructed from incomplete observations with\ndifferent missing rates, ensuring semantic alignment across different missing\nrates. Therefore, Merlin is capable of effectively enhancing the robustness of\nexisting models against unfixed missing rates while preserving forecasting\naccuracy. Experiments on four real-world datasets demonstrate the superiority\nof Merlin."
    },
    {
        "date": "2025-06",
        "title": "On the existence of consistent adversarial attacks in high-dimensional linear classification",
        "author": "Matteo Vilucchio, Lenka Zdeborov\u00e1, and Bruno Loureiro",
        "link": "http://arxiv.org/abs/2506.12454v1",
        "abstract": "What fundamentally distinguishes an adversarial attack from a\nmisclassification due to limited model expressivity or finite data? In this\nwork, we investigate this question in the setting of high-dimensional binary\nclassification, where statistical effects due to limited data availability play\na central role. We introduce a new error metric that precisely capture this\ndistinction, quantifying model vulnerability to consistent adversarial attacks\n-- perturbations that preserve the ground-truth labels. Our main technical\ncontribution is an exact and rigorous asymptotic characterization of these\nmetrics in both well-specified models and latent space models, revealing\ndifferent vulnerability patterns compared to standard robust error measures.\nThe theoretical results demonstrate that as models become more\noverparameterized, their vulnerability to label-preserving perturbations grows,\noffering theoretical insight into the mechanisms underlying model sensitivity\nto adversarial attacks."
    },
    {
        "date": "2025-06",
        "title": "LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning",
        "author": "Haotian Zhang, Liu Liu, Baosheng Yu, Jiayan Qiu, Yanwei Ren, and Xianglong Liu",
        "link": "http://arxiv.org/abs/2506.12394v1",
        "abstract": "The advent of parameter-efficient fine-tuning methods has significantly\nreduced the computational burden of adapting large-scale pretrained models to\ndiverse downstream tasks. However, existing approaches often struggle to\nachieve robust performance under domain shifts while maintaining computational\nefficiency. To address this challenge, we propose Low-rAnk Regulated Gradient\nProjection (LARGO) algorithm that integrates dynamic constraints into low-rank\nadaptation methods. Specifically, LARGO incorporates parallel trainable\ngradient projections to dynamically regulate layer-wise updates, retaining the\nOut-Of-Distribution robustness of pretrained model while preserving inter-layer\nindependence. Additionally, it ensures computational efficiency by mitigating\nthe influence of gradient dependencies across layers during weight updates.\nBesides, through leveraging singular value decomposition of pretrained weights\nfor structured initialization, we incorporate an SVD-based initialization\nstrategy that minimizing deviation from pretrained knowledge. Through extensive\nexperiments on diverse benchmarks, LARGO achieves state-of-the-art performance\nacross in-domain and out-of-distribution scenarios, demonstrating improved\nrobustness under domain shifts with significantly lower computational overhead\ncompared to existing PEFT methods. The source code will be released soon."
    },
    {
        "date": "2025-06",
        "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks",
        "author": "Haoyu Zhai, Shuo Wang, Pirouz Naghavi, Qingying Hao, and Gang Wang",
        "link": "http://arxiv.org/abs/2506.12344v1",
        "abstract": "Gaussian blur is widely used to blur human faces in sensitive photos before\nthe photos are posted on the Internet. However, it is unclear to what extent\nthe blurred faces can be restored and used to re-identify the person,\nespecially under a high-blurring setting. In this paper, we explore this\nquestion by developing a deblurring method called Revelio. The key intuition is\nto leverage a generative model's memorization effect and approximate the\ninverse function of Gaussian blur for face restoration. Compared with existing\nmethods, we design the deblurring process to be identity-preserving. It uses a\nconditional Diffusion model for preliminary face restoration and then uses an\nidentity retrieval model to retrieve related images to further enhance\nfidelity. We evaluate Revelio with large public face image datasets and show\nthat it can effectively restore blurred faces, especially under a high-blurring\nsetting. It has a re-identification accuracy of 95.9%, outperforming existing\nsolutions. The result suggests that Gaussian blur should not be used for face\nanonymization purposes. We also demonstrate the robustness of this method\nagainst mismatched Gaussian kernel sizes and functions, and test preliminary\ncountermeasures and adaptive attacks to inspire future work."
    },
    {
        "date": "2025-06",
        "title": "Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models",
        "author": "Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, and Suhang Wang",
        "link": "http://arxiv.org/abs/2506.12340v2",
        "abstract": "Large vision-language models (LVLMs) have demonstrated outstanding\nperformance in many downstream tasks. However, LVLMs are trained on large-scale\ndatasets, which can pose privacy risks if training images contain sensitive\ninformation. Therefore, it is important to detect whether an image is used to\ntrain the LVLM. Recent studies have investigated membership inference attacks\n(MIAs) against LVLMs, including detecting image-text pairs and single-modality\ncontent. In this work, we focus on detecting whether a target image is used to\ntrain the target LVLM. We design simple yet effective Image Corruption-Inspired\nMembership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by\nLVLM's different sensitivity to image corruption for member and non-member\nimages. We first perform an MIA method under the white-box setting, where we\ncan obtain the embeddings of the image through the vision part of the target\nLVLM. The attacks are based on the embedding similarity between the image and\nits corrupted version. We further explore a more practical scenario where we\nhave no knowledge about target LVLMs and we can only query the target LVLMs\nwith an image and a question. We then conduct the attack by utilizing the\noutput text embeddings' similarity. Experiments on existing datasets validate\nthe effectiveness of our proposed attack methods under those two different\nsettings."
    },
    {
        "date": "2025-06",
        "title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device",
        "author": "Chuntao Ding, Jianhang Xie, Junna Zhang, Salman Raza, Shangguang Wang, and Jiannong Cao",
        "link": "http://arxiv.org/abs/2506.12335v1",
        "abstract": "It has become mainstream to deploy Convolutional Neural Network (CNN) models\non ubiquitous Internet of Things (IoT) devices with the help of the cloud to\nprovide users with a variety of high-quality services. Most existing methods\nhave two limitations: (i) low robustness in handling corrupted image data\ncollected by IoT devices; and (ii) high consumption of computational and\ntransmission resources. To this end, we propose the Grouped NonLinear\ntransformation generation method (GroupNL), which generates diversified feature\nmaps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to\nimprove the robustness of the CNN model. Specifically, partial convolution\nfilters are designated as seed filters in a convolutional layer, and a small\nset of feature maps, i.e., seed feature maps, are first generated based on\nvanilla convolution operation. Then, we split seed feature maps into several\ngroups, each with a set of different NLFs, to generate corresponding diverse\nfeature maps with in-place nonlinear processing. Moreover, GroupNL effectively\nreduces the parameter transmission between multiple nodes during model training\nby setting the hyperparameters of NLFs to random initialization and not\nupdating them during model training, and reduces the computing resources by\nusing NLFs to generate feature maps instead of most feature maps generated\nbased on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,\nIcons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the\nproposed GroupNL outperforms other state-of-the-art methods in model robust and\ntraining acceleration. Specifically, on the Icons-50 dataset, the accuracy of\nGroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla\nResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN\nwhen trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset."
    },
    {
        "date": "2025-06",
        "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions",
        "author": "Stephen Mell, Botong Zhang, David Mell, Shuo Li, Ramya Ramalingam, Nathan Yu, Steve Zdancewic, and Osbert Bastani",
        "link": "http://arxiv.org/abs/2506.12202v1",
        "abstract": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level."
    },
    {
        "date": "2025-06",
        "title": "A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11901v1",
        "abstract": "Advantages of deep learning over traditional methods have been demonstrated\nfor radio signal classification in the recent years. However, various\nresearchers have discovered that even a small but intentional feature\nperturbation known as adversarial examples can significantly deteriorate the\nperformance of the deep learning based radio signal classification. Among\nvarious kinds of adversarial examples, universal adversarial perturbation has\ngained considerable attention due to its feature of being data independent,\nhence as a practical strategy to fool the radio signal classification with a\nhigh success rate. Therefore, in this paper, we investigate a defense system\ncalled neural rejection system to propose against universal adversarial\nperturbations, and evaluate its performance by generating white-box universal\nadversarial perturbations. We show that the proposed neural rejection system is\nable to defend universal adversarial perturbations with significantly higher\naccuracy than the undefended deep neural network."
    },
    {
        "date": "2025-06",
        "title": "Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices",
        "author": "Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Basil AsSadhan, and Fabio Roli",
        "link": "http://arxiv.org/abs/2506.11892v1",
        "abstract": "Due to great success of transformers in many applications such as natural\nlanguage processing and computer vision, transformers have been successfully\napplied in automatic modulation classification. We have shown that\ntransformer-based radio signal classification is vulnerable to imperceptible\nand carefully crafted attacks called adversarial examples. Therefore, we\npropose a defense system against adversarial examples in transformer-based\nmodulation classifications. Considering the need for computationally efficient\narchitecture particularly for Internet of Things (IoT)-based applications or\noperation of devices in environment where power supply is limited, we propose a\ncompact transformer for modulation classification. The advantages of robust\ntraining such as adversarial training in transformers may not be attainable in\ncompact transformers. By demonstrating this, we propose a novel compact\ntransformer that can enhance robustness in the presence of adversarial attacks.\nThe new method is aimed at transferring the adversarial attention map from the\nrobustly trained large transformer to a compact transformer. The proposed\nmethod outperforms the state-of-the-art techniques for the considered white-box\nscenarios including fast gradient method and projected gradient descent\nattacks. We have provided reasoning of the underlying working mechanisms and\ninvestigated the transferability of the adversarial examples between different\narchitectures. The proposed method has the potential to protect the transformer\nfrom the transferability of adversarial examples."
    },
    {
        "date": "2025-06",
        "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
        "author": "Jina Kim, Jeffrey Willette, Bruno Andreis, and Sung Ju Hwang",
        "link": "http://arxiv.org/abs/2506.11877v1",
        "abstract": "A widely recognized limitation of molecular prediction models is their\nreliance on structures observed in the training data, resulting in poor\ngeneralization to out-of-distribution compounds. Yet in drug discovery, the\ncompounds most critical for advancing research often lie beyond the training\nset, making the bias toward the training data particularly problematic. This\nmismatch introduces substantial covariate shift, under which standard deep\nlearning models produce unstable and inaccurate predictions. Furthermore, the\nscarcity of labeled data, stemming from the onerous and costly nature of\nexperimental validation, further exacerbates the difficulty of achieving\nreliable generalization. To address these limitations, we propose a novel\nmeta-learning-based approach that leverages unlabeled data to interpolate\nbetween in-distribution (ID) and out-of-distribution (OOD) data, enabling the\nmodel to meta-learn how to generalize beyond the training distribution. We\ndemonstrate significant performance gains over state-of-the-art methods on\nchallenging real-world datasets that exhibit substantial covariate shift."
    },
    {
        "date": "2025-06",
        "title": "In Defense of Defensive Forecasting",
        "author": "Juan Carlos Perdomo, and Benjamin Recht",
        "link": "http://arxiv.org/abs/2506.11848v1",
        "abstract": "This tutorial provides a survey of algorithms for Defensive Forecasting,\nwhere predictions are derived not by prognostication but by correcting past\nmistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of\nprediction as a sequential game, and derives predictions to minimize metrics no\nmatter what outcomes occur. We present an elementary introduction to this\ngeneral theory and derive simple, near-optimal algorithms for online learning,\ncalibration, prediction with expert advice, and online conformal prediction."
    },
    {
        "date": "2025-06",
        "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks",
        "author": "Qihai Zhang, Xinyue Sheng, Yuanfu Sun, and Qiaoyu Tan",
        "link": "http://arxiv.org/abs/2506.11844v1",
        "abstract": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field."
    },
    {
        "date": "2025-06",
        "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks",
        "author": "Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, and Lingming Zhang",
        "link": "http://arxiv.org/abs/2506.11791v1",
        "abstract": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
    },
    {
        "date": "2025-06",
        "title": "LLMs on support of privacy and security of mobile apps: state of the art and research directions",
        "author": "Tran Thanh Lam Nguyen, Barbara Carminati, and Elena Ferrari",
        "link": "http://arxiv.org/abs/2506.11679v1",
        "abstract": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges."
    },
    {
        "date": "2025-06",
        "title": "Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments",
        "author": "Deliang Jin, Gang Chen, Shuo Feng, Yufeng Ling, and Haoran Zhu",
        "link": "http://arxiv.org/abs/2506.11615v1",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success across diverse\ndomains, but their performance can be severely degraded by noisy or corrupted\ntraining data. Conventional noise mitigation methods often rely on explicit\nassumptions about noise distributions or require extensive retraining, which\ncan be impractical for large-scale models. Inspired by the principles of\nmachine unlearning, we propose a novel framework that integrates\nattribution-guided data partitioning, discriminative neuron pruning, and\ntargeted fine-tuning to mitigate the impact of noisy samples. Our approach\nemploys gradient-based attribution to probabilistically distinguish\nhigh-quality examples from potentially corrupted ones without imposing\nrestrictive assumptions on the noise. It then applies regression-based\nsensitivity analysis to identify and prune neurons that are most vulnerable to\nnoise. Finally, the resulting network is fine-tuned on the high-quality data\nsubset to efficiently recover and enhance its generalization performance. This\nintegrated unlearning-inspired framework provides several advantages over\nconventional noise-robust learning approaches. Notably, it combines data-level\nunlearning with model-level adaptation, thereby avoiding the need for full\nmodel retraining or explicit noise modeling. We evaluate our method on\nrepresentative tasks (e.g., CIFAR-10 image classification and speech\nrecognition) under various noise levels and observe substantial gains in both\naccuracy and efficiency. For example, our framework achieves approximately a\n10% absolute accuracy improvement over standard retraining on CIFAR-10 with\ninjected label noise, while reducing retraining time by up to 47% in some\nsettings. These results demonstrate the effectiveness and scalability of the\nproposed approach for achieving robust generalization in noisy environments."
    },
    {
        "date": "2025-06",
        "title": "KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity",
        "author": "Yaning Jia, Shenyang Deng, Chiyu Ma, Yaoqing Yang, and Soroush Vosoughi",
        "link": "http://arxiv.org/abs/2506.11611v1",
        "abstract": "Graph Neural Networks (GNNs) have achieved impressive success across a wide\nrange of graph-based tasks, yet they remain highly vulnerable to small,\nimperceptible perturbations and adversarial attacks. Although numerous defense\nmethods have been proposed to address these vulnerabilities, many rely on\nheuristic metrics, overfit to specific attack patterns, and suffer from high\ncomputational complexity. In this paper, we propose Kernel Complexity-Based\nEdge Sanitization (KCES), a training-free, model-agnostic defense framework.\nKCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the\ngraph's Gram matrix that characterizes GNN generalization via its test error\nbound. Building on GKC, we define a KC score for each edge, measuring the\nchange in GKC when the edge is removed. Edges with high KC scores, typically\nintroduced by adversarial perturbations, are pruned to mitigate their harmful\neffects, thereby enhancing GNNs' robustness. KCES can also be seamlessly\nintegrated with existing defense strategies as a plug-and-play module without\nrequiring training. Theoretical analysis and extensive experiments demonstrate\nthat KCES consistently enhances GNN robustness, outperforms state-of-the-art\nbaselines, and amplifies the effectiveness of existing defenses, offering a\nprincipled and efficient solution for securing GNNs."
    },
    {
        "date": "2025-06",
        "title": "SecONNds: Secure Outsourced Neural Network Inference on ImageNet",
        "author": "Shashank Balla",
        "link": "http://arxiv.org/abs/2506.11586v1",
        "abstract": "The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds."
    },
    {
        "date": "2025-06",
        "title": "Linearly Solving Robust Rotation Estimation",
        "author": "Yinlong Liu, Tianyu Huang, and Zhi-Xin Yang",
        "link": "http://arxiv.org/abs/2506.11547v1",
        "abstract": "Rotation estimation plays a fundamental role in computer vision and robot\ntasks, and extremely robust rotation estimation is significantly useful for\nsafety-critical applications. Typically, estimating a rotation is considered a\nnon-linear and non-convex optimization problem that requires careful design.\nHowever, in this paper, we provide some new perspectives that solving a\nrotation estimation problem can be reformulated as solving a linear model\nfitting problem without dropping any constraints and without introducing any\nsingularities. In addition, we explore the dual structure of a rotation motion,\nrevealing that it can be represented as a great circle on a quaternion sphere\nsurface. Accordingly, we propose an easily understandable voting-based method\nto solve rotation estimation. The proposed method exhibits exceptional\nrobustness to noise and outliers and can be computed in parallel with graphics\nprocessing units (GPUs) effortlessly. Particularly, leveraging the power of\nGPUs, the proposed method can obtain a satisfactory rotation solution for\nlarge-scale($10^6$) and severely corrupted (99$\\%$ outlier ratio) rotation\nestimation problems under 0.5 seconds. Furthermore, to validate our theoretical\nframework and demonstrate the superiority of our proposed method, we conduct\ncontrolled experiments and real-world dataset experiments. These experiments\nprovide compelling evidence supporting the effectiveness and robustness of our\napproach in solving rotation estimation problems."
    },
    {
        "date": "2025-06",
        "title": "Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications",
        "author": "Aamir Hussain Chughtai",
        "link": "http://arxiv.org/abs/2506.11530v1",
        "abstract": "State estimation or filtering serves as a fundamental task to enable\nintelligent decision-making in applications such as autonomous vehicles,\nrobotics, healthcare monitoring, smart grids, intelligent transportation, and\npredictive maintenance. Standard filtering assumes prior knowledge of noise\nstatistics to extract latent system states from noisy sensor data. However,\nreal-world scenarios involve abnormalities like outliers, biases, drifts, and\nmissing observations with unknown or partially known statistics, limiting\nconventional approaches. This thesis presents novel robust nonlinear filtering\nmethods to mitigate these challenges. Based on insights from our filtering\nproposals, we extend the formulations to offline estimation/learning setups and\npropose smoothing extensions. Our methods leverage Bayesian inference\nframeworks, employing both deterministic and stochastic approximation\ntechniques including Variational Inference (VI) and Particle Filters/Sequential\nMonte Carlo (SMC). We also study theoretical estimation limits using Bayesian\nCram\\'er-Rao bounds (BCRBs) in the context of measurement abnormalities. To\nvalidate the performance gains of the proposed methods, we perform simulations\nand experiments in scenarios including target tracking, indoor localization, 3D\npoint cloud registration, mesh registration, and pose graph optimization. The\nfundamental nature of the work makes it useful in diverse applications, with\npossible future extensions toward developing outlier-robust machine learning\npipelines, learning system dynamics from anomalous data, and addressing\nchallenges in generative AI where standard diffusion models struggle with\noutliers, imbalanced datasets, and mode collapse."
    },
    {
        "date": "2025-06",
        "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models",
        "author": "Jinming Wen, Xinyi Wu, Shuai Zhao, Yanhao Jia, and Yuwen Li",
        "link": "http://arxiv.org/abs/2506.11521v1",
        "abstract": "Multimodal large language models (MLLMs), which bridge the gap between\naudio-visual and natural language processing, achieve state-of-the-art\nperformance on several audio-visual tasks. Despite the superior performance of\nMLLMs, the scarcity of high-quality audio-visual training data and\ncomputational resources necessitates the utilization of third-party data and\nopen-source MLLMs, a trend that is increasingly observed in contemporary\nresearch. This prosperity masks significant security risks. Empirical studies\ndemonstrate that the latest MLLMs can be manipulated to produce malicious or\nharmful content. This manipulation is facilitated exclusively through\ninstructions or inputs, including adversarial perturbations and malevolent\nqueries, effectively bypassing the internal security mechanisms embedded within\nthe models. To gain a deeper comprehension of the inherent security\nvulnerabilities associated with audio-visual-based multimodal models, a series\nof surveys investigates various types of attacks, including adversarial and\nbackdoor attacks. While existing surveys on audio-visual attacks provide a\ncomprehensive overview, they are limited to specific types of attacks, which\nlack a unified review of various types of attacks. To address this issue and\ngain insights into the latest trends in the field, this paper presents a\ncomprehensive and systematic review of audio-visual attacks, which include\nadversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this\npaper also reviews various types of attacks in the latest audio-visual-based\nMLLMs, a dimension notably absent in existing surveys. Drawing upon\ncomprehensive insights from a substantial review, this paper delineates both\nchallenges and emergent trends for future research on audio-visual attacks and\ndefense."
    },
    {
        "date": "2025-06",
        "title": "On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving",
        "author": "Pedram MohajerAnsari, Amir Salarpour, Michael K\u00fchr, Siyu Huang, Mohammad Hamad, Sebastian Steinhorst, Habeeb Olufowobi, and Mert D. Pes\u00e9",
        "link": "http://arxiv.org/abs/2506.11472v1",
        "abstract": "Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical\ntasks such as traffic sign recognition (TSR), automated lane centering (ALC),\nand vehicle detection (VD). However, these models are vulnerable to attacks\nthat can cause misclassifications and compromise safety. Traditional defense\nmechanisms, including adversarial training, often degrade benign accuracy and\nfail to generalize against unseen attacks. In this work, we introduce Vehicle\nVision Language Models (V2LMs), fine-tuned vision-language models specialized\nfor AV perception. Our findings demonstrate that V2LMs inherently exhibit\nsuperior robustness against unseen attacks without requiring adversarial\ntraining, maintaining significantly higher accuracy than conventional DNNs\nunder adversarial conditions. We evaluate two deployment strategies: Solo Mode,\nwhere individual V2LMs handle specific perception tasks, and Tandem Mode, where\na single unified V2LM is fine-tuned for multiple tasks simultaneously.\nExperimental results reveal that DNNs suffer performance drops of 33% to 46%\nunder attacks, whereas V2LMs maintain adversarial accuracy with reductions of\nless than 8% on average. The Tandem Mode further offers a memory-efficient\nalternative while achieving comparable robustness to Solo Mode. We also explore\nintegrating V2LMs as parallel components to AV perception to enhance resilience\nagainst adversarial threats. Our results suggest that V2LMs offer a promising\npath toward more secure and resilient AV perception systems."
    },
    {
        "date": "2025-06",
        "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
        "author": "Hao Li, Xiaogeng Liu, Hung-Chun Chiu, Dianqi Li, Ning Zhang, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2506.12104v1",
        "abstract": "Large Language Models (LLMs) are increasingly central to agentic systems due\nto their strong reasoning and planning capabilities. By interacting with\nexternal environments through predefined tools, these agents can carry out\ncomplex user tasks. Nonetheless, this interaction also introduces the risk of\nprompt injection attacks, where malicious inputs from external sources can\nmislead the agent's behavior, potentially resulting in economic loss, privacy\nleakage, or system compromise. System-level defenses have recently shown\npromise by enforcing static or predefined policies, but they still face two key\nchallenges: the ability to dynamically update security rules and the need for\nmemory stream isolation. To address these challenges, we propose DRIFT, a\nDynamic Rule-based Isolation Framework for Trustworthy agentic systems, which\nenforces both control- and data-level constraints. A Secure Planner first\nconstructs a minimal function trajectory and a JSON-schema-style parameter\nchecklist for each function node based on the user query. A Dynamic Validator\nthen monitors deviations from the original plan, assessing whether changes\ncomply with privilege limitations and the user's intent. Finally, an Injection\nIsolator detects and masks any instructions that may conflict with the user\nquery from the memory stream to mitigate long-term risks. We empirically\nvalidate the effectiveness of DRIFT on the AgentDojo benchmark, demonstrating\nits strong security performance while maintaining high utility across diverse\nmodels -- showcasing both its robustness and adaptability."
    },
    {
        "date": "2025-06",
        "title": "GaussMarker: Robust Dual-Domain Watermark for Diffusion Models",
        "author": "Kecen Li, Zhicong Huang, Xinwen Hou, and Cheng Hong",
        "link": "http://arxiv.org/abs/2506.11444v1",
        "abstract": "As Diffusion Models (DM) generate increasingly realistic images, related\nissues such as copyright and misuse have become a growing concern. Watermarking\nis one of the promising solutions. Existing methods inject the watermark into\nthe single-domain of initial Gaussian noise for generation, which suffers from\nunsatisfactory robustness. This paper presents the first dual-domain DM\nwatermarking approach using a pipelined injector to consistently embed\nwatermarks in both the spatial and frequency domains. To further boost\nrobustness against certain image manipulations and advanced attacks, we\nintroduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine\nGaussian noise extracted from manipulated images and enhance detection\nrobustness by integrating the detection scores of both watermarks. GaussMarker\nefficiently achieves state-of-the-art performance under eight image distortions\nand four advanced attacks across three versions of Stable Diffusion with better\nrecall and lower false positive rates, as preferred in real applications."
    },
    {
        "date": "2025-06",
        "title": "Improving Group Robustness on Spurious Correlation via Evidential Alignment",
        "author": "Wenqian Ye, Guangtao Zheng, and Aidong Zhang",
        "link": "http://arxiv.org/abs/2506.11347v2",
        "abstract": "Deep neural networks often learn and rely on spurious correlations, i.e.,\nsuperficial associations between non-causal features and the targets. For\ninstance, an image classifier may identify camels based on the desert\nbackgrounds. While it can yield high overall accuracy during training, it\ndegrades generalization on more diverse scenarios where such correlations do\nnot hold. This problem poses significant challenges for out-of-distribution\nrobustness and trustworthiness. Existing methods typically mitigate this issue\nby using external group annotations or auxiliary deterministic models to learn\nunbiased representations. However, such information is costly to obtain, and\ndeterministic models may fail to capture the full spectrum of biases learned by\nthe models. To address these limitations, we propose Evidential Alignment, a\nnovel framework that leverages uncertainty quantification to understand the\nbehavior of the biased models without requiring group annotations. By\nquantifying the evidence of model prediction with second-order risk\nminimization and calibrating the biased models with the proposed evidential\ncalibration technique, Evidential Alignment identifies and suppresses spurious\ncorrelations while preserving core features. We theoretically justify the\neffectiveness of our method as capable of learning the patterns of biased\nmodels and debiasing the model without requiring any spurious correlation\nannotations. Empirical results demonstrate that our method significantly\nimproves group robustness across diverse architectures and data modalities,\nproviding a scalable and principled solution to spurious correlations."
    },
    {
        "date": "2025-06",
        "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving",
        "author": "Luke Rowe, Rodrigue de Schaetzen, Roger Girgis, Christopher Pal, and Liam Paull",
        "link": "http://arxiv.org/abs/2506.11234v1",
        "abstract": "We present Poutine, a 3B-parameter vision-language model (VLM) tailored for\nend-to-end autonomous driving in long-tail driving scenarios. Poutine is\ntrained in two stages. To obtain strong base driving capabilities, we train\nPoutine-Base in a self-supervised vision-language-trajectory (VLT) next-token\nprediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo\nlong-tail driving. Accompanying language annotations are auto-generated with a\n72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group\nRelative Policy Optimization (GRPO) using less than 500 preference-labeled\nframes from the Waymo validation set. We show that both VLT pretraining and RL\nfine-tuning are critical to attain strong driving performance in the long-tail.\nPoutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation\nset, nearly matching Waymo's expert ground-truth RFS. The final Poutine model\nachieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025\nWaymo Vision-Based End-to-End Driving Challenge by a significant margin. These\nresults highlight the promise of scalable VLT pre-training and lightweight RL\nfine-tuning to enable robust and generalizable autonomy."
    },
    {
        "date": "2025-06",
        "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors",
        "author": "Chen Yueh-Han, Nitish Joshi, Yulin Chen, Maksym Andriushchenko, Rico Angell, and He He",
        "link": "http://arxiv.org/abs/2506.10949v2",
        "abstract": "Current LLM safety defenses fail under decomposition attacks, where a\nmalicious goal is decomposed into benign subtasks that circumvent refusals. The\nchallenge lies in the existing shallow safety alignment techniques: they only\ndetect harm in the immediate prompt and do not reason about long-range intent,\nleaving them blind to malicious intent that emerges over a sequence of\nseemingly benign instructions. We therefore propose adding an external monitor\nthat observes the conversation at a higher granularity. To facilitate our study\nof monitoring decomposition attacks, we curate the largest and most diverse\ndataset to date, including question-answering, text-to-image, and agentic\ntasks. We verify our datasets by testing them on frontier LLMs and show an 87%\nattack success rate on average on GPT-4o. This confirms that decomposition\nattack is broadly effective. Additionally, we find that random tasks can be\ninjected into the decomposed subtasks to further obfuscate malicious intents.\nTo defend in real time, we propose a lightweight sequential monitoring\nframework that cumulatively evaluates each subtask. We show that a carefully\nprompt engineered lightweight monitor achieves a 93% defense success rate,\nbeating reasoning models like o3 mini as a monitor. Moreover, it remains robust\nagainst random task injection and cuts cost by 90% and latency by 50%. Our\nfindings suggest that lightweight sequential monitors are highly effective in\nmitigating decomposition attacks and are viable in deployment."
    },
    {
        "date": "2025-06",
        "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers",
        "author": "Lucas Gnecco-Heredia, Benjamin Negrevergne, and Yann Chevaleyre",
        "link": "http://arxiv.org/abs/2506.10888v1",
        "abstract": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been\nproposed as a way to improve robustness against adversarial attacks. However,\nexisting attacks have been shown to not suit this kind of classifier. In this\npaper, we discuss the problem of attacking a mixture in a principled way and\nintroduce two desirable properties of attacks based on a geometrical analysis\nof the problem (effectiveness and maximality). We then show that existing\nattacks do not meet both of these properties. Finally, we introduce a new\nattack called {\\em lattice climber attack} with theoretical guarantees in the\nbinary linear setting, and demonstrate its performance by conducting\nexperiments on synthetic and real datasets."
    },
    {
        "date": "2025-06",
        "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization",
        "author": "Pierre-Fran\u00e7ois Massiani, Alexander von Rohr, Lukas Haverbeck, and Sebastian Trimpe",
        "link": "http://arxiv.org/abs/2506.10871v1",
        "abstract": "Despite the many recent advances in reinforcement learning (RL), the question\nof learning policies that robustly satisfy state constraints under unknown\ndisturbances remains open. In this paper, we offer a new perspective on\nachieving robust safety by analyzing the interplay between two well-established\ntechniques in model-free RL: entropy regularization, and constraints\npenalization. We reveal empirically that entropy regularization in constrained\nRL inherently biases learning toward maximizing the number of future viable\nactions, thereby promoting constraints satisfaction robust to action noise.\nFurthermore, we show that by relaxing strict safety constraints through\npenalties, the constrained RL problem can be approximated arbitrarily closely\nby an unconstrained one and thus solved using standard model-free RL. This\nreformulation preserves both safety and optimality while empirically improving\nresilience to disturbances. Our results indicate that the connection between\nentropy regularization and robustness is a promising avenue for further\nempirical and theoretical investigation, as it enables robust safety in RL\nthrough simple reward shaping."
    },
    {
        "date": "2025-06",
        "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security",
        "author": "Nudrat Fariha, Md Nazmuddin Moin Khan, Md Iqbal Hossain, Syed Ali Reza, Joy Chakra Bortty, Kazi Sharmin Sultana, Md Shadidur Islam Jawad, Saniah Safat, Md Abdul Ahad, and Maksuda Begum",
        "link": "http://arxiv.org/abs/2506.10842v1",
        "abstract": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions."
    },
    {
        "date": "2025-06",
        "title": "Efficiency Robustness of Dynamic Deep Learning Systems",
        "author": "Ravishka Rathnasuriya, Tingxi Li, Zexin Xu, Zihe Song, Mirazul Haque, Simin Chen, and Wei Yang",
        "link": "http://arxiv.org/abs/2506.10831v1",
        "abstract": "Deep Learning Systems (DLSs) are increasingly deployed in real-time\napplications, including those in resourceconstrained environments such as\nmobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning\nSystems (DDLSs) adapt inference computation based on input complexity, reducing\noverhead. While this dynamic behavior improves efficiency, such behavior\nintroduces new attack surfaces. In particular, efficiency adversarial attacks\nexploit these dynamic mechanisms to degrade system performance. This paper\nsystematically explores efficiency robustness of DDLSs, presenting the first\ncomprehensive taxonomy of efficiency attacks. We categorize these attacks based\non three dynamic behaviors: (i) attacks on dynamic computations per inference,\n(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic\noutput production for downstream tasks. Through an in-depth evaluation, we\nanalyze adversarial strategies that target DDLSs efficiency and identify key\nchallenges in securing these systems. In addition, we investigate existing\ndefense mechanisms, demonstrating their limitations against increasingly\npopular efficiency attacks and the necessity for novel mitigation strategies to\nsecure future adaptive DDLSs."
    },
    {
        "date": "2025-06",
        "title": "Quantum Computing and Cybersecurity in Accounting and Finance: Current and the Future Challenges and Opportunities for Securing Accounting and Finance Systems",
        "author": "Huma Habib Shadan, and Sardar Islam",
        "link": "http://arxiv.org/abs/2506.12096v1",
        "abstract": "Quantum computing is revolutionising information systems and will have a\nsignificant impact on accounting and finance, especially in the area of\ncybersecurity. It presents both opportunities and risks in ensuring\nconfidentiality and protecting financial data. The purpose of this thesis is to\nshow the application of quantum technologies in accounting cybersecurity,\nutilising quantum algorithms and QKD to overcome the limitations of classical\ncomputing.\n  The literature review reveals the vulnerabilities of the current accounting\ncybersecurity to quantum attacks and the need for quantum-resistant\ncryptographic mechanisms. It elaborates on the risks associated with\nconventional encryption in the context of quantum capabilities. This study\ncontributes to the understanding of how quantum computing can revolutionise\naccounting cybersecurity by enhancing quantum-resistant algorithms and\nutilising quantum key distribution (QKD) in accounting.\n  The study employs PSALSAR systematic review methodology to ensure rigour and\ndepth. The analysis shows that quantum computing enhances encryption techniques\nto superior possibilities than classical ones. Using quantum technologies in\naccounting minimises data breaches and unauthorised access. The study concludes\nthat quantum-resistant algorithms and quantum key distribution (QKD) are\nnecessary for securing the accounting and finance systems of the future.\n  Keywords Quantum Computing, Cybersecurity, Accounting, Machine Learning,\nArtificial Intelligence, Quantum Key Distribution, Operations Management"
    },
    {
        "date": "2025-06",
        "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement",
        "author": "Feiyu Yang, Siyuan Liang, Aishan Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2506.10776v1",
        "abstract": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all."
    },
    {
        "date": "2025-06",
        "title": "ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks",
        "author": "Xiaobei Yan, Han Qiu, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.10744v1",
        "abstract": "Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks\n(DNNs), where flipping a small number of bits in the model parameters or binary\ncode can significantly degrade the model accuracy or mislead the model\nprediction in a desired way. Existing defenses exclusively focus on protecting\nmodels for specific attacks and platforms, while lacking effectiveness for\nother scenarios. We propose ObfusBFA, an efficient and holistic methodology to\nmitigate BFAs targeting both the high-level model weights and low-level\ncodebase (executables or shared libraries). The key idea of ObfusBFA is to\nintroduce random dummy operations during the model inference, which effectively\ntransforms the delicate attacks into random bit flips, making it much harder\nfor attackers to pinpoint and exploit vulnerable bits. We design novel\nalgorithms to identify critical bits and insert obfuscation operations. We\nevaluate ObfusBFA against different types of attacks, including the adaptive\nscenarios where the attacker increases the flip bit budget to attempt to\ncircumvent our defense. The results show that ObfusBFA can consistently\npreserve the model accuracy across various datasets and DNN architectures while\nsignificantly reducing the attack success rates. Additionally, it introduces\nminimal latency and storage overhead, making it a practical solution for\nreal-world applications."
    },
    {
        "date": "2025-06",
        "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks",
        "author": "Xiaoxing Mo, Yuxuan Cheng, Nan Sun, Leo Yu Zhang, Wei Luo, and Shang Gao",
        "link": "http://arxiv.org/abs/2506.10722v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where\nattackers implant hidden triggers during training to maliciously control model\nbehavior. Topological Evolution Dynamics (TED) has recently emerged as a\npowerful tool for detecting backdoor attacks in DNNs. However, TED can be\nvulnerable to backdoor attacks that adaptively distort topological\nrepresentation distributions across network layers. To address this limitation,\nwe propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow\nrelease, and Target mapping attack strategies), a novel defense strategy that\nenhances TED's robustness against adaptive attacks. TED-LaST introduces two key\ninnovations: label-supervised dynamics tracking and adaptive layer emphasis.\nThese enhancements enable the identification of stealthy threats that evade\ntraditional TED-based defenses, even in cases of inseparability in topological\nspace and subtle topological perturbations. We review and classify data\npoisoning tricks in state-of-the-art adaptive attacks and propose enhanced\nadaptive attack with target mapping, which can dynamically shift malicious\ntasks and fully leverage the stealthiness that adaptive attacks possess. Our\ncomprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and\nImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST\neffectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,\nand the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for\nrobust backdoor detection, substantially enhancing DNN security against\nevolving threats."
    },
    {
        "date": "2025-06",
        "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework",
        "author": "Xia Du, Xiaoyuan Liu, Jizhe Zhou, Zheng Lin, Chi-man Pun, Zhe Chen, Wei Ni, and Jun Luo",
        "link": "http://arxiv.org/abs/2506.10685v1",
        "abstract": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs."
    },
    {
        "date": "2025-06",
        "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation",
        "author": "Narges Nemati, and Mostafa Haghir Chehreghani",
        "link": "http://arxiv.org/abs/2506.10658v1",
        "abstract": "Matrix completion is a widely adopted framework in recommender systems, as\npredicting the missing entries in the user-item rating matrix enables a\ncomprehensive understanding of user preferences. However, current graph neural\nnetwork (GNN)-based approaches are highly sensitive to noisy or irrelevant\nedges--due to their inherent message-passing mechanisms--and are prone to\noverfitting, which limits their generalizability. To overcome these challenges,\nwe propose a novel method called Matrix Completion using Contrastive Learning\n(MCCL). Our approach begins by extracting local neighborhood subgraphs for each\ninteraction and subsequently generates two distinct graph representations. The\nfirst representation emphasizes denoising by integrating GNN layers with an\nattention mechanism, while the second is obtained via a graph variational\nautoencoder that aligns the feature distribution with a standard prior. A\nmutual learning loss function is employed during training to gradually\nharmonize these representations, enabling the model to capture common patterns\nand significantly enhance its generalizability. Extensive experiments on\nseveral real-world datasets demonstrate that our approach not only improves the\nnumerical accuracy of the predicted scores--achieving up to a 0.8% improvement\nin RMSE--but also produces superior rankings with improvements of up to 36% in\nranking metrics."
    },
    {
        "date": "2025-06",
        "title": "Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes",
        "author": "Rogier C. van Dalen, Shucong Zhang, Titouan Parcollet, and Sourav Bhattacharya",
        "link": "http://arxiv.org/abs/2506.10653v1",
        "abstract": "Speech recognisers usually perform optimally only in a specific environment\nand need to be adapted to work well in another. For adaptation to a new\nspeaker, there is often too little data for fine-tuning to be robust, and that\ndata is usually unlabelled. This paper proposes a combination of approaches to\nmake adaptation to a single minute of data robust. First, instead of estimating\nthe adaptation parameters with cross-entropy on a single error-prone hypothesis\nor \"pseudo-label\", this paper proposes a novel loss function, the conditional\nentropy over complete hypotheses. Using multiple hypotheses makes adaptation\nmore robust to errors in the initial recognition. Second, a \"speaker code\"\ncharacterises a speaker in a vector short enough that it requires little data\nto estimate. On a far-field noise-augmented version of Common Voice, the\nproposed scheme yields a 20% relative improvement in word error rate on one\nminute of adaptation data, increasing on 10 minutes to 29%."
    },
    {
        "date": "2025-06",
        "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment",
        "author": "Stefano Longari, Alessandro Pozone, Jessica Leoni, Mario Polino, Michele Carminati, Mara Tanelli, and Stefano Zanero",
        "link": "http://arxiv.org/abs/2506.10638v1",
        "abstract": "In the last decades, Cyber-physical Systems (CPSs) have experienced a\nsignificant technological evolution and increased connectivity, at the cost of\ngreater exposure to cyber-attacks. Since many CPS are used in safety-critical\nsystems, such attacks entail high risks and potential safety harms. Although\nseveral defense strategies have been proposed, they rarely exploit the\ncyber-physical nature of the system. In this work, we exploit the nature of CPS\nby proposing CyFence, a novel architecture that improves the resilience of\nclosed-loop control systems against cyber-attacks by adding a semantic check,\nused to confirm that the system is behaving as expected. To ensure the security\nof the semantic check code, we use the Trusted Execution Environment\nimplemented by modern processors. We evaluate CyFence considering a real-world\napplication, consisting of an active braking digital controller, demonstrating\nthat it can mitigate different types of attacks with a negligible computation\noverhead."
    },
    {
        "date": "2025-06",
        "title": "Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation",
        "author": "Stefano Longari, Paolo Cerracchio, Michele Carminati, and Stefano Zanero",
        "link": "http://arxiv.org/abs/2506.10620v1",
        "abstract": "The security of modern vehicles has become increasingly important, with the\ncontroller area network (CAN) bus serving as a critical communication backbone\nfor various Electronic Control Units (ECUs). The absence of robust security\nmeasures in CAN, coupled with the increasing connectivity of vehicles, makes\nthem susceptible to cyberattacks. While intrusion detection systems (IDSs) have\nbeen developed to counter such threats, they are not foolproof. Adversarial\nattacks, particularly evasion attacks, can manipulate inputs to bypass\ndetection by IDSs. This paper extends our previous work by investigating the\nfeasibility and impact of gradient-based adversarial attacks performed with\ndifferent degrees of knowledge against automotive IDSs. We consider three\nscenarios: white-box (attacker with full system knowledge), grey-box (partial\nsystem knowledge), and the more realistic black-box (no knowledge of the IDS'\ninternal workings or data). We evaluate the effectiveness of the proposed\nattacks against state-of-the-art IDSs on two publicly available datasets.\nAdditionally, we study effect of the adversarial perturbation on the attack\nimpact and evaluate real-time feasibility by precomputing evasive payloads for\ntimed injection based on bus traffic. Our results demonstrate that, besides\nattacks being challenging due to the automotive domain constraints, their\neffectiveness is strongly dependent on the dataset quality, the target IDS, and\nthe attacker's degree of knowledge."
    },
    {
        "date": "2025-06",
        "title": "Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance",
        "author": "Chun Liu, Bingqian Zhu, Tao Xu, Zheng Zheng, Zheng Li, Wei Yang, Zhigang Han, and Jiayao Wang",
        "link": "http://arxiv.org/abs/2506.10459v1",
        "abstract": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose\nsecurity challenges to hyperspectral image (HSI) classification technologies\nbased on DNNs. In the domain of natural images, numerous transfer-based\nadversarial attack methods have been studied. However, HSIs differ from natural\nimages due to their high-dimensional and rich spectral information. Current\nresearch on HSI adversarial examples remains limited and faces challenges in\nfully utilizing the structural and feature information of images. To address\nthese issues, this paper proposes a novel method to enhance the transferability\nof the adversarial examples for HSI classification models. First, while keeping\nthe image structure unchanged, the proposed method randomly divides the image\ninto blocks in both spatial and spectral dimensions. Then, various\ntransformations are applied on a block by block basis to increase input\ndiversity and mitigate overfitting. Second, a feature distancing loss targeting\nintermediate layers is designed, which measures the distance between the\namplified features of the original examples and the features of the adversarial\nexamples as the primary loss, while the output layer prediction serves as the\nauxiliary loss. This guides the perturbation to disrupt the features of the\ntrue class in adversarial examples, effectively enhancing transferability.\nExtensive experiments demonstrate that the adversarial examples generated by\nthe proposed method achieve effective transferability to black-box models on\ntwo public HSI datasets. Furthermore, the method maintains robust attack\nperformance even under defense strategies."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts",
        "author": "Guowei Zhong, Ruohong Huan, Mingzhen Wu, Ronghua Liang, and Peng Chen",
        "link": "http://arxiv.org/abs/2506.10452v1",
        "abstract": "Recent advancements in Multimodal Emotion Recognition (MER) face challenges\nin addressing both modality missing and Out-Of-Distribution (OOD) data\nsimultaneously. Existing methods often rely on specific models or introduce\nexcessive parameters, which limits their practicality. To address these issues,\nwe propose a novel robust MER framework, Causal Inference Distiller (CIDer),\nand introduce a new task, Random Modality Feature Missing (RMFM), to generalize\nthe definition of modality missing. CIDer integrates two key components: a\nModel-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal\nInference (MACI) module. MSSD enhances robustness under the RMFM task through a\nweight-sharing self-distillation approach applied across low-level features,\nattention maps, and high-level representations. Additionally, a Word-level\nSelf-aligned Attention Module (WSAM) reduces computational complexity, while a\nMultimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.\nTo tackle OOD challenges, MACI employs a tailored causal graph to mitigate\nlabel and language biases using a Multimodal Causal Module (MCM) and\nfine-grained counterfactual texts. Notably, MACI can independently enhance OOD\ngeneralization with minimal additional parameters. Furthermore, we also\nintroduce the new repartitioned MER OOD datasets. Experimental results\ndemonstrate that CIDer achieves robust performance in both RMFM and OOD\nscenarios, with fewer parameters and faster training compared to\nstate-of-the-art methods. The implementation of this work is publicly\naccessible at https://github.com/gw-zhong/CIDer."
    },
    {
        "date": "2025-06",
        "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks",
        "author": "Kaiyuan Zhang, Siyuan Cheng, Hanxi Guo, Yuetian Chen, Zian Su, Shengwei An, Yuntao Du, Charles Fleming, Ashish Kundu, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2506.10424v1",
        "abstract": "Large language models (LLMs) have achieved remarkable success and are widely\nadopted for diverse applications. However, fine-tuning these models often\ninvolves private or sensitive information, raising critical privacy concerns.\nIn this work, we conduct the first comprehensive study evaluating the\nvulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our\nempirical analysis demonstrates that MIAs exploit the loss reduction during\nfine-tuning, making them highly effective in revealing membership information.\nThese findings motivate the development of our defense. We propose SOFT\n(\\textbf{S}elective data \\textbf{O}bfuscation in LLM\n\\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates\nprivacy leakage by leveraging influential data selection with an adjustable\nparameter to balance utility preservation and privacy protection. Our extensive\nexperiments span six diverse domains and multiple LLM architectures and scales.\nResults show that SOFT effectively reduces privacy risks while maintaining\ncompetitive model performance, offering a practical and scalable solution to\nsafeguard sensitive information in fine-tuned LLMs."
    },
    {
        "date": "2025-06",
        "title": "Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning",
        "author": "Xue Zhou, Dapeng Man, Chen Xu, Fanyi Zeng, Tao Liu, Huan Wang, Shucheng He, Chaoyang Gao, and Wu Yang",
        "link": "http://arxiv.org/abs/2506.11172v1",
        "abstract": "Offline reinforcement learning (RL) heavily relies on the coverage of\npre-collected data over the target policy's distribution. Existing studies aim\nto improve data-policy coverage to mitigate distributional shifts, but overlook\nsecurity risks from insufficient coverage, and the single-step analysis is not\nconsistent with the multi-step decision-making nature of offline RL. To address\nthis, we introduce the sequence-level concentrability coefficient to quantify\ncoverage, and reveal its exponential amplification on the upper bound of\nestimation errors through theoretical analysis. Building on this, we propose\nthe Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack.\nConsidering the continuous nature of offline RL data, we convert state-action\npairs into decision units, and extract representative decision patterns that\ncapture multi-step behavior. We identify rare patterns likely to cause\ninsufficient coverage, and poison them to reduce coverage and exacerbate\ndistributional shifts. Experiments show that poisoning just 1% of the dataset\ncan degrade agent performance by 90%. This finding provides new perspectives\nfor analyzing and safeguarding the security of offline RL."
    },
    {
        "date": "2025-06",
        "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials",
        "author": "Binyao Guo, Zihan Lin, and QiZhi He",
        "link": "http://arxiv.org/abs/2506.10352v1",
        "abstract": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers."
    },
    {
        "date": "2025-06",
        "title": "Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption",
        "author": "Kwangsu Lee",
        "link": "http://arxiv.org/abs/2506.10338v1",
        "abstract": "Distributed broadcast encryption (DBE) is a specific kind of broadcast\nencryption (BE) where users independently generate their own public and private\nkeys, and a sender can efficiently create a ciphertext for a subset of users by\nusing the public keys of the subset users. Previously proposed DBE schemes have\nbeen proven in the adaptive chosen-plaintext attack (CPA) security model and\nhave the disadvantage of requiring linear number of pairing operations when\nverifying the public key of a user. In this paper, we propose an efficient DBE\nscheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)\nsecurity for the first time. To do this, we first propose a semi-static CCA\nsecure DBE scheme and prove the security under the $q$-Type assumption. Then,\nby modifying the generic transformation of Gentry and Waters that converts a\nsemi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be\napplied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme\nand prove its adaptive CCA security. Our proposed DBE scheme is efficient\nbecause it requires constant size ciphertexts, constant size private keys, and\nlinear size public keys, and the public key verification requires only a\nconstant number of pairing operations and efficient group membership checks."
    },
    {
        "date": "2025-06",
        "title": "Distributionally-Constrained Adversaries in Online Learning",
        "author": "Mo\u00efse Blanchard, and Samory Kpotufe",
        "link": "http://arxiv.org/abs/2506.10293v1",
        "abstract": "There has been much recent interest in understanding the continuum from\nadversarial to stochastic settings in online learning, with various frameworks\nincluding smoothed settings proposed to bridge this gap. We consider the more\ngeneral and flexible framework of distributionally constrained adversaries in\nwhich instances are drawn from distributions chosen by an adversary within some\nconstrained distribution class [RST11]. Compared to smoothed analysis, we\nconsider general distributional classes which allows for a fine-grained\nunderstanding of learning settings between fully stochastic and fully\nadversarial for which a learner can achieve non-trivial regret. We give a\ncharacterization for which distribution classes are learnable in this context\nagainst both oblivious and adaptive adversaries, providing insights into the\ntypes of interplay between the function class and distributional constraints on\nadversaries that enable learnability. In particular, our results recover and\ngeneralize learnability for known smoothed settings. Further, we show that for\nseveral natural function classes including linear classifiers, learning can be\nachieved without any prior knowledge of the distribution class -- in other\nwords, a learner can simultaneously compete against any constrained adversary\nwithin learnable distribution classes."
    },
    {
        "date": "2025-06",
        "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning",
        "author": "Jun Qi, Chao-Han Yang, Pin-Yu Chen, and Min-Hsiu Hsieh",
        "link": "http://arxiv.org/abs/2506.10275v1",
        "abstract": "Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine\nlearning, yet their practical application is hindered by inherent limitations\nsuch as constrained linear expressivity, optimization challenges, and acute\nsensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a\nscalable and robust hybrid quantum-classical architecture designed to overcome\nthese obstacles. By innovatively employing quantum circuits to dynamically\ngenerate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude\nencoding and parameterized quantum operations, VQC-MLPNet substantially expands\nrepresentation capabilities and augments training stability. We provide\nrigorous theoretical guarantees via statistical learning techniques and Neural\nTangent Kernel analysis, explicitly deriving upper bounds on approximation,\nuniform deviation, and optimization errors. These theoretical insights\ndemonstrate exponential improvements in representation capacity relative to\nquantum circuit depth and the number of qubits, providing clear computational\nadvantages over standalone quantum circuits and existing hybrid quantum\narchitectures. Our theoretical claims are empirically corroborated through\nextensive experiments, including classifying semiconductor quantum-dot charge\nstates and predicting genomic transcription factor binding sites, demonstrating\nresilient performance even under realistic IBM quantum noise simulations. This\nresearch establishes a theoretically sound and practically robust framework,\nadvancing the frontiers of quantum-enhanced learning for unconventional\ncomputing paradigms in the Noisy Intermediate-Scale Quantum era and beyond."
    },
    {
        "date": "2025-06",
        "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods",
        "author": "Yeonwoo Jang, Shariqah Hossain, Ashwin Sreevatsa, and Diogo Cruz",
        "link": "http://arxiv.org/abs/2506.10236v1",
        "abstract": "In this work, we show that some machine unlearning methods may fail when\nsubjected to straightforward prompt attacks. We systematically evaluate eight\nunlearning techniques across three model families, and employ output-based,\nlogit-based, and probe analysis to determine to what extent supposedly\nunlearned knowledge can be retrieved. While methods like RMU and TAR\ndemonstrate robust unlearning, ELM remains vulnerable to specific prompt\nattacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).\nOur logit analysis also confirms that unlearned models are generally not hiding\nknowledge by modifying the way the answer is formatted, as the correlation\nbetween output and logit accuracy is strong. These results challenge prevailing\nassumptions about unlearning effectiveness and highlight the need for\nevaluation frameworks that can reliably distinguish between true knowledge\nremoval and superficial output suppression. We also publicly make available our\nevaluation framework to easily evaluate prompting techniques to retrieve\nunlearning knowledge."
    },
    {
        "date": "2025-06",
        "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection",
        "author": "Tina Behrouzi, Sana Tonekaboni, Rahul G. Krishnan, and Anna Goldenberg",
        "link": "http://arxiv.org/abs/2506.10200v1",
        "abstract": "Real-world observational data often contain existing or emerging\nheterogeneous subpopulations that deviate from global patterns. The majority of\nmodels tend to overlook these underrepresented groups, leading to inaccurate or\neven harmful predictions. Existing solutions often rely on detecting these\nsamples as Out-of-domain (OOD) rather than adapting the model to new emerging\npatterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational\nAutoencoder framework that jointly performs representation learning and\nadaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with\nthe data by dynamically updating its latent structure to capture new trends. It\nleverages a novel non-parametric clustering mechanism, inspired by Gaussian\nMixture Models, to discover and model latent subgroups based on embedding\nsimilarity. Extensive experiments show that DynaSubVAE achieves competitive\nperformance in both near-OOD and far-OOD detection, and excels in class-OOD\nscenarios where an entire class is missing during training. We further\nillustrate that our dynamic subgrouping mechanism outperforms standalone\nclustering methods such as GMM and KMeans++ in terms of both OOD accuracy and\nregret precision."
    },
    {
        "date": "2025-06",
        "title": "Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise",
        "author": "Lucas Truax, Sandip Roy, and Laszlo B. Kish",
        "link": "http://arxiv.org/abs/2506.10147v1",
        "abstract": "In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an\napproach to securing satellite communications. KLJN has the potential to\nrevolutionize satellite communication security through its combination of\nsimplicity, cost-effectiveness, and resilience with unconditional security.\nUnlike quantum key distribution (QKD), which requires complex, fragile, and\nexpensive infrastructure like photon detectors and dedicated optical links,\nKLJN operates using standard electronic components and wires, significantly\nreducing implementation costs and logistical hurdles. KLJN's security, grounded\nin the fundamental laws of classical physics, is impervious to environmental\nand radiation-induced noise, making it highly reliable in the harsh conditions\nof satellite communications. This robustness, coupled with its ability to\nintegrate seamlessly with existing infrastructure, positions KLJN as a\nrevolutionary alternative to quantum solutions for ensuring secure, resilient\nsatellite communications. The authors explore the value of achieving\nunconditionally secure communications in strategic ground-to-satellite networks\nwhich address vulnerabilities posed by advanced computational threats,\nincluding quantum computing. Our team has examined two leading approaches to\nunconditional security - the KLJN scheme and QKD - and analyzed the potential\nuse of each for space systems. While QKD leverages quantum mechanics for\nsecurity, it faces challenges related to cost, complexity, and environmental\nsensitivity. In contrast, the KLJN scheme utilizes classical physics principles\nto provide a simpler, more cost-effective, and resilient alternative,\nparticularly for ground-based systems. The study concludes that KLJN offers\nsignificant advantages in simplicity, cost-efficiency, and robustness, making\nit a practical choice for many secure communication applications."
    },
    {
        "date": "2025-06",
        "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving",
        "author": "Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, and Fatih Porikli",
        "link": "http://arxiv.org/abs/2506.10145v1",
        "abstract": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance."
    },
    {
        "date": "2025-06",
        "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy",
        "author": "Sushant Gautam, Michael A. Riegler, and P\u00e5l Halvorsen",
        "link": "http://arxiv.org/abs/2506.09958v1",
        "abstract": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1"
    },
    {
        "date": "2025-06",
        "title": "Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning",
        "author": "Liou Tang, James Joshi, and Ashish Kundu",
        "link": "http://arxiv.org/abs/2506.09923v1",
        "abstract": "Machine Unlearning (MU) aims to update Machine Learning (ML) models following\nrequests to remove training samples and their influences on a trained model\nefficiently without retraining the original ML model from scratch. While MU\nitself has been employed to provide privacy protection and regulatory\ncompliance, it can also increase the attack surface of the model. Existing\nprivacy inference attacks towards MU that aim to infer properties of the\nunlearned set rely on the weaker threat model that assumes the attacker has\naccess to both the unlearned model and the original model, limiting their\nfeasibility toward real-life scenarios. We propose a novel privacy attack, A\nPosteriori Label-Only Membership Inference Attack towards MU, Apollo, that\ninfers whether a data sample has been unlearned, following a strict threat\nmodel where an adversary has access to the label-output of the unlearned model\nonly. We demonstrate that our proposed attack, while requiring less access to\nthe target model compared to previous attacks, can achieve relatively high\nprecision on the membership status of the unlearned samples."
    },
    {
        "date": "2025-06",
        "title": "A look at adversarial attacks on radio waveforms from discrete latent space",
        "author": "Attanasia Garuso, Silvija Kokalj-Filipovic, and Yagna Kaasaragadda",
        "link": "http://arxiv.org/abs/2506.09896v1",
        "abstract": "Having designed a VQVAE that maps digital radio waveforms into discrete\nlatent space, and yields a perfectly classifiable reconstruction of the\noriginal data, we here analyze the attack suppressing properties of VQVAE when\nan adversarial attack is performed on high-SNR radio-frequency (RF)\ndata-points. To target amplitude modulations from a subset of digitally\nmodulated waveform classes, we first create adversarial attacks that preserve\nthe phase between the in-phase and quadrature component whose values are\nadversarially changed. We compare them with adversarial attacks of the same\nintensity where phase is not preserved. We test the classification accuracy of\nsuch adversarial examples on a classifier trained to deliver 100% accuracy on\nthe original data. To assess the ability of VQVAE to suppress the strength of\nthe attack, we evaluate the classifier accuracy on the reconstructions by VQVAE\nof the adversarial datapoints and show that VQVAE substantially decreases the\neffectiveness of the attack. We also compare the I/Q plane diagram of the\nattacked data, their reconstructions and the original data. Finally, using\nmultiple methods and metrics, we compare the probability distribution of the\nVQVAE latent space with and without attack. Varying the attack strength, we\nobserve interesting properties of the discrete space, which may help detect the\nattacks."
    },
    {
        "date": "2025-06",
        "title": "A Weighted Loss Approach to Robust Federated Learning under Data Heterogeneity",
        "author": "Johan Erbani, Sonia Ben Mokhtar, Pierre-Edouard Portier, Elod Egyed-Zsigmond, and Diana Nurbakova",
        "link": "http://arxiv.org/abs/2506.09824v2",
        "abstract": "Federated learning (FL) is a machine learning paradigm that enables multiple\ndata holders to collaboratively train a machine learning model without sharing\ntheir training data with external parties. In this paradigm, workers locally\nupdate a model and share with a central server their updated gradients (or\nmodel parameters). While FL seems appealing from a privacy perspective, it\nopens a number of threats from a security perspective as (Byzantine)\nparticipants can contribute poisonous gradients (or model parameters) harming\nmodel convergence. Byzantine-resilient FL addresses this issue by ensuring that\nthe training proceeds as if Byzantine participants were absent. Towards this\npurpose, common strategies ignore outlier gradients during model aggregation,\nassuming that Byzantine gradients deviate more from honest gradients than\nhonest gradients do from each other. However, in heterogeneous settings, honest\ngradients may differ significantly, making it difficult to distinguish honest\noutliers from Byzantine ones. In this paper, we introduce the Worker Label\nAlignement Loss (WoLA), a weighted loss that aligns honest worker gradients\ndespite data heterogeneity, which facilitates the identification of Byzantines'\ngradients. This approach significantly outperforms state-of-the-art methods in\nheterogeneous settings. In this paper, we provide both theoretical insights and\nempirical evidence of its effectiveness."
    },
    {
        "date": "2025-06",
        "title": "Physical Layer-Based Device Fingerprinting for Wireless Security: From Theory to Practice",
        "author": "Junqing Zhang, Francesco Ardizzon, Mattia Piana, Guanxiong Shen, and Stefano Tomasin",
        "link": "http://arxiv.org/abs/2506.09807v1",
        "abstract": "The identification of the devices from which a message is received is part of\nsecurity mechanisms to ensure authentication in wireless communications.\nConventional authentication approaches are cryptography-based, which, however,\nare usually computationally expensive and not adequate in the Internet of\nThings (IoT), where devices tend to be low-cost and with limited resources.\nThis paper provides a comprehensive survey of physical layer-based device\nfingerprinting, which is an emerging device authentication for wireless\nsecurity. In particular, this article focuses on hardware impairment-based\nidentity authentication and channel features-based authentication. They are\npassive techniques that are readily applicable to legacy IoT devices. Their\nintrinsic hardware and channel features, algorithm design methodologies,\napplication scenarios, and key research questions are extensively reviewed\nhere. The remaining research challenges are discussed, and future work is\nsuggested that can further enhance the physical layer-based device\nfingerprinting."
    },
    {
        "date": "2025-06",
        "title": "Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols",
        "author": "Longzhu He, Chaozhuo Li, Peng Tang, Litian Zhang, and Sen Su",
        "link": "http://arxiv.org/abs/2506.09803v1",
        "abstract": "Graph neural networks (GNNs) have achieved significant success in graph\nrepresentation learning and have been applied to various domains. However, many\nreal-world graphs contain sensitive personal information, such as user profiles\nin social networks, raising serious privacy concerns when graph learning is\nperformed using GNNs. To address this issue, locally private graph learning\nprotocols have gained considerable attention. These protocols leverage the\nprivacy advantages of local differential privacy (LDP) and the effectiveness of\nGNN's message-passing in calibrating noisy data, offering strict privacy\nguarantees for users' local data while maintaining high utility (e.g., node\nclassification accuracy) for graph learning. Despite these advantages, such\nprotocols may be vulnerable to data poisoning attacks, a threat that has not\nbeen considered in previous research. Identifying and addressing these threats\nis crucial for ensuring the robustness and security of privacy-preserving graph\nlearning frameworks. This work introduces the first data poisoning attack\ntargeting locally private graph learning protocols. The attacker injects fake\nusers into the protocol, manipulates these fake users to establish links with\ngenuine users, and sends carefully crafted data to the server, ultimately\ncompromising the utility of private graph learning. The effectiveness of the\nattack is demonstrated both theoretically and empirically. In addition, several\ndefense strategies have also been explored, but their limited effectiveness\nhighlights the need for more robust defenses."
    },
    {
        "date": "2025-06",
        "title": "Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization",
        "author": "Pierre Vernimmen, and Fran\u00e7ois Glineur",
        "link": "http://arxiv.org/abs/2506.09730v2",
        "abstract": "This work assesses both empirically and theoretically, using the performance\nestimation methodology, how robust different first-order optimization methods\nare when subject to relative inexactness in their gradient computations.\nRelative inexactness occurs, for example, when compressing the gradient using\nfewer bits of information, which happens when dealing with large-scale problems\non GPUs. Three major families of methods are analyzed: constant step gradient\ndescent, long-step methods, and accelerated methods. The latter two are first\nshown to be theoretically not robust to inexactness. Then, a semi-heuristic\nshortening factor is introduced to improve their theoretical guarantees. All\nmethods are subsequently tested on a concrete inexact problem, with two\ndifferent types of relative inexactness, and it is observed that both\naccelerated methods are much more robust than expected, and that the shortening\nfactor significantly helps the long-step methods. In the end, all shortened\nmethods appear to be promising, even in this inexact setting."
    },
    {
        "date": "2025-06",
        "title": "On the Virtues of Information Security in the UK Climate Movement",
        "author": "Mikaela Brough, Rikke Bjerg Jensen, and Martin R. Albrecht",
        "link": "http://arxiv.org/abs/2506.09719v1",
        "abstract": "We report on an ethnographic study with members of the climate movement in\nthe United Kingdom (UK). We conducted participant observation and interviews at\nprotests and in various activist settings. Reporting on the findings as they\nrelate to information security, we show that members of the UK climate movement\nwrestled with (i) a fundamental tension between openness and secrecy; (ii)\ntensions between autonomy and collective interdependence in\ninformation-security decision-making; (iii) conflicting activist ideals that\nshape security discourses; and (iv) pressures from different social gazes --\nfrom each other, from people outside the movement and from their adversaries.\nOverall, our findings shed light on the social complexities of\ninformation-security research in activist settings and provoke methodological\nquestions about programmes that aim to design for activists."
    },
    {
        "date": "2025-06",
        "title": "Evasion Attacks Against Bayesian Predictive Models",
        "author": "Pablo G. Arce, Roi Naveiro, and David R\u00edos Insua",
        "link": "http://arxiv.org/abs/2506.09640v1",
        "abstract": "There is an increasing interest in analyzing the behavior of machine learning\nsystems against adversarial attacks. However, most of the research in\nadversarial machine learning has focused on studying weaknesses against evasion\nor poisoning attacks to predictive models in classical setups, with the\nsusceptibility of Bayesian predictive models to attacks remaining\nunderexplored. This paper introduces a general methodology for designing\noptimal evasion attacks against such models. We investigate two adversarial\nobjectives: perturbing specific point predictions and altering the entire\nposterior predictive distribution. For both scenarios, we propose novel\ngradient-based attacks and study their implementation and properties in various\ncomputational setups."
    },
    {
        "date": "2025-06",
        "title": "The Everyday Security of Living with Conflict",
        "author": "Jessica McClearn, Reem Talhouk, and Rikke Bjerg Jensen",
        "link": "http://arxiv.org/abs/2506.09580v1",
        "abstract": "When `cyber' is used as a prefix, attention is typically drawn to the\ntechnological and spectacular aspects of war and conflict -- and, by extension,\nsecurity. We offer a different approach to engaging with and understanding\nsecurity in such contexts, by foregrounding the everyday -- mundane --\nexperiences of security within communities living with and fleeing from war. We\ndo so through three vignettes from our field research in Colombia, Lebanon and\nSweden, respectively, and by highlighting the significance of ethnography for\nsecurity research with communities living in regions afflicted by war. We\nconclude by setting out a call to action for security researchers and\npractitioners to consider such lived experiences in the design of security\ntechnology that aims to cater to the needs of communities in `global conflict\nand disaster regions'."
    },
    {
        "date": "2025-06",
        "title": "TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning",
        "author": "Songze Li, Mingxuan Zhang, Kang Wei, and Shouling Ji",
        "link": "http://arxiv.org/abs/2506.09562v2",
        "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success in a wide\nrange of sequential decision-making domains, including robotics, healthcare,\nsmart grids, and finance. Recent research demonstrates that attackers can\nefficiently exploit system vulnerabilities during the training phase to execute\nbackdoor attacks, producing malicious actions when specific trigger patterns\nare present in the state observations. However, most existing backdoor attacks\nrely primarily on simplistic and heuristic trigger configurations, overlooking\nthe potential efficacy of trigger optimization. To address this gap, we\nintroduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor\nAttacks on DRL), the first framework to systematically optimize DRL backdoor\ntriggers along three critical axes, i.e., temporal, spatial, and magnitude.\nSpecifically, we first introduce a performance-aware adaptive freezing\nmechanism for injection timing. Then, we formulate dimension selection as a\ncooperative game, utilizing Shapley value analysis to identify the most\ninfluential state variable for the injection dimension. Furthermore, we propose\na gradient-based adversarial procedure to optimize the injection magnitude\nunder environment constraints. Evaluations on three mainstream DRL algorithms\nand nine benchmark tasks show that TooBadRL significantly improves attack\nsuccess rates, while ensuring minimal degradation of normal task performance.\nThese results highlight the previously underappreciated importance of\nprincipled trigger optimization in DRL backdoor attacks. The source code of\nTooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL."
    },
    {
        "date": "2025-06",
        "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches",
        "author": "Wenjun Ji, Yuxiang Fu, Luyang Ying, Deng-Ping Fan, Yuyi Wang, Ming-Ming Cheng, Ivor Tsang, and Qing Guo",
        "link": "http://arxiv.org/abs/2506.09538v1",
        "abstract": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents."
    },
    {
        "date": "2025-06",
        "title": "The Security Overview and Analysis of 3GPP 5G MAC CE",
        "author": "Jin Cao, Yuanyuan Yang, Ruhui Ma, Sheng Li, and Hui Li",
        "link": "http://arxiv.org/abs/2506.09502v2",
        "abstract": "To more effectively control and allocate network resources, MAC CE has been\nintroduced into the network protocol, which is a type of control signaling\nlocated in the MAC layer. Since MAC CE lacks encryption and integrity\nprotection mechanisms provided by PDCP, the control signaling carried by MAC CE\nis vulnerable to interception or tampering by attackers during resource\nscheduling and allocation. Currently, the 3GPP has analyzed the security risks\nof Layer 1/Layer 2 Triggered Mobility (LTM), where handover signaling sent to\nthe UE via MAC CE by the network can lead to privacy leaks and network attacks.\nHowever, in addition to LTM, there may be other potential security\nvulnerabilities in other protocol procedures. Therefore, this paper explores\nthe security threats to MAC CE and the corresponding protection mechanisms. The\nresearch is expected to support the 3GPP's study of MAC CE and be integrated\nwith the security research of lower-layer protocols, thereby enhancing the\nsecurity and reliability of the entire communication system."
    },
    {
        "date": "2025-06",
        "title": "LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge",
        "author": "Songze Li, Chuokun Xu, Jiaying Wang, Xueluan Gong, Chen Chen, Jirui Zhang, Jun Wang, Kwok-Yan Lam, and Shouling Ji",
        "link": "http://arxiv.org/abs/2506.09443v1",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable intelligence across\nvarious tasks, which has inspired the development and widespread adoption of\nLLM-as-a-Judge systems for automated model testing, such as red teaming and\nbenchmarking. However, these systems are susceptible to adversarial attacks\nthat can manipulate evaluation outcomes, raising concerns about their\nrobustness and, consequently, their trustworthiness. Existing evaluation\nmethods adopted by LLM-based judges are often piecemeal and lack a unified\nframework for comprehensive assessment. Furthermore, prompt template and model\nselections for improving judge robustness have been rarely explored, and their\nperformance in real-world settings remains largely unverified. To address these\ngaps, we introduce RobustJudge, a fully automated and scalable framework\ndesigned to systematically evaluate the robustness of LLM-as-a-Judge systems.\nRobustJudge investigates the impact of attack methods and defense strategies\n(RQ1), explores the influence of prompt template and model selection (RQ2), and\nassesses the robustness of real-world LLM-as-a-Judge applications (RQ3).Our\nmain findings are: (1) LLM-as-a-Judge systems are still vulnerable to a range\nof adversarial attacks, including Combined Attack and PAIR, while defense\nmechanisms such as Re-tokenization and LLM-based Detectors offer improved\nprotection; (2) Robustness is highly sensitive to the choice of prompt template\nand judge models. Our proposed prompt template optimization method can improve\nrobustness, and JudgeLM-13B demonstrates strong performance as a robust\nopen-source judge; (3) Applying RobustJudge to Alibaba's PAI platform reveals\npreviously unreported vulnerabilities. The source code of RobustJudge is\nprovided at https://github.com/S3IC-Lab/RobustJudge."
    },
    {
        "date": "2025-06",
        "title": "Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity",
        "author": "Haoxiang Ye, Tao Sun, and Qing Ling",
        "link": "http://arxiv.org/abs/2506.09438v1",
        "abstract": "Decentralized learning, which facilitates joint model training across\ngeographically scattered agents, has gained significant attention in the field\nof signal and information processing in recent years. While the optimization\nerrors of decentralized learning algorithms have been extensively studied,\ntheir generalization errors remain relatively under-explored. As the\ngeneralization errors reflect the scalability of trained models on unseen data\nand are crucial in determining the performance of trained models in real-world\napplications, understanding the generalization errors of decentralized learning\nis of paramount importance. In this paper, we present fine-grained\ngeneralization error analysis for both attack-free and Byzantine-resilient\ndecentralized learning with heterogeneous data as well as under mild\nassumptions, in contrast to prior studies that consider homogeneous data and/or\nrely on a stringent bounded stochastic gradient assumption. Our results shed\nlight on the impact of data heterogeneity, model initialization and stochastic\ngradient noise -- factors that have not been closely investigated before -- on\nthe generalization error of decentralized learning. We also reveal that\nByzantine attacks performed by malicious agents largely affect the\ngeneralization error, and their negative impact is inherently linked to the\ndata heterogeneity while remaining independent on the sample size. Numerical\nexperiments on both convex and non-convex tasks are conducted to validate our\ntheoretical findings."
    },
    {
        "date": "2025-06",
        "title": "Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G",
        "author": "Ryan Barker, and Fatemeh Afghah",
        "link": "http://arxiv.org/abs/2506.09418v1",
        "abstract": "The advent of Open Radio Access Networks (O-RAN) introduces modularity and\nflexibility into 5G deployments but also surfaces novel security challenges\nacross disaggregated interfaces. This literature review synthesizes recent\nresearch across thirteen academic and industry sources, examining\nvulnerabilities such as cipher bidding-down attacks, partial encryption\nexposure on control/user planes, and performance trade-offs in securing O-RAN\ninterfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,\nAES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,\nand adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is\nplaced on emerging testbeds and AI-driven controllers that facilitate dynamic\norchestration, anomaly detection, and secure configuration. We conclude by\noutlining future research directions, including hardware offloading,\ncross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN\nAlliance security mandates, all of which point toward the need for integrated,\nzero-trust architectures in 6G."
    },
    {
        "date": "2025-06",
        "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models",
        "author": "Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, and Shun-Feng Su",
        "link": "http://arxiv.org/abs/2506.09408v1",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications."
    },
    {
        "date": "2025-06",
        "title": "ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)",
        "author": "Ronal Singh, Mohan Baruwal Chhetri, Surya Nepal, and Cecile Paris",
        "link": "http://arxiv.org/abs/2506.09365v1",
        "abstract": "Modern Security Operations Centres (SOCs) integrate diverse tools, such as\nSIEM, IDS, and XDR systems, offering rich contextual data, including alert\nenrichments, flow features, and similar case histories. Yet, analysts must\nstill manually determine which of these contextual cues are most relevant when\nvalidating specific alerts. We introduce ContextBuddy, an AI assistant that\nlearns from analysts' prior investigations to help them identify the most\nrelevant context for new alerts. Rather than providing enrichments,\nContextBuddy models how analysts have previously selected context and suggests\ntailored cues based on the characteristics of each alert. We formulate context\nselection as a sequential decision-making problem and apply imitation learning\n(IL) to capture analysts' strategies, evaluating multiple IL approaches.\nThrough staged evaluation, we validate ContextBuddy using two intrusion\ndetection datasets (HIKARI-2021, UNSW-NB15). In simulation-based experiments,\nContextBuddy helped simulated reinforcement learning analysts improve\nclassification accuracy (p < 0.001) (increasing F1 by 2.5% for HIKARI and 9%\nfor UNSW), reducing false negatives (1.5% for HIKARI and 10% for UNSW), and\nkeeping false positives below 1%. Decision confidence among agents also\nimproved by 2-3% (p < 0.001). In a within-subject user study (N=13; power =\n0.8), non-experts using ContextBuddy improved classification accuracy by 21.1%\n(p = 0.008) and reduced alert validation time by 24% (p = 0.01). These results\ndemonstrate that by learning context-selection patterns from analysts,\nContextBuddy can yield notable improvements in investigation effectiveness and\nefficiency."
    },
    {
        "date": "2025-06",
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
        "author": "Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang",
        "link": "http://arxiv.org/abs/2506.09350v1",
        "abstract": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
    },
    {
        "date": "2025-06",
        "title": "Adversarial Surrogate Risk Bounds for Binary Classification",
        "author": "Natalie S. Frank",
        "link": "http://arxiv.org/abs/2506.09348v1",
        "abstract": "A central concern in classification is the vulnerability of machine learning\nmodels to adversarial attacks. Adversarial training is one of the most popular\ntechniques for training robust classifiers, which involves minimizing an\nadversarial surrogate risk. Recent work characterized when a minimizing\nsequence of an adversarial surrogate risk is also a minimizing sequence of the\nadversarial classification risk for binary classification -- a property known\nas adversarial consistency. However, these results do not address the rate at\nwhich the adversarial classification risk converges to its optimal value for\nsuch a sequence of functions that minimize the adversarial surrogate. This\npaper provides surrogate risk bounds that quantify that convergence rate.\nAdditionally, we derive distribution-dependent surrogate risk bounds in the\nstandard (non-adversarial) learning setting, that may be of independent\ninterest."
    },
    {
        "date": "2025-06",
        "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies",
        "author": "Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2506.09237v1",
        "abstract": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard ."
    },
    {
        "date": "2025-06",
        "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule",
        "author": "Boyu Jiang, Liang Shi, Zhengzhi Lin, Loren Stowe, and Feng Guo",
        "link": "http://arxiv.org/abs/2506.09217v1",
        "abstract": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python."
    },
    {
        "date": "2025-06",
        "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs",
        "author": "Greyson Brothers",
        "link": "http://arxiv.org/abs/2506.09215v1",
        "abstract": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks."
    },
    {
        "date": "2025-06",
        "title": "Adversarial Text Generation with Dynamic Contextual Perturbation",
        "author": "Hetvi Waghela, Jaydip Sen, Sneha Rakshit, and Subhasis Dasgupta",
        "link": "http://arxiv.org/abs/2506.09148v1",
        "abstract": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Deep Reinforcement Learning against Environmental State Perturbation",
        "author": "Chenxu Wang, and Huaping Liu",
        "link": "http://arxiv.org/abs/2506.08961v1",
        "abstract": "Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have\nbeen widely studied in various threat models; however, few consider\nenvironmental state perturbations, which are natural in embodied scenarios. To\nimprove the robustness of DRL agents, we formulate the problem of environmental\nstate perturbation, introducing a preliminary non-targeted attack method as a\ncalibration adversary, and then propose a defense framework, named Boosted\nAdversarial Training (BAT), which first tunes the agents via supervised\nlearning to avoid catastrophic failure and subsequently adversarially trains\nthe agent with reinforcement learning. Extensive experimental results\nsubstantiate the vulnerability of mainstream agents under environmental state\nperturbations and the effectiveness of our proposed attack. The defense results\ndemonstrate that while existing robust reinforcement learning algorithms may\nnot be suitable, our BAT framework can significantly enhance the robustness of\nagents against environmental state perturbations across various situations."
    },
    {
        "date": "2025-06",
        "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)",
        "author": "Danush Khanna, Krishna Kumar, Basab Ghosh, Vinija Jain, Vasu Sharma, Aman Chadha, and Amitava Das",
        "link": "http://arxiv.org/abs/2506.08885v2",
        "abstract": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md."
    },
    {
        "date": "2025-06",
        "title": "SmartAttack: Air-Gap Attack via Smartwatches",
        "author": "Mordechai Guri",
        "link": "http://arxiv.org/abs/2506.08866v1",
        "abstract": "Air-gapped systems are considered highly secure against data leaks due to\ntheir physical isolation from external networks. Despite this protection,\nultrasonic communication has been demonstrated as an effective method for\nexfiltrating data from such systems. While smartphones have been extensively\nstudied in the context of ultrasonic covert channels, smartwatches remain an\nunderexplored yet effective attack vector.\n  In this paper, we propose and evaluate SmartAttack, a novel method that\nleverages smartwatches as receivers for ultrasonic covert communication in\nair-gapped environments. Our approach utilizes the built-in microphones of\nsmartwatches to capture covert signals in real time within the ultrasonic\nfrequency range of 18-22 kHz. Through experimental validation, we assess the\nfeasibility of this attack under varying environmental conditions, distances,\norientations, and noise levels. Furthermore, we analyze smartwatch-specific\nfactors that influence ultrasonic covert channels, including their continuous\npresence on the user's wrist, the impact of the human body on signal\npropagation, and the directional constraints of built-in microphones. Our\nfindings highlight the security risks posed by smartwatches in high-security\nenvironments and outline mitigation strategies to counteract this emerging\nthreat."
    },
    {
        "date": "2025-06",
        "title": "Design Patterns for Securing LLM Agents against Prompt Injections",
        "author": "Luca Beurer-Kellner, Beat Buesser Ana-Maria Cre\u0163u, Edoardo Debenedetti, Daniel Dobos, Daniel Fabian, Marc Fischer, David Froelicher, Kathrin Grosse, Daniel Naeff, Ezinwanne Ozoani, Andrew Paverd, Florian Tram\u00e8r, and V\u00e1clav Volhejn",
        "link": "http://arxiv.org/abs/2506.08837v2",
        "abstract": "As AI agents powered by Large Language Models (LLMs) become increasingly\nversatile and capable of addressing a broad spectrum of tasks, ensuring their\nsecurity has become a critical challenge. Among the most pressing threats are\nprompt injection attacks, which exploit the agent's resilience on natural\nlanguage inputs -- an especially dangerous threat when agents are granted tool\naccess or handle sensitive information. In this work, we propose a set of\nprincipled design patterns for building AI agents with provable resistance to\nprompt injection. We systematically analyze these patterns, discuss their\ntrade-offs in terms of utility and security, and illustrate their real-world\napplicability through a series of case studies."
    },
    {
        "date": "2025-06",
        "title": "Lightweight Electronic Signatures and Reliable Access Control Included in Sensor Networks to Prevent Cyber Attacks from Modifying Patient Data",
        "author": "Mishall Al-Zubaidie",
        "link": "http://arxiv.org/abs/2506.08828v1",
        "abstract": "Digital terrorism is a major cause of securing patient/healthcare providers\ndata and information. Sensitive topics that may have an impact on a patient's\nhealth or even national security include patient health records and information\non healthcare providers. Health databases and data sets have been continually\nbreached by many, regular assaults, as well as local and remote servers\nequipped with wireless sensor networks (WSNs) in diverse locations. The problem\nwas addressed by some contemporary strategies that were created to stop these\nassaults and guarantee the privacy of patient data and information transferred\nand gathered by sensors. Nevertheless, the literature analysis outlines many\nindications of weakness that persist in these methods. This study suggests a\nnovel, reliable method that bolsters the information security and data gathered\nby sensors and kept on base station datasets. The proposed approach combines a\nnumber of security mechanisms, including symmetric cryptography for encryption,\nasymmetric cryptography for access control and signatures, and the Lesamnta-LW\nmethod in the signature process. Users' information is shielded from prying\neyes by the careful application of these measures and a sound approach.\nInvestigational comparisons, security studies, and thorough results show that\nthe suggested method is better than earlier methods."
    },
    {
        "date": "2025-06",
        "title": "Lightweight and High-Throughput Secure Logging for Internet of Things and Cold Cloud Continuum",
        "author": "Saif E. Nouma, and Attila A. Yavuz",
        "link": "http://arxiv.org/abs/2506.08781v1",
        "abstract": "The growing deployment of resource-limited Internet of Things (IoT) devices\nand their expanding attack surfaces demand efficient and scalable security\nmechanisms. System logs are vital for the trust and auditability of IoT, and\noffloading their maintenance to a Cold Storage-as-a-Service (Cold-STaaS)\nenhances cost-effectiveness and reliability. However, existing cryptographic\nlogging solutions either burden low-end IoT devices with heavy computation or\ncreate verification delays and storage inefficiencies at Cold-STaaS. There is a\npressing need for cryptographic primitives that balance security, performance,\nand scalability across IoT-Cold-STaaS continuum.\n  In this work, we present Parallel Optimal Signatures for Secure Logging\n(POSLO), a novel digital signature framework that, to our knowledge, is the\nfirst to offer constant-size signatures and public keys, near-optimal signing\nefficiency, and tunable fine-to-coarse-grained verification for log auditing.\nPOSLO achieves these properties through efficient randomness management,\nflexible aggregation, and multiple algorithmic instantiations. It also\nintroduces a GPU-accelerated batch verification framework that exploits\nhomomorphic signature aggregation to deliver ultra-fast performance. For\nexample, POSLO can verify 231 log entries per second on a mid-range consumer\nGPU (NVIDIA GTX 3060) while being significantly more compact than\nstate-of-the-art. POSLO also preserves signer-side efficiency, offering\nsubstantial battery savings for IoT devices, and is well-suited for the\nIoT-Cold-STaaS ecosystem."
    },
    {
        "date": "2025-06",
        "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification",
        "author": "Matthias Beckmann, Robert Beinert, and Jonas Bresch",
        "link": "http://arxiv.org/abs/2506.08761v1",
        "abstract": "The Radon cumulative distribution transform (R-CDT), is an easy-to-compute\nfeature extractor that facilitates image classification tasks especially in the\nsmall data regime. It is closely related to the sliced Wasserstein distance and\nprovably guaranties the linear separability of image classes that emerge from\ntranslations or scalings. In many real-world applications, like the recognition\nof watermarks in filigranology, however, the data is subject to general affine\ntransformations originating from the measurement process. To overcome this\nissue, we recently introduced the so-called max-normalized R-CDT that only\nrequires elementary operations and guaranties the separability under arbitrary\naffine transformations. The aim of this paper is to continue our study of the\nmax-normalized R-CDT especially with respect to its robustness against\nnon-affine image deformations. Our sensitivity analysis shows that its\nseparability properties are stable provided the Wasserstein-infinity distance\nbetween the samples can be controlled. Since the Wasserstein-infinity distance\nonly allows small local image deformations, we moreover introduce a\nmean-normalized version of the R-CDT. In this case, robustness relates to the\nWasserstein-2 distance and also covers image deformations caused by impulsive\nnoise for instance. Our theoretical results are supported by numerical\nexperiments showing the effectiveness of our novel feature extractors as well\nas their robustness against local non-affine deformations and impulsive noise."
    },
    {
        "date": "2025-06",
        "title": "Towards Secure and Private Language Models for Nuclear Power Plants",
        "author": "Muhammad Anwar, Mishca de Costa, Issam Hammad, and Daniel Lau",
        "link": "http://arxiv.org/abs/2506.08746v1",
        "abstract": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain."
    },
    {
        "date": "2025-06",
        "title": "On the Ethics of Using LLMs for Offensive Security",
        "author": "Andreas Happe, and J\u00fcrgen Cito",
        "link": "http://arxiv.org/abs/2506.08693v1",
        "abstract": "Large Language Models (LLMs) have rapidly evolved over the past few years and\nare currently evaluated for their efficacy within the domain of offensive\ncyber-security. While initial forays showcase the potential of LLMs to enhance\nsecurity research, they also raise critical ethical concerns regarding the\ndual-use of offensive security tooling.\n  This paper analyzes a set of papers that leverage LLMs for offensive\nsecurity, focusing on how ethical considerations are expressed and justified in\ntheir work. The goal is to assess the culture of AI in offensive security\nresearch regarding ethics communication, highlighting trends, best practices,\nand gaps in current discourse.\n  We provide insights into how the academic community navigates the fine line\nbetween innovation and ethical responsibility. Particularly, our results show\nthat 13 of 15 reviewed prototypes (86.6\\%) mentioned ethical considerations and\nare thus aware of the potential dual-use of their research. Main motivation\ngiven for the research was allowing broader access to penetration-testing as\nwell as preparing defenders for AI-guided attackers."
    },
    {
        "date": "2025-06",
        "title": "Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness",
        "author": "Jinkwan Jang, Hyungjin Park, Jinmyeong Choi, and Taesup Kim",
        "link": "http://arxiv.org/abs/2506.08660v1",
        "abstract": "Real-world time series data are inherently multivariate, often exhibiting\ncomplex inter-channel dependencies. Each channel is typically sampled at its\nown period and is prone to missing values due to various practical and\noperational constraints. These characteristics pose fundamental challenges\nrelated to channel dependency, sampling asynchrony, and missingness, all of\nwhich must be addressed to enable robust and reliable forecasting in practical\nsettings. However, most existing architectures are built on oversimplified\nassumptions, such as identical sampling periods across channels and fully\nobserved inputs at test time, which often do not hold in real-world scenarios.\nTo bridge this gap, we propose ChannelTokenFormer, a Transformer-based\nforecasting model with a flexible architecture designed to explicitly capture\ncross-channel interactions, accommodate channel-wise asynchronous sampling, and\neffectively handle missing values. Extensive experiments on three benchmark\ndatasets modified to reflect practical settings, along with one real-world\nindustrial dataset, demonstrate the superior robustness and accuracy of\nChannelTokenFormer under challenging real-world conditions."
    },
    {
        "date": "2025-06",
        "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation",
        "author": "Shiji Zhao, Chi Chen, Ranjie Duan, Xizhe Wang, and Xingxing Wei",
        "link": "http://arxiv.org/abs/2506.08611v1",
        "abstract": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness."
    },
    {
        "date": "2025-06",
        "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement",
        "author": "Xinyue Niu, and Akira Furui",
        "link": "http://arxiv.org/abs/2506.08555v1",
        "abstract": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."
    },
    {
        "date": "2025-06",
        "title": "Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)",
        "author": "Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, and Andreas Ebert",
        "link": "http://arxiv.org/abs/2506.08533v1",
        "abstract": "This paper introduces Evolutionary Multi-Objective Network Architecture\nSearch (EMNAS) for the first time to optimize neural network architectures in\nlarge-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses\ngenetic algorithms to automate network design, tailored to enhance rewards and\nreduce model size without compromising performance. Additionally,\nparallelization techniques are employed to accelerate the search, and\nteacher-student methodologies are implemented to ensure scalable optimization.\nThis research underscores the potential of transfer learning as a robust\nframework for optimizing performance across iterative learning processes by\neffectively leveraging knowledge from earlier generations to enhance learning\nefficiency and stability in subsequent generations. Experimental results\ndemonstrate that tailored EMNAS outperforms manually designed models, achieving\nhigher rewards with fewer parameters. The findings of these strategies\ncontribute positively to EMNAS for RL in autonomous driving, advancing the\nfield toward better-performing networks suitable for real-world scenarios."
    },
    {
        "date": "2025-06",
        "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer",
        "author": "Zhongtao Tian, Wenhao Huang, Zhidong Chen, and Xiao Wei Sun",
        "link": "http://arxiv.org/abs/2506.08526v1",
        "abstract": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments."
    },
    {
        "date": "2025-06",
        "title": "DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training",
        "author": "Jacob Piland, Chris Sweet, and Adam Czakja",
        "link": "http://arxiv.org/abs/2506.08514v1",
        "abstract": "Class Activation Mapping (CAM) and its gradient-based variants (e.g.,\nGradCAM) have become standard tools for explaining Convolutional Neural Network\n(CNN) predictions. However, these approaches typically focus on individual\nlogits, while for neural networks using softmax, the class membership\nprobability estimates depend \\textit{only} on the \\textit{differences} between\nlogits, not on their absolute values. This disconnect leaves standard CAMs\nvulnerable to adversarial manipulation, such as passive fooling, where a model\nis trained to produce misleading CAMs without affecting decision performance.\nWe introduce \\textbf{Salience-Hoax Activation Maps (SHAMs)}, an\n\\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM\nrobustness under adversarial conditions. To address the passive fooling\nvulnerability, we then propose \\textbf{DiffGradCAM}, a novel, lightweight, and\ncontrastive approach to class activation mapping that is both non-suceptible to\npassive fooling, but also matches the output of standard CAM methods such as\nGradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a\nnew framework for probing and improving the robustness of saliency-based\nexplanations. We validate both contributions across multi-class tasks with few\nand many classes."
    },
    {
        "date": "2025-06",
        "title": "Secure Data Access in Cloud Environments Using Quantum Cryptography",
        "author": "S. Vasavi Venkata Lakshmi, and Ziaul Haque Choudhury",
        "link": "http://arxiv.org/abs/2506.10028v1",
        "abstract": "Cloud computing has made storing and accessing data easier but keeping it\nsecure is a big challenge nowadays. Traditional methods of ensuring data may\nnot be strong enough in the future when powerful quantum computers become\navailable. To solve this problem, this study uses quantum cryptography to\nprotect data in the cloud environment. Quantum Key Distribution (QKD) creates\nsecure keys by sending information using quantum particles like photons.\nSpecifically, we use the BB84 protocol, a simple and reliable way to make\nsecure keys that cannot be stolen without detection. To protect the data, we\nuse the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the\ndata stays completely private. This study shows how these Quantum methods can\nbe applied in cloud systems to provide a strong defense against hackers, even\nif they have access to quantum computers. The combination of QKD, BB84, and\nQOTP creates a safe and reliable way to keep data secure when it is stored or\nshared in the cloud. Using quantum cryptography, this paper provides a way to\nensure data security now and in the future, making cloud computing safer for\neveryone to store their data securely and safely."
    },
    {
        "date": "2025-06",
        "title": "One Patch to Rule Them All: Transforming Static Patches into Dynamic Attacks in the Physical World",
        "author": "Xingshuo Han, Chen Ling, Shiyi Yao, Haozhao Wang, Hangcheng Liu, Yutong Wu, Shengmin Xu, Changhai Ou, Xinyi Huang, and Tianwei Zhang",
        "link": "http://arxiv.org/abs/2506.08482v1",
        "abstract": "Numerous methods have been proposed to generate physical adversarial patches\n(PAPs) against real-world machine learning systems. However, each existing PAP\ntypically supports only a single, fixed attack goal, and switching to a\ndifferent objective requires re-generating and re-deploying a new PAP. This\nrigidity limits their practicality in dynamic environments like autonomous\ndriving, where traffic conditions and attack goals can change rapidly. For\nexample, if no obstacles are present around the target vehicle, the attack may\nfail to cause meaningful consequences.\n  To overcome this limitation, we propose SwitchPatch, a novel PAP that is\nstatic yet enables dynamic and controllable attack outcomes based on real-time\nscenarios. Attackers can alter pre-defined conditions, e.g., by projecting\ndifferent natural-color lights onto SwitchPatch to seamlessly switch between\nattack goals. Unlike prior work, SwitchPatch does not require re-generation or\nre-deployment for different objectives, significantly reducing cost and\ncomplexity. Furthermore, SwitchPatch remains benign when the enabling\nconditions are absent, enhancing its stealth.\n  We evaluate SwitchPatch on two key tasks: traffic sign recognition\n(classification and detection) and depth estimation. First, we conduct\ntheoretical analysis and empirical studies to demonstrate the feasibility of\nSwitchPatch and explore how many goals it can support using techniques like\ncolor light projection and occlusion. Second, we perform simulation-based\nexperiments and ablation studies to verify its effectiveness and\ntransferability. Third, we conduct outdoor tests using a Unmanned Ground\nVehicle (UGV) to confirm its robustness in the physical world. Overall,\nSwitchPatch introduces a flexible and practical adversarial strategy that can\nbe adapted to diverse tasks and real-world conditions."
    },
    {
        "date": "2025-06",
        "title": "The interplay of robustness and generalization in quantum machine learning",
        "author": "Julian Berberich, Tobias Fellner, and Christian Holm",
        "link": "http://arxiv.org/abs/2506.08455v1",
        "abstract": "While adversarial robustness and generalization have individually received\nsubstantial attention in the recent literature on quantum machine learning,\ntheir interplay is much less explored. In this chapter, we address this\ninterplay for variational quantum models, which were recently proposed as\nfunction approximators in supervised learning. We discuss recent results\nquantifying both robustness and generalization via Lipschitz bounds, which\nexplicitly depend on model parameters. Thus, they give rise to a\nregularization-based training approach for robust and generalizable quantum\nmodels, highlighting the importance of trainable data encoding strategies. The\npractical implications of the theoretical results are demonstrated with an\napplication to time series analysis."
    },
    {
        "date": "2025-06",
        "title": "GPS Spoofing Attacks on AI-based Navigation Systems with Obstacle Avoidance in UAV",
        "author": "Ji Hyuk Jung, Mi Yeon Hong, and Ji Won Yoon",
        "link": "http://arxiv.org/abs/2506.08445v1",
        "abstract": "Recently, approaches using Deep Reinforcement Learning (DRL) have been\nproposed to solve UAV navigation systems in complex and unknown environments.\nHowever, despite extensive research and attention, systematic studies on\nvarious security aspects have not yet been conducted. Therefore, in this paper,\nwe conduct research on security vulnerabilities in DRL-based navigation\nsystems, particularly focusing on GPS spoofing attacks against the system. Many\nrecent basic DRL-based navigation systems fundamentally share an efficient\nstructure. This paper presents an attack model that operates through GPS\nspoofing attacks briefly modeling the range of spoofing attack against EKF\nsensor fusion of PX4 autopilot, and combine this with the DRL-based system to\ndesign attack scenarios that are closer to reality. Finally, this paper\nexperimentally demonstrated that attacks are possible both in the basic DRL\nsystem and in attack models combining the DRL system with PX4 autopilot system."
    },
    {
        "date": "2025-06",
        "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings",
        "author": "Mingyuan Fan, Fuyi Wang, Cen Chen, and Jianying Zhou",
        "link": "http://arxiv.org/abs/2506.08435v1",
        "abstract": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods."
    },
    {
        "date": "2025-06",
        "title": "Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems",
        "author": "Runze Li, Di Jin, Xiaobao Wang, Dongxiao He, Bingdao Feng, and Zhen Wang",
        "link": "http://arxiv.org/abs/2506.08401v1",
        "abstract": "Graph recommendation systems have been widely studied due to their ability to\neffectively capture the complex interactions between users and items. However,\nthese systems also exhibit certain vulnerabilities when faced with attacks. The\nprevailing shilling attack methods typically manipulate recommendation results\nby injecting a large number of fake nodes and edges. However, such attack\nstrategies face two primary challenges: low stealth and high destructiveness.\nTo address these challenges, this paper proposes a novel graph backdoor attack\nmethod that aims to enhance the exposure of target items to the target user in\na covert manner, without affecting other unrelated nodes. Specifically, we\ndesign a single-node trigger generator, which can effectively expose multiple\ntarget items to the target user by inserting only one fake user node.\nAdditionally, we introduce constraint conditions between the target nodes and\nirrelevant nodes to mitigate the impact of fake nodes on the recommendation\nsystem's performance. Experimental results show that the exposure of the target\nitems reaches no less than 50% in 99% of the target users, while the impact on\nthe recommendation system's performance is controlled within approximately 5%."
    },
    {
        "date": "2025-06",
        "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR",
        "author": "Wei-Ping Huang, Guan-Ting Lin, and Hung-yi Lee",
        "link": "http://arxiv.org/abs/2506.11121v1",
        "abstract": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains."
    },
    {
        "date": "2025-06",
        "title": "AlphaFold Database Debiasing for Robust Inverse Folding",
        "author": "Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Siyuan Li, Yufei Huang, and Stan Z. Li",
        "link": "http://arxiv.org/abs/2506.08365v1",
        "abstract": "The AlphaFold Protein Structure Database (AFDB) offers unparalleled\nstructural coverage at near-experimental accuracy, positioning it as a valuable\nresource for data-driven protein design. However, its direct use in training\ndeep models that are sensitive to fine-grained atomic geometry, such as inverse\nfolding, exposes a critical limitation. Comparative analysis of structural\nfeature distributions reveals that AFDB structures exhibit distinct statistical\nregularities, reflecting a systematic geometric bias that deviates from the\nconformational diversity found in experimentally determined structures from the\nProtein Data Bank (PDB). While AFDB structures are cleaner and more idealized,\nPDB structures capture the intrinsic variability and physical realism essential\nfor generalization in downstream tasks. To address this discrepancy, we\nintroduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct\nnative-like conformations from intentionally corrupted backbone geometries. By\ntraining the model to recover plausible structural states, DeSAE implicitly\ncaptures a more robust and natural structural manifold. At inference, applying\nDeSAE to AFDB structures produces debiased structures that significantly\nimprove inverse folding performance across multiple benchmarks. This work\nhighlights the critical impact of subtle systematic biases in predicted\nstructures and presents a principled framework for debiasing, significantly\nboosting the performance of structure-based learning tasks like inverse\nfolding."
    },
    {
        "date": "2025-06",
        "title": "SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models",
        "author": "Wenhan Yao, Fen Xiao, Xiarun Chen, Jia Liu, YongQiang He, and Weiping Wen",
        "link": "http://arxiv.org/abs/2506.08346v1",
        "abstract": "Deep speech classification tasks, including keyword spotting and speaker\nverification, are vital in speech-based human-computer interaction. Recently,\nthe security of these technologies has been revealed to be susceptible to\nbackdoor attacks. Specifically, attackers use noisy disruption triggers and\nspeech element triggers to produce poisoned speech samples that train models to\nbecome vulnerable. However, these methods typically create only a limited\nnumber of backdoors due to the inherent constraints of the trigger function. In\nthis paper, we propose that speech backdoor attacks can strategically focus on\nspeech elements such as timbre and emotion, leveraging the Speech Large\nLanguage Model (SLLM) to generate diverse triggers. Increasing the number of\ntriggers may disproportionately elevate the poisoning rate, resulting in higher\nattack costs and a lower success rate per trigger. We introduce the Multiple\nGradient Descent Algorithm (MGDA) as a mitigation strategy to address this\nchallenge. The proposed attack is called the Speech Prompt Backdoor Attack\n(SPBA). Building on this foundation, we conducted attack experiments on two\nspeech classification tasks, demonstrating that SPBA shows significant trigger\neffectiveness and achieves exceptional performance in attack metrics."
    },
    {
        "date": "2025-06",
        "title": "Your Agent Can Defend Itself against Backdoor Attacks",
        "author": "Li Changjiang, Liang Jiacheng, Cao Bochuan, Chen Jinghui, and Wang Ting",
        "link": "http://arxiv.org/abs/2506.08336v2",
        "abstract": "Despite their growing adoption across domains, large language model\n(LLM)-powered agents face significant security risks from backdoor attacks\nduring training and fine-tuning. These compromised agents can subsequently be\nmanipulated to execute malicious operations when presented with specific\ntriggers in their inputs or environments. To address this pressing risk, we\npresent ReAgent, a novel defense against a range of backdoor attacks on\nLLM-based agents. Intuitively, backdoor attacks often result in inconsistencies\namong the user's instruction, the agent's planning, and its execution. Drawing\non this insight, ReAgent employs a two-level approach to detect potential\nbackdoors. At the execution level, ReAgent verifies consistency between the\nagent's thoughts and actions; at the planning level, ReAgent leverages the\nagent's capability to reconstruct the instruction based on its thought\ntrajectory, checking for consistency between the reconstructed instruction and\nthe user's instruction. Extensive evaluation demonstrates ReAgent's\neffectiveness against various backdoor attacks across tasks. For instance,\nReAgent reduces the attack success rate by up to 90\\% in database operation\ntasks, outperforming existing defenses by large margins. This work reveals the\npotential of utilizing compromised agents themselves to mitigate backdoor\nrisks."
    },
    {
        "date": "2025-06",
        "title": "SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense",
        "author": "Patryk Krukowski, \u0141ukasz Gorczyca, Piotr Helm, Kamil Ksi\u0105\u017cek, and Przemys\u0142aw Spurek",
        "link": "http://arxiv.org/abs/2506.08255v1",
        "abstract": "Traditional deep neural networks suffer from several limitations, including\ncatastrophic forgetting. When models are adapted to new datasets, they tend to\nquickly forget previously learned knowledge. Another significant issue is the\nlack of robustness to even small perturbations in the input data. In practice,\nwe can often easily perform adversarial attacks and change the network's\npredictions, adding minimal noise to the input. Dedicated architectures and\ntraining procedures can solve each of the above problems separately.\nUnfortunately, currently, no model can simultaneously address both catastrophic\nforgetting and vulnerability to adversarial attacks. We introduce SHIELD\n(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel\napproach that integrates a hypernetwork-based continual learning approach with\ninterval arithmetic. SHIELD use the hypernetwork to transfer trainable task\nembedding vectors into the weights of a target model dedicated to specific\ndata. This paradigm allows for the dynamic generation of separate networks for\neach subtask, while the hypernetwork aggregates and analyzes information across\nall tasks. The target model takes in the input a data sample with a defined\ninterval range, and by creating a hypercube, produces a prediction for the\ngiven range. Therefore, such target models provide strict guarantees against\nall possible attacks for data samples within the interval range. Our approach\nenhances security without sacrificing network adaptability, addressing the\noverlooked challenge of safety in continual learning."
    },
    {
        "date": "2025-06",
        "title": "PoSyn: Secure Power Side-Channel Aware Synthesis",
        "author": "Amisha Srivastava, Samit S. Miftah, Hyunmin Kim, Debjit Pal, and Kanad Basu",
        "link": "http://arxiv.org/abs/2506.08252v1",
        "abstract": "Power Side-Channel (PSC) attacks exploit power consumption patterns to\nextract sensitive information, posing risks to cryptographic operations crucial\nfor secure systems. Traditional countermeasures, such as masking, face\nchallenges including complex integration during synthesis, substantial area\noverhead, and susceptibility to optimization removal during logic synthesis. To\naddress these issues, we introduce PoSyn, a novel logic synthesis framework\ndesigned to enhance cryptographic hardware resistance against PSC attacks. Our\nmethod centers on optimal bipartite mapping of vulnerable RTL components to\nstandard cells from the technology library, aiming to minimize PSC leakage. By\nutilizing a cost function integrating critical characteristics from both the\nRTL design and the standard cell library, we strategically modify mapping\ncriteria during RTL-to-netlist conversion without altering design\nfunctionality. Furthermore, we theoretically establish that PoSyn minimizes\nmutual information leakage, strengthening its security against PSC\nvulnerabilities. We evaluate PoSyn across various cryptographic hardware\nimplementations, including AES, RSA, PRESENT, and post-quantum cryptographic\nalgorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,\nand 15nm. Experimental results demonstrate a substantial reduction in success\nrates for Differential Power Analysis (DPA) and Correlation Power Analysis\n(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further\nconfirms that synthesized netlists exhibit negligible leakage. Additionally,\ncompared to conventional countermeasures like masking and shuffling, PoSyn\nsignificantly lowers attack success rates, achieving reductions of up to 72%,\nwhile simultaneously enhancing area efficiency by as much as 3.79 times."
    },
    {
        "date": "2025-06",
        "title": "Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers",
        "author": "Yuanhaur Chang, Oren Heller, Yaniv Shlomo, Iddo Bar-Noy, Ella Bokobza, Michal Grinstein-Weiss, and Ning Zhang",
        "link": "http://arxiv.org/abs/2506.10025v1",
        "abstract": "Key decision-makers in small and medium businesses (SMBs) often lack the\nawareness and knowledge to implement cybersecurity measures effectively. To\ngain a deeper understanding of how SMB executives navigate cybersecurity\ndecision-making, we deployed a mixed-method approach, conducting\nsemi-structured interviews (n=21) and online surveys (n=322) with SMB key\ndecision-makers. Using thematic analysis, we revealed SMB decision-makers'\nperceived risks in terms of the digital assets they valued, and found reasons\nfor their choice of defense measures and factors impacting security perception.\nWe employed the situational awareness model to characterize decision-makers\nbased on cybersecurity awareness, identifying those who have comparatively low\nawareness in the fight against adversaries. We further explored the\nrelationship between awareness and business attributes, and constructed a\nholistic structural equation model to understand how awareness can be improved.\nFinally, we proposed interventions to help SMBs overcome potential challenges."
    },
    {
        "date": "2025-06",
        "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models",
        "author": "Elena Sofia Ruzzetti, Giancarlo A. Xompero, Davide Venditti, and Fabio Massimo Zanzotto",
        "link": "http://arxiv.org/abs/2506.10024v1",
        "abstract": "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero."
    },
    {
        "date": "2025-06",
        "title": "Exposing Hidden Backdoors in NFT Smart Contracts: A Static Security Analysis of Rug Pull Patterns",
        "author": "Chetan Pathade, and Shweta Hooli",
        "link": "http://arxiv.org/abs/2506.07974v1",
        "abstract": "The explosive growth of Non-Fungible Tokens (NFTs) has revolutionized digital\nownership by enabling the creation, exchange, and monetization of unique assets\non blockchain networks. However, this surge in popularity has also given rise\nto a disturbing trend: the emergence of rug pulls - fraudulent schemes where\ndevelopers exploit trust and smart contract privileges to drain user funds or\ninvalidate asset ownership. Central to many of these scams are hidden backdoors\nembedded within NFT smart contracts. Unlike unintentional bugs, these backdoors\nare deliberately coded and often obfuscated to bypass traditional audits and\nexploit investor confidence. In this paper, we present a large-scale static\nanalysis of 49,940 verified NFT smart contracts using Slither, a static\nanalysis framework, to uncover latent vulnerabilities commonly linked to rug\npulls. We introduce a custom risk scoring model that classifies contracts into\nhigh, medium, or low risk tiers based on the presence and severity of rug pull\nindicators. Our dataset was derived from verified contracts on the Ethereum\nmainnet, and we generate multiple visualizations to highlight red flag\nclusters, issue prevalence, and co-occurrence of critical vulnerabilities.\nWhile we do not perform live exploits, our results reveal how malicious\npatterns often missed by simple reviews can be surfaced through static analysis\nat scale. We conclude by offering mitigation strategies for developers,\nmarketplaces, and auditors to enhance smart contract security. By exposing how\nhidden backdoors manifest in real-world smart contracts, this work contributes\na practical foundation for detecting and mitigating NFT rug pulls through\nscalable automated analysis."
    },
    {
        "date": "2025-06",
        "title": "Secure Distributed Learning for CAVs: Defending Against Gradient Leakage with Leveled Homomorphic Encryption",
        "author": "Muhammad Ali Najjar, Ren-Yi Huang, Dumindu Samaraweera, and Prashant Shekhar",
        "link": "http://arxiv.org/abs/2506.07894v1",
        "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning in domains like Connected and\nAutonomous Vehicles (CAVs). However, recent studies have shown that exchanged\nmodel gradients remain susceptible to inference attacks such as Deep Leakage\nfrom Gradients (DLG), which can reconstruct private training data. While\nexisting defenses like Differential Privacy (DP) and Secure Multi-Party\nComputation (SMPC) offer protection, they often compromise model accuracy. To\nthat end, Homomorphic Encryption (HE) offers a promising alternative by\nenabling lossless computation directly on encrypted data, thereby preserving\nboth privacy and model utility. However, HE introduces significant\ncomputational and communication overhead, which can hinder its practical\nadoption. To address this, we systematically evaluate various leveled HE\nschemes to identify the most suitable for FL in resource-constrained\nenvironments due to its ability to support fixed-depth computations without\nrequiring costly bootstrapping. Our contributions in this paper include a\ncomprehensive evaluation of HE schemes for real-world FL applications, a\nselective encryption strategy that targets only the most sensitive gradients to\nminimize computational overhead, and the development of a full HE-based FL\npipeline that effectively mitigates DLG attacks while preserving model\naccuracy. We open-source our implementation to encourage reproducibility and\nfacilitate adoption in safety-critical domains."
    },
    {
        "date": "2025-06",
        "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark",
        "author": "Rui Wen, Yiyong Liu, Michael Backes, and Yang Zhang",
        "link": "http://arxiv.org/abs/2506.07888v1",
        "abstract": "Data reconstruction attacks, which aim to recover the training dataset of a\ntarget model with limited access, have gained increasing attention in recent\nyears. However, there is currently no consensus on a formal definition of data\nreconstruction attacks or appropriate evaluation metrics for measuring their\nquality. This lack of rigorous definitions and universal metrics has hindered\nfurther advancement in this field. In this paper, we address this issue in the\nvision domain by proposing a unified attack taxonomy and formal definitions of\ndata reconstruction attacks. We first propose a set of quantitative evaluation\nmetrics that consider important criteria such as quantifiability, consistency,\nprecision, and diversity. Additionally, we leverage large language models\n(LLMs) as a substitute for human judgment, enabling visual evaluation with an\nemphasis on high-quality reconstructions. Using our proposed taxonomy and\nmetrics, we present a unified framework for systematically evaluating the\nstrengths and limitations of existing attacks and establishing a benchmark for\nfuture research. Empirical results, primarily from a memorization perspective,\nnot only validate the effectiveness of our metrics but also offer valuable\ninsights for designing new attacks."
    },
    {
        "date": "2025-06",
        "title": "Securing Unbounded Differential Privacy Against Timing Attacks",
        "author": "Zachary Ratliff, and Salil Vadhan",
        "link": "http://arxiv.org/abs/2506.07868v1",
        "abstract": "Recent works have started to theoretically investigate how we can protect\ndifferentially private programs against timing attacks, by making the joint\ndistribution the output and the runtime differentially private (JOT-DP).\nHowever, the existing approaches to JOT-DP have some limitations, particularly\nin the setting of unbounded DP (which protects the size of the dataset and\napplies to arbitrarily large datasets). First, the known conversion of pure DP\nprograms to pure JOT-DP programs in the unbounded setting (a) incurs a constant\nadditive increase in error probability (and thus does not provide vanishing\nerror as $n\\to\\infty$) (b) produces JOT-DP programs that fail to preserve the\ncomputational efficiency of the original pure DP program and (c) is analyzed in\na toy computational model in which the runtime is defined to be the number of\ncoin flips. In this work, we overcome these limitations. Specifically, we show\nthat the error required for pure JOT-DP in the unbounded setting depends on the\nmodel of computation. In a randomized RAM model where the dataset size $n$ is\ngiven (or can be computed in constant time) and we can generate random numbers\n(not just random bits) in constant time, polynomially small error probability\nis necessary and sufficient. If $n$ is not given or we only have a random-bit\ngenerator, an (arbitrarily small) constant error probability is necessary and\nsufficient. The aforementioned positive results are proven by efficient\nprocedures to convert any pure JOT-DP program $P$ in the upper-bounded setting\nto a pure JOT-DP program $P'$ in the unbounded setting, such that the output\ndistribution of $P'$ is $\\gamma$-close in total variation distance to that of\n$P$, where $\\gamma$ is either an arbitrarily small constant or polynomially\nsmall, depending on the model of computation."
    },
    {
        "date": "2025-06",
        "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks",
        "author": "Silvia Lucia Sanna, Diego Soi, Davide Maiorca, and Giorgio Giacinto",
        "link": "http://arxiv.org/abs/2506.07836v1",
        "abstract": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is\ngrowing exponentially because it facilitates remote monitoring, predictive\nmaintenance, and data-driven decision making, especially in the healthcare and\nindustrial sectors. However, IoT devices remain vulnerable due to their\nresource constraints and difficulty in applying security patches. Consequently,\nvarious cybersecurity attacks are reported daily, such as Denial of Service,\nparticularly in IoT-driven solutions. Most attack detection methodologies are\nbased on Machine Learning (ML) techniques, which can detect attack patterns.\nHowever, the focus is more on identification rather than considering the impact\nof ML algorithms on computational resources. This paper proposes a green\nmethodology to identify IoT malware networking attacks based on flow\nprivacy-preserving statistical features. In particular, the hyperparameters of\nthree tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are\noptimized based on energy consumption and test-time performance in terms of\nMatthew's Correlation Coefficient. Our results show that models maintain high\nperformance and detection accuracy while consistently reducing power usage in\nterms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion\nDetection Systems are suitable for IoT and other resource-constrained devices."
    },
    {
        "date": "2025-06",
        "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability",
        "author": "Jie Bao, Chuangyin Dang, Rui Luo, Hanwei Zhang, and Zhixin Zhou",
        "link": "http://arxiv.org/abs/2506.07804v1",
        "abstract": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction."
    },
    {
        "date": "2025-06",
        "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards",
        "author": "Jingnan Zheng, Xiangtian Ji, Yijun Lu, Chenhang Cui, Weixiang Zhao, Gelei Deng, Zhenkai Liang, An Zhang, and Tat-Seng Chua",
        "link": "http://arxiv.org/abs/2506.07736v2",
        "abstract": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements."
    },
    {
        "date": "2025-06",
        "title": "\"I wasn't sure if this is indeed a security risk\": Data-driven Understanding of Security Issue Reporting in GitHub Repositories of Open Source npm Packages",
        "author": "Rajdeep Ghosh, Shiladitya De, and Mainack Mondal",
        "link": "http://arxiv.org/abs/2506.07728v1",
        "abstract": "The npm (Node Package Manager) ecosystem is the most important package\nmanager for JavaScript development with millions of users. Consequently, a\nplethora of earlier work investigated how vulnerability reporting, patch\npropagation, and in general detection as well as resolution of security issues\nin such ecosystems can be facilitated. However, understanding the ground\nreality of security-related issue reporting by users (and bots) in npm-along\nwith the associated challenges has been relatively less explored at scale.\n  In this work, we bridge this gap by collecting 10,907,467 issues reported\nacross GitHub repositories of 45,466 diverse npm packages. We found that the\ntags associated with these issues indicate the existence of only 0.13%\nsecurity-related issues. However, our approach of manual analysis followed by\ndeveloping high accuracy machine learning models identify 1,617,738\nsecurity-related issues which are not tagged as security-related (14.8% of all\nissues) as well as 4,461,934 comments made on these issues. We found that the\nbots which are in wide use today might not be sufficient for either detecting\nor offering assistance. Furthermore, our analysis of user-developer interaction\ndata hints that many user-reported security issues might not be addressed by\ndevelopers-they are not tagged as security-related issues and might be closed\nwithout valid justification. Consequently, a correlation analysis hints that\nthe developers quickly handle security issues with known solutions (e.g.,\ncorresponding to CVE). However, security issues without such known solutions\n(even with reproducible code) might not be resolved. Our findings offer\nactionable insights for improving security management in open-source\necosystems, highlighting the need for smarter tools and better collaboration.\nThe data and code for this work is available at\nhttps://doi.org/10.5281/zenodo.15614029"
    },
    {
        "date": "2025-06",
        "title": "Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation",
        "author": "Boris Martirosyan, and Alexey Karmanov",
        "link": "http://arxiv.org/abs/2506.07706v1",
        "abstract": "Latent diffusion models (LDMs) achieve state-of-the-art performance across\nvarious tasks, including image generation and video synthesis. However, they\ngenerally lack robustness, a limitation that remains not fully explored in\ncurrent research. In this paper, we propose several methods to address this\ngap. First, we hypothesize that the robustness of LDMs primarily should be\nmeasured without their text encoder, because if we take and explore the whole\narchitecture, the problems of image generator and text encoders wll be fused.\nSecond, we introduce novel data augmentation techniques designed to reveal\nrobustness shortcomings in LDMs when processing diverse textual prompts. We\nthen fine-tune Stable Diffusion 3 and Stable Diffusion XL models using\nDreambooth, incorporating these proposed augmentation methods across multiple\ntasks. Finally, we propose a novel evaluation pipeline specifically tailored to\nassess the robustness of LDMs fine-tuned via Dreambooth."
    },
    {
        "date": "2025-06",
        "title": "ProARD: progressive adversarial robustness distillation: provide wide range of robust students",
        "author": "Seyedhamidreza Mousavi, Seyedali Mousavi, and Masoud Daneshtalab",
        "link": "http://arxiv.org/abs/2506.07666v1",
        "abstract": "Adversarial Robustness Distillation (ARD) has emerged as an effective method\nto enhance the robustness of lightweight deep neural networks against\nadversarial attacks. Current ARD approaches have leveraged a large robust\nteacher network to train one robust lightweight student. However, due to the\ndiverse range of edge devices and resource constraints, current approaches\nrequire training a new student network from scratch to meet specific\nconstraints, leading to substantial computational costs and increased CO2\nemissions. This paper proposes Progressive Adversarial Robustness Distillation\n(ProARD), enabling the efficient one-time training of a dynamic network that\nsupports a diverse range of accurate and robust student networks without\nrequiring retraining. We first make a dynamic deep neural network based on\ndynamic layers by encompassing variations in width, depth, and expansion in\neach design stage to support a wide range of architectures. Then, we consider\nthe student network with the largest size as the dynamic teacher network.\nProARD trains this dynamic network using a weight-sharing mechanism to jointly\noptimize the dynamic teacher network and its internal student networks.\nHowever, due to the high computational cost of calculating exact gradients for\nall the students within the dynamic network, a sampling mechanism is required\nto select a subset of students. We show that random student sampling in each\niteration fails to produce accurate and robust students."
    },
    {
        "date": "2025-06",
        "title": "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems",
        "author": "Marco Di Gennaro, Giovanni De Lucia, Stefano Longari, Stefano Zanero, and Michele Carminati",
        "link": "http://arxiv.org/abs/2506.07605v2",
        "abstract": "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design."
    },
    {
        "date": "2025-06",
        "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts",
        "author": "Torsten Krau\u00df, Hamid Dashtbani, and Alexandra Dmitrienko",
        "link": "http://arxiv.org/abs/2506.07596v1",
        "abstract": "Machine learning is advancing rapidly, with applications bringing notable\nbenefits, such as improvements in translation and code generation. Models like\nChatGPT, powered by Large Language Models (LLMs), are increasingly integrated\ninto daily life. However, alongside these benefits, LLMs also introduce social\nrisks. Malicious users can exploit LLMs by submitting harmful prompts, such as\nrequesting instructions for illegal activities. To mitigate this, models often\ninclude a security mechanism that automatically rejects such harmful prompts.\nHowever, they can be bypassed through LLM jailbreaks. Current jailbreaks often\nrequire significant manual effort, high computational costs, or result in\nexcessive model modifications that may degrade regular utility.\n  We introduce TwinBreak, an innovative safety alignment removal method.\nBuilding on the idea that the safety mechanism operates like an embedded\nbackdoor, TwinBreak identifies and prunes parameters responsible for this\nfunctionality. By focusing on the most relevant model layers, TwinBreak\nperforms fine-grained analysis of parameters essential to model utility and\nsafety. TwinBreak is the first method to analyze intermediate outputs from\nprompts with high structural and content similarity to isolate safety\nparameters. We present the TwinPrompt dataset containing 100 such twin prompts.\nExperiments confirm TwinBreak's effectiveness, achieving 89% to 98% success\nrates with minimal computational requirements across 16 LLMs from five vendors."
    },
    {
        "date": "2025-06",
        "title": "HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model",
        "author": "Yuling Wang, Zihui Chen, Pengfei Jiao, and Xiao Wang",
        "link": "http://arxiv.org/abs/2506.07428v1",
        "abstract": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the\nneed for tailored attacks to assess their robustness and ensure security.\nHowever, existing HGNN attacks often require complex retraining of parameters\nto generate specific perturbations for new scenarios. Recently, foundation\nmodels have opened new horizons for the generalization of graph neural networks\nby capturing shared semantics across various graph distributions. This leads us\nto ask:Can we design a foundation attack model for HGNNs that enables\ngeneralizable perturbations across different HGNNs, and quickly adapts to new\nheterogeneous graphs (HGs)? Empirical findings reveal that, despite significant\ndifferences in model design and parameter space, different HGNNs surprisingly\nshare common vulnerability patterns from a relation-aware perspective.\nTherefore, we explore how to design foundation HGNN attack criteria by mining\nshared attack units. In this paper, we propose a novel relation-wise\nheterogeneous graph foundation attack model, HeTa. We introduce a foundation\nsurrogate model to align heterogeneity and identify the importance of shared\nrelation-aware attack units. Building on this, we implement a serialized\nrelation-by-relation attack based on the identified relational weights. In this\nway, the perturbation can be transferred to various target HGNNs and easily\nfine-tuned for new HGs. Extensive experiments exhibit powerful attack\nperformances and generalizability of our method."
    },
    {
        "date": "2025-06",
        "title": "A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing",
        "author": "Sabbir M. Saleh, Nazim Madhavji, and John Steinbacher",
        "link": "http://arxiv.org/abs/2506.08055v1",
        "abstract": "As cloud environments become widespread, cybersecurity has emerged as a top\npriority across areas such as networks, communication, data privacy, response\ntimes, and availability. Various sectors, including industries, healthcare, and\ngovernment, have recently faced cyberattacks targeting their computing systems.\nEnsuring secure app deployment in cloud environments requires substantial\neffort. With the growing interest in cloud security, conducting a systematic\nliterature review (SLR) is critical to identifying research gaps. Continuous\nSoftware Engineering, which includes continuous integration (CI), delivery\n(CDE), and deployment (CD), is essential for software development and\ndeployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,\nand challenges related to the security of CI/CD in the cloud. We addressed key\naspects of cloud security and CI/CD and reported on tools such as Harbor,\nSonarQube, and GitHub Actions. Challenges such as image manipulation,\nunauthorised access, and weak authentication were highlighted. The review also\nuncovered research gaps in how tools and practices address these security\nissues in CI/CD pipelines, revealing a need for further study to improve\ncloud-based security solutions."
    },
    {
        "date": "2025-06",
        "title": "Pixel-Sensitive and Robust Steganography Based on Polar Codes",
        "author": "Yujun Ji, Jinsheng Li, Ling Liu, Qi Cao, and Tao Dai",
        "link": "http://arxiv.org/abs/2506.07404v1",
        "abstract": "Steganography is an information hiding technique for covert communication.\nThe core issue in steganography design is the rate-distortion coding problem.\nPolar codes, which have been proven to achieve the rate-distortion bound for\nany binary symmetric source, are utilized to design a steganographic scheme\nthat can reach the embedding capacity for the Distortion-Limited Sender problem\nin certain cases. In adaptive steganography, for attack scenarios where each\nnoise element can have different intensities, existing steganographic coding\nmethods fail to resist such attacks. In this paper, we propose a\npixel-sensitive and robust steganographic scheme based on polar codes. Our\nsteganographic scheme not only matches the adaptive distortion well but is also\nrobust against sophisticated noise attacks. Futher, it is proven that our\nscheme achieves the embedding capacity in certain cases. Experimentally, a\nsteganographic scheme can be designed and implemented with a secret message\nerror rate at the $10^{-5}$ level when the attack noise is known to both the\nsender and the receiver. This demonstrates its significant robustness."
    },
    {
        "date": "2025-06",
        "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures",
        "author": "Yukai Zhou, Sibei Yang, and Wenjie Wang",
        "link": "http://arxiv.org/abs/2506.07402v1",
        "abstract": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms."
    },
    {
        "date": "2025-06",
        "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems",
        "author": "Peiru Yang, Jinhua Yin, Haoran Zheng, Xueying Bai, Huili Wang, Yufei Sun, Xintian Li, Shangguang Wang, Yongfeng Huang, and Tao Qi",
        "link": "http://arxiv.org/abs/2506.07399v1",
        "abstract": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses."
    },
    {
        "date": "2025-06",
        "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks",
        "author": "Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen, Tian Qin, and Yuyu Zhao",
        "link": "http://arxiv.org/abs/2506.07392v1",
        "abstract": "The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide\nrange of mission-critical applications, but also exposes UAV networks to severe\nDenial-of-Service (DoS) threats due to their open wireless environment, dynamic\ntopology, and resource constraints. Traditional static or centralized defense\nmechanisms are often inadequate for such dynamic and distributed scenarios. To\naddress these challenges, we propose a novel federated multi-agent deep\nreinforcement learning (FMADRL)-driven moving target defense (MTD) framework\nfor proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,\nwe design three lightweight and coordinated MTD mechanisms, including leader\nswitching, route mutation, and frequency hopping, that leverage the inherent\nflexibility of UAV swarms to disrupt attacker efforts and enhance network\nresilience. The defense problem is formulated as a multi-agent partially\nobservable Markov decision process (POMDP), capturing the distributed,\nresource-constrained, and uncertain nature of UAV swarms under attack. Each UAV\nis equipped with a local policy agent that autonomously selects MTD actions\nbased on partial observations and local experiences. By employing a policy\ngradient-based FMADRL algorithm, UAVs collaboratively optimize their defense\npolicies via reward-weighted aggregation, enabling distributed learning without\nsharing raw data and thus reducing communication overhead. Extensive\nsimulations demonstrate that our approach significantly outperforms\nstate-of-the-art baselines, achieving up to a 34.6% improvement in attack\nmitigation rate, a reduction in average recovery time of up to 94.6%, and\ndecreases in energy consumption and defense cost by as much as 29.3% and 98.3%,\nrespectively, while maintaining robust mission continuity under various DoS\nattack strategies."
    },
    {
        "date": "2025-06",
        "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security",
        "author": "Yash Datta, and Sharath Rajasekar",
        "link": "http://arxiv.org/abs/2506.07330v1",
        "abstract": "We present JavelinGuard, a suite of low-cost, high-performance model\narchitectures designed for detecting malicious intent in Large Language Model\n(LLM) interactions, optimized specifically for production deployment. Recent\nadvances in transformer architectures, including compact BERT(Devlin et al.\n2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build\nhighly accurate classifiers with as few as approximately 400M parameters that\nachieve rapid inference speeds even on standard CPU hardware. We systematically\nexplore five progressively sophisticated transformer-based architectures:\nSharanga (baseline transformer classifier), Mahendra (enhanced\nattention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid\nneural ensemble architectures), and Raudra (an advanced multi-task framework\nwith specialized loss functions). Our models are rigorously benchmarked across\nnine diverse adversarial datasets, including popular sets like the NotInject\nseries, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly\nintroduced JavelinBench, specifically crafted to test generalization on\nchallenging borderline and hard-negative cases. Additionally, we compare our\narchitectures against leading open-source guardrail models as well as large\ndecoder-only LLMs such as gpt-4o, demonstrating superior cost-performance\ntrade-offs in terms of accuracy, and latency. Our findings reveal that while\nRaudra's multi-task design offers the most robust performance overall, each\narchitecture presents unique trade-offs in speed, interpretability, and\nresource requirements, guiding practitioners in selecting the optimal balance\nof complexity and efficiency for real-world LLM security applications."
    },
    {
        "date": "2025-06",
        "title": "SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code Generation with Agentic Workflows",
        "author": "Rebecca Saul, Hao Wang, Koushik Sen, and David Wagner",
        "link": "http://arxiv.org/abs/2506.07313v1",
        "abstract": "Large language models (LLMs) have seen widespread success in code generation\ntasks for different scenarios, both everyday and professional. However current\nLLMs, despite producing functional code, do not prioritize security and may\ngenerate code with exploitable vulnerabilities. In this work, we propose\ntechniques for generating code that is more likely to be secure and introduce\nSCGAgent, a proactive secure coding agent that implements our techniques. We\nuse security coding guidelines that articulate safe programming practices,\ncombined with LLM-generated unit tests to preserve functional correctness. In\nour evaluation, we find that SCGAgent is able to preserve nearly 98% of the\nfunctionality of the base Sonnet-3.7 LLM while achieving an approximately 25%\nimprovement in security. Moreover, SCGAgent is able to match or best the\nperformance of sophisticated reasoning LLMs using a non-reasoning model and an\nagentic workflow."
    },
    {
        "date": "2025-06",
        "title": "Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling",
        "author": "Hans Buehler, Blanka Horvath, Yannick Limmer, and Thorsten Schmidt",
        "link": "http://arxiv.org/abs/2506.07299v1",
        "abstract": "This paper addresses the challenge of model uncertainty in quantitative\nfinance, where decisions in portfolio allocation, derivative pricing, and risk\nmanagement rely on estimating stochastic models from limited data. In practice,\nthe unavailability of the true probability measure forces reliance on an\nempirical approximation, and even small misestimations can lead to significant\ndeviations in decision quality. Building on the framework of Klibanoff et al.\n(2005), we enhance the conventional objective - whether this is expected\nutility in an investing context or a hedging metric - by superimposing an outer\n\"uncertainty measure\", motivated by traditional monetary risk measures, on the\nspace of models. In scenarios where a natural model distribution is lacking or\nBayesian methods are impractical, we propose an ad hoc subsampling strategy,\nanalogous to bootstrapping in statistical finance and related to mini-batch\nsampling in deep learning, to approximate model uncertainty. To address the\nquadratic memory demands of naive implementations, we also present an adapted\nstochastic gradient descent algorithm that enables efficient parallelization.\nThrough analytical, simulated, and empirical studies - including multi-period,\nreal data and high-dimensional examples - we demonstrate that uncertainty\nmeasures outperform traditional mixture of measures strategies and our\nmodel-agnostic subsampling-based approach not only enhances robustness against\nmodel risk but also achieves performance comparable to more elaborate Bayesian\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Exploiting Inaccurate Branch History in Side-Channel Attacks",
        "author": "Yuhui Zhu, and Alessandro Biondi",
        "link": "http://arxiv.org/abs/2506.07263v1",
        "abstract": "Modern out-of-order CPUs heavily rely on speculative execution for\nperformance optimization, with branch prediction serving as a cornerstone to\nminimize stalls and maximize efficiency. Whenever shared branch prediction\nresources lack proper isolation and sanitization methods, they may originate\nsecurity vulnerabilities that expose sensitive data across different software\ncontexts.\n  This paper examines the fundamental components of modern Branch Prediction\nUnits (BPUs) and investigates how resource sharing and contention affect two\nwidely implemented but underdocumented features: Bias-Free Branch Prediction\nand Branch History Speculation. Our analysis demonstrates that these BPU\nfeatures, while designed to enhance speculative execution efficiency through\nmore accurate branch histories, can also introduce significant security risks.\nWe show that these features can inadvertently modify the Branch History Buffer\n(BHB) update behavior and create new primitives that trigger malicious\nmis-speculations.\n  This discovery exposes previously unknown cross-privilege attack surfaces for\nBranch History Injection (BHI). Based on these findings, we present three novel\nattack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and\na cross-privilege control flow side-channel attack called BiasScope. Our\nresearch identifies corresponding patterns of vulnerable control flows and\ndemonstrates exploitation on multiple processors. Finally, Chimera is\npresented: an attack demonstrator based on eBPF for a variant of Spectre-BHS\nthat is capable of leaking kernel memory contents at 24,628 bit/s."
    },
    {
        "date": "2025-06",
        "title": "Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models",
        "author": "Ngoc-Quan Pham, Tuan Truong, Quyen Tran, Tan Nguyen, Dinh Phung, and Trung Le",
        "link": "http://arxiv.org/abs/2506.07247v1",
        "abstract": "We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel\nBayesian inference framework that allows modeling the interactions between\nparticles, thereby enhancing ensemble quality through increased particle\ndiversity. IBDR is grounded in a generalized theoretical framework that\nconnects the distributional population loss with the approximate posterior,\nmotivating a practical dual optimization procedure that enforces distributional\nrobustness while fostering particle diversity. We evaluate IBDR's performance\nagainst various baseline methods using the VTAB-1K benchmark and the common\nreasoning language task. The results consistently show that IBDR outperforms\nthese baselines, underscoring its effectiveness in real-world applications."
    },
    {
        "date": "2025-06",
        "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks",
        "author": "Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, and Hong-Han Shuai",
        "link": "http://arxiv.org/abs/2506.11113v1",
        "abstract": "Peer review is essential for maintaining academic quality, but the increasing\nvolume of submissions places a significant burden on reviewers. Large language\nmodels (LLMs) offer potential assistance in this process, yet their\nsusceptibility to textual adversarial attacks raises reliability concerns. This\npaper investigates the robustness of LLMs used as automated reviewers in the\npresence of such attacks. We focus on three key questions: (1) The\neffectiveness of LLMs in generating reviews compared to human reviewers. (2)\nThe impact of adversarial attacks on the reliability of LLM-generated reviews.\n(3) Challenges and potential mitigation strategies for LLM-based review. Our\nevaluation reveals significant vulnerabilities, as text manipulations can\ndistort LLM assessments. We offer a comprehensive evaluation of LLM performance\nin automated peer reviewing and analyze its robustness against adversarial\nattacks. Our findings emphasize the importance of addressing adversarial risks\nto ensure AI strengthens, rather than compromises, the integrity of scholarly\ncommunication."
    },
    {
        "date": "2025-06",
        "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation",
        "author": "Zhiyuan Zhong, Zhen Sun, Yepang Liu, Xinlei He, and Guanhong Tao",
        "link": "http://arxiv.org/abs/2506.07214v1",
        "abstract": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment."
    },
    {
        "date": "2025-06",
        "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions",
        "author": "Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, and Dacao Zhang",
        "link": "http://arxiv.org/abs/2506.11111v1",
        "abstract": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity."
    },
    {
        "date": "2025-06",
        "title": "Mind the Web: The Security of Web Use Agents",
        "author": "Avishag Shapira, Parth Atulbhai Gandhi, Edan Habler, Oleg Brodt, and Asaf Shabtai",
        "link": "http://arxiv.org/abs/2506.07153v1",
        "abstract": "Web-use agents are rapidly being deployed to automate complex web tasks,\noperating with extensive browser capabilities including multi-tab navigation,\nDOM manipulation, JavaScript execution and authenticated session access.\nHowever, these powerful capabilities create a critical and previously\nunexplored attack surface. This paper demonstrates how attackers can exploit\nweb-use agents' high-privilege capabilities by embedding malicious content in\nweb pages such as comments, reviews, or advertisements that agents encounter\nduring legitimate browsing tasks. In addition, we introduce the task-aligned\ninjection technique that frame malicious commands as helpful task guidance\nrather than obvious attacks. This technique exploiting fundamental limitations\nin LLMs' contextual reasoning: agents struggle in maintaining coherent\ncontextual awareness and fail to detect when seemingly helpful web content\ncontains steering attempts that deviate from their original task goal. Through\nsystematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do\nBrowser, OpenOperator), we demonstrate nine payload types that compromise\nconfidentiality, integrity, and availability, including unauthorized camera\nactivation, user impersonation, local file exfiltration, password leakage, and\ndenial of service, with validation across multiple LLMs achieving success rates\nof 80%-100%. These payloads succeed across agents with built-in safety\nmechanisms, requiring only the ability to post content on public websites,\ncreating unprecedented risks given the ease of exploitation combined with\nagents' high-privilege access. To address this attack, we propose comprehensive\nmitigation strategies including oversight mechanisms, execution constraints,\nand task-aware reasoning techniques, providing practical directions for secure\ndevelopment and deployment."
    },
    {
        "date": "2025-06",
        "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models",
        "author": "Ren-Jian Wang, Ke Xue, Zeyu Qin, Ziniu Li, Sheng Tang, Hao-Tian Li, Shengcai Liu, and Chao Qian",
        "link": "http://arxiv.org/abs/2506.07121v1",
        "abstract": "Ensuring safety of large language models (LLMs) is important. Red teaming--a\nsystematic approach to identifying adversarial prompts that elicit harmful\nresponses from target LLMs--has emerged as a crucial safety evaluation method.\nWithin this framework, the diversity of adversarial prompts is essential for\ncomprehensive safety assessments. We find that previous approaches to\nred-teaming may suffer from two key limitations. First, they often pursue\ndiversity through simplistic metrics like word frequency or sentence embedding\nsimilarity, which may not capture meaningful variation in attack strategies.\nSecond, the common practice of training a single attacker model restricts\ncoverage across potential attack styles and risk categories. This paper\nintroduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to\naddress these limitations. QDRT achieves goal-driven diversity through\nbehavior-conditioned training and implements a behavioral replay buffer in an\nopen-ended manner. Additionally, it trains multiple specialized attackers\ncapable of generating high-quality attacks across diverse styles and risk\ncategories. Our empirical evaluation demonstrates that QDRT generates attacks\nthat are both more diverse and more effective against a wide range of target\nLLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the\nfield of LLM safety by providing a systematic and effective approach to\nautomated red-teaming, ultimately supporting the responsible deployment of\nLLMs."
    },
    {
        "date": "2025-06",
        "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis",
        "author": "Yu-Xuan Wu, Ziyan Huang, Bin Hu, and Zhi-Hong Guan",
        "link": "http://arxiv.org/abs/2506.07118v1",
        "abstract": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability."
    },
    {
        "date": "2025-06",
        "title": "State Entropy Regularization for Robust Reinforcement Learning",
        "author": "Uri Koren, Yonatan Ashlag, Mirco Mutti, Esther Derman, Pierre-Luc Bacon, and Shie Mannor",
        "link": "http://arxiv.org/abs/2506.07085v1",
        "abstract": "State entropy regularization has empirically shown better exploration and\nsample complexity in reinforcement learning (RL). However, its theoretical\nguarantees have not been studied. In this paper, we show that state entropy\nregularization improves robustness to structured and spatially correlated\nperturbations. These types of variation are common in transfer learning but\noften overlooked by standard robust RL methods, which typically focus on small,\nuncorrelated changes. We provide a comprehensive characterization of these\nrobustness properties, including formal guarantees under reward and transition\nuncertainty, as well as settings where the method performs poorly. Much of our\nanalysis contrasts state entropy with the widely used policy entropy\nregularization, highlighting their different benefits. Finally, from a\npractical standpoint, we illustrate that compared with policy entropy, the\nrobustness advantages of state entropy are more sensitive to the number of\nrollouts used for policy evaluation."
    },
    {
        "date": "2025-06",
        "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness",
        "author": "Zhenyu Liu, Huizhi Liang, Rajiv Ranjan, Zhanxing Zhu, Vaclav Snasel, and Varun Ojha",
        "link": "http://arxiv.org/abs/2506.07056v1",
        "abstract": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models."
    },
    {
        "date": "2025-06",
        "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning",
        "author": "Yang Xu, Swetha Ganesh, and Vaneet Aggarwal",
        "link": "http://arxiv.org/abs/2506.07040v1",
        "abstract": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting."
    },
    {
        "date": "2025-06",
        "title": "NanoZone: Scalable, Efficient, and Secure Memory Protection for Arm CCA",
        "author": "Shiqi Liu, Yongpeng Gao, Mingyang Zhang, and Jie Wang",
        "link": "http://arxiv.org/abs/2506.07034v1",
        "abstract": "Arm Confidential Computing Architecture (CCA) currently isolates at the\ngranularity of an entire Confidential Virtual Machine (CVM), leaving intra-VM\nbugs such as Heartbleed unmitigated. The state-of-the-art narrows this to the\nprocess level, yet still cannot stop attacks that pivot within the same\nprocess, and prior intra-enclave schemes are either too slow or incompatible\nwith CVM-style isolation. We extend CCA with a three-tier zone model that\nspawns an unlimited number of lightweight isolation domains inside a single\nprocess, while shielding them from kernel-space adversaries. To block\ndomain-switch abuse, we also add a fast user-level Code-Pointer Integrity (CPI)\nmechanism. We developed two prototypes: a functional version on Arm's official\nsimulator to validate resistance against intra-process and kernel-space\nadversaries, and a performance variant on Arm development boards evaluated for\nsession-key isolation within server applications, in-memory key-value\nprotection, and non-volatile-memory data isolation. NanoZone incurs roughly a\n20% performance overhead while retaining 95% throughput compared to the system\nwithout fine-grained isolation."
    },
    {
        "date": "2025-06",
        "title": "HauntAttack: When Attack Follows Reasoning as a Shadow",
        "author": "Jingyuan Ma, Rui Li, Zheng Li, Junfeng Liu, Lei Sha, and Zhifang Sui",
        "link": "http://arxiv.org/abs/2506.07031v1",
        "abstract": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs."
    },
    {
        "date": "2025-06",
        "title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis",
        "author": "Yuan-Hao Wei, and Yan-Jie Sun",
        "link": "http://arxiv.org/abs/2506.07011v1",
        "abstract": "This study advances the Variational Autoencoder (VAE) framework by addressing\nchallenges in Independent Component Analysis (ICA) under both determined and\nunderdetermined conditions, focusing on enhancing the independence and\ninterpretability of latent variables. Traditional VAEs map observed data to\nlatent variables and back via an encoder-decoder architecture, but struggle\nwith underdetermined ICA where the number of latent variables exceeds observed\nsignals. The proposed Half Adversarial VAE (Half-AVAE) builds on the\nencoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle\nunderdetermined scenarios. By integrating adversarial networks and External\nEnhancement (EE) terms, Half-AVAE promotes mutual independence among latent\ndimensions, achieving factorized and interpretable representations. Experiments\nwith synthetic signals demonstrate that Half-AVAE outperforms baseline models,\nincluding GP-AVAE and Half-VAE, in recovering independent components under\nunderdetermined conditions, as evidenced by lower root mean square errors. The\nstudy highlights the flexibility of VAEs in variational inference, showing that\nencoder omission, combined with adversarial training and structured priors,\nenables effective solutions for complex ICA tasks, advancing applications in\ndisentanglement, causal inference, and generative modeling."
    },
    {
        "date": "2025-06",
        "title": "ModelForge: Using GenAI to Improve the Development of Security Protocols",
        "author": "Martin Duclos, Ivan A. Fernandez, Kaneesha Moore, Sudip Mittal, and Edward Zieglar",
        "link": "http://arxiv.org/abs/2506.07010v1",
        "abstract": "Formal methods can be used for verifying security protocols, but their\nadoption can be hindered by the complexity of translating natural language\nprotocol specifications into formal representations. In this paper, we\nintroduce ModelForge, a novel tool that automates the translation of protocol\nspecifications for the Cryptographic Protocol Shapes Analyzer (CPSA). By\nleveraging advances in Natural Language Processing (NLP) and Generative AI\n(GenAI), ModelForge processes protocol specifications and generates a CPSA\nprotocol definition. This approach reduces the manual effort required, making\nformal analysis more accessible. We evaluate ModelForge by fine-tuning a large\nlanguage model (LLM) to generate protocol definitions for CPSA, comparing its\nperformance with other popular LLMs. The results from our evaluation show that\nModelForge consistently produces quality outputs, excelling in syntactic\naccuracy, though some refinement is needed to handle certain protocol details.\nThe contributions of this work include the architecture and proof of concept\nfor a translating tool designed to simplify the adoption of formal methods in\nthe development of security protocols."
    },
    {
        "date": "2025-06",
        "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization",
        "author": "Yanting Gao, Yepeng Liu, Junming Liu, Qi Zhang, Hongyun Zhang, Duoqian Miao, and Cairong Zhao",
        "link": "http://arxiv.org/abs/2506.06992v1",
        "abstract": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods."
    },
    {
        "date": "2025-06",
        "title": "Chain-of-Code Collapse: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation",
        "author": "Jaechul Roh, Varun Gandhi, Shivani Anilkumar, and Arin Garg",
        "link": "http://arxiv.org/abs/2506.06971v2",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we introduce Chain-of-Code Collapse, where we systematically investigate\nthe robustness of reasoning LLMs by introducing a suite of semantically\nfaithful yet adversarially structured prompt perturbations. Our evaluation --\nspanning 700 perturbed code generations derived from LeetCode-style problems --\napplies transformations such as storytelling reframing, irrelevant constraint\ninjection, example reordering, and numeric perturbation. We observe that while\ncertain modifications severely degrade performance (with accuracy drops up to\n-42.1%), others surprisingly improve model accuracy by up to 35.3%, suggesting\nsensitivity not only to semantics but also to surface-level prompt dynamics.\nThese findings expose the fragility and unpredictability of current reasoning\nsystems, underscoring the need for more principles approaches to reasoning\nalignments and prompting robustness. We release our perturbation datasets and\nevaluation framework to promote further research in trustworthy and resilient\nLLM reasoning."
    },
    {
        "date": "2025-06",
        "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry",
        "author": "Mahdi Salmani, Alireza Abdollahpoorrostam, and Seyed-Mohsen Moosavi-Dezfooli",
        "link": "http://arxiv.org/abs/2506.06933v1",
        "abstract": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings."
    },
    {
        "date": "2025-06",
        "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search",
        "author": "Nima Jamali, Matina Mahdizadeh Sani, Hanieh Naderi, and Shohreh Kasaei",
        "link": "http://arxiv.org/abs/2506.06906v1",
        "abstract": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense)."
    },
    {
        "date": "2025-06",
        "title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?",
        "author": "Paulius Sasnauskas, Yi\u011fit Yal\u0131n, and Goran Radanovi\u0107",
        "link": "http://arxiv.org/abs/2506.06891v1",
        "abstract": "We study the corruption-robustness of in-context reinforcement learning\n(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,\n2023). To address the challenge of reward poisoning attacks targeting the DPT,\nwe propose a novel adversarial training framework, called Adversarially Trained\nDecision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an\nattacker to minimize the true reward of the DPT by poisoning environment\nrewards, and a DPT model to infer optimal actions from the poisoned data. We\nevaluate the effectiveness of our approach against standard bandit algorithms,\nincluding robust baselines designed to handle reward contamination. Our results\nshow that the proposed method significantly outperforms these baselines in\nbandit settings, under a learned attacker. We additionally evaluate AT-DPT on\nan adaptive attacker, and observe similar results. Furthermore, we extend our\nevaluation to the MDP setting, confirming that the robustness observed in\nbandit scenarios generalizes to more complex environments."
    },
    {
        "date": "2025-06",
        "title": "FREE: Fast and Robust Vision Language Models with Early Exits",
        "author": "Divya Jyoti Bajpai, and Manjesh Kumar Hanawal",
        "link": "http://arxiv.org/abs/2506.06884v1",
        "abstract": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE."
    },
    {
        "date": "2025-06",
        "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond",
        "author": "Qi Li, Liangzhi Li, Zhouqiang Jiang, Bowen Wang, and Keke Tang",
        "link": "http://arxiv.org/abs/2506.06823v1",
        "abstract": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy."
    },
    {
        "date": "2025-06",
        "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security",
        "author": "Nidheesh Gorthi, Kartik Thakral, Rishabh Ranjan, Richa Singh, and Mayank Vatsa",
        "link": "http://arxiv.org/abs/2506.06759v1",
        "abstract": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS."
    },
    {
        "date": "2025-06",
        "title": "Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning",
        "author": "Rabah Rahal, Abdelaziz Amara Korba, and Yacine Ghamri-Doudane",
        "link": "http://arxiv.org/abs/2506.06730v1",
        "abstract": "The rapid global adoption of electric vehicles (EVs) has established electric\nvehicle supply equipment (EVSE) as a critical component of smart grid\ninfrastructure. While essential for ensuring reliable energy delivery and\naccessibility, EVSE systems face significant cybersecurity challenges,\nincluding network reconnaissance, backdoor intrusions, and distributed\ndenial-of-service (DDoS) attacks. These emerging threats, driven by the\ninterconnected and autonomous nature of EVSE, require innovative and adaptive\nsecurity mechanisms that go beyond traditional intrusion detection systems\n(IDS). Existing approaches, whether network-based or host-based, often fail to\ndetect sophisticated and targeted attacks specifically crafted to exploit new\nvulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion\ndetection framework that leverages multimodal data sources, including network\ntraffic and kernel events, to identify complex attack patterns. The framework\nemploys a distributed learning approach, enabling collaborative intelligence\nacross EVSE stations while preserving data privacy through federated learning.\nExperimental results demonstrate that the proposed framework outperforms\nexisting solutions, achieving a detection rate above 98% and a precision rate\nexceeding 97% in decentralized environments. This solution addresses the\nevolving challenges of EVSE security, offering a scalable and privacypreserving\nresponse to advanced cyber threats"
    },
    {
        "date": "2025-06",
        "title": "Mitigating Object Hallucination via Robust Local Perception Search",
        "author": "Zixian Gao, Chao Yang, Zhanhui Zhou, Xing Xu, and Chaochao Lu",
        "link": "http://arxiv.org/abs/2506.06729v1",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings."
    },
    {
        "date": "2025-06",
        "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment",
        "author": "Kyubyung Chae, Hyunbin Jin, and Taesup Kim",
        "link": "http://arxiv.org/abs/2506.10020v1",
        "abstract": "Safely aligning large language models (LLMs) often demands extensive\nhuman-labeled preference data, a process that's both costly and time-consuming.\nWhile synthetic data offers a promising alternative, current methods frequently\nrely on complex iterative prompting or auxiliary models. To address this, we\nintroduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,\ntraining-free, and model-agnostic framework that repurposes LLM attack\ntechniques. RAAI works by detecting internal refusal signals and adaptively\ninjecting predefined phrases to elicit harmful, yet fluent, completions. Our\nexperiments show RAAI effectively jailbreaks LLMs, increasing the harmful\nresponse rate from a baseline of 2.15% to up to 61.04% on average across four\nbenchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by\nRAAI improves model robustness against harmful prompts while preserving general\ncapabilities on standard tasks like MMLU and ARC. This work highlights how LLM\nattack methodologies can be reframed as practical tools for scalable and\ncontrollable safety alignment."
    },
    {
        "date": "2025-06",
        "title": "Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics",
        "author": "Di Lin, Wanjing Ren, Xuanbin Li, and Rui Zhang",
        "link": "http://arxiv.org/abs/2506.06682v1",
        "abstract": "In graph self-supervised learning, masked autoencoders (MAE) and contrastive\nlearning (CL) are two prominent paradigms. MAE focuses on reconstructing masked\nelements, while CL maximizes similarity between augmented graph views. Recent\nstudies highlight their complementarity: MAE excels at local feature capture,\nand CL at global information extraction. Hybrid frameworks for homogeneous\ngraphs have been proposed, but face challenges in designing shared encoders to\nmeet the semantic requirements of both tasks. In semantically sparse scenarios,\nCL struggles with view construction, and gradient imbalance between positive\nand negative samples persists. This paper introduces HetCRF, a novel\ndual-channel self-supervised learning framework for heterogeneous graphs.\nHetCRF uses a two-stage aggregation strategy to adapt embedding semantics,\nmaking it compatible with both MAE and CL. To address semantic sparsity, it\nenhances encoder output for view construction instead of relying on raw\nfeatures, improving efficiency. Two positive sample augmentation strategies are\nalso proposed to balance gradient contributions. Node classification\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHetCRF outperforms state-of-the-art baselines. On datasets with missing node\nfeatures, such as Aminer and Freebase, at a 40% label rate in node\nclassification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%\nrespectively compared to the second-best baseline, validating its effectiveness\nand superiority."
    },
    {
        "date": "2025-06",
        "title": "Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations",
        "author": "Arefe Boushehrian, and Amir Najafi",
        "link": "http://arxiv.org/abs/2506.06613v1",
        "abstract": "Learning distribution families over $\\mathbb{R}^d$ is a fundamental problem\nin unsupervised learning and statistics. A central question in this setting is\nwhether a given family of distributions possesses sufficient structure to be\n(at least) information-theoretically learnable and, if so, to characterize its\nsample complexity. In 2018, Ashtiani et al. reframed \\emph{sample\ncompressibility}, originally due to Littlestone and Warmuth (1986), as a\nstructural property of distribution classes, proving that it guarantees\nPAC-learnability. This discovery subsequently enabled a series of recent\nadvancements in deriving nearly tight sample complexity bounds for various\nhigh-dimensional open problems. It has been further conjectured that the\nconverse also holds: every learnable class admits a tight sample compression\nscheme.\n  In this work, we establish that sample compressible families remain learnable\neven from perturbed samples, subject to a set of necessary and sufficient\nconditions. We analyze two models of data perturbation: (i) an additive\nindependent noise model, and (ii) an adversarial corruption model, where an\nadversary manipulates a limited subset of the samples unknown to the learner.\nOur results are general and rely on as minimal assumptions as possible. We\ndevelop a perturbation-quantization framework that interfaces naturally with\nthe compression scheme and leads to sample complexity bounds that scale\ngracefully with the noise level and corruption budget. As concrete\napplications, we establish new sample complexity bounds for learning finite\nmixtures of high-dimensional uniform distributions under both noise and\nadversarial perturbations, as well as for learning Gaussian mixture models from\nadversarially corrupted samples, resolving two open problems in the literature."
    },
    {
        "date": "2025-06",
        "title": "Cyber Security of Sensor Systems for State Sequence Estimation: an AI Approach",
        "author": "Xubin Fang, Rick S. Blum, Ramesh Bharadwaj, and Brian M. Sadler",
        "link": "http://arxiv.org/abs/2506.06572v1",
        "abstract": "Sensor systems are extremely popular today and vulnerable to sensor data\nattacks. Due to possible devastating consequences, counteracting sensor data\nattacks is an extremely important topic, which has not seen sufficient study.\nThis paper develops the first methods that accurately identify/eliminate only\nthe problematic attacked sensor data presented to a sequence\nestimation/regression algorithm under a powerful attack model constructed based\non known/observed attacks. The approach does not assume a known form for the\nstatistical model of the sensor data, allowing data-driven and machine learning\nsequence estimation/regression algorithms to be protected. A simple protection\napproach for attackers not endowed with knowledge of the details of our\nprotection approach is first developed, followed by additional processing for\nattacks based on protection system knowledge. In the cases tested for which it\nwas designed, experimental results show that the simple approach achieves\nperformance indistinguishable, to two decimal places, from that for an approach\nwhich knows which sensors are attacked. For cases where the attacker has\nknowledge of the protection approach, experimental results indicate the\nadditional processing can be configured so that the worst-case degradation\nunder the additional processing and a large number of sensors attacked can be\nmade significantly smaller than the worst-case degradation of the simple\napproach, and close to an approach which knows which sensors are attacked, for\nthe same number of attacked sensors with just a slight degradation under no\nattacks. Mathematical descriptions of the worst-case attacks are used to\ndemonstrate the additional processing will provide similar advantages for cases\nfor which we do not have numerical results. All the data-driven processing used\nin our approaches employ only unattacked training data."
    },
    {
        "date": "2025-06",
        "title": "Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security",
        "author": "Emilia Rivas, Sabrina Saika, Ahtesham Bakht, Aritran Piplai, Nathaniel D. Bastian, and Ankit Shah",
        "link": "http://arxiv.org/abs/2506.06565v1",
        "abstract": "Evolving attacks are a critical challenge for the long-term success of\nNetwork Intrusion Detection Systems (NIDS). The rise of these changing patterns\nhas exposed the limitations of traditional network security methods. While\nsignature-based methods are used to detect different types of attacks, they\noften fail to detect unknown attacks. Moreover, the system requires frequent\nupdates with new signatures as the attackers are constantly changing their\ntactics. In this paper, we design an environment where two agents improve their\npolicies over time. The adversarial agent, referred to as the red agent,\nperturbs packets to evade the intrusion detection mechanism, whereas the blue\nagent learns new defensive policies using drift adaptation techniques to\ncounter the attacks. Both agents adapt iteratively: the red agent responds to\nthe evolving NIDS, while the blue agent adjusts to emerging attack patterns. By\nstudying the model's learned policy, we offer concrete insights into drift\nadaptation techniques with high utility. Experiments show that the blue agent\nboosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to\n30 samples each."
    },
    {
        "date": "2025-06",
        "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles",
        "author": "Thushari Hapuarachchi, Long Dang, and Kaiqi Xiong",
        "link": "http://arxiv.org/abs/2506.06563v1",
        "abstract": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks."
    },
    {
        "date": "2025-06",
        "title": "SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks",
        "author": "Long Dang, Thushari Hapuarachchi, Kaiqi Xiong, and Yi Li",
        "link": "http://arxiv.org/abs/2506.06556v1",
        "abstract": "As the development of autonomous and connected vehicles advances, the\ncomplexity of modern vehicles increases, with numerous Electronic Control Units\n(ECUs) integrated into the system. In an in-vehicle network, these ECUs\ncommunicate with one another using an standard protocol called Controller Area\nNetwork (CAN). Securing communication among ECUs plays a vital role in\nmaintaining the safety and security of the vehicle. This paper proposes a\nrobust SDN-based False Data Detection and Mitigation System (FDDMS) for\nin-vehicle networks. Leveraging the unique capabilities of Software-Defined\nNetworking (SDN), FDDMS is designed to monitor and detect false data injection\nattacks in real-time. Specifically, we focus on brake-related ECUs within an\nSDN-enabled in-vehicle network. First, we decode raw CAN data to create an\nattack model that illustrates how false data can be injected into the system.\nThen, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection\nmodel, is used to identify false data injection attacks. We further propose an\neffective variant of DeepFool attack to evaluate the model's robustness. To\ncountermeasure the impacts of four adversarial attacks including Fast gradient\ndescent method, Basic iterative method, DeepFool, and the DeepFool variant, we\nfurther enhance a re-training technique method with a threshold based selection\nstrategy. Finally, a mitigation scheme is implemented to redirect attack\ntraffic by dynamically updating flow rules through SDN. Our experimental\nresults show that the proposed FDDMS is robust against adversarial attacks and\neffectively detects and mitigates false data injection attacks in real-time."
    },
    {
        "date": "2025-06",
        "title": "A Systematic Review of Poisoning Attacks Against Large Language Models",
        "author": "Neil Fendley, Edward W. Staley, Joshua Carney, William Redman, Marie Chau, and Nathan Drenkow",
        "link": "http://arxiv.org/abs/2506.06518v1",
        "abstract": "With the widespread availability of pretrained Large Language Models (LLMs)\nand their training datasets, concerns about the security risks associated with\ntheir usage has increased significantly. One of these security risks is the\nthreat of LLM poisoning attacks where an attacker modifies some part of the LLM\ntraining process to cause the LLM to behave in a malicious way. As an emerging\narea of research, the current frameworks and terminology for LLM poisoning\nattacks are derived from earlier classification poisoning literature and are\nnot fully equipped for generative LLM settings. We conduct a systematic review\nof published LLM poisoning attacks to clarify the security implications and\naddress inconsistencies in terminology across the literature. We propose a\ncomprehensive poisoning threat model applicable to categorize a wide range of\nLLM poisoning attacks. The poisoning threat model includes four poisoning\nattack specifications that define the logistics and manipulation strategies of\nan attack as well as six poisoning metrics used to measure key characteristics\nof an attack. Under our proposed framework, we organize our discussion of\npublished LLM poisoning literature along four critical dimensions of LLM\npoisoning attacks: concept poisons, stealthy poisons, persistent poisons, and\npoisons for unique tasks, to better understand the current landscape of\nsecurity risks."
    },
    {
        "date": "2025-06",
        "title": "Membership Inference Attacks for Unseen Classes",
        "author": "Pratiksha Thaker, Neil Kale, Zhiwei Steven Wu, and Virginia Smith",
        "link": "http://arxiv.org/abs/2506.06488v1",
        "abstract": "Shadow model attacks are the state-of-the-art approach for membership\ninference attacks on machine learning models. However, these attacks typically\nassume an adversary has access to a background (nonmember) data distribution\nthat matches the distribution the target model was trained on. We initiate a\nstudy of membership inference attacks where the adversary or auditor cannot\naccess an entire subclass from the distribution -- a more extreme but realistic\nversion of distribution shift than has been studied previously. In this\nsetting, we first show that the performance of shadow model attacks degrades\ncatastrophically, and then demonstrate the promise of another approach,\nquantile regression, that does not have the same limitations. We show that\nquantile regression attacks consistently outperform shadow model attacks in the\nclass dropout setting -- for example, quantile regression attacks achieve up to\n11$\\times$ the TPR of shadow models on the unseen class on CIFAR-100, and\nachieve nontrivial TPR on ImageNet even with 90% of training classes removed.\nWe also provide a theoretical model that illustrates the potential and\nlimitations of this approach."
    },
    {
        "date": "2025-06",
        "title": "Benchmarking Misuse Mitigation Against Covert Adversaries",
        "author": "Davis Brown, Mahdi Sabbaghi, Luze Sun, Alexander Robey, George J. Pappas, Eric Wong, and Hamed Hassani",
        "link": "http://arxiv.org/abs/2506.06414v1",
        "abstract": "Existing language model safety evaluations focus on overt attacks and\nlow-stakes tasks. Realistic attackers can subvert current safeguards by\nrequesting help on small, benign-seeming tasks across many independent queries.\nBecause individual queries do not appear harmful, the attack is hard to\n{detect}. However, when combined, these fragments uplift misuse by helping the\nattacker complete hard and dangerous tasks. Toward identifying defenses against\nsuch strategies, we develop Benchmarks for Stateful Defenses (BSD), a data\ngeneration pipeline that automates evaluations of covert attacks and\ncorresponding defenses. Using this pipeline, we curate two new datasets that\nare consistently refused by frontier models and are too difficult for weaker\nopen-weight models. Our evaluations indicate that decomposition attacks are\neffective misuse enablers, and highlight stateful defenses as a countermeasure."
    },
    {
        "date": "2025-06",
        "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems",
        "author": "Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, and Qing Wang",
        "link": "http://arxiv.org/abs/2506.06151v1",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG."
    },
    {
        "date": "2025-06",
        "title": "SATversary: Adversarial Attacks on Satellite Fingerprinting",
        "author": "Joshua Smailes, Sebastian K\u00f6hler, Simon Birnbach, Martin Strohmeier, and Ivan Martinovic",
        "link": "http://arxiv.org/abs/2506.06119v1",
        "abstract": "As satellite systems become increasingly vulnerable to physical layer attacks\nvia SDRs, novel countermeasures are being developed to protect critical\nsystems, particularly those lacking cryptographic protection, or those which\ncannot be upgraded to support modern cryptography. Among these is transmitter\nfingerprinting, which provides mechanisms by which communication can be\nauthenticated by looking at characteristics of the transmitter, expressed as\nimpairments on the signal.\n  Previous works show that fingerprinting can be used to classify satellite\ntransmitters, or authenticate them against SDR-equipped attackers under simple\nreplay scenarios. In this paper we build upon this by looking at attacks\ndirectly targeting the fingerprinting system, with an attacker optimizing for\nmaximum impact in jamming, spoofing, and dataset poisoning attacks, and\ndemonstrate these attacks on the SatIQ system designed to authenticate Iridium\ntransmitters. We show that an optimized jamming signal can cause a 50% error\nrate with attacker-to-victim ratios as low as -30dB (far less power than\ntraditional jamming) and demonstrate successful identity forgery during\nspoofing attacks, with an attacker successfully removing their own\ntransmitter's fingerprint from messages. We also present a data poisoning\nattack, enabling persistent message spoofing by altering the data used to\nauthenticate incoming messages to include the fingerprint of the attacker's\ntransmitter.\n  Finally, we show that our model trained to optimize spoofing attacks can also\nbe used to detect spoofing and replay attacks, even when it has never seen the\nattacker's transmitter before. Furthermore, this technique works even when the\ntraining dataset includes only a single transmitter, enabling fingerprinting to\nbe used to protect small constellations and even individual satellites,\nproviding additional protection where it is needed the most."
    },
    {
        "date": "2025-06",
        "title": "Synthetic Tabular Data: Methods, Attacks and Defenses",
        "author": "Graham Cormode, Samuel Maddock, Enayat Ullah, and Shripad Gade",
        "link": "http://arxiv.org/abs/2506.06108v1",
        "abstract": "Synthetic data is often positioned as a solution to replace sensitive\nfixed-size datasets with a source of unlimited matching data, freed from\nprivacy concerns. There has been much progress in synthetic data generation\nover the last decade, leveraging corresponding advances in machine learning and\ndata analytics. In this survey, we cover the key developments and the main\nconcepts in tabular synthetic data generation, including paradigms based on\nprobabilistic graphical models and on deep learning. We provide background and\nmotivation, before giving a technical deep-dive into the methodologies. We also\naddress the limitations of synthetic data, by studying attacks that seek to\nretrieve information about the original sensitive data. Finally, we present\nextensions and open problems in this area."
    },
    {
        "date": "2025-06",
        "title": "Minoritised Ethnic People's Security and Privacy Concerns and Responses towards Essential Online Services",
        "author": "Aunam Quyoum, Mark Wong, Sebati Ghosh, and Siamak F. Shahandashti",
        "link": "http://arxiv.org/abs/2506.06062v2",
        "abstract": "Minoritised ethnic people are marginalised in society, and therefore at a\nhigher risk of adverse online harms, including those arising from the loss of\nsecurity and privacy of personal data. Despite this, there has been very little\nresearch focused on minoritised ethnic people's security and privacy concerns,\nattitudes, and behaviours. In this work, we provide the results of one of the\nfirst studies in this regard. We explore minoritised ethnic people's\nexperiences of using essential online services across three sectors: health,\nsocial housing, and energy, their security and privacy-related concerns, and\nresponses towards these services. We conducted a thematic analysis of 44\nsemi-structured interviews with people of various reported minoritised\nethnicities in the UK. Privacy concerns and lack of control over personal data\nemerged as a major theme, with many interviewees considering privacy as their\nmost significant concern when using online services. Several creative tactics\nto exercise some agency were reported, including selective and inconsistent\ndisclosure of personal data. A core concern about how data may be used was\ndriven by a fear of repercussions, including penalisation and discrimination,\ninfluenced by prior experiences of institutional and online racism. The\nincreased concern and potential for harm resulted in minoritised ethnic people\ngrappling with a higher-stakes dilemma of whether to disclose personal\ninformation online or not. Furthermore, trust in institutions, or lack thereof,\nwas found to be embedded throughout as a basis for adapting behaviour. We draw\non our results to provide lessons learned for the design of more inclusive,\nmarginalisation-aware, and privacy-preserving online services."
    },
    {
        "date": "2025-06",
        "title": "Sample-Specific Noise Injection For Diffusion-Based Adversarial Purification",
        "author": "Yuhao Sun, Jiacheng Zhang, Zesheng Ye, Chaowei Xiao, and Feng Liu",
        "link": "http://arxiv.org/abs/2506.06027v1",
        "abstract": "Diffusion-based purification (DBP) methods aim to remove adversarial noise\nfrom the input sample by first injecting Gaussian noise through a forward\ndiffusion process, and then recovering the clean example through a reverse\ngenerative process. In the above process, how much Gaussian noise is injected\nto the input sample is key to the success of DBP methods, which is controlled\nby a constant noise level $t^*$ for all samples in existing methods. In this\npaper, we discover that an optimal $t^*$ for each sample indeed could be\ndifferent. Intuitively, the cleaner a sample is, the less the noise it should\nbe injected, and vice versa. Motivated by this finding, we propose a new\nframework, called Sample-specific Score-aware Noise Injection (SSNI).\nSpecifically, SSNI uses a pre-trained score network to estimate how much a data\npoint deviates from the clean data distribution (i.e., score norms). Then,\nbased on the magnitude of score norms, SSNI applies a reweighting function to\nadaptively adjust $t^*$ for each sample, achieving sample-specific noise\ninjections. Empirically, incorporating our framework with existing DBP methods\nresults in a notable improvement in both accuracy and robustness on CIFAR-10\nand ImageNet-1K, highlighting the necessity to allocate distinct noise levels\nto different samples in DBP methods. Our code is available at:\nhttps://github.com/tmlr-group/SSNI."
    },
    {
        "date": "2025-06",
        "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks",
        "author": "Zonglin Wu, Yule Xue, Xin Wei, and Yiren Song",
        "link": "http://arxiv.org/abs/2506.05982v2",
        "abstract": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online."
    }
]