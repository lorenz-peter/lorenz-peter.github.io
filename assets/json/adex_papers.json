[
    {
        "date": "2025-10",
        "title": "Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures",
        "author": "Yuhua Xu, Wei Sun, Chengpei Tang, Jiaxing Lu, Jingying Zhou, and Chen Gu",
        "link": "http://arxiv.org/abs/2510.07219v1",
        "abstract": "Current generative steganography research mainly pursues computationally\nexpensive mappings to perfect Gaussian priors within single diffusion model\narchitectures. This work introduces an efficient framework based on approximate\nGaussian mapping governed by a scale factor calibrated through capacity-aware\nadaptive optimization. Using this framework as a unified analytical tool,\nsystematic comparative analysis of steganography in pixel-space models versus\nVAE-based latent-space systems is conducted. The investigation reveals a\npronounced architecture dependent security-robustness trade-off: pixel-space\nmodels achieve high security against steganalysis but exhibit fragility to\nchannel distortions, while VAE-based systems like Stable Diffusion offer\nsubstantial robustness at the cost of security vulnerabilities. Further\nanalysis indicates that the VAE component drives this behavior through opposing\nmechanisms where the encoder confers robustness via manifold regularization\nwhile the decoder introduces vulnerabilities by amplifying latent perturbations\ninto detectable artifacts. These findings characterize the conflicting\narchitectural roles in generative steganography and establish a foundation for\nfuture research."
    },
    {
        "date": "2025-10",
        "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
        "author": "Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan, Vasilios Mavroudis, Erik Jones, Chris Hicks, Nicholas Carlini, Yarin Gal, and Robert Kirk",
        "link": "http://arxiv.org/abs/2510.07192v1",
        "abstract": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models."
    },
    {
        "date": "2025-10",
        "title": "Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios",
        "author": "Himanshu Choudhary, Arishi Orra, and Manoj Thakur",
        "link": "http://arxiv.org/abs/2510.07099v1",
        "abstract": "In the ever-changing and intricate landscape of financial markets, portfolio\noptimisation remains a formidable challenge for investors and asset managers.\nConventional methods often struggle to capture the complex dynamics of market\nbehaviour and align with diverse investor preferences. To address this, we\npropose an innovative framework, termed Diffusion-Augmented Reinforcement\nLearning (DARL), which synergistically integrates Denoising Diffusion\nProbabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for\nportfolio management. By leveraging DDPMs to generate synthetic market crash\nscenarios conditioned on varying stress intensities, our approach significantly\nenhances the robustness of training data. Empirical evaluations demonstrate\nthat DARL outperforms traditional baselines, delivering superior risk-adjusted\nreturns and resilience against unforeseen crises, such as the 2025 Tariff\nCrisis. This work offers a robust and practical methodology to bolster stress\nresilience in DRL-driven financial applications."
    },
    {
        "date": "2025-10",
        "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
        "author": "Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, Jo\u00e3o Soares, Rafael Ferreira, Diogo Tavares, Diogo Gl\u00f3ria-Silva, Jo\u00e3o Magalh\u00e3es, and David Semedo",
        "link": "http://arxiv.org/abs/2510.06994v1",
        "abstract": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Revisiting Mixout: An Overlooked Path to Robust Finetuning",
        "author": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, and Marco Pedersoli",
        "link": "http://arxiv.org/abs/2510.06982v1",
        "abstract": "Finetuning vision foundation models often improves in-domain accuracy but\ncomes at the cost of robustness under distribution shift. We revisit Mixout, a\nstochastic regularizer that intermittently replaces finetuned weights with\ntheir pretrained reference, through the lens of a single-run, weight-sharing\nimplicit ensemble. This perspective reveals three key levers that govern\nrobustness: the \\emph{masking anchor}, \\emph{resampling frequency}, and\n\\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)\nreplaces the fixed anchor with an exponential moving-average snapshot that\nadapts during training, and (ii) regulates masking period via an explicit\nresampling-frequency hyperparameter. Our sparse-kernel implementation updates\nonly a small fraction of parameters with no inference-time overhead, enabling\ntraining on consumer-grade GPUs. Experiments on benchmarks covering covariate\nshift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,\niWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy\nbeyond zero-shot performance while surpassing both Model Soups and strong\nparameter-efficient finetuning baselines under distribution shift."
    },
    {
        "date": "2025-10",
        "title": "High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization",
        "author": "Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, and Marco Pedersoli",
        "link": "http://arxiv.org/abs/2510.06955v1",
        "abstract": "Ensembling fine-tuned models initialized from powerful pre-trained weights is\na common strategy to improve robustness under distribution shifts, but it comes\nwith substantial computational costs due to the need to train and store\nmultiple models. Dropout offers a lightweight alternative by simulating\nensembles through random neuron deactivation; however, when applied to\npre-trained models, it tends to over-regularize and disrupt critical\nrepresentations necessary for generalization. In this work, we investigate\nMixout, a stochastic regularization technique that provides an alternative to\nDropout for domain generalization. Rather than deactivating neurons, Mixout\nmitigates overfitting by probabilistically swapping a subset of fine-tuned\nweights with their pre-trained counterparts during training, thereby\nmaintaining a balance between adaptation and retention of prior knowledge. Our\nstudy reveals that achieving strong performance with Mixout on domain\ngeneralization benchmarks requires a notably high masking probability of 0.9\nfor ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it\nyields two key advantages for domain generalization: (1) higher masking rates\nmore strongly penalize deviations from the pre-trained parameters, promoting\nbetter generalization to unseen domains; and (2) high-rate masking\nsubstantially reduces computational overhead, cutting gradient computation by\nup to 45% and gradient memory usage by up to 90%. Experiments across five\ndomain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and\nDomainNet, using ResNet and ViT architectures, show that our approach,\nHigh-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based\nmethods while significantly reducing training costs."
    },
    {
        "date": "2025-10",
        "title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning",
        "author": "Ke Guo, Haochen Liu, Xiaojun Wu, and Chen Lv",
        "link": "http://arxiv.org/abs/2510.06913v1",
        "abstract": "Realistic traffic simulation is critical for the development of autonomous\ndriving systems and urban mobility planning, yet existing imitation learning\napproaches often fail to model realistic traffic behaviors. Behavior cloning\nsuffers from covariate shift, while Generative Adversarial Imitation Learning\n(GAIL) is notoriously unstable in multi-agent settings. We identify a key\nsource of this instability: irrelevant interaction misguidance, where a\ndiscriminator penalizes an ego vehicle's realistic behavior due to unrealistic\ninteractions among its neighbors. To address this, we propose Decomposed\nMulti-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map\nand ego-neighbor components, filtering out misleading neighbor: neighbor and\nneighbor: map interactions. We further introduce a social PPO objective that\naugments ego rewards with distance-weighted neighborhood rewards, encouraging\noverall realism across agents. Integrated into a lightweight SMART-based\nbackbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim\nAgents 2025 benchmark."
    },
    {
        "date": "2025-10",
        "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs",
        "author": "Daria Ozerova, and Ekaterina Trofimova",
        "link": "http://arxiv.org/abs/2510.06878v1",
        "abstract": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs."
    },
    {
        "date": "2025-10",
        "title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness",
        "author": "Tavish McDonald, Bo Lei, Stanislav Fort, Bhavya Kailkhura, and Brian Bartoldson",
        "link": "http://arxiv.org/abs/2510.06790v1",
        "abstract": "Models are susceptible to adversarially out-of-distribution (OOD) data\ndespite large training-compute investments into their robustification. Zaremba\net al. (2025) make progress on this problem at test time, showing LLM reasoning\nimproves satisfaction of model specifications designed to thwart attacks,\nresulting in a correlation between reasoning effort and robustness to\njailbreaks. However, this benefit of test compute fades when attackers are\ngiven access to gradients or multimodal inputs. We address this gap, clarifying\nthat inference-compute offers benefits even in such cases. Our approach argues\nthat compositional generalization, through which OOD data is understandable via\nits in-distribution (ID) components, enables adherence to defensive\nspecifications on adversarially OOD inputs. Namely, we posit the Robustness\nfrom Inference Compute Hypothesis (RICH): inference-compute defenses profit as\nthe model's training data better reflects the attacked data's components. We\nempirically support this hypothesis across vision language model and attack\ntypes, finding robustness gains from test-time compute if specification\nfollowing on OOD data is unlocked by compositional generalization, while RL\nfinetuning and protracted reasoning are not critical. For example, increasing\nemphasis on defensive specifications via prompting lowers the success rate of\ngradient-based multimodal attacks on VLMs robustified by adversarial\npretraining, but this same intervention provides no such benefit to\nnot-robustified models. This correlation of inference-compute's robustness\nbenefit with base model robustness is the rich-get-richer dynamic of the RICH:\nattacked data components are more ID for robustified models, aiding\ncompositional generalization to OOD data. Accordingly, we advise layering\ntrain-time and test-time defenses to obtain their synergistic benefit."
    },
    {
        "date": "2025-10",
        "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness",
        "author": "Luca Giordano, and Simon Razniewski",
        "link": "http://arxiv.org/abs/2510.06780v2",
        "abstract": "Large Language Models (LLMs) encode substantial factual knowledge, yet\nmeasuring and systematizing this knowledge remains challenging. Converting it\ninto structured format, for example through recursive extraction approaches\nsuch as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key\nopen questions include whether such extraction can terminate, whether its\noutputs are reproducible, and how robust they are to variations. We\nsystematically study LLM knowledge materialization using miniGPTKBs\n(domain-specific, tractable subcrawls), analyzing termination, reproducibility,\nand robustness across three categories of metrics: yield, lexical similarity,\nand semantic similarity. We experiment with four variations (seed, language,\nrandomness, model) and three illustrative domains (from history, entertainment,\nand finance). Our findings show (i) high termination rates, though\nmodel-dependent; (ii) mixed reproducibility; and (iii) robustness that varies\nby perturbation type: high for seeds and temperature, lower for languages and\nmodels. These results suggest that LLM knowledge materialization can reliably\nsurface core knowledge, while also revealing important limitations."
    },
    {
        "date": "2025-10",
        "title": "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation",
        "author": "Justin Cheung, Samuel Savine, Calvin Nguyen, Lin Lu, and Alhassan S. Yasin",
        "link": "http://arxiv.org/abs/2510.06584v1",
        "abstract": "Deep learning models which perform well on images from their training\ndistribution can degrade substantially when applied to new distributions. If a\nCT scanner introduces a new artifact not present in the training labels, the\nmodel may misclassify the images. Although modern CT scanners include design\nfeatures which mitigate these artifacts, unanticipated or difficult-to-mitigate\nartifacts can still appear in practice. The direct solution of labeling images\nfrom this new distribution can be costly. As a more accessible alternative,\nthis study evaluates domain adaptation as an approach for training models that\nmaintain classification performance despite new artifacts, even without\ncorresponding labels. We simulate ring artifacts from detector gain error in\nsinogram space and evaluate domain adversarial neural networks (DANN) against\nbaseline and augmentation-based approaches on the OrganAMNIST abdominal CT\ndataset. Our results demonstrate that baseline models trained only on clean\nimages fail to generalize to images with ring artifacts, and traditional\naugmentation with other distortion types provides no improvement on unseen\nartifact domains. In contrast, the DANN approach successfully maintains high\nclassification accuracy on ring artifact images using only unlabeled artifact\ndata during training, demonstrating the viability of domain adaptation for\nartifact robustness. The domain-adapted model achieved classification\nperformance on ring artifact test data comparable to models explicitly trained\nwith labeled artifact images, while also showing unexpected generalization to\nuniform noise. These findings provide empirical evidence that domain adaptation\ncan effectively address distribution shift in medical imaging without requiring\nexpensive expert labeling of new artifact distributions, suggesting promise for\ndeployment in clinical settings where novel artifacts may emerge."
    },
    {
        "date": "2025-10",
        "title": "SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems",
        "author": "Jack Vanlyssel, Enrique Sobrados, Ramsha Anwar, Gruia-Catalin Roman, and Afsah Anwar",
        "link": "http://arxiv.org/abs/2510.06535v1",
        "abstract": "Small satellites are integral to scientific, commercial, and defense\nmissions, but reliance on commercial off-the-shelf (COTS) hardware broadens\ntheir attack surface. Although supply chain threats are well studied in other\ncyber-physical domains, their feasibility and stealth in space systems remain\nlargely unexplored. Prior work has focused on flight software, which benefits\nfrom strict security practices and oversight. In contrast, auxiliary COTS\ncomponents often lack robust assurance yet enjoy comparable access to critical\non-board resources, including telemetry, system calls, and the software bus.\nDespite this privileged access, the insider threat within COTS hardware supply\nchains has received little attention. In this work, we present SpyChain, the\nfirst end-to-end design and implementation of independent and colluding\nhardware supply chain threats targeting small satellites. Using NASA's\nsatellite simulation (NOS3), we demonstrate that SpyChain can evade testing,\nexfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)\nattacks through covert channels that bypass ground monitoring. Our study traces\nan escalation from a simple solo component to dynamic, coordinating malware,\nintroducing a taxonomy of stealth across five scenarios. We showcase how\nimplicit trust in auxiliary components enables covert persistence and reveal\nnovel attack vectors, highlighting a new multi-component execution technique\nthat is now incorporated into the SPARTA matrix. Our findings are reinforced by\nacknowledgment and affirmation from NASA's NOS3 team. Finally, we implement\nlightweight onboard defenses, including runtime monitoring, to mitigate threats\nlike SpyChain."
    },
    {
        "date": "2025-10",
        "title": "Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security",
        "author": "Ali Naseh, Anshuman Suri, Yuefeng Peng, Harsh Chaudhari, Alina Oprea, and Amir Houmansadr",
        "link": "http://arxiv.org/abs/2510.06525v1",
        "abstract": "Generative AI leaderboards are central to evaluating model capabilities, but\nremain vulnerable to manipulation. Among key adversarial objectives is rank\nmanipulation, where an attacker must first deanonymize the models behind\ndisplayed outputs -- a threat previously demonstrated and explored for large\nlanguage models (LLMs). We show that this problem can be even more severe for\ntext-to-image leaderboards, where deanonymization is markedly easier. Using\nover 150,000 generated images from 280 prompts and 19 diverse models spanning\nmultiple organizations, architectures, and sizes, we demonstrate that simple\nreal-time classification in CLIP embedding space identifies the generating\nmodel with high accuracy, even without prompt control or historical data. We\nfurther introduce a prompt-level separability metric and identify prompts that\nenable near-perfect deanonymization. Our results indicate that rank\nmanipulation in text-to-image leaderboards is easier than previously\nrecognized, underscoring the need for stronger defenses."
    },
    {
        "date": "2025-10",
        "title": "A Survey on Agentic Security: Applications, Threats and Defenses",
        "author": "Asif Shahriar, Md Nafiu Rahman, Sadif Ahmed, Farig Sadeque, and Md Rizwan Parvez",
        "link": "http://arxiv.org/abs/2510.06445v1",
        "abstract": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage."
    },
    {
        "date": "2025-10",
        "title": "Automated Repeatable Adversary Threat Emulation with Effects Language (EL)",
        "author": "Suresh K. Damodaran, and Paul D. Rowe",
        "link": "http://arxiv.org/abs/2510.06420v1",
        "abstract": "The emulation of multi-step attacks attributed to advanced persistent threats\nis valuable for training defenders and evaluating defense tools. In this paper,\nwe discuss the numerous challenges and desired attributes associated with such\nautomation. Additionally, we introduce the use of Effects Language (EL), a\nvisual programming language with graph-based operational semantics, as a\nsolution to address many of these challenges and requirements. We formally\ndefine the execution semantics of EL, and prove important execution properties.\nFurthermore, we showcase the application of EL to codify attacks using an\nexample from one of the publicly available attack scenarios. We also\ndemonstrate how EL can be utilized to provide proof-of-attack of complex\nmulti-step attacks. Our results highlight the improvements in time and resource\nefficiency achieved through the use of EL for repeatable automation."
    },
    {
        "date": "2025-10",
        "title": "Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings",
        "author": "Ali Baheri",
        "link": "http://arxiv.org/abs/2510.06397v1",
        "abstract": "Non-Euclidean foundation models increasingly place representations in curved\nspaces such as hyperbolic geometry. We show that this geometry creates a\nboundary-driven asymmetry that backdoor triggers can exploit. Near the\nboundary, small input changes appear subtle to standard input-space detectors\nbut produce disproportionately large shifts in the model's representation\nspace. Our analysis formalizes this effect and also reveals a limitation for\ndefenses: methods that act by pulling points inward along the radius can\nsuppress such triggers, but only by sacrificing useful model sensitivity in\nthat same direction. Building on these insights, we propose a simple\ngeometry-adaptive trigger and evaluate it across tasks and architectures.\nEmpirically, attack success increases toward the boundary, whereas conventional\ndetectors weaken, mirroring the theoretical trends. Together, these results\nsurface a geometry-specific vulnerability in non-Euclidean models and offer\nanalysis-backed guidance for designing and understanding the limits of\ndefenses."
    },
    {
        "date": "2025-10",
        "title": "Protecting De-identified Documents from Search-based Linkage Attacks",
        "author": "Pierre Lison, and Mark Anderson",
        "link": "http://arxiv.org/abs/2510.06383v1",
        "abstract": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content."
    },
    {
        "date": "2025-10",
        "title": "Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data",
        "author": "Mohammed Alsubaie, Wenxi Liu, Linxia Gu, Ovidiu C. Andronesi, Sirani M. Perera, and Xianqi Li",
        "link": "http://arxiv.org/abs/2510.06335v1",
        "abstract": "Magnetic Resonance Imaging (MRI) is a critical tool in modern medical\ndiagnostics, yet its prolonged acquisition time remains a critical limitation,\nespecially in time-sensitive clinical scenarios. While undersampling strategies\ncan accelerate image acquisition, they often result in image artifacts and\ndegraded quality. Recent diffusion models have shown promise for reconstructing\nhigh-fidelity images from undersampled data by learning powerful image priors;\nhowever, most existing approaches either (i) rely on unsupervised score\nfunctions without paired supervision or (ii) apply data consistency only as a\npost-processing step. In this work, we introduce a conditional denoising\ndiffusion framework with iterative data-consistency correction, which differs\nfrom prior methods by embedding the measurement model directly into every\nreverse diffusion step and training the model on paired undersampled-ground\ntruth data. This hybrid design bridges generative flexibility with explicit\nenforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that\nour framework consistently outperforms recent state-of-the-art deep learning\nand diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing\nperceptual improvements more faithfully. These results demonstrate that\nintegrating conditional supervision with iterative consistency updates yields\nsubstantial improvements in both pixel-level fidelity and perceptual realism,\nestablishing a principled and practical advance toward robust, accelerated MRI\nreconstruction."
    },
    {
        "date": "2025-10",
        "title": "Training Dynamics Impact Post-Training Quantization Robustness",
        "author": "Albert Catalan-Tatjer, Niccol\u00f2 Ajroldi, and Jonas Geiping",
        "link": "http://arxiv.org/abs/2510.06213v1",
        "abstract": "While post-training quantization is widely adopted for efficient deployment\nof large language models, the mechanisms underlying quantization robustness\nremain unclear. We conduct a comprehensive analysis of quantization degradation\nacross open-source language model training trajectories up to 32B parameters\nand 15T training tokens to accurately assess the relationship between training\ndynamics and quantization performance. Our key finding is that quantization\nerrors in large-scale training runs are driven by a complex interplay between\nlearning rate and other training hyperparameters. Specifically, once learning\nrates decay, validation loss and quantization error diverge, largely\nindependent of training data scale. To investigate interventions on the\ntraining dynamics and identify specific configurations that can modulate\nquantization robustness favorably, we train our own models in controlled\nexperiments up to 100B tokens. Our results challenge the assumption that\nincreasing dataset scale inherently compromises quantization effectiveness,\ndemonstrating instead that strategic training hyperparameter interventions can\nimprove quantization quality at scale."
    },
    {
        "date": "2025-10",
        "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
        "author": "Mi Luo, Zihui Xue, Alex Dimakis, and Kristen Grauman",
        "link": "http://arxiv.org/abs/2510.06077v1",
        "abstract": "Video reasoning, the task of enabling machines to infer from dynamic visual\ncontent through multi-step logic, is crucial for advanced AI. While the\nChain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,\nits application to video understanding remains underexplored. This paper\npresents a systematic analysis revealing that CoT often degrades performance in\nvideo reasoning, generating verbose but misleading internal monologues, and\nleading to hallucinated visual details and overridden correct intuitions - a\nphenomenon we term \"visual thinking drift\". We explain this drift through a\nBayesian lens, positing that CoT traces often diverge from actual visual\nevidence, instead amplifying internal biases or language priors, causing models\nto storytell rather than engage in grounded reasoning. To counteract this, we\nintroduce Visual Evidence Reward (VER), a novel reinforcement learning\nframework that explicitly rewards the generation of reasoning traces that are\nverifiably grounded in visual evidence. Comprehensive evaluation across 10\ndiverse video understanding benchmarks demonstrates that our Video-VER\nconsistently achieves top performance. Our work sheds light on the distinct\nchallenges of video-centric reasoning and encourages the development of AI that\nrobustly grounds its inferences in visual evidence - for large multimodal\nmodels that not only \"think before answering\", but also \"see while thinking\"."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches",
        "author": "Rohit Goswami, and Hannes J\u00f3nsson",
        "link": "http://arxiv.org/abs/2510.06030v1",
        "abstract": "Gaussian process (GP) regression provides a strategy for accelerating saddle\npoint searches on high-dimensional energy surfaces by reducing the number of\ntimes the energy and its derivatives with respect to atomic coordinates need to\nbe evaluated. The computational overhead in the hyperparameter optimization\ncan, however, be large and make the approach inefficient. Failures can also\noccur if the search ventures too far into regions that are not represented well\nenough by the GP model. Here, these challenges are resolved by using\ngeometry-aware optimal transport measures and an active pruning strategy using\na summation over Wasserstein-1 distances for each atom-type in farthest-point\nsampling, selecting a fixed-size subset of geometrically diverse configurations\nto avoid rapidly increasing cost of GP updates as more observations are made.\nStability is enhanced by permutation-invariant metric that provides a reliable\ntrust radius for early-stopping and a logarithmic barrier penalty for the\ngrowth of the signal variance. These physically motivated algorithmic changes\nprove their efficacy by reducing to less than a half the mean computational\ntime on a set of 238 challenging configurations from a previously published\ndata set of chemical reactions. With these improvements, the GP approach is\nestablished as, a robust and scalable algorithm for accelerating saddle point\nsearches when the evaluation of the energy and atomic forces requires\nsignificant computational effort."
    },
    {
        "date": "2025-10",
        "title": "Diffusion-Based Image Editing for Breaking Robust Watermarks",
        "author": "Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, and Bo Zhang",
        "link": "http://arxiv.org/abs/2510.05978v1",
        "abstract": "Robust invisible watermarking aims to embed hidden information into images\nsuch that the watermark can survive various image manipulations. However, the\nrise of powerful diffusion-based image generation and editing techniques poses\na new threat to these watermarking schemes. In this paper, we present a\ntheoretical study and method demonstrating that diffusion models can\neffectively break robust image watermarks that were designed to resist\nconventional perturbations. We show that a diffusion-driven ``image\nregeneration'' process can erase embedded watermarks while preserving\nperceptual image content. We further introduce a novel guided diffusion attack\nthat explicitly targets the watermark signal during generation, significantly\ndegrading watermark detectability. Theoretically, we prove that as an image\nundergoes sufficient diffusion-based transformation, the mutual information\nbetween the watermarked image and the embedded watermark payload vanishes,\nresulting in decoding failure. Experimentally, we evaluate our approach on\nmultiple state-of-the-art watermarking schemes (including the deep\nlearning-based methods StegaStamp, TrustMark, and VINE) and demonstrate\nnear-zero watermark recovery rates after attack, while maintaining high visual\nfidelity of the regenerated images. Our findings highlight a fundamental\nvulnerability in current robust watermarking techniques against generative\nmodel-based attacks, underscoring the need for new watermarking strategies in\nthe era of generative AI."
    },
    {
        "date": "2025-10",
        "title": "MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization",
        "author": "Dayy\u00e1n O'Brien, Barry Haddow, Emily Allaway, and Pinzhen Chen",
        "link": "http://arxiv.org/abs/2510.05962v1",
        "abstract": "Conducting contamination-free evaluation of mathematical capabilities can be\ndifficult for two reasons: models may memorize a test set once it is made\npublic, and current mathematical benchmarks are prone to overfitting due to\nhaving limited diversity of symbols and rules, coupled with closed-ended\nanswers. This paper proposes a method to leverage these shortcomings as useful\nfeatures to a construct dynamic, counterfactual benchmark, which can be used to\nboth reveal overfitting and measure true reasoning. We demonstrate this via\nMatheMagic, which generates math test instances with the interpretations of\nnumbers and operators altered, yet has automatically verifiable answers. Test\ninstances are randomly seeded and constructed at test time to evaluate a\nmodel's induction or deduction capability, offering stability, extensibility,\ncomparability, and robustness to overfitting. Our experiments find that models\nsolve deduction more easily than induction, but they revert to standard math.\nFurther analysis reveals that math-adapted models fail to exhibit a general\n\"skill\" of reasoning, and fine-tuning on induction tasks generalizes poorly."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust and Realible Multimodal Fake News Detection with Incomplete Modality",
        "author": "Hengyang Zhou, Yiwei Wei, Jian Yang, and Zhenyu Zhang",
        "link": "http://arxiv.org/abs/2510.05839v1",
        "abstract": "Multimodal fake news detection (MFND) has become an urgent task with the\nemergence of huge multimodal fake content on social media platforms. Previous\nstudies mainly focus on complex feature extraction and fusion to learn\ndiscriminative information from multimodal content. However, in real-world\napplications, multimedia news may naturally lose some information during\ndissemination, resulting in modality incompleteness, which is detrimental to\nthe generalization and robustness of existing models. To this end, we propose a\nnovel generic and robust multimodal fusion strategy, termed Multi-expert\nModality-incomplete Learning Network (MMLNet), which is simple yet effective.\nIt consists of three key steps: (1) Multi-Expert Collaborative Reasoning to\ncompensate for missing modalities by dynamically leveraging complementary\ninformation through multiple experts. (2) Incomplete Modality Adapters\ncompensates for the missing information by leveraging the new feature\ndistribution. (3) Modality Missing Learning leveraging an label-aware adaptive\nweighting strategy to learn a robust representation with contrastive learning.\nWe evaluate MMLNet on three real-world benchmarks across two languages,\ndemonstrating superior performance compared to state-of-the-art methods while\nmaintaining relative simplicity. By ensuring the accuracy of fake news\ndetection in incomplete modality scenarios caused by information propagation,\nMMLNet effectively curbs the spread of malicious misinformation. Code is\npublicly available at https://github.com/zhyhome/MMLNet."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Automotive Security with a Hybrid Approach towards Universal Intrusion Detection System",
        "author": "Md Rezanur Islam, Mahdi Sahlabadi, Keunkyoung Kim, and Kangbin Yim",
        "link": "http://arxiv.org/abs/2510.05824v1",
        "abstract": "Security measures are essential in the automotive industry to detect\nintrusions in-vehicle networks. However, developing a one-size-fits-all\nIntrusion Detection System (IDS) is challenging because each vehicle has unique\ndata profiles. This is due to the complex and dynamic nature of the data\ngenerated by vehicles regarding their model, driving style, test environment,\nand firmware update. To address this issue, a universal IDS has been developed\nthat can be applied to all types of vehicles without the need for\ncustomization. Unlike conventional IDSs, the universal IDS can adapt to\nevolving data security issues resulting from firmware updates. In this study, a\nnew hybrid approach has been developed, combining Pearson correlation with deep\nlearning techniques. This approach has been tested using data obtained from\nfour distinct mechanical and electronic vehicles, including Tesla, Sonata, and\ntwo Kia models. The data has been combined into two frequency datasets, and\nwavelet transformation has been employed to convert them into the frequency\ndomain, enhancing generalizability. Additionally, a statistical method based on\nindependent rule-based systems using Pearson correlation has been utilized to\nimprove system performance. The system has been compared with eight different\nIDSs, three of which utilize the universal approach, while the remaining five\nare based on conventional techniques. The accuracy of each system has been\nevaluated through benchmarking, and the results demonstrate that the hybrid\nsystem effectively detects intrusions in various vehicle models."
    },
    {
        "date": "2025-10",
        "title": "SBOMproof: Beyond Alleged SBOM Compliance for Supply Chain Security of Container Images",
        "author": "Jacopo Bufalino, Mario Di Francesco, Agathe Blaise, and Stefano Secci",
        "link": "http://arxiv.org/abs/2510.05798v1",
        "abstract": "Supply chain security is extremely important for modern applications running\nat scale in the cloud. In fact, they involve a large number of heterogeneous\nmicroservices that also include third-party software. As a result, security\nvulnerabilities are hard to identify and mitigate before they start being\nactively exploited by attackers. For this reason, governments have recently\nintroduced cybersecurity regulations that require vendors to share a software\nbill of material (SBOM) with end users or regulators. An SBOM can be employed\nto identify the security vulnerabilities of a software component even without\naccess to its source code, as long as it is accurate and interoperable across\ndifferent tools. This work evaluates this issue through a comprehensive study\nof tools for SBOM generation and vulnerability scanning, including both\nopen-source software and cloud services from major providers. We specifically\ntarget software containers and focus on operating system packages in Linux\ndistributions that are widely used as base images due to their far-reaching\nsecurity impact. Our findings show that the considered tools are largely\nincompatible, leading to inaccurate reporting and a large amount of undetected\nvulnerabilities. We uncover the SBOM confusion vulnerability, a byproduct of\nsuch fragmented ecosystem, where inconsistent formats prevent reliable\nvulnerability detection across tools."
    },
    {
        "date": "2025-10",
        "title": "A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data",
        "author": "Gianmarco Perantoni, and Lorenzo Bruzzone",
        "link": "http://arxiv.org/abs/2510.05760v1",
        "abstract": "Deep learning has gained broad interest in remote sensing image scene\nclassification thanks to the effectiveness of deep neural networks in\nextracting the semantics from complex data. However, deep networks require\nlarge amounts of training samples to obtain good generalization capabilities\nand are sensitive to errors in the training labels. This is a problem in remote\nsensing since highly reliable labels can be obtained at high costs and in\nlimited amount. However, many sources of less reliable labeled data are\navailable, e.g., obsolete digital maps. In order to train deep networks with\nlarger datasets, we propose both the combination of single or multiple weak\nsources of labeled data with a small but reliable dataset to generate\nmultisource labeled datasets and a novel training strategy where the\nreliability of each source is taken in consideration. This is done by\nexploiting the transition matrices describing the statistics of the errors of\neach source. The transition matrices are embedded into the labels and used\nduring the training process to weigh each label according to the related\nsource. The proposed method acts as a weighting scheme at gradient level, where\neach instance contributes with different weights to the optimization of\ndifferent classes. The effectiveness of the proposed method is validated by\nexperiments on different datasets. The results proved the robustness and\ncapability of leveraging on unreliable source of labels of the proposed method."
    },
    {
        "date": "2025-10",
        "title": "Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning",
        "author": "Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, and Antti Honkela",
        "link": "http://arxiv.org/abs/2510.05753v2",
        "abstract": "With the emergence of powerful large-scale foundation models, the training\nparadigm is increasingly shifting from from-scratch training to transfer\nlearning. This enables high utility training with small, domain-specific\ndatasets typical in sensitive applications. Membership inference attacks (MIAs)\nprovide an empirical estimate of the privacy leakage by machine learning\nmodels. Yet, prior assessments of MIAs against models fine-tuned with transfer\nlearning rely on a small subset of possible attacks. We address this by\ncomparing performance of diverse MIAs in transfer learning settings to help\npractitioners identify the most efficient attacks for privacy risk evaluation.\nWe find that attack efficacy decreases with the increase in training data for\nscore-based MIAs. We find that there is no one MIA which captures all privacy\nrisks in models trained with transfer learning. While the Likelihood Ratio\nAttack (LiRA) demonstrates superior performance across most experimental\nscenarios, the Inverse Hessian Attack (IHA) proves to be more effective against\nmodels fine-tuned on PatchCamelyon dataset in high data regime."
    },
    {
        "date": "2025-10",
        "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
        "author": "Mary Llewellyn, Annie Gray, Josh Collyer, and Michael Harries",
        "link": "http://arxiv.org/abs/2510.05709v1",
        "abstract": "Before adopting a new large language model (LLM) architecture, it is critical\nto understand vulnerabilities accurately. Existing evaluations can be difficult\nto trust, often drawing conclusions from LLMs that are not meaningfully\ncomparable, relying on heuristic inputs or employing metrics that fail to\ncapture the inherent uncertainty. In this paper, we propose a principled and\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\ninjection attacks. First, we propose practical approaches to experimental\ndesign, tackling unfair LLM comparisons by considering two practitioner\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\nwe address the analysis of experiments and propose a Bayesian hierarchical\nmodel with embedding-space clustering. This model is designed to improve\nuncertainty quantification in the common scenario that LLM outputs are not\ndeterministic, test prompts are designed imperfectly, and practitioners only\nhave a limited amount of compute to evaluate vulnerabilities. We show the\nimproved inferential capabilities of the model in several prompt injection\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\nof Transformer versus Mamba architectures. Our findings show that consideration\nof output variability can suggest less definitive findings. However, for some\nattacks, we find notably increased Transformer and Mamba-variant\nvulnerabilities across LLMs with the same training data or mathematical\nability."
    },
    {
        "date": "2025-10",
        "title": "Membership Inference Attacks on Tokenizers of Large Language Models",
        "author": "Meng Tong, Yuntao Du, Kejiang Chen, Weiming Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2510.05699v1",
        "abstract": "Membership inference attacks (MIAs) are widely used to assess the privacy\nrisks associated with machine learning models. However, when these attacks are\napplied to pre-trained large language models (LLMs), they encounter significant\nchallenges, including mislabeled samples, distribution shifts, and\ndiscrepancies in model size between experimental and real-world settings. To\naddress these limitations, we introduce tokenizers as a new attack vector for\nmembership inference. Specifically, a tokenizer converts raw text into tokens\nfor LLMs. Unlike full models, tokenizers can be efficiently trained from\nscratch, thereby avoiding the aforementioned challenges. In addition, the\ntokenizer's training data is typically representative of the data used to\npre-train LLMs. Despite these advantages, the potential of tokenizers as an\nattack vector remains unexplored. To this end, we present the first study on\nmembership leakage through tokenizers and explore five attack methods to infer\ndataset membership. Extensive experiments on millions of Internet samples\nreveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To\nmitigate this emerging risk, we further propose an adaptive defense. Our\nfindings highlight tokenizers as an overlooked yet critical privacy threat,\nunderscoring the urgent need for privacy-preserving mechanisms specifically\ndesigned for them."
    },
    {
        "date": "2025-10",
        "title": "Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection",
        "author": "Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, and Dongjie Wang",
        "link": "http://arxiv.org/abs/2510.05535v1",
        "abstract": "Feature selection eliminates redundancy among features to improve downstream\ntask performance while reducing computational overhead. Existing methods often\nstruggle to capture intricate feature interactions and adapt across diverse\napplication scenarios. Recent advances employ generative intelligence to\nalleviate these drawbacks. However, these methods remain constrained by\npermutation sensitivity in embedding and reliance on convexity assumptions in\ngradient-based search. To address these limitations, our initial work\nintroduces a novel framework that integrates permutation-invariant embedding\nwith policy-guided search. Although effective, it still left opportunities to\nadapt to realistic distributed scenarios. In practice, data across local\nclients is highly imbalanced, heterogeneous and constrained by strict privacy\nregulations, limiting direct sharing. These challenges highlight the need for a\nframework that can integrate feature selection knowledge across clients without\nexposing sensitive information. In this extended journal version, we advance\nthe framework from two perspectives: 1) developing a privacy-preserving\nknowledge fusion strategy to derive a unified representation space without\nsharing sensitive raw data. 2) incorporating a sample-aware weighting strategy\nto address distributional imbalance among heterogeneous local clients.\nExtensive experiments validate the effectiveness, robustness, and efficiency of\nour framework. The results further demonstrate its strong generalization\nability in federated learning scenarios. The code and data are publicly\navailable: https://anonymous.4open.science/r/FedCAPS-08BF."
    },
    {
        "date": "2025-10",
        "title": "LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability",
        "author": "Harshil Vejendla",
        "link": "http://arxiv.org/abs/2510.05530v1",
        "abstract": "Test-time adaptation (TTA) aims to adapt a pretrained model to distribution\nshifts using only unlabeled test data. While promising, existing methods like\nTent suffer from instability and can catastrophically forget the source\nknowledge, especially with small batch sizes or challenging corruptions. We\nargue that this arises from overly deterministic updates on a complex loss\nsurface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation\n(LATTA), a novel approach that regularizes adaptation through two key\nmechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient\nLangevin Dynamics (SGLD) to explore the local parameter space and escape poor\nlocal minima, and (2) a stable weight anchor that prevents the model from\ndiverging from its robust source pre-training. This combination allows LATTA to\nadapt effectively without sacrificing stability. Unlike prior Bayesian TTA\nmethods, LATTA requires no architectural changes or expensive Monte Carlo\npasses. We conduct extensive experiments on standard benchmarks, including\nRotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that\nLATTA significantly outperforms existing methods, including Tent, CoTTA, and\nEATA, setting a new state of the art for self-supervised TTA by improving\naverage accuracy on CIFAR-10-C by over 2% while simultaneously reducing\nperformance variance."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety",
        "author": "Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, and Lukas Rutishauser",
        "link": "http://arxiv.org/abs/2510.05442v1",
        "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to\ncomplete complex tasks. However, this tool usage introduces the risk of\nindirect prompt injections, where malicious instructions hidden in tool outputs\ncan manipulate the agent, posing security risks like data leakage. Current\ndefense strategies typically rely on fine-tuning LLM agents on datasets of\nknown attacks. However, the generation of these datasets relies on manually\ncrafted attack patterns, which limits their diversity and leaves agents\nvulnerable to novel prompt injections. To address this limitation, we propose\nAdversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework\nthat leverages adversarial reinforcement learning (RL) by formulating the\nproblem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker\nthat learns to autonomously generate diverse prompt injections and an agent\nthat learns to defend against them while completing its assigned tasks. To\nensure robustness against a wide range of attacks and to prevent cyclic\nlearning, we employ a population-based learning framework that trains the agent\nto defend against all previous attacker checkpoints. Evaluated on BrowserGym\nand AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower\nattack success rate than the original model while also improving their task\nsuccess rate. Our analysis further confirms that the adversarial process\ngenerates a diverse and challenging set of attacks, leading to a more robust\nagent compared to the base model."
    },
    {
        "date": "2025-10",
        "title": "AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling",
        "author": "Xiaogeng Liu, and Chaowei Xiao",
        "link": "http://arxiv.org/abs/2510.05379v2",
        "abstract": "Recent advancements in jailbreaking large language models (LLMs), such as\nAutoDAN-Turbo, have demonstrated the power of automated strategy discovery.\nAutoDAN-Turbo employs a lifelong learning agent to build a rich library of\nattack strategies from scratch. While highly effective, its test-time\ngeneration process involves sampling a strategy and generating a single\ncorresponding attack prompt, which may not fully exploit the potential of the\nlearned strategy library. In this paper, we propose to further improve the\nattack performance of AutoDAN-Turbo through test-time scaling. We introduce two\ndistinct scaling methods: Best-of-N and Beam Search. The Best-of-N method\ngenerates N candidate attack prompts from a sampled strategy and selects the\nmost effective one based on a scorer model. The Beam Search method conducts a\nmore exhaustive search by exploring combinations of strategies from the library\nto discover more potent and synergistic attack vectors. According to the\nexperiments, the proposed methods significantly boost performance, with Beam\nSearch increasing the attack success rate by up to 15.6 percentage points on\nLlama-3.1-70B-Instruct and achieving a nearly 60% relative improvement against\nthe highly robust GPT-o4-mini compared to the vanilla method."
    },
    {
        "date": "2025-10",
        "title": "RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness",
        "author": "Zhenyu Liu, and Varun Ojha",
        "link": "http://arxiv.org/abs/2510.05317v1",
        "abstract": "Adversarial training is the most effective defense against adversarial\nattacks. The effectiveness of the adversarial attacks has been on the design of\nits loss function and regularization term. The most widely used loss function\nin adversarial training is cross-entropy and mean squared error (MSE) as its\nregularization objective. However, MSE enforces overly uniform optimization\nbetween two output distributions during training, which limits its robustness\nin adversarial training scenarios. To address this issue, we revisit the idea\nof mutual learning (originally designed for knowledge distillation) and propose\ntwo novel regularization strategies tailored for adversarial training: (i)\nweighted adversarial mutual regularization and (ii) adversarial generalization\nregularization. In the former, we formulate a decomposed adversarial mutual\nKullback-Leibler divergence (KL-divergence) loss, which allows flexible control\nover the optimization process by assigning unequal weights to the main and\nauxiliary objectives. In the latter, we introduce an additional clean target\ndistribution into the adversarial training objective, improving generalization\nand enhancing model robustness. Extensive experiments demonstrate that our\nproposed methods significantly improve adversarial robustness compared to\nexisting regularization-based approaches."
    },
    {
        "date": "2025-10",
        "title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts",
        "author": "Yining She, Daniel W. Peterson, Marianne Menglin Liu, Vikas Upadhyay, Mohammad Hossein Chaghazardi, Eunsuk Kang, and Dan Roth",
        "link": "http://arxiv.org/abs/2510.05310v1",
        "abstract": "With the increasing adoption of large language models (LLMs), ensuring the\nsafety of LLM systems has become a pressing concern. External LLM-based\nguardrail models have emerged as a popular solution to screen unsafe inputs and\noutputs, but they are themselves fine-tuned or prompt-engineered LLMs that are\nvulnerable to data distribution shifts. In this paper, taking Retrieval\nAugmentation Generation (RAG) as a case study, we investigated how robust\nLLM-based guardrails are against additional information embedded in the\ncontext. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss\nmodels, we confirmed that inserting benign documents into the guardrail context\nalters the judgments of input and output guardrails in around 11% and 8% of\ncases, making them unreliable. We separately analyzed the effect of each\ncomponent in the augmented context: retrieved documents, user query, and\nLLM-generated response. The two mitigation methods we tested only bring minor\nimprovements. These results expose a context-robustness gap in current\nguardrails and motivate training and evaluation protocols that are robust to\nretrieval and query composition."
    },
    {
        "date": "2025-10",
        "title": "SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography",
        "author": "Zahra Maleki, Amirhossein Akbari, Amirhossein Binesh, and Babak Khalaj",
        "link": "http://arxiv.org/abs/2510.05296v1",
        "abstract": "Remote photoplethysmography (rPPG) is an innovative method for monitoring\nheart rate and vital signs by using a simple camera to record a person, as long\nas any part of their skin is visible. This low-cost, contactless approach helps\nin remote patient monitoring, emotion analysis, smart vehicle utilization, and\nmore. Over the years, various techniques have been proposed to improve the\naccuracy of this technology, especially given its sensitivity to lighting and\nmovement. In the unsupervised pipeline, it is necessary to first select skin\nregions from the video to extract the rPPG signal from the skin color changes.\nWe introduce a novel skin segmentation technique that prioritizes skin regions\nto enhance the quality of the extracted signal. It can detect areas of skin all\nover the body, making it more resistant to movement, while removing areas such\nas the mouth, eyes, and hair that may cause interference. Our model is\nevaluated on publicly available datasets, and we also present a new dataset,\ncalled SYNC-rPPG, to better represent real-world conditions. The results\nindicate that our model demonstrates a prior ability to capture heartbeats in\nchallenging conditions, such as talking and head rotation, and maintain the\nmean absolute error (MAE) between predicted and actual heart rates, while other\nmethods fail to do so. In addition, we demonstrate high accuracy in detecting a\ndiverse range of skin tones, making this technique a promising option for\nreal-world applications."
    },
    {
        "date": "2025-10",
        "title": "Proactive defense against LLM Jailbreak",
        "author": "Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, and Junfeng Yang",
        "link": "http://arxiv.org/abs/2510.05052v1",
        "abstract": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks."
    },
    {
        "date": "2025-10",
        "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings",
        "author": "Ahmed Elhussein, Paul Meddeb, Abigail Newbury, Jeanne Mirone, Martin Stoll, and Gamze Gursoy",
        "link": "http://arxiv.org/abs/2510.05049v1",
        "abstract": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments."
    },
    {
        "date": "2025-10",
        "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based Vulnerability Detection",
        "author": "Avilash Rath, Weiliang Qi, Youpeng Li, and Xinda Wang",
        "link": "http://arxiv.org/abs/2510.04987v1",
        "abstract": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD."
    },
    {
        "date": "2025-10",
        "title": "StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R",
        "author": "Allen Daniel Sunny",
        "link": "http://arxiv.org/abs/2510.04974v1",
        "abstract": "We present StructuralDecompose, an R package for modular and interpretable\ntime series decomposition. Unlike existing approaches that treat decomposition\nas a monolithic process, StructuralDecompose separates the analysis into\ndistinct components: changepoint detection, anomaly detection, smoothing, and\ndecomposition. This design provides flexibility and robust- ness, allowing\nusers to tailor methods to specific time series characteristics. We demonstrate\nthe package on simulated and real-world datasets, benchmark its performance\nagainst state-of-the- art tools such as Rbeast and autostsm, and discuss its\nrole in interpretable machine learning workflows."
    },
    {
        "date": "2025-10",
        "title": "\u03bcDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy",
        "author": "Elena Corbetta, and Thomas Bocklitz",
        "link": "http://arxiv.org/abs/2510.04859v1",
        "abstract": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions."
    },
    {
        "date": "2025-10",
        "title": "Distributionally Robust Causal Abstractions",
        "author": "Yorgos Felekis, Theodoros Damoulas, and Paris Giampouras",
        "link": "http://arxiv.org/abs/2510.04842v1",
        "abstract": "Causal Abstraction (CA) theory provides a principled framework for relating\ncausal models that describe the same system at different levels of granularity\nwhile ensuring interventional consistency between them. Recently, several\napproaches for learning CAs have been proposed, but all assume fixed and\nwell-specified exogenous distributions, making them vulnerable to environmental\nshifts and misspecification. In this work, we address these limitations by\nintroducing the first class of distributionally robust CAs and their associated\nlearning algorithms. The latter cast robust causal abstraction learning as a\nconstrained min-max optimization problem with Wasserstein ambiguity sets. We\nprovide theoretical results, for both empirical and Gaussian environments,\nleading to principled selection of the level of robustness via the radius of\nthese sets. Furthermore, we present empirical evidence across different\nproblems and CA learning methods, demonstrating our framework's robustness not\nonly to environmental shifts but also to structural model and intervention\nmapping misspecification."
    },
    {
        "date": "2025-10",
        "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation",
        "author": "Malith Premarathna, Fabrizio Ruggeri, and Dixon Vimalajeewa",
        "link": "http://arxiv.org/abs/2510.04811v1",
        "abstract": "Understanding signal behavior across scales is vital in areas such as natural\nphenomena analysis and financial modeling. A key property is self-similarity,\nquantified by the Hurst exponent (H), which reveals long-term dependencies.\nWavelet-based methods are effective for estimating H due to their multi-scale\nanalysis capability, but additive noise in real-world measurements often\ndegrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an\nenhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),\nincorporating noise mitigation and generating multiple level-pairwise estimates\nfrom signal energy pairs. A neural network (NN) combines these estimates,\nreplacing traditional averaging. This adaptive learning maintains ALPHEE's\nbehavior in noise-free cases while improving performance in noisy conditions.\nExtensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's\naccuracy using both averaging and NN-based methods. Under noise, however,\ntraditional averaging deteriorates and requires impractical level restrictions,\nwhile NC-ALPHEE consistently outperforms existing techniques without such\nconstraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,\nsignificantly enhancing the reliability of wavelet-based methods in noisy\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Collusion-Resistant Quantum Secure Key Leasing Beyond Decryption",
        "author": "Fuyuki Kitagawa, Ryo Nishimaki, and Nikhil Pappu",
        "link": "http://arxiv.org/abs/2510.04754v1",
        "abstract": "Secure key leasing (SKL) enables the holder of a secret key for a\ncryptographic function to temporarily lease the key using quantum information.\nLater, the recipient can produce a deletion certificate, which proves that they\nno longer have access to the secret key. The security guarantee ensures that\neven a malicious recipient cannot continue to evaluate the function, after\nproducing a valid deletion certificate.\n  Most prior work considers an adversarial recipient that obtains a single\nleased key, which is insufficient for many applications. In the more realistic\ncollusion-resistant setting, security must hold even when polynomially many\nkeys are leased (and subsequently deleted). However, achieving\ncollusion-resistant SKL from standard assumptions remains poorly understood,\nespecially for functionalities beyond decryption.\n  We improve upon this situation by introducing new pathways for constructing\ncollusion-resistant SKL. Our main contributions are as follows:\n  - A generalization of quantum-secure collusion-resistant traitor tracing\ncalled multi-level traitor tracing (MLTT), and a compiler that transforms an\nMLTT scheme for a primitive X into a collusion-resistant SKL scheme for\nprimitive X.\n  - The first bounded collusion-resistant SKL scheme for PRFs, assuming LWE.\n  - A compiler that upgrades any single-key secure SKL scheme for digital\nsignatures into one with unbounded collusion-resistance, assuming OWFs.\n  - A compiler that upgrades collusion-resistant SKL schemes with classical\ncertificates to ones having verification-query resilience, assuming OWFs."
    },
    {
        "date": "2025-10",
        "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection",
        "author": "Alina Ciocarlan, Sylvie Le H\u00e9garat-Mascle, and Sidonie Lefebvre",
        "link": "http://arxiv.org/abs/2510.04741v1",
        "abstract": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released."
    },
    {
        "date": "2025-10",
        "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model",
        "author": "Shuche Wang, Adarsh Barik, Peng Zhao, and Vincent Y. F. Tan",
        "link": "http://arxiv.org/abs/2510.04685v1",
        "abstract": "We develop the first parameter-free algorithms for the Stochastically\nExtended Adversarial (SEA) model, a framework that bridges adversarial and\nstochastic online convex optimization. Existing approaches for the SEA model\nrequire prior knowledge of problem-specific parameters, such as the diameter of\nthe domain $D$ and the Lipschitz constant of the loss functions $G$, which\nlimits their practical applicability. Addressing this, we develop\nparameter-free methods by leveraging the Optimistic Online Newton Step (OONS)\nalgorithm to eliminate the need for these parameters. We first establish a\ncomparator-adaptive algorithm for the scenario with unknown domain diameter but\nknown Lipschitz constant, achieving an expected regret bound of\n$\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} +\n\\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and\n$\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic\nvariance and cumulative adversarial variation, respectively. We then extend\nthis to the more general setting where both $D$ and $G$ are unknown, attaining\nthe comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound\nexhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$,\ndemonstrating the efficacy of our proposed methods even when both parameters\nare unknown in the SEA model."
    },
    {
        "date": "2025-10",
        "title": "Backing the Wrong Horse: How Bit-Level Netlist Augmentation can Counter Power Side Channel Attacks",
        "author": "Ali Asghar, Andreas Becher, and Daniel Ziener",
        "link": "http://arxiv.org/abs/2510.04640v1",
        "abstract": "The dependence of power-consumption on the processed data is a known\nvulnerability of CMOS circuits, resulting in side channels which can be\nexploited by power-based side channel attacks (SCAs). These attacks can extract\nsensitive information, such as secret keys, from the implementation of\ncryptographic algorithms. Existing countermeasures against power-based side\nchannel attacks focus on analyzing information leakage at the byte level.\nHowever, this approach neglects the impact of individual bits on the overall\nresistance of a cryptographic implementation. In this work, we present a\ncountermeasure based on single-bit leakage. The results suggest that the\nproposed countermeasure cannot be broken by attacks using conventional SCA\nleakage models."
    },
    {
        "date": "2025-10",
        "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI",
        "author": "Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, and Joonhyuk Kang",
        "link": "http://arxiv.org/abs/2510.04622v1",
        "abstract": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research."
    },
    {
        "date": "2025-10",
        "title": "Computational Certified Deletion Property of Magic Square Game and its Application to Classical Secure Key Leasing",
        "author": "Yuki Takeuchi, and Duo Xu",
        "link": "http://arxiv.org/abs/2510.04529v2",
        "abstract": "We present the first construction of a computational Certified Deletion\nProperty (CDP) achievable with classical communication, derived from the\ncompilation of the non-local Magic Square Game (MSG). We leverage the KLVY\ncompiler to transform the non-local MSG into a 2-round interactive protocol,\nrigorously demonstrating that this compilation preserves the game-specific CDP.\nPreviously, the quantum value and rigidity of the compiled game were\ninvestigated. We emphasize that we are the first to investigate CDP (local\nrandomness in [Fu and Miller, Phys. Rev. A 97, 032324 (2018)]) for the compiled\ngame. Then, we combine this CDP with the framework [Kitagawa, Morimae, and\nYamakawa, Eurocrypt 2025] to construct Secure Key Leasing with classical Lessor\n(cSKL). SKL enables the Lessor to lease the secret key to the Lessee and verify\nthat a quantum Lessee has indeed deleted the key. In this paper, we realize\ncSKL for PKE, PRF, and digital signature. Compared to prior works for cSKL, we\nrealize cSKL for PRF and digital signature for the first time. In addition, we\nsucceed in weakening the assumption needed to construct cSKL."
    },
    {
        "date": "2025-10",
        "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
        "author": "Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, and Anh Tuan Luu",
        "link": "http://arxiv.org/abs/2510.04503v1",
        "abstract": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community."
    },
    {
        "date": "2025-10",
        "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
        "author": "Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, and Ren\u00e9 Vidal",
        "link": "http://arxiv.org/abs/2510.04398v1",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA."
    },
    {
        "date": "2025-10",
        "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models",
        "author": "Anindya Sundar Das, Kangjie Chen, and Monowar Bhuyan",
        "link": "http://arxiv.org/abs/2510.04347v1",
        "abstract": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense."
    },
    {
        "date": "2025-10",
        "title": "Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics",
        "author": "Harshil Vejendla",
        "link": "http://arxiv.org/abs/2510.04342v1",
        "abstract": "Forecasting chaotic systems is a cornerstone challenge in many scientific\nfields, complicated by the exponential amplification of even infinitesimal\nprediction errors. Modern machine learning approaches often falter due to two\nopposing pitfalls: over-specializing on a single, well-known chaotic system\n(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing\nvast, unrelated time-series, which prevents the model from learning the nuances\nof any specific dynamical regime. We propose Curriculum Chaos Forecasting\n(CCF), a training paradigm that bridges this gap. CCF organizes training data\nbased on fundamental principles of dynamical systems theory, creating a\ncurriculum that progresses from simple, periodic behaviors to highly complex,\nchaotic dynamics. We quantify complexity using the largest Lyapunov exponent\nand attractor dimension, two well-established metrics of chaos. By first\ntraining a sequence model on predictable systems and gradually introducing more\nchaotic trajectories, CCF enables the model to build a robust and generalizable\nrepresentation of dynamical behaviors. We curate a library of over 50 synthetic\nODE/PDE systems to build this curriculum. Our experiments show that\npre-training with CCF significantly enhances performance on unseen, real-world\nbenchmarks. On datasets including Sunspot numbers, electricity demand, and\nhuman ECG signals, CCF extends the valid prediction horizon by up to 40%\ncompared to random-order training and more than doubles it compared to training\non real-world data alone. We demonstrate that this benefit is consistent across\nvarious neural architectures (GRU, Transformer) and provide extensive ablations\nto validate the importance of the curriculum's structure."
    },
    {
        "date": "2025-10",
        "title": "VortexPIA: Indirect Prompt Injection Attack against LLMs for Efficient Extraction of User Privacy",
        "author": "Yu Cui, Sicheng Pan, Yifei Liu, Haibin Zhang, and Cong Zuo",
        "link": "http://arxiv.org/abs/2510.04261v1",
        "abstract": "Large language models (LLMs) have been widely deployed in Conversational AIs\n(CAIs), while exposing privacy and security threats. Recent research shows that\nLLM-based CAIs can be manipulated to extract private information from human\nusers, posing serious security threats. However, the methods proposed in that\nstudy rely on a white-box setting that adversaries can directly modify the\nsystem prompt. This condition is unlikely to hold in real-world deployments.\nThe limitation raises a critical question: can unprivileged attackers still\ninduce such privacy risks in practical LLM-integrated applications? To address\nthis question, we propose \\textsc{VortexPIA}, a novel indirect prompt injection\nattack that induces privacy extraction in LLM-integrated applications under\nblack-box settings. By injecting token-efficient data containing false\nmemories, \\textsc{VortexPIA} misleads LLMs to actively request private\ninformation in batches. Unlike prior methods, \\textsc{VortexPIA} allows\nattackers to flexibly define multiple categories of sensitive data. We evaluate\n\\textsc{VortexPIA} on six LLMs, covering both traditional and reasoning LLMs,\nacross four benchmark datasets. The results show that \\textsc{VortexPIA}\nsignificantly outperforms baselines and achieves state-of-the-art (SOTA)\nperformance. It also demonstrates efficient privacy requests, reduced token\nconsumption, and enhanced robustness against defense mechanisms. We further\nvalidate \\textsc{VortexPIA} on multiple realistic open-source LLM-integrated\napplications, demonstrating its practical effectiveness."
    },
    {
        "date": "2025-10",
        "title": "AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents",
        "author": "Yanjie Li, Yiming Cao, Dong Wang, and Bin Xiao",
        "link": "http://arxiv.org/abs/2510.04257v1",
        "abstract": "Multimodal agents built on large vision-language models (LVLMs) are\nincreasingly deployed in open-world settings but remain highly vulnerable to\nprompt injection, especially through visual inputs. We introduce AgentTypo, a\nblack-box red-teaming framework that mounts adaptive typographic prompt\ninjection by embedding optimized text into webpage images. Our automatic\ntypographic prompt injection (ATPI) algorithm maximizes prompt reconstruction\nby substituting captioners while minimizing human detectability via a stealth\nloss, with a Tree-structured Parzen Estimator guiding black-box optimization\nover text placement, size, and color. To further enhance attack strength, we\ndevelop AgentTypo-pro, a multi-LLM system that iteratively refines injection\nprompts using evaluation feedback and retrieves successful past examples for\ncontinual learning. Effective prompts are abstracted into generalizable\nstrategies and stored in a strategy repository, enabling progressive knowledge\naccumulation and reuse in future attacks. Experiments on the VWA-Adv benchmark\nacross Classifieds, Shopping, and Reddit scenarios show that AgentTypo\nsignificantly outperforms the latest image-based attacks such as AgentAttack.\nOn GPT-4o agents, our image-only attack raises the success rate from 0.23 to\n0.45, with consistent results across GPT-4V, GPT-4o-mini, Gemini 1.5 Pro, and\nClaude 3 Opus. In image+text settings, AgentTypo achieves 0.68 ASR, also\noutperforming the latest baselines. Our findings reveal that AgentTypo poses a\npractical and potent threat to multimodal agents and highlight the urgent need\nfor effective defense."
    },
    {
        "date": "2025-10",
        "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks",
        "author": "Ayushi Mehrotra, Derek Peng, Dipkamal Bhusal, and Nidhi Rastogi",
        "link": "http://arxiv.org/abs/2510.04245v1",
        "abstract": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks."
    },
    {
        "date": "2025-10",
        "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
        "author": "Peigui Qi, Kunsheng Tang, Wenbo Zhou, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, and Jie Zhang",
        "link": "http://arxiv.org/abs/2510.05173v2",
        "abstract": "Text-to-image models have shown remarkable capabilities in generating\nhigh-quality images from natural language descriptions. However, these models\nare highly vulnerable to adversarial prompts, which can bypass safety measures\nand produce harmful content. Despite various defensive strategies, achieving\nrobustness against attacks while maintaining practical utility in real-world\napplications remains a significant challenge. To address this issue, we first\nconduct an empirical study of the text encoder in the Stable Diffusion (SD)\nmodel, which is a widely used and representative text-to-image model. Our\nfindings reveal that the [EOS] token acts as a semantic aggregator, exhibiting\ndistinct distributional patterns between benign and adversarial prompts in its\nembedding space. Building on this insight, we introduce \\textbf{SafeGuider}, a\ntwo-step framework designed for robust safety control without compromising\ngeneration quality. SafeGuider combines an embedding-level recognition model\nwith a safety-aware feature erasure beam search algorithm. This integration\nenables the framework to maintain high-quality image generation for benign\nprompts while ensuring robust defense against both in-domain and out-of-domain\nattacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack\nsuccess rates, achieving a maximum rate of only 5.48\\% across various attack\nscenarios. Moreover, instead of refusing to generate or producing black images\nfor unsafe prompts, \\textbf{SafeGuider} generates safe and meaningful images,\nenhancing its practical utility. In addition, SafeGuider is not limited to the\nSD model and can be effectively applied to other text-to-image models, such as\nthe Flux model, demonstrating its versatility and adaptability across different\narchitectures. We hope that SafeGuider can shed some light on the practical\ndeployment of secure text-to-image systems."
    },
    {
        "date": "2025-10",
        "title": "Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems",
        "author": "Guixian Zhang, Guan Yuan, Ziqi Xu, Yanmei Zhang, Jing Ren, Zhenyun Deng, and Debo Cheng",
        "link": "http://arxiv.org/abs/2510.04093v2",
        "abstract": "Cognitive diagnostics in the Web-based Intelligent Education System (WIES)\naims to assess students' mastery of knowledge concepts from heterogeneous,\nnoisy interactions. Recent work has tried to utilize Large Language Models\n(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are\nprone to noise-induced misjudgments. Specially, WIES's open environment\ncontinuously attracts new students and produces vast amounts of response logs,\nexacerbating the data imbalance and noise issues inherent in traditional\neducational systems. To address these challenges, we propose DLLM, a\nDiffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first\nconstructs independent subgraphs based on response correctness, then applies\nrelation augmentation alignment module to mitigate data imbalance. The two\nsubgraph representations are then fused and aligned with LLM-derived,\nsemantically augmented representations. Importantly, before each alignment\nstep, DLLM employs a two-stage denoising diffusion module to eliminate\nintrinsic noise while assisting structural representation alignment.\nSpecifically, unconditional denoising diffusion first removes erroneous\ninformation, followed by conditional denoising diffusion based on graph-guided\nto eliminate misleading information. Finally, the noise-robust representation\nthat integrates semantic knowledge and structural information is fed into\nexisting cognitive diagnosis models for prediction. Experimental results on\nthree publicly available web-based educational platform datasets demonstrate\nthat our DLLM achieves optimal predictive performance across varying noise\nlevels, which demonstrates that DLLM achieves noise robustness while\neffectively leveraging semantic knowledge from LLM."
    },
    {
        "date": "2025-10",
        "title": "Quantifying Distributional Robustness of Agentic Tool-Selection",
        "author": "Jehyeok Yeon, Isha Chaudhary, and Gagandeep Singh",
        "link": "http://arxiv.org/abs/2510.03992v1",
        "abstract": "Large language models (LLMs) are increasingly deployed in agentic systems\nwhere they map user intents to relevant external tools to fulfill a task. A\ncritical step in this process is tool selection, where a retriever first\nsurfaces candidate tools from a larger pool, after which the LLM selects the\nmost appropriate one. This pipeline presents an underexplored attack surface\nwhere errors in selection can lead to severe outcomes like unauthorized data\naccess or denial of service, all without modifying the agent's model or code.\nWhile existing evaluations measure task performance in benign settings, they\noverlook the specific vulnerabilities of the tool selection mechanism under\nadversarial conditions. To address this gap, we introduce ToolCert, the first\nstatistical framework that formally certifies tool selection robustness.\nToolCert models tool selection as a Bernoulli success process and evaluates it\nagainst a strong, adaptive attacker who introduces adversarial tools with\nmisleading metadata, and are iteratively refined based on the agent's previous\nchoices. By sampling these adversarial interactions, ToolCert produces a\nhigh-confidence lower bound on accuracy, formally quantifying the agent's\nworst-case performance. Our evaluation with ToolCert uncovers the severe\nfragility: under attacks injecting deceptive tools or saturating retrieval, the\ncertified accuracy bound drops near zero, an average performance drop of over\n60% compared to non-adversarial settings. For attacks targeting the retrieval\nand selection stages, the certified accuracy bound plummets to less than 20%\nafter just a single round of adversarial adaptation. ToolCert thus reveals\npreviously unexamined security threats inherent to tool selection and provides\na principled method to quantify an agent's robustness to such threats, a\nnecessary step for the safe deployment of agentic systems."
    },
    {
        "date": "2025-10",
        "title": "Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks",
        "author": "Minh K. Quan, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2510.05165v1",
        "abstract": "Cross-slice attack attribution in 6G networks faces the fundamental challenge\nof distinguishing genuine causal relationships from spurious correlations in\nshared infrastructure environments. We propose a theoretically-grounded\ndomain-adapted Granger causality framework that integrates statistical causal\ninference with network-specific resource modeling for real-time attack\nattribution. Our approach addresses key limitations of existing methods by\nincorporating resource contention dynamics and providing formal statistical\nguarantees. Comprehensive evaluation on a production-grade 6G testbed with\n1,100 empirically-validated attack scenarios demonstrates 89.2% attribution\naccuracy with sub-100ms response time, representing a statistically significant\n10.1 percentage point improvement over state-of-the-art baselines. The\nframework provides interpretable causal explanations suitable for autonomous 6G\nsecurity orchestration."
    },
    {
        "date": "2025-10",
        "title": "BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty",
        "author": "Akshay Kudva, and Joel A. Paulson",
        "link": "http://arxiv.org/abs/2510.03893v1",
        "abstract": "Optimal design under uncertainty remains a fundamental challenge in advancing\nreliable, next-generation process systems. Robust optimization (RO) offers a\nprincipled approach by safeguarding against worst-case scenarios across a range\nof uncertain parameters. However, traditional RO methods typically require\nknown problem structure, which limits their applicability to high-fidelity\nsimulation environments. To overcome these limitations, recent work has\nexplored robust Bayesian optimization (RBO) as a flexible alternative that can\naccommodate expensive, black-box objectives. Existing RBO methods, however,\ngenerally ignore available structural information and struggle to scale to\nhigh-dimensional settings. In this work, we introduce BONSAI (Bayesian\nOptimization of Network Systems under uncertAInty), a new RBO framework that\nleverages partial structural knowledge commonly available in simulation-based\nmodels. Instead of treating the objective as a monolithic black box, BONSAI\nrepresents it as a directed graph of interconnected white- and black-box\ncomponents, allowing the algorithm to utilize intermediate information within\nthe optimization process. We further propose a scalable Thompson sampling-based\nacquisition function tailored to the structured RO setting, which can be\nefficiently optimized using gradient-based methods. We evaluate BONSAI across a\ndiverse set of synthetic and real-world case studies, including applications in\nprocess systems engineering. Compared to existing simulation-based RO\nalgorithms, BONSAI consistently delivers more sample-efficient and\nhigher-quality robust solutions, highlighting its practical advantages for\nuncertainty-aware design in complex engineering systems."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Agent Collaboration for C to Rust Translation",
        "author": "Tianyu Li, Ruishi Li, Bo Wang, Brandon Paulsen, Umang Mathur, and Prateek Saxena",
        "link": "http://arxiv.org/abs/2510.03879v1",
        "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches."
    },
    {
        "date": "2025-10",
        "title": "SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks",
        "author": "Nikolaos Kaparinos, and Vasileios Mezaris",
        "link": "http://arxiv.org/abs/2510.03870v1",
        "abstract": "Generative Adversarial Networks (GANs) achieve excellent performance in\ngenerative tasks, such as image super-resolution, but their computational\nrequirements make difficult their deployment on resource-constrained devices.\nWhile knowledge distillation is a promising research direction for GAN\ncompression, effectively training a smaller student generator is challenging\ndue to the capacity mismatch between the student generator and the teacher\ndiscriminator. In this work, we propose Student Discriminator Assisted\nKnowledge Distillation (SDAKD), a novel GAN distillation methodology that\nintroduces a student discriminator to mitigate this capacity mismatch. SDAKD\nfollows a three-stage training strategy, and integrates an adapted feature map\ndistillation approach in its last two training stages. We evaluated SDAKD on\ntwo well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our\nexperiments demonstrate consistent improvements over the baselines and SOTA GAN\nknowledge distillation methods. The SDAKD source code will be made openly\navailable upon acceptance of the paper."
    },
    {
        "date": "2025-10",
        "title": "Technical note on Fisher Information for Robust Federated Cross-Validation",
        "author": "Behraj Khan, and Tahir Qasim Syed",
        "link": "http://arxiv.org/abs/2510.03838v1",
        "abstract": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets."
    },
    {
        "date": "2025-10",
        "title": "Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events",
        "author": "Shuoyan Wei, Feng Li, Shengeng Tang, Runmin Cong, Yao Zhao, Meng Wang, and Huihui Bai",
        "link": "http://arxiv.org/abs/2510.03833v1",
        "abstract": "Continuous space-time video super-resolution (C-STVSR) has garnered\nincreasing interest for its capability to reconstruct high-resolution and\nhigh-frame-rate videos at arbitrary spatial and temporal scales. However,\nprevailing methods often generalize poorly, producing unsatisfactory results\nwhen applied to out-of-distribution (OOD) scales. To overcome this limitation,\nwe present EvEnhancer, a novel approach that marries the unique properties of\nhigh temporal resolution and high dynamic range encapsulated in event streams\nto achieve robust and generalizable C-STVSR. Our approach incorporates\nevent-adapted synthesis that capitalizes on the spatiotemporal correlations\nbetween frames and events to capture long-term motion trajectories, enabling\nadaptive interpolation and fusion across space and time. This is then coupled\nwith a local implicit video transformer that integrates local implicit video\nneural function with cross-scale spatiotemporal attention to learn continuous\nvideo representations and generate plausible videos at arbitrary resolutions\nand frame rates. We further develop EvEnhancerPlus, which builds a controllable\nswitching mechanism that dynamically determines the reconstruction difficulty\nfor each spatiotemporal pixel based on local event statistics. This allows the\nmodel to adaptively route reconstruction along the most suitable pathways at a\nfine-grained pixel level, substantially reducing computational overhead while\nmaintaining excellent performance. Furthermore, we devise a cross-derivative\ntraining strategy that stabilizes the convergence of such a multi-pathway\nframework through staged cross-optimization. Extensive experiments demonstrate\nthat our method achieves state-of-the-art performance on both synthetic and\nreal-world datasets, while maintaining superior generalizability at OOD scales.\nThe code is available at https://github.com/W-Shuoyan/EvEnhancerPlus."
    },
    {
        "date": "2025-10",
        "title": "Pilot Contamination Attacks Detection with Machine Learning for Multi-User Massive MIMO",
        "author": "Pedro Ivo da Cruz, Dimitri Silva, Tito Spadini, Ricardo Suyama, and Murilo Bellezoni Loiola",
        "link": "http://arxiv.org/abs/2510.03831v1",
        "abstract": "Massive multiple-input multiple-output (MMIMO) is essential to modern\nwireless communication systems, like 5G and 6G, but it is vulnerable to active\neavesdropping attacks. One type of such attack is the pilot contamination\nattack (PCA), where a malicious user copies pilot signals from an authentic\nuser during uplink, intentionally interfering with the base station's (BS)\nchannel estimation accuracy. In this work, we propose to use a Decision Tree\n(DT) algorithm for PCA detection at the BS in a multi-user system. We present a\nmethodology to generate training data for the DT classifier and select the best\nDT according to their depth. Then, we simulate different scenarios that could\nbe encountered in practice and compare the DT to a classical technique based on\nlikelihood ratio testing (LRT) submitted to the same scenarios. The results\nrevealed that a DT with only one level of depth is sufficient to outperform the\nLRT. The DT shows a good performance regarding the probability of detection in\nnoisy scenarios and when the malicious user transmits with low power, in which\ncase the LRT fails to detect the PCA. We also show that the reason for the good\nperformance of the DT is its ability to compute a threshold that separates PCA\ndata from non-PCA data better than the LRT's threshold. Moreover, the DT does\nnot necessitate prior knowledge of noise power or assumptions regarding the\nsignal power of malicious users, prerequisites typically essential for LRT and\nother hypothesis testing methodologies."
    },
    {
        "date": "2025-10",
        "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
        "author": "Xueyang Zhou, Yangming Xu, Guiyao Tie, Yongchao Chen, Guowen Zhang, Duanfeng Chu, Pan Zhou, and Lichao Sun",
        "link": "http://arxiv.org/abs/2510.03827v1",
        "abstract": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis of Ponzi Schemes in Ethereum Smart Contracts",
        "author": "Chunyi Zhang, Qinghong Wei, and Xiaoqi Li",
        "link": "http://arxiv.org/abs/2510.03819v1",
        "abstract": "The rapid advancement of blockchain technology has precipitated the\nwidespread adoption of Ethereum and smart contracts across a variety of\nsectors. However, this has also given rise to numerous fraudulent activities,\nwith many speculators embedding Ponzi schemes within smart contracts, resulting\nin significant financial losses for investors. Currently, there is a lack of\neffective methods for identifying and analyzing such new types of fraudulent\nactivities. This paper categorizes these scams into four structural types and\nexplores the intrinsic characteristics of Ponzi scheme contract source code\nfrom a program analysis perspective. The Mythril tool is employed to conduct\nstatic and dynamic analyses of representative cases, thereby revealing their\nvulnerabilities and operational mechanisms. Furthermore, this paper employs\nshell scripts and command patterns to conduct batch detection of open-source\nsmart contract code, thereby unveiling the common characteristics of Ponzi\nscheme smart contracts."
    },
    {
        "date": "2025-10",
        "title": "Robust Batched Bandits",
        "author": "Yunwen Guo, Yunlun Shu, Gongyi Zhuo, and Tianyu Wang",
        "link": "http://arxiv.org/abs/2510.03798v1",
        "abstract": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness."
    },
    {
        "date": "2025-10",
        "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes",
        "author": "Zuomin Qu, Yimao Guo, Qianyue Hu, and Wei Lu",
        "link": "http://arxiv.org/abs/2510.03747v1",
        "abstract": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching."
    },
    {
        "date": "2025-10",
        "title": "Securing Operating Systems Through Fine-grained Kernel Access Limitation for IoT Systems",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, Lin Ye, and Likun Liu",
        "link": "http://arxiv.org/abs/2510.03737v1",
        "abstract": "With the development of Internet of Things (IoT), it is gaining a lot of\nattention. It is important to secure the embedded systems with low overhead.\nThe Linux Seccomp is widely used by developers to secure the kernels by\nblocking the access of unused syscalls, which introduces less overhead.\nHowever, there are no systematic Seccomp configuration approaches for IoT\napplications without the help of developers. In addition, the existing Seccomp\nconfiguration approaches are coarse-grained, which cannot analyze and limit the\nsyscall arguments. In this paper, a novel static dependent syscall analysis\napproach for embedded applications is proposed, which can obtain all of the\npossible dependent syscalls and the corresponding arguments of the target\napplications. So, a fine-grained kernel access limitation can be performed for\nthe IoT applications. To this end, the mappings between dynamic library APIs\nand syscalls according with their arguments are built, by analyzing the control\nflow graphs and the data dependency relationships of the dynamic libraries. To\nthe best of our knowledge, this is the first work to generate the fine-grained\nSeccomp profile for embedded applications."
    },
    {
        "date": "2025-10",
        "title": "Shrinking the Kernel Attack Surface Through Static and Dynamic Syscall Limitation",
        "author": "Dongyang Zhan, Zhaofeng Yu, Xiangzhan Yu, Hongli Zhang, and Lin Ye",
        "link": "http://arxiv.org/abs/2510.03720v1",
        "abstract": "Linux Seccomp is widely used by the program developers and the system\nmaintainers to secure the operating systems, which can block unused syscalls\nfor different applications and containers to shrink the attack surface of the\noperating systems. However, it is difficult to configure the whitelist of a\ncontainer or application without the help of program developers. Docker\ncontainers block about only 50 syscalls by default, and lots of unblocked\nuseless syscalls introduce a big kernel attack surface. To obtain the dependent\nsyscalls, dynamic tracking is a straight-forward approach but it cannot get the\nfull syscall list. Static analysis can construct an over-approximated syscall\nlist, but the list contains many false positives. In this paper, a systematic\ndependent syscall analysis approach, sysverify, is proposed by combining static\nanalysis and dynamic verification together to shrink the kernel attack surface.\nThe semantic gap between the binary executables and syscalls is bridged by\nanalyzing the binary and the source code, which builds the mapping between the\nlibrary APIs and syscalls systematically. To further reduce the attack surface\nat best effort, we propose a dynamic verification approach to intercept and\nanalyze the security of the invocations of indirect-call-related or rarely\ninvoked syscalls with low overhead."
    },
    {
        "date": "2025-10",
        "title": "Backdoor-Powered Prompt Injection Attacks Nullify Defense Methods",
        "author": "Yulin Chen, Haoran Li, Yuan Sui, Yangqiu Song, and Bryan Hooi",
        "link": "http://arxiv.org/abs/2510.03705v1",
        "abstract": "With the development of technology, large language models (LLMs) have\ndominated the downstream natural language processing (NLP) tasks. However,\nbecause of the LLMs' instruction-following abilities and inability to\ndistinguish the instructions in the data content, such as web pages from search\nengines, the LLMs are vulnerable to prompt injection attacks. These attacks\ntrick the LLMs into deviating from the original input instruction and executing\nthe attackers' target instruction. Recently, various instruction hierarchy\ndefense strategies are proposed to effectively defend against prompt injection\nattacks via fine-tuning. In this paper, we explore more vicious attacks that\nnullify the prompt injection defense methods, even the instruction hierarchy:\nbackdoor-powered prompt injection attacks, where the attackers utilize the\nbackdoor attack for prompt injection attack purposes. Specifically, the\nattackers poison the supervised fine-tuning samples and insert the backdoor\ninto the model. Once the trigger is activated, the backdoored model executes\nthe injected instruction surrounded by the trigger. We construct a benchmark\nfor comprehensive evaluation. Our experiments demonstrate that backdoor-powered\nprompt injection attacks are more harmful than previous prompt injection\nattacks, nullifying existing prompt injection defense methods, even the\ninstruction hierarchy techniques."
    },
    {
        "date": "2025-10",
        "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
        "author": "Zehua Liu, Han Wu, Xiaojin Fu, Shuqi Liu, Xiongwei Han, Tao Zhong, and Mingxuan Yuan",
        "link": "http://arxiv.org/abs/2510.03691v1",
        "abstract": "Optimizers are crucial for the efficient training of Large Language Models\n(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers\nlike Muon have emerged, which regularize gradient updates by operating on\nentire weight matrices. The Muon optimizer balances the gradient updates along\nall the directions. However, Muon's reliance on the matrix sign function can\nlead to training instability, exhibits incompatibility when fine-tuning models\npre-trained with AdamW. To address these limitations, we propose \\textbf{REG},\na novel optimizer that replaces Muon's aggressive matrix sign operator with the\nRow-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a\nmatrix, the RACS operator regularizes the update steps in a less drastic\nmanner, making it simpler to implement and more compatible with established\ntraining dynamics. Through extensive empirical experiments on LLM training, we\ndemonstrate that our REG optimizer not only achieves superior performance and\nstability over AdamW, but also maintains consistency with the AdamW training\nparadigm. This consistency is particularly evident during the fine-tuning\nstage, where REG optimizer avoids the performance degradation observed with\nMuon."
    },
    {
        "date": "2025-10",
        "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse",
        "author": "Rabeya Amin Jhuma, and Mostafa Mohaimen Akand Faisal",
        "link": "http://arxiv.org/abs/2510.03636v1",
        "abstract": "This study explored how in-context learning (ICL) in large language models\ncan be disrupted by data poisoning attacks in the setting of public health\nsentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small\nadversarial perturbations such as synonym replacement, negation insertion, and\nrandomized perturbation were introduced into the support examples. Even these\nminor manipulations caused major disruptions, with sentiment labels flipping in\nup to 67% of cases. To address this, a Spectral Signature Defense was applied,\nwhich filtered out poisoned examples while keeping the data's meaning and\nsentiment intact. After defense, ICL accuracy remained steady at around 46.7%,\nand logistic regression validation reached 100% accuracy, showing that the\ndefense successfully preserved the dataset's integrity. Overall, the findings\nextend prior theoretical studies of ICL poisoning to a practical, high-stakes\nsetting in public health discourse analysis, highlighting both the risks and\npotential defenses for robust LLM deployment. This study also highlights the\nfragility of ICL under attack and the value of spectral defenses in making AI\nsystems more reliable for health-related social media monitoring."
    },
    {
        "date": "2025-10",
        "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications",
        "author": "Maraz Mia, and Mir Mehedi A. Pritom",
        "link": "http://arxiv.org/abs/2510.03623v1",
        "abstract": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML)\nresearchers with the power of scrutinizing the decisions of the black-box\nmodels. XAI methods enable looking deep inside the models' behavior, eventually\ngenerating explanations along with a perceived trust and transparency. However,\ndepending on any specific XAI method, the level of trust can vary. It is\nevident that XAI methods can themselves be a victim of post-adversarial attacks\nthat manipulate the expected outcome from the explanation module. Among such\nattack tactics, fairwashing explanation (FE), manipulation explanation (ME),\nand backdoor-enabled manipulation attacks (BD) are the notable ones. In this\npaper, we try to understand these adversarial attack techniques, tactics, and\nprocedures (TTPs) on explanation alteration and thus the effect on the model's\ndecisions. We have explored a total of six different individual attack\nprocedures on post-hoc explanation methods such as SHAP (SHapley Additive\nexPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG\n(Integrated Gradients), and investigated those adversarial attacks in\ncybersecurity applications scenarios such as phishing, malware, intrusion, and\nfraudulent website detection. Our experimental study reveals the actual\neffectiveness of these attacks, thus providing an urgency for immediate\nattention to enhance the resiliency of XAI methods and their applications."
    },
    {
        "date": "2025-10",
        "title": "Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs",
        "author": "Fatmazohra Rezkellah, and Ramzi Dakhmouche",
        "link": "http://arxiv.org/abs/2510.03567v1",
        "abstract": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach."
    },
    {
        "date": "2025-10",
        "title": "A Quantum-Secure Voting Framework Using QKD, Dual-Key Symmetric Encryption, and Verifiable Receipts",
        "author": "Taha M. Mahmoud, and Naima Kaabouch",
        "link": "http://arxiv.org/abs/2510.03489v1",
        "abstract": "Electronic voting systems face growing risks from cyberattacks and data\nbreaches, which are expected to intensify with the advent of quantum computing.\nTo address these challenges, we introduce a quantum-secure voting framework\nthat integrates Quantum Key Distribution (QKD), Dual-Key Symmetric Encryption,\nand verifiable receipt mechanisms to strengthen the privacy, integrity, and\nreliability of the voting process. The framework enables voters to establish\nencryption keys securely, cast encrypted ballots, and verify their votes\nthrough receipt-based confirmation, all without exposing the vote contents. To\nevaluate performance, we simulate both quantum and classical communication\nchannels using the Message Queuing Telemetry Transport (MQTT) protocol. Results\ndemonstrate that the system can process large numbers of votes efficiently with\nlow latency and minimal error rates. This approach offers a scalable and\npractical path toward secure, transparent, and verifiable electronic voting in\nthe quantum era."
    },
    {
        "date": "2025-10",
        "title": "Security Analysis and Threat Modeling of Research Management Applications [Extended Version]",
        "author": "Boniface M. Sindala, and Ragib Hasan",
        "link": "http://arxiv.org/abs/2510.03407v1",
        "abstract": "Research management applications (RMA) are widely used in clinical research\nenvironments to collect, transmit, analyze, and store sensitive data. This data\nis so valuable making RMAs susceptible to security threats. This analysis,\nanalyzes RMAs' security, focusing on Research Electronic Data Capture (REDCap)\nas an example. We explore the strengths and vulnerabilities within RMAs by\nevaluating the architecture, data flow, and security features. We identify and\nassess potential risks using the MITRE ATT\\&CK framework and STRIDE model. We\nassess REDCap's defenses against common attack vectors focusing on security to\nprovide confidentiality, integrity, availability, non-repudiation, and\nauthentication. We conclude by proposing recommendations for enhancing the\nsecurity of RMAs, ensuring that critical research data remains protected\nwithout compromising usability. This research aims to contribute towards a more\nsecure framework for managing sensitive information in research-intensive\nenvironments."
    },
    {
        "date": "2025-10",
        "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles",
        "author": "Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, and Stefano Soatto",
        "link": "http://arxiv.org/abs/2510.03224v1",
        "abstract": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks."
    },
    {
        "date": "2025-10",
        "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
        "author": "Tianyu Xu, Jiawei Chen, Jiazhao Zhang, Wenyao Zhang, Zekun Qi, Minghan Li, Zhizheng Zhang, and He Wang",
        "link": "http://arxiv.org/abs/2510.03142v1",
        "abstract": "Visual navigation policy is widely regarded as a promising direction, as it\nmimics humans by using egocentric visual observations for navigation. However,\noptical information of visual observations is difficult to be explicitly\nmodeled like LiDAR point clouds or depth maps, which subsequently requires\nintelligent models and large-scale data. To this end, we propose to leverage\nthe intelligence of the Vision-Language-Action (VLA) model to learn diverse\nnavigation capabilities from synthetic expert data in a teacher-student manner.\nSpecifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360\nobservations) based on pretrained large language models and visual foundation\nmodels. For large-scale navigation data, we collect expert data from three\nreinforcement learning (RL) experts trained with privileged depth information\nin three challenging tailor-made environments for different navigation\ncapabilities: reaching, squeezing, and avoiding. We iteratively train our VLA\nmodel using data collected online from RL experts, where the training ratio is\ndynamically balanced based on performance on individual capabilities. Through\nextensive experiments in synthetic environments, we demonstrate that our model\nachieves strong generalization capability. Moreover, we find that our student\nVLA model outperforms the RL teachers, demonstrating the synergistic effect of\nintegrating multiple capabilities. Extensive real-world experiments further\nconfirm the effectiveness of our method."
    },
    {
        "date": "2025-10",
        "title": "A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew",
        "author": "Michael Ben Ali, Imen Megdiche, Andr\u00e9 Peninou, and Olivier Teste",
        "link": "http://arxiv.org/abs/2510.03380v1",
        "abstract": "Federated Learning (FL) is a decentralized paradigm that enables a\nclient-server architecture to collaboratively train a global Artificial\nIntelligence model without sharing raw data, thereby preserving privacy. A key\nchallenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of\nNon-IID, where clients hold highly heterogeneous data volumes. Clustered\nFederated Learning (CFL) is an emergent variant of FL that presents a promising\nsolution to Non-IID problem. It improves models' performance by grouping\nclients with similar data distributions into clusters. CFL methods generally\nfall into two operating strategies. In the first strategy, clients select the\ncluster that minimizes the local training loss. In the second strategy, the\nserver groups clients based on local model similarities. However, most CFL\nmethods lack systematic evaluation under QS but present significant challenges\nbecause of it. In this paper, we present two main contributions. The first one\nis an evaluation of state-of-the-art CFL algorithms under various Non-IID\nsettings, applying multiple QS scenarios to assess their robustness. Our second\ncontribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes\nan optimal coordination between both operating strategies of CFL. Our approach\nis robust against the different variations of QS settings. We conducted\nintensive experiments on six image classification datasets, resulting in 270\nNon-IID configurations. The results show that CORNFLQS achieves the highest\naverage ranking in both accuracy and clustering quality, as well as strong\nrobustness to QS perturbations. Overall, our approach outperforms actual CFL\nalgorithms."
    },
    {
        "date": "2025-10",
        "title": "InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition",
        "author": "Ahsan Farabi, Israt Khandaker, Ibrahim Khalil Shanto, Md Abdul Ahad Minhaz, and Tanisha Zaman",
        "link": "http://arxiv.org/abs/2510.03066v1",
        "abstract": "Facial Emotion Recognition (FER) is a key task in affective computing,\nenabling applications in human-computer interaction, e-learning, healthcare,\nand safety systems. Despite advances in deep learning, FER remains challenging\ndue to occlusions, illumination and pose variations, subtle intra-class\ndifferences, and dataset imbalance that hinders recognition of minority\nemotions. We present InsideOut, a reproducible FER framework built on\nEfficientNetV2-S with transfer learning, strong data augmentation, and\nimbalance-aware optimization. The approach standardizes FER2013 images, applies\nstratified splitting and augmentation, and fine-tunes a lightweight\nclassification head with class-weighted loss to address skewed distributions.\nInsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,\nshowing competitive results compared to conventional CNN baselines. The novelty\nlies in demonstrating that efficient architectures, combined with tailored\nimbalance handling, can provide practical, transparent, and reproducible FER\nsolutions."
    },
    {
        "date": "2025-10",
        "title": "Learning Robust Diffusion Models from Imprecise Supervision",
        "author": "Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, and Masashi Sugiyama",
        "link": "http://arxiv.org/abs/2510.03016v1",
        "abstract": "Conditional diffusion models have achieved remarkable success in various\ngenerative tasks recently, but their training typically relies on large-scale\ndatasets that inevitably contain imprecise information in conditional inputs.\nSuch supervision, often stemming from noisy, ambiguous, or incomplete labels,\nwill cause condition mismatch and degrade generation quality. To address this\nchallenge, we propose DMIS, a unified framework for training robust Diffusion\nModels from Imprecise Supervision, which is the first systematic study within\ndiffusion models. Our framework is derived from likelihood maximization and\ndecomposes the objective into generative and classification components: the\ngenerative component models imprecise-label distributions, while the\nclassification component leverages a diffusion classifier to infer\nclass-posterior probabilities, with its efficiency further improved by an\noptimized timestep sampling strategy. Extensive experiments on diverse forms of\nimprecise supervision, covering tasks of image generation, weakly supervised\nlearning, and noisy dataset condensation demonstrate that DMIS consistently\nproduces high-quality and class-discriminative samples."
    },
    {
        "date": "2025-10",
        "title": "Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources",
        "author": "Sara Mobsite, Renaud Hostache, Laure Berti Equille, Emmanuel Roux, and Joris Guerin",
        "link": "http://arxiv.org/abs/2510.03006v1",
        "abstract": "Supervised deep learning for land cover semantic segmentation (LCS) relies on\nlabeled satellite data. However, most existing Sentinel-2 datasets are\ncloud-free, which limits their usefulness in tropical regions where clouds are\ncommon. To properly evaluate the extent of this problem, we developed a cloud\ninjection algorithm that simulates realistic cloud cover, allowing us to test\nhow Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed\noptical imagery. We also tackle the issue of losing spatial and/or spectral\ndetails during encoder downsampling in deep networks. To mitigate this loss, we\npropose a lightweight method that injects Normalized Difference Indices (NDIs)\ninto the final decoding layers, enabling the model to retain key spatial\nfeatures with minimal additional computation. Injecting NDIs enhanced land\ncover segmentation performance on the DFC2020 dataset, yielding improvements of\n1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under\ncloud-covered conditions, incorporating Sentinel-1 data led to significant\nperformance gains across all models compared to using optical data alone,\nhighlighting the effectiveness of radar-optical fusion in challenging\natmospheric scenarios."
    },
    {
        "date": "2025-10",
        "title": "Untargeted Jailbreak Attack",
        "author": "Xinzhe Huang, Wenjing Hu, Tianhang Zheng, Kedong Xiu, Xiaojun Jia, Di Wang, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.02999v1",
        "abstract": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%."
    },
    {
        "date": "2025-10",
        "title": "External Data Extraction Attacks against Retrieval-Augmented Large Language Models",
        "author": "Yu He, Yifei Chen, Yiming Li, Shuo Shao, Leyi Qi, Boheng Li, Dacheng Tao, and Zhan Qin",
        "link": "http://arxiv.org/abs/2510.02964v1",
        "abstract": "In recent years, RAG has emerged as a key paradigm for enhancing large\nlanguage models (LLMs). By integrating externally retrieved information, RAG\nalleviates issues like outdated knowledge and, crucially, insufficient domain\nexpertise. While effective, RAG introduces new risks of external data\nextraction attacks (EDEAs), where sensitive or copyrighted data in its\nknowledge base may be extracted verbatim. These risks are particularly acute\nwhen RAG is used to customize specialized LLM applications with private\nknowledge bases. Despite initial studies exploring these risks, they often lack\na formalized framework, robust attack performance, and comprehensive\nevaluation, leaving critical questions about real-world EDEA feasibility\nunanswered.\n  In this paper, we present the first comprehensive study to formalize EDEAs\nagainst retrieval-augmented LLMs. We first formally define EDEAs and propose a\nunified framework decomposing their design into three components: extraction\ninstruction, jailbreak operator, and retrieval trigger, under which prior\nattacks can be considered instances within our framework. Guided by this\nframework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction\naTtack. Specifically, SECRET incorporates (1) an adaptive optimization process\nusing LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,\nand (2) cluster-focused triggering, an adaptive strategy that alternates\nbetween global exploration and local exploitation to efficiently generate\neffective retrieval triggers. Extensive evaluations across 4 models reveal that\nSECRET significantly outperforms previous attacks, and is highly effective\nagainst all 16 tested RAG instances. Notably, SECRET successfully extracts 35%\nof the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas\nother attacks yield 0% extraction. Our findings call for attention to this\nemerging threat."
    },
    {
        "date": "2025-10",
        "title": "SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge",
        "author": "Khaled Serag, Zhaozhou Tang, Sungwoo Kim, Vireshwar Kumar, Dave, Tian, Saman Zonouz, Raheem Beyah, Dongyan Xu, and Z. Berkay Celik",
        "link": "http://arxiv.org/abs/2510.02960v1",
        "abstract": "For decades, the Controller Area Network (CAN) has served as the primary\nin-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over\nthe past years, CAN security has been intensively scrutinized, yielding\nextensive research literature. Despite its wealth, the literature lacks\nstructured systematization, complicating efforts to assess attack severity,\ndefense efficacy, identify security gaps, or root causes. This leaves non\nexperts uncertain about the relevancy of specific attacks or defenses to their\nsystems, inadvertently portraying CAN as irredeemably insecure. Further, the\nintroduction of new IVB technologies--CAN evolutions, add-ons, and alternative\nbuses--with heightened security claims risks fostering the misconception that\nmerely adopting these technologies resolves CAN's security challenges.\n  This paper systematizes existing CAN security knowledge, presenting a\ncomprehensive taxonomy and assessment models of attackers, attacks, and\ndefenses. It identifies replicable attacks and defense gaps, investigating\ntheir root causes as inherent, accidental, unique, or universal. It then\nextrapolates these insights to emerging IVB technologies by formally analyzing\nthree emerging IVBs to identify shared root causes with CAN and assess their\nability to close security gaps. The findings challenge common perceptions,\ndemonstrating that CAN is more securable than perceived, that most insecurity\nroot causes are shared across IVBs, and that merely adopting newer IVB\ntechnology does not solve persistent security issues. The paper concludes by\nhighlighting future research directions to secure IVB communication down the\nroad."
    },
    {
        "date": "2025-10",
        "title": "Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting",
        "author": "Nikoo Naghavian, and Mostafa Tavassolipour",
        "link": "http://arxiv.org/abs/2510.02913v1",
        "abstract": "Vision-language models like CLIP demonstrate impressive zero-shot\ngeneralization but remain highly vulnerable to adversarial attacks. In this\nwork, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot\nrobustness in vision-language models. CAW consists of two components: (1) a\nConfidence-Aware loss that prioritizes uncertain adversarial examples by\nscaling the KL divergence between clean and adversarial predictions, and (2) a\nfeature alignment regularization that preserves semantic consistency by\nminimizing the distance between frozen and fine-tuned image encoder features on\nadversarial inputs. These components work jointly to improve both clean and\nrobust accuracy without sacrificing generalization. Extensive experiments on\nTinyImageNet and 14 additional datasets show that CAW outperforms recent\nmethods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while\nusing less memory."
    },
    {
        "date": "2025-10",
        "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs",
        "author": "Zhixin Xie, Xurui Song, and Jun Luo",
        "link": "http://arxiv.org/abs/2510.02833v1",
        "abstract": "Despite substantial efforts in safety alignment, recent research indicates\nthat Large Language Models (LLMs) remain highly susceptible to jailbreak\nattacks. Among these attacks, finetuning-based ones that compromise LLMs'\nsafety alignment via fine-tuning stand out due to its stable jailbreak\nperformance. In particular, a recent study indicates that fine-tuning with as\nfew as 10 harmful question-answer (QA) pairs can lead to successful\njailbreaking across various harmful questions. However, such malicious\nfine-tuning attacks are readily detectable and hence thwarted by moderation\nmodels. In this paper, we demonstrate that LLMs can be jailbroken by\nfine-tuning with only 10 benign QA pairs; our attack exploits the increased\nsensitivity of LLMs to fine-tuning data after being overfitted. Specifically,\nour fine-tuning process starts with overfitting an LLM via fine-tuning with\nbenign QA pairs involving identical refusal answers. Further fine-tuning is\nthen performed with standard benign answers, causing the overfitted LLM to\nforget the refusal attitude and thus provide compliant answers regardless of\nthe harmfulness of a question. We implement our attack on the ten LLMs and\ncompare it with five existing baselines. Experiments demonstrate that our\nmethod achieves significant advantages in both attack effectiveness and attack\nstealth. Our findings expose previously unreported security vulnerabilities in\ncurrent LLMs and provide a new perspective on understanding how LLMs' security\nis compromised, even with benign fine-tuning. Our code is available at\nhttps://github.com/ZHIXINXIE/tenBenign."
    },
    {
        "date": "2025-10",
        "title": "Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets",
        "author": "Sung Ho Jo, Seonghwi Kim, and Minwoo Chae",
        "link": "http://arxiv.org/abs/2510.02818v1",
        "abstract": "Conventional supervised learning methods are often vulnerable to spurious\ncorrelations, particularly under distribution shifts in test data. To address\nthis issue, several approaches, most notably Group DRO, have been developed.\nWhile these methods are highly robust to subpopulation or group shifts, they\nremain vulnerable to intra-group distributional shifts, which frequently occur\nin minority groups with limited samples. We propose a hierarchical extension of\nGroup DRO that addresses both inter-group and intra-group uncertainties,\nproviding robustness to distribution shifts at multiple levels. We also\nintroduce new benchmark settings that simulate realistic minority group\ndistribution shifts-an important yet previously underexplored challenge in\nspurious correlation research. Our method demonstrates strong robustness under\nthese conditions-where existing robust learning methods consistently fail-while\nalso achieving superior performance on standard benchmarks. These results\nhighlight the importance of broadening the ambiguity set to better capture both\ninter-group and intra-group distributional uncertainties."
    },
    {
        "date": "2025-10",
        "title": "Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving",
        "author": "Yifan Liao, Zhen Sun, Xiaoyun Qiu, Zixiao Zhao, Wenbing Tang, Xinlei He, Xinhu Zheng, Tianwei Zhang, Xinyi Huang, and Xingshuo Han",
        "link": "http://arxiv.org/abs/2510.02803v1",
        "abstract": "Visual Language Models (VLMs), with powerful multimodal reasoning\ncapabilities, are gradually integrated into autonomous driving by several\nautomobile manufacturers to enhance planning capability in challenging\nenvironments. However, the trajectory planning capability of VLMs in work\nzones, which often include irregular layouts, temporary traffic control, and\ndynamically changing geometric structures, is still unexplored. To bridge this\ngap, we conduct the \\textit{first} systematic study of VLMs for work zone\ntrajectory planning, revealing that mainstream VLMs fail to generate correct\ntrajectories in $68.0%$ of cases. To better understand these failures, we first\nidentify candidate patterns via subgraph mining and clustering analysis, and\nthen confirm the validity of $8$ common failure patterns through human\nverification. Building on these findings, we propose REACT-Drive, a trajectory\nplanning framework that integrates VLMs with Retrieval-Augmented Generation\n(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases\ninto constraint rules and executable trajectory planning code, while RAG\nretrieves similar patterns in new scenarios to guide trajectory generation.\nExperimental results on the ROADWork dataset show that REACT-Drive yields a\nreduction of around $3\\times$ in average displacement error relative to VLM\nbaselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the\nlowest inference time ($0.58$s) compared with other methods such as fine-tuning\n($17.90$s). We further conduct experiments using a real vehicle in 15 work zone\nscenarios in the physical world, demonstrating the strong practicality of\nREACT-Drive."
    },
    {
        "date": "2025-10",
        "title": "Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment",
        "author": "Abrar Shahid, Ibteeker Mahir Ishum, AKM Tahmidul Haque, M Sohel Rahman, and A. B. M. Alim Al Islam",
        "link": "http://arxiv.org/abs/2510.05157v1",
        "abstract": "This paper presents a controlled study of adversarial reinforcement learning\nin network security through a custom OpenAI Gym environment that models\nbrute-force attacks and reactive defenses on multi-port services. The\nenvironment captures realistic security trade-offs including background traffic\nnoise, progressive exploitation mechanics, IP-based evasion tactics, honeypot\ntraps, and multi-level rate-limiting defenses. Competing attacker and defender\nagents are trained using Deep Q-Networks (DQN) within a zero-sum reward\nframework, where successful exploits yield large terminal rewards while\nincremental actions incur small costs. Through systematic evaluation across\nmultiple configurations (varying trap detection probabilities, exploitation\ndifficulty thresholds, and training regimens), the results demonstrate that\ndefender observability and trap effectiveness create substantial barriers to\nsuccessful attacks. The experiments reveal that reward shaping and careful\ntraining scheduling are critical for learning stability in this adversarial\nsetting. The defender consistently maintains strategic advantage across 50,000+\ntraining episodes, with performance gains amplifying when exposed to complex\ndefensive strategies including adaptive IP blocking and port-specific controls.\nComplete implementation details, reproducible hyperparameter configurations,\nand architectural guidelines are provided to support future research in\nadversarial RL for cybersecurity. The zero-sum formulation and realistic\noperational constraints make this environment suitable for studying autonomous\ndefense systems, attacker-defender co-evolution, and transfer learning to\nreal-world network security scenarios."
    },
    {
        "date": "2025-10",
        "title": "Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising",
        "author": "Weimin Yuan, and Cai Meng",
        "link": "http://arxiv.org/abs/2510.02733v1",
        "abstract": "Traditional denoising methods for noise removal have largely relied on\nhandcrafted priors, often perform well in controlled environments but struggle\nto address the complexity and variability of real noise. In contrast, deep\nlearning-based approaches have gained prominence for learning noise\ncharacteristics from large datasets, but these methods frequently require\nextensive labeled data and may not generalize effectively across diverse noise\ntypes and imaging conditions. In this paper, we present an innovative method,\ntermed as Net2Net, that combines the strengths of untrained and pre-trained\nnetworks to tackle the challenges of real-world noise removal. The innovation\nof Net2Net lies in its combination of unsupervised DIP and supervised\npre-trained model DRUNet by regularization by denoising (RED). The untrained\nnetwork adapts to the unique noise characteristics of each input image without\nrequiring labeled data, while the pre-trained network leverages learned\nrepresentations from large-scale datasets to deliver robust denoising\nperformance. This hybrid framework enhances generalization across varying noise\npatterns and improves performance, particularly in scenarios with limited\ntraining data. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of our method for real-world noise removal."
    },
    {
        "date": "2025-10",
        "title": "Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering",
        "author": "Tianxiang Zhao, Youqing Wang, Jinlu Wang, Jiapu Wang, Mingliang Cui, Junbin Gao, and Jipeng Guo",
        "link": "http://arxiv.org/abs/2510.02731v1",
        "abstract": "Due to its powerful capability of self-supervised representation learning and\nclustering, contrastive attributed graph clustering (CAGC) has achieved great\nsuccess, which mainly depends on effective data augmentation and contrastive\nobjective setting. However, most CAGC methods utilize edges as auxiliary\ninformation to obtain node-level embedding representation and only focus on\nnode-level embedding augmentation. This approach overlooks edge-level embedding\naugmentation and the interactions between node-level and edge-level embedding\naugmentations across various granularity. Moreover, they often treat all\ncontrastive sample pairs equally, neglecting the significant differences\nbetween hard and easy positive-negative sample pairs, which ultimately limits\ntheir discriminative capability. To tackle these issues, a novel robust\nattributed graph clustering (RAGC), incorporating hybrid-collaborative\naugmentation (HCA) and contrastive sample adaptive-differential awareness\n(CSADA), is proposed. First, node-level and edge-level embedding\nrepresentations and augmentations are simultaneously executed to establish a\nmore comprehensive similarity measurement criterion for subsequent contrastive\nlearning. In turn, the discriminative similarity further consciously guides\nedge augmentation. Second, by leveraging pseudo-label information with high\nconfidence, a CSADA strategy is elaborately designed, which adaptively\nidentifies all contrastive sample pairs and differentially treats them by an\ninnovative weight modulation function. The HCA and CSADA modules mutually\nreinforce each other in a beneficent cycle, thereby enhancing discriminability\nin representation learning. Comprehensive graph clustering evaluations over six\nbenchmark datasets demonstrate the effectiveness of the proposed RAGC against\nseveral state-of-the-art CAGC methods."
    },
    {
        "date": "2025-10",
        "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
        "author": "Yubo Li, Ramayya Krishnan, and Rema Padman",
        "link": "http://arxiv.org/abs/2510.02712v1",
        "abstract": "Large Language Models (LLMs) have revolutionized conversational AI, yet their\nrobustness in extended multi-turn dialogues remains poorly understood. Existing\nevaluation frameworks focus on static benchmarks and single-turn assessments,\nfailing to capture the temporal dynamics of conversational degradation that\ncharacterize real-world interactions. In this work, we present the first\ncomprehensive survival analysis of conversational AI robustness, analyzing\n36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a\ntime-to-event process. Our survival modeling framework-employing Cox\nproportional hazards, Accelerated Failure Time, and Random Survival Forest\napproaches-reveals extraordinary temporal dynamics. We find that abrupt,\nprompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing\nthe hazard of conversational failure. In stark contrast, gradual, cumulative\ndrift is highly protective, vastly reducing the failure hazard and enabling\nsignificantly longer dialogues. AFT models with interactions demonstrate\nsuperior performance, achieving excellent discrimination and exceptional\ncalibration. These findings establish survival analysis as a powerful paradigm\nfor evaluating LLM robustness, offer concrete insights for designing resilient\nconversational agents, and challenge prevailing assumptions about the necessity\nof semantic consistency in conversational AI Systems."
    },
    {
        "date": "2025-10",
        "title": "A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison",
        "author": "Chinthana Wimalasuriya, and Spyros Tragoudas",
        "link": "http://arxiv.org/abs/2510.02707v1",
        "abstract": "Adversarial attacks present a significant threat to modern machine learning\nsystems. Yet, existing detection methods often lack the ability to detect\nunseen attacks or detect different attack types with a high level of accuracy.\nIn this work, we propose a statistical approach that establishes a detection\nbaseline before a neural network's deployment, enabling effective real-time\nadversarial detection. We generate a metric of adversarial presence by\ncomparing the behavior of a compressed/uncompressed neural network pair. Our\nmethod has been tested against state-of-the-art techniques, and it achieves\nnear-perfect detection across a wide range of attack types. Moreover, it\nsignificantly reduces false positives, making it both reliable and practical\nfor real-world applications."
    },
    {
        "date": "2025-10",
        "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks",
        "author": "Zhaorun Chen, Xun Liu, Mintong Kang, Jiawei Zhang, Minzhou Pan, Shuang Yang, and Bo Li",
        "link": "http://arxiv.org/abs/2510.02677v1",
        "abstract": "As vision-language models (VLMs) gain prominence, their multimodal interfaces\nalso introduce new safety vulnerabilities, making the safety evaluation\nchallenging and critical. Existing red-teaming efforts are either restricted to\na narrow set of adversarial patterns or depend heavily on manual engineering,\nlacking scalable exploration of emerging real-world VLM vulnerabilities. To\nbridge this gap, we propose ARMs, an adaptive red-teaming agent that\nsystematically conducts comprehensive risk assessments for VLMs. Given a target\nharmful behavior or risk definition, ARMs automatically optimizes diverse\nred-teaming strategies with reasoning-enhanced multi-step orchestration, to\neffectively elicit harmful outputs from target VLMs. We propose 11 novel\nmultimodal attack strategies, covering diverse adversarial patterns of VLMs\n(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming\nalgorithms into ARMs via model context protocol (MCP). To balance the diversity\nand effectiveness of the attack, we design a layered memory with an\nepsilon-greedy attack exploration algorithm. Extensive experiments on instance-\nand policy-based benchmarks show that ARMs achieves SOTA attack success rates,\nexceeding baselines by an average of 52.1% and surpassing 90% on\nClaude-4-Sonnet. We show that the diversity of red-teaming instances generated\nby ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.\nLeveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety\ndataset comprising over 30K red-teaming instances spanning 51 diverse risk\ncategories, grounded in both real-world multimodal threats and regulatory\nrisks. Safety fine-tuning with ARMs-Bench substantially improves the robustness\nof VLMs while preserving their general utility, providing actionable guidance\nto improve multimodal safety alignment against emerging threats."
    },
    {
        "date": "2025-10",
        "title": "Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles",
        "author": "Abhishek Joshi, Jahnavi Krishna Koda, and Abhishek Phadke",
        "link": "http://arxiv.org/abs/2510.02642v1",
        "abstract": "Traffic light and sign recognition are key for Autonomous Vehicles (AVs)\nbecause perception mistakes directly influence navigation and safety. In\naddition to digital adversarial attacks, models are vulnerable to existing\nperturbations (glare, rain, dirt, or graffiti), which could lead to dangerous\nmisclassifications. The current work lacks consideration of temporal\ncontinuity, multistatic field-of-view (FoV) sensing, and robustness to both\ndigital and natural degradation. This study proposes a dual FoV,\nsequence-preserving robustness framework for traffic lights and signs in the\nUSA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and\nself-recorded videos from the region of Texas. Mid and long-term sequences of\nRGB images are temporally aligned for four operational design domains (ODDs):\nhighway, night, rainy, and urban. Over a series of experiments on a real-life\napplication of anomaly detection, this study outlines a unified three-layer\ndefense stack framework that incorporates feature squeezing, defensive\ndistillation, and entropy-based anomaly detection, as well as sequence-wise\ntemporal voting for further enhancement. The evaluation measures included\naccuracy, attack success rate (ASR), risk-weighted misclassification severity,\nand confidence stability. Physical transferability was confirmed using probes\nfor recapture. The results showed that the Unified Defense Stack achieved\n79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and\nBEVFormer, while reducing the high-risk misclassification to 32%."
    },
    {
        "date": "2025-10",
        "title": "ToolTweak: An Attack on Tool Selection in LLM-based Agents",
        "author": "Jonathan Sneh, Ruomei Yan, Jialin Yu, Philip Torr, Yarin Gal, Sunando Sengupta, Eric Sommerlade, Alasdair Paren, and Adel Bibi",
        "link": "http://arxiv.org/abs/2510.02554v1",
        "abstract": "As LLMs increasingly power agents that interact with external tools, tool use\nhas become an essential mechanism for extending their capabilities. These\nagents typically select tools from growing databases or marketplaces to solve\nuser tasks, creating implicit competition among tool providers and developers\nfor visibility and usage. In this paper, we show that this selection process\nharbors a critical vulnerability: by iteratively manipulating tool names and\ndescriptions, adversaries can systematically bias agents toward selecting\nspecific tools, gaining unfair advantage over equally capable alternatives. We\npresent ToolTweak, a lightweight automatic attack that increases selection\nrates from a baseline of around 20% to as high as 81%, with strong\ntransferability between open-source and closed-source models. Beyond individual\ntools, we show that such attacks cause distributional shifts in tool usage,\nrevealing risks to fairness, competition, and security in emerging tool\necosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and\nperplexity filtering, which reduce bias and lead agents to select functionally\nsimilar tools more equally. All code will be open-sourced upon acceptance."
    },
    {
        "date": "2025-10",
        "title": "TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT",
        "author": "Atonu Ghosh, Akhilesh Mohanasundaram, Srishivanth R F, and Sudip Misra",
        "link": "http://arxiv.org/abs/2510.02519v1",
        "abstract": "We present TLoRa, an end-to-end architecture for HTTPS communication over\nLoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables\na seamless and secure communication channel between WiFi-enabled end devices\nand the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH\ntethers a WiFi hotspot and a captive portal for user devices to connect and\nrequest URLs. The EH forwards the requested URLs to the NR using a secure\ntunnel over LoRa. The NR, which acts as a server-side proxy, receives and\nresolves the request from the Internet-based server. It then relays back the\nencrypted response from the server over the same secure tunnel. TLoRa operates\nin three phases -session setup, secure tunneling, and rendering. In the first\nphase, it manages the TCP socket and initiates the TLS handshake. In the\nsecond, it creates a secure tunnel and transfers encrypted TLS data over LoRa.\nFinally, it delivers the URL content to the user. TLoRa also implements a\nlightweight TLS record reassembly layer and a queuing mechanism for session\nmultiplexing. We evaluate TLoRa on real hardware using multiple accesses to a\nweb API. Results indicate that it provides a practical solution by successfully\nestablishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to\nfulfill API requests. To the best of our knowledge, this is the first work to\ncomprehensively design, implement, and evaluate the performance of HTTPS access\nover LoRa using full TLS."
    },
    {
        "date": "2025-10",
        "title": "A Bilevel Optimization Framework for Adversarial Control of Gas Pipeline Operations",
        "author": "Tejaswini Sanjay Katale, Lu Gao, Yunpeng Zhang, and Alaa Senouci",
        "link": "http://arxiv.org/abs/2510.02503v1",
        "abstract": "Cyberattacks on pipeline operational technology systems pose growing risks to\nenergy infrastructure. This study develops a physics-informed simulation and\noptimization framework for analyzing cyber-physical threats in petroleum\npipeline networks. The model integrates networked hydraulic dynamics,\nSCADA-based state estimation, model predictive control (MPC), and a bi-level\nformulation for stealthy false-data injection (FDI) attacks. Pipeline flow and\npressure dynamics are modeled on a directed graph using nodal pressure\nevolution and edge-based Weymouth-type relations, including control-aware\nequipment such as valves and compressors. An extended Kalman filter estimates\nthe full network state from partial SCADA telemetry. The controller computes\npressure-safe control inputs via MPC under actuator constraints and forecasted\ndemands. Adversarial manipulation is formalized as a bi-level optimization\nproblem where an attacker perturbs sensor data to degrade throughput while\nremaining undetected by bad-data detectors. This attack-control interaction is\nsolved via Karush-Kuhn-Tucker (KKT) reformulation, which results in a tractable\nmixed-integer quadratic program. Test gas pipeline case studies demonstrate the\ncovert reduction of service delivery under attack. Results show that\nundetectable attacks can cause sustained throughput loss with minimal\ninstantaneous deviation. This reveals the need for integrated detection and\ncontrol strategies in cyber-physical infrastructure."
    },
    {
        "date": "2025-10",
        "title": "Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking",
        "author": "Shaifalee Saxena, Alan Williams, Rafael Fierro, and Alexander Scheinker",
        "link": "http://arxiv.org/abs/2510.02490v1",
        "abstract": "In this paper, we study the use of robust model independent bounded extremum\nseeking (ES) feedback control to improve the robustness of deep reinforcement\nlearning (DRL) controllers for a class of nonlinear time-varying systems. DRL\nhas the potential to learn from large datasets to quickly control or optimize\nthe outputs of many-parameter systems, but its performance degrades\ncatastrophically when the system model changes rapidly over time. Bounded ES\ncan handle time-varying systems with unknown control directions, but its\nconvergence speed slows down as the number of tuned parameters increases and,\nlike all local adaptive methods, it can get stuck in local minima. We\ndemonstrate that together, DRL and bounded ES result in a hybrid controller\nwhose performance exceeds the sum of its parts with DRL taking advantage of\nhistorical data to learn how to quickly control a many-parameter system to a\ndesired setpoint while bounded ES ensures its robustness to time variations. We\npresent a numerical study of a general time-varying system and a combined\nES-DRL controller for automatic tuning of the Low Energy Beam Transport section\nat the Los Alamos Neutron Science Center linear particle accelerator."
    },
    {
        "date": "2025-10",
        "title": "Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems",
        "author": "Ahmed Danladi Abdullahi, Erfan Bahrami, Tooska Dargahi, Mohammed Al-Khalidi, and Mohammad Hammoudeh",
        "link": "http://arxiv.org/abs/2510.02487v1",
        "abstract": "The advancement of 6G technology has the potential to revolutionize the\ntransportation sector and significantly improve how we travel. 6G-enabled\nIntelligent Transportation Systems (ITS) promise to offer high-speed,\nlow-latency communication and advanced data analytics capabilities, supporting\nthe development of safer, more efficient, and more sustainable transportation\nsolutions. However, various security and privacy challenges were identified in\nthe literature that must be addressed to enable the safe and secure deployment\nof 6G-ITS and ensure people's trust in using these technologies. This paper\nreviews the opportunities and challenges of 6G-ITS, particularly focusing on\ntrust, security, and privacy, with special attention to quantum technologies\nthat both enhance security through quantum key distribution and introduce new\nvulnerabilities. It discusses the potential benefits of 6G technology in the\ntransportation sector, including improved communication, device\ninteroperability support, data analytic capabilities, and increased automation\nfor different components, such as transportation management and communication\nsystems. A taxonomy of different attack models in 6G-ITS is proposed, and a\ncomparison of the security threats in 5G-ITS and 6G-ITS is provided, along with\npotential mitigating solutions. This research highlights the urgent need for a\ncomprehensive, multi-layered security framework spanning physical\ninfrastructure protection, network protocol security, data management\nsafeguards, application security measures, and trust management systems to\neffectively mitigate emerging security and privacy risks and ensure the\nintegrity and resilience of future transportation ecosystems."
    },
    {
        "date": "2025-10",
        "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
        "author": "Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, and Wei-Chen Chiu",
        "link": "http://arxiv.org/abs/2510.02314v1",
        "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/"
    },
    {
        "date": "2025-10",
        "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
        "author": "Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, and Alexander Cloninger",
        "link": "http://arxiv.org/abs/2510.02308v1",
        "abstract": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation."
    },
    {
        "date": "2025-10",
        "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks",
        "author": "Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, and Dan Roth",
        "link": "http://arxiv.org/abs/2510.02286v1",
        "abstract": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns."
    },
    {
        "date": "2025-10",
        "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning",
        "author": "Yalin E. Sagduyu, Tugba Erpek, Kemal Davaslioglu, and Sastry Kompella",
        "link": "http://arxiv.org/abs/2510.02265v1",
        "abstract": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime."
    },
    {
        "date": "2025-10",
        "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks",
        "author": "Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, and Chadi Assi",
        "link": "http://arxiv.org/abs/2510.02236v1",
        "abstract": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense",
        "author": "Basil Abdullah AL-Zahrani",
        "link": "http://arxiv.org/abs/2510.02424v1",
        "abstract": "This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive\ndeception framework achieving 99.88% detection rate with 0.13% false positive\nrate on the CICIDS2017 dataset. The framework employs ensemble machine learning\n(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to\nidentify and adapt responses to network intrusions. Through a coordinated\nsignal bus architecture, security components share real-time intelligence,\nenabling collective decision-making. The system profiles attackers based on\ntemporal patterns and deploys customized deception strategies across five\nescalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates\nthat CADL significantly outperforms traditional intrusion detection systems\n(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false\npositive rates. The framework's behavioral analysis achieves 89% accuracy in\nclassifying attacker profiles. We provide open-source implementation and\ntransparent performance metrics, offering an accessible alternative to\ncommercial deception platforms costing $150-400 per host annually."
    },
    {
        "date": "2025-10",
        "title": "Authentication Security of PRF GNSS Ranging",
        "author": "Jason Anderson",
        "link": "http://arxiv.org/abs/2510.02196v1",
        "abstract": "This work derives the authentication security of pseudorandom function (PRF)\nGNSS ranging under multiple GNSS spoofing models, including the Security Code\nEstimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF\nutilizing a secret known only to the broadcaster, the spoofer cannot predict\nthe ranging code before broadcast. Therefore, PRF ranging can be used to\nestablish trust in the GNSS pseudoranges and the resulting receiver position,\nnavigation, and timing (PNT) solution. I apply the methods herein to Galileo's\nSignal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal\nto compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit\nauthentication security under non-SCER models. For the SCER adversary, I\npredict the adversary's needed receiving radio equipment to break\nauthentication security. One can use this work to design a PRF GNSS ranging\nprotocol to meet useful authentication security requirements by computing the\nprobability of missed detection."
    },
    {
        "date": "2025-10",
        "title": "Dynamic Target Attack",
        "author": "Kedong Xiu, Churui Zeng, Tianhang Zheng, Xinzhe Huang, Xiaojun Jia, Di Wang, Puning Zhao, Zhan Qin, and Kui Ren",
        "link": "http://arxiv.org/abs/2510.02422v1",
        "abstract": "Existing gradient-based jailbreak attacks typically optimize an adversarial\nsuffix to induce a fixed affirmative response. However, this fixed target\nusually resides in an extremely low-density region of a safety-aligned LLM's\noutput distribution conditioned on diverse harmful inputs. Due to the\nsubstantial discrepancy between the target and the original output, existing\nattacks require numerous iterations to optimize the adversarial prompt, which\nmight still fail to induce the low-probability target response from the target\nLLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking\nframework relying on the target LLM's own responses as targets to optimize the\nadversarial prompts. In each optimization round, DTA iteratively samples\nmultiple candidate responses directly from the output distribution conditioned\non the current prompt, and selects the most harmful response as a temporary\ntarget for prompt optimization. In contrast to existing attacks, DTA\nsignificantly reduces the discrepancy between the target and the output\ndistribution, substantially easing the optimization process to search for an\neffective adversarial prompt.\n  Extensive experiments demonstrate the superior effectiveness and efficiency\nof DTA: under the white-box setting, DTA only needs 200 optimization iterations\nto achieve an average attack success rate (ASR) of over 87\\% on recent\nsafety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\\%. The\ntime cost of DTA is 2-26 times less than existing baselines. Under the\nblack-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target\nsampling and achieves an ASR of 85\\% against the black-box target model\nLlama-3-70B-Instruct, exceeding its counterparts by over 25\\%."
    },
    {
        "date": "2025-10",
        "title": "NoMod: A Non-modular Attack on Module Learning With Errors",
        "author": "Cristian Bassotto, Ermes Franch, Marina Kr\u010dek, and Stjepan Picek",
        "link": "http://arxiv.org/abs/2510.02162v1",
        "abstract": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4."
    },
    {
        "date": "2025-10",
        "title": "Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems",
        "author": "Junjie Su, Weifei Jin, Yuxin Cao, Derui Wang, Kai Ye, and Jie Hao",
        "link": "http://arxiv.org/abs/2510.02158v1",
        "abstract": "Sound Event Detection (SED) systems are increasingly deployed in\nsafety-critical applications such as industrial monitoring and audio\nsurveillance. However, their robustness against adversarial attacks has not\nbeen well explored. Existing audio adversarial attacks targeting SED systems,\nwhich incorporate both detection and localization capabilities, often lack\neffectiveness due to SED's strong contextual dependencies or lack precision by\nfocusing solely on misclassifying the target region as the target event,\ninadvertently affecting non-target regions. To address these challenges, we\npropose the Mirage and Mute Attack (M2A) framework, which is designed for\ntargeted adversarial attacks on polyphonic SED systems. In our optimization\nprocess, we impose specific constraints on the non-target output, which we\nrefer to as preservation loss, ensuring that our attack does not alter the\nmodel outputs for non-target region, thus achieving precise attacks.\nFurthermore, we introduce a novel evaluation metric Editing Precison (EP) that\nbalances effectiveness and precision, enabling our method to simultaneously\nenhance both. Comprehensive experiments show that M2A achieves 94.56% and\n99.11% EP on two state-of-the-art SED models, demonstrating that the framework\nis sufficiently effective while significantly enhancing attack precision."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference",
        "author": "Benjamin Wiriyapong, Oktay Karaku\u015f, and Kirill Sidorov",
        "link": "http://arxiv.org/abs/2510.02056v1",
        "abstract": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias."
    },
    {
        "date": "2025-10",
        "title": "Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions",
        "author": "Camilo Andr\u00e9s Garc\u00eda Trillos, and Nicol\u00e1s Garc\u00eda Trillos",
        "link": "http://arxiv.org/abs/2510.01969v1",
        "abstract": "We consider adversarially robust classification in a multiclass setting under\narbitrary loss functions and derive dual and barycentric reformulations of the\ncorresponding learner-agnostic robust risk minimization problem. We provide\nexplicit characterizations for important cases such as the cross-entropy loss,\nloss functions with a power form, and the quadratic loss, extending in this way\navailable results for the 0-1 loss. These reformulations enable efficient\ncomputation of sharp lower bounds for adversarial risks and facilitate the\ndesign of robust classifiers beyond the 0-1 loss setting. Our paper uncovers\ninteresting connections between adversarial robustness, $\\alpha$-fair packing\nproblems, and generalized barycenter problems for arbitrary positive measures\nwhere Kullback-Leibler and Tsallis entropies are used as penalties. Our\ntheoretical results are accompanied with illustrative numerical experiments\nwhere we obtain tighter lower bounds for adversarial risks with the\ncross-entropy loss function."
    },
    {
        "date": "2025-10",
        "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement",
        "author": "Zhaoyan Wang, Zheng Gao, Arogya Kharel, and In-Young Ko",
        "link": "http://arxiv.org/abs/2510.01910v1",
        "abstract": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement."
    },
    {
        "date": "2025-10",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
        "author": "Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, and Xuming Ran",
        "link": "http://arxiv.org/abs/2510.01879v1",
        "abstract": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs."
    },
    {
        "date": "2025-10",
        "title": "Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.01780v1",
        "abstract": "Secure and interoperable integration of heterogeneous medical data remains a\ngrand challenge in digital health. Current federated learning (FL) frameworks\noffer privacy-preserving model training but lack standardized mechanisms to\norchestrate multi-modal data fusion across distributed and resource-constrained\nenvironments. This study introduces a novel framework that leverages the Model\nContext Protocol (MCP) as an interoperability layer for secure, cross-agent\ncommunication in multi-modal federated healthcare systems. The proposed\narchitecture unifies three pillars: (i) multi-modal feature alignment for\nclinical imaging, electronic medical records, and wearable IoT data; (ii)\nsecure aggregation with differential privacy to protect patient-sensitive\nupdates; and (iii) energy-aware scheduling to mitigate dropouts in mobile\nclients. By employing MCP as a schema-driven interface, the framework enables\nadaptive orchestration of AI agents and toolchains while ensuring compliance\nwith privacy regulations. Experimental evaluation on benchmark datasets and\npilot clinical cohorts demonstrates up to 9.8\\% improvement in diagnostic\naccuracy compared with baseline FL, a 54\\% reduction in client dropout rates,\nand clinically acceptable privacy--utility trade-offs. These results highlight\nMCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward\nequitable, next-generation federated health infrastructures."
    },
    {
        "date": "2025-10",
        "title": "Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks",
        "author": "Bruno Corcuera, Carlos Eiras-Franco, and Brais Cancela",
        "link": "http://arxiv.org/abs/2510.01758v1",
        "abstract": "Latent representations are critical for the performance and robustness of\nmachine learning models, as they encode the essential features of data in a\ncompact and informative manner. However, in vision tasks, these representations\nare often affected by noisy or irrelevant features, which can degrade the\nmodel's performance and generalization capabilities. This paper presents a\nnovel approach for enhancing latent representations using unsupervised Dynamic\nFeature Selection (DFS). For each instance, the proposed method identifies and\nremoves misleading or redundant information in images, ensuring that only the\nmost relevant features contribute to the latent space. By leveraging an\nunsupervised framework, our approach avoids reliance on labeled data, making it\nbroadly applicable across various domains and datasets. Experiments conducted\non image datasets demonstrate that models equipped with unsupervised DFS\nachieve significant improvements in generalization performance across various\ntasks, including clustering and image generation, while incurring a minimal\nincrease in the computational cost."
    },
    {
        "date": "2025-10",
        "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation",
        "author": "Saptarshi Mandal, Yashaswini Murthy, and R. Srikant",
        "link": "http://arxiv.org/abs/2510.01721v1",
        "abstract": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation."
    },
    {
        "date": "2025-10",
        "title": "Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations",
        "author": "Yue Li, Linying Xue, Dongdong Lin, Qiushi Li, Hui Tian, and Hongxia Wang",
        "link": "http://arxiv.org/abs/2510.01699v1",
        "abstract": "With the flourishing prosperity of generative models, manipulated facial\nimages have become increasingly accessible, raising concerns regarding privacy\ninfringement and societal trust. In response, proactive defense strategies\nembed adversarial perturbations into facial images to counter deepfake\nmanipulation. However, existing methods often face a tradeoff between\nimperceptibility and defense effectiveness-strong perturbations may disrupt\nforgeries but degrade visual fidelity. Recent studies have attempted to address\nthis issue by introducing additional visual loss constraints, yet often\noverlook the underlying gradient conflicts among losses, ultimately weakening\ndefense performance. To bridge the gap, we propose a gradient-projection-based\nadversarial proactive defense (GRASP) method that effectively counters facial\ndeepfakes while minimizing perceptual degradation. GRASP is the first approach\nto successfully integrate both structural similarity loss and low-frequency\nloss to enhance perturbation imperceptibility. By analyzing gradient conflicts\nbetween defense effectiveness loss and visual quality losses, GRASP pioneers\nthe design of the gradient-projection mechanism to mitigate these conflicts,\nenabling balanced optimization that preserves image fidelity without\nsacrificing defensive performance. Extensive experiments validate the efficacy\nof GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense\nsuccess rate against facial attribute manipulations, significantly\noutperforming existing approaches in visual quality."
    },
    {
        "date": "2025-10",
        "title": "Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis",
        "author": "Han Wu, Yanming Sun, Yunhe Yang, and Derek F. Wong",
        "link": "http://arxiv.org/abs/2510.01677v1",
        "abstract": "Multimodal sentiment analysis (MSA) leverages information fusion from diverse\nmodalities (e.g., text, audio, visual) to enhance sentiment prediction.\nHowever, simple fusion techniques often fail to account for variations in\nmodality quality, such as those that are noisy, missing, or semantically\nconflicting. This oversight leads to suboptimal performance, especially in\ndiscerning subtle emotional nuances. To mitigate this limitation, we introduce\na simple yet efficient \\textbf{A}daptive \\textbf{G}ated \\textbf{F}usion\n\\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion\nmechanism based on information entropy and modality importance. This mechanism\nmitigates the influence of noisy modalities and prioritizes informative cues\nfollowing unimodal encoding and cross-modal interaction. Experiments on\nCMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong\nbaselines in accuracy, effectively discerning subtle emotions with robust\nperformance. Visualization analysis of feature representations demonstrates\nthat AGFN enhances generalization by learning from a broader feature\ndistribution, achieved by reducing the correlation between feature location and\nprediction error, thereby decreasing reliance on specific locations and\ncreating more robust multimodal feature representations."
    },
    {
        "date": "2025-10",
        "title": "Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks",
        "author": "Milad Nasr, Yanick Fratantonio, Luca Invernizzi, Ange Albertini, Loua Farah, Alex Petit-Bianco, Andreas Terzis, Kurt Thomas, Elie Bursztein, and Nicholas Carlini",
        "link": "http://arxiv.org/abs/2510.01676v1",
        "abstract": "As deep learning models become widely deployed as components within larger\nproduction systems, their individual shortcomings can create system-level\nvulnerabilities with real-world impact. This paper studies how adversarial\nattacks targeting an ML component can degrade or bypass an entire\nproduction-grade malware detection system, performing a case study analysis of\nGmail's pipeline where file-type identification relies on a ML model.\n  The malware detection pipeline in use by Gmail contains a machine learning\nmodel that routes each potential malware sample to a specialized malware\nclassifier to improve accuracy and performance. This model, called Magika, has\nbeen open sourced. By designing adversarial examples that fool Magika, we can\ncause the production malware service to incorrectly route malware to an\nunsuitable malware detector thereby increasing our chance of evading detection.\nSpecifically, by changing just 13 bytes of a malware sample, we can\nsuccessfully evade Magika in 90% of cases and thereby allow us to send malware\nfiles over Gmail. We then turn our attention to defenses, and develop an\napproach to mitigate the severity of these types of attacks. For our defended\nproduction model, a highly resourced adversary requires 50 bytes to achieve\njust a 20% attack success rate. We implement this defense, and, thanks to a\ncollaboration with Google engineers, it has already been deployed in production\nfor the Gmail classifier."
    },
    {
        "date": "2025-10",
        "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction",
        "author": "Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, and Sida Peng",
        "link": "http://arxiv.org/abs/2510.01669v2",
        "abstract": "This paper tackles the challenge of robust reconstruction, i.e., the task of\nreconstructing a 3D scene from a set of inconsistent multi-view images. Some\nrecent works have attempted to simultaneously remove image inconsistencies and\nperform reconstruction by integrating image degradation modeling into neural 3D\nscene representations. However, these methods rely heavily on dense\nobservations for robustly optimizing model parameters. To address this issue,\nwe propose to decouple robust reconstruction into two subtasks: restoration and\nreconstruction, which naturally simplifies the optimization process. To this\nend, we introduce UniVerse, a unified framework for robust reconstruction based\non a video diffusion model. Specifically, UniVerse first converts inconsistent\nimages into initial videos, then uses a specially designed video diffusion\nmodel to restore them into consistent images, and finally reconstructs the 3D\nscenes from these restored images. Compared with case-by-case per-view\ndegradation modeling, the diffusion model learns a general scene prior from\nlarge-scale data, making it applicable to diverse image inconsistencies.\nExtensive experiments on both synthetic and real-world datasets demonstrate the\nstrong generalization capability and superior performance of our method in\nrobust reconstruction. Moreover, UniVerse can control the style of the\nreconstructed 3D scene. Project page:\nhttps://jin-cao-tma.github.io/UniVerse.github.io/"
    },
    {
        "date": "2025-10",
        "title": "SoK: Measuring What Matters for Closed-Loop Security Agents",
        "author": "Mudita Khurana, and Raunak Jain",
        "link": "http://arxiv.org/abs/2510.01654v1",
        "abstract": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents."
    },
    {
        "date": "2025-10",
        "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
        "author": "Changmin Lee, Jihyun Lee, and Tae-Kyun Kim",
        "link": "http://arxiv.org/abs/2510.01619v1",
        "abstract": "While there has been significant progress in the field of 3D avatar creation\nfrom visual observations, modeling physically plausible dynamics of humans with\nloose garments remains a challenging problem. Although a few existing works\naddress this problem by leveraging physical simulation, they suffer from\nlimited accuracy or robustness to novel animation inputs. In this work, we\npresent MPMAvatar, a framework for creating 3D human avatars from multi-view\nvideos that supports highly realistic, robust animation, as well as\nphotorealistic rendering from free viewpoints. For accurate and robust dynamics\nmodeling, our key idea is to use a Material Point Method-based simulator, which\nwe carefully tailor to model garments with complex deformations and contact\nwith the underlying body by incorporating an anisotropic constitutive model and\na novel collision handling algorithm. We combine this dynamics modeling scheme\nwith our canonical avatar that can be rendered using 3D Gaussian Splatting with\nquasi-shadowing, enabling high-fidelity rendering for physically realistic\nanimations. In our experiments, we demonstrate that MPMAvatar significantly\noutperforms the existing state-of-the-art physics-based avatar in terms of (1)\ndynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and\nefficiency. Additionally, we present a novel application in which our avatar\ngeneralizes to unseen interactions in a zero-shot manner-which was not\nachievable with previous learning-based methods due to their limited simulation\ngeneralizability. Our project page is at:\nhttps://KAISTChangmin.github.io/MPMAvatar/"
    },
    {
        "date": "2025-10",
        "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness",
        "author": "Youwei Bao, Shuhan Yang, and Hyunsoo Yang",
        "link": "http://arxiv.org/abs/2510.01598v1",
        "abstract": "Deterministic pseudo random number generators (PRNGs) used in generative\nartificial intelligence (GAI) models produce predictable patterns vulnerable to\nexploitation by attackers. Conventional defences against the vulnerabilities\noften come with significant energy and latency overhead. Here, we embed\nhardware-generated true random bits from spin-transfer torque magnetic tunnel\njunctions (STT-MTJs) to address the challenges. A highly parallel,\nFPGA-assisted prototype computing system delivers megabit-per-second true\nrandom numbers, passing NIST randomness tests after in-situ operations with\nminimal overhead. Integrating the hardware random bits into a generative\nadversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to\n18.6 times compared to the low-quality random number generators (RNG) baseline.\nWith nanosecond switching speed, high energy efficiency, and established\nscalability, our STT-MTJ-based system holds the potential to scale beyond 106\nparallel cells, achieving gigabit-per-second throughput suitable for large\nlanguage model sampling. This advancement highlights spintronic RNGs as\npractical security components for next-generation GAI systems."
    },
    {
        "date": "2025-10",
        "title": "Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation",
        "author": "Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, and Hairong Lv",
        "link": "http://arxiv.org/abs/2510.01588v1",
        "abstract": "Parkinson's disease (PD) is one of the most common neurodegenerative\ndisorder. PD telemonitoring emerges as a novel assessment modality enabling\nself-administered at-home tests of Unified Parkinson's Disease Rating Scale\n(UPDRS) scores, enhancing accessibility for PD patients. However, three types\nof noise would occur during measurements: (1) patient-induced measurement\ninaccuracies, (2) environmental noise, and (3) data packet loss during\ntransmission, resulting in higher prediction errors. To address these\nchallenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,\nthe original speech features are grouped into ordered bins, based on the\ncontinuous values of a selected feature, to construct contrastive pairs.\nSecond, the contrastive pairs are employed to train a multilayer perceptron\nencoder for generating noise-robust features. Finally, these features are\nconcatenated with the original features as the augmented features, which are\nthen fed into the UPDRS prediction models. Notably, we further introduces a\nnovel evaluation approach with customizable noise injection module, and\nextensive experiments show that NoRo can successfully enhance the noise\nrobustness of UPDRS prediction across various downstream prediction models\nunder different noisy environments."
    },
    {
        "date": "2025-10",
        "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
        "author": "Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, and Han Liu",
        "link": "http://arxiv.org/abs/2510.01586v1",
        "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead."
    },
    {
        "date": "2025-10",
        "title": "Robust Classification of Oral Cancer with Limited Training Data",
        "author": "Akshay Bhagwan Sonawane, Lena D. Swamikannan, and Lakshman Tamil",
        "link": "http://arxiv.org/abs/2510.01547v1",
        "abstract": "Oral cancer ranks among the most prevalent cancers globally, with a\nparticularly high mortality rate in regions lacking adequate healthcare access.\nEarly diagnosis is crucial for reducing mortality; however, challenges persist\ndue to limited oral health programs, inadequate infrastructure, and a shortage\nof healthcare practitioners. Conventional deep learning models, while\npromising, often rely on point estimates, leading to overconfidence and reduced\nreliability. Critically, these models require large datasets to mitigate\noverfitting and ensure generalizability, an unrealistic demand in settings with\nlimited training data. To address these issues, we propose a hybrid model that\ncombines a convolutional neural network (CNN) with Bayesian deep learning for\noral cancer classification using small training sets. This approach employs\nvariational inference to enhance reliability through uncertainty\nquantification. The model was trained on photographic color images captured by\nsmartphones and evaluated on three distinct test datasets. The proposed method\nachieved 94% accuracy on a test dataset with a distribution similar to that of\nthe training data, comparable to traditional CNN performance. Notably, for\nreal-world photographic image data, despite limitations and variations\ndiffering from the training dataset, the proposed model demonstrated superior\ngeneralizability, achieving 88% accuracy on diverse datasets compared to 72.94%\nfor traditional CNNs, even with a smaller dataset. Confidence analysis revealed\nthat the model exhibits low uncertainty (high confidence) for correctly\nclassified samples and high uncertainty (low confidence) for misclassified\nsamples. These results underscore the effectiveness of Bayesian inference in\ndata-scarce environments in enhancing early oral cancer diagnosis by improving\nmodel reliability and generalizability."
    },
    {
        "date": "2025-10",
        "title": "Information Seeking for Robust Decision Making under Partial Observability",
        "author": "Djengo Cyun-Jyun Fang, and Tsung-Wei Ke",
        "link": "http://arxiv.org/abs/2510.01531v1",
        "abstract": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io"
    },
    {
        "date": "2025-10",
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "author": "Isha Gupta, Rylan Schaeffer, Joshua Kazdan, Ken Ziyu Liu, and Sanmi Koyejo",
        "link": "http://arxiv.org/abs/2510.01494v2",
        "abstract": "The field of adversarial robustness has long established that adversarial\nexamples can successfully transfer between image classifiers and that text\njailbreaks can successfully transfer between language models (LMs). However, a\npair of recent studies reported being unable to successfully transfer image\njailbreaks between vision-language models (VLMs). To explain this striking\ndifference, we propose a fundamental distinction regarding the transferability\nof attacks against machine learning models: attacks in the input data-space can\ntransfer, whereas attacks in model representation space do not, at least not\nwithout geometric alignment of representations. We then provide theoretical and\nempirical evidence of this hypothesis in four different settings. First, we\nmathematically prove this distinction in a simple setting where two networks\ncompute the same input-output map but via different representations. Second, we\nconstruct representation-space attacks against image classifiers that are as\nsuccessful as well-known data-space attacks, but fail to transfer. Third, we\nconstruct representation-space attacks against LMs that successfully jailbreak\nthe attacked models but again fail to transfer. Fourth, we construct data-space\nattacks against VLMs that successfully transfer to new VLMs, and we show that\nrepresentation space attacks can transfer when VLMs' latent geometries are\nsufficiently aligned in post-projector space. Our work reveals that adversarial\ntransfer is not an inherent property of all attacks but contingent on their\noperational domain - the shared data-space versus models' unique representation\nspaces - a critical insight for building more robust models."
    },
    {
        "date": "2025-10",
        "title": "Securing IoT Devices in Smart Cities: A Review of Proposed Solutions",
        "author": "Andr\u00e9s F. Betancur-L\u00f3pez",
        "link": "http://arxiv.org/abs/2510.01445v1",
        "abstract": "Privacy and security in Smart Cities remain at constant risk due to the\nvulnerabilities introduced by Internet of Things (IoT) devices. The limited\ncomputational resources of these devices make them especially susceptible to\nattacks, while their widespread adoption increases the potential impact of\nsecurity breaches. This article presents a review of security proposals aimed\nat protecting IoT devices in Smart City environments. The review was conducted\nby analyzing recent literature on device-level security, with particular\nemphasis on lightweight cryptography, physically unclonable functions (PUFs),\nand blockchain-based solutions. Findings highlight both the strengths and\nlimitations of current approaches, as well as the need for more practical,\nscalable, and resource-efficient mechanisms to ensure user privacy and data\nprotection in IoT ecosystems."
    },
    {
        "date": "2025-10",
        "title": "E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing",
        "author": "Davide Rusconi, Osama Yousef, Mirco Picca, Flavio Toffalini, and Andrea Lanzi",
        "link": "http://arxiv.org/abs/2510.01393v1",
        "abstract": "In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted\ntowards improving the throughput of fuzzing campaigns in contexts where\nscalability is unavailable. E-FuzzEdge addresses the inefficiencies of\nhardware-in-the-loop fuzzing for microcontrollers by optimizing execution\nspeed. We evaluated our system against state-of-the-art benchmarks,\ndemonstrating significant performance improvements. A key advantage of\nE-FuzzEdgearchitecture is its compatibility with other embedded fuzzing\ntechniques that perform on device testing instead of firmware emulation. This\nmeans that the broader embedded fuzzing community can integrate E-FuzzEdge into\ntheir workflows to enhance overall testing efficiency."
    },
    {
        "date": "2025-10",
        "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks",
        "author": "Shoumik Saha, Jifan Chen, Sam Mayers, Sanjay Krishna Gouda, Zijian Wang, and Varun Kumar",
        "link": "http://arxiv.org/abs/2510.01359v1",
        "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into\nsoftware engineering workflows where they can read, write, and execute code,\nraising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only\nsettings. Prior evaluations emphasize refusal or harmful-text detection,\nleaving open whether agents actually compile and run malicious programs. We\npresent JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three\nescalating workspace regimes that mirror attacker capability: empty (JAWS-0),\nsingle-file (JAWS-1), and multi-file (JAWS-M). We pair this with a\nhierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)\nattack success, (iii) syntactic correctness, and (iv) runtime executability,\nmoving beyond refusal to measure deployable harm. Using seven LLMs from five\nfamilies as backends, we find that under prompt-only conditions in JAWS-0, code\nagents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%\nrun end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~\n100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the\nmulti-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly\ndeployable attack code. Across models, wrapping an LLM in an agent\nsubstantially increases vulnerability -- ASR raises by 1.6x -- because initial\nrefusals are frequently overturned during later planning/tool-use steps.\nCategory-level analyses identify which attack classes are most vulnerable and\nmost readily deployable, while others exhibit large execution gaps. These\nfindings motivate execution-aware defenses, code-contextual safety filters, and\nmechanisms that preserve refusal decisions throughout the agent's multi-step\nreasoning and tool use."
    },
    {
        "date": "2025-10",
        "title": "Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays",
        "author": "Muhammad Faheemur Rahman, and Wayne Burleson",
        "link": "http://arxiv.org/abs/2510.01350v1",
        "abstract": "Memristive crossbar arrays enable in-memory computing by performing parallel\nanalog computations directly within memory, making them well-suited for machine\nlearning, neural networks, and neuromorphic systems. However, despite their\nadvantages, non-volatile memristors are vulnerable to security threats (such as\nadversarial extraction of stored weights when the hardware is compromised.\nProtecting these weights is essential since they represent valuable\nintellectual property resulting from lengthy and costly training processes\nusing large, often proprietary, datasets. As a solution we propose two security\nmechanisms: Keyed Permutor and Watermark Protection Columns; where both\nsafeguard critical weights and establish verifiable ownership (even in cases of\ndata leakage). Our approach integrates efficiently with existing memristive\ncrossbar architectures without significant design modifications. Simulations\nacross 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and\na large RF dataset, show that both mechanisms offer robust protection with\nunder 10% overhead in area, delay and power. We also present initial\nexperiments employing the widely known MNIST dataset; further highlighting the\nfeasibility of securing memristive in-memory computing systems with minimal\nperformance trade-offs."
    },
    {
        "date": "2025-10",
        "title": "DECOR: Deep Embedding Clustering with Orientation Robustness",
        "author": "Fiona Victoria Stanley Jothiraj, Arunaggiri Pandian Karunanidhi, and Seth A. Eichmeyer",
        "link": "http://arxiv.org/abs/2510.03328v1",
        "abstract": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems."
    },
    {
        "date": "2025-10",
        "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness",
        "author": "Wa\u00efss Azizian, and Ali Hasan",
        "link": "http://arxiv.org/abs/2510.01163v1",
        "abstract": "The emergence of in-context learning (ICL) in large language models (LLMs)\nremains poorly understood despite its consistent effectiveness, enabling models\nto adapt to new tasks from only a handful of examples. To clarify and improve\nthese capabilities, we characterize how the statistical properties of the\npretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical\ntasks. We develop a theoretical framework that unifies task selection and\ngeneralization, extending and sharpening earlier results, and show how\ndistributional properties govern sample efficiency, task retrieval, and\nrobustness. To this end, we generalize Bayesian posterior consistency and\nconcentration results to heavy-tailed priors and dependent sequences, better\nreflecting the structure of LLM pretraining data. We then empirically study how\nICL performance varies with the pretraining distribution on challenging tasks\nsuch as stochastic differential equations and stochastic processes with memory.\nTogether, these findings suggest that controlling key statistical properties of\nthe pretraining distribution is essential for building ICL-capable and reliable\nLLMs."
    },
    {
        "date": "2025-10",
        "title": "Multi-Marginal Flow Matching with Adversarially Learnt Interpolants",
        "author": "Oskar Kviman, Kirill Tamogashev, Nicola Branchini, V\u00edctor Elvira, Jens Lagergren, and Nikolay Malkin",
        "link": "http://arxiv.org/abs/2510.01159v1",
        "abstract": "Learning the dynamics of a process given sampled observations at several time\npoints is an important but difficult task in many scientific applications. When\nno ground-truth trajectories are available, but one has only snapshots of data\ntaken at discrete time steps, the problem of modelling the dynamics, and thus\ninferring the underlying trajectories, can be solved by multi-marginal\ngeneralisations of flow matching algorithms. This paper proposes a novel flow\nmatching method that overcomes the limitations of existing multi-marginal\ntrajectory inference algorithms. Our proposed method, ALI-CFM, uses a\nGAN-inspired adversarial loss to fit neurally parametrised interpolant curves\nbetween source and target points such that the marginal distributions at\nintermediate time points are close to the observed distributions. The resulting\ninterpolants are smooth trajectories that, as we show, are unique under mild\nassumptions. These interpolants are subsequently marginalised by a flow\nmatching algorithm, yielding a trained vector field for the underlying\ndynamics. We showcase the versatility and scalability of our method by\noutperforming the existing baselines on spatial transcriptomics and cell\ntracking datasets, while performing on par with them on single-cell trajectory\nprediction.\n  Code: https://github.com/mmacosha/adversarially-learned-interpolants."
    },
    {
        "date": "2025-10",
        "title": "Backdoor Attacks Against Speech Language Models",
        "author": "Alexandrine Fortier, Thomas Thebaud, Jes\u00fas Villalba, Najim Dehak, and Patrick Cardinal",
        "link": "http://arxiv.org/abs/2510.01157v1",
        "abstract": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders."
    },
    {
        "date": "2025-10",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense",
        "author": "Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, and Yi Zeng",
        "link": "http://arxiv.org/abs/2510.01088v1",
        "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the\nabsence of universal standards and reliable content validators, making it\ndifficult to obtain effective training signals. We discover that aligned models\nalready possess robust internal safety beliefs: they consistently produce\nhigh-confidence refusals to harmful requests while exhibiting high entropy when\ngenerating potentially dangerous content. This entropy gap reveals an untapped\nsignal--models intrinsically \"know\" when to refuse. We introduce Safety\nInstincts Reinforcement Learning (SIRL), which transforms this internal\nconfidence into a self-generated reward signal, eliminating dependence on\nexternal validators or human annotations. SIRL teaches models to trust their\nsafety instincts by reinforcing low-entropy refusal behaviors. Evaluated on\nLlama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against\n20+ jailbreak methods, from static prompts to adaptive attacks. Using only\n15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods\nwhile preserving performance on mathematics, coding, and conversation\nbenchmarks. Our work demonstrates that effective alignment can emerge from\nwithin, paving the way for more autonomous and robust AI safety mechanisms that\nscale without extensive human oversight."
    },
    {
        "date": "2025-10",
        "title": "Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI",
        "author": "Akchunya Chanchal, David A. Kelly, and Hana Chockler",
        "link": "http://arxiv.org/abs/2510.01038v1",
        "abstract": "Black-box explainability methods are popular tools for explaining the\ndecisions of image classifiers. A major drawback of these tools is their\nreliance on mutants obtained by occluding parts of the input, leading to\nout-of-distribution images. This raises doubts about the quality of the\nexplanations. Moreover, choosing an appropriate occlusion value often requires\ndomain knowledge. In this paper we introduce a novel forward-pass paradigm\nActivation-Deactivation (AD), which removes the effects of occluded input\nfeatures from the model's decision-making by switching off the parts of the\nmodel that correspond to the occlusions. We introduce ConvAD, a drop-in\nmechanism that can be easily added to any trained Convolutional Neural Network\n(CNN), and which implements the AD paradigm. This leads to more robust\nexplanations without any additional training or fine-tuning. We prove that the\nConvAD mechanism does not change the decision-making process of the network. We\nprovide experimental evaluation across several datasets and model\narchitectures. We compare the quality of AD-explanations with explanations\nachieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with\nocclusions, demonstrating that ConvAD extracts more robust explanations without\nthe need for domain knowledge."
    },
    {
        "date": "2025-10",
        "title": "Secure and reversible face anonymization with diffusion models",
        "author": "Pol Labarbarie, Vincent Itier, and William Puech",
        "link": "http://arxiv.org/abs/2510.01031v1",
        "abstract": "Face images processed by computer vision algorithms contain sensitive\npersonal information that malicious actors can capture without consent. These\nprivacy and security risks highlight the need for effective face anonymization\nmethods. Current methods struggle to propose a good trade-off between a secure\nscheme with high-quality image generation and reversibility for later person\nauthentication. Diffusion-based approaches produce high-quality anonymized\nimages but lack the secret key mechanism to ensure that only authorized parties\ncan reverse the process. In this paper, we introduce, to our knowledge, the\nfirst secure, high-quality reversible anonymization method based on a diffusion\nmodel. We propose to combine the secret key with the latent faces\nrepresentation of the diffusion model. To preserve identity-irrelevant\nfeatures, generation is constrained by a facial mask, maintaining high-quality\nimages. By using a deterministic forward and backward diffusion process, our\napproach enforces that the original face can be recovered with the correct\nsecret key. We also show that the proposed method produces anonymized faces\nthat are less visually similar to the original faces, compared to other\nprevious work."
    },
    {
        "date": "2025-10",
        "title": "Towards Adversarial Training under Hyperspectral Images",
        "author": "Weihua Zhang, Chengze Jiang, Jie Gui, and Lu Dong",
        "link": "http://arxiv.org/abs/2510.01014v1",
        "abstract": "Recent studies have revealed that hyperspectral classification models based\non deep learning are highly vulnerable to adversarial attacks, which pose\nsignificant security risks. Although several approaches have attempted to\nenhance adversarial robustness by modifying network architectures, these\nmethods often rely on customized designs that limit scalability and fail to\ndefend effectively against strong attacks. To address these challenges, we\nintroduce adversarial training to the hyperspectral domain, which is widely\nregarded as one of the most effective defenses against adversarial attacks.\nThrough extensive empirical analyses, we demonstrate that while adversarial\ntraining does enhance robustness across various models and datasets,\nhyperspectral data introduces unique challenges not seen in RGB images.\nSpecifically, we find that adversarial noise and the non-smooth nature of\nadversarial examples can distort or eliminate important spectral semantic\ninformation. To mitigate this issue, we employ data augmentation techniques and\npropose a novel hyperspectral adversarial training method, termed AT-RA. By\nincreasing the diversity of spectral information and ensuring spatial\nsmoothness, AT-RA preserves and corrects spectral semantics in hyperspectral\nimages. Experimental results show that AT-RA improves adversarial robustness by\n21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign\naccuracy by 2.68%."
    },
    {
        "date": "2025-10",
        "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation",
        "author": "Aueaphum Aueawatthanaphisut",
        "link": "http://arxiv.org/abs/2510.00976v1",
        "abstract": "Rare-disease diagnosis remains one of the most pressing challenges in digital\nhealth, hindered by extreme data scarcity, privacy concerns, and the limited\nresources of edge devices. This paper proposes the Adaptive Federated Few-Shot\nRare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)\nfew-shot federated optimization with meta-learning to generalize from limited\npatient samples, (ii) energy-aware client scheduling to mitigate device\ndropouts and ensure balanced participation, and (iii) secure aggregation with\ncalibrated differential privacy to safeguard sensitive model updates. Unlike\nprior work that addresses these aspects in isolation, AFFR unifies them into a\nmodular pipeline deployable on real-world clinical networks. Experimental\nevaluation on simulated rare-disease detection datasets demonstrates up to 10%\nimprovement in accuracy compared with baseline FL, while reducing client\ndropouts by over 50% without degrading convergence. Furthermore,\nprivacy-utility trade-offs remain within clinically acceptable bounds. These\nfindings highlight AFFR as a practical pathway for equitable and trustworthy\nfederated diagnosis of rare conditions."
    },
    {
        "date": "2025-10",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning",
        "author": "Shashank Reddy Chirra, Jayden Teoh, Praveen Paruchuri, and Pradeep Varakantham",
        "link": "http://arxiv.org/abs/2510.00922v1",
        "abstract": "Adversarial Imitation Learning (AIL) methods, while effective in settings\nwith limited expert demonstrations, are often considered unstable. These\napproaches typically decompose into two components: Density Ratio (DR)\nestimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the\nrelative occupancy of state-action pairs under the policy versus the expert;\nand Reward Assignment (RA), where this ratio is transformed into a reward\nsignal used to train the policy. While significant research has focused on\nimproving density estimation, the role of reward assignment in influencing\ntraining dynamics and final policy performance has been largely overlooked. RA\nfunctions in AIL are typically derived from divergence minimization objectives,\nrelying heavily on human design and ingenuity. In this work, we take a\ndifferent approach: we investigate the discovery of data-driven RA functions,\ni.e, based directly on the performance of the resulting imitation policy. To\nthis end, we leverage an LLM-guided evolutionary framework that efficiently\nexplores the space of RA functions, yielding \\emph{Discovered Adversarial\nImitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,\nDAIL generalises across unseen environments and policy optimization algorithms,\noutperforming the current state-of-the-art of \\emph{human-designed} baselines.\nFinally, we analyse why DAIL leads to more stable training, offering novel\ninsights into the role of RA functions in the stability of AIL. Code is\npublicly available: https://github.com/shshnkreddy/DAIL."
    },
    {
        "date": "2025-10",
        "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
        "author": "Xiangtao Kong, Rongyuan Wu, Shuaizheng Liu, Lingchen Sun, and Lei Zhang",
        "link": "http://arxiv.org/abs/2510.00820v1",
        "abstract": "Most recent real-world image super-resolution (Real-ISR) methods employ\npre-trained text-to-image (T2I) diffusion models to synthesize the high-quality\nimage either from random Gaussian noise, which yields realistic results but is\nslow due to iterative denoising, or directly from the input low-quality image,\nwhich is efficient but at the price of lower output quality. These approaches\ntrain ControlNet or LoRA modules while keeping the pre-trained model fixed,\nwhich often introduces over-enhanced artifacts and hallucinations, suffering\nfrom the robustness to inputs of varying degradations. Recent visual\nautoregressive (AR) models, such as pre-trained Infinity, can provide strong\nT2I generation capabilities while offering superior efficiency by using the\nbitwise next-scale prediction strategy. Building upon next-scale prediction, we\nintroduce a robust Real-ISR framework, namely Next-Scale Autoregressive\nModeling (NSARM). Specifically, we train NSARM in two stages: a transformation\nnetwork is first trained to map the input low-quality image to preliminary\nscales, followed by an end-to-end full-model fine-tuning. Such a comprehensive\nfine-tuning enhances the robustness of NSARM in Real-ISR tasks without\ncompromising its generative capability. Extensive quantitative and qualitative\nevaluations demonstrate that as a pure AR model, NSARM achieves superior visual\nresults over existing Real-ISR methods while maintaining a fast inference\nspeed. Most importantly, it demonstrates much higher robustness to the quality\nof input images, showing stronger generalization performance. Project page:\nhttps://github.com/Xiangtaokong/NSARM"
    },
    {
        "date": "2025-10",
        "title": "Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors",
        "author": "Gautier Evennou, Vivien Chappelier, and Ewa Kijak",
        "link": "http://arxiv.org/abs/2510.00799v1",
        "abstract": "Most image watermarking systems focus on robustness, capacity, and\nimperceptibility while treating the embedded payload as meaningless bits. This\nbit-centric view imposes a hard ceiling on capacity and prevents watermarks\nfrom carrying useful information. We propose LatentSeal, which reframes\nwatermarking as semantic communication: a lightweight text autoencoder maps\nfull-sentence messages into a compact 256-dimensional unit-norm latent vector,\nwhich is robustly embedded by a finetuned watermark model and secured through a\nsecret, invertible rotation. The resulting system hides full-sentence messages,\ndecodes in real time, and survives valuemetric and geometric attacks. It\nsurpasses prior state of the art in BLEU-4 and Exact Match on several\nbenchmarks, while breaking through the long-standing 256-bit payload ceiling.\nIt also introduces a statistically calibrated score that yields a ROC AUC score\nof 0.97-0.99, and practical operating points for deployment. By shifting from\nbit payloads to semantic latent vectors, LatentSeal enables watermarking that\nis not only robust and high-capacity, but also secure and interpretable,\nproviding a concrete path toward provenance, tamper explanation, and\ntrustworthy AI governance. Models, training and inference code, and data splits\nwill be available upon publication."
    },
    {
        "date": "2025-10",
        "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
        "author": "Yifan Shen, Yangyang Shu, Hye-young Paik, and Yulei Sui",
        "link": "http://arxiv.org/abs/2510.00796v1",
        "abstract": "Recent advances in text-to-image (T2I) models, especially diffusion-based\narchitectures, have significantly improved the visual quality of generated\nimages. However, these models continue to struggle with a critical limitation:\nmaintaining semantic consistency when input prompts undergo minor linguistic\nvariations. Despite being logically equivalent, such prompt pairs often yield\nmisaligned or semantically inconsistent images, exposing a lack of robustness\nin reasoning and generalisation. To address this, we propose MetaLogic, a novel\nevaluation framework that detects T2I misalignment without relying on ground\ntruth images. MetaLogic leverages metamorphic testing, generating image pairs\nfrom prompts that differ grammatically but are semantically identical. By\ndirectly comparing these image pairs, the framework identifies inconsistencies\nthat signal failures in preserving the intended meaning, effectively diagnosing\nrobustness issues in the model's logic understanding. Unlike existing\nevaluation methods that compare a generated image to a single prompt, MetaLogic\nevaluates semantic equivalence between paired images, offering a scalable,\nground-truth-free approach to identifying alignment failures. It categorises\nthese alignment errors (e.g., entity omission, duplication, positional\nmisalignment) and surfaces counterexamples that can be used for model debugging\nand refinement. We evaluate MetaLogic across multiple state-of-the-art T2I\nmodels and reveal consistent robustness failures across a range of logical\nconstructs. We find that even the SOTA text-to-image models like Flux.dev and\nDALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,\nrespectively. Our results show that MetaLogic is not only efficient and\nscalable, but also effective in uncovering fine-grained logical inconsistencies\nthat are overlooked by existing evaluation metrics."
    },
    {
        "date": "2025-10",
        "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
        "author": "Seunghoo Hong, Geonho Son, Juhun Lee, and Simon S. Woo",
        "link": "http://arxiv.org/abs/2510.00778v1",
        "abstract": "Diffusion models have shown to be strong representation learners, showcasing\nstate-of-the-art performance across multiple domains. Aside from accelerated\nsampling, DDIM also enables the inversion of real images back to their latent\ncodes. A direct inheriting application of this inversion operation is real\nimage editing, where the inversion yields latent trajectories to be utilized\nduring the synthesis of the edited image. Unfortunately, this practical tool\nhas enabled malicious users to freely synthesize misinformative or deepfake\ncontents with greater ease, which promotes the spread of unethical and abusive,\nas well as privacy-, and copyright-infringing contents. While defensive\nalgorithms such as AdvDM and Photoguard have been shown to disrupt the\ndiffusion process on these images, the misalignment between their objectives\nand the iterative denoising trajectory at test time results in weak disruptive\nperformance.In this work, we present the DDIM Inversion Attack (DIA) that\nattacks the integrated DDIM trajectory path. Our results support the effective\ndisruption, surpassing previous defensive methods across various editing\nmethods. We believe that our frameworks and results can provide practical\ndefense methods against the malicious use of AI for both the industry and the\nresearch community. Our code is available here:\nhttps://anonymous.4open.science/r/DIA-13419/."
    },
    {
        "date": "2025-10",
        "title": "ZQBA: Zero Query Black-box Adversarial Attack",
        "author": "Joana C. Costa, Tiago Roxo, Hugo Proen\u00e7a, and Pedro R. M. In\u00e1cio",
        "link": "http://arxiv.org/abs/2510.00769v1",
        "abstract": "Current black-box adversarial attacks either require multiple queries or\ndiffusion models to produce adversarial samples that can impair the target\nmodel performance. However, these methods require training a surrogate loss or\ndiffusion models to produce adversarial samples, which limits their\napplicability in real-world settings. Thus, we propose a Zero Query Black-box\nAdversarial (ZQBA) attack that exploits the representations of Deep Neural\nNetworks (DNNs) to fool other networks. Instead of requiring thousands of\nqueries to produce deceiving adversarial samples, we use the feature maps\nobtained from a DNN and add them to clean images to impair the classification\nof a target model. The results suggest that ZQBA can transfer the adversarial\nsamples to different models and across various datasets, namely CIFAR and Tiny\nImageNet. The experiments also show that ZQBA is more effective than\nstate-of-the-art black-box attacks with a single query, while maintaining the\nimperceptibility of perturbations, evaluated both quantitatively (SSIM) and\nqualitatively, emphasizing the vulnerabilities of employing DNNs in real-world\ncontexts. All the source code is available at\nhttps://github.com/Joana-Cabral/ZQBA."
    },
    {
        "date": "2025-10",
        "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning",
        "author": "Yicheng Lang, Yihua Zhang, Chongyu Fan, Changsheng Wang, Jinghan Jia, and Sijia Liu",
        "link": "http://arxiv.org/abs/2510.00761v2",
        "abstract": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality."
    },
    {
        "date": "2025-10",
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "author": "Nanxiang Jiang, Zhaoxin Fan, Enhan Kang, Daiheng Gao, Yun Zhou, Yanxia Chang, Zheng Zhu, Yeying Jin, and Wenjun Wu",
        "link": "http://arxiv.org/abs/2510.00635v2",
        "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive generative capabilities, but they also raise significant safety\nconcerns due to the potential to produce harmful or undesirable content. While\nconcept erasure has been explored as a mitigation strategy, most existing\napproaches and corresponding attack evaluations are tailored to Stable\nDiffusion (SD) and exhibit limited effectiveness when transferred to\nnext-generation rectified flow transformers such as Flux. In this work, we\npresent ReFlux, the first concept attack method specifically designed to assess\nthe robustness of concept erasure in the latest rectified flow-based T2I\nframework. Our approach is motivated by the observation that existing concept\nerasure techniques, when applied to Flux, fundamentally rely on a phenomenon\nknown as attention localization. Building on this insight, we propose a simple\nyet effective attack strategy that specifically targets this property. At its\ncore, a reverse-attention optimization strategy is introduced to effectively\nreactivate suppressed signals while stabilizing attention. This is further\nreinforced by a velocity-guided dynamic that enhances the robustness of concept\nreactivation by steering the flow matching process, and a\nconsistency-preserving objective that maintains the global layout and preserves\nunrelated content. Extensive experiments consistently demonstrate the\neffectiveness and efficiency of the proposed attack method, establishing a\nreliable benchmark for evaluating the robustness of concept erasure strategies\nin rectified flow transformers."
    },
    {
        "date": "2025-10",
        "title": "Robust Context-Aware Object Recognition",
        "author": "Klara Janouskova, Cristian Gavrus, and Jiri Matas",
        "link": "http://arxiv.org/abs/2510.00618v1",
        "abstract": "In visual recognition, both the object of interest (referred to as\nforeground, FG, for simplicity) and its surrounding context (background, BG)\nplay an important role. However, standard supervised learning often leads to\nunintended over-reliance on the BG, known as shortcut learning of spurious\ncorrelations, limiting model robustness in real-world deployment settings. In\nthe literature, the problem is mainly addressed by suppressing the BG,\nsacrificing context information for improved generalization.\n  We propose RCOR -- Robust Context-Aware Object Recognition -- the first\napproach that jointly achieves robustness and context-awareness without\ncompromising either. RCOR treats localization as an integral part of\nrecognition to decouple object-centric and context-aware modelling, followed by\na robust, non-parametric fusion. It improves the performance of both supervised\nmodels and VLM on datasets with both in-domain and out-of-domain BG, even\nwithout fine-tuning. The results confirm that localization before recognition\nis now possible even in complex scenes as in ImageNet-1k."
    },
    {
        "date": "2025-10",
        "title": "Designing Ambiguity Sets for Distributionally Robust Optimization Using Structural Causal Optimal Transport",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2510.00599v1",
        "abstract": "Distributionally robust optimization tackles out-of-sample issues like\noverfitting and distribution shifts by adopting an adversarial approach over a\nrange of possible data distributions, known as the ambiguity set. To balance\nconservatism and accuracy, these sets must include realistic probability\ndistributions by leveraging information from the nominal distribution. Assuming\nthat nominal distributions arise from a structural causal model with a directed\nacyclic graph $\\mathcal{G}$ and structural equations, previous methods such as\nadapted and $\\mathcal{G}$-causal optimal transport have only utilized causal\ngraph information in designing ambiguity sets. In this work, we propose\nincorporating structural equations, which include causal graph information, to\nenhance ambiguity sets, resulting in more realistic distributions. We introduce\nstructural causal optimal transport and its associated ambiguity set,\ndemonstrating their advantages and connections to previous methods. A key\nbenefit of our approach is a relaxed version, where a regularization term\nreplaces the complex causal constraints, enabling an efficient algorithm via\ndifference-of-convex programming to solve structural causal optimal transport.\nWe also show that when structural information is absent and must be estimated,\nour approach remains effective and provides finite sample guarantees. Lastly,\nwe address the radius of ambiguity sets, illustrating how our method overcomes\nthe curse of dimensionality in optimal transport problems, achieving faster\nshrinkage with dimension-free order."
    },
    {
        "date": "2025-10",
        "title": "Private Online Learning against an Adaptive Adversary: Realizable and Agnostic Settings",
        "author": "Bo Li, Wei Wang, and Peng Ye",
        "link": "http://arxiv.org/abs/2510.00574v1",
        "abstract": "We revisit the problem of private online learning, in which a learner\nreceives a sequence of $T$ data points and has to respond at each time-step a\nhypothesis. It is required that the entire stream of output hypotheses should\nsatisfy differential privacy. Prior work of Golowich and Livni [2021]\nestablished that every concept class $\\mathcal{H}$ with finite Littlestone\ndimension $d$ is privately online learnable in the realizable setting. In\nparticular, they proposed an algorithm that achieves an $O_{d}(\\log T)$ mistake\nbound against an oblivious adversary. However, their approach yields a\nsuboptimal $\\tilde{O}_{d}(\\sqrt{T})$ bound against an adaptive adversary. In\nthis work, we present a new algorithm with a mistake bound of $O_{d}(\\log T)$\nagainst an adaptive adversary, closing this gap. We further investigate the\nproblem in the agnostic setting, which is more general than the realizable\nsetting as it does not impose any assumptions on the data. We give an algorithm\nthat obtains a sublinear regret of $\\tilde{O}_d(\\sqrt{T})$ for generic\nLittlestone classes, demonstrating that they are also privately online\nlearnable in the agnostic setting."
    },
    {
        "date": "2025-10",
        "title": "Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties",
        "author": "Raik Dankworth, and Gesina Schwalbe",
        "link": "http://arxiv.org/abs/2510.03320v1",
        "abstract": "Deep neural networks (NNs) for computer vision are vulnerable to adversarial\nattacks, i.e., miniscule malicious changes to inputs may induce unintuitive\noutputs. One key approach to verify and mitigate such robustness issues is to\nfalsify expected output behavior. This allows, e.g., to locally proof security,\nor to (re)train NNs on obtained adversarial input examples. Due to the\nblack-box nature of NNs, current attacks only falsify a class of the final\noutput, such as flipping from $\\texttt{stop_sign}$ to $\\neg\\texttt{stop_sign}$.\nIn this short position paper we generalize this to search for generally\nillogical behavior, as considered in NN verification: falsify constraints\n(concept-based properties) involving further human-interpretable concepts, like\n$\\texttt{red}\\wedge\\texttt{octogonal}\\rightarrow\\texttt{stop_sign}$. For this,\nan easy implementation of concept-based properties on already trained NNs is\nproposed using techniques from explainable artificial intelligence. Further, we\nsketch the theoretical proof that attacks on concept-based properties are\nexpected to have a reduced search space compared to simple class falsification,\nwhilst arguably be more aligned with intuitive robustness targets. As an\noutlook to this work in progress we hypothesize that this approach has\npotential to efficiently and simultaneously improve logical compliance and\nrobustness."
    },
    {
        "date": "2025-10",
        "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs",
        "author": "Anbi Guo, and Mahfuza Farooque",
        "link": "http://arxiv.org/abs/2510.00529v1",
        "abstract": "Structured security logs are critical for detecting advanced persistent\nthreats (APTs). Large language models (LLMs) struggle in this domain due to\nlimited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory\nretrieval-augmented generation framework for structured log analysis. It\nintegrates a short-term memory buffer for recent summaries and a long-term\nFAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini\nprocesses the combined context and outputs structured predictions. Bayesian\nfusion promotes reliable persistence into memory. On the UNSW-NB15 dataset,\nDM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and\nRAG baselines in recall. The architecture is lightweight, interpretable, and\nscalable, enabling real-time threat monitoring without extra corpora or heavy\ntuning."
    },
    {
        "date": "2025-10",
        "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
        "author": "Tsubasa Takahashi, Shojiro Yamabe, Futa Waseda, and Kento Sasaki",
        "link": "http://arxiv.org/abs/2510.00517v1",
        "abstract": "Differential Attention (DA) has been proposed as a refinement to standard\nattention, suppressing redundant or noisy context through a subtractive\nstructure and thereby reducing contextual hallucination. While this design\nsharpens task-relevant focus, we show that it also introduces a structural\nfragility under adversarial perturbations. Our theoretical analysis identifies\nnegative gradient alignment-a configuration encouraged by DA's subtraction-as\nthe key driver of sensitivity amplification, leading to increased gradient\nnorms and elevated local Lipschitz constants. We empirically validate this\nFragile Principle through systematic experiments on ViT/DiffViT and evaluations\nof pretrained CLIP/DiffCLIP, spanning five datasets in total. These results\ndemonstrate higher attack success rates, frequent gradient opposition, and\nstronger local sensitivity compared to standard attention. Furthermore,\ndepth-dependent experiments reveal a robustness crossover: stacking DA layers\nattenuates small perturbations via depth-dependent noise cancellation, though\nthis protection fades under larger attack budgets. Overall, our findings\nuncover a fundamental trade-off: DA improves discriminative focus on clean\ninputs but increases adversarial vulnerability, underscoring the need to\njointly design for selectivity and robustness in future attention mechanisms."
    },
    {
        "date": "2025-10",
        "title": "On the Adversarial Robustness of Learning-based Conformal Novelty Detection",
        "author": "Daofu Zhang, Mehrdad Pournaderi, Hanne M. Clifford, Yu Xiang, and Pramod K. Varshney",
        "link": "http://arxiv.org/abs/2510.00463v1",
        "abstract": "This paper studies the adversarial robustness of conformal novelty detection.\nIn particular, we focus on AdaDetect, a powerful learning-based framework for\nnovelty detection with finite-sample false discovery rate (FDR) control. While\nAdaDetect provides rigorous statistical guarantees under benign conditions, its\nbehavior under adversarial perturbations remains unexplored. We first formulate\nan oracle attack setting that quantifies the worst-case degradation of FDR,\nderiving an upper bound that characterizes the statistical cost of attacks.\nThis idealized formulation directly motivates a practical and effective attack\nscheme that only requires query access to AdaDetect's output labels. Coupling\nthese formulations with two popular and complementary black-box adversarial\nalgorithms, we systematically evaluate the vulnerability of AdaDetect on\nsynthetic and real-world datasets. Our results show that adversarial\nperturbations can significantly increase the FDR while maintaining high\ndetection power, exposing fundamental limitations of current error-controlled\nnovelty detection methods and motivating the development of more robust\nalternatives."
    },
    {
        "date": "2025-10",
        "title": "Robust Spatiotemporally Contiguous Anomaly Detection Using Tensor Decomposition",
        "author": "Rachita Mondal, Mert Indibi, Tapabrata Maiti, and Selin Aviyente",
        "link": "http://arxiv.org/abs/2510.00460v1",
        "abstract": "Anomaly detection in spatiotemporal data is a challenging problem encountered\nin a variety of applications, including video surveillance, medical imaging\ndata, and urban traffic monitoring. Existing anomaly detection methods focus\nmainly on point anomalies and cannot deal with temporal and spatial\ndependencies that arise in spatio-temporal data. Tensor-based anomaly detection\nmethods have been proposed to address this problem. Although existing methods\ncan capture dependencies across different modes, they are primarily supervised\nand do not account for the specific structure of anomalies. Moreover, these\nmethods focus mainly on extracting anomalous features without providing any\nstatistical confidence. In this paper, we introduce an unsupervised\ntensor-based anomaly detection method that simultaneously considers the sparse\nand spatiotemporally smooth nature of anomalies. The anomaly detection problem\nis formulated as a regularized robust low-rank + sparse tensor decomposition\nwhere the total variation of the tensor with respect to the underlying spatial\nand temporal graphs quantifies the spatiotemporal smoothness of the anomalies.\nOnce the anomalous features are extracted, we introduce a statistical anomaly\nscoring framework that accounts for local spatio-temporal dependencies. The\nproposed framework is evaluated on both synthetic and real data."
    },
    {
        "date": "2025-10",
        "title": "SVDefense: Effective Defense against Gradient Inversion Attacks via Singular Value Decomposition",
        "author": "Chenxiang Luo, David K. Y. Yau, and Qun Song",
        "link": "http://arxiv.org/abs/2510.03319v1",
        "abstract": "Federated learning (FL) enables collaborative model training without sharing\nraw data but is vulnerable to gradient inversion attacks (GIAs), where\nadversaries reconstruct private data from shared gradients. Existing defenses\neither incur impractical computational overhead for embedded platforms or fail\nto achieve privacy protection and good model utility at the same time.\nMoreover, many defenses can be easily bypassed by adaptive adversaries who have\nobtained the defense details. To address these limitations, we propose\nSVDefense, a novel defense framework against GIAs that leverages the truncated\nSingular Value Decomposition (SVD) to obfuscate gradient updates. SVDefense\nintroduces three key innovations, a Self-Adaptive Energy Threshold that adapts\nto client vulnerability, a Channel-Wise Weighted Approximation that selectively\npreserves essential gradient information for effective model training while\nenhancing privacy protection, and a Layer-Wise Weighted Aggregation for\neffective model aggregation under class imbalance. Our extensive evaluation\nshows that SVDefense outperforms existing defenses across multiple\napplications, including image classification, human activity recognition, and\nkeyword spotting, by offering robust privacy protection with minimal impact on\nmodel accuracy. Furthermore, SVDefense is practical for deployment on various\nresource-constrained embedded platforms. We will make our code publicly\navailable upon paper acceptance."
    },
    {
        "date": "2025-10",
        "title": "A Call to Action for a Secure-by-Design Generative AI Paradigm",
        "author": "Dalal Alharthi, and Ivan Roberto Kawaminami Garcia",
        "link": "http://arxiv.org/abs/2510.00451v1",
        "abstract": "Large language models have gained widespread prominence, yet their\nvulnerability to prompt injection and other adversarial attacks remains a\ncritical concern. This paper argues for a security-by-design AI paradigm that\nproactively mitigates LLM vulnerabilities while enhancing performance. To\nachieve this, we introduce PromptShield, an ontology-driven framework that\nensures deterministic and secure prompt interactions. It standardizes user\ninputs through semantic validation, eliminating ambiguity and mitigating\nadversarial manipulation. To assess PromptShield's security and performance\ncapabilities, we conducted an experiment on an agent-based system to analyze\ncloud logs within Amazon Web Services (AWS), containing 493 distinct events\nrelated to malicious activities and anomalies. By simulating prompt injection\nattacks and assessing the impact of deploying PromptShield, our results\ndemonstrate a significant improvement in model security and performance,\nachieving precision, recall, and F1 scores of approximately 94%. Notably, the\nontology-based framework not only mitigates adversarial threats but also\nenhances the overall performance and reliability of the system. Furthermore,\nPromptShield's modular and adaptable design ensures its applicability beyond\ncloud security, making it a robust solution for safeguarding generative AI\napplications across various domains. By laying the groundwork for AI safety\nstandards and informing future policy development, this work stimulates a\ncrucial dialogue on the pivotal role of deterministic prompt engineering and\nontology-based validation in ensuring the safe and responsible deployment of\nLLMs in high-stakes environments."
    },
    {
        "date": "2025-10",
        "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
        "author": "Jiayi Liu, Jiaming Zhou, Ke Ye, Kun-Yu Lin, Allan Wang, and Junwei Liang",
        "link": "http://arxiv.org/abs/2510.00405v1",
        "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for\nrobotic navigation in human-centric environments. However, existing methods\ntypically assume idealized observation histories, failing to account for the\nperceptual artifacts inherent in first-person vision, such as occlusions, ID\nswitches, and tracking drift. This discrepancy between training assumptions and\ndeployment reality severely limits model robustness. To bridge this gap, we\nintroduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,\nfirst-person visual histories in clean, bird's-eye-view future trajectories,\nenabling robust learning under realistic perceptual constraints. Building on\nthis benchmark, we propose BiFlow, a dual-stream flow matching model that\nconcurrently denoises historical observations and forecasts future motion by\nleveraging a shared latent representation. To better model agent intent, BiFlow\nincorporates our EgoAnchor mechanism, which conditions the prediction decoder\non distilled historical features via feature modulation. Extensive experiments\nshow that BiFlow achieves state-of-the-art performance, reducing minADE and\nminFDE by 10-15% on average and demonstrating superior robustness. We\nanticipate that our benchmark and model will provide a critical foundation for\ndeveloping trajectory forecasting systems truly resilient to the challenges of\nreal-world, ego-centric perception."
    },
    {
        "date": "2025-09",
        "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
        "author": "Linjin He, Xinda Qi, Dong Chen, Zhaojian Li, and Xiaobo Tan",
        "link": "http://arxiv.org/abs/2510.00358v1",
        "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex\nenvironments, yet their control remains challenging due to highly nonlinear\ndynamics. Existing model-based and bio-inspired controllers rely on simplified\nassumptions that limit performance. Deep reinforcement learning (DRL) has\nrecently emerged as a promising alternative, but online training is often\nimpractical because of costly and potentially damaging real-world interactions.\nOffline RL provides a safer option by leveraging pre-collected datasets, but it\nsuffers from distribution shift, which degrades generalization to unseen\nscenarios. To overcome this challenge, we propose DiSA-IQL\n(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that\nincorporates robustness modulation by penalizing unreliable state-action pairs\nto mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks\nacross two settings: in-distribution and out-of-distribution evaluation.\nSimulation results show that DiSA-IQL consistently outperforms baseline models,\nincluding Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla\nIQL, achieving higher success rates, smoother trajectories, and improved\nrobustness. The codes are open-sourced to support reproducibility and to\nfacilitate further research in offline RL for soft robot control."
    },
    {
        "date": "2025-09",
        "title": "Security and Privacy Analysis of Tile's Location Tracking Protocol",
        "author": "Akshaya Kumar, Anna Raymaker, and Michael Specter",
        "link": "http://arxiv.org/abs/2510.00350v1",
        "abstract": "We conduct the first comprehensive security analysis of Tile, the second most\npopular crowd-sourced location-tracking service behind Apple's AirTags. We\nidentify several exploitable vulnerabilities and design flaws, disproving many\nof the platform's claimed security and privacy guarantees: Tile's servers can\npersistently learn the location of all users and tags, unprivileged adversaries\ncan track users through Bluetooth advertisements emitted by Tile's devices, and\nTile's anti-theft mode is easily subverted.\n  Despite its wide deployment -- millions of users, devices, and purpose-built\nhardware tags -- Tile provides no formal description of its protocol or threat\nmodel. Worse, Tile intentionally weakens its antistalking features to support\nan antitheft use-case and relies on a novel \"accountability\" mechanism to\npunish those abusing the system to stalk victims.\n  We examine Tile's accountability mechanism, a unique feature of independent\ninterest; no other provider attempts to guarantee accountability. While an\nideal accountability mechanism may disincentivize abuse in crowd-sourced\nlocation tracking protocols, we show that Tile's implementation is subvertible\nand introduces new exploitable vulnerabilities. We conclude with a discussion\non the need for new, formal definitions of accountability in this setting."
    },
    {
        "date": "2025-09",
        "title": "When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets",
        "author": "Zeshi Dai, Zimo Peng, Zerui Cheng, and Ryan Yihe Li",
        "link": "http://arxiv.org/abs/2510.00332v1",
        "abstract": "We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:\nthe inability of state-of-the-art models to operate in adversarial, high-stakes\nenvironments where misinformation is weaponized and errors are irreversible.\nWhile existing benchmarks measure task completion in controlled settings,\nreal-world deployment demands resilience against active deception. Using crypto\nmarkets as a testbed where $30 billion was lost to exploits in 2024, we\nevaluate 17 models on 178 time-anchored tasks requiring agents to distinguish\ntruth from manipulation, navigate fragmented information landscapes, and make\nirreversible financial decisions under adversarial pressure.\n  Our results reveal a fundamental capability gap: without tools, even frontier\nmodels achieve only 28% accuracy on tasks junior analysts routinely handle.\nTool augmentation improves performance but plateaus at 67.4% versus 80% human\nbaseline, despite unlimited access to professional resources. Most critically,\nwe uncover a systematic tool selection catastrophe: models preferentially\nchoose unreliable web search over authoritative data, falling for SEO-optimized\nmisinformation and social media manipulation. This behavior persists even when\ncorrect answers are directly accessible through specialized tools, suggesting\nfoundational limitations rather than knowledge gaps. We also find that Pass@k\nmetrics mask dangerous trial-and-error behavior for autonomous deployment.\n  The implications extend beyond crypto to any domain with active adversaries,\ne.g. cybersecurity, content moderation, etc. We release CAIA with contamination\ncontrols and continuous updates, establishing adversarial robustness as a\nnecessary condition for trustworthy AI autonomy. The benchmark reveals that\ncurrent models, despite impressive reasoning scores, remain fundamentally\nunprepared for environments where intelligence must survive active opposition."
    },
    {
        "date": "2025-09",
        "title": "Robust Federated Inference",
        "author": "Akash Dhasade, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Maxime Jacovella, Anne-Marie Kermarrec, and Rafael Pinot",
        "link": "http://arxiv.org/abs/2510.00310v1",
        "abstract": "Federated inference, in the form of one-shot federated learning, edge\nensembles, or federated ensembles, has emerged as an attractive solution to\ncombine predictions from multiple models. This paradigm enables each model to\nremain local and proprietary while a central server queries them and aggregates\npredictions. Yet, the robustness of federated inference has been largely\nneglected, leaving them vulnerable to even simple attacks. To address this\ncritical gap, we formalize the problem of robust federated inference and\nprovide the first robustness analysis of this class of methods. Our analysis of\naveraging-based aggregators shows that the error of the aggregator is small\neither when the dissimilarity between honest responses is small or the margin\nbetween the two most probable classes is large. Moving beyond linear averaging,\nwe show that problem of robust federated inference with non-linear aggregators\ncan be cast as an adversarial machine learning problem. We then introduce an\nadvanced technique using the DeepSet aggregation model, proposing a novel\ncomposition of adversarial training and test-time robust aggregation to\nrobustify non-linear aggregators. Our composition yields significant\nimprovements, surpassing existing robust aggregation methods by 4.7 - 22.2% in\naccuracy points across diverse benchmarks."
    },
    {
        "date": "2025-09",
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
        "author": "Xin Yu, Cong Xie, Ziyu Zhao, Tiantian Fan, Lingzhou Xue, and Zhi Zhang",
        "link": "http://arxiv.org/abs/2510.00192v1",
        "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for\nparameter-efficient fine-tuning of large language models, yet its\nrepresentational capacity often lags behind full fine-tuning. Within the\ncontext of LoRA, a key open question is how to obtain expressive low-rank\nadapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new\nframework that leverages structured pruning to obtain highly representative\nlow-rank adapters from an over-parameterized initialization. Unlike prior\napproaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes\nless important components during fine-tuning and prevents their reactivation,\nenabling flexible and adaptive rank allocation. For structured pruning, by\nminimizing the pruning error for overall loss, we provide fine-grained pruning\nand recovery updates in a gradient-based pruning strategy with grounded\ninterpretation. We provide the first theoretical analysis of the robustness of\nstructured pruning and provably show that under the impact of weight\nperturbation, gradient-based pruning is more robust than activation-based\npruning with respect to overall loss. Empirically, PrunedLoRA consistently\noutperforms LoRA and its variants across supervised fine-tuning tasks in\nmathematical reasoning, code generation, and natural language understanding,\nand it also demonstrates advantages over existing structured pruning methods\nacross diverse sparsity levels."
    },
    {
        "date": "2025-09",
        "title": "Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey",
        "author": "Jie Cao, Qi Li, Zelin Zhang, and Jianbing Ni",
        "link": "http://arxiv.org/abs/2510.02384v1",
        "abstract": "The rapid advancement of generative artificial intelligence (Gen-AI) has\nfacilitated the effortless creation of high-quality images, while\nsimultaneously raising critical concerns regarding intellectual property\nprotection, authenticity, and accountability. Watermarking has emerged as a\npromising solution to these challenges by distinguishing AI-generated images\nfrom natural content, ensuring provenance, and fostering trustworthy digital\necosystems. This paper presents a comprehensive survey of the current state of\nAI-generated image watermarking, addressing five key dimensions: (1)\nformalization of image watermarking systems; (2) an overview and comparison of\ndiverse watermarking techniques; (3) evaluation methodologies with respect to\nvisual quality, capacity, and detectability; (4) vulnerabilities to malicious\nattacks; and (5) prevailing challenges and future directions. The survey aims\nto equip researchers with a holistic understanding of AI-generated image\nwatermarking technologies, thereby promoting their continued development."
    },
    {
        "date": "2025-09",
        "title": "Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning",
        "author": "Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, and Wenzhe Jiao",
        "link": "http://arxiv.org/abs/2510.01278v1",
        "abstract": "Positive-Unlabeled (PU) learning aims to train a binary classifier (positive\nvs. negative) where only limited positive data and abundant unlabeled data are\navailable. While widely applicable, state-of-the-art PU learning methods\nsubstantially underperform their supervised counterparts on complex datasets,\nespecially without auxiliary negatives or pre-estimated parameters (e.g., a\n14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the\nchallenge of learning discriminative representations under unreliable\nsupervision. To tackle this challenge, we propose NcPU, a non-contrastive PU\nlearning framework that requires no auxiliary information. NcPU combines a\nnoisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns\nintra-class representations despite unreliable supervision, with a phantom\nlabel disambiguation (PLD) scheme that supplies conservative negative\nsupervision via regret-based label updates. Theoretically, NoiSNCL and PLD can\niteratively benefit each other from the perspective of the\nExpectation-Maximization framework. Empirically, extensive experiments\ndemonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive\nperformance; and (2) NcPU achieves substantial improvements over\nstate-of-the-art PU methods across diverse datasets, including challenging\ndatasets on post-disaster building damage mapping, highlighting its promise for\nreal-world applications. Code: Code will be open-sourced after review."
    },
    {
        "date": "2025-09",
        "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval",
        "author": "Nima Sheikholeslami, Erfan Hosseini, Patrice Bechard, Srivatsava Daruru, and Sai Rajeswar",
        "link": "http://arxiv.org/abs/2510.00137v1",
        "abstract": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG."
    },
    {
        "date": "2025-09",
        "title": "Are Robust LLM Fingerprints Adversarially Robust?",
        "author": "Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, and Sewoong Oh",
        "link": "http://arxiv.org/abs/2509.26598v1",
        "abstract": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
    },
    {
        "date": "2025-09",
        "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids",
        "author": "Justin Tackett, Benjamin Francis, Luis Garcia, David Grimsman, and Sean Warnick",
        "link": "http://arxiv.org/abs/2509.26532v1",
        "abstract": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms."
    },
    {
        "date": "2025-09",
        "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors",
        "author": "Aleksandra Knapi\u0144ska, and Marija Furdek",
        "link": "http://arxiv.org/abs/2509.26530v1",
        "abstract": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data."
    },
    {
        "date": "2025-09",
        "title": "DEPTHOR++: Robust Depth Enhancement from a Real-World Lightweight dToF and RGB Guidance",
        "author": "Jijun Xiang, Longliang Liu, Xuan Zhu, Xianqi Wang, Min Lin, and Xin Yang",
        "link": "http://arxiv.org/abs/2509.26498v1",
        "abstract": "Depth enhancement, which converts raw dToF signals into dense depth maps\nusing RGB guidance, is crucial for improving depth perception in high-precision\ntasks such as 3D reconstruction and SLAM. However, existing methods often\nassume ideal dToF inputs and perfect dToF-RGB alignment, overlooking\ncalibration errors and anomalies, thus limiting real-world applicability. This\nwork systematically analyzes the noise characteristics of real-world\nlightweight dToF sensors and proposes a practical and novel depth completion\nframework, DEPTHOR++, which enhances robustness to noisy dToF inputs from three\nkey aspects. First, we introduce a simulation method based on synthetic\ndatasets to generate realistic training samples for robust model training.\nSecond, we propose a learnable-parameter-free anomaly detection mechanism to\nidentify and remove erroneous dToF measurements, preventing misleading\npropagation during completion. Third, we design a depth completion network\ntailored to noisy dToF inputs, which integrates RGB images and pre-trained\nmonocular depth estimation priors to improve depth recovery in challenging\nregions. On the ZJU-L5 dataset and real-world samples, our training strategy\nsignificantly boosts existing depth completion models, with our model achieving\nstate-of-the-art performance, improving RMSE and Rel by 22% and 11% on average.\nOn the Mirror3D-NYU dataset, by incorporating the anomaly detection method, our\nmodel improves upon the previous SOTA by 37% in mirror regions. On the Hammer\ndataset, using simulated low-cost dToF data from RealSense L515, our method\nsurpasses the L515 measurements with an average gain of 22%, demonstrating its\npotential to enable low-cost sensors to outperform higher-end devices.\nQualitative results across diverse real-world datasets further validate the\neffectiveness and generalizability of our approach."
    },
    {
        "date": "2025-09",
        "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models",
        "author": "Shaoxiong Guo, Tianyi Du, Lijun Li, Yuyao Wu, Jie Li, and Jing Shao",
        "link": "http://arxiv.org/abs/2509.26473v1",
        "abstract": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs."
    },
    {
        "date": "2025-09",
        "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks",
        "author": "Tharindu Lakshan Yasarathna, and Nhien-An Le-Khac",
        "link": "http://arxiv.org/abs/2509.26350v1",
        "abstract": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
    },
    {
        "date": "2025-09",
        "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models",
        "author": "Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, and Kaizhu Huang",
        "link": "http://arxiv.org/abs/2509.26345v1",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
    },
    {
        "date": "2025-09",
        "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness",
        "author": "Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi",
        "link": "http://arxiv.org/abs/2509.26275v1",
        "abstract": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning."
    },
    {
        "date": "2025-09",
        "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
        "author": "Xiaobao Wang, Ruoxiao Sun, Yujun Zhang, Bingdao Feng, Dongxiao He, Luzhi Wang, and Di Jin",
        "link": "http://arxiv.org/abs/2509.26032v1",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines."
    },
    {
        "date": "2025-09",
        "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier",
        "author": "Gaojie Jin, Xinping Yi, and Xiaowei Huang",
        "link": "http://arxiv.org/abs/2509.25979v1",
        "abstract": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod."
    },
    {
        "date": "2025-09",
        "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions",
        "author": "Junbeom Kim, Kyuyoung Kim, Jihoon Tack, Dongha Lim, and Jinwoo Shin",
        "link": "http://arxiv.org/abs/2509.25973v1",
        "abstract": "Language models trained on web-scale corpora risk memorizing and exposing\nsensitive information, prompting the need for effective machine unlearning.\nPrior methods mainly focus on input queries to suppress sensitive outputs, yet\nthis often fails to eliminate the underlying knowledge and limits scalability.\nTo address this, we propose Corrective Unlearning with Retrieved Exclusions\n(CURE), a novel unlearning framework that verifies model outputs for leakage\nand revises them into safe responses. Specifically, CURE employs a lightweight\ncorrector that is applied to the original model to verify whether outputs\ncontain target knowledge and to rewrite them if any leakage is detected. To\nefficiently handle large-scale unlearning requests, CURE retrieves unlearning\ntargets that are relevant to the initial response and provides them as\nin-context references to the corrector for detection and conditional revision.\nBy leveraging this retrieval augmentation, the corrector can adapt to new\nunlearning requests without additional training. Extensive evaluations\ndemonstrate that CURE substantially reduces information leakage, even from\nindirect queries where prior works fall short, while maintaining response\nquality and general utility. Moreover, it demonstrates robustness under\ncontinual unlearning scenarios, making it practical for real-world\napplications."
    },
    {
        "date": "2025-09",
        "title": "The Impact of Scaling Training Data on Adversarial Robustness",
        "author": "Marco Zimmerli, Andreas Plesner, Till Aczel, and Roger Wattenhofer",
        "link": "http://arxiv.org/abs/2509.25927v1",
        "abstract": "Deep neural networks remain vulnerable to adversarial examples despite\nadvances in architectures and training paradigms. We investigate how training\ndata characteristics affect adversarial robustness across 36 state-of-the-art\nvision models spanning supervised, self-supervised, and contrastive learning\napproaches, trained on datasets from 1.2M to 22B images. Models were evaluated\nunder six black-box attack categories: random perturbations, two types of\ngeometric masks, COCO object manipulations, ImageNet-C corruptions, and\nImageNet-R style shifts. Robustness follows a logarithmic scaling law with both\ndata volume and model size: a tenfold increase in data reduces attack success\nrate (ASR) on average by ~3.2%, whereas a tenfold increase in model size\nreduces ASR on average by ~13.4%. Notably, some self-supervised models trained\non curated datasets, such as DINOv2, outperform others trained on much larger\nbut less curated datasets, challenging the assumption that scale alone drives\nrobustness. Adversarial fine-tuning of ResNet50s improves generalization across\nstructural variations but not across color distributions. Human evaluation\nreveals persistent gaps between human and machine vision. These results show\nthat while scaling improves robustness, data quality, architecture, and\ntraining objectives play a more decisive role than raw scale in achieving\nbroad-spectrum adversarial resilience."
    },
    {
        "date": "2025-09",
        "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack",
        "author": "Yein Park, Jungwoo Park, and Jaewoo Kang",
        "link": "http://arxiv.org/abs/2509.25843v1",
        "abstract": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety."
    },
    {
        "date": "2025-09",
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "author": "Hanjiang Hu, Bowei Li, Ziwei Wang, Tianhao Wei, Casidhe Hutchison, Eric Sample, and Changliu Liu",
        "link": "http://arxiv.org/abs/2510.00083v1",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics\napplications with visual inputs. It is essential to verify its robustness\nagainst semantic transformation perturbations, such as brightness and contrast.\nHowever, current certified training and robustness certification methods face\nthe challenge of over-parameterization, which hinders the tightness and\nscalability due to the over-complicated neural networks. To this end, we first\nanalyze stability and variance of layers and neurons against input\nperturbation, showing that certifiable robustness can be indicated by a\nfundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce\na novel neural network pruning method that removes neurons with low USN and\nretains those with high USN, thereby preserving model expressiveness without\nover-parameterization. To further enhance this pruning process, we propose a\nnew Wasserstein distance loss to ensure that pruned neurons are more\nconcentrated across layers. We validate our approach through extensive\nexperiments on the challenging robust keypoint detection task, which involves\nrealistic brightness and contrast perturbations, demonstrating that our method\nachieves superior robustness certification performance and efficiency compared\nto baselines."
    },
    {
        "date": "2025-09",
        "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks",
        "author": "Alexander Branch, Omead Pooladzandi, Radin Khosraviani, Sunay Gajanan Bhat, Jeffrey Jiang, and Gregory Pottie",
        "link": "http://arxiv.org/abs/2509.25792v1",
        "abstract": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines."
    },
    {
        "date": "2025-09",
        "title": "Lightweight and Robust Federated Data Valuation",
        "author": "Guojun Tang, Jiayu Zhou, Mohammad Mamun, and Steve Drew",
        "link": "http://arxiv.org/abs/2509.25560v1",
        "abstract": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments."
    },
    {
        "date": "2025-09",
        "title": "Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity",
        "author": "Tu-Hoa Pham, Philip Bailey, Daniel Posada, Georgios Georgakis, Jorge Enriquez, Surya Suresh, Marco Dolci, and Philip Twu",
        "link": "http://arxiv.org/abs/2509.25520v1",
        "abstract": "We consider the problem of vision-based 6-DoF object pose estimation in the\ncontext of the notional Mars Sample Return campaign, in which a robotic arm\nwould need to localize multiple objects of interest for low-clearance pickup\nand insertion, under severely constrained hardware. We propose a novel\nlocalization algorithm leveraging a custom renderer together with a new\ntemplate matching metric tailored to the edge domain to achieve robust pose\nestimation using only low-fidelity, textureless 3D models as inputs. Extensive\nevaluations on synthetic datasets as well as from physical testbeds on Earth\nand in situ Mars imagery shows that our method consistently beats the state of\nthe art in compute and memory-constrained localization, both in terms of\nrobustness and accuracy, in turn enabling new possibilities for cheap and\nreliable localization on general-purpose hardware."
    },
    {
        "date": "2025-09",
        "title": "Environmental Rate Manipulation Attacks on Power Grid Security",
        "author": "Yonatan Gizachew Achamyeleh, Yang Xiang, Yun-Ping Hsiao, Yasamin Moghaddas, and Mohammad Abdullah Al Faruque",
        "link": "http://arxiv.org/abs/2509.25476v1",
        "abstract": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security."
    },
    {
        "date": "2025-09",
        "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System",
        "author": "Panagiotis Michalopoulos, Anthony Mack, Cameron Clark, Linus Chen, Johannes Sedlmeir, and Andreas Veneris",
        "link": "http://arxiv.org/abs/2509.25469v1",
        "abstract": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection."
    },
    {
        "date": "2025-09",
        "title": "Managing Differentiated Secure Connectivity using Intents",
        "author": "Loay Abdelrazek, and Filippo Rebecchi",
        "link": "http://arxiv.org/abs/2509.25462v1",
        "abstract": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks."
    },
    {
        "date": "2025-09",
        "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
        "author": "Zhibo Hou, Zhiyu An, and Wan Du",
        "link": "http://arxiv.org/abs/2509.25438v1",
        "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration"
    },
    {
        "date": "2025-09",
        "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors",
        "author": "Hui Wang, Nima Tashakor, Xiaoyang Tian, Hans D. Schotten, and Stefan M. Goetz",
        "link": "http://arxiv.org/abs/2509.25394v1",
        "abstract": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention."
    },
    {
        "date": "2025-09",
        "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following",
        "author": "FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, and Yichao Wu",
        "link": "http://arxiv.org/abs/2509.25148v1",
        "abstract": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment."
    },
    {
        "date": "2025-09",
        "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary",
        "author": "Daniil Dmitriev, Harald Eskelund Franck, Carolin Heinzler, and Amartya Sanyal",
        "link": "http://arxiv.org/abs/2509.25135v1",
        "abstract": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms."
    },
    {
        "date": "2025-09",
        "title": "MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial Purification",
        "author": "Xiaoyi Huang, Junwei Wu, Kejia Zhang, Carl Yang, and Zhiming Luo",
        "link": "http://arxiv.org/abs/2509.25082v1",
        "abstract": "Adversarial purification with diffusion models has emerged as a promising\ndefense strategy, but existing methods typically rely on uniform noise\ninjection, which indiscriminately perturbs all frequencies, corrupting semantic\nstructures and undermining robustness. Our empirical study reveals that\nadversarial perturbations are not uniformly distributed: they are predominantly\nconcentrated in high-frequency regions, with heterogeneous magnitude intensity\npatterns that vary across frequencies and attack types. Motivated by this\nobservation, we introduce MANI-Pure, a magnitude-adaptive purification\nframework that leverages the magnitude spectrum of inputs to guide the\npurification process. Instead of injecting homogeneous noise, MANI-Pure\nadaptively applies heterogeneous, frequency-targeted noise, effectively\nsuppressing adversarial perturbations in fragile high-frequency, low-magnitude\nbands while preserving semantically critical low-frequency content. Extensive\nexperiments on CIFAR-10 and ImageNet-1K validate the effectiveness of\nMANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original\nclassifier, while boosting robust accuracy by 2.15, and achieves the top-1\nrobust accuracy on the RobustBench leaderboard, surpassing the previous\nstate-of-the-art method."
    },
    {
        "date": "2025-09",
        "title": "Domain-Robust Marine Plastic Detection Using Vision Models",
        "author": "Saanvi Kataria",
        "link": "http://arxiv.org/abs/2510.03294v1",
        "abstract": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths."
    }
]