[
    {
        "date": "2025-01",
        "title": "SMT-Boosted Security Types for Low-Level MPC",
        "author": "Christian Skalka, and Joseph P. Near",
        "link": "http://arxiv.org/abs/2501.17824v1",
        "abstract": "Secure Multi-Party Computation (MPC) is an important enabling technology for\ndata privacy in modern distributed applications. We develop a new type theory\nto automatically enforce correctness,confidentiality, and integrity properties\nof protocols written in the \\emph{Prelude/Overture} language framework.\nJudgements in the type theory are predicated on SMT verifications in a theory\nof finite fields, which supports precise and efficient analysis. Our approach\nis automated, compositional, scalable, and generalizes to arbitrary prime\nfields for data and key sizes."
    },
    {
        "date": "2025-01",
        "title": "U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal Learning",
        "author": "Md Kaykobad Reza, Niki Nezakati, Ameya Patil, Mashhour Solh, and M. Salman Asif",
        "link": "http://arxiv.org/abs/2501.17823v1",
        "abstract": "Multimodal learning often relies on designing new models and complex training\nstrategies to achieve optimal performance. We present Unified Unimodal\nAdaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using\nlow-rank adaptation (LoRA) for various multimodal tasks. Our method\nsignificantly reduces the number of learnable parameters and eliminates the\nneed for complex training strategies, such as alternating training, gradient\nmodifications, or unimodal fine-tuning. To address missing modalities during\nboth training and testing, we introduce Mask Tokens (MT), which generate\nmissing modality features from available modalities using a single token per\nmodality. This simplifies the process, removing the need for specialized\nfeature estimation or prompt-tuning methods. Our evaluation demonstrates that\nU2A matches or outperforms state-of-the-art methods in both complete and\nmissing modality settings, showcasing strong performance and robustness across\nvarious modalities, tasks, and datasets. We also analyze and report the\neffectiveness of Mask Tokens in different missing modality scenarios. Overall,\nour method provides a robust, flexible, and efficient solution for multimodal\nlearning, with minimal computational overhead."
    },
    {
        "date": "2025-01",
        "title": "Atomic Transfer Graphs: Secure-by-design Protocols for Heterogeneous Blockchain Ecosystems",
        "author": "Stephan D\u00fcbler, Federico Badaloni, Pedro Moreno-Sanchez, and Clara Schneidewind",
        "link": "http://arxiv.org/abs/2501.17786v1",
        "abstract": "The heterogeneity of the blockchain landscape has motivated the design of\nblockchain protocols tailored to specific blockchains and applications that,\nhence, require custom security proofs. We observe that many blockchain\nprotocols share common security and functionality goals, which can be captured\nby an atomic transfer graph (ATG) describing the structure of desired\ntransfers. Based on this observation, we contribute a framework for generating\nsecure-by-design protocols that realize these goals. The resulting protocols\nbuild upon Conditional Timelock Contracts (CTLCs), a novel minimal smart\ncontract functionality that can be implemented in a large variety of\ncryptocurrencies with a restricted scripting language (e.g., Bitcoin), and\npayment channels. We show how ATGs, in addition to enabling novel applications,\ncapture the security and functionality goals of existing applications,\nincluding many examples from payment channel networks and complex multi-party\ncross-currency swaps among Ethereum-style cryptocurrencies. Our framework is\nthe first to provide generic and provably secure protocols for all these use\ncases while matching or improving the performance of existing use-case-specific\nprotocols."
    },
    {
        "date": "2025-01",
        "title": "Investigating Vulnerability Disclosures in Open-Source Software Using Bug Bounty Reports and Security Advisories",
        "author": "Jessy Ayala, Yu-Jye Tung, and Joshua Garcia",
        "link": "http://arxiv.org/abs/2501.17748v1",
        "abstract": "In the world of open-source software (OSS), the number of known\nvulnerabilities has tremendously increased. The GitHub Advisory Database\ncontains advisories for security risks in GitHub-hosted OSS projects. As of\n09/25/2023, there are 197,609 unreviewed GitHub security advisories. Of those\nunreviewed, at least 63,852 are publicly documented vulnerabilities,\npotentially leaving many OSS projects vulnerable. Recently, bug bounty\nplatforms have emerged to focus solely on providing bounties to help secure\nOSS. In this paper, we conduct an empirical study on 3,798 reviewed GitHub\nsecurity advisories and 4,033 disclosed OSS bug bounty reports, a perspective\nthat is currently understudied, because they contain comprehensive information\nabout security incidents, e.g., the nature of vulnerabilities, their impact,\nand how they were resolved. We are the first to determine the explicit process\ndescribing how OSS vulnerabilities propagate from security advisories and bug\nbounty reports, which are the main intermediaries between vulnerability\nreporters, OSS maintainers, and dependent projects, to vulnerable OSS projects\nand entries in global vulnerability databases and possibly back. This process\nuncovers how missing or delayed CVE assignments for OSS vulnerabilities result\nin projects, both in and out of OSS, not being notified of necessary security\nupdates promptly and corresponding bottlenecks. Based on our findings, we\nprovide suggestions, actionable items, and future research directions to help\nimprove the security posture of OSS projects."
    },
    {
        "date": "2025-01",
        "title": "Attacker Control and Bug Prioritization",
        "author": "Guilhem Lacombe, and S\u00e9bastien Bardin",
        "link": "http://arxiv.org/abs/2501.17740v1",
        "abstract": "As bug-finding methods improve, bug-fixing capabilities are exceeded,\nresulting in an accumulation of potential vulnerabilities. There is thus a need\nfor efficient and precise bug prioritization based on exploitability. In this\nwork, we explore the notion of control of an attacker over a vulnerability's\nparameters, which is an often overlooked factor of exploitability. We show that\ntaint as well as straightforward qualitative and quantitative notions of\ncontrol are not enough to effectively differentiate vulnerabilities. Instead,\nwe propose to focus analysis on feasible value sets, which we call domains of\ncontrol, in order to better take into account threat models and expert insight.\nOur new Shrink and Split algorithm efficiently extracts domains of control from\npath constraints obtained with symbolic execution and renders them in an easily\nprocessed, human-readable form. This in turn allows to automatically compute\nmore complex control metrics, such as weighted Quantitative Control, which\nfactors in the varying threat levels of different values. Experiments show that\nour method is both efficient and precise. In particular, it is the only one\nable to distinguish between vulnerabilities such as cve-2019-14192 and\ncve-2022-30552, while revealing a mistake in the human evaluation of\ncve-2022-30790. The high degree of automation of our tool also brings us closer\nto a fully-automated evaluation pipeline."
    },
    {
        "date": "2025-01",
        "title": "BitMLx: Secure Cross-chain Smart Contracts For Bitcoin-style Cryptocurrencies",
        "author": "Federico Badaloni, Sebastian Holler, Chrysoula Oikonomou, Pedro Moreno-Sanchez, and Clara Schneidewind",
        "link": "http://arxiv.org/abs/2501.17733v1",
        "abstract": "A smart contract is an interactive program that governs funds in the realm of\na single cryptocurrency. Yet, the many existing cryptocurrencies have spurred\nthe design of cross-chain applications that require interactions with multiple\ncryptocurrencies simultaneously. Currently, cross-chain applications are\nimplemented as use-case-specific cryptographic protocols that serve as overlay\nto synchronize smart contract executions in the different cryptocurrencies.\nHence, their design requires substantial expertise, as well as a security\nanalysis in complex cryptographic frameworks.\n  In this work, we present BitMLx, the first domain-specific language for\ncross-chain smart contracts, enabling interactions with several users that hold\nfunds across multiple Bitcoin-like cryptocurrencies. We contribute a compiler\nto automatically translate a BitMLx contract into one contract per involved\ncryptocurrency and a user strategy that synchronizes the execution of these\ncontracts. We prove that an honest user, who follows the prescribed strategy\nwhen interacting with the several contracts, ends up with at least as many\nfunds as in the corresponding execution of the BitMLx contract. Last, but not\nleast, we implement the BitMLx compiler and demonstrate its utility in the\ndesign of illustrative examples of cross-chain applications such as multi-chain\ndonations or loans across different cryptocurrencies."
    },
    {
        "date": "2025-01",
        "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
        "author": "Derui Wang, Kristen Moore, Diksha Goel, Minjune Kim, Gang Li, Yang Li, Robin Doss, Minhui Xue, Bo Li, Seyit Camtepe, and Liming Zhu",
        "link": "http://arxiv.org/abs/2501.17667v1",
        "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control\nand decision-making tasks due to its strong performance in dynamic\nenvironments. However, DRL agents are vulnerable to noisy observations and\nadversarial attacks, and concerns about the adversarial robustness of DRL\nsystems have emerged. Recent efforts have focused on addressing these\nrobustness issues by establishing rigorous theoretical guarantees for the\nreturns achieved by DRL agents in adversarial settings. Among these approaches,\npolicy smoothing has proven to be an effective and scalable method for\ncertifying the robustness of DRL agents. Nevertheless, existing certifiably\nrobust DRL relies on policies trained with simple Gaussian augmentations,\nresulting in a suboptimal trade-off between certified robustness and certified\nreturn. To address this issue, we introduce a novel paradigm dubbed\n\\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy\n(\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies,\nachieving better utility without compromising provable robustness. By\nleveraging the insight that the global certified radius can be derived from\nlocal certified radii based on training-time statistics, \\texttt{CAMP}\nformulates a surrogate loss related to the local certified radius and optimizes\nthe policy guided by this surrogate loss. We also introduce \\textit{policy\nimitation} as a novel technique to stabilize \\texttt{CAMP} training.\nExperimental results demonstrate that \\texttt{CAMP} significantly improves the\nrobustness-return trade-off across various tasks. Based on the results,\n\\texttt{CAMP} can achieve up to twice the certified expected return compared to\nthat of baselines. Our code is available at\nhttps://github.com/NeuralSec/camp-robust-rl."
    },
    {
        "date": "2025-01",
        "title": "VoicePrompter: Robust Zero-Shot Voice Conversion with Voice Prompt and Conditional Flow Matching",
        "author": "Ha-Yeong Choi, and Jaehan Park",
        "link": "http://arxiv.org/abs/2501.17612v1",
        "abstract": "Despite remarkable advancements in recent voice conversion (VC) systems,\nenhancing speaker similarity in zero-shot scenarios remains challenging. This\nchallenge arises from the difficulty of generalizing and adapting speaker\ncharacteristics in speech within zero-shot environments, which is further\ncomplicated by mismatch between the training and inference processes. To\naddress these challenges, we propose VoicePrompter, a robust zero-shot VC model\nthat leverages in-context learning with voice prompts. VoicePrompter is\ncomposed of (1) a factorization method that disentangles speech components and\n(2) a DiT-based conditional flow matching (CFM) decoder that conditions on\nthese factorized features and voice prompts. Additionally, (3) latent mixup is\nused to enhance in-context learning by combining various speaker features. This\napproach improves speaker similarity and naturalness in zero-shot VC by\napplying mixup to latent representations. Experimental results demonstrate that\nVoicePrompter outperforms existing zero-shot VC systems in terms of speaker\nsimilarity, speech intelligibility, and audio quality. Our demo is available at\n\\url{https://hayeong0.github.io/VoicePrompter-demo/}."
    },
    {
        "date": "2025-01",
        "title": "Solving Urban Network Security Games: Learning Platform, Benchmark, and Challenge for AI Research",
        "author": "Shuxin Zhuang, Shuxin Li, Tianji Yang, Muheng Li, Xianjie Shi, Bo An, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2501.17559v1",
        "abstract": "After the great achievement of solving two-player zero-sum games, more and\nmore AI researchers focus on solving multiplayer games. To facilitate the\ndevelopment of designing efficient learning algorithms for solving multiplayer\ngames, we propose a multiplayer game platform for solving Urban Network\nSecurity Games (\\textbf{UNSG}) that model real-world scenarios. That is,\npreventing criminal activity is a highly significant responsibility assigned to\npolice officers in cities, and police officers have to allocate their limited\nsecurity resources to interdict the escaping criminal when a crime takes place\nin a city. This interaction between multiple police officers and the escaping\ncriminal can be modeled as a UNSG. The variants of UNSGs can model different\nreal-world settings, e.g., whether real-time information is available or not,\nand whether police officers can communicate or not. The main challenges of\nsolving this game include the large size of the game and the co-existence of\ncooperation and competition. While previous efforts have been made to tackle\nUNSGs, they have been hampered by performance and scalability issues.\nTherefore, we propose an open-source UNSG platform (\\textbf{GraphChase}) for\ndesigning efficient learning algorithms for solving UNSGs. Specifically,\nGraphChase offers a unified and flexible game environment for modeling various\nvariants of UNSGs, supporting the development, testing, and benchmarking of\nalgorithms. We believe that GraphChase not only facilitates the development of\nefficient algorithms for solving real-world problems but also paves the way for\nsignificant advancements in algorithmic development for solving general\nmultiplayer games."
    },
    {
        "date": "2025-01",
        "title": "How Much Do Code Language Models Remember? An Investigation on Data Extraction Attacks before and after Fine-tuning",
        "author": "Fabio Salerno, Ali Al-Kaswan, and Maliheh Izadi",
        "link": "http://arxiv.org/abs/2501.17501v1",
        "abstract": "Code language models, while widely popular, are often trained on unsanitized\nsource code gathered from across the Internet. Previous work revealed that\npre-trained models can remember the content of their training data and\nregurgitate them through data extraction attacks. Due to the large size of\ncurrent models, only a few entities have the resources for pre-training such\nmodels. However, fine-tuning requires fewer resources and is increasingly used\nby both small and large entities for its effectiveness on specialized data.\nSuch small curated data for fine-tuning might contain sensitive information or\nproprietary assets. In this study, we attack both pre-trained and fine-tuned\ncode language models to investigate the extent of data extractability. We first\ndevelop a custom benchmark to assess the vulnerability of both pre-training and\nfine-tuning samples to extraction attacks. Our findings reveal that 54.9% of\nextractable pre-training data could be retrieved from StarCoder2-15B, whereas\nthis number decreased to 23.5% after fine-tuning. This indicates that\nfine-tuning reduces the extractability of pre-training data. However, compared\nto larger models, fine-tuning smaller models increases their vulnerability to\ndata extraction attacks on fine-tuning data. Given the potential sensitivity of\nfine-tuning data, this can lead to more severe consequences. Lastly, we also\nmanually analyzed 2000 extractable samples before and after fine-tuning. We\nalso found that data carriers and licensing information are the most likely\ndata categories to be memorized from pre-trained and fine-tuned models, while\nthe latter is the most likely to be forgotten after fine-tuning."
    },
    {
        "date": "2025-01",
        "title": "Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation",
        "author": "Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu",
        "link": "http://arxiv.org/abs/2501.17433v1",
        "abstract": "Recent research shows that Large Language Models (LLMs) are vulnerable to\nharmful fine-tuning attacks -- models lose their safety alignment ability after\nfine-tuning on a few harmful samples. For risk mitigation, a guardrail is\ntypically used to filter out harmful samples before fine-tuning. By designing a\nnew red-teaming method, we in this paper show that purely relying on the\nmoderation guardrail for data filtration is not reliable. Our proposed attack\nmethod, dubbed Virus, easily bypasses the guardrail moderation by slightly\nmodifying the harmful data. Experimental results show that the harmful data\noptimized by Virus is not detectable by the guardrail with up to 100\\% leakage\nratio, and can simultaneously achieve superior attack performance. Finally, the\nkey message we want to convey through this paper is that: \\textbf{it is\nreckless to consider guardrail moderation as a clutch at straws towards harmful\nfine-tuning attack}, as it cannot solve the inherent safety issue of the\npre-trained LLMs. Our code is available at https://github.com/git-disl/Virus"
    },
    {
        "date": "2025-01",
        "title": "Reqo: A Robust and Explainable Query Optimization Cost Model",
        "author": "Baoming Chang, Amin Kamali, and Verena Kantere",
        "link": "http://arxiv.org/abs/2501.17414v1",
        "abstract": "In recent years, there has been a growing interest in using machine learning\n(ML) in query optimization to select more efficient plans. Existing\nlearning-based query optimizers use certain model architectures to convert\ntree-structured query plans into representations suitable for downstream ML\ntasks. As the design of these architectures significantly impacts cost\nestimation, we propose a tree model architecture based on Bidirectional Graph\nNeural Networks (Bi-GNN) aggregated by Gated Recurrent Units (GRUs) to achieve\nmore accurate cost estimates. The inherent uncertainty of data and model\nparameters also leads to inaccurate cost estimates, resulting in suboptimal\nplans and less robust query performance. To address this, we implement a novel\nlearning-to-rank cost model that effectively quantifies the uncertainty in cost\nestimates using approximate probabilistic ML. This model adaptively integrates\nquantified uncertainty with estimated costs and learns from comparing pairwise\nplans, achieving more robust performance. In addition, we propose the first\nexplainability technique specifically designed for learning-based cost models.\nThis technique explains the contribution of any subgraphs in the query plan to\nthe final predicted cost, which can be integrated and trained with any\nlearning-based cost model to significantly boost the model's explainability. By\nincorporating these innovations, we propose a cost model for a Robust and\nExplainable Query Optimizer, Reqo, that improves the accuracy, robustness, and\nexplainability of cost estimation, outperforming state-of-the-art approaches in\nall three dimensions."
    },
    {
        "date": "2025-01",
        "title": "When Everyday Devices Become Weapons: A Closer Look at the Pager and Walkie-talkie Attacks",
        "author": "Pantha Protim Sarker, Upoma Das, Nitin Varshney, Shang Shi, Akshay Kulkarni, Farimah Farahmandi, and Mark Tehranipoor",
        "link": "http://arxiv.org/abs/2501.17405v1",
        "abstract": "Battery-powered technologies like pagers and walkie-talkies have long been\nintegral to civilian and military operations. However, the potential for such\neveryday devices to be weaponized has largely been underestimated in the realm\nof cybersecurity. In September 2024, Lebanon experienced a series of\nunprecedented, coordinated explosions triggered through compromised pagers and\nwalkie-talkies, creating a new category of attack in the domain of\ncyber-physical warfare. This attack not only disrupted critical communication\nnetworks but also resulted in injuries, loss of life, and exposed significant\nnational security vulnerabilities, prompting governments and organizations\nworldwide to reevaluate their cybersecurity frameworks. This article provides\nan in-depth investigation into the infamous Pager and Walkie-Talkie attacks,\nanalyzing both technical and non-technical dimensions. Furthermore, the study\nextends its scope to explore vulnerabilities in other battery-powered\ninfrastructures, such as battery management systems, highlighting their\npotential exploitation. Existing prevention and detection techniques are\nreviewed, with an emphasis on their limitations and the challenges they face in\naddressing emerging threats. Finally, the article discusses emerging\nmethodologies, particularly focusing on the role of physical inspection, as a\ncritical component of future security measures. This research aims to provide\nactionable insights to bolster the resilience of cyber-physical systems in an\nincreasingly interconnected world."
    },
    {
        "date": "2025-01",
        "title": "Poisoning Attacks and Defenses to Federated Unlearning",
        "author": "Wenbin Wang, Qiwen Ma, Zifan Zhang, Yuchen Liu, Zhuqing Liu, and Minghong Fang",
        "link": "http://arxiv.org/abs/2501.17396v1",
        "abstract": "Federated learning allows multiple clients to collaboratively train a global\nmodel with the assistance of a server. However, its distributed nature makes it\nsusceptible to poisoning attacks, where malicious clients can compromise the\nglobal model by sending harmful local model updates to the server. To unlearn\nan accurate global model from a poisoned one after identifying malicious\nclients, federated unlearning has been introduced. Yet, current research on\nfederated unlearning has primarily concentrated on its effectiveness and\nefficiency, overlooking the security challenges it presents. In this work, we\nbridge the gap via proposing BadUnlearn, the first poisoning attacks targeting\nfederated unlearning. In BadUnlearn, malicious clients send specifically\ndesigned local model updates to the server during the unlearning process,\naiming to ensure that the resulting unlearned model remains poisoned. To\nmitigate these threats, we propose UnlearnGuard, a robust federated unlearning\nframework that is provably robust against both existing poisoning attacks and\nour BadUnlearn. The core concept of UnlearnGuard is for the server to estimate\nthe clients' local model updates during the unlearning process and employ a\nfiltering strategy to verify the accuracy of these estimations. Theoretically,\nwe prove that the model unlearned through UnlearnGuard closely resembles one\nobtained by train-from-scratch. Empirically, we show that BadUnlearn can\neffectively corrupt existing federated unlearning methods, while UnlearnGuard\nremains secure against poisoning attacks."
    },
    {
        "date": "2025-01",
        "title": "Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed Computing",
        "author": "Minghong Fang, Zhuqing Liu, Xuecen Zhao, and Jia Liu",
        "link": "http://arxiv.org/abs/2501.17392v1",
        "abstract": "Federated learning (FL) has gained attention as a distributed learning\nparadigm for its data privacy benefits and accelerated convergence through\nparallel computation. Traditional FL relies on a server-client (SC)\narchitecture, where a central server coordinates multiple clients to train a\nglobal model, but this approach faces scalability challenges due to server\ncommunication bottlenecks. To overcome this, the ring-all-reduce (RAR)\narchitecture has been introduced, eliminating the central server and achieving\nbandwidth optimality. However, the tightly coupled nature of RAR's ring\ntopology exposes it to unique Byzantine attack risks not present in SC-based\nFL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms\nremains an open problem. To address this gap, we propose BRACE\n(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve\nboth Byzantine robustness and communication efficiency. We provide theoretical\nguarantees for the convergence of BRACE under Byzantine attacks, demonstrate\nits bandwidth efficiency, and validate its practical effectiveness through\nexperiments. Our work offers a foundational understanding of Byzantine-robust\nRAR-based FL design."
    },
    {
        "date": "2025-01",
        "title": "A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning",
        "author": "Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, and Renjing Xu",
        "link": "http://arxiv.org/abs/2501.17384v1",
        "abstract": "Recently, empowered with the powerful capabilities of neural networks,\nreinforcement learning (RL) has successfully tackled numerous challenging\ntasks. However, while these models demonstrate enhanced decision-making\nabilities, they are increasingly prone to overfitting. For instance, a trained\nRL model often fails to generalize to even minor variations of the same task,\nsuch as a change in background color or other minor semantic differences. To\naddress this issue, we propose a dual-agent adversarial policy learning\nframework, which allows agents to spontaneously learn the underlying semantics\nwithout introducing any human prior knowledge. Specifically, our framework\ninvolves a game process between two agents: each agent seeks to maximize the\nimpact of perturbing on the opponent's policy by producing representation\ndifferences for the same state, while maintaining its own stability against\nsuch perturbations. This interaction encourages agents to learn generalizable\npolicies, capable of handling irrelevant features from the high-dimensional\nobservations. Extensive experimental results on the Procgen benchmark\ndemonstrate that the adversarial process significantly improves the\ngeneralization performance of both agents, while also being applied to various\nRL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial\nframework, the RL agent outperforms the baseline methods by a significant\nmargin, especially in hard-level tasks, marking a significant step forward in\nthe generalization capabilities of deep reinforcement learning."
    },
    {
        "date": "2025-01",
        "title": "Do We Really Need to Design New Byzantine-robust Aggregation Rules?",
        "author": "Minghong Fang, Seyedsina Nabavirazavi, Zhuqing Liu, Wei Sun, Sundararaja Sitharama Iyengar, and Haibo Yang",
        "link": "http://arxiv.org/abs/2501.17381v1",
        "abstract": "Federated learning (FL) allows multiple clients to collaboratively train a\nglobal machine learning model through a server, without exchanging their\nprivate training data. However, the decentralized aspect of FL makes it\nsusceptible to poisoning attacks, where malicious clients can manipulate the\nglobal model by sending altered local model updates. To counter these attacks,\na variety of aggregation rules designed to be resilient to Byzantine failures\nhave been introduced. Nonetheless, these methods can still be vulnerable to\nsophisticated attacks or depend on unrealistic assumptions about the server. In\nthis paper, we demonstrate that there is no need to design new Byzantine-robust\naggregation rules; instead, FL can be secured by enhancing the robustness of\nwell-established aggregation rules. To this end, we present FoundationFL, a\nnovel defense mechanism against poisoning attacks. FoundationFL involves the\nserver generating synthetic updates after receiving local model updates from\nclients. It then applies existing Byzantine-robust foundational aggregation\nrules, such as Trimmed-mean or Median, to combine clients' model updates with\nthe synthetic ones. We theoretically establish the convergence performance of\nFoundationFL under Byzantine settings. Comprehensive experiments across several\nreal-world datasets validate the efficiency of our FoundationFL method."
    },
    {
        "date": "2025-01",
        "title": "Enabling Low-Cost Secure Computing on Untrusted In-Memory Architectures",
        "author": "Sahar Ghoflsaz Ghinani, Jingyao Zhang, and Elaheh Sadredini",
        "link": "http://arxiv.org/abs/2501.17292v1",
        "abstract": "Modern computing systems are limited in performance by the memory bandwidth\navailable to processors, a problem known as the memory wall.\nProcessing-in-Memory (PIM) promises to substantially improve this problem by\nmoving processing closer to the data, improving effective data bandwidth, and\nleading to superior performance on memory-intensive workloads. However,\nintegrating PIM modules within a secure computing system raises an interesting\nchallenge: unencrypted data has to move off-chip to the PIM, exposing the data\nto attackers and breaking assumptions on Trusted Computing Bases (TCBs). To\ntackle this challenge, this paper leverages multi-party computation (MPC)\ntechniques, specifically arithmetic secret sharing and Yao's garbled circuits,\nto outsource bandwidth-intensive computation securely to PIM. Additionally, we\nleverage precomputation optimization to prevent the CPU's portion of the MPC\nfrom becoming a bottleneck. We evaluate our approach using the UPMEM PIM system\nover various applications such as Deep Learning Recommendation Model inference\nand Logistic Regression. Our evaluations demonstrate up to a $14.66\\times$\nspeedup compared to a secure CPU configuration while maintaining data\nconfidentiality and integrity when outsourcing linear and/or nonlinear\ncomputation."
    },
    {
        "date": "2025-01",
        "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks Detection: A Comparative Analysis",
        "author": "Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, and Amit Joshi",
        "link": "http://arxiv.org/abs/2501.17123v1",
        "abstract": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
    },
    {
        "date": "2025-01",
        "title": "Context is Key in Agent Security",
        "author": "Lillian Tsai, and Eugene Bagdasarian",
        "link": "http://arxiv.org/abs/2501.17070v1",
        "abstract": "Judging the safety of an action, whether taken by a human or a system, must\ntake into account the context in which the action takes place. Deleting an\nemail from user's mailbox may or may not be appropriate depending on email's\ncontent, user's goals, or even available space. Systems today that make these\njudgements -- providing security against harmful or inappropriate actions --\nrely on manually-crafted policies or user confirmation for each relevant\ncontext. With the upcoming deployment of systems like generalist agents, we\nargue that we must rethink security designs to adapt to the scale of contexts\nand capabilities of these systems. As a first step, this paper explores\ncontextual security in the domain of agents and proposes contextual security\nfor agents (Conseca), a framework to generate just-in-time, contextual, and\nhuman-verifiable security policies."
    },
    {
        "date": "2025-01",
        "title": "RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples",
        "author": "Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Ali Ansari, Sepehr Ghobadi, Masoud Hadi, Arshia Soltani Moakhar, Mohammad Azizmalayeri, Mahdieh Soleymani Baghshah, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.16971v1",
        "abstract": "In recent years, there have been significant improvements in various forms of\nimage outlier detection. However, outlier detection performance under\nadversarial settings lags far behind that in standard settings. This is due to\nthe lack of effective exposure to adversarial scenarios during training,\nespecially on unseen outliers, leading to detection models failing to learn\nrobust features. To bridge this gap, we introduce RODEO, a data-centric\napproach that generates effective outliers for robust outlier detection. More\nspecifically, we show that incorporating outlier exposure (OE) and adversarial\ntraining can be an effective strategy for this purpose, as long as the exposed\ntraining outliers meet certain characteristics, including diversity, and both\nconceptual differentiability and analogy to the inlier samples. We leverage a\ntext-to-image model to achieve this goal. We demonstrate both quantitatively\nand qualitatively that our adaptive OE method effectively generates ``diverse''\nand ``near-distribution'' outliers, leveraging information from both text and\nimage domains. Moreover, our experimental results show that utilizing our\nsynthesized outliers significantly enhances the performance of the outlier\ndetector, particularly in adversarial settings."
    },
    {
        "date": "2025-01",
        "title": "Few Edges Are Enough: Few-Shot Network Attack Detection with Graph Neural Networks",
        "author": "Tristan Bilot, Nour El Madhoun, Khaldoun Al Agha, and Anis Zouaoui",
        "link": "http://arxiv.org/abs/2501.16964v1",
        "abstract": "Detecting cyberattacks using Graph Neural Networks (GNNs) has seen promising\nresults recently. Most of the state-of-the-art models that leverage these\ntechniques require labeled examples, hard to obtain in many real-world\nscenarios. To address this issue, unsupervised learning and Self-Supervised\nLearning (SSL) have emerged as interesting approaches to reduce the dependency\non labeled data. Nonetheless, these methods tend to yield more anomalous\ndetection algorithms rather than effective attack detection systems. This paper\nintroduces Few Edges Are Enough (FEAE), a GNN-based architecture trained with\nSSL and Few-Shot Learning (FSL) to better distinguish between false positive\nanomalies and actual attacks. To maximize the potential of few-shot examples,\nour model employs a hybrid self-supervised objective that combines the\nadvantages of contrastive-based and reconstruction-based SSL. By leveraging\nonly a minimal number of labeled attack events, represented as attack edges,\nFEAE achieves competitive performance on two well-known network datasets\ncompared to both supervised and unsupervised methods. Remarkably, our\nexperimental results unveil that employing only 1 malicious event for each\nattack type in the dataset is sufficient to achieve substantial improvements.\nFEAE not only outperforms self-supervised GNN baselines but also surpasses some\nsupervised approaches on one of the datasets."
    },
    {
        "date": "2025-01",
        "title": "Stack Overflow Meets Replication: Security Research Amid Evolving Code Snippets (Extended Version)",
        "author": "Alfusainey Jallow, and Sven Bugiel",
        "link": "http://arxiv.org/abs/2501.16948v1",
        "abstract": "We study the impact of Stack Overflow code evolution on the stability of\nprior research findings derived from Stack Overflow data and provide\nrecommendations for future studies. We systematically reviewed papers published\nbetween 2005--2023 to identify key aspects of Stack Overflow that can affect\nstudy results, such as the language or context of code snippets. Our analysis\nreveals that certain aspects are non-stationary over time, which could lead to\ndifferent conclusions if experiments are repeated at different times. We\nreplicated six studies using a more recent dataset to demonstrate this risk.\nOur findings show that four papers produced significantly different results\nthan the original findings, preventing the same conclusions from being drawn\nwith a newer dataset version. Consequently, we recommend treating Stack\nOverflow as a time series data source to provide context for interpreting\ncross-sectional research conclusions."
    },
    {
        "date": "2025-01",
        "title": "Projection-free Algorithms for Online Convex Optimization with Adversarial Constraints",
        "author": "Dhruv Sarkar, Aprameyo Chakrabartty, Subhamon Supantha, Palash Dey, and Abhishek Sinha",
        "link": "http://arxiv.org/abs/2501.16919v1",
        "abstract": "We study a generalization of the Online Convex Optimization (OCO) framework\nwith time-varying adversarial constraints. In this problem, after selecting a\nfeasible action from the convex decision set $X,$ a convex constraint function\nis revealed alongside the cost function in each round. Our goal is to design a\ncomputationally efficient learning policy that achieves a small regret with\nrespect to the cost functions and a small cumulative constraint violation (CCV)\nwith respect to the constraint functions over a horizon of length $T$. It is\nwell-known that the projection step constitutes the major computational\nbottleneck of the standard OCO algorithms. However, for many structured\ndecision sets, linear functions can be efficiently optimized over the decision\nset. We propose a *projection-free* online policy which makes a single call to\na Linear Program (LP) solver per round. Our method outperforms state-of-the-art\nprojection-free online algorithms with adversarial constraints, achieving\nimproved bounds of $\\tilde{O}(T^{\\frac{3}{4}})$ for both regret and CCV. The\nproposed algorithm is conceptually simple - it first constructs a surrogate\ncost function as a non-negative linear combination of the cost and constraint\nfunctions. Then, it passes the surrogate costs to a new, adaptive version of\nthe online conditional gradient subroutine, which we propose in this paper."
    },
    {
        "date": "2025-01",
        "title": "Adversarial Masked Autoencoder Purifier with Defense Transferability",
        "author": "Yuan-Chih Chen, and Chun-Shien Lu",
        "link": "http://arxiv.org/abs/2501.16904v1",
        "abstract": "The study of adversarial defense still struggles to combat with advanced\nadversarial attacks. In contrast to most prior studies that rely on the\ndiffusion model for test-time defense to remarkably increase the inference\ntime, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked\nAutoEncoder (MAE) into an adversarial purifier framework for test-time\npurification. While MAEP achieves promising adversarial robustness, it\nparticularly features model defense transferability and attack generalization\nwithout relying on using additional data that is different from the training\ndataset. To our knowledge, MAEP is the first study of adversarial purifier\nbased on MAE. Extensive experimental results demonstrate that our method can\nnot only maintain clear accuracy with only a slight drop but also exhibit a\nclose gap between the clean and robust accuracy. Notably, MAEP trained on\nCIFAR10 achieves state-of-the-art performance even when tested directly on\nImageNet, outperforming existing diffusion-based models trained specifically on\nImageNet."
    },
    {
        "date": "2025-01",
        "title": "RAINER: A Robust Ensemble Learning Grid Search-Tuned Framework for Rainfall Patterns Prediction",
        "author": "Zhenqi Li, Junhao Zhong, Hewei Wang, Jinfeng Xu, Yijie Li, Jinjiang You, Jiayi Zhang, Runzhi Wu, and Soumyabrata Dev",
        "link": "http://arxiv.org/abs/2501.16900v1",
        "abstract": "Rainfall prediction remains a persistent challenge due to the highly\nnonlinear and complex nature of meteorological data. Existing approaches lack\nsystematic utilization of grid search for optimal hyperparameter tuning,\nrelying instead on heuristic or manual selection, frequently resulting in\nsub-optimal results. Additionally, these methods rarely incorporate newly\nconstructed meteorological features such as differences between temperature and\nhumidity to capture critical weather dynamics. Furthermore, there is a lack of\nsystematic evaluation of ensemble learning techniques and limited exploration\nof diverse advanced models introduced in the past one or two years. To address\nthese limitations, we propose a robust ensemble learning grid search-tuned\nframework (RAINER) for rainfall prediction. RAINER incorporates a comprehensive\nfeature engineering pipeline, including outlier removal, imputation of missing\nvalues, feature reconstruction, and dimensionality reduction via Principal\nComponent Analysis (PCA). The framework integrates novel meteorological\nfeatures to capture dynamic weather patterns and systematically evaluates\nnon-learning mathematical-based methods and a variety of machine learning\nmodels, from weak classifiers to advanced neural networks such as\nKolmogorov-Arnold Networks (KAN). By leveraging grid search for hyperparameter\ntuning and ensemble voting techniques, RAINER achieves promising results within\nreal-world datasets."
    },
    {
        "date": "2025-01",
        "title": "Secure Federated Graph-Filtering for Recommender Systems",
        "author": "Julien Nicolas, C\u00e9sar Sabater, Mohamed Maouche, Sonia Ben Mokhtar, and Mark Coates",
        "link": "http://arxiv.org/abs/2501.16888v1",
        "abstract": "Recommender systems often rely on graph-based filters, such as normalized\nitem-item adjacency matrices and low-pass filters. While effective, the\ncentralized computation of these components raises concerns about privacy,\nsecurity, and the ethical use of user data. This work proposes two\ndecentralized frameworks for securely computing these critical graph components\nwithout centralizing sensitive information. The first approach leverages\nlightweight Multi-Party Computation and distributed singular vector\ncomputations to privately compute key graph filters. The second extends this\nframework by incorporating low-rank approximations, enabling a trade-off\nbetween communication efficiency and predictive performance. Empirical\nevaluations on benchmark datasets demonstrate that the proposed methods achieve\ncomparable accuracy to centralized state-of-the-art systems while ensuring data\nconfidentiality and maintaining low communication costs. Our results highlight\nthe potential for privacy-preserving decentralized architectures to bridge the\ngap between utility and user data protection in modern recommender systems."
    },
    {
        "date": "2025-01",
        "title": "Bones of Contention: Exploring Query-Efficient Attacks Against Skeleton Recognition Systems",
        "author": "Yuxin Cao, Kai Ye, Derui Wang, Minhui Xue, Hao Ge, Chenxiong Qian, and Jin Song Dong",
        "link": "http://arxiv.org/abs/2501.16843v1",
        "abstract": "Skeleton action recognition models have secured more attention than\nvideo-based ones in various applications due to privacy preservation and lower\nstorage requirements. Skeleton data are typically transmitted to cloud servers\nfor action recognition, with results returned to clients via Apps/APIs.\nHowever, the vulnerability of skeletal models against adversarial perturbations\ngradually reveals the unreliability of these systems. Existing black-box\nattacks all operate in a decision-based manner, resulting in numerous queries\nthat hinder efficiency and feasibility in real-world applications. Moreover,\nall attacks off the shelf focus on only restricted perturbations, while\nignoring model weaknesses when encountered with non-semantic perturbations. In\nthis paper, we propose two query-effIcient Skeletal Adversarial AttaCks,\nISAAC-K and ISAAC-N. As a black-box attack, ISAAC-K utilizes Grad-CAM in a\nsurrogate model to extract key joints where minor sparse perturbations are then\nadded to fool the classifier. To guarantee natural adversarial motions, we\nintroduce constraints of both bone length and temporal consistency. ISAAC-K\nfinds stronger adversarial examples on $\\ell_\\infty$ norm, which can encompass\nthose on other norms. Exhaustive experiments substantiate that ISAAC-K can\nuplift the attack efficiency of the perturbations under 10 skeletal models.\nAdditionally, as a byproduct, ISAAC-N fools the classifier by replacing\nskeletons unrelated to the action. We surprisingly find that skeletal models\nare vulnerable to large perturbations where the part-wise non-semantic joints\nare just replaced, leading to a query-free no-box attack without any prior\nknowledge. Based on that, four adaptive defenses are eventually proposed to\nimprove the robustness of skeleton recognition models."
    },
    {
        "date": "2025-01",
        "title": "TORCHLIGHT: Shedding LIGHT on Real-World Attacks on Cloudless IoT Devices Concealed within the Tor Network",
        "author": "Yumingzhi Pan, Zhen Ling, Yue Zhang, Hongze Wang, Guangchi Liu, Junzhou Luo, and Xinwen Fu",
        "link": "http://arxiv.org/abs/2501.16784v1",
        "abstract": "The rapidly expanding Internet of Things (IoT) landscape is shifting toward\ncloudless architectures, removing reliance on centralized cloud services but\nexposing devices directly to the internet and increasing their vulnerability to\ncyberattacks. Our research revealed an unexpected pattern of substantial Tor\nnetwork traffic targeting cloudless IoT devices. suggesting that attackers are\nusing Tor to anonymously exploit undisclosed vulnerabilities (possibly obtained\nfrom underground markets). To delve deeper into this phenomenon, we developed\nTORCHLIGHT, a tool designed to detect both known and unknown threats targeting\ncloudless IoT devices by analyzing Tor traffic. TORCHLIGHT filters traffic via\nspecific IP patterns, strategically deploys virtual private server (VPS) nodes\nfor cost-effective detection, and uses a chain-of-thought (CoT) process with\nlarge language models (LLMs) for accurate threat identification.\n  Our results are significant: for the first time, we have demonstrated that\nattackers are indeed using Tor to conceal their identities while targeting\ncloudless IoT devices. Over a period of 12 months, TORCHLIGHT analyzed 26 TB of\ntraffic, revealing 45 vulnerabilities, including 29 zero-day exploits with 25\nCVE-IDs assigned (5 CRITICAL, 3 HIGH, 16 MEDIUM, and 1 LOW) and an estimated\nvalue of approximately $312,000. These vulnerabilities affect around 12.71\nmillion devices across 148 countries, exposing them to severe risks such as\ninformation disclosure, authentication bypass, and arbitrary command execution.\nThe findings have attracted significant attention, sparking widespread\ndiscussion in cybersecurity circles, reaching the top 25 on Hacker News, and\ngenerating over 190,000 views."
    },
    {
        "date": "2025-01",
        "title": "Information security control as a task of control a dynamic system",
        "author": "Sergey Masaev, Andrey Minkin, Yuri Bezborodov, Dmitry Edimichev, and Yass Salal",
        "link": "http://arxiv.org/abs/2501.16732v1",
        "abstract": "The area of research includes control theory, dynamic systems, parameters of\nthe external environment, mode, integral indicators, British standards. The\nmain idea of the article is information security. The activity of a large-scale\nobject (enterprise) is considered. The activity of the enterprise is presented\nas a multidimensional dynamic system and is displayed as a digital copy of 1.2\nmillion parameters. A British digital copy-based information security standard\nis being introduced. Information security equipment and software were\npurchased. The training of the company's personnel was completed. Evaluation of\nimplementation (activities) is done as an integral indicator. The dynamics of\nthe integral indicator assesses the implementation of the British standard."
    },
    {
        "date": "2025-01",
        "title": "DFCon: Attention-Driven Supervised Contrastive Learning for Robust Deepfake Detection",
        "author": "MD Sadik Hossain Shanto, Mahir Labib Dihan, Souvik Ghosh, Riad Ahmed Anonto, Hafijul Hoque Chowdhury, Abir Muhtasim, Rakib Ahsan, MD Tanvir Hassan, MD Roqunuzzaman Sojib, Sheikh Azizul Hakim, and M. Saifur Rahman",
        "link": "http://arxiv.org/abs/2501.16704v1",
        "abstract": "This report presents our approach for the IEEE SP Cup 2025: Deepfake Face\nDetection in the Wild (DFWild-Cup), focusing on detecting deepfakes across\ndiverse datasets. Our methodology employs advanced backbone models, including\nMaxViT, CoAtNet, and EVA-02, fine-tuned using supervised contrastive loss to\nenhance feature separation. These models were specifically chosen for their\ncomplementary strengths. Integration of convolution layers and strided\nattention in MaxViT is well-suited for detecting local features. In contrast,\nhybrid use of convolution and attention mechanisms in CoAtNet effectively\ncaptures multi-scale features. Robust pretraining with masked image modeling of\nEVA-02 excels at capturing global features. After training, we freeze the\nparameters of these models and train the classification heads. Finally, a\nmajority voting ensemble is employed to combine the predictions from these\nmodels, improving robustness and generalization to unseen scenarios. The\nproposed system addresses the challenges of detecting deepfakes in real-world\nconditions and achieves a commendable accuracy of 95.83% on the validation\ndataset."
    },
    {
        "date": "2025-01",
        "title": "Data-Free Model-Related Attacks: Unleashing the Potential of Generative AI",
        "author": "Dayong Ye, Tianqing Zhu, Shang Wang, Bo Liu, Leo Yu Zhang, Wanlei Zhou, and Yang Zhang",
        "link": "http://arxiv.org/abs/2501.16671v1",
        "abstract": "Generative AI technology has become increasingly integrated into our daily\nlives, offering powerful capabilities to enhance productivity. However, these\nsame capabilities can be exploited by adversaries for malicious purposes. While\nexisting research on adversarial applications of generative AI predominantly\nfocuses on cyberattacks, less attention has been given to attacks targeting\ndeep learning models. In this paper, we introduce the use of generative AI for\nfacilitating model-related attacks, including model extraction, membership\ninference, and model inversion. Our study reveals that adversaries can launch a\nvariety of model-related attacks against both image and text models in a\ndata-free and black-box manner, achieving comparable performance to baseline\nmethods that have access to the target models' training data and parameters in\na white-box manner. This research serves as an important early warning to the\ncommunity about the potential risks associated with generative AI-powered\nattacks on deep learning models."
    },
    {
        "date": "2025-01",
        "title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning",
        "author": "Dayong Ye, Tainqing Zhu, Jiayang Li, Kun Gao, Bo Liu, Leo Yu Zhang, Wanlei Zhou, and Yang Zhang",
        "link": "http://arxiv.org/abs/2501.16663v1",
        "abstract": "Duplication is a prevalent issue within datasets. Existing research has\ndemonstrated that the presence of duplicated data in training datasets can\nsignificantly influence both model performance and data privacy. However, the\nimpact of data duplication on the unlearning process remains largely\nunexplored. This paper addresses this gap by pioneering a comprehensive\ninvestigation into the role of data duplication, not only in standard machine\nunlearning but also in federated and reinforcement unlearning paradigms.\nSpecifically, we propose an adversary who duplicates a subset of the target\nmodel's training set and incorporates it into the training set. After training,\nthe adversary requests the model owner to unlearn this duplicated subset, and\nanalyzes the impact on the unlearned model. For example, the adversary can\nchallenge the model owner by revealing that, despite efforts to unlearn it, the\ninfluence of the duplicated subset remains in the model. Moreover, to\ncircumvent detection by de-duplication techniques, we propose three novel\nnear-duplication methods for the adversary, each tailored to a specific\nunlearning paradigm. We then examine their impacts on the unlearning process\nwhen de-duplication techniques are applied. Our findings reveal several crucial\ninsights: 1) the gold standard unlearning method, retraining from scratch,\nfails to effectively conduct unlearning under certain conditions; 2) unlearning\nduplicated data can lead to significant model degradation in specific\nscenarios; and 3) meticulously crafted duplicates can evade detection by\nde-duplication methods."
    },
    {
        "date": "2025-01",
        "title": "Analysis of Zero Day Attack Detection Using MLP and XAI",
        "author": "Ashim Dahal, Prabin Bajgai, and Nick Rahimi",
        "link": "http://arxiv.org/abs/2501.16638v1",
        "abstract": "Any exploit taking advantage of zero-day is called a zero-day attack.\nPrevious research and social media trends show a massive demand for research in\nzero-day attack detection. This paper analyzes Machine Learning (ML) and Deep\nLearning (DL) based approaches to create Intrusion Detection Systems (IDS) and\nscrutinizing them using Explainable AI (XAI) by training an explainer based on\nrandomly sampled data from the testing set. The focus is on using the KDD99\ndataset, which has the most research done among all the datasets for detecting\nzero-day attacks. The paper aims to synthesize the dataset to have fewer\nclasses for multi-class classification, test ML and DL approaches on pattern\nrecognition, establish the robustness and dependability of the model, and\nestablish the interpretability and scalability of the model. We evaluated the\nperformance of four multilayer perceptron (MLP) trained on the KDD99 dataset,\nincluding baseline ML models, weighted ML models, truncated ML models, and\nweighted truncated ML models. Our results demonstrate that the truncated ML\nmodel achieves the highest accuracy (99.62%), precision, and recall, while\nweighted truncated ML model shows lower accuracy (97.26%) but better class\nrepresentation (less bias) among all the classes with improved unweighted\nrecall score. We also used Shapely Additive exPlanations (SHAP) to train\nexplainer for our truncated models to check for feature importance among the\ntwo weighted and unweighted models."
    },
    {
        "date": "2025-01",
        "title": "SHIELD: Secure Host-Independent Extensible Logging for SATA/Network Storage Towards Ransomware Detection",
        "author": "Md Raz, P. V. Sai Charan, Prashanth Krishnamurthy, Farshad Khorrami, and Ramesh Karri",
        "link": "http://arxiv.org/abs/2501.16619v1",
        "abstract": "As malware such as ransomware becomes sophisticated, the ability to find and\nneutralize it requires more robust and tamper-resistant solutions. Current\nmethods rely on data from compromised hosts, lack hardware isolation, and\ncannot detect emerging threats. To address these limitations, we introduce\nSHIELD - a detection architecture leveraging FPGA-based open-source SATA and\nNetwork Block Device (NBD) technology to provide off-host, tamper-proof\nmeasurements for continuous observation of disk activity for software executing\non a target device. SHIELD provides three distinct contributions: It (1)\ndevelops a framework to obtain and analyze multi-level hardware metrics at NBD,\nFPGA, and SATA storage levels, and shows their ability to differentiate between\nharmless and malicious software; (2) Broadens the functionality of an\nopen-source FPGA-driven SATA Host Bus Adapter (HBA) to offer complete data\nstorage capabilities through NBD without relying on the host system; (3)\nProvides a foundation for using the methodology and metrics in automated\nmachine learning-assisted detection and ASIC integration for advanced\nmitigation capabilities in data storage devices. SHIELD analyzes 10 benign\nprograms and 10 modern ransomware families to illustrate its capacity for\nreal-time monitoring and use in distinguishing between ransomware and benign\nsoftware. Experimental evidence shows SHIELD's robust host-independent and\nhardware-assisted metrics are a basis for detection, allowing to observe\nprogram execution and detect malicious activities at the storage level."
    },
    {
        "date": "2025-01",
        "title": "UniPET-SPK: A Unified Framework for Parameter-Efficient Tuning of Pre-trained Speech Models for Robust Speaker Verification",
        "author": "Mufan Sang, and John H. L. Hansen",
        "link": "http://arxiv.org/abs/2501.16542v1",
        "abstract": "With excellent generalization ability, SSL speech models have shown\nimpressive performance on various downstream tasks in the pre-training and\nfine-tuning paradigm. However, as the size of pre-trained models grows,\nfine-tuning becomes practically unfeasible due to expanding computation and\nstorage requirements and the risk of overfitting. This study explores\nparameter-efficient tuning (PET) methods for adapting large-scale pre-trained\nSSL speech models to speaker verification task. Correspondingly, we propose\nthree PET methods: (i)an adapter-tuning method, (ii)a prompt-tuning method, and\n(iii)a unified framework that effectively incorporates adapter-tuning and\nprompt-tuning with a dynamically learnable gating mechanism. First, we propose\nthe Inner+Inter Adapter framework, which inserts two types of adapters into\npre-trained models, allowing for adaptation of latent features within the\nintermediate Transformer layers and output embeddings from all Transformer\nlayers, through a parallel adapter design. Second, we propose the Deep Speaker\nPrompting method that concatenates trainable prompt tokens into the input space\nof pre-trained models to guide adaptation. Lastly, we propose the UniPET-SPK, a\nunified framework that effectively incorporates these two alternate PET methods\ninto a single framework with a dynamic trainable gating mechanism. The proposed\nUniPET-SPK learns to find the optimal mixture of PET methods to match different\ndatasets and scenarios. We conduct a comprehensive set of experiments on\nseveral datasets to validate the effectiveness of the proposed PET methods.\nExperimental results on VoxCeleb, CN-Celeb, and 1st 48-UTD forensic datasets\ndemonstrate that the proposed UniPET-SPK consistently outperforms the two PET\nmethods, fine-tuning, and other parameter-efficient tuning methods, achieving\nsuperior performance while updating only 5.4% of the parameters."
    },
    {
        "date": "2025-01",
        "title": "Smoothed Embeddings for Robust Language Models",
        "author": "Ryo Hase, Md Rafi Ur Rashid, Ashley Lewis, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, and Ye Wang",
        "link": "http://arxiv.org/abs/2501.16497v1",
        "abstract": "Improving the safety and reliability of large language models (LLMs) is a\ncrucial aspect of realizing trustworthy AI systems. Although alignment methods\naim to suppress harmful content generation, LLMs are often still vulnerable to\njailbreaking attacks that employ adversarial inputs that subvert alignment and\ninduce harmful outputs. We propose the Randomized Embedding Smoothing and Token\nAggregation (RESTA) defense, which adds random noise to the embedding vectors\nand performs aggregation during the generation of each output token, with the\naim of better preserving semantic information. Our experiments demonstrate that\nour approach achieves superior robustness versus utility tradeoffs compared to\nthe baseline defenses."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Stability Prediction in Smart Grids: GAN-based Approach under Data Constraints and Adversarial Challenges",
        "author": "Emad Efatinasab, Alessandro Brighente, Denis Donadel, Mauro Conti, and Mirco Rampazzo",
        "link": "http://arxiv.org/abs/2501.16490v1",
        "abstract": "Smart grids are critical for addressing the growing energy demand due to\nglobal population growth and urbanization. They enhance efficiency,\nreliability, and sustainability by integrating renewable energy. Ensuring their\navailability and safety requires advanced operational control and safety\nmeasures. Researchers employ AI and machine learning to assess grid stability,\nbut challenges like the lack of datasets and cybersecurity threats, including\nadversarial attacks, persist. In particular, data scarcity is a key issue:\nobtaining grid instability instances is tough due to the need for significant\nexpertise, resources, and time. However, they are essential to test novel\nresearch advancements and security mitigations. In this paper, we introduce a\nnovel framework to detect instability in smart grids by employing only stable\ndata. It relies on a Generative Adversarial Network (GAN) where the generator\nis trained to create instability data that are used along with stable data to\ntrain the discriminator. Moreover, we include a new adversarial training layer\nto improve robustness against adversarial attacks. Our solution, tested on a\ndataset composed of real-world stable and unstable samples, achieve accuracy up\nto 97.5\\% in predicting grid stability and up to 98.9\\% in detecting\nadversarial attacks. Moreover, we implemented our model in a single-board\ncomputer demonstrating efficient real-time decision-making with an average\nresponse time of less than 7ms. Our solution improves prediction accuracy and\nresilience while addressing data scarcity in smart grid management."
    },
    {
        "date": "2025-01",
        "title": "On the Feasibility of Using LLMs to Execute Multistage Network Attacks",
        "author": "Brian Singer, Keane Lucas, Lakshmi Adiga, Meghna Jain, Lujo Bauer, and Vyas Sekar",
        "link": "http://arxiv.org/abs/2501.16466v1",
        "abstract": "LLMs have shown preliminary promise in some security tasks and CTF\nchallenges. However, it is unclear whether LLMs are able to realize multistage\nnetwork attacks, which involve executing a wide variety of actions across\nmultiple hosts such as conducting reconnaissance, exploiting vulnerabilities to\ngain initial access, leveraging internal hosts to move laterally, and using\nmultiple compromised hosts to exfiltrate data. We evaluate LLMs across 10\nmultistage networks and find that popular LLMs are unable to realize these\nattacks. To enable LLMs to realize these attacks, we introduce Incalmo, an\nLLM-agnostic high-level attack abstraction layer that sits between an LLM and\nthe environment. Rather than LLMs issuing low-level command-line instructions,\nwhich can lead to incorrect implementations, Incalmo allows LLMs to specify\nhigh-level tasks (e.g., infect a host, scan a network), which are then carried\nout by Incalmo. Incalmo realizes these tasks by translating them into low-level\nprimitives (e.g., commands to exploit tools). Incalmo also provides an\nenvironment state service and an attack graph service to provide structure to\nLLMs in selecting actions relevant to a multistage attack. Across 9 out of 10\nrealistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can\nsuccessfully autonomously execute multistage attacks. We also conduct an\nablation analysis to show the key role the high-level abstractions play. For\ninstance, we find that both Incalmo's high-level tasks and services are\ncrucial. Furthermore, even smaller-parameter LLMs with Incalmo can fully\nsucceed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do\nnot fully succeed in any."
    },
    {
        "date": "2025-01",
        "title": "Detecting Zero-Day Attacks in Digital Substations via In-Context Learning",
        "author": "Faizan Manzoor, Vanshaj Khattar, Akila Herath, Clifton Black, Matthew C Nielsen, Junho Hong, Chen-Ching Liu, and Ming Jin",
        "link": "http://arxiv.org/abs/2501.16453v1",
        "abstract": "The occurrences of cyber attacks on the power grids have been increasing\nevery year, with novel attack techniques emerging every year. In this paper, we\naddress the critical challenge of detecting novel/zero-day attacks in digital\nsubstations that employ the IEC-61850 communication protocol. While many\nheuristic and machine learning (ML)-based methods have been proposed for attack\ndetection in IEC-61850 digital substations, generalization to novel or zero-day\nattacks remains challenging. We propose an approach that leverages the\nin-context learning (ICL) capability of the transformer architecture, the\nfundamental building block of large language models. The ICL approach enables\nthe model to detect zero-day attacks and learn from a few examples of that\nattack without explicit retraining. Our experiments on the IEC-61850 dataset\ndemonstrate that the proposed method achieves more than $85\\%$ detection\naccuracy on zero-day attacks while the existing state-of-the-art baselines\nfail. This work paves the way for building more secure and resilient digital\nsubstations of the future."
    },
    {
        "date": "2025-01",
        "title": "Distilling foundation models for robust and efficient models in digital pathology",
        "author": "Alexandre Filiot, Nicolas Dop, Oussama Tchita, Auriane Riou, R\u00e9my Dubois, Thomas Peeters, Daria Valter, Marin Scalbert, Charlie Saillard, Genevi\u00e8ve Robin, and Antoine Olivier",
        "link": "http://arxiv.org/abs/2501.16239v2",
        "abstract": "In recent years, the advent of foundation models (FM) for digital pathology\nhas relied heavily on scaling the pre-training datasets and the model size,\nyielding large and powerful models. While it resulted in improving the\nperformance on diverse downstream tasks, it also introduced increased\ncomputational cost and inference time. In this work, we explore the\ndistillation of a large foundation model into a smaller one, reducing the\nnumber of parameters by several orders of magnitude. Leveraging distillation\ntechniques, our distilled model, H0-mini, achieves nearly comparable\nperformance to large FMs at a significantly reduced inference cost. It is\nevaluated on several public benchmarks, achieving 3rd place on the HEST\nbenchmark and 5th place on the EVA benchmark. Additionally, a robustness\nanalysis conducted on the PLISM dataset demonstrates that our distilled model\nreaches excellent robustness to variations in staining and scanning conditions,\nsignificantly outperforming other state-of-the art models. This opens new\nperspectives to design lightweight and robust models for digital pathology,\nwithout compromising on performance."
    },
    {
        "date": "2025-01",
        "title": "Generating Spatial Synthetic Populations Using Wasserstein Generative Adversarial Network: A Case Study with EU-SILC Data for Helsinki and Thessaloniki",
        "author": "Vanja Falck",
        "link": "http://arxiv.org/abs/2501.16080v1",
        "abstract": "Using agent-based social simulations can enhance our understanding of urban\nplanning, public health, and economic forecasting. Realistic synthetic\npopulations with numerous attributes strengthen these simulations. The\nWasserstein Generative Adversarial Network, trained on census data like\nEU-SILC, can create robust synthetic populations. These methods, aided by\nexternal statistics or EU-SILC weights, generate spatial synthetic populations\nfor agent-based models. The increased access to high-quality micro-data has\nsparked interest in synthetic populations, which preserve demographic profiles\nand analytical strength while ensuring privacy and preventing discrimination.\nThis study uses national data from Finland and Greece for Helsinki and\nThessaloniki to explore balanced spatial synthetic population generation.\nResults show challenges related to balancing data with or without aggregated\nstatistics for the target population and the general under-representation of\nfringe profiles by deep generative methods. The latter can lead to\ndiscrimination in agent-based simulations."
    },
    {
        "date": "2025-01",
        "title": "Provisioning Time-Based Subscription in NDN: A Secure and Efficient Access Control Scheme",
        "author": "Nazatul H. Sultan, Chandan Kumar, Saurab Dulal, Vijay Varadharajan, Seyit Camtepe, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15975v1",
        "abstract": "This paper proposes a novel encryption-based access control mechanism for\nNamed Data Networking (NDN). The scheme allows data producers to share their\ncontent in encrypted form before transmitting it to consumers. The encryption\nmechanism incorporates time-based subscription access policies directly into\nthe encrypted content, enabling only consumers with valid subscriptions to\ndecrypt it. This makes the scheme well-suited for real-world,\nsubscription-based applications like Netflix. Additionally, the scheme\nintroduces an anonymous and unlinkable signature-based authentication mechanism\nthat empowers edge routers to block bogus content requests at the network's\nentry point, thereby mitigating Denial of Service (DoS) attacks. A formal\nsecurity proof demonstrates the scheme's resistance to Chosen Plaintext Attacks\n(CPA). Performance analysis, using Mini-NDN-based emulation and a Charm library\nimplementation, further confirms the practicality of the scheme. Moreover, it\noutperforms closely related works in terms of functionality, security, and\ncommunication overhead."
    },
    {
        "date": "2025-01",
        "title": "LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models",
        "author": "Yuewen Mei, Tong Nie, Jian Sun, and Ye Tian",
        "link": "http://arxiv.org/abs/2501.15850v1",
        "abstract": "Ensuring and improving the safety of autonomous driving systems (ADS) is\ncrucial for the deployment of highly automated vehicles, especially in\nsafety-critical events. To address the rarity issue, adversarial scenario\ngeneration methods are developed, in which behaviors of traffic participants\nare manipulated to induce safety-critical events. However, existing methods\nstill face two limitations. First, identification of the adversarial\nparticipant directly impacts the effectiveness of the generation. However, the\ncomplexity of real-world scenarios, with numerous participants and diverse\nbehaviors, makes identification challenging. Second, the potential of generated\nsafety-critical scenarios to continuously improve ADS performance remains\nunderexplored. To address these issues, we propose LLM-attacker: a closed-loop\nadversarial scenario generation framework leveraging large language models\n(LLMs). Specifically, multiple LLM agents are designed and coordinated to\nidentify optimal attackers. Then, the trajectories of the attackers are\noptimized to generate adversarial scenarios. These scenarios are iteratively\nrefined based on the performance of ADS, forming a feedback loop to improve\nADS. Experimental results show that LLM-attacker can create more dangerous\nscenarios than other methods, and the ADS trained with it achieves a collision\nrate half that of training with normal scenarios. This indicates the ability of\nLLM-attacker to test and enhance the safety and robustness of ADS. Video\ndemonstrations are provided at:\nhttps://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view."
    },
    {
        "date": "2025-01",
        "title": "Beyond In-Distribution Performance: A Cross-Dataset Study of Trajectory Prediction Robustness",
        "author": "Yue Yao, Daniel Goehring, and Joerg Reichardt",
        "link": "http://arxiv.org/abs/2501.15842v1",
        "abstract": "We study the Out-of-Distribution (OoD) generalization ability of three SotA\ntrajectory prediction models with comparable In-Distribution (ID) performance\nbut different model designs. We investigate the influence of inductive bias,\nsize of training data and data augmentation strategy by training the models on\nArgoverse 2 (A2) and testing on Waymo Open Motion (WO) and vice versa. We find\nthat the smallest model with highest inductive bias exhibits the best OoD\ngeneralization across different augmentation strategies when trained on the\nsmaller A2 dataset and tested on the large WO dataset. In the converse setting,\ntraining all models on the larger WO dataset and testing on the smaller A2\ndataset, we find that all models generalize poorly, even though the model with\nthe highest inductive bias still exhibits the best generalization ability. We\ndiscuss possible reasons for this surprising finding and draw conclusions about\nthe design and test of trajectory prediction models and benchmarks."
    },
    {
        "date": "2025-01",
        "title": "FuzzyLight: A Robust Two-Stage Fuzzy Approach for Traffic Signal Control Works in Real Cities",
        "author": "Mingyuan Li, Jiahao Wang, Bo Du, Jun Shen, and Qiang Wu",
        "link": "http://arxiv.org/abs/2501.15820v1",
        "abstract": "Effective traffic signal control (TSC) is crucial in mitigating urban\ncongestion and reducing emissions. Recently, reinforcement learning (RL) has\nbeen the research trend for TSC. However, existing RL algorithms face several\nreal-world challenges that hinder their practical deployment in TSC: (1) Sensor\naccuracy deteriorates with increased sensor detection range, and data\ntransmission is prone to noise, potentially resulting in unsafe TSC decisions.\n(2) During the training of online RL, interactions with the environment could\nbe unstable, potentially leading to inappropriate traffic signal phase (TSP)\nselection and traffic congestion. (3) Most current TSC algorithms focus only on\nTSP decisions, overlooking the critical aspect of phase duration, affecting\nsafety and efficiency. To overcome these challenges, we propose a robust\ntwo-stage fuzzy approach called FuzzyLight, which integrates compressed sensing\nand RL for TSC deployment. FuzzyLight offers several key contributions: (1) It\nemploys fuzzy logic and compressed sensing to address sensor noise and enhances\nthe efficiency of TSP decisions. (2) It maintains stable performance during\ntraining and combines fuzzy logic with RL to generate precise phases. (3) It\nworks in real cities across 22 intersections and demonstrates superior\nperformance in both real-world and simulated environments. Experimental results\nindicate that FuzzyLight enhances traffic efficiency by 48% compared to\nexpert-designed timings in the real world. Furthermore, it achieves\nstate-of-the-art (SOTA) performance in simulated environments using six\nreal-world datasets with transmission noise. The code and deployment video are\navailable at the URL1"
    },
    {
        "date": "2025-01",
        "title": "CENSOR: Defense Against Gradient Inversion via Orthogonal Subspace Bayesian Sampling",
        "author": "Kaiyuan Zhang, Siyuan Cheng, Guangyu Shen, Bruno Ribeiro, Shengwei An, Pin-Yu Chen, Xiangyu Zhang, and Ninghui Li",
        "link": "http://arxiv.org/abs/2501.15718v1",
        "abstract": "Federated learning collaboratively trains a neural network on a global\nserver, where each local client receives the current global model weights and\nsends back parameter updates (gradients) based on its local private data. The\nprocess of sending these model updates may leak client's private data\ninformation. Existing gradient inversion attacks can exploit this vulnerability\nto recover private training instances from a client's gradient vectors.\nRecently, researchers have proposed advanced gradient inversion techniques that\nexisting defenses struggle to handle effectively. In this work, we present a\nnovel defense tailored for large neural network models. Our defense capitalizes\non the high dimensionality of the model parameters to perturb gradients within\na subspace orthogonal to the original gradient. By leveraging cold posteriors\nover orthogonal subspaces, our defense implements a refined gradient update\nmechanism. This enables the selection of an optimal gradient that not only\nsafeguards against gradient inversion attacks but also maintains model utility.\nWe conduct comprehensive experiments across three different datasets and\nevaluate our defense against various state-of-the-art attacks and defenses.\nCode is available at https://censor-gradient.github.io."
    },
    {
        "date": "2025-01",
        "title": "People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text",
        "author": "Jenna Russell, Marzena Karpinska, and Mohit Iyyer",
        "link": "http://arxiv.org/abs/2501.15654v1",
        "abstract": "In this paper, we study how well humans can detect text generated by\ncommercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300\nnon-fiction English articles, label them as either human-written or\nAI-generated, and provide paragraph-length explanations for their decisions.\nOur experiments show that annotators who frequently use LLMs for writing tasks\nexcel at detecting AI-generated text, even without any specialized training or\nfeedback. In fact, the majority vote among five such \"expert\" annotators\nmisclassifies only 1 of 300 articles, significantly outperforming most\ncommercial and open-source detectors we evaluated even in the presence of\nevasion tactics like paraphrasing and humanization. Qualitative analysis of the\nexperts' free-form explanations shows that while they rely heavily on specific\nlexical clues ('AI vocabulary'), they also pick up on more complex phenomena\nwithin the text (e.g., formality, originality, clarity) that are challenging to\nassess for automatic detectors. We release our annotated dataset and code to\nspur future research into both human and automated detection of AI-generated\ntext."
    },
    {
        "date": "2025-01",
        "title": "A Privacy Enhancing Technique to Evade Detection by Street Video Cameras Without Using Adversarial Accessories",
        "author": "Jacob Shams, Ben Nassi, Satoru Koda, Asaf Shabtai, and Yuval Elovici",
        "link": "http://arxiv.org/abs/2501.15653v1",
        "abstract": "In this paper, we propose a privacy-enhancing technique leveraging an\ninherent property of automatic pedestrian detection algorithms, namely, that\nthe training of deep neural network (DNN) based methods is generally performed\nusing curated datasets and laboratory settings, while the operational areas of\nthese methods are dynamic real-world environments. In particular, we leverage a\nnovel side effect of this gap between the laboratory and the real world:\nlocation-based weakness in pedestrian detection. We demonstrate that the\nposition (distance, angle, height) of a person, and ambient light level,\ndirectly impact the confidence of a pedestrian detector when detecting the\nperson. We then demonstrate that this phenomenon is present in pedestrian\ndetectors observing a stationary scene of pedestrian traffic, with blind spot\nareas of weak detection of pedestrians with low confidence. We show how\nprivacy-concerned pedestrians can leverage these blind spots to evade detection\nby constructing a minimum confidence path between two points in a scene,\nreducing the maximum confidence and average confidence of the path by up to\n0.09 and 0.13, respectively, over direct and random paths through the scene. To\ncounter this phenomenon, and force the use of more costly and sophisticated\nmethods to leverage this vulnerability, we propose a novel countermeasure to\nimprove the confidence of pedestrian detectors in blind spots, raising the\nmax/average confidence of paths generated by our technique by 0.09 and 0.05,\nrespectively. In addition, we demonstrate that our countermeasure improves a\nFaster R-CNN-based pedestrian detector's TPR and average true positive\nconfidence by 0.03 and 0.15, respectively."
    },
    {
        "date": "2025-01",
        "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
        "author": "Mahshid shiri, Chandra Bortolotto, Alessandro Bruno, Alessio Consonni, Daniela Maria Grasso, Leonardo Brizzi, Daniele Loiacono, and Lorenzo Preda",
        "link": "http://arxiv.org/abs/2501.15572v1",
        "abstract": "Introduction: Generative Adversarial Networks (GANs) are increasingly used to\ngenerate synthetic medical images, addressing the critical shortage of\nannotated data for training Artificial Intelligence (AI) systems. This study\nintroduces a novel memory-efficient GAN architecture, incorporating Conditional\nRandom Fields (CRFs) to generate high-resolution 3D medical images and\nevaluates its performance against the state-of-the-art hierarchical (HA)-GAN\nmodel.\n  Materials and Methods: The CRF-GAN was trained using the open-source lung CT\nLUNA16 dataset. The architecture was compared to HA-GAN through a quantitative\nevaluation, using Frechet Inception Distance (FID) and Maximum Mean Discrepancy\n(MMD) metrics, and a qualitative evaluation, through a two-alternative forced\nchoice (2AFC) test completed by a pool of 12 resident radiologists, in order to\nassess the realism of the generated images.\n  Results: CRF-GAN outperformed HA-GAN with lower FID (0.047 vs. 0.061) and MMD\n(0.084 vs. 0.086) scores, indicating better image fidelity. The 2AFC test\nshowed a significant preference for images generated by CRF-Gan over those\ngenerated by HA-GAN with a p-value of 1.93e-05. Additionally, CRF-GAN\ndemonstrated 9.34% lower memory usage at 256 resolution and achieved up to\n14.6% faster training speeds, offering substantial computational savings.\n  Discussion: CRF-GAN model successfully generates high-resolution 3D medical\nimages with non-inferior quality to conventional models, while being more\nmemory-efficient and faster. Computational power and time saved can be used to\nimprove the spatial resolution and anatomical accuracy of generated images,\nwhich is still a critical factor limiting their direct clinical applicability."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Graph Out-of-Distribution Recommendation via Diffusion Model",
        "author": "Chu Zhao, Enneng Yang, Yuliang Liang, Jianzhe Zhao, Guibing Guo, and Xingwei Wang",
        "link": "http://arxiv.org/abs/2501.15555v1",
        "abstract": "The distributionally robust optimization (DRO)-based graph neural network\nmethods improve recommendation systems' out-of-distribution (OOD)\ngeneralization by optimizing the model's worst-case performance. However, these\nstudies fail to consider the impact of noisy samples in the training data,\nwhich results in diminished generalization capabilities and lower accuracy.\nThrough experimental and theoretical analysis, this paper reveals that current\nDRO-based graph recommendation methods assign greater weight to noise\ndistribution, leading to model parameter learning being dominated by it. When\nthe model overly focuses on fitting noise samples in the training data, it may\nlearn irrelevant or meaningless features that cannot be generalized to OOD\ndata. To address this challenge, we design a Distributionally Robust Graph\nmodel for OOD recommendation (DRGO). Specifically, our method first employs a\nsimple and effective diffusion paradigm to alleviate the noisy effect in the\nlatent space. Additionally, an entropy regularization term is introduced in the\nDRO objective function to avoid extreme sample weights in the worst-case\ndistribution. Finally, we provide a theoretical proof of the generalization\nerror bound of DRGO as well as a theoretical analysis of how our approach\nmitigates noisy sample effects, which helps to better understand the proposed\nframework from a theoretical perspective. We conduct extensive experiments on\nfour datasets to evaluate the effectiveness of our framework against three\ntypical distribution shifts, and the results demonstrate its superiority in\nboth independently and identically distributed distributions (IID) and OOD."
    },
    {
        "date": "2025-01",
        "title": "UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning",
        "author": "Oubo Ma, Linkang Du, Yang Dai, Chunyi Zhou, Qingming Li, Yuwen Pu, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.15529v1",
        "abstract": "Deep reinforcement learning (DRL) is widely applied to safety-critical\ndecision-making scenarios. However, DRL is vulnerable to backdoor attacks,\nespecially action-level backdoors, which pose significant threats through\nprecise manipulation and flexible activation, risking outcomes like vehicle\ncollisions or drone crashes. The key distinction of action-level backdoors lies\nin the utilization of the backdoor reward function to associate triggers with\ntarget actions. Nevertheless, existing studies typically rely on backdoor\nreward functions with fixed values or conditional flipping, which lack\nuniversality across diverse DRL tasks and backdoor designs, resulting in\nfluctuations or even failure in practice.\n  This paper proposes the first universal action-level backdoor attack\nframework, called UNIDOOR, which enables adaptive exploration of backdoor\nreward functions through performance monitoring, eliminating the reliance on\nexpert knowledge and grid search. We highlight that action tampering serves as\na crucial component of action-level backdoor attacks in continuous action\nscenarios, as it addresses attack failures caused by low-frequency target\nactions. Extensive evaluations demonstrate that UNIDOOR significantly enhances\nthe attack performance of action-level backdoors, showcasing its universality\nacross diverse attack scenarios, including single/multiple agents,\nsingle/multiple backdoors, discrete/continuous action spaces, and sparse/dense\nreward signals. Furthermore, visualization results encompassing state\ndistribution, neuron activation, and animations demonstrate the stealthiness of\nUNIDOOR. The source code of UNIDOOR can be found at\nhttps://github.com/maoubo/UNIDOOR."
    },
    {
        "date": "2025-01",
        "title": "Mitigating Spurious Negative Pairs for Robust Industrial Anomaly Detection",
        "author": "Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15434v1",
        "abstract": "Despite significant progress in Anomaly Detection (AD), the robustness of\nexisting detection methods against adversarial attacks remains a challenge,\ncompromising their reliability in critical real-world applications such as\nautonomous driving. This issue primarily arises from the AD setup, which\nassumes that training data is limited to a group of unlabeled normal samples,\nmaking the detectors vulnerable to adversarial anomaly samples during testing.\nAdditionally, implementing adversarial training as a safeguard encounters\ndifficulties, such as formulating an effective objective function without\naccess to labels. An ideal objective function for adversarial training in AD\nshould promote strong perturbations both within and between the normal and\nanomaly groups to maximize margin between normal and anomaly distribution. To\naddress these issues, we first propose crafting a pseudo-anomaly group derived\nfrom normal group samples. Then, we demonstrate that adversarial training with\ncontrastive loss could serve as an ideal objective function, as it creates both\ninter- and intra-group perturbations. However, we notice that spurious negative\npairs compromise the conventional contrastive loss to achieve robust AD.\nSpurious negative pairs are those that should be closely mapped but are\nerroneously separated. These pairs introduce noise and misguide the direction\nof inter-group adversarial perturbations. To overcome the effect of spurious\nnegative pairs, we define opposite pairs and adversarially pull them apart to\nstrengthen inter-group perturbations. Experimental results demonstrate our\nsuperior performance in both clean and adversarial scenarios, with a 26.1%\nimprovement in robust detection across various challenging benchmark datasets.\nThe implementation of our work is available at:\nhttps://github.com/rohban-lab/COBRA."
    },
    {
        "date": "2025-01",
        "title": "Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults",
        "author": "Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, and Martin Guay",
        "link": "http://arxiv.org/abs/2501.15373v1",
        "abstract": "Merely pursuing performance may adversely affect the safety, while a\nconservative policy for safe exploration will degrade the performance. How to\nbalance the safety and performance in learning-based control problems is an\ninteresting yet challenging issue. This paper aims to enhance system\nperformance with safety guarantee in solving the reinforcement learning\n(RL)-based optimal control problems of nonlinear systems subject to\nhigh-relative-degree state constraints and unknown time-varying\ndisturbance/actuator faults. First, to combine control barrier functions (CBFs)\nwith RL, a new type of CBFs, termed high-order reciprocal control barrier\nfunction (HO-RCBF) is proposed to deal with high-relative-degree constraints\nduring the learning process. Then, the concept of gradient similarity is\nproposed to quantify the relationship between the gradient of safety and the\ngradient of performance. Finally, gradient manipulation and adaptive mechanisms\nare introduced in the safe RL framework to enhance the performance with a\nsafety guarantee. Two simulation examples illustrate that the proposed safe RL\nframework can address high-relative-degree constraint, enhance safety\nrobustness and improve system performance."
    },
    {
        "date": "2025-01",
        "title": "AI-Driven Secure Data Sharing: A Trustworthy and Privacy-Preserving Approach",
        "author": "Al Amin, Kamrul Hasan, Sharif Ullah, and Liang Hong",
        "link": "http://arxiv.org/abs/2501.15363v1",
        "abstract": "In the era of data-driven decision-making, ensuring the privacy and security\nof shared data is paramount across various domains. Applying existing deep\nneural networks (DNNs) to encrypted data is critical and often compromises\nperformance, security, and computational overhead. To address these\nlimitations, this research introduces a secure framework consisting of a\nlearnable encryption method based on the block-pixel operation to encrypt the\ndata and subsequently integrate it with the Vision Transformer (ViT). The\nproposed framework ensures data privacy and security by creating unique\nscrambling patterns per key, providing robust performance against adversarial\nattacks without compromising computational efficiency and data integrity. The\nframework was tested on sensitive medical datasets to validate its efficacy,\nproving its ability to handle highly confidential information securely. The\nsuggested framework was validated with a 94\\% success rate after extensive\ntesting on real-world datasets, such as MRI brain tumors and histological scans\nof lung and colon cancers. Additionally, the framework was tested under diverse\nadversarial attempts against secure data sharing with optimum performance and\ndemonstrated its effectiveness in various threat scenarios. These comprehensive\nanalyses underscore its robustness, making it a trustworthy solution for secure\ndata sharing in critical applications."
    },
    {
        "date": "2025-01",
        "title": "Killing it with Zero-Shot: Adversarially Robust Novelty Detection",
        "author": "Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, and Mohammad Hossein Rohban",
        "link": "http://arxiv.org/abs/2501.15271v1",
        "abstract": "Novelty Detection (ND) plays a crucial role in machine learning by\nidentifying new or unseen data during model inference. This capability is\nespecially important for the safe and reliable operation of automated systems.\nDespite advances in this field, existing techniques often fail to maintain\ntheir performance when subject to adversarial attacks. Our research addresses\nthis gap by marrying the merits of nearest-neighbor algorithms with robust\nfeatures obtained from models pretrained on ImageNet. We focus on enhancing the\nrobustness and performance of ND algorithms. Experimental results demonstrate\nthat our approach significantly outperforms current state-of-the-art methods\nacross various benchmarks, particularly under adversarial conditions. By\nincorporating robust pretrained features into the k-NN algorithm, we establish\na new standard for performance and robustness in the field of robust ND. This\nwork opens up new avenues for research aimed at fortifying machine learning\nsystems against adversarial vulnerabilities. Our implementation is publicly\navailable at https://github.com/rohban-lab/ZARND."
    },
    {
        "date": "2025-01",
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "author": "Yining Wang, Mi Zhang, Junjie Sun, Chenyue Wang, Min Yang, Hui Xue, Jialing Tao, Ranjie Duan, and Jiexi Liu",
        "link": "http://arxiv.org/abs/2501.15269v1",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large\nLanguage Models (MLLMs) are revolutionizing visual-language applications. Yet,\nthese models are often plagued by the hallucination problem, which involves\ngenerating inaccurate objects, attributes, and relationships that do not match\nthe visual content. In this work, we delve into the internal attention\nmechanisms of MLLMs to reveal the underlying causes of hallucination, exposing\nthe inherent vulnerabilities in the instruction-tuning process.\n  We propose a novel hallucination attack against MLLMs that exploits attention\nsink behaviors to trigger hallucinated content with minimal image-text\nrelevance, posing a significant threat to critical downstream applications.\nDistinguished from previous adversarial methods that rely on fixed patterns,\nour approach generates dynamic, effective, and highly transferable visual\nadversarial inputs, without sacrificing the quality of model responses.\nComprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our\nattack in compromising black-box MLLMs even with extensive mitigating\nmechanisms, as well as the promising results against cutting-edge commercial\nAPIs, such as GPT-4o and Gemini 1.5. Our code is available at\nhttps://huggingface.co/RachelHGF/Mirage-in-the-Eyes."
    },
    {
        "date": "2025-01",
        "title": "Pre-trained Model Guided Mixture Knowledge Distillation for Adversarial Federated Learning",
        "author": "Yu Qiao, Huy Q. Le, Apurba Adhikary, and Choong Seon Hong",
        "link": "http://arxiv.org/abs/2501.15257v1",
        "abstract": "This paper aims to improve the robustness of a small global model while\nmaintaining clean accuracy under adversarial attacks and non-IID challenges in\nfederated learning. By leveraging the concise knowledge embedded in the class\nprobabilities from a pre-trained model for both clean and adversarial image\nclassification, we propose a Pre-trained Model-guided Adversarial Federated\nLearning (PM-AFL) training paradigm. This paradigm integrates vanilla mixture\nand adversarial mixture knowledge distillation to effectively balance accuracy\nand robustness while promoting local models to learn from diverse data.\nSpecifically, for clean accuracy, we adopt a dual distillation strategy where\nthe class probabilities of randomly paired images and their blended versions\nare aligned between the teacher model and the local models. For adversarial\nrobustness, we use a similar distillation approach but replace clean samples on\nthe local side with adversarial examples. Moreover, considering the bias\nbetween local and global models, we also incorporate a consistency\nregularization term to ensure that local adversarial predictions stay aligned\nwith their corresponding global clean ones. These strategies collectively\nenable local models to absorb diverse knowledge from the teacher model while\nmaintaining close alignment with the global model, thereby mitigating\noverfitting to local optima and enhancing the generalization of the global\nmodel. Experiments demonstrate that the PM-AFL-based paradigm outperforms other\nmethods that integrate defense strategies by a notable margin."
    },
    {
        "date": "2025-01",
        "title": "Median of Forests for Robust Density Estimation",
        "author": "Hongwei Wen, Annika Betken, and Tao Huang",
        "link": "http://arxiv.org/abs/2501.15157v1",
        "abstract": "Robust density estimation refers to the consistent estimation of the density\nfunction even when the data is contaminated by outliers. We find that existing\nforest density estimation at a certain point is inherently resistant to the\noutliers outside the cells containing the point, which we call\n\\textit{non-local outliers}, but not resistant to the rest \\textit{local\noutliers}. To achieve robustness against all outliers, we propose an ensemble\nlearning algorithm called \\textit{medians of forests for robust density\nestimation} (\\textit{MFRDE}), which adopts a pointwise median operation on\nforest density estimators fitted on subsampled datasets. Compared to existing\nrobust kernel-based methods, MFRDE enables us to choose larger subsampling\nsizes, sacrificing less accuracy for density estimation while achieving\nrobustness. On the theoretical side, we introduce the local outlier exponent to\nquantify the number of local outliers. Under this exponent, we show that even\nif the number of outliers reaches a certain polynomial order in the sample\nsize, MFRDE is able to achieve almost the same convergence rate as the same\nalgorithm on uncontaminated data, whereas robust kernel-based methods fail. On\nthe practical side, real data experiments show that MFRDE outperforms existing\nrobust kernel-based methods. Moreover, we apply MFRDE to anomaly detection to\nshowcase a further application."
    },
    {
        "date": "2025-01",
        "title": "PromptShield: Deployable Detection for Prompt Injection Attacks",
        "author": "Dennis Jacob, Hend Alzahrani, Zhanhao Hu, Basel Alomair, and David Wagner",
        "link": "http://arxiv.org/abs/2501.15145v1",
        "abstract": "Current application designers have moved to integrate large language models\n(LLMs) into their products. These LLM-integrated applications are vulnerable to\nprompt injection vulnerabilities. While attempts have been made to address this\nproblem by building a detector that can monitor inputs to the LLM and detect\nattacks, we find that many detectors are not yet suitable for practical\ndeployment. To support research in this area, we design the PromptShield\nbenchmark for evaluating practical prompt injection detectors. We also\nconstruct a new detector, the PromptShield detector, which achieves\nsignificantly better performance at detecting prompt injection attacks than any\nprior scheme. Our work suggests that larger models, more training data,\nappropriate metrics, and careful curation of training data can contribute to\nstrong detector performance."
    },
    {
        "date": "2025-01",
        "title": "TranStable: Towards Robust Pixel-level Online Video Stabilization by Jointing Transformer and CNN",
        "author": "zhizhen li, tianyi zhuo, Yifei Cao, Jizhe Yu, and Yu Liu",
        "link": "http://arxiv.org/abs/2501.15138v1",
        "abstract": "Video stabilization often struggles with distortion and excessive cropping.\nThis paper proposes a novel end-to-end framework, named TranStable, to address\nthese challenges, comprising a genera tor and a discriminator. We establish\nTransformerUNet (TUNet) as the generator to utilize the Hierarchical Adaptive\nFusion Module (HAFM), integrating Transformer and CNN to leverage both global\nand local features across multiple visual cues. By modeling frame-wise\nrelationships, it generates robust pixel-level warping maps for stable\ngeometric transformations. Furthermore, we design the Stability Discriminator\nModule (SDM), which provides pixel-wise supervision for authenticity and\nconsistency in training period, ensuring more complete field-of-view while\nminimizing jitter artifacts and enhancing visual fidelity. Extensive\nexperiments on NUS, DeepStab, and Selfie benchmarks demonstrate\nstate-of-the-art performance."
    },
    {
        "date": "2025-01",
        "title": "Comprehensive Evaluation of Cloaking Backdoor Attacks on Object Detector in Real-World",
        "author": "Hua Ma, Alsharif Abuadbba, Yansong Gao, Hyoungshick Kim, and Surya Nepal",
        "link": "http://arxiv.org/abs/2501.15101v1",
        "abstract": "The exploration of backdoor vulnerabilities in object detectors, particularly\nin real-world scenarios, remains limited. A significant challenge lies in the\nabsence of a natural physical backdoor dataset, and constructing such a dataset\nis both time- and labor-intensive. In this work, we address this gap by\ncreating a large-scale dataset comprising approximately 11,800 images/frames\nwith annotations featuring natural objects (e.g., T-shirts and hats) as\ntriggers to incur cloaking adversarial effects in diverse real-world scenarios.\nThis dataset is tailored for the study of physical backdoors in object\ndetectors. Leveraging this dataset, we conduct a comprehensive evaluation of an\ninsidious cloaking backdoor effect against object detectors, wherein the\nbounding box around a person vanishes when the individual is near a natural\nobject (e.g., a commonly available T-shirt) in front of the detector. Our\nevaluations encompass three prevalent attack surfaces: data outsourcing, model\noutsourcing, and the use of pretrained models. The cloaking effect is\nsuccessfully implanted in object detectors across all three attack surfaces. We\nextensively evaluate four popular object detection algorithms (anchor-based\nYolo-V3, Yolo-V4, Faster R-CNN, and anchor-free CenterNet) using 19 videos\n(totaling approximately 11,800 frames) in real-world scenarios. Our results\ndemonstrate that the backdoor attack exhibits remarkable robustness against\nvarious factors, including movement, distance, angle, non-rigid deformation,\nand lighting. In data and model outsourcing scenarios, the attack success rate\n(ASR) in most videos reaches 100% or near it, while the clean data accuracy of\nthe backdoored model remains indistinguishable from that of the clean model,\nmaking it impossible to detect backdoor behavior through a validation set."
    },
    {
        "date": "2025-01",
        "title": "Bringing RGB and IR Together: Hierarchical Multi-Modal Enhancement for Robust Transmission Line Detection",
        "author": "Shengdong Zhang, Xiaoqin Zhang, Wenqi Ren, Linlin Shen, Shaohua Wan, Jun Zhang, and Yujing M Jiang",
        "link": "http://arxiv.org/abs/2501.15099v1",
        "abstract": "Ensuring a stable power supply in rural areas relies heavily on effective\ninspection of power equipment, particularly transmission lines (TLs). However,\ndetecting TLs from aerial imagery can be challenging when dealing with\nmisalignments between visible light (RGB) and infrared (IR) images, as well as\nmismatched high- and low-level features in convolutional networks. To address\nthese limitations, we propose a novel Hierarchical Multi-Modal Enhancement\nNetwork (HMMEN) that integrates RGB and IR data for robust and accurate TL\ndetection. Our method introduces two key components: (1) a Mutual Multi-Modal\nEnhanced Block (MMEB), which fuses and enhances hierarchical RGB and IR feature\nmaps in a coarse-to-fine manner, and (2) a Feature Alignment Block (FAB) that\ncorrects misalignments between decoder outputs and IR feature maps by\nleveraging deformable convolutions. We employ MobileNet-based encoders for both\nRGB and IR inputs to accommodate edge-computing constraints and reduce\ncomputational overhead. Experimental results on diverse weather and lighting\nconditionsfog, night, snow, and daytimedemonstrate the superiority and\nrobustness of our approach compared to state-of-the-art methods, resulting in\nfewer false positives, enhanced boundary delineation, and better overall\ndetection performance. This framework thus shows promise for practical\nlarge-scale power line inspections with unmanned aerial vehicles."
    },
    {
        "date": "2025-01",
        "title": "Towards Better Robustness: Progressively Joint Pose-3DGS Learning for Arbitrarily Long Videos",
        "author": "Zhen-Hui Dong, Sheng Ye, Yu-Hui Wen, Nannan Li, and Yong-Jin Liu",
        "link": "http://arxiv.org/abs/2501.15096v1",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation due to\nits efficiency and high-fidelity rendering. However, 3DGS training requires a\nknown camera pose for each input view, typically obtained by\nStructure-from-Motion (SfM) pipelines. Pioneering works have attempted to relax\nthis restriction but still face difficulties when handling long sequences with\ncomplex camera trajectories. In this work, we propose Rob-GS, a robust\nframework to progressively estimate camera poses and optimize 3DGS for\narbitrarily long video sequences. Leveraging the inherent continuity of videos,\nwe design an adjacent pose tracking method to ensure stable pose estimation\nbetween consecutive frames. To handle arbitrarily long inputs, we adopt a\n\"divide and conquer\" scheme that adaptively splits the video sequence into\nseveral segments and optimizes them separately. Extensive experiments on the\nTanks and Temples dataset and our collected real-world dataset show that our\nRob-GS outperforms the state-of-the-arts."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Unsupervised Attention Prediction in Autonomous Driving",
        "author": "Mengshi Qi, Xiaoyang Bi, Pengfei Zhu, and Huadong Ma",
        "link": "http://arxiv.org/abs/2501.15045v2",
        "abstract": "Robustly predicting attention regions of interest for self-driving systems is\ncrucial for driving safety but presents significant challenges due to the\nlabor-intensive nature of obtaining large-scale attention labels and the domain\ngap between self-driving scenarios and natural scenes. These challenges are\nfurther exacerbated by complex traffic environments, including camera\ncorruption under adverse weather, noise interferences, and central bias from\nlong-tail distributions. To address these issues, we propose a robust\nunsupervised attention prediction method. An Uncertainty Mining Branch refines\npredictions by analyzing commonalities and differences across multiple\npre-trained models on natural scenes, while a Knowledge Embedding Block bridges\nthe domain gap by incorporating driving knowledge to adaptively enhance\npseudo-labels. Additionally, we introduce RoboMixup, a novel data augmentation\nmethod that improves robustness against corruption through soft attention and\ndynamic augmentation, and mitigates central bias by integrating random cropping\ninto Mixup as a regularizer. To systematically evaluate robustness in\nself-driving attention prediction, we introduce the DriverAttention-C\nbenchmark, comprising over 100k frames across three subsets: BDD-A-C,\nDR(eye)VE-C, and DADA-2000-C. Our method achieves performance equivalent to or\nsurpassing fully supervised state-of-the-art approaches on three public\ndatasets and the proposed robustness benchmark, reducing relative corruption\ndegradation by 58.8% and 52.8%, and improving central bias robustness by 12.4%\nand 11.4% in KLD and CC metrics, respectively. Code and data are available at\nhttps://github.com/zaplm/DriverAttention."
    },
    {
        "date": "2025-01",
        "title": "A Portable and Stealthy Inaudible Voice Attack Based on Acoustic Metamaterials",
        "author": "Zhiyuan Ning, Juan He, Zhanyong Tang, Weihang Hu, and Xiaojiang Chen",
        "link": "http://arxiv.org/abs/2501.15031v1",
        "abstract": "We present METAATTACK, the first approach to leverage acoustic metamaterials\nfor inaudible attacks for voice control systems. Compared to the\nstate-of-the-art inaudible attacks requiring complex and large speaker setups,\nMETAATTACK achieves a longer attacking range and higher accuracy using a\ncompact, portable device small enough to be put into a carry bag. These\nimprovements in portability and stealth have led to the practical applicability\nof inaudible attacks and their adaptation to a wider range of scenarios. We\ndemonstrate how the recent advancement in metamaterials can be utilized to\ndesign a voice attack system with carefully selected implementation parameters\nand commercial off-the-shelf components. We showcase that METAATTACK can be\nused to launch inaudible attacks for representative voice-controlled personal\nassistants, including Siri, Alexa, Google Assistant, XiaoAI, and Xiaoyi. The\naverage word accuracy of all assistants is 76%, with a range of 8.85 m."
    },
    {
        "date": "2025-01",
        "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations",
        "author": "Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, and Kehong Yuan",
        "link": "http://arxiv.org/abs/2501.16383v1",
        "abstract": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
    },
    {
        "date": "2025-01",
        "title": "Towards Distributed Backdoor Attacks with Network Detection in Decentralized Federated Learning",
        "author": "Bohan Liu, Yang Xiao, Ruimeng Ye, Zinan Ling, Xiaolong Ma, and Bo Hui",
        "link": "http://arxiv.org/abs/2501.15005v1",
        "abstract": "Distributed backdoor attacks (DBA) have shown a higher attack success rate\nthan centralized attacks in centralized federated learning (FL). However, it\nhas not been investigated in the decentralized FL. In this paper, we\nexperimentally demonstrate that, while directly applying DBA to decentralized\nFL, the attack success rate depends on the distribution of attackers in the\nnetwork architecture. Considering that the attackers can not decide their\nlocation, this paper aims to achieve a high attack success rate regardless of\nthe attackers' location distribution. Specifically, we first design a method to\ndetect the network by predicting the distance between any two attackers on the\nnetwork. Then, based on the distance, we organize the attackers in different\nclusters. Lastly, we propose an algorithm to \\textit{dynamically} embed local\npatterns decomposed from a global pattern into the different attackers in each\ncluster. We conduct a thorough empirical investigation and find that our method\ncan, in benchmark datasets, outperform both centralized attacks and naive DBA\nin different decentralized frameworks."
    },
    {
        "date": "2025-01",
        "title": "VideoPure: Diffusion-based Adversarial Purification for Video Recognition",
        "author": "Kaixun Jiang, Zhaoyu Chen, Jiyuan Fu, Lingyi Hong, Jinglun Li, and Wenqiang Zhang",
        "link": "http://arxiv.org/abs/2501.14999v1",
        "abstract": "Recent work indicates that video recognition models are vulnerable to\nadversarial examples, posing a serious security risk to downstream\napplications. However, current research has primarily focused on adversarial\nattacks, with limited work exploring defense mechanisms. Furthermore, due to\nthe spatial-temporal complexity of videos, existing video defense methods face\nissues of high cost, overfitting, and limited defense performance. Recently,\ndiffusion-based adversarial purification methods have achieved robust defense\nperformance in the image domain. However, due to the additional temporal\ndimension in videos, directly applying these diffusion-based adversarial\npurification methods to the video domain suffers performance and efficiency\ndegradation. To achieve an efficient and effective video adversarial defense\nmethod, we propose the first diffusion-based video purification framework to\nimprove video recognition models' adversarial robustness: VideoPure. Given an\nadversarial example, we first employ temporal DDIM inversion to transform the\ninput distribution into a temporally consistent and trajectory-defined\ndistribution, covering adversarial noise while preserving more video structure.\nThen, during DDIM denoising, we leverage intermediate results at each denoising\nstep and conduct guided spatial-temporal optimization, removing adversarial\nnoise while maintaining temporal consistency. Finally, we input the list of\noptimized intermediate results into the video recognition model for multi-step\nvoting to obtain the predicted class. We investigate the defense performance of\nour method against black-box, gray-box, and adaptive attacks on benchmark\ndatasets and models. Compared with other adversarial purification methods, our\nmethod overall demonstrates better defense performance against different\nattacks. Our code is available at https://github.com/deep-kaixun/VideoPure."
    },
    {
        "date": "2025-01",
        "title": "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech Recognition",
        "author": "Satwinder Singh, Qianli Wang, Zihan Zhong, Clarion Mendes, Mark Hasegawa-Johnson, Waleed Abdulla, and Seyed Reza Shahamiri",
        "link": "http://arxiv.org/abs/2501.14994v1",
        "abstract": "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria."
    },
    {
        "date": "2025-01",
        "title": "Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy",
        "author": "Fan Chen, and Alexander Rakhlin",
        "link": "http://arxiv.org/abs/2501.14928v1",
        "abstract": "We study the problem of interactive decision making in which the underlying\nenvironment changes over time subject to given constraints. We propose a\nframework, which we call \\textit{hybrid Decision Making with Structured\nObservations} (hybrid DMSO), that provides an interpolation between the\nstochastic and adversarial settings of decision making. Within this framework,\nwe can analyze local differentially private (LDP) decision making, query-based\nlearning (in particular, SQ learning), and robust and smooth decision making\nunder the same umbrella, deriving upper and lower bounds based on variants of\nthe Decision-Estimation Coefficient (DEC). We further establish strong\nconnections between the DEC's behavior, the SQ dimension, local minimax\ncomplexity, learnability, and joint differential privacy. To showcase the\nframework's power, we provide new results for contextual bandits under the LDP\nconstraint."
    },
    {
        "date": "2025-01",
        "title": "A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles",
        "author": "Stanislav Fort",
        "link": "http://arxiv.org/abs/2501.14496v1",
        "abstract": "This note documents an implementation issue in recent adaptive attacks (Zhang\net al. [2024]) against the multi-resolution self-ensemble defense (Fort and\nLakshminarayanan [2024]). The implementation allowed adversarial perturbations\nto exceed the standard $L_\\infty = 8/255$ bound by up to a factor of\n20$\\times$, reaching magnitudes of up to $L_\\infty = 160/255$. When attacks are\nproperly constrained within the intended bounds, the defense maintains\nnon-trivial robustness. Beyond highlighting the importance of careful\nvalidation in adversarial machine learning research, our analysis reveals an\nintriguing finding: properly bounded adaptive attacks against strong\nmulti-resolution self-ensembles often align with human perception, suggesting\nthe need to reconsider how we measure adversarial robustness."
    },
    {
        "date": "2025-01",
        "title": "Learning more with the same effort: how randomization improves the robustness of a robotic deep reinforcement learning agent",
        "author": "Luc\u00eda G\u00fcitta-L\u00f3pez, Jaime Boal, and \u00c1lvaro J. L\u00f3pez-L\u00f3pez",
        "link": "http://arxiv.org/abs/2501.14443v1",
        "abstract": "The industrial application of Deep Reinforcement Learning (DRL) is frequently\nslowed down because of the inability to generate the experience required to\ntrain the models. Collecting data often involves considerable time and economic\neffort that is unaffordable in most cases. Fortunately, devices like robots can\nbe trained with synthetic experience thanks to virtual environments. With this\napproach, the sample efficiency problems of artificial agents are mitigated,\nbut another issue arises: the need for efficiently transferring the synthetic\nexperience into the real world (sim-to-real).\n  This paper analyzes the robustness of a state-of-the-art sim-to-real\ntechnique known as progressive neural networks (PNNs) and studies how adding\ndiversity to the synthetic experience can complement it. To better understand\nthe drivers that lead to a lack of robustness, the robotic agent is still\ntested in a virtual environment to ensure total control on the divergence\nbetween the simulated and real models.\n  The results show that a PNN-like agent exhibits a substantial decrease in its\nrobustness at the beginning of the real training phase. Randomizing certain\nvariables during simulation-based training significantly mitigates this issue.\nOn average, the increase in the model's accuracy is around 25% when diversity\nis introduced in the training process. This improvement can be translated into\na decrease in the required real experience for the same final robustness\nperformance. Notwithstanding, adding real experience to agents should still be\nbeneficial regardless of the quality of the virtual experience fed into the\nagent."
    },
    {
        "date": "2025-01",
        "title": "Thunderdome: Timelock-Free Rationally-Secure Virtual Channels",
        "author": "Zeta Avarikioti, Yuheng Wang, and Yuyi Wang",
        "link": "http://arxiv.org/abs/2501.14418v2",
        "abstract": "Payment channel networks (PCNs) offer a promising solution to address the\nlimited transaction throughput of deployed blockchains. However, several\nattacks have recently been proposed that stress the vulnerability of PCNs to\ntimelock and censoring attacks. To address such attacks, we introduce\nThunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the\ndesign rationale of virtual channels to extend a timelock-free payment channel\nprimitive, thereby enabling multi-hop transactions without timelocks. Previous\nworks either utilize timelocks or do not accommodate transactions between\nparties that do not share a channel.\n  At its core, Thunderdome relies on a committee of non-trusted watchtowers,\nknown as wardens, who ensure that no honest party loses funds, even when\noffline, during the channel closure process. We introduce tailored incentive\nmechanisms to ensure that all participants follow the protocol's correct\nexecution. Besides a traditional security proof that assumes an honest majority\nof the committee, we conduct a formal game-theoretic analysis to demonstrate\nthe security of Thunderdome when all participants, including wardens, act\nrationally. We implement a proof of concept of Thunderdome on Ethereum to\nvalidate its feasibility and evaluate its costs. Our evaluation shows that\ndeploying Thunderdome, including opening the underlying payment channel, costs\napproximately \\$15 (0.0089 ETH), while the worst-case cost for closing a\nchannel is about \\$7 (0.004 ETH)."
    },
    {
        "date": "2025-01",
        "title": "Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis",
        "author": "Shinsaku Sakaue, Taira Tsuchiya, Han Bao, and Taihei Oki",
        "link": "http://arxiv.org/abs/2501.14349v2",
        "abstract": "We study an online learning problem where, over $T$ rounds, a learner\nobserves both time-varying sets of feasible actions and an agent's optimal\nactions, selected by solving linear optimization over the feasible actions. The\nlearner sequentially makes predictions of the agent's underlying linear\nobjective function, and their quality is measured by the regret, the cumulative\ngap between optimal objective values and those achieved by following the\nlearner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed\nthat online learning methods can be applied to this problem to achieve regret\nbounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023)\nsignificantly improved the result by achieving an $O(n^4\\ln T)$ regret bound,\nwhere $n$ is the dimension of the ambient space of objective vectors. Their\nmethod, based on the ellipsoid method, runs in polynomial time but is\ninefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$\nregret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of\n$n^3$. Our method is simple and efficient: we apply the online Newton step\n(ONS) to appropriate exp-concave loss functions. Moreover, for the case where\nthe agent's actions are possibly suboptimal, we establish an $O(n\\ln\nT+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative\nsuboptimality of the agent's actions. This bound is achieved by using MetaGrad,\nwhich runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We\nalso provide a simple instance that implies an $\\Omega(n)$ lower bound, showing\nthat our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise\nto a natural question: can the $O(\\ln T)$ factor in the upper bound be removed?\nFor the special case of $n=2$, we show that an $O(1)$ regret bound is possible,\nwhile we delineate challenges in extending this result to higher dimensions."
    },
    {
        "date": "2025-01",
        "title": "Securing DRAM at Scale: ARFM-Driven Row Hammer Defense with Unveiling the Threat of Short tRC Patterns",
        "author": "Nogeun Joo, Donghyuk Kim, Hyunjun Cho, Junseok Noh, Dongha Jung, and Joo-Young Kim",
        "link": "http://arxiv.org/abs/2501.14328v1",
        "abstract": "To address the issue of powerful row hammer (RH) attacks, our study involved\nan extensive analysis of the prevalent attack patterns in the field. We\ndiscovered a strong correlation between the timing and density of the\nactive-to-active command period, ${tRC}$, and the likelihood of RH attacks. In\nthis paper, we introduce MARC, an innovative ARFM-driven RH mitigation IP that\nsignificantly reinforces existing RH mitigation IPs. MARC dynamically adjusts\nthe frequency of RFM in response to the severity of the RH attack environment,\noffering a tailored security solution that not only detects the threats but\nalso adapts to varying threat levels. MARC's detection mechanism has\ndemonstrated remarkable efficiency, identifying over 99\\% of attack patterns.\nMoreover, MARC is designed as a compact hardware module, facilitating tight\nintegration either on the memory controller-side or DRAM-side within the memory\nsystem. It only occupies a negligible hardware area of 3363~\\textit{$\\mu m^2$}.\nBy activating ARFM based on MARC's detection, the additional energy overhead is\nalso negligible in normal workloads. We conduct experiments to compare the\nhighest row count throughout the patterns, defined as max exposure, between the\nvanilla RH mitigation IPs and the MARC-enhanced versions of the same IPs,\nfocusing on both DRAM-side and memory controller-side. On the DRAM-side, MARC +\nprobabilistic scheme and MARC + counter-based tracking scheme achieve\n8.1$\\times$ and 1.5$\\times$ improvement in max exposure ratio compared to the\nvanilla IPs, respectively. On the memory controller-side, the MARC + PARA and\nMARC + Graphene achieve 50$\\times$ and 5.7$\\times$ improvement in max exposure\nratio compared to the vanilla IPs, respectively. MARC ensures optimal security\nwithout sacrificing system performance, making MARC a pioneering solution in\nthe realm of RH attack mitigation."
    },
    {
        "date": "2025-01",
        "title": "Relative Layer-Wise Relevance Propagation: a more Robust Neural Networks eXplaination",
        "author": "Eric Nyiri, and Olivier Gibaru",
        "link": "http://arxiv.org/abs/2501.14322v1",
        "abstract": "Machine learning methods are solving very successfully a plethora of tasks,\nbut they have the disadvantage of not providing any information about their\ndecision. Consequently, estimating the reasoning of the system provides\nadditional information. For this, Layer-Wise Relevance Propagation (LRP) is one\nof the methods in eXplainable Machine Learning (XML). Its purpose is to provide\ncontributions of any neural network output in the domain of its input. The main\ndrawback of current methods is mainly due to division by small values. To\novercome this problem, we provide a new definition called Relative LRP where\nthe classical conservation law is satisfied up to a multiplicative factor but\nwithout divisions by small values except for Resnet skip connection. In this\narticle, we will focus on image classification. This allows us to visualize the\ncontributions of a pixel to the predictions of a multi-layer neural network.\nPixel contributions provide a focus to further analysis on regions of potential\ninterest. R-LRP can be applied for any dense, CNN or residual neural networks.\nMoreover, R-LRP doesn't need any hyperparameters to tune contrary to other LRP\nmethods. We then compare the R-LRP method on different datasets with simple\nCNN, VGG16, VGG19 and Resnet50 networks."
    },
    {
        "date": "2025-01",
        "title": "Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video",
        "author": "Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, and Xiaonan Huang",
        "link": "http://arxiv.org/abs/2501.14319v1",
        "abstract": "We aim to redefine robust ego-motion estimation and photorealistic 3D\nreconstruction by addressing a critical limitation: the reliance on noise-free\ndata in existing models. While such sanitized conditions simplify evaluation,\nthey fail to capture the unpredictable, noisy complexities of real-world\nenvironments. Dynamic motion, sensor imperfections, and synchronization\nperturbations lead to sharp performance declines when these models are deployed\nin practice, revealing an urgent need for frameworks that embrace and excel\nunder real-world noise. To bridge this gap, we tackle three core challenges:\nscalable data generation, comprehensive benchmarking, and model robustness\nenhancement. First, we introduce a scalable noisy data synthesis pipeline that\ngenerates diverse datasets simulating complex motion, sensor imperfections, and\nsynchronization errors. Second, we leverage this pipeline to create\nRobust-Ego3D, a benchmark rigorously designed to expose noise-induced\nperformance degradation, highlighting the limitations of current learning-based\nmethods in ego-motion accuracy and 3D reconstruction quality. Third, we propose\nCorrespondence-guided Gaussian Splatting (CorrGS), a novel test-time adaptation\nmethod that progressively refines an internal clean 3D representation by\naligning noisy observations with rendered RGB-D frames from clean 3D map,\nenhancing geometric alignment and appearance restoration through visual\ncorrespondence. Extensive experiments on synthetic and real-world data\ndemonstrate that CorrGS consistently outperforms prior state-of-the-art\nmethods, particularly in scenarios involving rapid motion and dynamic\nillumination."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Coreset Selection under Covariate Shift",
        "author": "Tomonari Tanaka, Hiroyuki Hanada, Hanting Yang, Tatsuya Aoyama, Yu Inatsu, Satoshi Akahane, Yoshito Okura, Noriaki Hashimoto, Taro Murayama, Hanju Lee, Shinya Kojima, and Ichiro Takeuchi",
        "link": "http://arxiv.org/abs/2501.14253v1",
        "abstract": "Coreset selection, which involves selecting a small subset from an existing\ntraining dataset, is an approach to reducing training data, and various\napproaches have been proposed for this method. In practical situations where\nthese methods are employed, it is often the case that the data distributions\ndiffer between the development phase and the deployment phase, with the latter\nbeing unknown. Thus, it is challenging to select an effective subset of\ntraining data that performs well across all deployment scenarios. We therefore\npropose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically\nderives an estimate of the upper bound for the worst-case test error, assuming\nthat the future covariate distribution may deviate within a defined range from\nthe training distribution. Furthermore, by selecting instances in a way that\nsuppresses the estimate of the upper bound for the worst-case test error, DRCS\nachieves distributionally robust training instance selection. This study is\nprimarily applicable to convex training computation, but we demonstrate that it\ncan also be applied to deep learning under appropriate approximations. In this\npaper, we focus on covariate shift, a type of data distribution shift, and\ndemonstrate the effectiveness of DRCS through experiments."
    },
    {
        "date": "2025-01",
        "title": "Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors",
        "author": "Yi Zhao, and Youzhi Zhang",
        "link": "http://arxiv.org/abs/2501.14250v1",
        "abstract": "Large language models (LLMs) are widely used in real-world applications,\nraising concerns about their safety and trustworthiness. While red-teaming with\njailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus\nprimarily on single-turn attacks, overlooking the multi-turn strategies used by\nreal-world adversaries. Existing multi-turn methods rely on static patterns or\npredefined logical chains, failing to account for the dynamic strategies during\nattacks. We propose Siren, a learning-based multi-turn attack framework\ndesigned to simulate real-world human jailbreak behaviors. Siren consists of\nthree stages: (1) training set construction utilizing Turn-Level LLM feedback\n(Turn-MF), (2) post-training attackers with supervised fine-tuning (SFT) and\ndirect preference optimization (DPO), and (3) interactions between the\nattacking and target LLMs. Experiments demonstrate that Siren achieves an\nattack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against\nGemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o,\nsignificantly outperforming single-turn baselines. Moreover, Siren with a\n7B-scale model achieves performance comparable to a multi-turn baseline that\nleverages GPT-4o as the attacker, while requiring fewer turns and employing\ndecomposition strategies that are better semantically aligned with attack\ngoals. We hope Siren inspires the development of stronger defenses against\nadvanced multi-turn jailbreak attacks under realistic scenarios. Code is\navailable at https://github.com/YiyiyiZhao/siren. Warning: This paper contains\npotentially harmful text."
    },
    {
        "date": "2025-01",
        "title": "GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm",
        "author": "Hanrui Wang, Ching-Chun Chang, Chun-Shien Lu, Christopher Leckie, and Isao Echizen",
        "link": "http://arxiv.org/abs/2501.14230v1",
        "abstract": "A critical requirement for deep learning models is ensuring their robustness\nagainst adversarial attacks. These attacks commonly introduce noticeable\nperturbations, compromising the visual fidelity of adversarial examples.\nAnother key challenge is that while white-box algorithms can generate effective\nadversarial perturbations, they require access to the model gradients, limiting\ntheir practicality in many real-world scenarios. Existing attack mechanisms\nstruggle to achieve similar efficacy without access to these gradients. In this\npaper, we introduce GreedyPixel, a novel pixel-wise greedy algorithm designed\nto generate high-quality adversarial examples using only query-based feedback\nfrom the target model. GreedyPixel improves computational efficiency in what is\ntypically a brute-force process by perturbing individual pixels in sequence,\nguided by a pixel-wise priority map. This priority map is constructed by\nranking gradients obtained from a surrogate model, providing a structured path\nfor perturbation. Our results demonstrate that GreedyPixel achieves attack\nsuccess rates comparable to white-box methods without the need for gradient\ninformation, and surpasses existing algorithms in black-box settings, offering\nhigher success rates, reduced computational time, and imperceptible\nperturbations. These findings underscore the advantages of GreedyPixel in terms\nof attack efficacy, time efficiency, and visual quality."
    },
    {
        "date": "2025-01",
        "title": "SelfPrompt: Confidence-Aware Semi-Supervised Tuning for Robust Vision-Language Model Adaptation",
        "author": "Shuvendu Roy, and Ali Etemad",
        "link": "http://arxiv.org/abs/2501.14148v2",
        "abstract": "We present SelfPrompt, a novel prompt-tuning approach for vision-language\nmodels (VLMs) in a semi-supervised learning setup. Existing methods for tuning\nVLMs in semi-supervised setups struggle with the negative impact of the\nmiscalibrated VLMs on pseudo-labelling, and the accumulation of noisy\npseudo-labels. SelfPrompt addresses these challenges by introducing a\ncluster-guided pseudo-labelling method that improves pseudo-label accuracy, and\na confidence-aware semi-supervised learning module that maximizes the\nutilization of unlabelled data by combining supervised learning and\nweakly-supervised learning. Additionally, we investigate our method in an\nactive semi-supervised learning setup, where the labelled set is strategically\nselected to ensure the best utilization of a limited labelling budget. To this\nend, we propose a weakly-supervised sampling technique that selects a diverse\nand representative labelled set, which can be seamlessly integrated into\nexisting methods to enhance their performance. We conduct extensive evaluations\nacross 13 datasets, significantly surpassing state-of-the-art performances with\naverage improvements of 6.23% in standard semi-supervised learning, 6.25% in\nactive semi-supervised learning, and 4.9% in base-to-novel generalization,\nusing a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in\nsingle-shot settings, achieving an average improvement of 11.78%."
    },
    {
        "date": "2025-01",
        "title": "Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters",
        "author": "Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha, Sahand Ghorbanpour, Avisek Naug, Ricardo Luna Gutierrez, and Antonio Guillen",
        "link": "http://arxiv.org/abs/2501.14122v1",
        "abstract": "We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact."
    },
    {
        "date": "2025-01",
        "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning",
        "author": "Joshua Davis, Thomas Sounack, Kate Sciacca, Jessie M Brain, Brigitte N Durieux, Nicole D Agaronnik, and Charlotta Lindvall",
        "link": "http://arxiv.org/abs/2501.14105v1",
        "abstract": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization",
        "author": "Hao Dong, Eleni Chatzi, and Olga Fink",
        "link": "http://arxiv.org/abs/2501.13924v1",
        "abstract": "Test-time adaptation (TTA) has demonstrated significant potential in\naddressing distribution shifts between training and testing data. Open-set\ntest-time adaptation (OSTTA) aims to adapt a source pre-trained model online to\nan unlabeled target domain that contains unknown classes. This task becomes\nmore challenging when multiple modalities are involved. Existing methods have\nprimarily focused on unimodal OSTTA, often filtering out low-confidence samples\nwithout addressing the complexities of multimodal data. In this work, we\npresent Adaptive Entropy-aware Optimization (AEO), a novel framework\nspecifically designed to tackle Multimodal Open-set Test-time Adaptation\n(MM-OSTTA) for the first time. Our analysis shows that the entropy difference\nbetween known and unknown samples in the target domain strongly correlates with\nMM-OSTTA performance. To leverage this, we propose two key components:\nUnknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality\nPrediction Discrepancy Optimization (AMP). These components enhance the ability\nof model to distinguish unknown class samples during online adaptation by\namplifying the entropy difference between known and unknown samples. To\nthoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish\na new benchmark derived from existing datasets. This benchmark includes two\ndownstream tasks and incorporates five modalities. Extensive experiments across\nvarious domain shift situations demonstrate the efficacy and versatility of the\nAEO framework. Additionally, we highlight the strong performance of AEO in\nlong-term and continual MM-OSTTA settings, both of which are challenging and\nhighly relevant to real-world applications. Our source code is available at\nhttps://github.com/donghao51/AEO."
    },
    {
        "date": "2025-01",
        "title": "Logical Maneuvers: Detecting and Mitigating Adversarial Hardware Faults in Space",
        "author": "Fatemeh Khojasteh Dana, Saleh Khalaj Monfared, and Shahin Tajik",
        "link": "http://arxiv.org/abs/2501.13894v1",
        "abstract": "Satellites are highly vulnerable to adversarial glitches or high-energy\nradiation in space, which could cause faults on the onboard computer. Various\nradiation- and fault-tolerant methods, such as error correction codes (ECC) and\nredundancy-based approaches, have been explored over the last decades to\nmitigate temporary soft errors on software and hardware. However, conventional\nECC methods fail to deal with hard errors or permanent faults in the hardware\ncomponents. This work introduces a detection- and response-based countermeasure\nto deal with partially damaged processor chips. It recovers the processor chip\nfrom permanent faults and enables continuous operation with available undamaged\nresources on the chip. We incorporate digitally-compatible delay-based sensors\non the target processor's chip to reliably detect the incoming radiation or\nglitching attempts on the physical fabric of the chip, even before a fault\noccurs. Upon detecting a fault in one or more components of the processor's\narithmetic logic unit (ALU), our countermeasure employs adaptive software\nrecompilations to resynthesize and substitute the affected instructions with\ninstructions of still functioning components to accomplish the task.\nFurthermore, if the fault is more widespread and prevents the correct operation\nof the entire processor, our approach deploys adaptive hardware partial\nreconfigurations to replace and reroute the failed components to undamaged\nlocations of the chip. To validate our claims, we deploy a high-energy\nnear-infrared (NIR) laser beam on a RISC-V processor implemented on a 28~nm\nFPGA to emulate radiation and even hard errors by partially damaging the FPGA\nfabric. We demonstrate that our sensor can confidently detect the radiation and\ntrigger the processor testing and fault recovery mechanisms. Finally, we\ndiscuss the overhead imposed by our countermeasure."
    },
    {
        "date": "2025-01",
        "title": "PhotoGAN: Generative Adversarial Neural Network Acceleration with Silicon Photonics",
        "author": "Tharini Suresh, Salma Afifi, and Sudeep Pasricha",
        "link": "http://arxiv.org/abs/2501.13828v1",
        "abstract": "Generative Adversarial Networks (GANs) are at the forefront of AI innovation,\ndriving advancements in areas such as image synthesis, medical imaging, and\ndata augmentation. However, the unique computational operations within GANs,\nsuch as transposed convolutions and instance normalization, introduce\nsignificant inefficiencies when executed on traditional electronic\naccelerators, resulting in high energy consumption and suboptimal performance.\nTo address these challenges, we introduce PhotoGAN, the first silicon-photonic\naccelerator designed to handle the specialized operations of GAN models. By\nleveraging the inherent high throughput and energy efficiency of silicon\nphotonics, PhotoGAN offers an innovative, reconfigurable architecture capable\nof accelerating transposed convolutions and other GAN-specific layers. The\naccelerator also incorporates a sparse computation optimization technique to\nreduce redundant operations, improving computational efficiency. Our\nexperimental results demonstrate that PhotoGAN achieves at least 4.4x higher\nGOPS and 2.18x lower energy-per-bit (EPB) compared to state-of-the-art\naccelerators, including GPUs and TPUs. These findings showcase PhotoGAN as a\npromising solution for the next generation of GAN acceleration, providing\nsubstantial gains in both performance and energy efficiency."
    },
    {
        "date": "2025-01",
        "title": "WAFBOOSTER: Automatic Boosting of WAF Security Against Mutated Malicious Payloads",
        "author": "Cong Wu, Jing Chen, Simeng Zhu, Wenqi Feng, Ruiying Du, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.14008v1",
        "abstract": "Web application firewall (WAF) examines malicious traffic to and from a web\napplication via a set of security rules. It plays a significant role in\nsecuring Web applications against web attacks. However, as web attacks grow in\nsophistication, it is becoming increasingly difficult for WAFs to block the\nmutated malicious payloads designed to bypass their defenses. In response to\nthis critical security issue, we have developed a novel learning-based\nframework called WAFBOOSTER, designed to unveil potential bypasses in WAF\ndetections and suggest rules to fortify their security. Using a combination of\nshadow models and payload generation techniques, we can identify malicious\npayloads and remove or modify them as needed. WAFBOOSTER generates signatures\nfor these malicious payloads using advanced clustering and regular expression\nmatching techniques to repair any security gaps we uncover. In our\ncomprehensive evaluation of eight real-world WAFs, WAFBOOSTER improved the true\nrejection rate of mutated malicious payloads from 21% to 96%, with no false\nrejections. WAFBOOSTER achieves a false acceptance rate 3X lower than\nstate-of-the-art methods for generating malicious payloads. With WAFBOOSTER, we\nhave taken a step forward in securing web applications against the\never-evolving threats."
    },
    {
        "date": "2025-01",
        "title": "Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems",
        "author": "Ping He, Lorenzo Cavallaro, and Shouling Ji",
        "link": "http://arxiv.org/abs/2501.13782v1",
        "abstract": "Android malware presents a persistent threat to users' privacy and data\nintegrity. To combat this, researchers have proposed machine learning-based\n(ML-based) Android malware detection (AMD) systems. However, adversarial\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\nsystems, raising significant concerns. Existing defenses against adversarial\nAndroid malware provide protections against feature space attacks which\ngenerate adversarial feature vectors only, leaving protection against realistic\nthreats from problem space attacks which generate real adversarial malware an\nopen problem. In this paper, we address this gap by proposing ADD, a practical\nadversarial Android malware defense framework designed as a plug-in to enhance\nthe adversarial robustness of the ML-based AMD systems against problem space\nattacks. Our extensive evaluation across various ML-based AMD systems\ndemonstrates that ADD is effective against state-of-the-art problem space\nadversarial Android malware attacks. Additionally, ADD shows the defense\neffectiveness in enhancing the adversarial robustness of real-world antivirus\nsolutions."
    },
    {
        "date": "2025-01",
        "title": "Crossfire: An Elastic Defense Framework for Graph Neural Networks Under Bit Flip Attacks",
        "author": "Lorenz Kummer, Samir Moustafa, Wilfried Gansterer, and Nils Kriege",
        "link": "http://arxiv.org/abs/2501.13776v1",
        "abstract": "Bit Flip Attacks (BFAs) are a well-established class of adversarial attacks,\noriginally developed for Convolutional Neural Networks within the computer\nvision domain. Most recently, these attacks have been extended to target Graph\nNeural Networks (GNNs), revealing significant vulnerabilities. This new\ndevelopment naturally raises questions about the best strategies to defend GNNs\nagainst BFAs, a challenge for which no solutions currently exist. Given the\napplications of GNNs in critical fields, any defense mechanism must not only\nmaintain network performance, but also verifiably restore the network to its\npre-attack state. Verifiably restoring the network to its pre-attack state also\neliminates the need for costly evaluations on test data to ensure network\nquality. We offer first insights into the effectiveness of existing honeypot-\nand hashing-based defenses against BFAs adapted from the computer vision domain\nto GNNs, and characterize the shortcomings of these approaches. To overcome\ntheir limitations, we propose Crossfire, a hybrid approach that exploits weight\nsparsity and combines hashing and honeypots with bit-level correction of\nout-of-distribution weight elements to restore network integrity. Crossfire is\nretraining-free and does not require labeled data. Averaged over 2,160\nexperiments on six benchmark datasets, Crossfire offers a 21.8% higher\nprobability than its competitors of reconstructing a GNN attacked by a BFA to\nits pre-attack state. These experiments cover up to 55 bit flips from various\nattacks. Moreover, it improves post-repair prediction quality by 10.85%.\nComputational and storage overheads are negligible compared to the inherent\ncomplexity of even the simplest GNNs."
    },
    {
        "date": "2025-01",
        "title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings",
        "author": "Yumeng Wang, Ziran Zhou, and Junjin Wang",
        "link": "http://arxiv.org/abs/2501.13758v1",
        "abstract": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task."
    },
    {
        "date": "2025-01",
        "title": "Exact Soft Analytical Side-Channel Attacks using Tractable Circuits",
        "author": "Thomas Wedenig, Rishub Nagpal, Ga\u00ebtan Cassiers, Stefan Mangard, and Robert Peharz",
        "link": "http://arxiv.org/abs/2501.13748v1",
        "abstract": "Detecting weaknesses in cryptographic algorithms is of utmost importance for\ndesigning secure information systems. The state-of-the-art soft analytical\nside-channel attack (SASCA) uses physical leakage information to make\nprobabilistic predictions about intermediate computations and combines these\n\"guesses\" with the known algorithmic logic to compute the posterior\ndistribution over the key. This attack is commonly performed via loopy belief\npropagation, which, however, lacks guarantees in terms of convergence and\ninference quality. In this paper, we develop a fast and exact inference method\nfor SASCA, denoted as ExSASCA, by leveraging knowledge compilation and\ntractable probabilistic circuits. When attacking the Advanced Encryption\nStandard (AES), the most widely used encryption algorithm to date, ExSASCA\noutperforms SASCA by more than 31% top-1 success rate absolute. By leveraging\nsparse belief messages, this performance is achieved with little more\ncomputational cost than SASCA, and about 3 orders of magnitude less than exact\ninference via exhaustive enumeration. Even with dense belief messages, ExSASCA\nstill uses 6 times less computations than exhaustive inference."
    },
    {
        "date": "2025-01",
        "title": "A Comprehensive Framework for Building Highly Secure, Network-Connected Devices: Chip to App",
        "author": "Khan Reaz, and Gerhard Wunder",
        "link": "http://arxiv.org/abs/2501.13716v1",
        "abstract": "The rapid expansion of connected devices has amplified the need for robust\nand scalable security frameworks. This paper proposes a holistic approach to\nsecuring network-connected devices, covering essential layers: hardware,\nfirmware, communication, and application. At the hardware level, we focus on\nsecure key management, reliable random number generation, and protecting\ncritical assets. Firmware security is addressed through mechanisms like\ncryptographic integrity validation and secure boot processes. For secure\ncommunication, we emphasize TLS 1.3 and optimized cipher suites tailored for\nboth standard and resource-constrained devices. To overcome the challenges of\nIoT, compact digital certificates, such as CBOR, are recommended to reduce\noverhead and enhance performance. Additionally, the paper explores\nforward-looking solutions, including post-quantum cryptography, to future-proof\nsystems against emerging threats. This framework provides actionable guidelines\nfor manufacturers and system administrators to build secure devices that\nmaintain confidentiality, integrity, and availability throughout their\nlifecycle."
    },
    {
        "date": "2025-01",
        "title": "Certified Robustness Under Bounded Levenshtein Distance",
        "author": "Elias Abad Rocamora, Grigorios G. Chrysos, and Volkan Cevher",
        "link": "http://arxiv.org/abs/2501.13676v1",
        "abstract": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain."
    },
    {
        "date": "2025-01",
        "title": "Device-aware Optical Adversarial Attack for a Portable Projector-camera System",
        "author": "Ning Jiang, Yanhong Liu, Dingheng Zeng, Yue Feng, Weihong Deng, and Ying Li",
        "link": "http://arxiv.org/abs/2501.14005v1",
        "abstract": "Deep-learning-based face recognition (FR) systems are susceptible to\nadversarial examples in both digital and physical domains. Physical attacks\npresent a greater threat to deployed systems as adversaries can easily access\nthe input channel, allowing them to provide malicious inputs to impersonate a\nvictim. This paper addresses the limitations of existing projector-camera-based\nadversarial light attacks in practical FR setups. By incorporating device-aware\nadaptations into the digital attack algorithm, such as resolution-aware and\ncolor-aware adjustments, we mitigate the degradation from digital to physical\ndomains. Experimental validation showcases the efficacy of our proposed\nalgorithm against real and spoof adversaries, achieving high physical\nsimilarity scores in FR models and state-of-the-art commercial systems. On\naverage, there is only a 14% reduction in scores from digital to physical\nattacks, with high attack success rate in both white- and black-box scenarios."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Incremental Learning under Ambiguous Supervision",
        "author": "Rui Wang, Mingxuan Xia, Chang Yao, Lei Feng, Junbo Zhao, Gang Chen, and Haobo Wang",
        "link": "http://arxiv.org/abs/2501.13584v2",
        "abstract": "Traditional Incremental Learning (IL) targets to handle sequential\nfully-supervised learning problems where novel classes emerge from time to\ntime. However, due to inherent annotation uncertainty and ambiguity, collecting\nhigh-quality annotated data in a dynamic learning system can be extremely\nexpensive. To mitigate this problem, we propose a novel weakly-supervised\nlearning paradigm called Incremental Partial Label Learning (IPLL), where the\nsequentially arrived data relate to a set of candidate labels rather than the\nground truth. Technically, we develop the Prototype-Guided Disambiguation and\nReplay Algorithm (PGDR) which leverages the class prototypes as a proxy to\nmitigate two intertwined challenges in IPLL, i.e., label ambiguity and\ncatastrophic forgetting. To handle the former, PGDR encapsulates a\nmomentum-based pseudo-labeling algorithm along with prototype-guided\ninitialization, resulting in a balanced perception of classes. To alleviate\nforgetting, we develop a memory replay technique that collects\nwell-disambiguated samples while maintaining representativeness and diversity.\nBy jointly distilling knowledge from curated memory data, our framework\nexhibits a great disambiguation ability for samples of new tasks and achieves\nless forgetting of knowledge. Extensive experiments demonstrate that PGDR\nachieves superior"
    },
    {
        "date": "2025-01",
        "title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving",
        "author": "Lu Wang, Tianyuan Zhang, Yang Qu, Siyuan Liang, Yuwei Chen, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.13563v1",
        "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities; however, these models remain highly\nsusceptible to adversarial attacks. While existing research has explored\nwhite-box attacks to some extent, the more practical and challenging black-box\nscenarios remain largely underexplored due to their inherent difficulty. In\nthis paper, we take the first step toward designing black-box adversarial\nattacks specifically targeting VLMs in AD. We identify two key challenges for\nachieving effective black-box attacks in this context: the effectiveness across\ndriving reasoning chains in AD systems and the dynamic nature of driving\nscenarios. To address this, we propose Cascading Adversarial Disruption (CAD).\nIt first introduces Decision Chain Disruption, which targets low-level\nreasoning breakdown by generating and injecting deceptive semantics, ensuring\nthe perturbations remain effective across the entire decision-making chain.\nBuilding on this, we present Risky Scene Induction, which addresses dynamic\nadaptation by leveraging a surrogate VLM to understand and construct high-level\nrisky scenarios that are likely to result in critical errors in the current\ndriving contexts. Extensive experiments conducted on multiple AD VLMs and\nbenchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness,\nsignificantly outperforming existing methods (+13.43% on average). Moreover, we\nvalidate its practical applicability through real-world attacks on AD vehicles\npowered by VLMs, where the route completion rate drops by 61.11% and the\nvehicle crashes directly into the obstacle vehicle with adversarial patches.\nFinally, we release CADA dataset, comprising 18,808 adversarial\nvisual-question-answer pairs, to facilitate further evaluation and research in\nthis critical domain. Our codes and dataset will be available after paper's\nacceptance."
    },
    {
        "date": "2025-01",
        "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
        "author": "Yehuda Afek, Harel Berger, and Anat Bremler-Barr",
        "link": "http://arxiv.org/abs/2501.13540v1",
        "abstract": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
    },
    {
        "date": "2025-01",
        "title": "Overcoming Support Dilution for Robust Few-shot Semantic Segmentation",
        "author": "Wailing Tang, Biqi Yang, Pheng-Ann Heng, Yun-Hui Liu, and Chi-Wing Fu",
        "link": "http://arxiv.org/abs/2501.13529v1",
        "abstract": "Few-shot Semantic Segmentation (FSS) is a challenging task that utilizes\nlimited support images to segment associated unseen objects in query images.\nHowever, recent FSS methods are observed to perform worse, when enlarging the\nnumber of shots. As the support set enlarges, existing FSS networks struggle to\nconcentrate on the high-contributed supports and could easily be overwhelmed by\nthe low-contributed supports that could severely impair the mask predictions.\nIn this work, we study this challenging issue, called support dilution, our\ngoal is to recognize, select, preserve, and enhance those high-contributed\nsupports in the raw support pool. Technically, our method contains three novel\nparts. First, we propose a contribution index, to quantitatively estimate if a\nhigh-contributed support dilutes. Second, we develop the Symmetric Correlation\n(SC) module to preserve and enhance the high-contributed support features,\nminimizing the distraction by the low-contributed features. Third, we design\nthe Support Image Pruning operation, to retrieve a compact and high quality\nsubset by discarding low-contributed supports. We conduct extensive experiments\non two FSS benchmarks, COCO-20i and PASCAL-5i, the segmentation results\ndemonstrate the compelling performance of our solution over state-of-the-art\nFSS approaches. Besides, we apply our solution for online segmentation and\nreal-world segmentation, convincing segmentation results showing the practical\nability of our work for real-world demonstrations."
    },
    {
        "date": "2025-01",
        "title": "Robust Amortized Bayesian Inference with Self-Consistency Losses on Unlabeled Data",
        "author": "Aayush Mishra, Daniel Habermann, Marvin Schmitt, Stefan T. Radev, and Paul-Christian B\u00fcrkner",
        "link": "http://arxiv.org/abs/2501.13483v1",
        "abstract": "Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
        "author": "Rishabh Agrawal",
        "link": "http://arxiv.org/abs/2501.13479v1",
        "abstract": "Few-shot learning (FSL) enables machine learning models to generalize\neffectively with minimal labeled data, making it crucial for data-scarce\ndomains such as healthcare, robotics, and natural language processing. Despite\nits potential, FSL faces challenges including sensitivity to initialization,\ndifficulty in adapting to diverse domains, and vulnerability to noisy datasets.\nTo address these issues, this paper introduces Adaptive Few-Shot Learning\n(AFSL), a framework that integrates advancements in meta-learning, domain\nalignment, noise resilience, and multi-modal integration. AFSL consists of four\nkey modules: a Dynamic Stability Module for performance consistency, a\nContextual Domain Alignment Module for domain adaptation, a Noise-Adaptive\nResilience Module for handling noisy data, and a Multi-Modal Fusion Module for\nintegrating diverse modalities. This work also explores strategies such as\ntask-aware data augmentation, semi-supervised learning, and explainable AI\ntechniques to enhance the applicability and robustness of FSL. AFSL provides\nscalable, reliable, and impactful solutions for real-world, high-stakes\ndomains."
    },
    {
        "date": "2025-01",
        "title": "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for Robust Deepfake Detection",
        "author": "Jiaxin Chen, Miao Hu, Dengyong Zhang, and Jingyang Meng",
        "link": "http://arxiv.org/abs/2501.13435v1",
        "abstract": "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios."
    },
    {
        "date": "2025-01",
        "title": "AEON: Adaptive Estimation of Instance-Dependent In-Distribution and Out-of-Distribution Label Noise for Robust Learning",
        "author": "Arpit Garg, Cuong Nguyen, Rafael Felix, Yuyuan Liu, Thanh-Toan Do, and Gustavo Carneiro",
        "link": "http://arxiv.org/abs/2501.13389v1",
        "abstract": "Robust training with noisy labels is a critical challenge in image\nclassification, offering the potential to reduce reliance on costly clean-label\ndatasets. Real-world datasets often contain a mix of in-distribution (ID) and\nout-of-distribution (OOD) instance-dependent label noise, a challenge that is\nrarely addressed simultaneously by existing methods and is further compounded\nby the lack of comprehensive benchmarking datasets. Furthermore, even though\ncurrent noisy-label learning approaches attempt to find noisy-label samples\nduring training, these methods do not aim to estimate ID and OOD noise rates to\npromote their effectiveness in the selection of such noisy-label samples, and\nthey are often represented by inefficient multi-stage learning algorithms. We\npropose the Adaptive Estimation of Instance-Dependent In-Distribution and\nOut-of-Distribution Label Noise (AEON) approach to address these research gaps.\nAEON is an efficient one-stage noisy-label learning methodology that\ndynamically estimates instance-dependent ID and OOD label noise rates to\nenhance robustness to complex noise settings. Additionally, we introduce a new\nbenchmark reflecting real-world ID and OOD noise scenarios. Experiments\ndemonstrate that AEON achieves state-of-the-art performance on both synthetic\nand real-world datasets"
    },
    {
        "date": "2025-01",
        "title": "False Sense of Security on Protected Wi-Fi Networks",
        "author": "Yong Zhi Lim, Hazmei Bin Abdul Rahman, and Biplab Sikdar",
        "link": "http://arxiv.org/abs/2501.13363v1",
        "abstract": "The Wi-Fi technology (IEEE 802.11) was introduced in 1997. With the\nincreasing use and deployment of such networks, their security has also\nattracted considerable attention. Current Wi-Fi networks use WPA2 (Wi-Fi\nProtected Access 2) for security (authentication and encryption) between access\npoints and clients. According to the IEEE 802.11i-2004 standard, wireless\nnetworks secured with WPA2-PSK (Pre-Shared Key) are required to be protected\nwith a passphrase between 8 to 63 ASCII characters. However, a poorly chosen\npassphrase significantly reduces the effectiveness of both WPA2 and\nWPA3-Personal Transition Mode. The objective of this paper is to empirically\nevaluate password choices in the wild and evaluate weakness in current common\npractices. We collected a total of 3,352 password hashes from Wi-Fi access\npoints and determine the passphrases that were protecting them. We then analyze\nthese passwords to investigate the impact of user's behavior and preference for\nconvenience on passphrase strength in secured private Wi-Fi networks in\nSingapore. We characterized the predictability of passphrases that use the\nminimum required length of 8 numeric or alphanumeric characters, and/or symbols\nstipulated in wireless security standards, and the usage of default passwords,\nand found that 16 percent of the passwords show such behavior. Our results also\nindicate the prevalence of the use of default passwords by hardware\nmanufacturers. We correlate our results with our findings and recommend methods\nthat will improve the overall security and future of our Wi-Fi networks."
    },
    {
        "date": "2025-01",
        "title": "50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal Detection, and Security Implications",
        "author": "Zewei Shi, Ruoxi Sun, Jieshan Chen, Jiamou Sun, Minhui Xue, Yansong Gao, Feng Liu, and Xingliang Yuan",
        "link": "http://arxiv.org/abs/2501.13351v1",
        "abstract": "Deceptive patterns (DPs) are user interface designs deliberately crafted to\nmanipulate users into unintended decisions, often by exploiting cognitive\nbiases for the benefit of companies or services. While numerous studies have\nexplored ways to identify these deceptive patterns, many existing solutions\nrequire significant human intervention and struggle to keep pace with the\nevolving nature of deceptive designs. To address these challenges, we expanded\nthe deceptive pattern taxonomy from security and privacy perspectives, refining\nits categories and scope. We created a comprehensive dataset of deceptive\npatterns by integrating existing small-scale datasets with new samples,\nresulting in 6,725 images and 10,421 DP instances from mobile apps and\nwebsites. We then developed DPGuard, a novel automatic tool leveraging\ncommercial multimodal large language models (MLLMs) for deceptive pattern\ndetection. Experimental results show that DPGuard outperforms state-of-the-art\nmethods. Finally, we conducted an extensive empirical evaluation on 2,000\npopular mobile apps and websites, revealing that 23.61% of mobile screenshots\nand 47.27% of website screenshots feature at least one deceptive pattern\ninstance. Through four unexplored case studies that inform security\nimplications, we highlight the critical importance of the unified taxonomy in\naddressing the growing challenges of Internet deception."
    },
    {
        "date": "2025-01",
        "title": "Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on Retrieval-Augmented Diffusion Models",
        "author": "Hao Fang, Xiaohang Sui, Hongyao Yu, Jiawei Kong, Sijin Yu, Bin Chen, Hao Wu, and Shu-Tao Xia",
        "link": "http://arxiv.org/abs/2501.13340v1",
        "abstract": "Diffusion models (DMs) have recently demonstrated remarkable generation\ncapability. However, their training generally requires huge computational\nresources and large-scale datasets. To solve these, recent studies empower DMs\nwith the advanced Retrieval-Augmented Generation (RAG) technique and propose\nretrieval-augmented diffusion models (RDMs). By incorporating rich knowledge\nfrom an auxiliary database, RAG enhances diffusion models' generation and\ngeneralization ability while significantly reducing model parameters. Despite\nthe great success, RAG may introduce novel security issues that warrant further\ninvestigation. In this paper, we reveal that the RDM is susceptible to backdoor\nattacks by proposing a multimodal contrastive attack approach named BadRDM. Our\nframework fully considers RAG's characteristics and is devised to manipulate\nthe retrieved items for given text triggers, thereby further controlling the\ngenerated contents. Specifically, we first insert a tiny portion of images into\nthe retrieval database as target toxicity surrogates. Subsequently, a malicious\nvariant of contrastive learning is adopted to inject backdoors into the\nretriever, which builds shortcuts from triggers to the toxicity surrogates.\nFurthermore, we enhance the attacks through novel entropy-based selection and\ngenerative augmentation strategies that can derive better toxicity surrogates.\nExtensive experiments on two mainstream tasks demonstrate the proposed BadRDM\nachieves outstanding attack effects while preserving the model's benign\nutility."
    },
    {
        "date": "2025-01",
        "title": "Gradient-Free Adversarial Purification with Diffusion Models",
        "author": "Xuelong Dai, Dong Wang, Duan Mingxing, and Bin Xiao",
        "link": "http://arxiv.org/abs/2501.13336v1",
        "abstract": "Adversarial training and adversarial purification are two effective and\npractical defense methods to enhance a model's robustness against adversarial\nattacks. However, adversarial training necessitates additional training, while\nadversarial purification suffers from low time efficiency. More critically,\ncurrent defenses are designed under the perturbation-based adversarial threat\nmodel, which is ineffective against the recently proposed unrestricted\nadversarial attacks. In this paper, we propose an effective and efficient\nadversarial defense method that counters both perturbation-based and\nunrestricted adversarial attacks. Our defense is inspired by the observation\nthat adversarial attacks are typically located near the decision boundary and\nare sensitive to pixel changes. To address this, we introduce adversarial\nanti-aliasing to mitigate adversarial modifications. Additionally, we propose\nadversarial super-resolution, which leverages prior knowledge from clean\ndatasets to benignly recover images. These approaches do not require additional\ntraining and are computationally efficient without calculating gradients.\nExtensive experiments against both perturbation-based and unrestricted\nadversarial attacks demonstrate that our defense method outperforms\nstate-of-the-art adversarial purification methods."
    },
    {
        "date": "2025-01",
        "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
        "author": "Akshit Achara, and Anshuman Chhabra",
        "link": "http://arxiv.org/abs/2501.13302v1",
        "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Robust Fairness via Confusional Spectral Regularization",
        "author": "Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, and Ronghui Mu",
        "link": "http://arxiv.org/abs/2501.13273v1",
        "abstract": "Recent research has highlighted a critical issue known as ``robust fairness\",\nwhere robust accuracy varies significantly across different classes,\nundermining the reliability of deep neural networks (DNNs). A common approach\nto address this has been to dynamically reweight classes during training,\ngiving more weight to those with lower empirical robust performance. However,\nwe find there is a divergence of class-wise robust performance between training\nset and testing set, which limits the effectiveness of these explicit\nreweighting methods, indicating the need for a principled alternative. In this\nwork, we derive a robust generalization bound for the worst-class robust error\nwithin the PAC-Bayesian framework, accounting for unknown data distributions.\nOur analysis shows that the worst-class robust error is influenced by two main\nfactors: the spectral norm of the empirical robust confusion matrix and the\ninformation embedded in the model and training set. While the latter has been\nextensively studied, we propose a novel regularization technique targeting the\nspectral norm of the robust confusion matrix to improve worst-class robust\naccuracy and enhance robust fairness. We validate our approach through\ncomprehensive experiments on various datasets and models, demonstrating its\neffectiveness in enhancing robust fairness."
    },
    {
        "date": "2025-01",
        "title": "Threat-based Security Controls to Protect Industrial Control Systems",
        "author": "Haritha Srinivasan, and Maryam Karimi",
        "link": "http://arxiv.org/abs/2501.13268v1",
        "abstract": "This paper analyzes the reported threats to Industrial Control Systems\n(ICS)/Operational Technology (OT) and identifies common tactics, techniques,\nand procedures (TTP) used by threat actors. The paper then uses the MITRE\nATT&CK framework to map the common TTPs and provide an understanding of the\nsecurity controls needed to defend against the reported ICS threats. The paper\nalso includes a review of ICS testbeds and ideas for future research using the\nidentified controls."
    },
    {
        "date": "2025-01",
        "title": "Active RIS-Assisted URLLC NOMA-Based 5G Network with FBL under Jamming Attacks",
        "author": "Ghazal Asemian, Mohammadreza Amini, and Burak Kantarci",
        "link": "http://arxiv.org/abs/2501.13231v1",
        "abstract": "In this paper, we tackle the challenge of jamming attacks in Ultra-Reliable\nLow Latency Communication (URLLC) within Non-Orthogonal Multiple Access\n(NOMA)-based 5G networks under Finite Blocklength (FBL) conditions. We\nintroduce an innovative approach that employs Reconfigurable Intelligent\nSurfaces (RIS) with active elements to enhance energy efficiency while ensuring\nreliability and meeting latency requirements. Our approach incorporates the\ntraffic model, making it practical for real-world scenarios with dynamic\ntraffic loads. We thoroughly analyze the impact of blocklength and packet\narrival rate on network performance metrics and investigate the optimal\namplitude value and number of RIS elements. Our results indicate that\nincreasing the number of RIS elements from 4 to 400 can improve\nsignal-to-jamming-plus-noise ratio (SJNR) by 13.64\\%. Additionally, optimizing\nblocklength and packet arrival rate can achieve a 31.68% improvement in energy\nefficiency and reduced latency. These findings underscore the importance of\noptimized settings for effective jamming mitigation."
    },
    {
        "date": "2025-01",
        "title": "Joint Task Offloading and User Scheduling in 5G MEC under Jamming Attacks",
        "author": "Mohammadreza Amini, Burak Kantarci, Claude D'Amours, and Melike Erol-Kantarci",
        "link": "http://arxiv.org/abs/2501.13227v1",
        "abstract": "In this paper, we propose a novel joint task offloading and user scheduling\n(JTO-US) framework for 5G mobile edge computing (MEC) systems under security\nthreats from jamming attacks. The goal is to minimize the delay and the ratio\nof dropped tasks, taking into account both communication and computation\ndelays. The system model includes a 5G network equipped with MEC servers and an\nadversarial on-off jammer that disrupts communication. The proposed framework\noptimally schedules tasks and users to minimize the impact of jamming while\nensuring that high-priority tasks are processed efficiently. Genetic algorithm\n(GA) is used to solve the optimization problem, and the results are compared\nwith benchmark methods such as GA without considering jamming effect, Shortest\nJob First (SJF), and Shortest Deadline First (SDF). The simulation results\ndemonstrate that the proposed JTO-US framework achieves the lowest drop ratio\nand effectively manages priority tasks, outperforming existing methods.\nParticularly, when the jamming probability is 0.8, the proposed framework\nmitigates the jammer's impact by reducing the drop ratio to 63%, compared to\n89% achieved by the next best method."
    },
    {
        "date": "2025-01",
        "title": "Robust Representation Consistency Model via Contrastive Denoising",
        "author": "Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Zhongjia Ba, Kui Ren, Jun Zhu, and Anima Anandkumar",
        "link": "http://arxiv.org/abs/2501.13094v1",
        "abstract": "Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM."
    },
    {
        "date": "2025-01",
        "title": "CHaRNet: Conditioned Heatmap Regression for Robust Dental Landmark Localization",
        "author": "Jos\u00e9 Rodr\u00edguez-Ortega, Francisco P\u00e9rez-Hern\u00e1ndez, and Siham Tabik",
        "link": "http://arxiv.org/abs/2501.13073v3",
        "abstract": "Identifying anatomical landmarks in 3D dental models is crucial for\northodontic treatment. Manually placing these key points is complex,\ntime-consuming, and requires expert knowledge. While some machine learning\nmethods have been proposed for automatic tooth landmark detection in 3D\nIntraoral Scans (IOS), research remains limited, with no fully end-to-end\napproaches that avoid teeth segmentation. We propose CHaRNet (Conditioned\nHeatmap Regression Network), the first end-to-end deep learning method for\ntooth landmark detection in 3D IOS. Unlike traditional two-stage methods that\nsegment teeth before detecting landmarks, CHaRNet directly detects landmarks on\nthe input point cloud. It consists of four key modules: (1) a point cloud\nencoder, (2) a point cloud decoder with a heatmap regression head, (3) a teeth\npresence classification head, and (4) the innovative Conditioned Heatmap\nRegression (CHaR) module. The CHaR module refines landmark regression by\nleveraging teeth presence classification, enabling dynamic adaptation to cases\nwith missing teeth and improving accuracy in complex dental models. We evaluate\nCHaRNet using five point cloud learning algorithms to validate the\neffectiveness of the CHaR module and test it on a clinical dataset of 1,214\nannotated 3D dental models. Both the dataset and code will be publicly released\nto address the lack of open datasets in orthodontics, promote benchmarking, and\ninspire new research. CHaRNet achieves a Mean Euclidean Distance Error (MEDE)\nof 1.28 mm and a Mean Success Ratio (MSR) of 82.40%, demonstrating robust\nperformance. Notably, it excels in handling irregular dental geometries, such\nas models with missing teeth. This end-to-end approach streamlines orthodontic\nworkflows, improves 3D IOS analysis precision, and facilitates efficient\ncomputer-assisted treatment planning."
    },
    {
        "date": "2025-01",
        "title": "Robust Body Composition Analysis by Generating 3D CT Volumes from Limited 2D Slices",
        "author": "Lianrui Zuo, Xin Yu, Dingjie Su, Kaiwen Xu, Aravind R. Krishnan, Yihao Liu, Shunxing Bao, Fabien Maldonado, Luigi Ferrucci, and Bennett A. Landman",
        "link": "http://arxiv.org/abs/2501.13071v1",
        "abstract": "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%."
    },
    {
        "date": "2025-01",
        "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study of a UK Degree Program",
        "author": "Carlton Shepherd",
        "link": "http://arxiv.org/abs/2501.12883v3",
        "abstract": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
    },
    {
        "date": "2025-01",
        "title": "Intelligent Attacks on Cyber-Physical Systems and Critical Infrastructures",
        "author": "Alan Oliveira de S\u00e1, Charles Bezerra Prado, Mariana Luiza Flavio, and Luiz F. Rust da C. Carmo",
        "link": "http://arxiv.org/abs/2501.12762v1",
        "abstract": "This chapter provides an overview of the evolving landscape of attacks in\ncyber-physical systems (CPS) and critical infrastructures, highlighting the\npossible use of Artificial Intelligence (AI) algorithms to develop intelligent\ncyberattacks. It describes various existing methods used to carry out\nintelligent attacks in Operational Technology (OT) environments and discusses\nAI-driven tools that automate penetration tests in Information Technology (IT)\nsystems, which could potentially be used as attack tools. The chapter also\ndiscusses mitigation strategies to counter these emerging intelligent attacks\nby hindering the learning process of AI-based attacks and points to future\nresearch directions on the matter."
    },
    {
        "date": "2025-01",
        "title": "Modality Unified Attack for Omni-Modality Person Re-Identification",
        "author": "Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yunfeng Ma, and Yaonan Wang",
        "link": "http://arxiv.org/abs/2501.12761v1",
        "abstract": "Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively."
    },
    {
        "date": "2025-01",
        "title": "Bad-PFL: Exploring Backdoor Attacks against Personalized Federated Learning",
        "author": "Mingyuan Fan, Zhanyi Hu, Fuyi Wang, and Cen Chen",
        "link": "http://arxiv.org/abs/2501.12736v1",
        "abstract": "Data heterogeneity and backdoor attacks rank among the most significant\nchallenges facing federated learning (FL). For data heterogeneity, personalized\nfederated learning (PFL) enables each client to maintain a private personalized\nmodel to cater to client-specific knowledge. Meanwhile, vanilla FL has proven\nvulnerable to backdoor attacks. However, recent advancements in PFL community\nhave demonstrated a potential immunity against such attacks. This paper\nexplores this intersection further, revealing that existing federated backdoor\nattacks fail in PFL because backdoors about manually designed triggers struggle\nto survive in personalized models. To tackle this, we design Bad-PFL, which\nemploys features from natural data as our trigger. As long as the model is\ntrained on natural data, it inevitably embeds the backdoor associated with our\ntrigger, ensuring its longevity in personalized models. Moreover, our trigger\nundergoes mutual reinforcement training with the model, further solidifying the\nbackdoor's durability and enhancing attack effectiveness. The large-scale\nexperiments across three benchmark datasets demonstrate the superior\nperformance of our attack against various PFL methods, even when equipped with\nstate-of-the-art defense mechanisms."
    },
    {
        "date": "2025-01",
        "title": "FedDAG: Federated Domain Adversarial Generation Towards Generalizable Medical Image Analysis",
        "author": "Haoxuan Che, Yifei Wu, Haibo Jin, Yong Xia, and Hao Chen",
        "link": "http://arxiv.org/abs/2501.13967v2",
        "abstract": "Federated domain generalization aims to train a global model from multiple\nsource domains and ensure its generalization ability to unseen target domains.\nDue to the target domain being with unknown domain shifts, attempting to\napproximate these gaps by source domains may be the key to improving model\ngeneralization capability. Existing works mainly focus on sharing and\nrecombining local domain-specific attributes to increase data diversity and\nsimulate potential domain shifts. However, these methods may be insufficient\nsince only the local attribute recombination can be hard to touch the\nout-of-distribution of global data. In this paper, we propose a\nsimple-yet-efficient framework named Federated Domain Adversarial Generation\n(FedDAG). It aims to simulate the domain shift and improve the model\ngeneralization by adversarially generating novel domains different from local\nand global source domains. Specifically, it generates novel-style images by\nmaximizing the instance-level feature discrepancy between original and\ngenerated images and trains a generalizable task model by minimizing their\nfeature discrepancy. Further, we observed that FedDAG could cause different\nperformance improvements for local models. It may be due to inherent data\nisolation and heterogeneity among clients, exacerbating the imbalance in their\ngeneralization contributions to the global model. Ignoring this imbalance can\nlead the global model's generalization ability to be sub-optimal, further\nlimiting the novel domain generation procedure. Thus, to mitigate this\nimbalance, FedDAG hierarchically aggregates local models at the within-client\nand across-client levels by using the sharpness concept to evaluate client\nmodel generalization contributions. Extensive experiments across four medical\nbenchmarks demonstrate FedDAG's ability to enhance generalization in federated\nmedical scenarios."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust Multi-tab Website Fingerprinting",
        "author": "Xinhao Deng, Xiyuan Zhao, Qilei Yin, Zhuotao Liu, Qi Li, Mingwei Xu, Ke Xu, and Jianping Wu",
        "link": "http://arxiv.org/abs/2501.12622v1",
        "abstract": "Website fingerprinting enables an eavesdropper to determine which websites a\nuser is visiting over an encrypted connection. State-of-the-art website\nfingerprinting (WF) attacks have demonstrated effectiveness even against\nTor-protected network traffic. However, existing WF attacks have critical\nlimitations on accurately identifying websites in multi-tab browsing sessions,\nwhere the holistic pattern of individual websites is no longer preserved, and\nthe number of tabs opened by a client is unknown a priori. In this paper, we\npropose ARES, a novel WF framework natively designed for multi-tab WF attacks.\nARES formulates the multi-tab attack as a multi-label classification problem\nand solves it using the novel Transformer-based models. Specifically, ARES\nextracts local patterns based on multi-level traffic aggregation features and\nutilizes the improved self-attention mechanism to analyze the correlations\nbetween these local patterns, effectively identifying websites. We implement a\nprototype of ARES and extensively evaluate its effectiveness using our\nlarge-scale datasets collected over multiple months. The experimental results\nillustrate that ARES achieves optimal performance in several realistic\nscenarios. Further, ARES remains robust even against various WF defenses."
    },
    {
        "date": "2025-01",
        "title": "Robustness of Selected Learning Models under Label-Flipping Attack",
        "author": "Sarvagya Bhargava, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.12516v1",
        "abstract": "In this paper we compare traditional machine learning and deep learning\nmodels trained on a malware dataset when subjected to adversarial attack based\non label-flipping. Specifically, we investigate the robustness of Support\nVector Machines (SVM), Random Forest, Gaussian Naive Bayes (GNB), Gradient\nBoosting Machine (GBM), LightGBM, XGBoost, Multilayer Perceptron (MLP),\nConvolutional Neural Network (CNN), MobileNet, and DenseNet models when facing\nvarying percentages of misleading labels. We empirically assess the the\naccuracy of each of these models under such an adversarial attack on the\ntraining data. This research aims to provide insights into which models are\ninherently more robust, in the sense of being better able to resist intentional\ndisruptions to the training data. We find wide variation in the robustness of\nthe models tested to adversarial attack, with our MLP model achieving the best\ncombination of initial accuracy and robustness."
    },
    {
        "date": "2025-01",
        "title": "Adaptive Cyber-Attack Detection in IIoT Using Attention-Based LSTM-CNN Models",
        "author": "Afrah Gueriani, Hamza Kheddar, and Ahmed Cherif Mazari",
        "link": "http://arxiv.org/abs/2501.13962v1",
        "abstract": "The rapid expansion of the industrial Internet of things (IIoT) has\nintroduced new challenges in securing critical infrastructures against\nsophisticated cyberthreats. This study presents the development and evaluation\nof an advanced Intrusion detection (IDS) based on a hybrid LSTM-convolution\nneural network (CNN)-Attention architecture, specifically designed to detect\nand classify cyberattacks in IIoT environments. The research focuses on two key\nclassification tasks: binary and multi-class classification. The proposed\nmodels was rigorously tested using the Edge-IIoTset dataset. To mitigate the\nclass imbalance in the dataset, the synthetic minority over-sampling technique\n(SMOTE) was employed to generate synthetic samples for the underrepresented\nclasses. This ensured that the model could learn effectively from all classes,\nthereby improving the overall classification performance. Through systematic\nexperimentation, various deep learning (DL) models were compared, ultimately\ndemonstrating that the LSTM-CNN-Attention model consistently outperformed\nothers across key performance metrics. In binary classification, the model\nachieved near-perfect accuracy, while in multi-class classification, it\nmaintained a high accuracy level (99.04%), effectively categorizing different\nattack types with a loss value of 0.0220%."
    },
    {
        "date": "2025-01",
        "title": "A Fast, Scalable, and Robust Deep Learning-based Iterative Reconstruction Framework for Accelerated Industrial Cone-beam X-ray Computed Tomography",
        "author": "Aniket Pramanik, Obaidullah Rahman, Singanallur V. Venkatakrishnan, and Amirkoushyar Ziabari",
        "link": "http://arxiv.org/abs/2501.13961v1",
        "abstract": "Cone-beam X-ray Computed Tomography (XCT) with large detectors and\ncorresponding large-scale 3D reconstruction plays a pivotal role in\nmicron-scale characterization of materials and parts across various industries.\nIn this work, we present a novel deep neural network-based iterative algorithm\nthat integrates an artifact reduction-trained CNN as a prior model with\nautomated regularization parameter selection, tailored for large-scale\nindustrial cone-beam XCT data. Our method achieves high-quality 3D\nreconstructions even for extremely dense thick metal parts - which\ntraditionally pose challenges to industrial CT images - in just a few\niterations. Furthermore, we show the generalizability of our approach to\nout-of-distribution scans obtained under diverse scanning conditions. Our\nmethod effectively handles significant noise and streak artifacts, surpassing\nstate-of-the-art supervised learning methods trained on the same data."
    },
    {
        "date": "2025-01",
        "title": "Cinepro: Robust Training of Foundation Models for Cancer Detection in Prostate Ultrasound Cineloops",
        "author": "Mohamed Harmanani, Amoon Jamzad, Minh Nguyen Nhat To, Paul F. R. Wilson, Zhuoxin Guo, Fahimeh Fooladgar, Samira Sojoudi, Mahdi Gilany, Silvia Chang, Peter Black, Michael Leveridge, Robert Siemens, Purang Abolmaesumi, and Parvin Mousavi",
        "link": "http://arxiv.org/abs/2501.12331v1",
        "abstract": "Prostate cancer (PCa) detection using deep learning (DL) models has shown\npotential for enhancing real-time guidance during biopsies. However, prostate\nultrasound images lack pixel-level cancer annotations, introducing label noise.\nCurrent approaches often focus on limited regions of interest (ROIs),\ndisregarding anatomical context necessary for accurate diagnosis. Foundation\nmodels can overcome this limitation by analyzing entire images to capture\nglobal spatial relationships; however, they still encounter challenges stemming\nfrom the weak labels associated with coarse pathology annotations in ultrasound\ndata. We introduce Cinepro, a novel framework that strengthens foundation\nmodels' ability to localize PCa in ultrasound cineloops. Cinepro adapts robust\ntraining by integrating the proportion of cancer tissue reported by pathology\nin a biopsy core into its loss function to address label noise, providing a\nmore nuanced supervision. Additionally, it leverages temporal data across\nmultiple frames to apply robust augmentations, enhancing the model's ability to\nlearn stable cancer-related features. Cinepro demonstrates superior performance\non a multi-center prostate ultrasound dataset, achieving an AUROC of 77.1% and\na balanced accuracy of 83.8%, surpassing current benchmarks. These findings\nunderscore Cinepro's promise in advancing foundation models for weakly labeled\nultrasound data."
    },
    {
        "date": "2025-01",
        "title": "With Great Backbones Comes Great Adversarial Transferability",
        "author": "Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, and Aram H. Markosyan",
        "link": "http://arxiv.org/abs/2501.12275v1",
        "abstract": "Advances in self-supervised learning (SSL) for machine vision have improved\nrepresentation robustness and model performance, giving rise to pre-trained\nbackbones like \\emph{ResNet} and \\emph{ViT} models tuned with SSL methods such\nas \\emph{SimCLR}. Due to the computational and data demands of pre-training,\nthe utilization of such backbones becomes a strenuous necessity. However,\nemploying these backbones may inherit vulnerabilities to adversarial attacks.\nWhile adversarial robustness has been studied under \\emph{white-box} and\n\\emph{black-box} settings, the robustness of models tuned on pre-trained\nbackbones remains largely unexplored. Additionally, the role of tuning\nmeta-information in mitigating exploitation risks is unclear. This work\nsystematically evaluates the adversarial robustness of such models across\n$20,000$ combinations of tuning meta-information, including fine-tuning\ntechniques, backbone families, datasets, and attack types. We propose using\nproxy models to transfer attacks, simulating varying levels of target knowledge\nby fine-tuning these proxies with diverse configurations. Our findings reveal\nthat proxy-based attacks approach the effectiveness of \\emph{white-box}\nmethods, even with minimal tuning knowledge. We also introduce a naive\n\"backbone attack,\" leveraging only the backbone to generate adversarial\nsamples, which outperforms \\emph{black-box} attacks and rivals \\emph{white-box}\nmethods, highlighting critical risks in model-sharing practices. Finally, our\nablations reveal how increasing tuning meta-information impacts attack\ntransferability, measuring each meta-information combination."
    },
    {
        "date": "2025-01",
        "title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework",
        "author": "Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, and Libing Wu",
        "link": "http://arxiv.org/abs/2501.12263v1",
        "abstract": "Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component."
    },
    {
        "date": "2025-01",
        "title": "Empower Healthcare through a Self-Sovereign Identity Infrastructure for Secure Electronic Health Data Access",
        "author": "Antonio L\u00f3pez Mart\u00ednez, Montassar Naghmouchi, Maryline Laurent, Joaquin Garcia-Alfaro, Manuel Gil P\u00e9rez, Antonio Ruiz Mart\u00ednez, and Pantaleone Nespoli",
        "link": "http://arxiv.org/abs/2501.12229v1",
        "abstract": "Health data is one of the most sensitive data for people, which attracts the\nattention of malicious activities. We propose an open-source health data\nmanagement framework, that follows a patient-centric approach. The proposed\nframework implements the Self-Sovereign Identity paradigm with innovative\ntechnologies such as Decentralized Identifiers and Verifiable Credentials. The\nframework uses Blockchain technology to provide immutability, verifiable data\nregistry, and auditability, as well as an agent-based model to provide\nprotection and privacy for the patient data. We also define different use cases\nregarding the daily patient-practitioner-laboratory interactions and specific\nfunctions to cover patient data loss, data access revocation, and emergency\ncases where patients are unable to give consent and access to their data. To\naddress this design, a proof of concept is created with an interaction between\npatient and doctor. The most feasible technologies are selected and the created\ndesign is validated. We discuss the differences and novelties of this\nframework, which includes the patient-centric approach also for data storage,\nthe designed recovery and emergency plan, the defined backup procedure, and the\nselected blockchain platform."
    },
    {
        "date": "2025-01",
        "title": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense",
        "author": "Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, and Min Yang",
        "link": "http://arxiv.org/abs/2501.12210v1",
        "abstract": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs."
    },
    {
        "date": "2025-01",
        "title": "FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments",
        "author": "Mehdi Ben Ghali, Reda Bellafqira, and Gouenou Coatrieux",
        "link": "http://arxiv.org/abs/2501.12123v1",
        "abstract": "Federated Learning (FL) enables clients to collaboratively train a global\nmodel using their local datasets while reinforcing data privacy. However, FL is\nsusceptible to poisoning attacks. Existing defense mechanisms assume that\nclients' data are independent and identically distributed (IID), making them\nineffective in real-world applications where data are non-IID. This paper\npresents FedCLEAN, the first defense capable of filtering attackers' model\nupdates in a non-IID FL environment. The originality of FedCLEAN is twofold.\nFirst, it relies on a client confidence score derived from the reconstruction\nerrors of each client's model activation maps for a given trigger set, with\nreconstruction errors obtained by means of a Conditional Variational\nAutoencoder trained according to a novel server-side strategy. Second, we\npropose an ad-hoc trust propagation algorithm based on client scores, which\nallows building a cluster of benign clients while flagging potential attackers.\nExperimental results on the datasets MNIST and FashionMNIST demonstrate the\nrobustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a\nclose-to-zero benign client misclassification rate, even in the absence of an\nattack."
    },
    {
        "date": "2025-01",
        "title": "Application of Machine Learning Techniques for Secure Traffic in NoC-based Manycores",
        "author": "Geaninne Lopes, C\u00e9sar Marcon, and Fernando Moraes",
        "link": "http://arxiv.org/abs/2501.12034v1",
        "abstract": "Like most computer systems, a manycore can also be the target of security\nattacks. It is essential to ensure the security of the NoC since all\ninformation travels through its channels, and any interference in the traffic\nof messages can reflect on the entire chip, causing communication problems.\nAmong the possible attacks on NoC, Denial of Service (DoS) attacks are the most\ncited in the literature. The state of the art shows a lack of work that can\ndetect such attacks through learning techniques. On the other hand, these\ntechniques are widely explored in computer network security via an Intrusion\nDetection System (IDS). In this context, the main goal of this document is to\npresent the progress of a work that explores an IDS technique using machine\nlearning and temporal series for detecting DoS attacks in NoC-based manycore\nsystems. To fulfill this goal, it is necessary to extract traffic data from a\nmanycore NoC and execute the learning techniques in the extracted data.\nHowever, while low-level platforms offer precision and slow execution,\nhigh-level platforms offer higher speed and data incompatible with reality.\nTherefore, a platform is being developed using the OVP tool, which has a higher\nlevel of abstraction. To solve the low precision problem, the developed\nplatform will have its data validated with a low-level platform."
    },
    {
        "date": "2025-01",
        "title": "Ratio Attack on G+G Convoluted Gaussian Signature",
        "author": "Chik How Tan, Theo Fanuela Prabowo, and Wei Guo Foo",
        "link": "http://arxiv.org/abs/2501.12009v1",
        "abstract": "A lattice-based signature, called G+G convoluted Gaussian signature was\nproposed in ASIACRYPT 2023 and was proved secure in the quantum random oracle\nmodel. In this paper, we propose a ratio attack on the G+G convoluted Gaussian\nsignature to recover the secret key. The attack exploits the fact, proved in\nthis paper, that the secret key can be obtained from the expected value of the\nratio of signatures which follows a truncated Cauchy distribution. Moreover, we\nalso compute the number of signatures required to successfully recover the\nsecret key. Furthermore, we simulate the ratio attack in Sagemath with a few\ndifferent parameters as a proof-of-concept of the ratio attack."
    },
    {
        "date": "2025-01",
        "title": "BRC20 Snipping Attack",
        "author": "Minfeng Qi, Qin Wang, Ningran Li, Shiping Chen, and Tianqing Zhu",
        "link": "http://arxiv.org/abs/2501.11942v1",
        "abstract": "In this paper, we introduce and implement BRC20 sniping attack. Our attack\nmanipulates the BRC20 token transfers in open markets and disrupts the fairness\namong bidding participants. The long-standing principle of ``highest bidder\nwins'' is rendered ineffective.\n  Typically, open BRC20 token markets rely on Partially Signed Bitcoin\nTransactions (PSBT) to broadcast selling intents and wait for buying auctions.\nOur attack targets the BRC20 buying process (i.e., transfer) by injecting a\nfront-running transaction to complete the full signature of the PSBT. At its\ncore, the attack exploits the mempool's fee-based transaction selection\nmechanism to snipe the victim transaction, replicate metadata, and front-run\nthe legesmate transaction. This attack applies to platforms using PSBT for\nBRC20 token transfers, including popular Bitcoin exchanges and marketplaces\n(e.g., Magic Eden, Unisat, Gate.io, OKX).\n  We implemented and tested the attack on a Bitcoin testnet (regtest),\nvalidating its effectiveness through multiple experimental rounds. Results show\nthat the attacker consistently replaces legitimate transactions by submitting\nhigher-fee PSBTs. We have also made responsible disclosures to the mentioned\nexchanges."
    },
    {
        "date": "2025-01",
        "title": "LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts",
        "author": "Md Kamrujjaman Mobin, and Md Saiful Islam",
        "link": "http://arxiv.org/abs/2501.11914v1",
        "abstract": "This paper presents a system developed for Task 1 of the COLING 2025 Workshop\non Detecting AI-Generated Content, focusing on the binary classification of\nmachine-generated versus human-written text. Our approach utilizes an ensemble\nof models, with weights assigned according to each model's inverse perplexity,\nto enhance classification accuracy. For the English text detection task, we\ncombined RoBERTa-base, RoBERTa-base with the OpenAI detector, and\nBERT-base-cased, achieving a Macro F1-score of 0.7458, which ranked us 12th out\nof 35 teams. We ensembled RemBERT, XLM-RoBERTa-base, and\nBERT-base-multilingual-case for the multilingual text detection task, employing\nthe same inverse perplexity weighting technique. This resulted in a Macro\nF1-score of 0.7513, positioning us 4th out of 25 teams. Our results demonstrate\nthe effectiveness of inverse perplexity weighting in improving the robustness\nof machine-generated text detection across both monolingual and multilingual\nsettings, highlighting the potential of ensemble methods for this challenging\ntask."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Adversarial Transferability via Component-Wise Augmentation Method",
        "author": "Hangyu Liu, Bo Peng, Pengxiang Ding, and Donglin Wang",
        "link": "http://arxiv.org/abs/2501.11901v1",
        "abstract": "Deep Neural Networks (DNNs) are highly vulnerable to adversarial examples,\nwhich pose significant challenges in security-sensitive applications. Among\nvarious adversarial attack strategies, input transformation-based attacks have\ndemonstrated remarkable effectiveness in enhancing adversarial transferability.\nHowever, existing methods fail to diversify attention regions across models\nadequately and introduce excessive information loss during transformations. In\nthis paper, we introduce a novel input transformation-based method, termed\nComponent-Wise Augmentation (CWA), designed to enhance transferability by\nlocally applying block-wise transformations. CWA strategically integrates\ninterpolation and selective rotation on individual image blocks to diversify\nmodel attention regions while preserving semantic integrity. Extensive\nexperiments on the standard ImageNet dataset show that CWA consistently\noutperforms state-of-the-art methods in both attack success rates and stability\nacross CNN- and Transformer-based models, while also demonstrating superior\nperformance against multiple defense methods."
    },
    {
        "date": "2025-01",
        "title": "LASER: Lip Landmark Assisted Speaker Detection for Robustness",
        "author": "Le Thien Phuc Nguyen, Zhuoran Yu, and Yong Jae Lee",
        "link": "http://arxiv.org/abs/2501.11899v1",
        "abstract": "Active Speaker Detection (ASD) aims to identify speaking individuals in\ncomplex visual scenes. While humans can easily detect speech by matching lip\nmovements to audio, current ASD models struggle to establish this\ncorrespondence, often misclassifying non-speaking instances when audio and lip\nmovements are unsynchronized. To address this limitation, we propose Lip\nlandmark Assisted Speaker dEtection for Robustness (LASER). Unlike models that\nrely solely on facial frames, LASER explicitly focuses on lip movements by\nintegrating lip landmarks in training. Specifically, given a face track, LASER\nextracts frame-level visual features and the 2D coordinates of lip landmarks\nusing a lightweight detector. These coordinates are encoded into dense feature\nmaps, providing spatial and structural information on lip positions.\nRecognizing that landmark detectors may sometimes fail under challenging\nconditions (e.g., low resolution, occlusions, extreme angles), we incorporate\nan auxiliary consistency loss to align predictions from both lip-aware and\nface-only features, ensuring reliable performance even when lip data is absent.\nExtensive experiments across multiple datasets show that LASER outperforms\nstate-of-the-art models, especially in scenarios with desynchronized audio and\nvisuals, demonstrating robust performance in real-world video contexts. Code is\navailable at \\url{https://github.com/plnguyen2908/LASER_ASD}."
    },
    {
        "date": "2025-01",
        "title": "Cross-Entropy Attacks to Language Models via Rare Event Simulation",
        "author": "Mingze Ni, Yongshun Gong, and Wei Liu",
        "link": "http://arxiv.org/abs/2501.11852v1",
        "abstract": "Black-box textual adversarial attacks are challenging due to the lack of\nmodel information and the discrete, non-differentiable nature of text. Existing\nmethods often lack versatility for attacking different models, suffer from\nlimited attacking performance due to the inefficient optimization with word\nsaliency ranking, and frequently sacrifice semantic integrity to achieve better\nattack outcomes. This paper introduces a novel approach to textual adversarial\nattacks, which we call Cross-Entropy Attacks (CEA), that uses Cross-Entropy\noptimization to address the above issues. Our CEA approach defines adversarial\nobjectives for both soft-label and hard-label settings and employs CE\noptimization to identify optimal replacements. Through extensive experiments on\ndocument classification and language translation problems, we demonstrate that\nour attack method excels in terms of attacking performance, imperceptibility,\nand sentence quality."
    },
    {
        "date": "2025-01",
        "title": "FedMUA: Exploring the Vulnerabilities of Federated Learning to Malicious Unlearning Attacks",
        "author": "Jian Chen, Zehui Lin, Wanyu Lin, Wenlong Shi, Xiaoyan Yin, and Di Wang",
        "link": "http://arxiv.org/abs/2501.11848v1",
        "abstract": "Recently, the practical needs of ``the right to be forgotten'' in federated\nlearning gave birth to a paradigm known as federated unlearning, which enables\nthe server to forget personal data upon the client's removal request. Existing\nstudies on federated unlearning have primarily focused on efficiently\neliminating the influence of requested data from the client's model without\nretraining from scratch, however, they have rarely doubted the reliability of\nthe global model posed by the discrepancy between its prediction performance\nbefore and after unlearning. To bridge this gap, we take the first step by\nintroducing a novel malicious unlearning attack dubbed FedMUA, aiming to unveil\npotential vulnerabilities emerging from federated learning during the\nunlearning process. The crux of FedMUA is to mislead the global model into\nunlearning more information associated with the influential samples for the\ntarget sample than anticipated, thus inducing adverse effects on target samples\nfrom other clients. To achieve this, we design a novel two-step method, known\nas Influential Sample Identification and Malicious Unlearning Generation, to\nidentify and subsequently generate malicious feature unlearning requests within\nthe influential samples. By doing so, we can significantly alter the\npredictions pertaining to the target sample by initiating the malicious feature\nunlearning requests, leading to the deliberate manipulation for the user\nadversely. Additionally, we design a new defense mechanism that is highly\nresilient against malicious unlearning attacks. Extensive experiments on three\nrealistic datasets reveal that FedMUA effectively induces misclassification on\ntarget samples and can achieve an 80% attack success rate by triggering only\n0.3% malicious unlearning requests."
    },
    {
        "date": "2025-01",
        "title": "CogMorph: Cognitive Morphing Attacks for Text-to-Image Models",
        "author": "Zonglei Jing, Zonghao Ying, Le Wang, Siyuan Liang, Aishan Liu, Xianglong Liu, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.11815v2",
        "abstract": "The development of text-to-image (T2I) generative models, that enable the\ncreation of high-quality synthetic images from textual prompts, has opened new\nfrontiers in creative design and content generation. However, this paper\nreveals a significant and previously unrecognized ethical risk inherent in this\ntechnology and introduces a novel method, termed the Cognitive Morphing Attack\n(CogMorph), which manipulates T2I models to generate images that retain the\noriginal core subjects but embeds toxic or harmful contextual elements. This\nnuanced manipulation exploits the cognitive principle that human perception of\nconcepts is shaped by the entire visual scene and its context, producing images\nthat amplify emotional harm far beyond attacks that merely preserve the\noriginal semantics. To address this, we first construct an imagery toxicity\ntaxonomy spanning 10 major and 48 sub-categories, aligned with human\ncognitive-perceptual dimensions, and further build a toxicity risk matrix\nresulting in 1,176 high-quality T2I toxic prompts. Based on this, our CogMorph\nfirst introduces Cognitive Toxicity Augmentation, which develops a cognitive\ntoxicity knowledge base with rich external toxic representations for humans\n(e.g., fine-grained visual features) that can be utilized to further guide the\noptimization of adversarial prompts. In addition, we present Contextual\nHierarchical Morphing, which hierarchically extracts critical parts of the\noriginal prompt (e.g., scenes, subjects, and body parts), and then iteratively\nretrieves and fuses toxic features to inject harmful contexts. Extensive\nexperiments on multiple open-sourced T2I models and black-box commercial APIs\n(e.g., DALLE-3) demonstrate the efficacy of CogMorph which significantly\noutperforms other baselines by large margins (+20.62% on average)."
    },
    {
        "date": "2025-01",
        "title": "Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense",
        "author": "Yaser Baseri, Abdelhakim Hafid, Yahya Shahsavari, Dimitrios Makrakis, and Hassan Khodaiemehr",
        "link": "http://arxiv.org/abs/2501.11798v1",
        "abstract": "The emergence of quantum computing presents a formidable challenge to the\nsecurity of blockchain systems. Traditional cryptographic algorithms,\nfoundational to digital signatures, message encryption, and hashing functions,\nbecome vulnerable to the immense computational power of quantum computers. This\npaper conducts a thorough risk assessment of transitioning to quantum-resistant\nblockchains, comprehensively analyzing potential threats targeting vital\nblockchain components: the network, mining pools, transaction verification\nmechanisms, smart contracts, and user wallets. By elucidating the intricate\nchallenges and strategic considerations inherent in transitioning to\nquantum-resistant algorithms, the paper evaluates risks and highlights\nobstacles in securing blockchain components with quantum-resistant\ncryptography. It offers a hybrid migration strategy to facilitate a smooth\ntransition from classical to quantum-resistant cryptography. The analysis\nextends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin,\nand Zcash, assessing vulnerable components, potential impacts, and associated\nSTRIDE threats, thereby identifying areas susceptible to quantum attacks.\nBeyond analysis, the paper provides actionable guidance for designing secure\nand resilient blockchain ecosystems in the quantum computing era. Recognizing\nthe looming threat of quantum computers, this research advocates for a\nproactive transition to quantum-resistant blockchain networks. It proposes a\ntailored security blueprint that strategically fortifies each component against\nthe evolving landscape of quantum-induced cyber threats. Emphasizing the\ncritical need for blockchain stakeholders to adopt proactive measures and\nimplement quantum-resistant solutions, the paper underscores the importance of\nembracing these insights to navigate the complexities of the quantum era with\nresilience and confidence."
    },
    {
        "date": "2025-01",
        "title": "Provably effective detection of effective data poisoning attacks",
        "author": "Jonathan Gallagher, Yasaman Esfandiari, Callen MacPhee, and Michael Warren",
        "link": "http://arxiv.org/abs/2501.11795v1",
        "abstract": "This paper establishes a mathematically precise definition of dataset\npoisoning attack and proves that the very act of effectively poisoning a\ndataset ensures that the attack can be effectively detected. On top of a\nmathematical guarantee that dataset poisoning is identifiable by a new\nstatistical test that we call the Conformal Separability Test, we provide\nexperimental evidence that we can adequately detect poisoning attempts in the\nreal world."
    },
    {
        "date": "2025-01",
        "title": "Disentangling stellar atmospheric parameters in astronomical spectra using Generative Adversarial Neural Networks",
        "author": "Minia Manteiga, Ra\u00fal Santove\u00f1a, Marco A. \u00c1lvarez, Carlos Dafonte, Manuel G. Penedo, Silvana Navarro, and Luis Corral",
        "link": "http://arxiv.org/abs/2501.11762v1",
        "abstract": "A method based on Generative Adversaria! Networks (GANs) is developed for\ndisentangling the physical (effective temperature and gravity) and chemical\n(metallicity, overabundance of a-elements with respect to iron) atmospheric\nproperties in astronomical spectra. Using a projection of the stellar spectra,\ncommonly called latent space, in which the contribution dueto one or several\nmain stellar physicochemical properties is minimised while others are enhanced,\nit was possible to maximise the information related to certain properties,\nwhich can then be extracted using artificial neural networks (ANN) as\nregressors with higher accuracy than a reference method based on the use of ANN\ntrained with the original spectra. Methods. Our model utilises autoencoders,\ncomprising two artificial neural networks: an encoder anda decoder which\ntransform input data into a low-dimensional representation known as latent\nspace. It also uses discriminators, which are additional neural networks aimed\nat transforming the traditional autoencoder training into an adversaria!\napproach, to disentangle or reinforce the astrophysical parameters from the\nlatent space. The GANDALF tool is described. It was developed to define, train,\nand test our GAN model with a web framework to show how the disentangling\nalgorithm works visually. It is open to the community in Github. Results. The\nperformance of our approach for retrieving atmospheric stellar properties from\nspectra is demonstrated using Gaia Radial Velocity Spectrograph (RVS) data from\nDR3. We use a data-driven perspective and obtain very competitive values, ali\nwithin the literature errors, and with the advantage of an important\ndimensionality reduction of the data to be processed."
    },
    {
        "date": "2025-01",
        "title": "Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI",
        "author": "Sathwik Narkedimilli, Sujith Makam, Amballa Venkata Sriram, Sai Prashanth Mallellu, MSVPJ Sathvik, and Ranga Rao Venkatesha Prasad",
        "link": "http://arxiv.org/abs/2501.11618v1",
        "abstract": "To address the critical need for secure IoT networks, this study presents a\nscalable and lightweight curriculum learning framework enhanced with\nExplainable AI (XAI) techniques, including LIME, to ensure transparency and\nadaptability. The proposed model employs novel neural network architecture\nutilized at every stage of Curriculum Learning to efficiently capture and focus\non both short- and long-term temporal dependencies, improve learning stability,\nand enhance accuracy while remaining lightweight and robust against noise in\nsequential IoT data. Robustness is achieved through staged learning, where the\nmodel iteratively refines itself by removing low-relevance features and\noptimizing performance. The workflow includes edge-optimized quantization and\npruning to ensure portability that could easily be deployed in the edge-IoT\ndevices. An ensemble model incorporating Random Forest, XGBoost, and the staged\nlearning base further enhances generalization. Experimental results demonstrate\n98% accuracy on CIC-IoV-2024 and CIC-APT-IIoT-2024 datasets and 97% on\nEDGE-IIoT, establishing this framework as a robust, transparent, and\nhigh-performance solution for IoT network security."
    },
    {
        "date": "2025-01",
        "title": "Rethinking Membership Inference Attacks Against Transfer Learning",
        "author": "Cong Wu, Jing Chen, Qianru Fang, Kun He, Ziming Zhao, Hao Ren, Guowen Xu, Yang Liu, and Yang Xiang",
        "link": "http://arxiv.org/abs/2501.11577v1",
        "abstract": "Transfer learning, successful in knowledge translation across related tasks,\nfaces a substantial privacy threat from membership inference attacks (MIAs).\nThese attacks, despite posing significant risk to ML model's training data,\nremain limited-explored in transfer learning. The interaction between teacher\nand student models in transfer learning has not been thoroughly explored in\nMIAs, potentially resulting in an under-examined aspect of privacy\nvulnerabilities within transfer learning. In this paper, we propose a new MIA\nvector against transfer learning, to determine whether a specific data point\nwas used to train the teacher model while only accessing the student model in a\nwhite-box setting. Our method delves into the intricate relationship between\nteacher and student models, analyzing the discrepancies in hidden layer\nrepresentations between the student model and its shadow counterpart. These\nidentified differences are then adeptly utilized to refine the shadow model's\ntraining process and to inform membership inference decisions effectively. Our\nmethod, evaluated across four datasets in diverse transfer learning tasks,\nreveals that even when an attacker only has access to the student model, the\nteacher model's training data remains susceptible to MIAs. We believe our work\nunveils the unexplored risk of membership inference in transfer learning."
    },
    {
        "date": "2025-01",
        "title": "Graph Defense Diffusion Model",
        "author": "Xin He, Wenqi Fan, Yili Wang, Chengyi Liu, Rui Miao, Xin Juan, and Xin Wang",
        "link": "http://arxiv.org/abs/2501.11568v1",
        "abstract": "Graph Neural Networks (GNNs) demonstrate significant potential in various\napplications but remain highly vulnerable to adversarial attacks, which can\ngreatly degrade their performance. Existing graph purification methods attempt\nto address this issue by filtering attacked graphs; however, they struggle to\neffectively defend against multiple types of adversarial attacks simultaneously\ndue to their limited flexibility, and they lack comprehensive modeling of graph\ndata due to their heavy reliance on heuristic prior knowledge. To overcome\nthese challenges, we propose a more versatile approach for defending against\nadversarial attacks on graphs. In this work, we introduce the Graph Defense\nDiffusion Model (GDDM), a flexible purification method that leverages the\ndenoising and modeling capabilities of diffusion models. The iterative nature\nof diffusion models aligns well with the stepwise process of adversarial\nattacks, making them particularly suitable for defense. By iteratively adding\nand removing noise, GDDM effectively purifies attacked graphs, restoring their\noriginal structure and features. Our GDDM consists of two key components: (1)\nGraph Structure-Driven Refiner, which preserves the basic fidelity of the graph\nduring the denoising process, and ensures that the generated graph remains\nconsistent with the original scope; and (2) Node Feature-Constrained\nRegularizer, which removes residual impurities from the denoised graph, further\nenhances the purification effect. Additionally, we design tailored denoising\nstrategies to handle different types of adversarial attacks, improving the\nmodel's adaptability to various attack scenarios. Extensive experiments\nconducted on three real-world datasets demonstrate that GDDM outperforms\nstate-of-the-art methods in defending against a wide range of adversarial\nattacks, showcasing its robustness and effectiveness."
    },
    {
        "date": "2025-01",
        "title": "Secure Resource Allocation via Constrained Deep Reinforcement Learning",
        "author": "Jianfei Sun, Qiang Gao, Cong Wu, Yuxian Li, Jiacheng Wang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.11557v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices and the advent of 6G\ntechnologies have introduced computationally intensive tasks that often surpass\nthe processing capabilities of user devices. Efficient and secure resource\nallocation in serverless multi-cloud edge computing environments is essential\nfor supporting these demands and advancing distributed computing. However,\nexisting solutions frequently struggle with the complexity of multi-cloud\ninfrastructures, robust security integration, and effective application of\ntraditional deep reinforcement learning (DRL) techniques under system\nconstraints. To address these challenges, we present SARMTO, a novel framework\nthat integrates an action-constrained DRL model. SARMTO dynamically balances\nresource allocation, task offloading, security, and performance by utilizing a\nMarkov decision process formulation, an adaptive security mechanism, and\nsophisticated optimization techniques. Extensive simulations across varying\nscenarios, including different task loads, data sizes, and MEC capacities, show\nthat SARMTO consistently outperforms five baseline approaches, achieving up to\na 40% reduction in system costs and a 41.5% improvement in energy efficiency\nover state-of-the-art methods. These enhancements highlight SARMTO's potential\nto revolutionize resource management in intricate distributed computing\nenvironments, opening the door to more efficient and secure IoT and edge\ncomputing applications."
    },
    {
        "date": "2025-01",
        "title": "An Exploratory Study on the Engineering of Security Features",
        "author": "Kevin Hermann, Sven Peldszus, Jan-Philipp Stegh\u00f6fer, and Thorsten Berger",
        "link": "http://arxiv.org/abs/2501.11546v1",
        "abstract": "Software security is of utmost importance for most software systems.\nDevelopers must systematically select, plan, design, implement, and especially\nmaintain and evolve security features -- functionalities to mitigate attacks or\nprotect personal data such as cryptography or access control, to ensure the\nsecurity of their software. While security features are usually available in\nlibraries, additional code needs to be written and maintained to integrate\nsecurity features and not all desired features can be reused this way. While\nthere have been studies on the use of such libraries, surprisingly little is\nknown about how developers engineer security features, how they select what\nsecurity features to implement, and the implications on maintenance. We\ntherefore currently rely on assumptions that are largely based on common sense\nor individual examples. However, researchers require hard empirical data to\nunderstand what practitioners need and how they view security, which we\ncurrently lack to provide them with effective solutions. We contribute an\nexploratory study with 26 knowledgeable industrial participants. We study how\nsecurity features of software systems are selected and engineered in practice,\nwhat their code-level characteristics are, and the challenges practitioners\nface. Based on the empirical data gathered, we validate four common assumptions\nand gain insights into engineering practices."
    },
    {
        "date": "2025-01",
        "title": "On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing",
        "author": "Tao Bai, Xingjian Tian, Yonghao Xu, and Bihan Wen",
        "link": "http://arxiv.org/abs/2501.11462v1",
        "abstract": "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks."
    },
    {
        "date": "2025-01",
        "title": "Nested Annealed Training Scheme for Generative Adversarial Networks",
        "author": "Chang Wan, Ming-Hsuan Yang, Minglu Li, Yunliang Jiang, and Zhonglong Zheng",
        "link": "http://arxiv.org/abs/2501.11318v1",
        "abstract": "Recently, researchers have proposed many deep generative models, including\ngenerative adversarial networks(GANs) and denoising diffusion models. Although\nsignificant breakthroughs have been made and empirical success has been\nachieved with the GAN, its mathematical underpinnings remain relatively\nunknown. This paper focuses on a rigorous mathematical theoretical framework:\nthe composite-functional-gradient GAN (CFG)[1]. Specifically, we reveal the\ntheoretical connection between the CFG model and score-based models. We find\nthat the training objective of the CFG discriminator is equivalent to finding\nan optimal D(x). The optimal gradient of D(x) differentiates the integral of\nthe differences between the score functions of real and synthesized samples.\nConversely, training the CFG generator involves finding an optimal G(x) that\nminimizes this difference. In this paper, we aim to derive an annealed weight\npreceding the weight of the CFG discriminator. This new explicit theoretical\nexplanation model is called the annealed CFG method. To overcome the limitation\nof the annealed CFG method, as the method is not readily applicable to the SOTA\nGAN model, we propose a nested annealed training scheme (NATS). This scheme\nkeeps the annealed weight from the CFG method and can be seamlessly adapted to\nvarious GAN models, no matter their structural, loss, or regularization\ndifferences. We conduct thorough experimental evaluations on various benchmark\ndatasets for image generation. The results show that our annealed CFG and NATS\nmethods significantly improve the quality and diversity of the synthesized\nsamples. This improvement is clear when comparing the CFG method and the SOTA\nGAN models."
    },
    {
        "date": "2025-01",
        "title": "PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues",
        "author": "Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, and Minglu Li",
        "link": "http://arxiv.org/abs/2501.11288v1",
        "abstract": "Multi-object tracking (MOT) is a rising topic in video processing\ntechnologies and has important application value in consumer electronics.\nCurrently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which\nperforms target detection and association frame by frame. However, the\nassociation performance of TBD methods degrades in complex scenes with heavy\nocclusions, which hinders the application of such methods in real-world\nscenarios.To this end, we incorporate pseudo-depth cues to enhance the\nassociation performance and propose Pseudo-Depth SORT (PD-SORT). First, we\nextend the Kalman filter state vector with pseudo-depth states. Second, we\nintroduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU\nwith pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement\n(QPDM) strategy for more robust data association. Besides, we also integrate\ncamera motion compensation (CMC) to handle dynamic camera situations. With the\nabove designs, PD-SORT significantly alleviates the occlusion-induced ambiguous\nassociations and achieves leading performances on DanceTrack, MOT17, and MOT20.\nNote that the improvement is especially obvious on DanceTrack, where objects\nshow complex motions, similar appearances, and frequent occlusions. The code is\navailable at https://github.com/Wangyc2000/PD_SORT."
    },
    {
        "date": "2025-01",
        "title": "Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare: Issues and Solutions",
        "author": "Zag ElSayed, Ahmed Abdelgawad, and Nelly Elsayed",
        "link": "http://arxiv.org/abs/2501.11250v1",
        "abstract": "Integrating Internet of Things (IoT) devices in healthcare has revolutionized\npatient care, offering improved monitoring, diagnostics, and treatment.\nHowever, the proliferation of these devices has also introduced significant\ncybersecurity challenges. This paper reviews the current landscape of\ncybersecurity threats targeting IoT devices in healthcare, discusses the\nunderlying issues contributing to these vulnerabilities, and explores potential\nsolutions. Additionally, this study offers solutions and suggestions for\nresearchers, agencies, and security specialists to overcome these IoT in\nhealthcare cybersecurity vulnerabilities. A comprehensive literature survey\nhighlights the nature and frequency of cyber attacks, their impact on\nhealthcare systems, and emerging strategies to mitigate these risks."
    },
    {
        "date": "2025-01",
        "title": "Conditional Feature Importance with Generative Modeling Using Adversarial Random Forests",
        "author": "Kristin Blesch, Niklas Koenen, Jan Kapar, Pegah Golchian, Lukas Burk, Markus Loecher, and Marvin N. Wright",
        "link": "http://arxiv.org/abs/2501.11178v1",
        "abstract": "This paper proposes a method for measuring conditional feature importance via\ngenerative modeling. In explainable artificial intelligence (XAI), conditional\nfeature importance assesses the impact of a feature on a prediction model's\nperformance given the information of other features. Model-agnostic post hoc\nmethods to do so typically evaluate changes in the predictive performance under\non-manifold feature value manipulations. Such procedures require creating\nfeature values that respect conditional feature distributions, which can be\nchallenging in practice. Recent advancements in generative modeling can\nfacilitate this. For tabular data, which may consist of both categorical and\ncontinuous features, the adversarial random forest (ARF) stands out as a\ngenerative model that can generate on-manifold data points without requiring\nintensive tuning efforts or computational resources, making it a promising\ncandidate model for subroutines in XAI methods. This paper proposes cARFi\n(conditional ARF feature importance), a method for measuring conditional\nfeature importance through feature values sampled from ARF-estimated\nconditional distributions. cARFi requires only little tuning to yield robust\nimportance scores that can flexibly adapt for conditional or marginal notions\nof feature importance, including straightforward extensions to condition on\nfeature subsets and allows for inferring the significance of feature\nimportances through statistical tests."
    },
    {
        "date": "2025-01",
        "title": "Counteracting temporal attacks in Video Copy Detection",
        "author": "Katarzyna Fojcik, and Piotr Syga",
        "link": "http://arxiv.org/abs/2501.11171v1",
        "abstract": "Video Copy Detection (VCD) plays a crucial role in copyright protection and\ncontent verification by identifying duplicates and near-duplicates in\nlarge-scale video databases. The META AI Challenge on video copy detection\nprovided a benchmark for evaluating state-of-the-art methods, with the\nDual-level detection approach emerging as a winning solution. This method\nintegrates Video Editing Detection and Frame Scene Detection to handle\nadversarial transformations and large datasets efficiently. However, our\nanalysis reveals significant limitations in the VED component, particularly in\nits ability to handle exact copies. Moreover, Dual-level detection shows\nvulnerability to temporal attacks. To address it, we propose an improved frame\nselection strategy based on local maxima of interframe differences, which\nenhances robustness against adversarial temporal modifications while\nsignificantly reducing computational overhead. Our method achieves an increase\nof 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to\nDual-level detection method, our approach maintains comparable micro-average\nprecision ($\\mu$AP) while also demonstrating improved robustness against\ntemporal attacks. Given 56\\% reduced representation size and the inference time\nof more than 2 times faster, our approach is more suitable to real-world\nresource restriction."
    },
    {
        "date": "2025-01",
        "title": "Federated Testing (FedTest): A New Scheme to Enhance Convergence and Mitigate Adversarial Attacks in Federating Learning",
        "author": "Mustafa Ghaleb, Mohanad Obeed, Muhamad Felemban, Anas Chaaban, and Halim Yanikomeroglu",
        "link": "http://arxiv.org/abs/2501.11167v1",
        "abstract": "Federated Learning (FL) has emerged as a significant paradigm for training\nmachine learning models. This is due to its data-privacy-preserving property\nand its efficient exploitation of distributed computational resources. This is\nachieved by conducting the training process in parallel at distributed users.\nHowever, traditional FL strategies grapple with difficulties in evaluating the\nquality of received models, handling unbalanced models, and reducing the impact\nof detrimental models. To resolve these problems, we introduce a novel\nfederated learning framework, which we call federated testing for federated\nlearning (FedTest). In the FedTest method, the local data of a specific user is\nused to train the model of that user and test the models of the other users.\nThis approach enables users to test each other's models and determine an\naccurate score for each. This score can then be used to aggregate the models\nefficiently and identify any malicious ones. Our numerical results reveal that\nthe proposed method not only accelerates convergence rates but also diminishes\nthe potential influence of malicious users. This significantly enhances the\noverall efficiency and robustness of FL systems."
    },
    {
        "date": "2025-01",
        "title": "A Novel Pearson Correlation-Based Merging Algorithm for Robust Distributed Machine Learning with Heterogeneous Data",
        "author": "Mohammad Ghabel Rahmat, and Majid Khalilian",
        "link": "http://arxiv.org/abs/2501.11112v2",
        "abstract": "Federated learning faces significant challenges in scenarios with\nheterogeneous data distributions and adverse network conditions, such as\ndelays, packet loss, and data poisoning attacks. This paper proposes a novel\nmethod based on the SCAFFOLD algorithm to improve the quality of local updates\nand enhance the robustness of the global model. The key idea is to form\nintermediary nodes by merging local models with high similarity, using the\nPearson correlation coefficient as a similarity measure. The proposed merging\nalgorithm reduces the number of local nodes while maintaining the accuracy of\nthe global model, effectively addressing communication overhead and bandwidth\nconsumption. Experimental results on the MNIST dataset under simulated\nfederated learning scenarios demonstrate the method's effectiveness. After 10\nrounds of training using a CNN model, the proposed approach achieved accuracies\nof 0.82, 0.73, and 0.66 under normal conditions, packet loss and data poisoning\nattacks, respectively, outperforming the baseline SCAFFOLD algorithm. These\nresults highlight the potential of the proposed method to improve efficiency\nand resilience in federated learning systems."
    },
    {
        "date": "2025-01",
        "title": "Enhancing Sample Utilization in Noise-Robust Deep Metric Learning With Subgroup-Based Positive-Pair Selection",
        "author": "Zhipeng Yu, Qianqian Xu, Yangbangyan Jiang, Yingfei Sun, and Qingming Huang",
        "link": "http://arxiv.org/abs/2501.11063v1",
        "abstract": "The existence of noisy labels in real-world data negatively impacts the\nperformance of deep learning models. Although much research effort has been\ndevoted to improving the robustness towards noisy labels in classification\ntasks, the problem of noisy labels in deep metric learning (DML) remains\nunder-explored. Existing noisy label learning methods designed for DML mainly\ndiscard suspicious noisy samples, resulting in a waste of the training data. To\naddress this issue, we propose a noise-robust DML framework with SubGroup-based\nPositive-pair Selection (SGPS), which constructs reliable positive pairs for\nnoisy samples to enhance the sample utilization. Specifically, SGPS first\neffectively identifies clean and noisy samples by a probability-based clean\nsample selectionstrategy. To further utilize the remaining noisy samples, we\ndiscover their potential similar samples based on the subgroup information\ngiven by a subgroup generation module and then aggregate them into informative\npositive prototypes for each noisy sample via a positive prototype generation\nmodule. Afterward, a new contrastive loss is tailored for the noisy samples\nwith their selected positive pairs. SGPS can be easily integrated into the\ntraining process of existing pair-wise DML tasks, like image retrieval and face\nrecognition. Extensive experiments on multiple synthetic and real-world\nlarge-scale label noise datasets demonstrate the effectiveness of our proposed\nmethod. Without any bells and whistles, our SGPS framework outperforms the\nstate-of-the-art noisy label DML methods. Code is available at\n\\url{https://github.com/smuelpeng/SGPS-NoiseFreeDML}."
    },
    {
        "date": "2025-01",
        "title": "Temporal Analysis of Adversarial Attacks in Federated Learning",
        "author": "Rohit Mapakshi, Sayma Akther, and Mark Stamp",
        "link": "http://arxiv.org/abs/2501.11054v1",
        "abstract": "In this paper, we experimentally analyze the robustness of selected Federated\nLearning (FL) systems in the presence of adversarial clients. We find that\ntemporal attacks significantly affect model performance in the FL models\ntested, especially when the adversaries are active throughout or during the\nlater rounds. We consider a variety of classic learning models, including\nMultinominal Logistic Regression (MLR), Random Forest, XGBoost, Support Vector\nClassifier (SVC), as well as various Neural Network models including Multilayer\nPerceptron (MLP), Convolution Neural Network (CNN), Recurrent Neural Network\n(RNN), and Long Short-Term Memory (LSTM). Our results highlight the\neffectiveness of temporal attacks and the need to develop strategies to make\nthe FL process more robust against such attacks. We also briefly consider the\neffectiveness of defense mechanisms, including outlier detection in the\naggregation algorithm."
    },
    {
        "date": "2025-01",
        "title": "Bridging the Security Gap: Lessons from 5G and What 6G Should Do Better",
        "author": "Isabella D. Lutz, and Matthew C. Valenti",
        "link": "http://arxiv.org/abs/2501.11045v1",
        "abstract": "The security requirements for future 6G mobile networks are anticipated to be\nsignificantly more complex and demanding than those of 5G. This increase stems\nfrom several factors: the proliferation of massive machine-type communications\nwill dramatically increase the density of devices competing for network access;\nsecure ultra-reliable low-latency communication will impose stringent\nrequirements on security, latency, and reliability; and the widespread\ndeployment of small cells and non-terrestrial networks, including satellite\nmega-constellations, will result in more frequent handovers. This paper\nprovides a set of security recommendations for 6G networks, with a particular\nfocus on access and handover procedures, which often lack encryption and\nintegrity protection, making them more vulnerable to exploitation. Since 6G is\nexpected to be a backward-compatible extension of 5G, and given that secure\nsystems cannot be effectively designed without a clear understanding of their\ngoals, it is imperative to first evaluate the limitations of the current\ngeneration. To this end, the paper begins by reviewing existing 5G access and\nauthentication mechanisms, highlighting several critical vulnerabilities in\nthese procedures. It then examines potential 6G challenges and concludes with\nactionable recommendations to enhance the security, resilience, and robustness\nof 6G access and handover mechanisms."
    },
    {
        "date": "2025-01",
        "title": "Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay",
        "author": "Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, and Xiangyang Ji",
        "link": "http://arxiv.org/abs/2501.11039v1",
        "abstract": "The foundation model enables fast problem-solving without learning from\nscratch, and such a desirable adaptation property benefits from its adopted\ncross-task generalization paradigms, e.g., pretraining, meta-training, or\nfinetuning. Recent trends have focused on the curation of task datasets during\noptimization, which includes task selection as an indispensable consideration\nfor either adaptation robustness or sampling efficiency purposes. Despite some\nprogress, selecting crucial task batches to optimize over iteration mostly\nexhausts massive task queries and requires intensive evaluation and\ncomputations to secure robust adaptation. This work underscores the criticality\nof both robustness and learning efficiency, especially in scenarios where tasks\nare risky to collect or costly to evaluate. To this end, we present Model\nPredictive Task Sampling (MPTS), a novel active task sampling framework to\nestablish connections between the task space and adaptation risk landscape\nachieve robust adaptation. Technically, MPTS characterizes the task episodic\ninformation with a generative model and predicts optimization outcome after\nadaptation from posterior inference, i.e., forecasting task-specific adaptation\nrisk values. The resulting risk learner amortizes expensive annotation,\nevaluation, or computation operations in task robust adaptation learning\nparadigms. Extensive experimental results show that MPTS can be seamlessly\nintegrated into zero-shot, few-shot, and many-shot learning paradigms,\nincreases adaptation robustness, and retains learning efficiency without\naffording extra cost. The code will be available at the project site\nhttps://github.com/thu-rllab/MPTS."
    },
    {
        "date": "2025-01",
        "title": "Effectiveness of Adversarial Benign and Malware Examples in Evasion and Poisoning Attacks",
        "author": "Matou\u0161 Koz\u00e1k, and Martin Jure\u010dek",
        "link": "http://arxiv.org/abs/2501.10996v1",
        "abstract": "Adversarial attacks present significant challenges for malware detection\nsystems. This research investigates the effectiveness of benign and malicious\nadversarial examples (AEs) in evasion and poisoning attacks on the Portable\nExecutable file domain. A novel focus of this study is on benign AEs, which,\nalthough not directly harmful, can increase false positives and undermine trust\nin antivirus solutions. We propose modifying existing adversarial malware\ngenerators to produce benign AEs and show they are as successful as malware AEs\nin evasion attacks. Furthermore, our data show that benign AEs have a more\ndecisive influence in poisoning attacks than standard malware AEs,\ndemonstrating their superior ability to decrease the model's performance. Our\nfindings introduce new opportunities for adversaries and further increase the\nattack surface that needs to be protected by security researchers."
    },
    {
        "date": "2025-01",
        "title": "GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models",
        "author": "Jiadong Lou, Xu Yuan, Rui Zhang, Xingliang Yuan, Neil Gong, and Nian-Feng Tzeng",
        "link": "http://arxiv.org/abs/2501.10985v1",
        "abstract": "Graph neural networks (GNNs) have exhibited superior performance in various\nclassification tasks on graph-structured data. However, they encounter the\npotential vulnerability from the link stealing attacks, which can infer the\npresence of a link between two nodes via measuring the similarity of its\nincident nodes' prediction vectors produced by a GNN model. Such attacks pose\nsevere security and privacy threats to the training graph used in GNN models.\nIn this work, we propose a novel solution, called Graph Link Disguise (GRID),\nto defend against link stealing attacks with the formal guarantee of GNN model\nutility for retaining prediction accuracy. The key idea of GRID is to add\ncarefully crafted noises to the nodes' prediction vectors for disguising\nadjacent nodes as n-hop indirect neighboring nodes. We take into account the\ngraph topology and select only a subset of nodes (called core nodes) covering\nall links for adding noises, which can avert the noises offset and have the\nfurther advantages of reducing both the distortion loss and the computation\ncost. Our crafted noises can ensure 1) the noisy prediction vectors of any two\nadjacent nodes have their similarity level like that of two non-adjacent nodes\nand 2) the model prediction is unchanged to ensure zero utility loss. Extensive\nexperiments on five datasets are conducted to show the effectiveness of our\nproposed GRID solution against different representative link-stealing attacks\nunder transductive settings and inductive settings respectively, as well as two\ninfluence-based attacks. Meanwhile, it achieves a much better privacy-utility\ntrade-off than existing methods when extended to GNNs."
    },
    {
        "date": "2025-01",
        "title": "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "author": "Zhe Zhou, Fei Tong, Hongyu Wang, Xiaoyu Cheng, Fang Jiang, Zhikun Zhang, and Yuxing Mao",
        "link": "http://arxiv.org/abs/2501.10983v1",
        "abstract": "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5."
    },
    {
        "date": "2025-01",
        "title": "TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval",
        "author": "Shuai Lyu, Zijing Tian, Zhonghong Ou, Yifan Zhu, Xiao Zhang, Qiankun Ha, Haoran Luo, and Meina Song",
        "link": "http://arxiv.org/abs/2501.10935v1",
        "abstract": "Cross-modal retrieval maps data under different modality via semantic\nrelevance. Existing approaches implicitly assume that data pairs are\nwell-aligned and ignore the widely existing annotation noise, i.e., noisy\ncorrespondence (NC). Consequently, it inevitably causes performance\ndegradation. Despite attempts that employ the co-teaching paradigm with\nidentical architectures to provide distinct data perspectives, the differences\nbetween these architectures are primarily stemmed from random initialization.\nThus, the model becomes increasingly homogeneous along with the training\nprocess. Consequently, the additional information brought by this paradigm is\nseverely limited. In order to resolve this problem, we introduce a Tripartite\nlearning with Semantic Variation Consistency (TSVC) for robust image-text\nretrieval. We design a tripartite cooperative learning mechanism comprising a\nCoordinator, a Master, and an Assistant model. The Coordinator distributes\ndata, and the Assistant model supports the Master model's noisy label\nprediction with diverse data. Moreover, we introduce a soft label estimation\nmethod based on mutual information variation, which quantifies the noise in new\nsamples and assigns corresponding soft labels. We also present a new loss\nfunction to enhance robustness and optimize training effectiveness. Extensive\nexperiments on three widely used datasets demonstrate that, even at increasing\nnoise ratios, TSVC exhibits significant advantages in retrieval accuracy and\nmaintains stable training performance."
    },
    {
        "date": "2025-01",
        "title": "LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice",
        "author": "M. Mikail Demir, Hakan T. Otal, and M. Abdullah Canbaz",
        "link": "http://arxiv.org/abs/2501.10915v1",
        "abstract": "Large Language Models (LLMs) hold promise for advancing legal practice by\nautomating complex tasks and improving access to justice. However, their\nadoption is limited by concerns over client confidentiality, especially when\nlawyers include sensitive Personally Identifiable Information (PII) in prompts,\nrisking unauthorized data exposure. To mitigate this, we introduce\nLegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers\nusing LLM-based tools. LegalGuardian employs Named Entity Recognition (NER)\ntechniques and local LLMs to mask and unmask confidential PII within prompts,\nsafeguarding sensitive data before any external interaction. We detail its\ndevelopment and assess its effectiveness using a synthetic prompt library in\nimmigration law scenarios. Comparing traditional NER models with one-shot\nprompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with\nGLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis\nconfirms that the framework maintains high fidelity in outputs, ensuring robust\nutility of LLM-based tools. Our findings indicate that legal professionals can\nharness advanced AI technologies without compromising client confidentiality or\nthe quality of legal documents."
    },
    {
        "date": "2025-01",
        "title": "Explainable Adversarial Attacks on Coarse-to-Fine Classifiers",
        "author": "Akram Heidarizadeh, Connor Hatfield, Lorenzo Lazzarotto, HanQin Cai, and George Atia",
        "link": "http://arxiv.org/abs/2501.10906v1",
        "abstract": "Traditional adversarial attacks typically aim to alter the predicted labels\nof input images by generating perturbations that are imperceptible to the human\neye. However, these approaches often lack explainability. Moreover, most\nexisting work on adversarial attacks focuses on single-stage classifiers, but\nmulti-stage classifiers are largely unexplored. In this paper, we introduce\ninstance-based adversarial attacks for multi-stage classifiers, leveraging\nLayer-wise Relevance Propagation (LRP), which assigns relevance scores to\npixels based on their influence on classification outcomes. Our approach\ngenerates explainable adversarial perturbations by utilizing LRP to identify\nand target key features critical for both coarse and fine-grained\nclassifications. Unlike conventional attacks, our method not only induces\nmisclassification but also enhances the interpretability of the model's\nbehavior across classification stages, as demonstrated by experimental results."
    },
    {
        "date": "2025-01",
        "title": "A Generative Security Application Engineering Curriculum",
        "author": "Wu-chang Feng, and David Baker-Robinson",
        "link": "http://arxiv.org/abs/2501.10900v1",
        "abstract": "Generative AI and large language models (LLMs) are transforming security by\nautomating many tasks being performed manually. With such automation changing\nthe practice of security as we know it, it is imperative that we prepare future\nstudents for the technology landscape they will ultimately face. Towards this\nend, we describe an initial curriculum and course that attempts to show\nstudents how to apply generative AI in order to solve problems in security. By\nrefocusing security education and training on aspects uniquely suited for\nhumans and showing students how to leverage automation for the rest, we believe\nwe can better align security education practices with generative AI as it\nevolves."
    },
    {
        "date": "2025-01",
        "title": "Certifying Robustness via Topological Representations",
        "author": "Jens Agerberg, Andrea Guidolin, Andrea Martinelli, Pepijn Roos Hoefgeest, David Eklund, and Martina Scolamiero",
        "link": "http://arxiv.org/abs/2501.10876v1",
        "abstract": "We propose a neural network architecture that can learn discriminative\ngeometric representations of data from persistence diagrams, common descriptors\nof Topological Data Analysis. The learned representations enjoy Lipschitz\nstability with a controllable Lipschitz constant. In adversarial learning, this\nstability can be used to certify $\\epsilon$-robustness for samples in a\ndataset, which we demonstrate on the ORBIT5K dataset representing the orbits of\na discrete dynamical system."
    },
    {
        "date": "2025-01",
        "title": "Model-Robust and Adaptive-Optimal Transfer Learning for Tackling Concept Shifts in Nonparametric Regression",
        "author": "Haotian Lin, and Matthew Reimherr",
        "link": "http://arxiv.org/abs/2501.10870v1",
        "abstract": "When concept shifts and sample scarcity are present in the target domain of\ninterest, nonparametric regression learners often struggle to generalize\neffectively. The technique of transfer learning remedies these issues by\nleveraging data or pre-trained models from similar source domains. While\nexisting generalization analyses of kernel-based transfer learning typically\nrely on correctly specified models, we present a transfer learning procedure\nthat is robust against model misspecification while adaptively attaining\noptimality. To facilitate our analysis and avoid the risk of saturation found\nin classical misspecified results, we establish a novel result in the\nmisspecified single-task learning setting, showing that spectral algorithms\nwith fixed bandwidth Gaussian kernels can attain minimax convergence rates\ngiven the true function is in a Sobolev space, which may be of independent\ninterest. Building on this, we derive the adaptive convergence rates of the\nexcess risk for specifying Gaussian kernels in a prevalent class of hypothesis\ntransfer learning algorithms. Our results are minimax optimal up to logarithmic\nfactors and elucidate the key determinants of transfer efficiency."
    },
    {
        "date": "2025-01",
        "title": "A comprehensive survey on RPL routing-based attacks, defences and future directions in Internet of Things",
        "author": "Anil K Prajapati, Emmanuel S Pilli, Ramesh B Battula, Vijay Varadharajan, Abhishek Verma, and R C Joshi",
        "link": "http://arxiv.org/abs/2501.10817v1",
        "abstract": "The Internet of Things (IoT) is a network of digital devices like sensors,\nprocessors, embedded and communication devices that can connect to and exchange\ndata with other devices and systems over the internet. IoT devices have\nlimitations on power, memory, and computational resources. Researchers have\ndeveloped the IPv6 Over Low-power Wireless Personal Area Network (6LoWPAN)\nprotocols to provide wireless connectivity among these devices while overcoming\nthe constraints on resources. 6LoWPAN has been approved subsequently by the\nInternet Engineering Task Force (IETF). The IETF Routing Over Low-power and\nLossy Networks (ROLL) standardized the Routing Protocol for LLNs known as RPL\n(IETF RFC 6550), which is part of the 6LoWPAN stack. However, IoT devices are\nvulnerable to various attacks on RPL-based routing. This survey provides an in\ndepth study of existing RPL-based attacks and defense published from year 2011\nto 2024 from highly reputed journals and conferences. By thematic analysis of\nexisting routing attacks on RPL, we developed a novel attack taxonomy which\nfocuses on the nature of routing attacks and classifies them into 12 major\ncategories. Subsequently, the impact of each attack on the network is analyzed\nand discussed real life scenarios of these attacks. Another contribution of\nthis survey proposed a novel taxonomy for classification of defense mechanisms\ninto 8 major categories against routing attacks based on type of defense\nstrategy. The detailed analysis of each defense mechanism with real life\napplicability is explained. Furthermore, evaluation tools such as testbeds and\nsimulators for RPL-based attack and defense are discussed and critically\nanalyzed in terms of real world applicability. Finally, open research\nchallenges are presented on the basis of research gaps of existing literature\nalong with research directions for practitioners and researchers."
    },
    {
        "date": "2025-01",
        "title": "Robust Local Polynomial Regression with Similarity Kernels",
        "author": "Yaniv Shulman",
        "link": "http://arxiv.org/abs/2501.10729v1",
        "abstract": "Local Polynomial Regression (LPR) is a widely used nonparametric method for\nmodeling complex relationships due to its flexibility and simplicity. It\nestimates a regression function by fitting low-degree polynomials to localized\nsubsets of the data, weighted by proximity. However, traditional LPR is\nsensitive to outliers and high-leverage points, which can significantly affect\nestimation accuracy. This paper revisits the kernel function used to compute\nregression weights and proposes a novel framework that incorporates both\npredictor and response variables in the weighting mechanism. By introducing two\npositive definite kernels, the proposed method robustly estimates weights,\nmitigating the influence of outliers through localized density estimation. The\nmethod is implemented in Python and is publicly available at\nhttps://github.com/yaniv-shulman/rsklpr, demonstrating competitive performance\nin synthetic benchmark experiments. Compared to standard LPR, the proposed\napproach consistently improves robustness and accuracy, especially in\nheteroscedastic and noisy environments, without requiring multiple iterations.\nThis advancement provides a promising extension to traditional LPR, opening new\npossibilities for robust regression applications."
    },
    {
        "date": "2025-01",
        "title": "Distributionally Robust Policy Evaluation and Learning for Continuous Treatment with Observational Data",
        "author": "Cheuk Hang Leung, Yiyan Huang, Yijun Li, and Qi Wu",
        "link": "http://arxiv.org/abs/2501.10693v1",
        "abstract": "Using offline observational data for policy evaluation and learning allows\ndecision-makers to evaluate and learn a policy that connects characteristics\nand interventions. Most existing literature has focused on either discrete\ntreatment spaces or assumed no difference in the distributions between the\npolicy-learning and policy-deployed environments. These restrict applications\nin many real-world scenarios where distribution shifts are present with\ncontinuous treatment. To overcome these challenges, this paper focuses on\ndeveloping a distributionally robust policy under a continuous treatment\nsetting. The proposed distributionally robust estimators are established using\nthe Inverse Probability Weighting (IPW) method extended from the discrete one\nfor policy evaluation and learning under continuous treatments. Specifically,\nwe introduce a kernel function into the proposed IPW estimator to mitigate the\nexclusion of observations that can occur in the standard IPW method to\ncontinuous treatments. We then provide finite-sample analysis that guarantees\nthe convergence of the proposed distributionally robust policy evaluation and\nlearning estimators. The comprehensive experiments further verify the\neffectiveness of our approach when distribution shifts are present."
    },
    {
        "date": "2025-01",
        "title": "Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks",
        "author": "Xin Yi, Yue Li, Linlin Wang, Xiaoling Wang, and Liang He",
        "link": "http://arxiv.org/abs/2501.10639v1",
        "abstract": "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks."
    },
    {
        "date": "2025-01",
        "title": "Differentiable Adversarial Attacks for Marked Temporal Point Processes",
        "author": "Pritish Chakraborty, Vinayak Gupta, Rahul R, Srikanta J. Bedathur, and Abir De",
        "link": "http://arxiv.org/abs/2501.10606v1",
        "abstract": "Marked temporal point processes (MTPPs) have been shown to be extremely\neffective in modeling continuous time event sequences (CTESs). In this work, we\npresent adversarial attacks designed specifically for MTPP models. A key\ncriterion for a good adversarial attack is its imperceptibility. For objects\nsuch as images or text, this is often achieved by bounding perturbation in some\nfixed $L_p$ norm-ball. However, similarly minimizing distance norms between two\nCTESs in the context of MTPPs is challenging due to their sequential nature and\nvarying time-scales and lengths. We address this challenge by first permuting\nthe events and then incorporating the additive noise to the arrival timestamps.\nHowever, the worst case optimization of such adversarial attacks is a hard\ncombinatorial problem, requiring exploration across a permutation space that is\nfactorially large in the length of the input sequence. As a result, we propose\na novel differentiable scheme PERMTPP using which we can perform adversarial\nattacks by learning to minimize the likelihood, while minimizing the distance\nbetween two CTESs. Our experiments on four real-world datasets demonstrate the\noffensive and defensive capabilities, and lower inference times of PERMTPP."
    },
    {
        "date": "2025-01",
        "title": "Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics",
        "author": "Haobin Hiroki Chen, Hongbo Chen, Mingshen Sun, Chenghong Wang, and XiaoFeng Wang",
        "link": "http://arxiv.org/abs/2501.10560v1",
        "abstract": "Ensuring the proper use of sensitive data in analytics under complex privacy\npolicies is an increasingly critical challenge. Many existing approaches lack\nportability, verifiability, and scalability across diverse data processing\nframeworks. We introduce Picachv, a novel security monitor that automatically\nenforces data use policies. It works on relational algebra as an abstraction\nfor program semantics, enabling policy enforcement on query plans generated by\nprograms during execution. This approach simplifies analysis across diverse\nanalytical operations and supports various front-end query languages. By\nformalizing both data use policies and relational algebra semantics in Coq, we\nprove that Picachv correctly enforces policies. Picachv also leverages Trusted\nExecution Environments (TEEs) to enhance trust in runtime, providing provable\npolicy compliance to stakeholders that the analytical tasks comply with their\ndata use policies. We integrated Picachv into Polars, a state-of-the-art data\nanalytics framework, and evaluate its performance using the TPC-H benchmark. We\nalso apply our approach to real-world use cases. Our work demonstrates the\npractical application of formal methods in securing data analytics, addressing\nkey challenges."
    },
    {
        "date": "2025-01",
        "title": "Credit Risk Identification in Supply Chains Using Generative Adversarial Networks",
        "author": "Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, and Qianying Liu",
        "link": "http://arxiv.org/abs/2501.10348v3",
        "abstract": "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation"
    },
    {
        "date": "2025-01",
        "title": "Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models",
        "author": "Pit Neitemeier, Bj\u00f6rn Deiseroth, Constantin Eichenberg, and Lukas Balles",
        "link": "http://arxiv.org/abs/2501.10322v2",
        "abstract": "Tokenization is a fundamental step in natural language processing, breaking\ntext into units that computational models can process. While learned subword\ntokenizers have become the de-facto standard, they present challenges such as\nlarge vocabularies, limited adaptability to new domains or languages, and\nsensitivity to spelling errors and variations. To overcome these limitations,\nwe investigate a hierarchical architecture for autoregressive language\nmodelling that combines character-level and word-level processing. It employs a\nlightweight character-level encoder to convert character sequences into word\nembeddings, which are then processed by a word-level backbone model and decoded\nback into characters via a compact character-level decoder. This method retains\nthe sequence compression benefits of word-level tokenization without relying on\na rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion\nparameters, that hierarchical transformers match the downstream task\nperformance of subword-tokenizer-based models while exhibiting significantly\ngreater robustness to input perturbations. Additionally, during continued\npretraining on an out-of-domain language, our model trains almost twice as\nfast, achieves superior performance on the target language, and retains more of\nits previously learned knowledge. Hierarchical transformers pave the way for\nNLP systems that are more robust, flexible, and generalizable across languages\nand domains."
    },
    {
        "date": "2025-01",
        "title": "Robust Egoistic Rigid Body Localization",
        "author": "Niclas F\u00fchrling, Giuseppe Thadeu Freitas de Abreu, David Gonz\u00e1lez G., and Osvaldo Gonsa",
        "link": "http://arxiv.org/abs/2501.10219v1",
        "abstract": "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions."
    },
    {
        "date": "2025-01",
        "title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach",
        "author": "Nicolas Atienza, Christophe Labreuche, Johanne Cohen, and Michele Sebag",
        "link": "http://arxiv.org/abs/2501.10202v1",
        "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic\nDetection using Extreme Value Theory (SPADE), which transforms a classifier\ninto an abstaining classifier, offering provable protection against\nout-of-distribution and adversarial samples. The approach is based on a\nGeneralized Extreme Value (GEV) model of the training distribution in the\nclassifier's latent space, enabling the formal characterization of OOD samples.\nInterestingly, under mild assumptions, the GEV model also allows for formally\ncharacterizing adversarial samples. The abstaining classifier, which rejects\nsamples based on their assessment by the GEV model, provably avoids OOD and\nadversarial samples. The empirical validation of the approach, conducted on\nvarious neural architectures (ResNet, VGG, and Vision Transformer) and medium\nand large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its\nfrugality, stability, and efficiency compared to the state of the art."
    },
    {
        "date": "2025-01",
        "title": "Contributions to the Decision Theoretic Foundations of Machine Learning and Robust Statistics under Weakly Structured Information",
        "author": "Christoph Jansen",
        "link": "http://arxiv.org/abs/2501.10195v1",
        "abstract": "This habilitation thesis is cumulative and, therefore, is collecting and\nconnecting research that I (together with several co-authors) have conducted\nover the last few years. Thus, the absolute core of the work is formed by the\nten publications listed on page 5 under the name Contributions 1 to 10. The\nreferences to the complete versions of these articles are also found in this\nlist, making them as easily accessible as possible for readers wishing to dive\ndeep into the different research projects. The chapters following this thesis,\nnamely Parts A to C and the concluding remarks, serve to place the articles in\na larger scientific context, to (briefly) explain their respective content on a\nless formal level, and to highlight some interesting perspectives for future\nresearch in their respective contexts. Naturally, therefore, the following\npresentation has neither the level of detail nor the formal rigor that can\n(hopefully) be found in the papers. The purpose of the following text is to\nprovide the reader an easy and high-level access to this interesting and\nimportant research field as a whole, thereby, advertising it to a broader\naudience."
    },
    {
        "date": "2025-01",
        "title": "Secure Semantic Communication With Homomorphic Encryption",
        "author": "Rui Meng, Dayu Fan, Haixiao Gao, Yifan Yuan, Bizhu Wang, Xiaodong Xu, Mengying Sun, Chen Dong, Xiaofeng Tao, Ping Zhang, and Dusit Niyato",
        "link": "http://arxiv.org/abs/2501.10182v1",
        "abstract": "In recent years, Semantic Communication (SemCom), which aims to achieve\nefficient and reliable transmission of meaning between agents, has garnered\nsignificant attention from both academia and industry. To ensure the security\nof communication systems, encryption techniques are employed to safeguard\nconfidentiality and integrity. However, traditional cryptography-based\nencryption algorithms encounter obstacles when applied to SemCom. Motivated by\nthis, this paper explores the feasibility of applying homomorphic encryption to\nSemCom. Initially, we review the encryption algorithms utilized in mobile\ncommunication systems and analyze the challenges associated with their\napplication to SemCom. Subsequently, we employ scale-invariant feature\ntransform to demonstrate that semantic features can be preserved in homomorphic\nencrypted ciphertext. Based on this finding, we propose a task-oriented SemCom\nscheme secured through homomorphic encryption. We design the privacy preserved\ndeep joint source-channel coding (JSCC) encoder and decoder, and the frequency\nof key updates can be adjusted according to service requirements without\ncompromising transmission performance. Simulation results validate that, when\ncompared to plaintext images, the proposed scheme can achieve almost the same\nclassification accuracy performance when dealing with homomorphic ciphertext\nimages. Furthermore, we provide potential future research directions for\nhomomorphic encrypted SemCom."
    },
    {
        "date": "2025-01",
        "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics",
        "author": "Chenhao Li, Andreas Krause, and Marco Hutter",
        "link": "http://arxiv.org/abs/2501.10100v1",
        "abstract": "Learning robust and generalizable world models is crucial for enabling\nefficient and scalable robotic control in real-world environments. In this\nwork, we introduce a novel framework for learning world models that accurately\ncapture complex, partially observable, and stochastic dynamics. The proposed\nmethod employs a dual-autoregressive mechanism and self-supervised training to\nachieve reliable long-horizon predictions without relying on domain-specific\ninductive biases, ensuring adaptability across diverse robotic tasks. We\nfurther propose a policy optimization framework that leverages world models for\nefficient training in imagined environments and seamless deployment in\nreal-world systems. Through extensive experiments, our approach consistently\noutperforms state-of-the-art methods, demonstrating superior autoregressive\nprediction accuracy, robustness to noise, and generalization across\nmanipulation and locomotion tasks. Notably, policies trained with our method\nare successfully deployed on ANYmal D hardware in a zero-shot transfer,\nachieving robust performance with minimal sim-to-real performance loss. This\nwork advances model-based reinforcement learning by addressing the challenges\nof long-horizon prediction, error accumulation, and sim-to-real transfer. By\nproviding a scalable and robust framework, the introduced methods pave the way\nfor adaptive and efficient robotic systems in real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Efficient Simulation of Quantum Secure Multiparty Computation",
        "author": "Kartick Sutradhar",
        "link": "http://arxiv.org/abs/2501.10083v1",
        "abstract": "One of the key characteristics of secure quantum communication is quantum\nsecure multiparty computation. In this paper, we propose a quantum secure\nmultiparty summation (QSMS) protocol that can be applied to many complex\nquantum operations. It is based on the $(t, n)$ threshold approach. We combine\nthe classical and quantum phenomena to make this protocol realistic and secure.\nBecause the current protocols employ the $(n, n)$ threshold approach, which\nrequires all honest players to execute the quantum multiparty summation\nprotocol, they have certain security and efficiency problems. However, we\nemploy a $(t, n)$ threshold approach, which requires the quantum summation\nprotocol to be computed only by $t$ honest players. Our suggested protocol is\nmore economical, practical, and secure than alternative protocols."
    },
    {
        "date": "2025-01",
        "title": "Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and MModalCC Framework",
        "author": "Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, and M. Fatih Amasyali",
        "link": "http://arxiv.org/abs/2501.10075v1",
        "abstract": "Remote sensing change captioning (RSICC) aims to describe changes between\nbitemporal images in natural language. Existing methods often fail under\nchallenges like illumination differences, viewpoint changes, blur effects,\nleading to inaccuracies, especially in no-change regions. Moreover, the images\nacquired at different spatial resolutions and have registration errors tend to\naffect the captions. To address these issues, we introduce SECOND-CC, a novel\nRSICC dataset featuring high-resolution RGB image pairs, semantic segmentation\nmaps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of\nbitemporal RS images and 30,205 sentences describing the differences between\nimages. Additionally, we propose MModalCC, a multimodal framework that\nintegrates semantic and visual data using advanced attention mechanisms,\nincluding Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross\nAttention (MGCA). Detailed ablation studies and attention visualizations\nfurther demonstrate its effectiveness and ability to address RSICC challenges.\nComprehensive experiments show that MModalCC outperforms state-of-the-art RSICC\nmethods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on\nBLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and\ncodebase publicly available to facilitate future research at\nhttps://github.com/ChangeCapsInRS/SecondCC"
    },
    {
        "date": "2025-01",
        "title": "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic and Static Data with Generative Adversarial Networks",
        "author": "Junlan Chen, Yiqun Li, Chenyu Ling, Ziyuan Pu, and Xiucheng Guo",
        "link": "http://arxiv.org/abs/2501.10041v1",
        "abstract": "Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy."
    },
    {
        "date": "2025-01",
        "title": "CaFA: Cost-aware, Feasible Attacks With Database Constraints Against Neural Tabular Classifiers",
        "author": "Matan Ben-Tov, Daniel Deutch, Nave Frost, and Mahmood Sharif",
        "link": "http://arxiv.org/abs/2501.10013v1",
        "abstract": "This work presents CaFA, a system for Cost-aware Feasible Attacks for\nassessing the robustness of neural tabular classifiers against adversarial\nexamples realizable in the problem space, while minimizing adversaries' effort.\nTo this end, CaFA leverages TabPGD$-$an algorithm we set forth to generate\nadversarial perturbations suitable for tabular data$-$ and incorporates\nintegrity constraints automatically mined by state-of-the-art database methods.\nAfter producing adversarial examples in the feature space via TabPGD, CaFA\nprojects them on the mined constraints, leading, in turn, to better attack\nrealizability. We tested CaFA with three datasets and two architectures and\nfound, among others, that the constraints we use are of higher quality\n(measured via soundness and completeness) than ones employed in prior work.\nMoreover, CaFA achieves higher feasible success rates$-$i.e., it generates\nadversarial examples that are often misclassified while satisfying\nconstraints$-$than prior attacks while simultaneously perturbing few features\nwith lower magnitudes, thus saving effort and improving inconspicuousness. We\nopen-source CaFA, hoping it will serve as a generic system enabling\nmachine-learning engineers to assess their models' robustness against\nrealizable attacks, thus advancing deployed models' trustworthiness."
    },
    {
        "date": "2025-01",
        "title": "Advancing Image Security with Quantum Key Distribution and Multi-Layer Chaotic Encryption for Quantum Resilient Transmission",
        "author": "Tasmin Karim, Md. Shazzad Hossain Shaon, Md. Fahim Sultan, and Mst Shapna Akter",
        "link": "http://arxiv.org/abs/2501.09895v2",
        "abstract": "Quantum security improves cryptographic protocols by applying quantum\nmechanics principles, assuring resistance to both quantum and conventional\ncomputer attacks. This work addresses these issues by integrating Quantum Key\nDistribution (QKD) utilizing the E91 method with Multi-Layer Chaotic\nEncryption, which employs a variety of patterns to detect eavesdropping,\nresulting in a highly secure image-transmission architecture. The method\nleverages entropy calculations to determine the unpredictability and integrity\nof encrypted and decrypted pictures, guaranteeing strong security. Extensive\nstatistical scenarios illustrate the framework's effectiveness in image\nencryption while preserving high entropy and sensitivity to the original\nvisuals. The findings indicate significant improvement in encryption and\ndecryption performance, demonstrating the framework's potential as a robust\nresponse to weaknesses introduced by advances in quantum computing. Several\nmetrics, such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index\n(SSIM), Normalized Cross-Correlation (NCC), Bit Error Rate (BER), entropy\nvalues for original, encrypted, and decrypted images, and the correlation\nbetween original and decrypted images, validate the framework's effectiveness.\nThe combination of QKD with Multi-Layer Chaotic Encryption provides a scalable\nand resilient technique to secure image communication. As quantum computing\nadvances, this framework offers a future-proof approach for defining secure\ncommunication protocols in crucial sectors such as medical treatment, forensic\ncomputing, and national security, where information confidentiality is\nvaluable."
    },
    {
        "date": "2025-01",
        "title": "FLORA: Formal Language Model Enables Robust Training-free Zero-shot Object Referring Analysis",
        "author": "Zhe Chen, and Zijing Chen",
        "link": "http://arxiv.org/abs/2501.09887v1",
        "abstract": "Object Referring Analysis (ORA), commonly known as referring expression\ncomprehension, requires the identification and localization of specific objects\nin an image based on natural descriptions. Unlike generic object detection, ORA\nrequires both accurate language understanding and precise visual localization,\nmaking it inherently more complex. Although recent pre-trained large visual\ngrounding detectors have achieved significant progress, they heavily rely on\nextensively labeled data and time-consuming learning. To address these, we\nintroduce a novel, training-free framework for zero-shot ORA, termed FLORA\n(Formal Language for Object Referring and Analysis). FLORA harnesses the\ninherent reasoning capabilities of large language models (LLMs) and integrates\na formal language model - a logical framework that regulates language within\nstructured, rule-based descriptions - to provide effective zero-shot ORA. More\nspecifically, our formal language model (FLM) enables an effective,\nlogic-driven interpretation of object descriptions without necessitating any\ntraining processes. Built upon FLM-regulated LLM outputs, we further devise a\nBayesian inference framework and employ appropriate off-the-shelf interpretive\nmodels to finalize the reasoning, delivering favorable robustness against LLM\nhallucinations and compelling ORA performance in a training-free manner. In\npractice, our FLORA boosts the zero-shot performance of existing pretrained\ngrounding detectors by up to around 45%. Our comprehensive evaluation across\ndifferent challenging datasets also confirms that FLORA consistently surpasses\ncurrent state-of-the-art zero-shot methods in both detection and segmentation\ntasks associated with zero-shot ORA. We believe our probabilistic parsing and\nreasoning of the LLM outputs elevate the reliability and interpretability of\nzero-shot ORA. We shall release codes upon publication."
    },
    {
        "date": "2025-01",
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "author": "Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, and Christoph Busch",
        "link": "http://arxiv.org/abs/2501.09817v1",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems\n(FRS), which are operated in border control and passport issuance use cases.\nCorrespondingly, morphing attack detection algorithms (MAD) are needed to\ndefend against such attacks. MAD approaches must be robust enough to handle\nunknown attacks in an open-set scenario where attacks can originate from\nvarious morphing generation algorithms, post-processing and the diversity of\nprinters/scanners. The problem of generalization is further pronounced when the\ndetection has to be made on a single suspected image. In this paper, we propose\na generalized single-image-based MAD (S-MAD) algorithm by learning the encoding\nfrom Vision Transformer (ViT) architecture. Compared to CNN-based\narchitectures, ViT model has the advantage on integrating local and global\ninformation and hence can be suitable to detect the morphing traces widely\ndistributed among the face region. Extensive experiments are carried out on\nface morphing datasets generated using publicly available FRGC face datasets.\nSeveral state-of-the-art (SOTA) MAD algorithms, including representative ones\nthat have been publicly evaluated, have been selected and benchmarked with our\nViT-based approach. Obtained results demonstrate the improved detection\nperformance of the proposed S-MAD method on inter-dataset testing (when\ndifferent data is used for training and testing) and comparable performance on\nintra-dataset testing (when the same data is used for training and testing)\nexperimental protocol."
    },
    {
        "date": "2025-01",
        "title": "Ruling the Unruly: Designing Effective, Low-Noise Network Intrusion Detection Rules for Security Operations Centers",
        "author": "Koen T. W. Teuwen, Tom Mulders, Emmanuele Zambon, and Luca Allodi",
        "link": "http://arxiv.org/abs/2501.09808v1",
        "abstract": "Many Security Operations Centers (SOCs) today still heavily rely on\nsignature-based Network Intrusion Detection Systems (NIDS) such as Suricata.\nThe specificity of intrusion detection rules and the coverage provided by\nrulesets are common concerns within the professional community surrounding\nSOCs, which impact the effectiveness of automated alert post-processing\napproaches. We postulate a better understanding of factors influencing the\nquality of rules can help address current SOC issues. In this paper, we\ncharacterize the rules in use at a collaborating commercial (managed) SOC\nserving customers in sectors including education and IT management. During this\nprocess, we discover six relevant design principles, which we consolidate\nthrough interviews with experienced rule designers at the SOC.We then validate\nour design principles by quantitatively assessing their effect on rule\nspecificity. We find that several of these design considerations significantly\nimpact unnecessary workload caused by rules. For instance, rules that leverage\nproxies for detection, and rules that do not employ alert throttling or do not\ndistinguish (un)successful malicious actions, cause significantly more workload\nfor SOC analysts. Moreover, rules that match a generalized characteristic to\ndetect malicious behavior, which is believed to increase coverage, also\nsignificantly increase workload, suggesting a tradeoff must be struck between\nrule specificity and coverage. We show that these design principles can be\napplied successfully at a SOC to reduce workload whilst maintaining coverage\ndespite the prevalence of violations of the principles."
    },
    {
        "date": "2025-01",
        "title": "W3ID: A Quantum Computing-Secure Digital Identity System Redefining Standards for Web3 and Digital Twins",
        "author": "Joseph Yun, Eli Lifton, Eunseo Lee, Yohan Yun, Abigail Song, Joshua Lee, Cristian Jimenez-Bert, Benedict Song, Yejun Lee, Alex Seo, and Sijung Yun",
        "link": "http://arxiv.org/abs/2501.09802v1",
        "abstract": "The rapid advancements in quantum computing present significant threats to\nexisting encryption standards and internet security. Simultaneously, the advent\nof Web 3.0 marks a transformative era in internet history, emphasizing enhanced\ndata security, decentralization, and user ownership. This white paper\nintroduces the W3ID, an abbreviation of Web3 standard meeting universal digital\nID, which is a Universal Digital Identity (UDI) model designed to meet Web3\nstandards while addressing vulnerabilities posed by quantum computing. W3ID\ninnovatively generates secure Digital Object Identifiers (DOIs) tailored for\nthe decentralized Web 3.0 ecosystem. Additionally, W3ID employs a dual-key\nsystem for secure authentication, enhancing both public and private\nverification mechanisms. To further enhance encryption strength and\nauthentication integrity in the quantum computing era, W3ID incorporates an\nadvanced security mechanism. By requiring quadruple application of SHA-256,\nwith consecutive matches for validation, the system expands the number of\npossibilities to 256^4, which is approximately 4.3 billion times the current\nSHA-256 capacity. This dramatic increase in computational complexity ensures\nthat even advanced quantum computing systems would face significant challenges\nin executing brute-force attacks. W3ID redefines digital identity standards for\nWeb 3.0 and the quantum computing era, setting a new benchmark for security,\nscalability, and decentralization in the global digital twin ecosystem."
    },
    {
        "date": "2025-01",
        "title": "Unified Face Matching and Physical-Digital Spoofing Attack Detection",
        "author": "Arun Kunwar, and Ajita Rattani",
        "link": "http://arxiv.org/abs/2501.09635v1",
        "abstract": "Face recognition technology has dramatically transformed the landscape of\nsecurity, surveillance, and authentication systems, offering a user-friendly\nand non-invasive biometric solution. However, despite its significant\nadvantages, face recognition systems face increasing threats from physical and\ndigital spoofing attacks. Current research typically treats face recognition\nand attack detection as distinct classification challenges. This approach\nnecessitates the implementation of separate models for each task, leading to\nconsiderable computational complexity, particularly on devices with limited\nresources. Such inefficiencies can stifle scalability and hinder performance.\nIn response to these challenges, this paper introduces an innovative unified\nmodel designed for face recognition and detection of physical and digital\nattacks. By leveraging the advanced Swin Transformer backbone and incorporating\nHiLo attention in a convolutional neural network framework, we address unified\nface recognition and spoof attack detection more effectively. Moreover, we\nintroduce augmentation techniques that replicate the traits of physical and\ndigital spoofing cues, significantly enhancing our model robustness. Through\ncomprehensive experimental evaluation across various datasets, we showcase the\neffectiveness of our model in unified face recognition and spoof detection.\nAdditionally, we confirm its resilience against unseen physical and digital\nspoofing attacks, underscoring its potential for real-world applications."
    },
    {
        "date": "2025-01",
        "title": "Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML",
        "author": "Tehila Dahan, and Kfir Y. Levy",
        "link": "http://arxiv.org/abs/2501.09621v1",
        "abstract": "We address the challenges of Byzantine-robust training in asynchronous\ndistributed machine learning systems, aiming to enhance efficiency amid massive\nparallelization and heterogeneous computing resources. Asynchronous systems,\nmarked by independently operating workers and intermittent updates, uniquely\nstruggle with maintaining integrity against Byzantine failures, which encompass\nmalicious or erroneous actions that disrupt learning. The inherent delays in\nsuch settings not only introduce additional bias to the system but also obscure\nthe disruptions caused by Byzantine faults. To tackle these issues, we adapt\nthe Byzantine framework to asynchronous dynamics by introducing a novel\nweighted robust aggregation framework. This allows for the extension of robust\naggregators and a recent meta-aggregator to their weighted versions, mitigating\nthe effects of delayed updates. By further incorporating a recent\nvariance-reduction technique, we achieve an optimal convergence rate for the\nfirst time in an asynchronous Byzantine environment. Our methodology is\nrigorously validated through empirical and theoretical analysis, demonstrating\nits effectiveness in enhancing fault tolerance and optimizing performance in\nasynchronous ML systems."
    },
    {
        "date": "2025-01",
        "title": "Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks",
        "author": "Mitul Goswami, Romit Chatterjee, Somnath Mahato, and Prasant Kumar Pattnaik",
        "link": "http://arxiv.org/abs/2501.09609v1",
        "abstract": "The research presents a study on enhancing the robustness of Wi-Fi-based\nindoor positioning systems against adversarial attacks. The goal is to improve\nthe positioning accuracy and resilience of these systems under two attack\nscenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are\ndeveloped and evaluated: a baseline model (M_Base), an adversarially trained\nrobust model (M_Rob), and an ensemble model (M_Ens). All models utilize a\nKolmogorov-Arnold Network (KAN) architecture. The robust model is trained with\nadversarially perturbed data, while the ensemble model combines predictions\nfrom both the base and robust models. Experimental results show that the robust\nmodel reduces positioning error by approximately 10% compared to the baseline,\nachieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal\nstrength manipulation. The ensemble model further outperforms with errors of\n2.01 meters and 1.975 meters for the respective attack types. This analysis\nhighlights the effectiveness of adversarial training techniques in mitigating\nattack impacts. The findings underscore the importance of considering\nadversarial scenarios in developing indoor positioning systems, as improved\nresilience can significantly enhance the accuracy and reliability of such\nsystems in mission-critical environments."
    },
    {
        "date": "2025-01",
        "title": "Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective Scenes",
        "author": "Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, and Wenzhen Yue",
        "link": "http://arxiv.org/abs/2501.09460v1",
        "abstract": "Neural Radiance Fields (NeRF) often struggle with reconstructing and\nrendering highly reflective scenes. Recent advancements have developed various\nreflection-aware appearance models to enhance NeRF's capability to render\nspecular reflections. However, the robust reconstruction of highly reflective\nscenes is still hindered by the inherent shape ambiguity on specular surfaces.\nExisting methods typically rely on additional geometry priors to regularize the\nshape prediction, but this can lead to oversmoothed geometry in complex scenes.\nObserving the critical role of surface normals in parameterizing reflections,\nwe introduce a transmittance-gradient-based normal estimation technique that\nremains robust even under ambiguous shape conditions. Furthermore, we propose a\ndual activated densities module that effectively bridges the gap between smooth\nsurface normals and sharp object boundaries. Combined with a reflection-aware\nappearance model, our proposed method achieves robust reconstruction and\nhigh-fidelity rendering of scenes featuring both highly specular reflections\nand intricate geometric structures. Extensive experiments demonstrate that our\nmethod outperforms existing state-of-the-art methods on various datasets."
    },
    {
        "date": "2025-01",
        "title": "Double Visual Defense: Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness",
        "author": "Zeyu Wang, Cihang Xie, Brian Bartoldson, and Bhavya Kailkhura",
        "link": "http://arxiv.org/abs/2501.09446v1",
        "abstract": "This paper investigates the robustness of vision-language models against\nadversarial visual perturbations and introduces a novel ``double visual\ndefense\" to enhance this robustness. Unlike previous approaches that resort to\nlightweight adversarial fine-tuning of a pre-trained CLIP model, we perform\nlarge-scale adversarial vision-language pre-training from scratch using\nweb-scale data. We then strengthen the defense by incorporating adversarial\nvisual instruction tuning. The resulting models from each stage, $\\Delta$CLIP\nand $\\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a\nnew state-of-the-art in adversarial defense for vision-language models. For\nexample, the adversarial robustness of $\\Delta$CLIP surpasses that of the\nprevious best models on ImageNet-1k by ~20%. %For example, $\\Delta$CLIP\nsurpasses the previous best models on ImageNet-1k by ~20% in terms of\nadversarial robustness. Similarly, compared to prior art, $\\Delta^2$LLaVA\nbrings a ~30% robustness improvement to image captioning task and a ~20%\nrobustness improvement to visual question answering task. Furthermore, our\nmodels exhibit stronger zero-shot recognition capability, fewer hallucinations,\nand superior reasoning performance compared to baselines. Our project page is\nhttps://doublevisualdefense.github.io/."
    },
    {
        "date": "2025-01",
        "title": "Towards Robust and Realistic Human Pose Estimation via WiFi Signals",
        "author": "Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, and Dacheng Tao",
        "link": "http://arxiv.org/abs/2501.09411v2",
        "abstract": "Robust WiFi-based human pose estimation is a challenging task that bridges\ndiscrete and subtle WiFi signals to human skeletons. This paper revisits this\nproblem and reveals two critical yet overlooked issues: 1) cross-domain gap,\ni.e., due to significant variations between source-target domain pose\ndistributions; and 2) structural fidelity gap, i.e., predicted skeletal poses\nmanifest distorted topology, usually with misplaced joints and disproportionate\nbone lengths. This paper fills these gaps by reformulating the task into a\nnovel two-phase framework dubbed DT-Pose: Domain-consistent representation\nlearning and Topology-constrained Pose decoding. Concretely, we first propose a\ntemporal-consistent contrastive learning strategy with uniformity\nregularization, coupled with self-supervised masking-reconstruction operations,\nto enable robust learning of domain-consistent and motion-discriminative\nWiFi-specific representations. Beyond this, we introduce a simple yet effective\npose decoder with task prompts, which integrates Graph Convolution Network\n(GCN) and Transformer layers to constrain the topology structure of the\ngenerated skeleton by exploring the adjacent-overarching relationships among\nhuman joints. Extensive experiments conducted on various benchmark datasets\nhighlight the superior performance of our method in tackling these fundamental\nchallenges in both 2D/3D human pose estimation tasks."
    },
    {
        "date": "2025-01",
        "title": "Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments",
        "author": "Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, and Pubudu N. Pathirana",
        "link": "http://arxiv.org/abs/2501.09394v1",
        "abstract": "The proliferation of Internet of Things (IoT) devices equipped with acoustic\nsensors necessitates robust acoustic scene classification (ASC) capabilities,\neven in noisy and data-limited environments. Traditional machine learning\nmethods often struggle to generalize effectively under such conditions. To\naddress this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene\nClassifier that leverages the power of quantum-inspired transformers. By\nintegrating quantum concepts like superposition and entanglement, Q-ASC\nachieves superior feature learning and enhanced noise resilience compared to\nclassical models. Furthermore, we introduce a Quantum Variational Autoencoder\n(QVAE) based data augmentation technique to mitigate the challenge of limited\nlabeled data in IoT deployments. Extensive evaluations on the Tampere\nUniversity of Technology (TUT) Acoustic Scenes 2016 benchmark dataset\ndemonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5%\nunder challenging conditions, outperforming state-of-the-art methods by over 5%\nin the best case. This research paves the way for deploying intelligent\nacoustic sensing in IoT networks, with potential applications in smart homes,\nindustrial monitoring, and environmental surveillance, even in adverse acoustic\nenvironments."
    },
    {
        "date": "2025-01",
        "title": "Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks",
        "author": "Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, and Zhihong Tian",
        "link": "http://arxiv.org/abs/2501.09328v2",
        "abstract": "Developing high-performance deep learning models is resource-intensive,\nleading model owners to utilize Machine Learning as a Service (MLaaS) platforms\ninstead of publicly releasing their models. However, malicious users may\nexploit query interfaces to execute model extraction attacks, reconstructing\nthe target model's functionality locally. While prior research has investigated\ntriggerable watermarking techniques for asserting ownership, existing methods\nface significant challenges: (1) most approaches require additional training,\nresulting in high overhead and limited flexibility, and (2) they often fail to\naccount for advanced attackers, leaving them vulnerable to adaptive attacks.\n  In this paper, we propose Neural Honeytrace, a robust plug-and-play\nwatermarking framework against model extraction attacks. We first formulate a\nwatermark transmission model from an information-theoretic perspective,\nproviding an interpretable account of the principles and limitations of\nexisting triggerable watermarking. Guided by the model, we further introduce:\n(1) a similarity-based training-free watermarking method for plug-and-play and\nflexible watermarking, and (2) a distribution-based multi-step watermark\ninformation transmission strategy for robust watermarking. Comprehensive\nexperiments on four datasets demonstrate that Neural Honeytrace outperforms\nprevious methods in efficiency and resisting adaptive attacks. Neural\nHoneytrace reduces the average number of samples required for a worst-case\nt-Test-based copyright claim from $12,000$ to $200$ with zero training cost."
    },
    {
        "date": "2025-01",
        "title": "Cooperative Decentralized Backdoor Attacks on Vertical Federated Learning",
        "author": "Seohyun Lee, Wenzhi Fang, Anindya Bijoy Das, Seyyedali Hosseinalipour, David J. Love, and Christopher G. Brinton",
        "link": "http://arxiv.org/abs/2501.09320v1",
        "abstract": "Federated learning (FL) is vulnerable to backdoor attacks, where adversaries\nalter model behavior on target classification labels by embedding triggers into\ndata samples. While these attacks have received considerable attention in\nhorizontal FL, they are less understood for vertical FL (VFL), where devices\nhold different features of the samples, and only the server holds the labels.\nIn this work, we propose a novel backdoor attack on VFL which (i) does not rely\non gradient information from the server and (ii) considers potential collusion\namong multiple adversaries for sample selection and trigger embedding. Our\nlabel inference model augments variational autoencoders with metric learning,\nwhich adversaries can train locally. A consensus process over the adversary\ngraph topology determines which datapoints to poison. We further propose\nmethods for trigger splitting across the adversaries, with an intensity-based\nimplantation scheme skewing the server towards the trigger. Our convergence\nanalysis reveals the impact of backdoor perturbations on VFL indicated by a\nstationarity gap for the trained model, which we verify empirically as well. We\nconduct experiments comparing our attack with recent backdoor VFL approaches,\nfinding that ours obtains significantly higher success rates for the same main\ntask performance despite not using server information. Additionally, our\nresults verify the impact of collusion on attack performance."
    },
    {
        "date": "2025-01",
        "title": "Clone-Robust AI Alignment",
        "author": "Ariel D. Procaccia, Benjamin Schiffer, and Shirley Zhang",
        "link": "http://arxiv.org/abs/2501.09254v1",
        "abstract": "A key challenge in training Large Language Models (LLMs) is properly aligning\nthem with human preferences. Reinforcement Learning with Human Feedback (RLHF)\nuses pairwise comparisons from human annotators to train reward functions and\nhas emerged as a popular alignment method. However, input datasets in RLHF are\nnot necessarily balanced in the types of questions and answers that are\nincluded. Therefore, we want RLHF algorithms to perform well even when the set\nof alternatives is not uniformly distributed. Drawing on insights from social\nchoice theory, we introduce robustness to approximate clones, a desirable\nproperty of RLHF algorithms which requires that adding near-duplicate\nalternatives does not significantly change the learned reward function. We\nfirst demonstrate that the standard RLHF algorithm based on regularized maximum\nlikelihood estimation (MLE) fails to satisfy this property. We then propose the\nweighted MLE, a new RLHF algorithm that modifies the standard regularized MLE\nby weighting alternatives based on their similarity to other alternatives. This\nnew algorithm guarantees robustness to approximate clones while preserving\ndesirable theoretical properties."
    }
]