[{"date":"2024-10","title":"EgoMimic: Scaling Imitation Learning via Egocentric Video","author":"Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, and Danfei Xu","link":"http://arxiv.org/abs/2410.24221v1","abstract":"The scale and diversity of demonstration data required for imitation learning\nis a significant challenge. We present EgoMimic, a full-stack framework which\nscales manipulation via human embodiment data, specifically egocentric human\nvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) a\nsystem to capture human embodiment data using the ergonomic Project Aria\nglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap\nto human data, (3) cross-domain data alignment techniques, and (4) an imitation\nlearning architecture that co-trains on human and robot data. Compared to prior\nworks that only extract high-level intent from human videos, our approach\ntreats human and robot data equally as embodied demonstration data and learns a\nunified policy from both data sources. EgoMimic achieves significant\nimprovement on a diverse set of long-horizon, single-arm and bimanual\nmanipulation tasks over state-of-the-art imitation learning methods and enables\ngeneralization to entirely new scenes. Finally, we show a favorable scaling\ntrend for EgoMimic, where adding 1 hour of additional hand data is\nsignificantly more valuable than 1 hour of additional robot data. Videos and\nadditional information can be found at https://egomimic.github.io/"},{"date":"2024-10","title":"GeoSplatting: Towards Geometry Guided Gaussian Splatting for Physically-based Inverse Rendering","author":"Kai Ye, Chong Gao, Guanbin Li, Wenzheng Chen, and Baoquan Chen","link":"http://arxiv.org/abs/2410.24204v2","abstract":"We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively."},{"date":"2024-10","title":"Length-Induced Embedding Collapse in Transformer-based Models","author":"Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, and Jun Xu","link":"http://arxiv.org/abs/2410.24200v1","abstract":"Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval."},{"date":"2024-10","title":"SelfCodeAlign: Self-Alignment for Code Generation","author":"Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang","link":"http://arxiv.org/abs/2410.24198v2","abstract":"Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance."},{"date":"2024-10","title":"DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models","author":"Heng-Jui Chang, Hongyu Gong, Changhan Wang, James Glass, and Yu-An Chung","link":"http://arxiv.org/abs/2410.24177v1","abstract":"Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs."},{"date":"2024-10","title":"Constraint Back-translation Improves Complex Instruction Following of Large Language Models","author":"Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li","link":"http://arxiv.org/abs/2410.24175v1","abstract":"Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."},{"date":"2024-10","title":"$\u03c0_0$: A Vision-Language-Action Flow Model for General Robot Control","author":"Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky","link":"http://arxiv.org/abs/2410.24164v1","abstract":"Robot learning holds tremendous promise to unlock the full potential of\nflexible, general, and dexterous robot systems, as well as to address some of\nthe deepest questions in artificial intelligence. However, bringing robot\nlearning to the level of generality required for effective real-world systems\nfaces major obstacles in terms of data, generalization, and robustness. In this\npaper, we discuss how generalist robot policies (i.e., robot foundation models)\ncan address these challenges, and how we can design effective generalist robot\npolicies for complex and highly dexterous tasks. We propose a novel flow\nmatching architecture built on top of a pre-trained vision-language model (VLM)\nto inherit Internet-scale semantic knowledge. We then discuss how this model\ncan be trained on a large and diverse dataset from multiple dexterous robot\nplatforms, including single-arm robots, dual-arm robots, and mobile\nmanipulators. We evaluate our model in terms of its ability to perform tasks in\nzero shot after pre-training, follow language instructions from people and from\na high-level VLM policy, and its ability to acquire new skills via fine-tuning.\nOur results cover a wide variety of tasks, such as laundry folding, table\ncleaning, and assembling boxes."},{"date":"2024-10","title":"Spectral features and variable circular polarisation in the radio emission from the pre-cataclysmic variable QS Vir","author":"M. E. Ridder, A. K. Hughes, C. O. Heinke, G. R. Sivakoff, and R. D. Sydora","link":"http://arxiv.org/abs/2410.24157v1","abstract":"QS Vir is a low-accretion rate cataclysmic variable (CV), or pre-CV, as the M\ndwarf companion is just filling its Roche lobe. We recently identified radio\nemission from QS Vir in the Very Large Array Sky Survey, at a flux of ~1 mJy.\nThe origin of radio emission from CVs is not fully understood, with evidence\nfor synchrotron emission from jets and other coherent plasma emission\nprocesses, such as electron cyclotron maser emission (ECME) or plasma\nradiation. Our aim is to constrain the radio emission mechanism for QS Vir,\nthrough spectroscopic, polarisation, and time variability measurements, all\nwhile checking for correlated X-ray variations. We took 3 epochs of new\nobservations with the VLA in S, C, and X bands, with full Stokes polarisation\ninformation, complemented by near-simultaneous Swift/XRT X-ray data. Radio\nspectra are extracted to search for emission features characteristic of\ncoherent plasma emission processes (e.g. high circular polarisation and\nnarrow-band emission). We fit the X-ray spectra with absorbed power-laws,\nfinding no strong X-ray variability. QS Vir showed a nearly flat radio\nspectrum, with fluxes of 0.4-0.6 mJy in all bands. Swift/XRT showed L_X ~\n5x10^29 erg/s in all observations. We identified strong, variable circular\npolarisation, ranging from 33+/-3% in S band in the last observation, to <11%\nin the middle observation in all bands. Linear polarisation was not detected,\nwith upper limits of at most 15%. Intriguingly, the S-band spectra show\ncircularly polarised spectral bumps (width ~0.5 GHz) that rise and decay within\n<5 minutes. We suggest that the radio emission from QS Vir consists of two\ncomponents: a relatively constant, low-polarisation flat-spectrum component and\na band-limited, rapidly variable, and strongly circularly polarised component.\nThis latter coherent component may be associated with ECME or plasma radiation."},{"date":"2024-10","title":"Scaling Concept With Text-Guided Diffusion Models","author":"Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Anurag Kumar, and Chenliang Xu","link":"http://arxiv.org/abs/2410.24151v1","abstract":"Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal."},{"date":"2024-10","title":"Exploring Vision Language Models for Facial Attribute Recognition: Emotion, Race, Gender, and Age","author":"Nouar AlDahoul, Myles Joshua Toledo Tan, Harishwar Reddy Kasireddy, and Yasir Zaki","link":"http://arxiv.org/abs/2410.24148v1","abstract":"Technologies for recognizing facial attributes like race, gender, age, and\nemotion have several applications, such as surveillance, advertising content,\nsentiment analysis, and the study of demographic trends and social behaviors.\nAnalyzing demographic characteristics based on images and analyzing facial\nexpressions have several challenges due to the complexity of humans' facial\nattributes. Traditional approaches have employed CNNs and various other deep\nlearning techniques, trained on extensive collections of labeled images. While\nthese methods demonstrated effective performance, there remains potential for\nfurther enhancements. In this paper, we propose to utilize vision language\nmodels (VLMs) such as generative pre-trained transformer (GPT), GEMINI, large\nlanguage and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 to\nrecognize facial attributes such as race, gender, age, and emotion from images\nwith human faces. Various datasets like FairFace, AffectNet, and UTKFace have\nbeen utilized to evaluate the solutions. The results show that VLMs are\ncompetitive if not superior to traditional techniques. Additionally, we propose\n\"FaceScanPaliGemma\"--a fine-tuned PaliGemma model--for race, gender, age, and\nemotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and\n59.4% for race, gender, age group, and emotion classification, respectively,\noutperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.\nFinally, we propose \"FaceScanGPT\", which is a GPT-4o model to recognize the\nabove attributes when several individuals are present in the image using a\nprompt engineered for a person with specific facial and/or physical attributes.\nThe results underscore the superior multitasking capability of FaceScanGPT to\ndetect the individual's attributes like hair cut, clothing color, postures,\netc., using only a prompt to drive the detection and recognition tasks."},{"date":"2024-10","title":"Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing","author":"Akash Dhruv, and Anshu Dubey","link":"http://arxiv.org/abs/2410.24119v1","abstract":"The emergence of foundational models and generative artificial intelligence\n(GenAI) is poised to transform productivity in scientific computing, especially\nin code development, refactoring, and translating from one programming language\nto another. However, because the output of GenAI cannot be guaranteed to be\ncorrect, manual intervention remains necessary. Some of this intervention can\nbe automated through task-specific tools, alongside additional methodologies\nfor correctness verification and effective prompt development. We explored the\napplication of GenAI in assisting with code translation, language\ninteroperability, and codebase inspection within a legacy Fortran codebase used\nto simulate particle interactions at the Large Hadron Collider (LHC). In the\nprocess, we developed a tool, CodeScribe, which combines prompt engineering\nwith user supervision to establish an efficient process for code conversion. In\nthis paper, we demonstrate how CodeScribe assists in converting Fortran code to\nC++, generating Fortran-C APIs for integrating legacy systems with modern C++\nlibraries, and providing developer support for code organization and algorithm\nimplementation. We also address the challenges of AI-driven code translation\nand highlight its benefits for enhancing productivity in scientific computing\nworkflows."},{"date":"2024-10","title":"On Sampling Strategies for Spectral Model Sharding","author":"Denis Korzhenkov, and Christos Louizos","link":"http://arxiv.org/abs/2410.24106v1","abstract":"The problem of heterogeneous clients in federated learning has recently drawn\na lot of attention. Spectral model sharding, i.e., partitioning the model\nparameters into low-rank matrices based on the singular value decomposition,\nhas been one of the proposed solutions for more efficient on-device training in\nsuch settings. In this work, we present two sampling strategies for such\nsharding, obtained as solutions to specific optimization problems. The first\nproduces unbiased estimators of the original weights, while the second aims to\nminimize the squared approximation error. We discuss how both of these\nestimators can be incorporated in the federated learning loop and practical\nconsiderations that arise during local training. Empirically, we demonstrate\nthat both of these methods can lead to improved performance on various commonly\nused datasets."},{"date":"2024-10","title":"Matchmaker: Self-Improving Large Language Model Programs for Schema Matching","author":"Nabeel Seedat, and Mihaela van der Schaar","link":"http://arxiv.org/abs/2410.24105v1","abstract":"Schema matching -- the task of finding matches between attributes across\ndisparate data sources with different tables and hierarchies -- is critical for\ncreating interoperable machine learning (ML)-ready data. Addressing this\nfundamental data-centric problem has wide implications, especially in domains\nlike healthcare, finance and e-commerce -- but also has the potential to\nbenefit ML models more generally, by increasing the data available for ML model\ntraining. However, schema matching is a challenging ML task due to\nstructural/hierarchical and semantic heterogeneity between different schemas.\nPrevious ML approaches to automate schema matching have either required\nsignificant labeled data for model training, which is often unrealistic or\nsuffer from poor zero-shot performance. To this end, we propose Matchmaker - a\ncompositional language model program for schema matching, comprised of\ncandidate generation, refinement and confidence scoring. Matchmaker also\nself-improves in a zero-shot manner without the need for labeled demonstrations\nvia a novel optimization approach, which constructs synthetic in-context\ndemonstrations to guide the language model's reasoning process. Empirically, we\ndemonstrate on real-world medical schema matching benchmarks that Matchmaker\noutperforms previous ML-based approaches, highlighting its potential to\naccelerate data integration and interoperability of ML-ready data."},{"date":"2024-10","title":"Progressive Safeguards for Safe and Model-Agnostic Reinforcement Learning","author":"Nabil Omi, Hosein Hasanbeig, Hiteshi Sharma, Sriram K. Rajamani, and Siddhartha Sen","link":"http://arxiv.org/abs/2410.24096v1","abstract":"In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform."},{"date":"2024-10","title":"In-Context Fine-Tuning for Time-Series Foundation Models","author":"Abhimanyu Das, Matthew Faw, Rajat Sen, and Yichen Zhou","link":"http://arxiv.org/abs/2410.24087v1","abstract":"Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain."},{"date":"2024-10","title":"Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models","author":"Jinlin Lai, Daniel Sheldon, and Justin Domke","link":"http://arxiv.org/abs/2410.24079v1","abstract":"Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences."},{"date":"2024-10","title":"Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure","author":"Xiang Li, Yixiang Dai, and Qing Qu","link":"http://arxiv.org/abs/2410.24060v1","abstract":"In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models."},{"date":"2024-10","title":"Identifying General Mechanism Shifts in Linear Causal Representations","author":"Tianyu Chen, Kevin Bello, Francesco Locatello, Bryon Aragam, and Pradeep Ravikumar","link":"http://arxiv.org/abs/2410.24059v1","abstract":"We consider the linear causal representation learning setting where we\nobserve a linear mixing of $d$ unknown latent factors, which follow a linear\nstructural causal model. Recent work has shown that it is possible to recover\nthe latent factors as well as the underlying structural causal model over them,\nup to permutation and scaling, provided that we have at least $d$ environments,\neach of which corresponds to perfect interventions on a single latent node\n(factor). After this powerful result, a key open problem faced by the community\nhas been to relax these conditions: allow for coarser than perfect single-node\ninterventions, and allow for fewer than $d$ of them, since the number of latent\nfactors $d$ could be very large. In this work, we consider precisely such a\nsetting, where we allow a smaller than $d$ number of environments, and also\nallow for very coarse interventions that can very coarsely \\textit{change the\nentire causal graph over the latent factors}. On the flip side, we relax what\nwe wish to extract to simply the \\textit{list of nodes that have shifted\nbetween one or more environments}. We provide a surprising identifiability\nresult that it is indeed possible, under some very mild standard assumptions,\nto identify the set of shifted nodes. Our identifiability proof moreover is a\nconstructive one: we explicitly provide necessary and sufficient conditions for\na node to be a shifted node, and show that we can check these conditions given\nobserved data. Our algorithm lends itself very naturally to the sample setting\nwhere instead of just interventional distributions, we are provided datasets of\nsamples from each of these distributions. We corroborate our results on both\nsynthetic experiments as well as an interesting psychometric dataset. The code\ncan be found at https://github.com/TianyuCodings/iLCS."},{"date":"2024-10","title":"Advanced Predictive Quality Assessment for Ultrasonic Additive Manufacturing with Deep Learning Model","author":"Lokendra Poudel, Sushant Jha, Ryan Meeker, Duy-Nhat Phan, and Rahul Bhowmik","link":"http://arxiv.org/abs/2410.24055v1","abstract":"Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond\nsimilar or dissimilar metal foils to a substrate, resulting in solid,\nconsolidated metal components. However, certain processing conditions can lead\nto inter-layer defects, affecting the final product's quality. This study\ndevelops a method to monitor in-process quality using deep learning-based\nconvolutional neural networks (CNNs). The CNN models were evaluated on their\nability to classify samples with and without embedded thermocouples across five\npower levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with\nsupervised labeling. Four distinct CNN classification models were created for\ndifferent scenarios including without (baseline) and with thermocouples, only\nwithout thermocouples across power levels, only with thermocouples across power\nlevels, and combined without and with thermocouples across power levels. The\nmodels achieved 98.29% accuracy on combined baseline and thermocouple images,\n97.10% for baseline images across power levels, 97.43% for thermocouple images,\nand 97.27% for both types across power levels. The high accuracy, above 97%,\ndemonstrates the system's effectiveness in identifying and classifying\nconditions within the UAM process, providing a reliable tool for quality\nassurance and process control in manufacturing environments."},{"date":"2024-10","title":"Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis","author":"Junliang Du, Yiru Cang, Tong Zhou, Jiacheng Hu, and Weijie He","link":"http://arxiv.org/abs/2410.24046v1","abstract":"This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a\ncutting-edge deep learning approach for the early diagnosis of glaucoma. The\nHM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,\nenabling the extraction of key features that are vital for identifying early\nsigns of glaucoma. Despite the common reliance on large annotated datasets, the\nHM-VGG model excels in scenarios with limited data, achieving remarkable\nresults with small sample sizes. The model's performance is underscored by its\nhigh metrics in Precision, Accuracy, and F1-Score, indicating its potential for\nreal-world application in glaucoma detection. The paper also discusses the\nchallenges associated with ophthalmic image analysis, particularly the\ndifficulty of obtaining large volumes of annotated data. It highlights the\nimportance of moving beyond single-modality data, such as VF or Optical\nCoherence Tomography (OCT) images alone, to a multimodal approach that can\nprovide a richer, more comprehensive dataset. This integration of different\ndata types is shown to significantly enhance diagnostic accuracy. The HM- VGG\nmodel offers a promising tool for doctors, streamlining the diagnostic process\nand improving patient outcomes. Furthermore, its applicability extends to\ntelemedicine and mobile healthcare, making diagnostic services more accessible.\nThe research presented in this paper is a significant step forward in the field\nof medical image processing and has profound implications for clinical\nophthalmology."},{"date":"2024-10","title":"Approximate attention with MLP: a pruning strategy for attention-based model in multivariate time series forecasting","author":"Suhan Guo, Jiahong Deng, Yi Wei, Hui Dou, Furao Shen, and Jian Zhao","link":"http://arxiv.org/abs/2410.24023v1","abstract":"Attention-based architectures have become ubiquitous in time series\nforecasting tasks, including spatio-temporal (STF) and long-term time series\nforecasting (LTSF). Yet, our understanding of the reasons for their\neffectiveness remains limited. This work proposes a new way to understand\nself-attention networks: we have shown empirically that the entire attention\nmechanism in the encoder can be reduced to an MLP formed by feedforward,\nskip-connection, and layer normalization operations for temporal and/or spatial\nmodeling in multivariate time series forecasting. Specifically, the Q, K, and V\nprojection, the attention score calculation, the dot-product between the\nattention score and the V, and the final projection can be removed from the\nattention-based networks without significantly degrading the performance that\nthe given network remains the top-tier compared to other SOTA methods. For\nspatio-temporal networks, the MLP-replace-attention network achieves a\nreduction in FLOPS of $62.579\\%$ with a loss in performance less than $2.5\\%$;\nfor LTSF, a reduction in FLOPs of $42.233\\%$ with a loss in performance less\nthan $2\\%$."},{"date":"2024-10","title":"SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced Protein Sequence Representation","author":"Liang He, Peiran Jin, Yaosen Min, Shufang Xie, Lijun Wu, Tao Qin, Xiaozhuan Liang, Kaiyuan Gao, Yuliang Jiang, and Tie-Yan Liu","link":"http://arxiv.org/abs/2410.24022v1","abstract":"Proteins, essential to biological systems, perform functions intricately\nlinked to their three-dimensional structures. Understanding the relationship\nbetween protein structures and their amino acid sequences remains a core\nchallenge in protein modeling. While traditional protein foundation models\nbenefit from pre-training on vast unlabeled datasets, they often struggle to\ncapture critical co-evolutionary information, which evolutionary-based methods\nexcel at. In this study, we introduce a novel pre-training strategy for protein\nfoundation models that emphasizes the interactions among amino acid residues to\nenhance the extraction of both short-range and long-range co-evolutionary\nfeatures from sequence data. Trained on a large-scale protein sequence dataset,\nour model demonstrates superior generalization ability, outperforming\nestablished baselines of similar size, including the ESM model, across diverse\ndownstream tasks. Experimental results confirm the model's effectiveness in\nintegrating co-evolutionary information, marking a significant step forward in\nprotein sequence-based modeling."},{"date":"2024-10","title":"Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models","author":"Paulius Rauba, Nabeel Seedat, Max Ruiz Luyten, and Mihaela van der Schaar","link":"http://arxiv.org/abs/2410.24005v1","abstract":"The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."},{"date":"2024-10","title":"On testing for independence between generalized error models of several time series","author":"Kilani Ghoudi, Bouchra R. Nasri, and Bruno N. Remillard","link":"http://arxiv.org/abs/2410.24003v1","abstract":"We propose new copula-based models for multivariate time series having\ncontinuous or discrete distributions, or a mixture of both. These models\ninclude stochastic volatility models and regime-switching models. We also\npropose statistics for testing independence between the generalized errors of\nthese models, extending previous results of Duchesne, Ghoudi and Remillard\n(2012) obtained for stochastic volatility models. We define families of\nempirical processes constructed from lagged generalized errors, and we show\nthat their joint asymptotic distributions are Gaussian and independent of the\nestimated parameters of the individual time series. Moebius transformations of\nthe empirical processes are used to obtain tractable covariances. Several tests\nstatistics are then proposed, based on Cramer-von Mises statistics and\ndependence measures, as well as graphical methods to visualize the dependence.\nIn addition, numerical experiments are performed to assess the power of the\nproposed tests. Finally, to show the usefulness of our methodologies, examples\nof applications for financial data and crime data are given to cover both\ndiscrete and continuous cases. ll developed methodologies are implemented in\nthe CRAN package IndGenErrors."},{"date":"2024-10","title":"Assessing the Efficacy of Classical and Deep Neuroimaging Biomarkers in Early Alzheimer's Disease Diagnosis","author":"Milla E. Nielsen, Mads Nielsen, and Mostafa Mehdipour Ghazi","link":"http://arxiv.org/abs/2410.24002v1","abstract":"Alzheimer's disease (AD) is the leading cause of dementia, and its early\ndetection is crucial for effective intervention, yet current diagnostic methods\noften fall short in sensitivity and specificity. This study aims to detect\nsignificant indicators of early AD by extracting and integrating various\nimaging biomarkers, including radiomics, hippocampal texture descriptors,\ncortical thickness measurements, and deep learning features. We analyze\nstructural magnetic resonance imaging (MRI) scans from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) cohorts, utilizing comprehensive image analysis\nand machine learning techniques. Our results show that combining multiple\nbiomarkers significantly improves detection accuracy. Radiomics and texture\nfeatures emerged as the most effective predictors for early AD, achieving AUCs\nof 0.88 and 0.72 for AD and MCI detection, respectively. Although deep learning\nfeatures proved to be less effective than traditional approaches, incorporating\nage with other biomarkers notably enhanced MCI detection performance.\nAdditionally, our findings emphasize the continued importance of classical\nimaging biomarkers in the face of modern deep-learning approaches, providing a\nrobust framework for early AD diagnosis."},{"date":"2024-10","title":"Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model","author":"Wenjia Xie, Hao Wang, Luankang Zhang, Rui Zhou, Defu Lian, and Enhong Chen","link":"http://arxiv.org/abs/2410.23994v2","abstract":"Sequential recommendation (SR) aims to predict items that users may be\ninterested in based on their historical behavior sequences. We revisit SR from\na novel information-theoretic perspective and find that conventional sequential\nmodeling methods fail to adequately capture the randomness and unpredictability\nof user behavior. Inspired by fuzzy information processing theory, this paper\nintroduces the DDSR model, which uses fuzzy sets of interaction sequences to\novercome the limitations and better capture the evolution of users' real\ninterests. Formally based on diffusion transition processes in discrete state\nspaces, which is unlike common diffusion models such as DDPM that operate in\ncontinuous domains. It is better suited for discrete data, using structured\ntransitions instead of arbitrary noise introduction to avoid information loss.\nAdditionally, to address the inefficiency of matrix transformations due to the\nvast discrete space, we use semantic labels derived from quantization or RQ-VAE\nto replace item IDs, enhancing efficiency and improving cold start issues.\nTesting on three public benchmark datasets shows that DDSR outperforms existing\nstate-of-the-art methods in various settings, demonstrating its potential and\neffectiveness in handling SR tasks."},{"date":"2024-10","title":"Stochastic Reconstruction of Gappy Lagrangian Turbulent Signals by Conditional Diffusion Models","author":"Tianyi Li, Luca Biferale, Fabio Bonaccorso, Michele Buzzicotti, and Luca Centurioni","link":"http://arxiv.org/abs/2410.23971v1","abstract":"We present a stochastic method for reconstructing missing spatial and\nvelocity data along the trajectories of small objects passively advected by\nturbulent flows with a wide range of temporal or spatial scales, such as small\nballoons in the atmosphere or drifters in the ocean. Our approach makes use of\nconditional generative diffusion models, a recently proposed data-driven\nmachine learning technique. We solve the problem for two paradigmatic open\nproblems, the case of 3D tracers in homogeneous and isotropic turbulence, and\n2D trajectories from the NOAA-funded Global Drifter Program. We show that for\nboth cases, our method is able to reconstruct velocity signals retaining\nnon-trivial scale-by-scale properties that are highly non-Gaussian and\nintermittent. A key feature of our method is its flexibility in dealing with\nthe location and shape of data gaps, as well as its ability to naturally\nexploit correlations between different components, leading to superior\naccuracy, with respect to Gaussian process regressions, for both pointwise\nreconstruction and statistical expressivity. Our method shows promising\napplications also to a wide range of other Lagrangian problems, including\nmulti-particle dispersion in turbulence, dynamics of charged particles in\nastrophysics and plasma physics, and pedestrian dynamics."},{"date":"2024-10","title":"Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical Scene Segmentation","author":"Yihang Zhou, Rebecca Towning, Zaid Awad, and Stamatia Giannarou","link":"http://arxiv.org/abs/2410.23962v1","abstract":"Surgical scene segmentation is essential for enhancing surgical precision,\nyet it is frequently compromised by the scarcity and imbalance of available\ndata. To address these challenges, semantic image synthesis methods based on\ngenerative adversarial networks and diffusion models have been developed.\nHowever, these models often yield non-diverse images and fail to capture small,\ncritical tissue classes, limiting their effectiveness. In response, we propose\nthe Class-Aware Semantic Diffusion Model (CASDM), a novel approach which\nutilizes segmentation maps as conditions for image synthesis to tackle data\nscarcity and imbalance. Novel class-aware mean squared error and class-aware\nself-perceptual loss functions have been defined to prioritize critical, less\nvisible classes, thereby enhancing image quality and relevance. Furthermore, to\nour knowledge, we are the first to generate multi-class segmentation maps using\ntext prompts in a novel fashion to specify their contents. These maps are then\nused by CASDM to generate surgical scene images, enhancing datasets for\ntraining and validating segmentation models. Our evaluation, which assesses\nboth image quality and downstream segmentation performance, demonstrates the\nstrong effectiveness and generalisability of CASDM in producing realistic\nimage-map pairs, significantly advancing surgical scene segmentation across\ndiverse and challenging datasets."},{"date":"2024-10","title":"MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption","author":"Ruixun Liu, Kaiyu Li, Jiayi Song, Dongwei Sun, and Xiangyong Cao","link":"http://arxiv.org/abs/2410.23946v1","abstract":"Remote sensing image change caption (RSICC) aims to provide natural language\ndescriptions for bi-temporal remote sensing images. Since Change Caption (CC)\ntask requires both spatial and temporal features, previous works follow an\nencoder-fusion-decoder architecture. They use an image encoder to extract\nspatial features and the fusion module to integrate spatial features and\nextract temporal features, which leads to increasingly complex manual design of\nthe fusion module. In this paper, we introduce a novel video model-based\nparadigm without design of the fusion module and propose a Mask-enhanced Video\nmodel for Change Caption (MV-CC). Specifically, we use the off-the-shelf video\nencoder to simultaneously extract the temporal and spatial features of\nbi-temporal images. Furthermore, the types of changes in the CC are set based\non specific task requirements, and to enable the model to better focus on the\nregions of interest, we employ masks obtained from the Change Detection (CD)\nmethod to explicitly guide the CC model. Experimental results demonstrate that\nour proposed method can obtain better performance compared with other\nstate-of-the-art RSICC methods. The code is available at\nhttps://github.com/liuruixun/MV-CC."},{"date":"2024-10","title":"Quantum Deep Equilibrium Models","author":"Philipp Schleich, Marta Skreta, Lasse B. Kristensen, Rodrigo A. Vargas-Hern\u00e1ndez, and Al\u00e1n Aspuru-Guzik","link":"http://arxiv.org/abs/2410.23940v1","abstract":"The feasibility of variational quantum algorithms, the most popular\ncorrespondent of neural networks on noisy, near-term quantum hardware, is\nhighly impacted by the circuit depth of the involved parametrized quantum\ncircuits (PQCs). Higher depth increases expressivity, but also results in a\ndetrimental accumulation of errors. Furthermore, the number of parameters\ninvolved in the PQC significantly influences the performance through the\nnecessary number of measurements to evaluate gradients, which scales linearly\nwith the number of parameters.\n  Motivated by this, we look at deep equilibrium models (DEQs), which mimic an\ninfinite-depth, weight-tied network using a fraction of the memory by employing\na root solver to find the fixed points of the network. In this work, we present\nQuantum Deep Equilibrium Models (QDEQs): a training paradigm that learns\nparameters of a quantum machine learning model given by a PQC using DEQs. To\nour knowledge, no work has yet explored the application of DEQs to QML models.\nWe apply QDEQs to find the parameters of a quantum circuit in two settings: the\nfirst involves classifying MNIST-4 digits with 4 qubits; the second extends it\nto 10 classes of MNIST, FashionMNIST and CIFAR. We find that QDEQ is not only\ncompetitive with comparable existing baseline models, but also achieves higher\nperformance than a network with 5 times more layers. This demonstrates that the\nQDEQ paradigm can be used to develop significantly more shallow quantum\ncircuits for a given task, something which is essential for the utility of\nnear-term quantum computers.\n  Our code is available at https://github.com/martaskrt/qdeq."},{"date":"2024-10","title":"Towards Fast Algorithms for the Preference Consistency Problem Based on Hierarchical Models","author":"Anne-Marie George, Nic Wilson, and Barry O'Sullivan","link":"http://arxiv.org/abs/2410.23934v1","abstract":"In this paper, we construct and compare algorithmic approaches to solve the\nPreference Consistency Problem for preference statements based on hierarchical\nmodels. Instances of this problem contain a set of preference statements that\nare direct comparisons (strict and non-strict) between some alternatives, and a\nset of evaluation functions by which all alternatives can be rated. An instance\nis consistent based on hierarchical preference models, if there exists an\nhierarchical model on the evaluation functions that induces an order relation\non the alternatives by which all relations given by the preference statements\nare satisfied. Deciding if an instance is consistent is known to be NP-complete\nfor hierarchical models. We develop three approaches to solve this decision\nproblem. The first involves a Mixed Integer Linear Programming (MILP)\nformulation, the other two are recursive algorithms that are based on\nproperties of the problem by which the search space can be pruned. Our\nexperiments on synthetic data show that the recursive algorithms are faster\nthan solving the MILP formulation and that the ratio between the running times\nincreases extremely quickly."},{"date":"2024-10","title":"Language Models can Self-Lengthen to Generate Long Texts","author":"Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin","link":"http://arxiv.org/abs/2410.23933v1","abstract":"Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen."},{"date":"2024-10","title":"Redundant Observer-Based Tracking Control for Object Extraction Using a Cable Connected UAV","author":"Benjamin J. Marshall, Yunda Yan, James Knowles, Chenguang Yang, and Cunjia Liu","link":"http://arxiv.org/abs/2410.23929v1","abstract":"A new disturbance observer based control scheme is developed for a quadrotor\nunder the concurrent disturbances from a lightweight elastic tether cable and a\nlumped vertical disturbance. This elastic tether is unusual as it creates a\ndisturbance proportional to the multicopter's translational movement. This\npaper takes an observer-based approach to estimate the stiffness coefficient of\nthe cable and uses the system model to update the estimates of the external\nforces, which are then compensated in the control action. Given that the\ntethered cable force affects both horizontal channels of the quadrotor and is\nalso coupled with the vertical channel, the proposed disturbance observer is\nconstructed to exploit the redundant measurements across all three channels to\njointly estimate the cable stiffness and the vertical disturbance. A\npseudo-inverse method is used to determine the observer gain functions, such\nthat the estimation of the two quantities is decoupled and stable. Compared to\nstandard disturbance observers which assume nearly constant disturbances, the\nproposed approach can quickly adjust its total force estimate as the tethered\nquadrotor changes its position or tautness of the tether. This is applied to\ntwo experiments - a tracking performance test where the multicopter moves under\na constant tether strain, and an object extraction test. In the second test,\nthe multicopter manipulates a nonlinear mechanism mimicking the extraction of a\nwedged object. In both cases, the proposed approach shows significant\nimprovement over standard Disturbance Observer and Extended State Observer\napproaches. A video summary of the experiments can be found at\nhttps://youtu.be/9gKr13WTj-k."},{"date":"2024-10","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments","author":"Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, and Xipeng Qiu","link":"http://arxiv.org/abs/2410.23918v1","abstract":"Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack."},{"date":"2024-10","title":"Transformer-based Model Predictive Control: Trajectory Optimization via Sequence Modeling","author":"Davide Celestini, Daniele Gammelli, Tommaso Guffanti, Simone D'Amico, Elisa Capello, and Marco Pavone","link":"http://arxiv.org/abs/2410.23916v1","abstract":"Model predictive control (MPC) has established itself as the primary\nmethodology for constrained control, enabling general-purpose robot autonomy in\ndiverse real-world scenarios. However, for most problems of interest, MPC\nrelies on the recursive solution of highly non-convex trajectory optimization\nproblems, leading to high computational complexity and strong dependency on\ninitialization. In this work, we present a unified framework to combine the\nmain strengths of optimization-based and learning-based methods for MPC. Our\napproach entails embedding high-capacity, transformer-based neural network\nmodels within the optimization process for trajectory generation, whereby the\ntransformer provides a near-optimal initial guess, or target plan, to a\nnon-convex optimization problem. Our experiments, performed in simulation and\nthe real world onboard a free flyer platform, demonstrate the capabilities of\nour framework to improve MPC convergence and runtime. Compared to purely\noptimization-based approaches, results show that our approach can improve\ntrajectory generation performance by up to 75%, reduce the number of solver\niterations by up to 45%, and improve overall MPC runtime by 7x without loss in\nperformance."},{"date":"2024-10","title":"Efficient Inference and Computation of Optimal Alternatives for Preference Languages Based On Lexicographic Models","author":"Nic Wilson, and Anne-Marie George","link":"http://arxiv.org/abs/2410.23913v1","abstract":"We analyse preference inference, through consistency, for general preference\nlanguages based on lexicographic models. We identify a property, which we call\nstrong compositionality, that applies for many natural kinds of preference\nstatement, and that allows a greedy algorithm for determining consistency of a\nset of preference statements. We also consider different natural definitions of\noptimality, and their relations to each other, for general preference languages\nbased on lexicographic models. Based on our framework, we show that testing\nconsistency, and thus inference, is polynomial for a specific preference\nlanguage LpqT, which allows strict and non-strict statements, comparisons\nbetween outcomes and between partial tuples, both ceteris paribus and strong\nstatements, and their combination. Computing different kinds of optimal sets is\nalso shown to be polynomial; this is backed up by our experimental results."},{"date":"2024-10","title":"From Web Data to Real Fields: Low-Cost Unsupervised Domain Adaptation for Agricultural Robots","author":"Vasileios Tzouras, Lazaros Nalpantidis, and Ronja G\u00fcldenring","link":"http://arxiv.org/abs/2410.23906v1","abstract":"In precision agriculture, vision models often struggle with new, unseen\nfields where crops and weeds have been influenced by external factors,\nresulting in compositions and appearances that differ from the learned\ndistribution. This paper aims to adapt to specific fields at low cost using\nUnsupervised Domain Adaptation (UDA). We explore a novel domain shift from a\ndiverse, large pool of internet-sourced data to a small set of data collected\nby a robot at specific locations, minimizing the need for extensive on-field\ndata collection. Additionally, we introduce a novel module -- the Multi-level\nAttention-based Adversarial Discriminator (MAAD) -- which can be integrated at\nthe feature extractor level of any detection model. In this study, we\nincorporate MAAD with CenterNet to simultaneously detect leaf, stem, and vein\ninstances. Our results show significant performance improvements in the\nunlabeled target domain compared to baseline models, with a 7.5% increase in\nobject detection accuracy and a 5.1% improvement in keypoint detection."},{"date":"2024-10","title":"Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model","author":"Hao Zhang, Lei Cao, and Jiayi Ma","link":"http://arxiv.org/abs/2410.23905v1","abstract":"Existing multi-modal image fusion methods fail to address the compound\ndegradations presented in source images, resulting in fusion images plagued by\nnoise, color bias, improper exposure, \\textit{etc}. Additionally, these methods\noften overlook the specificity of foreground objects, weakening the salience of\nthe objects of interest within the fused images. To address these challenges,\nthis study proposes a novel interactive multi-modal image fusion framework\nbased on the text-modulated diffusion model, called Text-DiFuse. First, this\nframework integrates feature-level information integration into the diffusion\nprocess, allowing adaptive degradation removal and multi-modal information\nfusion. This is the first attempt to deeply and explicitly embed information\nfusion within the diffusion process, effectively addressing compound\ndegradation in image fusion. Second, by embedding the combination of the text\nand zero-shot location model into the diffusion fusion process, a\ntext-controlled fusion re-modulation strategy is developed. This enables\nuser-customized text control to improve fusion performance and highlight\nforeground objects in the fused images. Extensive experiments on diverse public\ndatasets show that our Text-DiFuse achieves state-of-the-art fusion performance\nacross various scenarios with complex degradation. Moreover, the semantic\nsegmentation experiment validates the significant enhancement in semantic\nperformance achieved by our text-controlled fusion re-modulation strategy. The\ncode is publicly available at https://github.com/Leiii-Cao/Text-DiFuse."},{"date":"2024-10","title":"DiffBatt: A Diffusion Model for Battery Degradation Prediction and Synthesis","author":"Hamidreza Eivazi, Andr\u00e9 Hebenbrock, Raphael Ginster, Steffen Bl\u00f6meke, Stefan Wittek, Christoph Hermann, Thomas S. Spengler, Thomas Turek, and Andreas Rausch","link":"http://arxiv.org/abs/2410.23893v1","abstract":"Battery degradation remains a critical challenge in the pursuit of green\ntechnologies and sustainable energy solutions. Despite significant research\nefforts, predicting battery capacity loss accurately remains a formidable task\ndue to its complex nature, influenced by both aging and cycling behaviors. To\naddress this challenge, we introduce a novel general-purpose model for battery\ndegradation prediction and synthesis, DiffBatt. Leveraging an innovative\ncombination of conditional and unconditional diffusion models with\nclassifier-free guidance and transformer architecture, DiffBatt achieves high\nexpressivity and scalability. DiffBatt operates as a probabilistic model to\ncapture uncertainty in aging behaviors and a generative model to simulate\nbattery degradation. The performance of the model excels in prediction tasks\nwhile also enabling the generation of synthetic degradation curves,\nfacilitating enhanced model training by data augmentation. In the remaining\nuseful life prediction task, DiffBatt provides accurate results with a mean\nRMSE of 196 cycles across all datasets, outperforming all other models and\ndemonstrating superior generalizability. This work represents an important step\ntowards developing foundational models for battery degradation."},{"date":"2024-10","title":"Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs","author":"Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong","link":"http://arxiv.org/abs/2410.23875v1","abstract":"Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG."},{"date":"2024-10","title":"Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?","author":"Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, and Bo Han","link":"http://arxiv.org/abs/2410.23856v1","abstract":"This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales."},{"date":"2024-10","title":"Leveraging Large Language Models for Medical Information Extraction and Query Generation","author":"Georgios Peikos, Pranav Kasela, and Gabriella Pasi","link":"http://arxiv.org/abs/2410.23851v1","abstract":"This paper introduces a system that integrates large language models (LLMs)\ninto the clinical trial retrieval process, enhancing the effectiveness of\nmatching patients with eligible trials while maintaining information privacy\nand allowing expert oversight. We evaluate six LLMs for query generation,\nfocusing on open-source and relatively small models that require minimal\ncomputational resources. Our evaluation includes two closed-source and four\nopen-source models, with one specifically trained in the medical field and five\ngeneral-purpose models. We compare the retrieval effectiveness achieved by\nLLM-generated queries against those created by medical experts and\nstate-of-the-art methods from the literature. Our findings indicate that the\nevaluated models reach retrieval effectiveness on par with or greater than\nexpert-created queries. The LLMs consistently outperform standard baselines and\nother approaches in the literature. The best performing LLMs exhibit fast\nresponse times, ranging from 1.7 to 8 seconds, and generate a manageable number\nof query terms (15-63 on average), making them suitable for practical\nimplementation. Our overall findings suggest that leveraging small, open-source\nLLMs for clinical trials retrieval can balance performance, computational\nefficiency, and real-world applicability in medical settings."},{"date":"2024-10","title":"Reasons and Solutions for the Decline in Model Performance after Editing","author":"Xiusheng Huang, Jiaxiang Liu, Yequan Wang, and Kang Liu","link":"http://arxiv.org/abs/2410.23843v1","abstract":"Knowledge editing technology has received widespread attention for low-cost\nupdates of incorrect or outdated knowledge in large-scale language models.\nHowever, recent research has found that edited models often exhibit varying\ndegrees of performance degradation. The reasons behind this phenomenon and\npotential solutions have not yet been provided. In order to investigate the\nreasons for the performance decline of the edited model and optimize the\nediting method, this work explores the underlying reasons from both data and\nmodel perspectives. Specifically, 1) from a data perspective, to clarify the\nimpact of data on the performance of editing models, this paper first\nconstructs a Multi-Question Dataset (MQD) to evaluate the impact of different\ntypes of editing data on model performance. The performance of the editing\nmodel is mainly affected by the diversity of editing targets and sequence\nlength, as determined through experiments. 2) From a model perspective, this\narticle explores the factors that affect the performance of editing models. The\nresults indicate a strong correlation between the L1-norm of the editing model\nlayer and the editing accuracy, and clarify that this is an important factor\nleading to the bottleneck of editing performance. Finally, in order to improve\nthe performance of the editing model, this paper further proposes a Dump for\nSequence (D4S) method, which successfully overcomes the previous editing\nbottleneck by reducing the L1-norm of the editing layer, allowing users to\nperform multiple effective edits and minimizing model damage. Our code is\navailable at https://github.com/nlpkeg/D4S."},{"date":"2024-10","title":"Counterfactual MRI Data Augmentation using Conditional Denoising Diffusion Generative Models","author":"Pedro Mor\u00e3o, Joao Santinha, Yasna Forghani, Nuno Lou\u00e7\u00e3o, Pedro Gouveia, and Mario A. T. Figueiredo","link":"http://arxiv.org/abs/2410.23835v1","abstract":"Deep learning (DL) models in medical imaging face challenges in\ngeneralizability and robustness due to variations in image acquisition\nparameters (IAP). In this work, we introduce a novel method using conditional\ndenoising diffusion generative models (cDDGMs) to generate counterfactual\nmagnetic resonance (MR) images that simulate different IAP without altering\npatient anatomy. We demonstrate that using these counterfactual images for data\naugmentation can improve segmentation accuracy, particularly in\nout-of-distribution settings, enhancing the overall generalizability and\nrobustness of DL models across diverse imaging conditions. Our approach shows\npromise in addressing domain and covariate shifts in medical imaging. The code\nis publicly available at https:\n//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation"},{"date":"2024-10","title":"Denoising Diffusion Models for Anomaly Localization in Medical Images","author":"Cosmin I. Bercea, Philippe C. Cattin, Julia A. Schnabel, and Julia Wolleb","link":"http://arxiv.org/abs/2410.23834v1","abstract":"This chapter explores anomaly localization in medical images using denoising\ndiffusion models. After providing a brief methodological background of these\nmodels, including their application to image reconstruction and their\nconditioning using guidance mechanisms, we provide an overview of available\ndatasets and evaluation metrics suitable for their application to anomaly\nlocalization in medical images. In this context, we discuss supervision schemes\nranging from fully supervised segmentation to semi-supervised, weakly\nsupervised, self-supervised, and unsupervised methods, and provide insights\ninto the effectiveness and limitations of these approaches. Furthermore, we\nhighlight open challenges in anomaly localization, including detection bias,\ndomain shift, computational cost, and model interpretability. Our goal is to\nprovide an overview of the current state of the art in the field, outline\nresearch gaps, and highlight the potential of diffusion models for robust\nanomaly localization in medical images."},{"date":"2024-10","title":"FRoundation: Are Foundation Models Ready for Face Recognition?","author":"Tahar Chettaoui, Naser Damer, and Fadi Boutros","link":"http://arxiv.org/abs/2410.23831v2","abstract":"Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models."},{"date":"2024-10","title":"Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding","author":"Jinlong He, Pengfei Li, Gang Liu, and Shenjun Zhong","link":"http://arxiv.org/abs/2410.23822v1","abstract":"Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review."},{"date":"2024-10","title":"Disentangling Disentangled Representations: Towards Improved Latent Units via Diffusion Models","author":"Youngjun Jun, Jiwoo Park, Kyobin Choo, Tae Eun Choi, and Seong Jae Hwang","link":"http://arxiv.org/abs/2410.23820v1","abstract":"Disentangled representation learning (DRL) aims to break down observed data\ninto core intrinsic factors for a profound understanding of the data. In\nreal-world scenarios, manually defining and labeling these factors are\nnon-trivial, making unsupervised methods attractive. Recently, there have been\nlimited explorations of utilizing diffusion models (DMs), which are already\nmainstream in generative modeling, for unsupervised DRL. They implement their\nown inductive bias to ensure that each latent unit input to the DM expresses\nonly one distinct factor. In this context, we design Dynamic Gaussian Anchoring\nto enforce attribute-separated latent units for more interpretable DRL. This\nunconventional inductive bias explicitly delineates the decision boundaries\nbetween attributes while also promoting the independence among latent units.\nAdditionally, we also propose Skip Dropout technique, which easily modifies the\ndenoising U-Net to be more DRL-friendly, addressing its uncooperative nature\nwith the disentangling feature extractor. Our methods, which carefully consider\nthe latent unit semantics and the distinct DM structure, enhance the\npracticality of DM-based disentangled representations, demonstrating\nstate-of-the-art disentanglement performance on both synthetic and real data,\nas well as advantages in downstream tasks."},{"date":"2024-10","title":"Neural Model Checking","author":"Mirco Giacobbe, Daniel Kroening, Abhinandan Pal, and Michael Tautschnig","link":"http://arxiv.org/abs/2410.23790v1","abstract":"We introduce a machine learning approach to model checking temporal logic,\nwith application to formal hardware verification. Model checking answers the\nquestion of whether every execution of a given system satisfies a desired\ntemporal logic specification. Unlike testing, model checking provides formal\nguarantees. Its application is expected standard in silicon design and the EDA\nindustry has invested decades into the development of performant symbolic model\nchecking algorithms. Our new approach combines machine learning and symbolic\nreasoning by using neural networks as formal proof certificates for linear\ntemporal logic. We train our neural certificates from randomly generated\nexecutions of the system and we then symbolically check their validity using\nsatisfiability solving which, upon the affirmative answer, establishes that the\nsystem provably satisfies the specification. We leverage the expressive power\nof neural networks to represent proof certificates as well as the fact that\nchecking a certificate is much simpler than finding one. As a result, our\nmachine learning procedure for model checking is entirely unsupervised,\nformally sound, and practically effective. We experimentally demonstrate that\nour method outperforms the state-of-the-art academic and commercial model\ncheckers on a set of standard hardware designs written in SystemVerilog."},{"date":"2024-10","title":"Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map","author":"Xinyuan Chang, Maixuan Xue, Xinran Liu, Zheng Pan, and Xing Wei","link":"http://arxiv.org/abs/2410.23780v1","abstract":"Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current benchmark datasets concentrate\non lane perception or basic traffic sign recognition, they often overlook the\nintricate task of integrating these regulations into lane operations.\nAddressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over 10,000 annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from\nTraffic Sign, which accurately deciphers regulatory instructions, and 2)\nRule-Lane Correspondence Reasoning, which aligns these rules with their\nrespective lanes. Built upon this benchmark, we provide a multimodal solution\nthat offers a strong baseline for advancing autonomous driving technologies. It\nfills a critical gap in the integration of traffic sign rules, contributing to\nthe development of reliable autonomous navigation systems."},{"date":"2024-10","title":"What is Wrong with Perplexity for Long-context Language Modeling?","author":"Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang","link":"http://arxiv.org/abs/2410.23771v1","abstract":"Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL."},{"date":"2024-10","title":"MLLA-UNet: Mamba-like Linear Attention in an Efficient U-Shape Model for Medical Image Segmentation","author":"Yufeng Jiang, Zongxi Li, Xiangyan Chen, Haoran Xie, and Jing Cai","link":"http://arxiv.org/abs/2410.23738v1","abstract":"Recent advancements in medical imaging have resulted in more complex and\ndiverse images, with challenges such as high anatomical variability, blurred\ntissue boundaries, low organ contrast, and noise. Traditional segmentation\nmethods struggle to address these challenges, making deep learning approaches,\nparticularly U-shaped architectures, increasingly prominent. However, the\nquadratic complexity of standard self-attention makes Transformers\ncomputationally prohibitive for high-resolution images. To address these\nchallenges, we propose MLLA-UNet (Mamba-Like Linear Attention UNet), a novel\narchitecture that achieves linear computational complexity while maintaining\nhigh segmentation accuracy through its innovative combination of linear\nattention and Mamba-inspired adaptive mechanisms, complemented by an efficient\nsymmetric sampling structure for enhanced feature processing. Our architecture\neffectively preserves essential spatial features while capturing long-range\ndependencies at reduced computational complexity. Additionally, we introduce a\nnovel sampling strategy for multi-scale feature fusion. Experiments demonstrate\nthat MLLA-UNet achieves state-of-the-art performance on six challenging\ndatasets with 24 different segmentation tasks, including but not limited to\nFLARE22, AMOS CT, and ACDC, with an average DSC of 88.32%. These results\nunderscore the superiority of MLLA-UNet over existing methods. Our\ncontributions include the novel 2D segmentation architecture and its empirical\nvalidation. The code is available via https://github.com/csyfjiang/MLLA-UNet."},{"date":"2024-10","title":"MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed Image Retrieval","author":"Haiwen Li, Fei Su, and Zhicheng Zhao","link":"http://arxiv.org/abs/2410.23736v1","abstract":"Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon."},{"date":"2024-10","title":"GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting","author":"Xiufeng Huang, Ruiqi Li, Yiu-ming Cheung, Ka Chun Cheung, Simon See, and Renjie Wan","link":"http://arxiv.org/abs/2410.23718v1","abstract":"3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D\nassets. To protect the copyright of these assets, digital watermarking\ntechniques can be applied to embed ownership information discreetly within 3DGS\nmodels. However, existing watermarking methods for meshes, point clouds, and\nimplicit radiance fields cannot be directly applied to 3DGS models, as 3DGS\nmodels use explicit 3D Gaussians with distinct structures and do not rely on\nneural networks. Naively embedding the watermark on a pre-trained 3DGS can\ncause obvious distortion in rendered images. In our work, we propose an\nuncertainty-based method that constrains the perturbation of model parameters\nto achieve invisible watermarking for 3DGS. At the message decoding stage, the\ncopyright messages can be reliably extracted from both 3D Gaussians and 2D\nrendered images even under various forms of 3D and 2D distortions. We conduct\nextensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate\nthe effectiveness of our proposed method, demonstrating state-of-the-art\nperformance on both message decoding accuracy and view synthesis quality."},{"date":"2024-10","title":"OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models","author":"Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, and Julian McAuley","link":"http://arxiv.org/abs/2410.23703v1","abstract":"Offline evaluation of LLMs is crucial in understanding their capacities,\nthough current methods remain underexplored in existing research. In this work,\nwe focus on the offline evaluation of the chain-of-thought capabilities and\nshow how to optimize LLMs based on the proposed evaluation method. To enable\noffline feedback with rich knowledge and reasoning paths, we use knowledge\ngraphs (e.g., Wikidata5m) to provide feedback on the generated chain of\nthoughts. Due to the heterogeneity between LLM reasoning and KG structures,\ndirect interaction and feedback from KGs on LLM behavior are challenging, as\nthey require accurate entity linking and grounding of LLM-generated chains of\nthought in the KG. To address the above challenge, we propose an offline\nchain-of-thought evaluation framework, OCEAN, which models chain-of-thought\nreasoning in LLMs as an MDP and evaluate the policy's alignment with KG\npreference modeling. To overcome the reasoning heterogeneity and grounding\nproblems, we leverage on-policy KG exploration and RL to model a KG policy that\ngenerates token-level likelihood distributions for LLM-generated\nchain-of-thought reasoning paths, simulating KG reasoning preference. Then we\nincorporate the knowledge-graph feedback on the validity and alignment of the\ngenerated reasoning paths into inverse propensity scores and propose KG-IPS\nestimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS\nestimator and provide a lower bound on its variance. With the off-policy\nevaluated value function, we can directly enable off-policy optimization to\nfurther enhance chain-of-thought alignment. Our empirical study shows that\nOCEAN can be efficiently optimized for generating chain-of-thought reasoning\npaths with higher estimated values without affecting LLMs' general abilities in\ndownstream tasks or their internal knowledge."},{"date":"2024-10","title":"Robust orbital-angular-momentum-based underwater acoustic communication with dynamic modal decomposition method","author":"Liulin Li, Bingyi Liu, and Zhongyi Guo","link":"http://arxiv.org/abs/2410.23675v1","abstract":"Recently, acoustic communication employing Orbital Angular Momentum (OAM)\nopens another avenue for efficient data transmission in aquatic environments.\nCurrent topological charge (TC) detection of OAM beams relies on the\northogonality among different-order OAM beams. Such strategy requires data\ncollection from the entire acoustic field, which inevitably reduces the\nefficiency and increases the bit error rate (BER). To address these challenges,\nthis study proposes a modified Dynamic Modal Decomposition (DMD) method by\npartially sampling the acoustic field for precise TC detection. Numerical\nsimulations confirm the accuracy of this approach in extracting single or\nmultiple TCs magnitudes within a partially-sampled acoustic field. We\ntheoretically compare the performance of the modified DMD approach with\nconventional orthogonal decoding method. Simulation results indicate that our\nmodified DMD scheme exhibits lower BER under the same noise interference and is\nmore robust to the array misalignment. This research introduces an efficient\ndemodulation solution for acoustic OAM communication, offering potential\nbenefits for simplifying receiver array design and enhancing long-distance\nunderwater data transmission."},{"date":"2024-10","title":"Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning","author":"Minghui Chen, Meirui Jiang, Xin Zhang, Qi Dou, Zehua Wang, and Xiaoxiao Li","link":"http://arxiv.org/abs/2410.23660v1","abstract":"Federated learning (FL) is a learning paradigm that enables collaborative\ntraining of models using decentralized data. Recently, the utilization of\npre-trained weight initialization in FL has been demonstrated to effectively\nimprove model performance. However, the evolving complexity of current\npre-trained models, characterized by a substantial increase in parameters,\nmarkedly intensifies the challenges associated with communication rounds\nrequired for their adaptation to FL. To address these communication cost issues\nand increase the performance of pre-trained model adaptation in FL, we propose\nan innovative model interpolation-based local training technique called ``Local\nSuperior Soups.'' Our method enhances local training across different clients,\nencouraging the exploration of a connected low-loss basin within a few\ncommunication rounds through regularized model interpolation. This approach\nacts as a catalyst for the seamless adaptation of pre-trained models in in FL.\nWe demonstrated its effectiveness and efficiency across diverse widely-used FL\ndatasets. Our code is available at\n\\href{https://github.com/ubc-tea/Local-Superior-Soups}{https://github.com/ubc-tea/Local-Superior-Soups}."},{"date":"2024-10","title":"Novel Clinical-Grade Prostate Cancer Detection and Grading Model: Development and Prospective Validation Using Real World Data, with Performance Assessment on IHC Requested Cases","author":"Ramin Nateghi, Ruoji Zhou, Madeline Saft, Marina Schnauss, Clayton Neill, Ridwan Alam, Nicole Handa, Mitchell Huang, Eric V Li, Jeffery A Goldstein, Edward M Schaeffer, Menatalla Nadim, Fattaneh Pourakpour, Bogdan Isaila, Christopher Felicelli, Vikas Mehta, Behtash G Nezami, Ashley Ross, Ximing Yang, and Lee AD Cooper","link":"http://arxiv.org/abs/2410.23642v1","abstract":"Artificial intelligence may assist healthcare systems in meeting increasing\ndemand for pathology services while maintaining diagnostic quality and reducing\nturnaround time and costs. We aimed to investigate the performance of an\ninstitutionally developed system for prostate cancer detection, grading, and\nworkflow optimization and to contrast this with commercial alternatives. From\nAugust 2021 to March 2023, we scanned 21,396 slides from 1,147 patients with\npositive biopsies. We developed models for cancer detection, grading, and\nscreening of equivocal cases for IHC ordering. We compared a task-specific\nmodel trained using the PANDA dataset of prostate cancer biopsies with one\nbuilt using features extracted by the general-purpose histology foundation\nmodel, UNI and compare their performance in an unfiltered prospectively\ncollected dataset that reflects our patient population (1737 slides,95\npatients). We evaluated the contributions of a bespoke model designed to\nimprove sensitivity in detecting small cancer foci and scoring of broader\npatterns observed at lower resolution. We found high concordance between the\ndeveloped systems and pathologist reference in detection (AUC 98.5, sensitivity\n95.0, and specificity 97.8), ISUP grading (quadratic Cohen's kappa 0.869),\ngrade group 3 or higher (AUC 97.5, sensitivity 94.9, specificity 96.6) and\ncomparable to published data from commercial systems. Screening could reduce\nIHC ordering for equivocal cases by 44.5% with an overall error rate of 1.8%\n(1.4% false positive, 0.4% false negative rates). Institutions like academic\nmedical centers that have high scanning volumes and report abstraction\ncapabilities can develop accurate computational pathology models for internal\nuse. These models have the potential to aid in quality control role and to\nimprove workflow in the pathology lab to help meet future challenges in\nprostate cancer diagnosis."},{"date":"2024-10","title":"Solving the Kinetic Ising Model with Non-Reciprocity","author":"Gabriel Weiderpass, Mayur Sharma, and Savdeep Sethi","link":"http://arxiv.org/abs/2410.23615v1","abstract":"Non-reciprocal interactions are a generic feature of non-equilibrium systems.\nWe define a non-reciprocal generalization of the kinetic Ising model in one\nspatial dimension. We solve the model exactly using two different approaches\nfor infinite, semi-infinite and finite systems with either periodic or open\nboundary conditions. The exact solution allows us to explore a range of novel\nphenomena tied to non-reciprocity like non-reciprocity induced frustration and\nwave phenomena with interesting parity-dependence for finite systems of size\n$N$.\n  We study dynamical questions like the approach to equilibrium with various\nboundary conditions. We find new regimes, separated by $N^{th}$-order\nexceptional points, which can be classified as overdamped, underdamped and\ncritically damped phases. Despite these new regimes, long-time order is only\npresent at zero temperature. Additionally, we explore the low-energy behavior\nof the system in various limits, including the ageing and spatio-temporal Porod\nregimes, demonstrating that non-reciprocity induces unique scaling behavior at\nzero temperature. Lastly, we present general results for systems where spins\ninteract with no more than two spins, outlining the conditions under which\nlong-time order may exist."},{"date":"2024-10","title":"How Do Flow Matching Models Memorize and Generalize in Sample Data Subspaces?","author":"Weiguo Gao, and Ming Li","link":"http://arxiv.org/abs/2410.23594v1","abstract":"Real-world data is often assumed to lie within a low-dimensional structure\nembedded in high-dimensional space. In practical settings, we observe only a\nfinite set of samples, forming what we refer to as the sample data subspace. It\nserves an essential approximation supporting tasks such as dimensionality\nreduction and generation. A major challenge lies in whether generative models\ncan reliably synthesize samples that stay within this subspace rather than\ndrifting away from the underlying structure. In this work, we provide\ntheoretical insights into this challenge by leveraging Flow Matching models,\nwhich transform a simple prior into a complex target distribution via a learned\nvelocity field. By treating the real data distribution as discrete, we derive\nanalytical expressions for the optimal velocity field under a Gaussian prior,\nshowing that generated samples memorize real data points and represent the\nsample data subspace exactly. To generalize to suboptimal scenarios, we\nintroduce the Orthogonal Subspace Decomposition Network (OSDNet), which\nsystematically decomposes the velocity field into subspace and off-subspace\ncomponents. Our analysis shows that the off-subspace component decays, while\nthe subspace component generalizes within the sample data subspace, ensuring\ngenerated samples preserve both proximity and diversity."},{"date":"2024-10","title":"Work Extraction from a Controlled Quantum Emitter","author":"Paulson Kavalambramalil George, Hanna Terletska, and Herbert F Fotso","link":"http://arxiv.org/abs/2410.23589v1","abstract":"We investigate how an external driving field can control the amount of\nextractable work from a quantum emitter, a two-level quantum system (TLS)\ninteracting with a photonic environment. In this scenario, the TLS functions as\na quantum battery, interacting with the photonic bath that discharges it while\nthe control field recharges it. Ergotropy serves as our measure of the\nextractable work from the quantum system. We systematically analyze how the\nergotropy of the system evolves as it interacts with the photonic bath under\nthe control of either a continuous driving field or a periodic pulse sequence.\nThe coherent and incoherent contributions to the total ergotropy for various\ninitial states are calculated. The role of detuning between the driving field\nand the emission frequency of the TLS, as well as the initial state of the\nsystem in work extraction, are investigated for continuous and periodic\npulse-driving fields. We show that detuning has little impact on work\nextraction for a system that is driven by a periodic sequence of instantaneous\npulses. However, for a continuously driven system, as the system approaches its\nsteady state, ergotropy increases with detuning increases."},{"date":"2024-10","title":"End-to-End Ontology Learning with Large Language Models","author":"Andy Lo, Albert Q. Jiang, Wenda Li, and Mateja Jamnik","link":"http://arxiv.org/abs/2410.23584v1","abstract":"Ontologies are useful for automatic machine processing of domain knowledge as\nthey represent it in a structured format. Yet, constructing ontologies requires\nsubstantial manual effort. To automate part of this process, large language\nmodels (LLMs) have been applied to solve various subtasks of ontology learning.\nHowever, this partial ontology learning does not capture the interactions\nbetween subtasks. We address this gap by introducing OLLM, a general and\nscalable method for building the taxonomic backbone of an ontology from\nscratch. Rather than focusing on subtasks, like individual relations between\nentities, we model entire subcomponents of the target ontology by finetuning an\nLLM with a custom regulariser that reduces overfitting on high-frequency\nconcepts. We introduce a novel suite of metrics for evaluating the quality of\nthe generated ontology by measuring its semantic and structural similarity to\nthe ground truth. In contrast to standard metrics, our metrics use deep\nlearning techniques to define more robust distance measures between graphs.\nBoth our quantitative and qualitative results on Wikipedia show that OLLM\noutperforms subtask composition methods, producing more semantically accurate\nontologies while maintaining structural integrity. We further demonstrate that\nour model can be effectively adapted to new domains, like arXiv, needing only a\nsmall number of training examples. Our source code and datasets are available\nat https://github.com/andylolu2/ollm."},{"date":"2024-10","title":"BioNCERE: Non-Contrastive Enhancement For Relation Extraction In Biomedical Texts","author":"Farshad Noravesh","link":"http://arxiv.org/abs/2410.23583v1","abstract":"State-of-the-art models for relation extraction (RE) in the biomedical domain\nconsider finetuning BioBERT using classification, but they may suffer from the\nanisotropy problem. Contrastive learning methods can reduce this anisotropy\nphenomena, and also help to avoid class collapse in any classification problem.\nIn the present paper, a new training method called biological non-contrastive\nrelation extraction (BioNCERE) is introduced for relation extraction without\nusing any named entity labels for training to reduce annotation costs. BioNCERE\nuses transfer learning and non-contrastive learning to avoid full or\ndimensional collapse as well as bypass overfitting. It resolves RE in three\nstages by leveraging transfer learning two times. By freezing the weights\nlearned in previous stages in the proposed pipeline and by leveraging\nnon-contrastive learning in the second stage, the model predicts relations\nwithout any knowledge of named entities. Experiments have been done on SemMedDB\nthat are almost similar to State-of-the-art performance on RE without using the\ninformation of named entities."},{"date":"2024-10","title":"Bayesian Hierarchical Model for Synthesizing Registry and Survey Data on Female Breast Cancer Prevalence","author":"Qiao Wang, Chester Lee Schmaltz, Jeannette Jackson-Thompson, Dongchu Sun, Zhuoqiong He, Zhongheng Cai, and Hwanhee Hong","link":"http://arxiv.org/abs/2410.23580v1","abstract":"In public health, it is critical for policymakers to assess the relationship\nbetween the disease prevalence and associated risk factors or clinical\ncharacteristics, facilitating effective resources allocation. However, for\ndiseases like female breast cancer (FBC), reliable prevalence data at specific\ngeographical levels, such as the county-level, are limited because the gold\nstandard data typically come from long-term cancer registries, which do not\nnecessarily collect needed risk factors. In addition, it remains unclear\nwhether fitting each model separately or jointly results in better estimation.\nIn this paper, we identify two data sources to produce reliable county-level\nprevalence estimates in Missouri, USA: the population-based Missouri Cancer\nRegistry (MCR) and the survey-based Missouri County-Level Study (CLS). We\npropose a two-stage Bayesian model to synthesize these sources, accounting for\ntheir differences in the methodological design, case definitions, and collected\ninformation. The first stage involves estimating the county-level FBC\nprevalence using the raking method for CLS data and the counting method for MCR\ndata, calibrating the differences in the methodological design and case\ndefinition. The second stage includes synthesizing two sources with different\nsets of covariates using a Bayesian generalized linear mixed model with\nZeller-Siow prior for the coefficients. Our data analyses demonstrate that\nusing both data sources have better results than at least one data source, and\nincluding a data source membership matters when there exist systematic\ndifferences in these sources. Finally, we translate results into policy making\nand discuss methodological differences for data synthesis of registry and\nsurvey data."},{"date":"2024-10","title":"A comparative study of dynamic models for gravity-driven particle-laden flows","author":"Wing Pok Lee, Jonathan D. Woo, Luke F. Triplett, Yifan Gu, Sarah C. Burnett, Lingyun Ding, and Andrea L. Bertozzi","link":"http://arxiv.org/abs/2410.23561v1","abstract":"The dynamics of viscous thin-film particle-laden flows down inclined surfaces\nare commonly modeled with one of two approaches: a diffusive flux model or a\nsuspension balance model. The diffusive flux model assumes that the particles\nmigrate via a diffusive flux induced by gradients in both the particle\nconcentration and the effective suspension viscosity. The suspension balance\nmodel introduces non-Newtonian bulk stress with shear-induced normal stresses,\nthe gradients of which cause particle migration. Both models have appeared in\nthe literature of particle-laden flow with virtually no comparison between the\ntwo models. For particle-laden viscous flow on an incline, in a thin-film\ngeometry, one can use lubrication theory to derive a compact dynamic model in\nthe form of a $2\\times 2$ system of conservation laws. We can then directly\ncompare the two theories side by side by looking at similarities and\ndifferences in the flux functions for the conservation laws, and in exact and\nnumerical simulations of the equations. We compare the flux profiles over a\nrange of parameters, showing fairly good agreement between the models, with the\nbiggest difference involving the behavior at the free surface. We also consider\nless dense suspensions at lower inclination angles where the dynamics involve\ntwo shock waves that can be clearly measured in experiments. In this context\nthe solutions differ by no more than about 10%, suggesting that either model\ncould be used for this configuration."},{"date":"2024-10","title":"Transferable Ensemble Black-box Jailbreak Attacks on Large Language Models","author":"Yiqi Yang, and Hongye Fu","link":"http://arxiv.org/abs/2410.23558v1","abstract":"In this report, we propose a novel black-box jailbreak attacking framework\nthat incorporates various LLM-as-Attacker methods to deliver transferable and\npowerful jailbreak attacks. Our method is designed based on three key\nobservations from existing jailbreaking studies and practices. First, we\nconsider an ensemble approach should be more effective in exposing the\nvulnerabilities of an aligned LLM compared to individual attacks. Second,\ndifferent malicious instructions inherently vary in their jailbreaking\ndifficulty, necessitating differentiated treatment to ensure more efficient\nattacks. Finally, the semantic coherence of a malicious instruction is crucial\nfor triggering the defenses of an aligned LLM; therefore, it must be carefully\ndisrupted to manipulate its embedding representation, thereby increasing the\njailbreak success rate. We validated our approach by participating in the\nCompetition for LLM and Agent Safety 2024, where our team achieved top\nperformance in the Jailbreaking Attack Track."},{"date":"2024-10","title":"Language-guided Hierarchical Fine-grained Image Forgery Detection and Localization","author":"Xiao Guo, Xiaohong Liu, Iacopo Masi, and Xiaoming Liu","link":"http://arxiv.org/abs/2410.23556v1","abstract":"Differences in forgery attributes of images generated in CNN-synthesized and\nimage-editing domains are large, and such differences make a unified image\nforgery detection and localization (IFDL) challenging. To this end, we present\na hierarchical fine-grained formulation for IFDL representation learning.\nSpecifically, we first represent forgery attributes of a manipulated image with\nmultiple labels at different levels. Then, we perform fine-grained\nclassification at these levels using the hierarchical dependency between them.\nAs a result, the algorithm is encouraged to learn both comprehensive features\nand the inherent hierarchical nature of different forgery attributes. In this\nwork, we propose a Language-guided Hierarchical Fine-grained IFDL, denoted as\nHiFi-Net++. Specifically, HiFi-Net++ contains four components: a multi-branch\nfeature extractor, a language-guided forgery localization enhancer, as well as\nclassification and localization modules. Each branch of the multi-branch\nfeature extractor learns to classify forgery attributes at one level, while\nlocalization and classification modules segment pixel-level forgery regions and\ndetect image-level forgery, respectively. Also, the language-guided forgery\nlocalization enhancer (LFLE), containing image and text encoders learned by\ncontrastive language-image pre-training (CLIP), is used to further enrich the\nIFDL representation. LFLE takes specifically designed texts and the given image\nas multi-modal inputs and then generates the visual embedding and manipulation\nscore maps, which are used to further improve HiFi-Net++ manipulation\nlocalization performance. Lastly, we construct a hierarchical fine-grained\ndataset to facilitate our study. We demonstrate the effectiveness of our method\non $8$ by using different benchmarks for both tasks of IFDL and forgery\nattribute classification. Our source code and dataset are available."},{"date":"2024-10","title":"On-demand microfluidic droplet pinching and splitting under local confinement gradients","author":"Margaux Kerdraon, Albane Th\u00e9ry, Marc Pascual, St\u00e9phanie Descroix, and Marie-Caroline Jullien","link":"http://arxiv.org/abs/2410.23538v1","abstract":"We report the pinching of an elongated liquid droplet that is confined in a\nrectangular microchannel. The droplet pinching is induced by a local variation\nof the channel topography and can lead to its break-up. The modification of the\nchannel topography is either induced by a reversible local dilation of the\nchannel bottom wall or by a confinement gradient that is irreversibly modelled\nin the channel. Interestingly, in both cases, a few micrometres contraction of\nthe channel height leads to the pinching of the droplet in the direction of the\nchannel width. If this pinching is such that the droplet is no longer confined\nat the level of its deformation, {\\it i.e.} the liquid thread becomes 3D, the\ndroplet splits in two. By minimizing the surface energy of the droplet under\nthe channel confinement gradient, we are able to predict its pinching or its\nbreak-up, if any. The dynamic of the droplet deformation is then captured by a\nsemi-analytical model, assuming that most of the viscous dissipation is located\nin the gutters at the four corners of the channel. Conversely, we show that\nthis kinetic model can be used to extract the average topographic variation of\nthe channel, down to few micrometers, by measuring the droplet pinching\ndynamics under a classical optical microscope."},{"date":"2024-10","title":"ALISE: Accelerating Large Language Model Serving with Speculative Scheduling","author":"Youpeng Zhao, and Jun Wang","link":"http://arxiv.org/abs/2410.23537v1","abstract":"Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."},{"date":"2024-10","title":"There and Back Again: On the relation between noises, images, and their inversions in diffusion models","author":"\u0141ukasz Staniszewski, \u0141ukasz Kuci\u0144ski, and Kamil Deja","link":"http://arxiv.org/abs/2410.23530v1","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) achieve state-of-the-art\nperformance in synthesizing new images from random noise, but they lack\nmeaningful latent space that encodes data into features. Recent DDPM-based\nediting techniques try to mitigate this issue by inverting images back to their\napproximated staring noise. In this work, we study the relation between the\ninitial Gaussian noise, the samples generated from it, and their corresponding\nlatent encodings obtained through the inversion procedure. First, we interpret\ntheir spatial distance relations to show the inaccuracy of the DDIM inversion\ntechnique by localizing latent representations manifold between the initial\nnoise and generated samples. Then, we demonstrate the peculiar relation between\ninitial Gaussian noise and its corresponding generations during diffusion\ntraining, showing that the high-level features of generated images stabilize\nrapidly, keeping the spatial distance relationship between noises and\ngenerations consistent throughout the training."},{"date":"2024-10","title":"Large Language Models for Patient Comments Multi-Label Classification","author":"Hajar Sakai, Sarah S. Lam, Mohammadsadegh Mikaeili, Joshua Bosire, and Franziska Jovin","link":"http://arxiv.org/abs/2410.23528v2","abstract":"Patient experience and care quality are crucial for a hospital's\nsustainability and reputation. The analysis of patient feedback offers valuable\ninsight into patient satisfaction and outcomes. However, the unstructured\nnature of these comments poses challenges for traditional machine learning\nmethods following a supervised learning paradigm. This is due to the\nunavailability of labeled data and the nuances these texts encompass. This\nresearch explores leveraging Large Language Models (LLMs) in conducting\nMulti-label Text Classification (MLTC) of inpatient comments shared after a\nstay in the hospital. GPT-4 Turbo was leveraged to conduct the classification.\nHowever, given the sensitive nature of patients' comments, a security layer is\nintroduced before feeding the data to the LLM through a Protected Health\nInformation (PHI) detection framework, which ensures patients'\nde-identification. Additionally, using the prompt engineering framework,\nzero-shot learning, in-context learning, and chain-of-thought prompting were\nexperimented with. Results demonstrate that GPT-4 Turbo, whether following a\nzero-shot or few-shot setting, outperforms traditional methods and Pre-trained\nLanguage Models (PLMs) and achieves the highest overall performance with an\nF1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the\nfew-shot learning results. Subsequently, the results' association with other\npatient experience structured variables (e.g., rating) was conducted. The study\nenhances MLTC through the application of LLMs, offering healthcare\npractitioners an efficient method to gain deeper insights into patient feedback\nand deliver prompt, appropriate responses."},{"date":"2024-10","title":"LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models","author":"Hieu Tran, Junda Wang, Yujan Ting, Weijing Huang, and Terrence Chen","link":"http://arxiv.org/abs/2410.23526v1","abstract":"Large language models (LLMs) have shown remarkable capabilities in various\nnatural language processing tasks, yet they often struggle with maintaining\nfactual accuracy, particularly in knowledge-intensive domains like healthcare.\nThis study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking,\na novel approach designed to enhance the factual reliability of LLMs, with a\nfocus on medical question answering (QA). LEAF utilizes a dual strategy to\nenhance the factual accuracy of responses from models such as Llama 3 70B\nInstruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG,\nimproves Retrieval-Augmented Generation (RAG) by incorporating fact-checking\nresults to guide the retrieval process without updating model parameters. The\nsecond strategy, Learning from Fact-Checks via Self-Training, involves\nsupervised fine-tuning (SFT) on fact-checked responses or applying Simple\nPreference Optimization (SimPO) with fact-checking as a ranking mechanism, both\nupdating LLM parameters from supervision. These findings suggest that\nintegrating fact-checked responses whether through RAG enhancement or\nself-training enhances the reliability and factual correctness of LLM outputs,\noffering a promising solution for applications where information accuracy is\ncrucial."},{"date":"2024-10","title":"LBurst: Learning-Based Robotic Burst Feature Extraction for 3D Reconstruction in Low Light","author":"Ahalya Ravendran, Mitch Bryson, and Donald G. Dansereau","link":"http://arxiv.org/abs/2410.23522v1","abstract":"Drones have revolutionized the fields of aerial imaging, mapping, and\ndisaster recovery. However, the deployment of drones in low-light conditions is\nconstrained by the image quality produced by their on-board cameras. In this\npaper, we present a learning architecture for improving 3D reconstructions in\nlow-light conditions by finding features in a burst. Our approach enhances\nvisual reconstruction by detecting and describing high quality true features\nand less spurious features in low signal-to-noise ratio images. We demonstrate\nthat our method is capable of handling challenging scenes in millilux\nillumination, making it a significant step towards drones operating at night\nand in extremely low-light applications such as underground mining and search\nand rescue operations."},{"date":"2024-10","title":"Dynamic Strategy Planning for Efficient Question Answering with Large Language Models","author":"Tanmay Parekh, Pradyot Prakash, Alexander Radovic, Akshay Shekher, and Denis Savenkov","link":"http://arxiv.org/abs/2410.23511v1","abstract":"Research has shown the effectiveness of reasoning (e.g., Chain-of-Thought),\nplanning (e.g., SelfAsk), and retrieval augmented generation strategies to\nimprove the performance of Large Language Models (LLMs) on various tasks, such\nas question answering. However, using a single fixed strategy to answer\ndifferent kinds of questions is suboptimal in performance and inefficient in\nterms of generated output tokens and performed retrievals. In our work, we\npropose a novel technique DyPlan, to induce a dynamic strategy selection\nprocess in LLMs, to improve performance and reduce costs in question-answering.\nDyPlan incorporates an initial decision step to select the most suitable\nstrategy conditioned on the input question and guides the LLM's response\ngeneration accordingly. We extend DyPlan to DyPlan-verify, adding an internal\nverification and correction process to further enrich the generated answer.\nExperiments on three prominent multi-hop question answering (MHQA) datasets\nreveal how DyPlan can improve model performance by 7-13% while reducing the\ncost by 11-32% relative to the best baseline model."},{"date":"2024-10","title":"Development and Comparative Analysis of Machine Learning Models for Hypoxemia Severity Triage in CBRNE Emergency Scenarios Using Physiological and Demographic Data from Medical-Grade Devices","author":"Santino Nanini, Mariem Abid, Yassir Mamouni, Arnaud Wiedemann, Philippe Jouvet, and Stephane Bourassa","link":"http://arxiv.org/abs/2410.23503v1","abstract":"This paper presents the development of machine learning (ML) models to\npredict hypoxemia severity during emergency triage, especially in Chemical,\nBiological, Radiological, Nuclear, and Explosive (CBRNE) events, using\nphysiological data from medical-grade sensors. Gradient Boosting Models\n(XGBoost, LightGBM, CatBoost) and sequential models (LSTM, GRU) were trained on\nphysiological and demographic data from the MIMIC-III and IV datasets. A robust\npreprocessing pipeline addressed missing data, class imbalances, and\nincorporated synthetic data flagged with masks. Gradient Boosting Models (GBMs)\noutperformed sequential models in terms of training speed, interpretability,\nand reliability, making them well-suited for real-time decision-making. While\ntheir performance was comparable to that of sequential models, the GBMs used\nscore features from six physiological variables derived from the enhanced\nNational Early Warning Score (NEWS) 2, which we termed NEWS2+. This approach\nsignificantly improved prediction accuracy. While sequential models handled\ntemporal data well, their performance gains did not justify the higher\ncomputational cost. A 5-minute prediction window was chosen for timely\nintervention, with minute-level interpolations standardizing the data. Feature\nimportance analysis highlighted the significant role of mask and score features\nin enhancing both transparency and performance. Temporal dependencies proved to\nbe less critical, as Gradient Boosting Models were able to capture key patterns\neffectively without relying on them. This study highlights ML's potential to\nimprove triage and reduce alarm fatigue. Future work will integrate data from\nmultiple hospitals to enhance model generalizability across clinical settings."},{"date":"2024-10","title":"All or None: Identifiable Linear Properties of Next-token Predictors in Language Modeling","author":"Emanuele Marconato, S\u00e9bastien Lachapelle, Sebastian Weichwald, and Luigi Gresele","link":"http://arxiv.org/abs/2410.23501v1","abstract":"We analyze identifiability as a possible explanation for the ubiquity of\nlinear properties across language models, such as the vector difference between\nthe representations of \"easy\" and \"easiest\" being parallel to that between\n\"lucky\" and \"luckiest\". For this, we ask whether finding a linear property in\none model implies that any model that induces the same distribution has that\nproperty, too. To answer that, we first prove an identifiability result to\ncharacterize distribution-equivalent next-token predictors, lifting a diversity\nrequirement of previous results. Second, based on a refinement of relational\nlinearity [Paccanaro and Hinton, 2001; Hernandez et al., 2024], we show how\nmany notions of linearity are amenable to our analysis. Finally, we show that\nunder suitable conditions, these linear properties either hold in all or none\ndistribution-equivalent next-token predictors."},{"date":"2024-10","title":"Causality-Driven Audits of Model Robustness","author":"Nathan Drenkow, Chris Ribaudo, and Mathias Unberath","link":"http://arxiv.org/abs/2410.23494v1","abstract":"Robustness audits of deep neural networks (DNN) provide a means to uncover\nmodel sensitivities to the challenging real-world imaging conditions that\nsignificantly degrade DNN performance in-the-wild. Such conditions are often\nthe result of the compounding of multiple factors inherent to the environment,\nsensor, or processing pipeline and may lead to complex image distortions that\nare not easily categorized. When robustness audits are limited to a set of\npre-determined imaging effects or distortions, the results cannot be (easily)\ntransferred to real-world conditions where image corruptions may be more\ncomplex or nuanced. To address this challenge, we present a new alternative\nrobustness auditing method that uses causal inference to measure DNN\nsensitivities to the factors of the imaging process that cause complex\ndistortions. Our approach uses causal models to explicitly encode assumptions\nabout the domain-relevant factors and their interactions. Then, through\nextensive experiments on natural and rendered images across multiple vision\ntasks, we show that our approach reliably estimates causal effects of each\nfactor on DNN performance using observational domain data. These causal effects\ndirectly tie DNN sensitivities to observable properties of the imaging pipeline\nin the domain of interest towards reducing the risk of unexpected DNN failures\nwhen deployed in that domain."},{"date":"2024-10","title":"Linear and non-linear relational analyses for Quantum Program Optimization","author":"Matthew Amy, and Joseph Lunderville","link":"http://arxiv.org/abs/2410.23493v1","abstract":"The phase folding optimization is a circuit optimization used in many quantum\ncompilers as a fast and effective way of reducing the number of high-cost gates\nin a quantum circuit. However, existing formulations of the optimization rely\non an exact, linear algebraic representation of the circuit, restricting the\noptimization to being performed on straightline quantum circuits or basic\nblocks in a larger quantum program.\n  We show that the phase folding optimization can be re-cast as an \\emph{affine\nrelation analysis}, which allows the direct application of classical techniques\nfor affine relations to extend phase folding to quantum \\emph{programs} with\narbitrarily complicated classical control flow including nested loops and\nprocedure calls. Through the lens of relational analysis, we show that the\noptimization can be powered-up by substituting other classical relational\ndomains, particularly ones for \\emph{non-linear} relations which are useful in\nanalyzing circuits involving classical arithmetic. To increase the precision of\nour analysis and infer non-linear relations from gate sets involving only\nlinear operations -- such as Clifford+$T$ -- we show that the\n\\emph{sum-over-paths} technique can be used to extract precise symbolic\ntransition relations for straightline circuits. Our experiments show that our\nmethods are able to generate and use non-trivial loop invariants for quantum\nprogram optimization, as well as achieve some optimizations of common circuits\nwhich were previously attainable only by hand."},{"date":"2024-10","title":"Keep on Swimming: Real Attackers Only Need Partial Knowledge of a Multi-Model System","author":"Julian Collado, and Kevin Stangl","link":"http://arxiv.org/abs/2410.23483v1","abstract":"Recent approaches in machine learning often solve a task using a composition\nof multiple models or agentic architectures. When targeting a composed system\nwith adversarial attacks, it might not be computationally or informationally\nfeasible to train an end-to-end proxy model or a proxy model for every\ncomponent of the system. We introduce a method to craft an adversarial attack\nagainst the overall multi-model system when we only have a proxy model for the\nfinal black-box model, and when the transformation applied by the initial\nmodels can make the adversarial perturbations ineffective. Current methods\nhandle this by applying many copies of the first model/transformation to an\ninput and then re-use a standard adversarial attack by averaging gradients, or\nlearning a proxy model for both stages. To our knowledge, this is the first\nattack specifically designed for this threat model and our method has a\nsubstantially higher attack success rate (80% vs 25%) and contains 9.4% smaller\nperturbations (MSE) compared to prior state-of-the-art methods. Our experiments\nfocus on a supervised image pipeline, but we are confident the attack will\ngeneralize to other multi-model settings [e.g. a mix of open/closed source\nfoundation models], or agentic systems"},{"date":"2024-10","title":"Graph-Augmented Relation Extraction Model with LLMs-Generated Support Document","author":"Vicky Dong, Hao Yu, and Yao Chen","link":"http://arxiv.org/abs/2410.23452v1","abstract":"This study introduces a novel approach to sentence-level relation extraction\n(RE) that integrates Graph Neural Networks (GNNs) with Large Language Models\n(LLMs) to generate contextually enriched support documents. By harnessing the\npower of LLMs to generate auxiliary information, our approach crafts an\nintricate graph representation of textual data. This graph is subsequently\nprocessed through a Graph Neural Network (GNN) to refine and enrich the\nembeddings associated with each entity ensuring a more nuanced and\ninterconnected understanding of the data. This methodology addresses the\nlimitations of traditional sentence-level RE models by incorporating broader\ncontexts and leveraging inter-entity interactions, thereby improving the\nmodel's ability to capture complex relationships across sentences. Our\nexperiments, conducted on the CrossRE dataset, demonstrate the effectiveness of\nour approach, with notable improvements in performance across various domains.\nThe results underscore the potential of combining GNNs with LLM-generated\ncontext to advance the field of relation extraction."},{"date":"2024-10","title":"Machine learning models with different cheminformatics data sets to forecast the power conversion efficiency of organic solar cells","author":"Omar A. Alvarez-Gonzaga, and Juan I. Rodriguez","link":"http://arxiv.org/abs/2410.23444v1","abstract":"Random Forest (RF) and Gradient Boosting Regression Trees (GBRT) regression\nmodels along with three cheminformatics data sets (RDkit, Mordred, Morgan) have\nbeen used to predict the power conversion efficiency (PCE) of organic solar\ncells (OSCs). The data consists of cheinformatics descriptors of the electron\ndonor used in 433 OSCs for which the experimental PCE (target variable) is\nreported in the literature. The donor is either a polymer or a small organic\nmolecule, and the acceptor the fullerene derivatives PCBM or PC71BM. Unlike\nprevious methods, our ML approach considers the type of donor and the acceptor\nby adding four extra donor's features using the one-hot encoder tool. It is\ndemonstrated that this additional information improves the prediction\nperformance up to 34%. We have also exploited this feature to theoretically\nforecast the PCE of new OSCs by evaluating the ML model for a different\nacceptor. It is predicted that more than 50% of the OCSs obtained by exchanging\nthe acceptor would have higher experimental PCE. The prediction accuracy of a\ngiven ML approach is analyzed for different PCE intervals. RF using RDkit\ndescriptors resulted in the best ML approach with a Pearson's correlation\ncoefficient for the training and testing sets equal to 0.96 and 0.62,\nrespectively."},{"date":"2024-10","title":"Model-free Low-Rank Reinforcement Learning via Leveraged Entry-wise Matrix Estimation","author":"Stefan Stojanovic, Yassir Jedra, and Alexandre Proutiere","link":"http://arxiv.org/abs/2410.23434v1","abstract":"We consider the problem of learning an $\\varepsilon$-optimal policy in\ncontrolled dynamical systems with low-rank latent structure. For this problem,\nwe present LoRa-PI (Low-Rank Policy Iteration), a model-free learning algorithm\nalternating between policy improvement and policy evaluation steps. In the\nlatter, the algorithm estimates the low-rank matrix corresponding to the\n(state, action) value function of the current policy using the following\ntwo-phase procedure. The entries of the matrix are first sampled uniformly at\nrandom to estimate, via a spectral method, the leverage scores of its rows and\ncolumns. These scores are then used to extract a few important rows and columns\nwhose entries are further sampled. The algorithm exploits these new samples to\ncomplete the matrix estimation using a CUR-like method. For this leveraged\nmatrix estimation procedure, we establish entry-wise guarantees that\nremarkably, do not depend on the coherence of the matrix but only on its\nspikiness. These guarantees imply that LoRa-PI learns an $\\varepsilon$-optimal\npolicy using $\\widetilde{O}({S+A\\over \\mathrm{poly}(1-\\gamma)\\varepsilon^2})$\nsamples where $S$ (resp. $A$) denotes the number of states (resp. actions) and\n$\\gamma$ the discount factor. Our algorithm achieves this order-optimal (in\n$S$, $A$ and $\\varepsilon$) sample complexity under milder conditions than\nthose assumed in previously proposed approaches."},{"date":"2024-10","title":"EchoFM: Foundation Model for Generalizable Echocardiogram Analysis","author":"Sekeun Kim, Pengfei Jin, Sifan Song, Cheng Chen, Yiwei Li, Hui Ren, Xiang Li, Tianming Liu, and Quanzheng Li","link":"http://arxiv.org/abs/2410.23413v1","abstract":"Foundation models have recently gained significant attention because of their\ngeneralizability and adaptability across multiple tasks and data distributions.\nAlthough medical foundation models have emerged, solutions for cardiac imaging,\nespecially echocardiography videos, are still unexplored. In this paper, we\nintroduce EchoFM, a foundation model specifically designed to represent and\nanalyze echocardiography videos. In EchoFM, we propose a self-supervised\nlearning framework that captures both spatial and temporal variability patterns\nthrough a spatio-temporal consistent masking strategy and periodic-driven\ncontrastive learning. This framework can effectively capture the\nspatio-temporal dynamics of echocardiography and learn the representative video\nfeatures without any labels. We pre-train our model on an extensive dataset\ncomprising over 290,000 echocardiography videos covering 26 scan views across\ndifferent imaging modes, with up to 20 million frames of images. The\npre-trained EchoFM can then be easily adapted and fine-tuned for a variety of\ndownstream tasks, serving as a robust backbone model. Our evaluation was\nsystemically designed for four downstream tasks after the echocardiography\nexamination routine. Experiment results show that EchoFM surpasses\nstate-of-the-art methods, including specialized echocardiography methods,\nself-supervised pre-training models, and general-purposed pre-trained\nfoundation models, across all downstream tasks."},{"date":"2024-10","title":"TPP-Gaze: Modelling Gaze Dynamics in Space and Time with Neural Temporal Point Processes","author":"Alessandro D'Amelio, Giuseppe Cartella, Vittorio Cuculo, Manuele Lucchi, Marcella Cornia, Rita Cucchiara, and Giuseppe Boccignone","link":"http://arxiv.org/abs/2410.23409v1","abstract":"Attention guides our gaze to fixate the proper location of the scene and\nholds it in that location for the deserved amount of time given current\nprocessing demands, before shifting to the next one. As such, gaze deployment\ncrucially is a temporal process. Existing computational models have made\nsignificant strides in predicting spatial aspects of observer's visual\nscanpaths (where to look), while often putting on the background the temporal\nfacet of attention dynamics (when). In this paper we present TPP-Gaze, a novel\nand principled approach to model scanpath dynamics based on Neural Temporal\nPoint Process (TPP), that jointly learns the temporal dynamics of fixations\nposition and duration, integrating deep learning methodologies with point\nprocess theory. We conduct extensive experiments across five publicly available\ndatasets. Our results show the overall superior performance of the proposed\nmodel compared to state-of-the-art approaches. Source code and trained models\nare publicly available at: https://github.com/phuselab/tppgaze."},{"date":"2024-10","title":"FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions","author":"Anuroop Sriram, Benjamin Kurt Miller, Ricky T. Q. Chen, and Brandon M. Wood","link":"http://arxiv.org/abs/2410.23405v1","abstract":"Material discovery is a critical area of research with the potential to\nrevolutionize various fields, including carbon capture, renewable energy, and\nelectronics. However, the immense scale of the chemical space makes it\nchallenging to explore all possible materials experimentally. In this paper, we\nintroduce FlowLLM, a novel generative model that combines large language models\n(LLMs) and Riemannian flow matching (RFM) to design novel crystalline\nmaterials. FlowLLM first fine-tunes an LLM to learn an effective base\ndistribution of meta-stable crystals in a text representation. After converting\nto a graph representation, the RFM model takes samples from the LLM and\niteratively refines the coordinates and lattice parameters. Our approach\nsignificantly outperforms state-of-the-art methods, increasing the generation\nrate of stable materials by over three times and increasing the rate for\nstable, unique, and novel crystals by $\\sim50\\%$ - a huge improvement on a\ndifficult problem. Additionally, the crystals generated by FlowLLM are much\ncloser to their relaxed state when compared with another leading model,\nsignificantly reducing post-hoc computational cost."},{"date":"2024-10","title":"Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective","author":"Haixiang sun, and Ye Shi","link":"http://arxiv.org/abs/2410.23391v1","abstract":"Deep Equilibrium Model (DEQ), which serves as a typical implicit neural\nnetwork, emphasizes their memory efficiency and competitive performance\ncompared to explicit neural networks. However, there has been relatively\nlimited theoretical analysis on the representation of DEQ. In this paper, we\nutilize the Neural Collapse ($\\mathcal{NC}$) as a tool to systematically\nanalyze the representation of DEQ under both balanced and imbalanced\nconditions. $\\mathcal{NC}$ is an interesting phenomenon in the neural network\ntraining process that characterizes the geometry of class features and\nclassifier weights. While extensively studied in traditional explicit neural\nnetworks, the $\\mathcal{NC}$ phenomenon has not received substantial attention\nin the context of implicit neural networks. We theoretically show that\n$\\mathcal{NC}$ exists in DEQ under balanced conditions. Moreover, in imbalanced\nsettings, despite the presence of minority collapse, DEQ demonstrated\nadvantages over explicit neural networks. These advantages include the\nconvergence of extracted features to the vertices of a simplex equiangular\ntight frame and self-duality properties under mild conditions, highlighting\nDEQ's superiority in handling imbalanced datasets. Finally, we validate our\ntheoretical analyses through experiments in both balanced and imbalanced\nscenarios."},{"date":"2024-10","title":"STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG","author":"Raquel Fern\u00e1ndez-Mart\u00edn, Alfonso Gij\u00f3n, Odile Feys, Elodie Juven\u00e9, Alec Aeby, Charline Urbain, Xavier De Ti\u00e8ge, and Vincent Wens","link":"http://arxiv.org/abs/2410.23386v1","abstract":"Magnetoencephalography (MEG) allows the non-invasive detection of interictal\nepileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients\ntraditionally relies on the visual identification of IEDs, which is time\nconsuming and partially subjective. Automatic, data-driven detection methods\nexist but show limited performance. Still, the rise of deep learning (DL)-with\nits ability to reproduce human-like abilities-could revolutionize clinical MEG\npractice. Here, we developed and validated STIED, a simple yet powerful\nsupervised DL algorithm combining two convolutional neural networks with\ntemporal (1D time-course) and spatial (2D topography) features of MEG signals\ninspired from current clinical guidelines. Our DL model enabled both temporal\nand spatial localization of IEDs in patients suffering from focal epilepsy with\nfrequent and high amplitude spikes (FE group), with high-performance\nmetrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning\nfrom spatiotemporal features of IEDs. This performance can be attributed to our\nhandling of input data, which mimics established clinical MEG practice. Reverse\nengineering further revealed that STIED encodes fine spatiotemporal features of\nIEDs rather than their mere amplitude. The model trained on the FE group also\nshowed promising results when applied to a separate group of presurgical\npatients with different types of refractory focal epilepsy, though further work\nis needed to distinguish IEDs from physiological transients. This study paves\nthe way of incorporating STIED and DL algorithms into the routine clinical MEG\nevaluation of epilepsy."},{"date":"2024-10","title":"ASURA-FDPS-ML: Star-by-star Galaxy Simulations Accelerated by Surrogate Modeling for Supernova Feedback","author":"Keiya Hirashima, Kana Moriwaki, Michiko S. Fujii, Yutaka Hirai, Takayuki R. Saitoh, Junnichiro Makino, Ulrich P. Steinwandel, and Shirley Ho","link":"http://arxiv.org/abs/2410.23346v1","abstract":"We introduce new high-resolution galaxy simulations accelerated by a\nsurrogate model that reduces the computation cost by approximately 75 percent.\nMassive stars with a Zero Age Main Sequence mass of about 8 solar masses and\nabove explode as core-collapse supernovae (CCSNe), which play a critical role\nin galaxy formation. The energy released by CCSNe is essential for regulating\nstar formation and driving feedback processes in the interstellar medium (ISM).\nHowever, the short integration timesteps required for SNe feedback present\nsignificant bottlenecks in star-by-star galaxy simulations that aim to capture\nindividual stellar dynamics and the inhomogeneous shell expansion of SNe within\nthe turbulent ISM. Our new framework combines direct numerical simulations and\nsurrogate modeling, including machine learning and Gibbs sampling. The star\nformation history and the time evolution of outflow rates in the galaxy match\nthose obtained from resolved direct numerical simulations. Our new approach\nachieves high-resolution fidelity while reducing computational costs,\neffectively bridging the physical scale gap and enabling multi-scale\nsimulations."},{"date":"2024-10","title":"Capturing Turbulence with Numerical Dissipation: a Simple Dynamical Model for Unresolved Turbulence in Hydrodynamic Simulations","author":"Vadim A. Semenov","link":"http://arxiv.org/abs/2410.23339v1","abstract":"Modeling unresolved turbulence in astrophysical gasdynamic simulations can\nimprove the modeling of other subgrid processes dependent on the turbulent\nstructure of gas: from flame propagation in the interiors of combusting white\ndwarfs to star formation and chemical reaction rates in the interstellar\nmedium, and non-thermal pressure support of circum- and intergalactic gas. We\npresent a simple method for modeling unresolved turbulence in hydrodynamic\nsimulations via tracking its sourcing by local numerical dissipation and\nmodeling its decay into heat. This method is physically justified by the\ngeneric property of turbulent flows that they dissipate kinetic energy at a\nrate set by the energy cascade rate from large scales, which is independent of\nfluid viscosity regardless of its nature, be it physical or numerical. We\ncalibrate and test our model against decaying supersonic turbulence\nsimulations. Despite its simplicity, the model quantitatively reproduces\nmultiple non-trivial features of the high-resolution turbulence run: the\ntemporal evolution of the average small-scale turbulence, its dependence on\nspatial scale, and the slope and scatter of the local correlation between\nsubgrid turbulent velocities, gas densities, and local compression rates. As an\nexample of practical applications, we use our model in isolated galactic disk\nsimulations to model locally variable star formation efficiency at the\nsubresolution scale. In the supersonic, star-forming gas, the new model\nperforms comparably to a more sophisticated model where the turbulent cascade\nis described by explicit subgrid terms. Our new model is straightforward to\nimplement in many hydrodynamic codes used in galaxy simulations as it utilizes\nalready existing infrastructure to implicitly track the numerical dissipation\nin such codes."},{"date":"2024-10","title":"Provable acceleration for diffusion models under minimal assumptions","author":"Gen Li, and Changxiao Cai","link":"http://arxiv.org/abs/2410.23285v1","abstract":"While score-based diffusion models have achieved exceptional sampling\nquality, their sampling speeds are often limited by the high computational\nburden of score function evaluations. Despite the recent remarkable empirical\nadvances in speeding up the score-based samplers, theoretical understanding of\nacceleration techniques remains largely limited. To bridge this gap, we propose\na novel training-free acceleration scheme for stochastic samplers. Under\nminimal assumptions -- namely, $L^2$-accurate score estimates and a finite\nsecond-moment condition on the target distribution -- our accelerated sampler\nprovably achieves $\\varepsilon$-accuracy in total variation within\n$\\widetilde{O}(d^{5/4}/\\sqrt{\\varepsilon})$ iterations, thereby significantly\nimproving upon the $\\widetilde{O}(d/\\varepsilon)$ iteration complexity of\nstandard score-based samplers. Notably, our convergence theory does not rely on\nrestrictive assumptions on the target distribution or higher-order score\nestimation guarantees."},{"date":"2024-10","title":"OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction","author":"Hongbo Zhao, Lue Fan, Yuntao Chen, Haochen Wang, yuran Yang, Xiaojuan Jin, Yixin Zhang, Gaofeng Meng, and Zhaoxiang Zhang","link":"http://arxiv.org/abs/2410.23278v1","abstract":"In this paper, we propose OpenSatMap, a fine-grained, high-resolution\nsatellite dataset for large-scale map construction. Map construction is one of\nthe foundations of the transportation industry, such as navigation and\nautonomous driving. Extracting road structures from satellite images is an\nefficient way to construct large-scale maps. However, existing satellite\ndatasets provide only coarse semantic-level labels with a relatively low\nresolution (up to level 19), impeding the advancement of this field. In\ncontrast, the proposed OpenSatMap (1) has fine-grained instance-level\nannotations; (2) consists of high-resolution images (level 20); (3) is\ncurrently the largest one of its kind; (4) collects data with high diversity.\nMoreover, OpenSatMap covers and aligns with the popular nuScenes dataset and\nArgoverse 2 dataset to potentially advance autonomous driving technologies. By\npublishing and maintaining the dataset, we provide a high-quality benchmark for\nsatellite-based map construction and downstream tasks like autonomous driving."},{"date":"2024-10","title":"TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models","author":"Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan","link":"http://arxiv.org/abs/2410.23266v1","abstract":"Existing benchmarks often highlight the remarkable performance achieved by\nstate-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal\ncontext for video understanding. However, how well do the models truly perform\nvisual temporal reasoning? Our study of existing benchmarks shows that this\ncapability of MFMs is likely overestimated as many questions can be solved by\nusing a single, few, or out-of-order frames. To systematically examine current\nvisual temporal reasoning tasks, we propose three principles with corresponding\nmetrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame\nInformation Disparity. Following these principles, we introduce TOMATO,\nTemporal Reasoning Multimodal Evaluation, a novel benchmark crafted to\nrigorously assess MFMs' temporal reasoning capabilities in video understanding.\nTOMATO comprises 1,484 carefully curated, human-annotated questions spanning\nsix tasks (i.e., action count, direction, rotation, shape & trend, velocity &\nfrequency, and visual cues), applied to 1,417 videos, including 805\nself-recorded and -generated videos, that encompass human-centric, real-world,\nand simulated scenarios. Our comprehensive evaluation reveals a human-model\nperformance gap of 57.3% with the best-performing model. Moreover, our in-depth\nanalysis uncovers more fundamental limitations beyond this gap in current MFMs.\nWhile they can accurately recognize events in isolated frames, they fail to\ninterpret these frames as a continuous sequence. We believe TOMATO will serve\nas a crucial testbed for evaluating the next-generation MFMs and as a call to\nthe community to develop AI systems capable of comprehending human world\ndynamics through the video modality."},{"date":"2024-10","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","author":"Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, James Guo, Dragomir Anguelov, and Mingxing Tan","link":"http://arxiv.org/abs/2410.23262v1","abstract":"We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures."},{"date":"2024-10","title":"Keypoint Abstraction using Large Models for Object-Relative Imitation Learning","author":"Xiaolin Fang, Bo-Ruei Huang, Jiayuan Mao, Jasmine Shone, Joshua B. Tenenbaum, Tom\u00e1s Lozano-P\u00e9rez, and Leslie Pack Kaelbling","link":"http://arxiv.org/abs/2410.23254v1","abstract":"Generalization to novel object configurations and instances across diverse\ntasks and environments is a critical challenge in robotics. Keypoint-based\nrepresentations have been proven effective as a succinct representation for\ncapturing essential object features, and for establishing a reference frame in\naction prediction, enabling data-efficient learning of robot skills. However,\ntheir manual design nature and reliance on additional human labels limit their\nscalability. In this paper, we propose KALM, a framework that leverages large\npre-trained vision-language models (LMs) to automatically generate\ntask-relevant and cross-instance consistent keypoints. KALM distills robust and\nconsistent keypoints across views and objects by generating proposals using LMs\nand verifies them against a small set of robot demonstration data. Based on the\ngenerated keypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to generalize\neffectively across varying object poses, camera views, and object instances\nwith similar functional shapes. Our method demonstrates strong performance in\nthe real world, adapting to different tasks and environments from only a\nhandful of demonstrations while requiring no additional labels. Website:\nhttps://kalm-il.github.io/"},{"date":"2024-10","title":"Emergence of meta-stable clustering in mean-field transformer models","author":"Giuseppe Bruno, Federico Pasqualotto, and Andrea Agazzi","link":"http://arxiv.org/abs/2410.23228v1","abstract":"We model the evolution of tokens within a deep stack of Transformer layers as\na continuous-time flow on the unit sphere, governed by a mean-field interacting\nparticle system, building on the framework introduced in (Geshkovski et al.,\n2023). Studying the corresponding mean-field Partial Differential Equation\n(PDE), which can be interpreted as a Wasserstein gradient flow, in this paper\nwe provide a mathematical investigation of the long-term behavior of this\nsystem, with a particular focus on the emergence and persistence of meta-stable\nphases and clustering phenomena, key elements in applications like next-token\nprediction. More specifically, we perform a perturbative analysis of the\nmean-field PDE around the iid uniform initialization and prove that, in the\nlimit of large number of tokens, the model remains close to a meta-stable\nmanifold of solutions with a given structure (e.g., periodicity). Further, the\nstructure characterizing the meta-stable manifold is explicitly identified, as\na function of the inverse temperature parameter of the model, by the index\nmaximizing a certain rescaling of Gegenbauer polynomials."},{"date":"2024-10","title":"Partial Channel Dependence with Channel Masks for Time Series Foundation Models","author":"Seunghan Lee, Taeyoung Park, and Kibok Lee","link":"http://arxiv.org/abs/2410.23222v1","abstract":"Recent advancements in foundation models have been successfully extended to\nthe time series (TS) domain, facilitated by the emergence of large-scale TS\ndatasets. However, previous efforts have primarily focused on designing model\narchitectures to address explicit heterogeneity among datasets such as various\nnumbers of channels, while often overlooking implicit heterogeneity such as\nvarying dependencies between channels. In this work, we introduce the concept\nof partial channel dependence (PCD), which enables a more sophisticated\nadjustment of channel dependencies based on dataset-specific information. To\nachieve PCD, we propose a channel mask that captures the relationships between\nchannels within a dataset using two key components: 1) a correlation matrix\nthat encodes relative dependencies between channels, and 2) domain parameters\nthat learn the absolute dependencies specific to each dataset, refining the\ncorrelation matrix. We validate the effectiveness of PCD across four tasks in\nTS including forecasting, classification, imputation, and anomaly detection,\nunder diverse settings, including few-shot and zero-shot scenarios with both TS\nfoundation models and single-task models. Code is available at\nhttps://github.com/seunghan96/CM."},{"date":"2024-10","title":"OS-ATLAS: A Foundation Action Model for Generalist GUI Agents","author":"Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao","link":"http://arxiv.org/abs/2410.23218v1","abstract":"Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs."},{"date":"2024-10","title":"A Catalog of First-Order Electroweak Phase Transitions in the Standard Model Effective Field Theory","author":"Eliel Camargo-Molina, Rikard Enberg, and Johan L\u00f6fgren","link":"http://arxiv.org/abs/2410.23210v1","abstract":"We use modern dimensionally-reduced effective field theory methods, with\ncareful attention to scale hierarchies, to analyze and catalog the types of\nfirst-order electroweak phase transitions that are possible in the Standard\nModel Effective Field Theory (SMEFT). Our calculations lay the necessary\ngroundwork to perform gauge invariant, properly resummed perturbative\nexpansions, and therefore address many of the theoretical problems with phase\ntransition calculations. We find three types of configurations of the scalar\npotential that allow for a first-order phase transition, namely tree-level\nbarriers, radiative barriers, or radiative symmetry breaking through the\nColeman-Weinberg mechanism. We also find versions of these with significant\nsupercooling. We perform a global likelihood scan over the Wilson coefficients\nof SMEFT operators involving only the Higgs field, to identify parameter\nregions that exhibit these first-order phase transitions and are consistent\nwith experimental and theoretical constraints. We comment on the possibilities\nfor electroweak baryogenesis within the SMEFT, and roughly estimate if the\ngravitational wave spectra generated by the phase transitions are detectable."},{"date":"2024-10","title":"Reliability of Topic Modeling","author":"Kayla Schroeder, and Zach Wood-Doughty","link":"http://arxiv.org/abs/2410.23186v1","abstract":"Topic models allow researchers to extract latent factors from text data and\nuse those variables in downstream statistical analyses. However, these\nmethodologies can vary significantly due to initialization differences,\nrandomness in sampling procedures, or noisy data. Reliability of these methods\nis of particular concern as many researchers treat learned topic models as\nground truth for subsequent analyses. In this work, we show that the standard\npractice for quantifying topic model reliability fails to capture essential\naspects of the variation in two widely-used topic models. Drawing from a\nextensive literature on measurement theory, we provide empirical and\ntheoretical analyses of three other metrics for evaluating the reliability of\ntopic models. On synthetic and real-world data, we show that McDonald's\n$\\omega$ provides the best encapsulation of reliability. This metric provides\nan essential tool for validation of topic model methodologies that should be a\nstandard component of any topic model-based research."},{"date":"2024-10","title":"TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters","author":"Haiyang Wang, Yue Fan, Muhammad Ferjad Naeem, Yongqin Xian, Jan Eric Lenssen, Liwei Wang, Federico Tombari, and Bernt Schiele","link":"http://arxiv.org/abs/2410.23168v1","abstract":"Transformers have become the predominant architecture in foundation models\ndue to their excellent performance across various domains. However, the\nsubstantial cost of scaling these models remains a significant concern. This\nproblem arises primarily from their dependence on a fixed number of parameters\nwithin linear projections. When architectural modifications (e.g., channel\ndimensions) are introduced, the entire model typically requires retraining from\nscratch. As model sizes continue growing, this strategy results in increasingly\nhigh computational costs and becomes unsustainable. To overcome this problem,\nwe introduce TokenFormer, a natively scalable architecture that leverages the\nattention mechanism not only for computations among input tokens but also for\ninteractions between tokens and model parameters, thereby enhancing\narchitectural flexibility. By treating model parameters as tokens, we replace\nall the linear projections in Transformers with our token-parameter attention\nlayer, where input tokens act as queries and model parameters as keys and\nvalues. This reformulation allows for progressive and efficient scaling without\nnecessitating retraining from scratch. Our model scales from 124M to 1.4B\nparameters by incrementally adding new key-value parameter pairs, achieving\nperformance comparable to Transformers trained from scratch while greatly\nreducing training costs. Code and models are available at\n\\url{https://github.com/Haiyang-W/TokenFormer}."},{"date":"2024-10","title":"SciPIP: An LLM-based Scientific Paper Idea Proposer","author":"Wenxiao Wang, Lihui Gu, Liye Zhang, Yunxiang Luo, Yi Dai, Chen Shen, Liang Xie, Binbin Lin, Xiaofei He, and Jieping Ye","link":"http://arxiv.org/abs/2410.23166v1","abstract":"The exponential growth of knowledge and the increasing complexity of\ninterdisciplinary research pose significant challenges for researchers,\nincluding information overload and difficulties in exploring novel ideas. The\nadvancements in large language models (LLMs), such as GPT-4, have shown great\npotential in enhancing idea proposals, but how to effectively utilize large\nmodels for reasonable idea proposal has not been thoroughly explored. This\npaper proposes a scientific paper idea proposer (SciPIP). Based on a\nuser-provided research background, SciPIP retrieves helpful papers from a\nliterature database while leveraging the capabilities of LLMs to generate more\nnovel and feasible ideas. To this end, 1) we construct a literature retrieval\ndatabase, extracting lots of papers' multi-dimension information for fast\naccess. Then, a literature retrieval method based on semantics, entity, and\ncitation co-occurrences is proposed to search relevant literature from multiple\naspects based on the user-provided background. 2) After literature retrieval,\nwe introduce dual-path idea proposal strategies, where one path infers\nsolutions from the retrieved literature and the other path generates original\nideas through model brainstorming. We then combine the two to achieve a good\nbalance between feasibility and originality. Through extensive experiments on\nthe natural language processing (NLP) field, we demonstrate that SciPIP can\nretrieve citations similar to those of existing top conference papers and\ngenerate many ideas consistent with them. Additionally, we evaluate the\noriginality of other ideas generated by SciPIP using large language models,\nfurther validating the effectiveness of our proposed method. The code and the\ndatabase are released at https://github.com/cheerss/SciPIP."},{"date":"2024-10","title":"FlexTSF: A Universal Forecasting Model for Time Series with Variable Regularities","author":"Jingge Xiao, Yile Chen, Gao Cong, Wolfgang Nejdl, and Simon Gottschalk","link":"http://arxiv.org/abs/2410.23160v1","abstract":"Developing a foundation model for time series forecasting across diverse\ndomains has attracted significant attention in recent years. Existing works\ntypically assume regularly sampled, well-structured data, limiting their\napplicability to more generalized scenarios where time series often contain\nmissing values, unequal sequence lengths, and irregular time intervals between\nmeasurements. To cover diverse domains and handle variable regularities, we\npropose FlexTSF, a universal time series forecasting model that possesses\nbetter generalization and natively support both regular and irregular time\nseries. FlexTSF produces forecasts in an autoregressive manner and incorporates\nthree novel designs: VT-Norm, a normalization strategy to ablate data domain\nbarriers, IVP Patcher, a patching module to learn representations from flexibly\nstructured time series, and LED attention, an attention mechanism to seamlessly\nintegrate these two and propagate forecasts with awareness of domain and time\ninformation. Experiments on 12 datasets show that FlexTSF outperforms\nstate-of-the-art forecasting models respectively designed for regular and\nirregular time series. Furthermore, after self-supervised pre-training, FlexTSF\nshows exceptional performance in both zero-shot and few-show settings for time\nseries forecasting."},{"date":"2024-10","title":"VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning","author":"Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, Jo\u00e3o F. Henriques, and Kevin Ellis","link":"http://arxiv.org/abs/2410.23156v1","abstract":"Broadly intelligent agents should form task-specific abstractions that\nselectively expose the essential elements of a task, while abstracting away the\ncomplexity of the raw sensorimotor space. In this work, we present\nNeuro-Symbolic Predicates, a first-order abstraction language that combines the\nstrengths of symbolic and neural knowledge representations. We outline an\nonline algorithm for inventing such predicates and learning abstract world\nmodels. We compare our approach to hierarchical reinforcement learning,\nvision-language model planning, and symbolic predicate invention approaches, on\nboth in- and out-of-distribution tasks across five simulated robotic domains.\nResults show that our approach offers better sample complexity, stronger\nout-of-distribution generalization, and improved interpretability."},{"date":"2024-10","title":"FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training","author":"Tejaswini Medi, Steffen Jung, and Margret Keuper","link":"http://arxiv.org/abs/2410.23142v1","abstract":"Deep neural networks are susceptible to adversarial attacks and common\ncorruptions, which undermine their robustness. In order to enhance model\nresilience against such challenges, Adversarial Training (AT) has emerged as a\nprominent solution. Nevertheless, adversarial robustness is often attained at\nthe expense of model fairness during AT, i.e., disparity in class-wise\nrobustness of the model. While distinctive classes become more robust towards\nsuch adversaries, hard to detect classes suffer. Recently, research has focused\non improving model fairness specifically for perturbed images, overlooking the\naccuracy of the most likely non-perturbed data. Additionally, despite their\nrobustness against the adversaries encountered during model training,\nstate-of-the-art adversarial trained models have difficulty maintaining\nrobustness and fairness when confronted with diverse adversarial threats or\ncommon corruptions. In this work, we address the above concerns by introducing\na novel approach called Fair Targeted Adversarial Training (FAIR-TAT). We show\nthat using targeted adversarial attacks for adversarial training (instead of\nuntargeted attacks) can allow for more favorable trade-offs with respect to\nadversarial fairness. Empirical results validate the efficacy of our approach."},{"date":"2024-10","title":"Deep learning meets tree phenology modeling: PhenoFormer vs. process-based models","author":"Vivien Sainte Fare Garnot, Lynsay Spafford, Jelle Lever, Christian Sigg, Barbara Pietragalla, Yann Vitasse, Arthur Gessler, and Jan Dirk Wegner","link":"http://arxiv.org/abs/2410.23327v1","abstract":"Phenology, the timing of cyclical plant life events such as leaf emergence\nand coloration, is crucial in the bio-climatic system. Climate change drives\nshifts in these phenological events, impacting ecosystems and the climate\nitself. Accurate phenology models are essential to predict the occurrence of\nthese phases under changing climatic conditions. Existing methods include\nhypothesis-driven process models and data-driven statistical approaches.\nProcess models account for dormancy stages and various phenology drivers, while\nstatistical models typically rely on linear or traditional machine learning\ntechniques. Research shows that process models often outperform statistical\nmethods when predicting under climate conditions outside historical ranges,\nespecially with climate change scenarios. However, deep learning approaches\nremain underexplored in climate phenology modeling. We introduce PhenoFormer, a\nneural architecture better suited than traditional statistical methods at\npredicting phenology under shift in climate data distribution, while also\nbringing significant improvements or performing on par to the best performing\nprocess-based models. Our numerical experiments on a 70-year dataset of 70,000\nphenological observations from 9 woody species in Switzerland show that\nPhenoFormer outperforms traditional machine learning methods by an average of\n13% R2 and 1.1 days RMSE for spring phenology, and 11% R2 and 0.7 days RMSE for\nautumn phenology, while matching or exceeding the best process-based models.\nOur results demonstrate that deep learning has the potential to be a valuable\nmethodological tool for accurate climate-phenology prediction, and our\nPhenoFormer is a first promising step in improving phenological predictions\nbefore a complete understanding of the underlying physiological mechanisms is\navailable."},{"date":"2024-10","title":"Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes","author":"Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu","link":"http://arxiv.org/abs/2410.23126v2","abstract":"We study the optimal memorization capacity of modern Hopfield models and\nKernelized Hopfield Models (KHMs), a transformer-compatible class of Dense\nAssociative Memories. We present a tight analysis by establishing a connection\nbetween the memory configuration of KHMs and spherical codes from information\ntheory. Specifically, we treat the stored memory set as a specialized spherical\ncode. This enables us to cast the memorization problem in KHMs into a point\narrangement problem on a hypersphere. We show that the optimal capacity of KHMs\noccurs when the feature space allows memories to form an optimal spherical\ncode. This unique perspective leads to: (i) An analysis of how KHMs achieve\noptimal memory capacity, and identify corresponding necessary conditions.\nImportantly, we establish an upper capacity bound that matches the well-known\nexponential lower bound in the literature. This provides the first tight and\noptimal asymptotic memory capacity for modern Hopfield models. (ii) A\nsub-linear time algorithm $\\mathtt{U}\\text{-}\\mathtt{Hop}$+ to reach KHMs'\noptimal capacity. (iii) An analysis of the scaling behavior of the required\nfeature dimension relative to the number of stored memories. These efforts\nimprove both the retrieval capability of KHMs and the representation learning\nof corresponding transformers. Experimentally, we provide thorough numerical\nresults to back up theoretical findings."},{"date":"2024-10","title":"Teaching a Language Model to Distinguish Between Similar Details using a Small Adversarial Training Set","author":"Chris Achard","link":"http://arxiv.org/abs/2410.23118v1","abstract":"Language models can achieve high accuracy on natural language tasks such as\nNLI, but performance suffers on manually created adversarial examples. We\ninvestigate the performance of a language model trained on the Stanford Natural\nLanguage Inference (SNLI) corpus on a manually created adversarial test set. We\nthen improve the model's performance by fine tuning the model on a small,\nmanually created adversarial training set, designed to help the language model\nto learn to differentiate between similar words and phrases in the data. We\nshow an increase in accuracy on the adversarial test set (+ 13%) while still\nmaintaining good performance on the original NLI task. We also show an increase\nin accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI\ntest set (as judged by cosine similarity)."},{"date":"2024-10","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models","author":"Junjie Wu, Tsz Ting Chung, Kai Chen, and Dit-Yan Yeung","link":"http://arxiv.org/abs/2410.23114v1","abstract":"Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, in this paper we\ndesign a unified framework to measure object and relation hallucination in\nLVLMs simultaneously. The core idea of our framework is to conduct\nhallucination evaluation on (object, relation, object) triplets extracted from\nLVLMs' responses, and thus, could be easily generalized to different\nvision-language tasks. Based on our framework, we further introduce Tri-HE, a\nnovel Triplet-level Hallucination Evaluation benchmark which can be used to\nstudy both object and relation hallucination at the same time. We conduct\ncomprehensive evaluations on Tri-HE and observe that the relation hallucination\nissue is even more serious than object hallucination among existing LVLMs,\nhighlighting a previously neglected problem towards reliable LVLMs. Moreover,\nbased on our findings, we design a simple yet effective training-free approach\nto mitigate hallucinations for LVLMs, with which, we exceed all open-sourced\ncounterparts on Tri-HE, achieving comparable performance with the powerful\nGPT-4V. Our dataset and code for the reproduction of our experiments are\navailable publicly at https://github.com/wujunjie1998/Tri-HE."},{"date":"2024-10","title":"Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models","author":"Navyansh Mahla, and Ganesh Ramakrishnan","link":"http://arxiv.org/abs/2410.23111v2","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies."},{"date":"2024-10","title":"Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models","author":"Mahsa Bazzaz, and Seth Cooper","link":"http://arxiv.org/abs/2410.23108v1","abstract":"Generative Adversarial Networks (GANs) are unsupervised models designed to\nlearn and replicate a target distribution. The vanilla versions of these models\ncan be extended to more controllable models. Conditional Generative Adversarial\nNetworks (CGANs) extend vanilla GANs by conditioning both the generator and\ndiscriminator on some additional information (labels). Controllable models\nbased on complementary learning, such as Rumi-GAN, have been introduced.\nRumi-GANs leverage negative examples to enhance the generator's ability to\nlearn positive examples. We evaluate the performance of two controllable GAN\nvariants, CGAN and Rumi-GAN, in generating game levels targeting specific\nconstraints of interest: playability and controllability. This evaluation is\nconducted under two scenarios: with and without the inclusion of negative\nexamples. The goal is to determine whether incorporating negative examples\nhelps the GAN models avoid generating undesirable outputs. Our findings\nhighlight the strengths and weaknesses of each method in enforcing the\ngeneration of specific conditions when generating outputs based on given\npositive and negative examples."},{"date":"2024-10","title":"Automated Image-Based Identification and Consistent Classification of Fire Patterns with Quantitative Shape Analysis and Spatial Location Identification","author":"Pengkun Liu, Shuna Ni, Stanislav I. Stoliarov, and Pingbo Tang","link":"http://arxiv.org/abs/2410.23105v1","abstract":"Fire patterns, consisting of fire effects that offer insights into fire\nbehavior and origin, are traditionally classified based on investigators'\nvisual observations, leading to subjective interpretations. This study proposes\na framework for quantitative fire pattern classification to support fire\ninvestigators, aiming for consistency and accuracy. The framework integrates\nfour components. First, it leverages human-computer interaction to extract fire\npatterns from surfaces, combining investigator expertise with computational\nanalysis. Second, it employs an aspect ratio-based random forest model to\nclassify fire pattern shapes. Third, fire scene point cloud segmentation\nenables precise identification of fire-affected areas and the mapping of 2D\nfire patterns to 3D scenes. Lastly, spatial relationships between fire patterns\nand indoor elements support an interpretation of the fire scene. These\ncomponents provide a method for fire pattern analysis that synthesizes\nqualitative and quantitative data. The framework's classification results\nachieve 93% precision on synthetic data and 83% on real fire patterns."},{"date":"2024-10","title":"Real birational implicitization for statistical models","author":"Tobias Boege, and Liam Solus","link":"http://arxiv.org/abs/2410.23102v1","abstract":"We derive an implicit description of the image of a semialgebraic set under a\nbirational map, provided that the denominators of the map are positive on the\nset. For statistical models which are globally rationally identifiable, this\nyields model-defining constraints which facilitate model membership testing,\nrepresentation learning, and model equivalence tests. Many examples illustrate\nthe applicability of our results. The implicit equations recover well-known\nMarkov properties of classical graphical models, as well as other well-studied\nequations such as the Verma constraint. They also provide Markov properties for\ngeneralizations of these frameworks, such as colored or interventional\ngraphical models, staged trees, and the recently introduced Lyapunov models.\nUnder a further mild assumption, we show that our implicit equations generate\nthe vanishing ideal of the model up to a saturation, generalizing previous\nresults of Geiger, Meek and Sturmfels, Duarte and G\\\"orgen, Sullivant, and\nothers."},{"date":"2024-10","title":"CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense","author":"Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, and Xueqi Cheng","link":"http://arxiv.org/abs/2410.23091v1","abstract":"Despite ongoing efforts to defend neural classifiers from adversarial\nattacks, they remain vulnerable, especially to unseen attacks. In contrast,\nhumans are difficult to be cheated by subtle manipulations, since we make\njudgments only based on essential factors. Inspired by this observation, we\nattempt to model label generation with essential label-causative factors and\nincorporate label-non-causative factors to assist data generation. For an\nadversarial example, we aim to discriminate the perturbations as non-causative\nfactors and make predictions only based on the label-causative factors.\nConcretely, we propose a casual diffusion model (CausalDiff) that adapts\ndiffusion models for conditional data generation and disentangles the two types\nof casual factors by learning towards a novel casual information bottleneck\nobjective. Empirically, CausalDiff has significantly outperformed\nstate-of-the-art defense methods on various unseen attacks, achieving an\naverage robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on\nCIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition\nBenchmark)."},{"date":"2024-10","title":"PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via Existing MLLM Structures","author":"Tianxiang Wu, Minxin Nie, and Ziqiang Cao","link":"http://arxiv.org/abs/2410.23089v1","abstract":"The Multimodal Large Language Models (MLLMs) have activated the\ncapabilitiesof Large Language Models (LLMs) in solving visual-language tasks by\nintegratingvisual information. The prevailing approach in existing MLLMs\ninvolvesemploying an image encoder to extract visual features, converting\nthesefeatures into visual tokens via an adapter, and then integrating them with\ntheprompt into the LLM. However, because the process of image encoding\nisprompt-agnostic, the extracted visual features only provide a\ncoarsedescription of the image, impossible to focus on the requirements of\ntheprompt. On one hand, it is easy for image features to lack information\naboutthe prompt-specified objects, resulting in unsatisfactory responses. On\ntheother hand, the visual features contain a large amount of\nirrelevantinformation, which not only increases the burden on memory but also\nworsens thegeneration effectiveness. To address the aforementioned issues, we\npropose\\textbf{PIP-MM}, a framework that\n\\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual\nencoding process using existingmodules of MLLMs. Specifically, We utilize the\nfrozen LLM in the MLLM tovectorize the input prompt, which summarizes the\nrequirements of the prompt.Then, we input the prompt vector into our trained\nMulti-Layer Perceptron (MLP)to align with the visual input requirements, and\nsubsequently replace the classembedding in the image encoder. Since our model\nonly requires adding atrainable MLP, it can be applied to any MLLM. To validate\nthe effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks.\nAutomated evaluationmetrics and manual assessments demonstrate the strong\nperformance of PIP-MM.Particularly noteworthy is that our model maintains\nexcellent generationresults even when half of the visual tokens are reduced."},{"date":"2024-10","title":"CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models","author":"Aymene Mohammed Bouayed, Samuel Deslauriers-Gauthier, Adrian Iaccovelli, and David Naccache","link":"http://arxiv.org/abs/2410.23072v1","abstract":"Interpreting the decisions of Convolutional Neural Networks (CNNs) is\nessential for understanding their behavior, yet explainability remains a\nsignificant challenge, particularly for self-supervised models. Most existing\nmethods for generating saliency maps rely on ground truth labels, restricting\ntheir use to supervised tasks. EigenCAM is the only notable label-independent\nalternative, leveraging Singular Value Decomposition to generate saliency maps\napplicable across CNN models, but it does not fully exploit the tensorial\nstructure of feature maps. In this work, we introduce the Tucker Saliency Map\n(TSM) method, which applies Tucker tensor decomposition to better capture the\ninherent structure of feature maps, producing more accurate singular vectors\nand values. These are used to generate high-fidelity saliency maps, effectively\nhighlighting objects of interest in the input. We further extend EigenCAM and\nTSM into multivector variants -Multivec-EigenCAM and Multivector Tucker\nSaliency Maps (MTSM)- which utilize all singular vectors and values, further\nimproving saliency map quality. Quantitative evaluations on supervised\nclassification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve\ncompetitive performance with label-dependent methods. Moreover, TSM enhances\nexplainability by approximately 50% over EigenCAM for both supervised and\nself-supervised models. Multivec-EigenCAM and MTSM further advance\nstate-of-the-art explainability performance on self-supervised models, with\nMTSM achieving the best results."},{"date":"2024-10","title":"Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification","author":"Debjyoti Saharoy, Javed A. Aslam, and Virgil Pavlu","link":"http://arxiv.org/abs/2410.23066v1","abstract":"State-of-the-art Extreme Multi-Label Text Classification (XMTC) models rely\nheavily on multi-label attention layers to focus on key tokens in input text,\nbut obtaining optimal attention weights is challenging and resource-intensive.\nTo address this, we introduce PLANT -- Pretrained and Leveraged AtteNTion -- a\nnovel transfer learning strategy for fine-tuning XMTC decoders. PLANT surpasses\nexisting state-of-the-art methods across all metrics on mimicfull, mimicfifty,\nmimicfour, eurlex, and wikiten datasets. It particularly excels in few-shot\nscenarios, outperforming previous models specifically designed for few-shot\nscenarios by over 50 percentage points in F1 scores on mimicrare and by over 36\npercentage points on mimicfew, demonstrating its superior capability in\nhandling rare codes. PLANT also shows remarkable data efficiency in few-shot\nscenarios, achieving precision comparable to traditional models with\nsignificantly less data. These results are achieved through key technical\ninnovations: leveraging a pretrained Learning-to-Rank model as the planted\nattention layer, integrating mutual-information gain to enhance attention,\nintroducing an inattention mechanism, and implementing a stateful-decoder to\nmaintain context. Comprehensive ablation studies validate the importance of\nthese contributions in realizing the performance gains."},{"date":"2024-10","title":"Controlling Language and Diffusion Models by Transporting Activations","author":"Pau Rodriguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, Marco Cuturi, and Xavier Suau","link":"http://arxiv.org/abs/2410.23054v1","abstract":"The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation."},{"date":"2024-10","title":"Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation","author":"Samuele Peri, Alessio Russo, Gabor Fodor, and Pablo Soldati","link":"http://arxiv.org/abs/2410.23031v1","abstract":"Contemporary radio access networks employ link adaption (LA) algorithms to\noptimize the modulation and coding schemes to adapt to the prevailing\npropagation conditions and are near-optimal in terms of the achieved spectral\nefficiency. LA is a challenging task in the presence of mobility, fast fading,\nand imperfect channel quality information and limited knowledge of the receiver\ncharacteristics at the transmitter, which render model-based LA algorithms\ncomplex and suboptimal. Model-based LA is especially difficult as connected\nuser equipment devices become increasingly heterogeneous in terms of receiver\ncapabilities, antenna configurations and hardware characteristics. Recognizing\nthese difficulties, previous works have proposed reinforcement learning (RL)\nfor LA, which faces deployment difficulties due to their potential negative\nimpacts on live performance. To address this challenge, this paper considers\noffline RL to learn LA policies from data acquired in live networks with\nminimal or no intrusive effects on the network operation. We propose three LA\ndesigns based on batch-constrained deep Q-learning, conservative Q-learning,\nand decision transformers, showing that offline RL algorithms can achieve\nperformance of state-of-the-art online RL methods when data is collected with a\nproper behavioral policy."},{"date":"2024-10","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback","author":"Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, and Brandon Amos","link":"http://arxiv.org/abs/2410.23022v1","abstract":"Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon)."},{"date":"2024-10","title":"Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A Knowledge Graph Generation Approach","author":"Deperias Kerre, Anne Laurent, Kenneth Maussang, and Dickson Owuor","link":"http://arxiv.org/abs/2410.22996v1","abstract":"A well structured collection of the various Quantum Cascade Laser (QCL)\ndesign and working properties data provides a platform to analyze and\nunderstand the relationships between these properties. By analyzing these\nrelationships, we can gain insights into how different design features impact\nlaser performance properties such as the working temperature. Most of these QCL\nproperties are captured in scientific text. There is therefore need for\nefficient methodologies that can be utilized to extract QCL properties from\ntext and generate a semantically enriched and interlinked platform where the\nproperties can be analyzed to uncover hidden relations. There is also the need\nto maintain provenance and reference information on which these properties are\nbased. Semantic Web technologies such as Ontologies and Knowledge Graphs have\nproven capability in providing interlinked data platforms for knowledge\nrepresentation in various domains. In this paper, we propose an approach for\ngenerating a QCL properties Knowledge Graph (KG) from text for semantic\nenrichment of the properties. The approach is based on the QCL ontology and a\nRetrieval Augmented Generation (RAG) enabled information extraction pipeline\nbased on GPT 4-Turbo language model. The properties of interest include:\nworking temperature, laser design type, lasing frequency, laser optical power\nand the heterostructure. The experimental results demonstrate the feasibility\nand effectiveness of this approach for efficiently extracting QCL properties\nfrom unstructured text and generating a QCL properties Knowledge Graph, which\nhas potential applications in semantic enrichment and analysis of QCL data."},{"date":"2024-10","title":"Higher-order Cross-structural Embedding Model for Time Series Analysis","author":"Guancen Lin, Cong Shen, and Aijing Lin","link":"http://arxiv.org/abs/2410.22984v1","abstract":"Time series analysis has gained significant attention due to its critical\napplications in diverse fields such as healthcare, finance, and sensor\nnetworks. The complexity and non-stationarity of time series make it\nchallenging to capture the interaction patterns across different timestamps.\nCurrent approaches struggle to model higher-order interactions within time\nseries, and focus on learning temporal or spatial dependencies separately,\nwhich limits performance in downstream tasks. To address these gaps, we propose\nHigher-order Cross-structural Embedding Model for Time Series (High-TS), a\nnovel framework that jointly models both temporal and spatial perspectives by\ncombining multiscale Transformer with Topological Deep Learning (TDL).\nMeanwhile, High-TS utilizes contrastive learning to integrate these two\nstructures for generating robust and discriminative representations. Extensive\nexperiments show that High-TS outperforms state-of-the-art methods in various\ntime series tasks and demonstrate the importance of higher-order\ncross-structural information in improving model performance."},{"date":"2024-10","title":"Dual-Optimized Adaptive Graph Reconstruction for Multi-View Graph Clustering","author":"Zichen Wen, Tianyi Wu, Yazhou Ren, Yawen Ling, Chenhang Cui, Xiaorong Pu, and Lifang He","link":"http://arxiv.org/abs/2410.22983v1","abstract":"Multi-view clustering is an important machine learning task for multi-media\ndata, encompassing various domains such as images, videos, and texts. Moreover,\nwith the growing abundance of graph data, the significance of multi-view graph\nclustering (MVGC) has become evident. Most existing methods focus on graph\nneural networks (GNNs) to extract information from both graph structure and\nfeature data to learn distinguishable node representations. However,\ntraditional GNNs are designed with the assumption of homophilous graphs, making\nthem unsuitable for widely prevalent heterophilous graphs. Several techniques\nhave been introduced to enhance GNNs for heterophilous graphs. While these\nmethods partially mitigate the heterophilous graph issue, they often neglect\nthe advantages of traditional GNNs, such as their simplicity, interpretability,\nand efficiency. In this paper, we propose a novel multi-view graph clustering\nmethod based on dual-optimized adaptive graph reconstruction, named DOAGC. It\nmainly aims to reconstruct the graph structure adapted to traditional GNNs to\ndeal with heterophilous graph issues while maintaining the advantages of\ntraditional GNNs. Specifically, we first develop an adaptive graph\nreconstruction mechanism that accounts for node correlation and original\nstructural information. To further optimize the reconstruction graph, we design\na dual optimization strategy and demonstrate the feasibility of our\noptimization strategy through mutual information theory. Numerous experiments\ndemonstrate that DOAGC effectively mitigates the heterophilous graph problem."},{"date":"2024-10","title":"DisenTS: Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting","author":"Zhiding Liu, Jiqian Yang, Qingyang Mao, Yuze Zhao, Mingyue Cheng, Zhi Li, Qi Liu, and Enhong Chen","link":"http://arxiv.org/abs/2410.22981v1","abstract":"Multivariate time series forecasting plays a crucial role in various\nreal-world applications. Significant efforts have been made to integrate\nadvanced network architectures and training strategies that enhance the capture\nof temporal dependencies, thereby improving forecasting accuracy. On the other\nhand, mainstream approaches typically utilize a single unified model with\nsimplistic channel-mixing embedding or cross-channel attention operations to\naccount for the critical intricate inter-channel dependencies. Moreover, some\nmethods even trade capacity for robust prediction based on the\nchannel-independent assumption. Nonetheless, as time series data may display\ndistinct evolving patterns due to the unique characteristics of each channel\n(including multiple strong seasonalities and trend changes), the unified\nmodeling methods could yield suboptimal results. To this end, we propose\nDisenTS, a tailored framework for modeling disentangled channel evolving\npatterns in general multivariate time series forecasting. The central idea of\nDisenTS is to model the potential diverse patterns within the multivariate time\nseries data in a decoupled manner. Technically, the framework employs multiple\ndistinct forecasting models, each tasked with uncovering a unique evolving\npattern. To guide the learning process without supervision of pattern\npartition, we introduce a novel Forecaster Aware Gate (FAG) module that\ngenerates the routing signals adaptively according to both the forecasters'\nstates and input series' characteristics. The forecasters' states are derived\nfrom the Linear Weight Approximation (LWA) strategy, which quantizes the\ncomplex deep neural networks into compact matrices. Additionally, the\nSimilarity Constraint (SC) is further proposed to guide each model to\nspecialize in an underlying pattern by minimizing the mutual information\nbetween the representations."},{"date":"2024-10","title":"Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices with Hierarchical Heatmaps and Feature Propagation","author":"Kaiqin Yang. Yixiang Dai, Guijin Wang, and Siang Chen","link":"http://arxiv.org/abs/2410.22980v1","abstract":"6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."},{"date":"2024-10","title":"A Study of Secure Algorithms for Vertical Federated Learning: Take Secure Logistic Regression as an Example","author":"Huan-Chih Wang, and Ja-Ling Wu","link":"http://arxiv.org/abs/2410.22960v1","abstract":"After entering the era of big data, more and more companies build services\nwith machine learning techniques. However, it is costly for companies to\ncollect data and extract helpful handcraft features on their own. Although it\nis a way to combine with other companies' data for boosting the model's\nperformance, this approach may be prohibited by laws. In other words, finding\nthe balance between sharing data with others and keeping data from privacy\nleakage is a crucial topic worthy of close attention. This paper focuses on\ndistributed data and conducts secure model training tasks on a vertical\nfederated learning scheme. Here, secure implies that the whole process is\nexecuted in the encrypted domain. Therefore, the privacy concern is released."},{"date":"2024-10","title":"EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models","author":"Shangquan Sun, Wenqi Ren, Zikun Liu, Hyunhee Park, Rui Wang, and Xiaochun Cao","link":"http://arxiv.org/abs/2410.22959v1","abstract":"Image restoration has experienced significant advancements due to the\ndevelopment of deep learning. Nevertheless, it encounters challenges related to\nill-posed problems, resulting in deviations between single model predictions\nand ground-truths. Ensemble learning, as a powerful machine learning technique,\naims to address these deviations by combining the predictions of multiple base\nmodels. Most existing works adopt ensemble learning during the design of\nrestoration models, while only limited research focuses on the inference-stage\nensemble of pre-trained restoration models. Regression-based methods fail to\nenable efficient inference, leading researchers in academia and industry to\nprefer averaging as their choice for post-training ensemble. To address this,\nwe reformulate the ensemble problem of image restoration into Gaussian mixture\nmodels (GMMs) and employ an expectation maximization (EM)-based algorithm to\nestimate ensemble weights for aggregating prediction candidates. We estimate\nthe range-wise ensemble weights on a reference set and store them in a lookup\ntable (LUT) for efficient ensemble inference on the test set. Our algorithm is\nmodel-agnostic and training-free, allowing seamless integration and enhancement\nof various pre-trained image restoration models. It consistently outperforms\nregression based methods and averaging ensemble approaches on 14 benchmarks\nacross 3 image restoration tasks, including super-resolution, deblurring and\nderaining. The codes and all estimated weights have been released in Github."},{"date":"2024-10","title":"MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering","author":"Yizhen Luo, Zikun Nie, Massimo Hong, Suyuan Zhao, Hao Zhou, and Zaiqing Nie","link":"http://arxiv.org/abs/2410.22949v1","abstract":"Studying protein mutations within amino acid sequences holds tremendous\nsignificance in life sciences. Protein language models (PLMs) have demonstrated\nstrong capabilities in broad biological applications. However, due to\narchitectural design and lack of supervision, PLMs model mutations implicitly\nwith evolutionary plausibility, which is not satisfactory to serve as\nexplainable and engineerable tools in real-world studies. To address these\nissues, we present MutaPLM, a unified framework for interpreting and navigating\nprotein mutations with protein language models. MutaPLM introduces a protein\ndelta network that captures explicit protein mutation representations within a\nunified feature space, and a transfer learning pipeline with a chain-of-thought\n(CoT) strategy to harvest protein mutation knowledge from biomedical texts. We\nalso construct MutaDescribe, the first large-scale protein mutation dataset\nwith rich textual annotations, which provides cross-modal supervision signals.\nThrough comprehensive experiments, we demonstrate that MutaPLM excels at\nproviding human-understandable explanations for mutational effects and\nprioritizing novel mutations with desirable properties. Our code, model, and\ndata are open-sourced at https://github.com/PharMolix/MutaPLM."},{"date":"2024-10","title":"DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data","author":"Hanyang Chen, Yang Jiang, Shengnan Guo, Xiaowei Mao, Youfang Lin, and Huaiyu Wan","link":"http://arxiv.org/abs/2410.22938v2","abstract":"The application of reinforcement learning in traffic signal control (TSC) has\nbeen extensively researched and yielded notable achievements. However, most\nexisting works for TSC assume that traffic data from all surrounding\nintersections is fully and continuously available through sensors. In\nreal-world applications, this assumption often fails due to sensor malfunctions\nor data loss, making TSC with missing data a critical challenge. To meet the\nneeds of practical applications, we introduce DiffLight, a novel conditional\ndiffusion model for TSC under data-missing scenarios in the offline setting.\nSpecifically, we integrate two essential sub-tasks, i.e., traffic data\nimputation and decision-making, by leveraging a Partial Rewards Conditioned\nDiffusion (PRCD) model to prevent missing rewards from interfering with the\nlearning process. Meanwhile, to effectively capture the spatial-temporal\ndependencies among intersections, we design a Spatial-Temporal transFormer\n(STFormer) architecture. In addition, we propose a Diffusion Communication\nMechanism (DCM) to promote better communication and control performance under\ndata-missing scenarios. Extensive experiments on five datasets with various\ndata-missing scenarios demonstrate that DiffLight is an effective controller to\naddress TSC with missing data. The code of DiffLight is released at\nhttps://github.com/lokol5579/DiffLight-release."},{"date":"2024-10","title":"An Individual Identity-Driven Framework for Animal Re-Identification","author":"Yihao Wu, Di Zhao, Jingfeng Zhang, and Yun Sing Koh","link":"http://arxiv.org/abs/2410.22927v1","abstract":"Reliable re-identification of individuals within large wildlife populations\nis crucial for biological studies, ecological research, and wildlife\nconservation. Classic computer vision techniques offer a promising direction\nfor Animal Re-identification (Animal ReID), but their backbones' close-set\nnature limits their applicability and generalizability. Despite the\ndemonstrated effectiveness of vision-language models like CLIP in\nre-identifying persons and vehicles, their application to Animal ReID remains\nlimited due to unique challenges, such as the various visual representations of\nanimals, including variations in poses and forms. To address these limitations,\nwe leverage CLIP's cross-modal capabilities to introduce a two-stage framework,\nthe \\textbf{Indiv}idual \\textbf{A}nimal \\textbf{ID}entity-Driven (IndivAID)\nframework, specifically designed for Animal ReID. In the first stage, IndivAID\ntrains a text description generator by extracting individual semantic\ninformation from each image, generating both image-specific and\nindividual-specific textual descriptions that fully capture the diverse visual\nconcepts of each individual across animal images. In the second stage, IndivAID\nrefines its learning of visual concepts by dynamically incorporating\nindividual-specific textual descriptions with an integrated attention module to\nfurther highlight discriminative features of individuals for Animal ReID.\nEvaluation against state-of-the-art methods across eight benchmark datasets and\na real-world Stoat dataset demonstrates IndivAID's effectiveness and\napplicability. Code is available at \\url{https://github.com/ywu840/IndivAID}."},{"date":"2024-10","title":"HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models","author":"Shengkai Zhang, Nianhong Jiao, Tian Li, Chaojie Yang, Chenhui Xue, Boya Niu, and Jun Gao","link":"http://arxiv.org/abs/2410.22901v1","abstract":"We propose an effective method for inserting adapters into text-to-image\nfoundation models, which enables the execution of complex downstream tasks\nwhile preserving the generalization ability of the base model. The core idea of\nthis method is to optimize the attention mechanism related to 2D feature maps,\nwhich enhances the performance of the adapter. This approach was validated on\nthe task of meme video generation and achieved significant results. We hope\nthis work can provide insights for post-training tasks of large text-to-image\nmodels. Additionally, as this method demonstrates good compatibility with SD1.5\nderivative models, it holds certain value for the open-source community.\nTherefore, we will release the related code\n(\\url{https://songkey.github.io/hellomeme})."},{"date":"2024-10","title":"Modelling vehicle and pedestrian collective dynamics: Challenges and advancements","author":"C\u00e9cile Appert-Rolland, Alexandre Nicolas, Armin Seyfried, Antoine Tordeux, and Denis Ullmo","link":"http://arxiv.org/abs/2410.22896v1","abstract":"In our urbanised societies, the management and regulation of traffic and\npedestrian flows is of considerable interest for public safety, economic\ndevelopment, and the conservation of the environment. However, modelling and\ncontrolling the collective dynamics of vehicles and pedestrians raises several\nchallenges. Not only are the individual entities self-propelled and hard to\ndescribe, but their complex nonlinear physical and social interactions makes\nthe multi-agent problem of crowd and traffic flow even more involved. In this\nchapter, we purport to review the suitability and limitations of classical\nmodelling approaches through four examples of collective behaviour: stop-and-go\nwaves in traffic flow, lane formation, long-term avoidance behaviour, and load\nbalancing in pedestrian dynamics. While stop-and-go dynamics and lane formation\ncan both be addressed by basic reactive models (at least to some extent), the\nlatter two require anticipation and/or coordination at the level of the group.\nThe results highlight the limitations of classical force-based models, but also\nthe need for long-term anticipation mechanisms and multiscale modelling\napproaches. In response, we review new developments and modelling concepts."},{"date":"2024-10","title":"Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector","author":"Youcheng Huang, Fengbin Zhu, Jingkun Tang, Pan Zhou, Wenqiang Lei, Jiancheng Lv, and Tat-Seng Chua","link":"http://arxiv.org/abs/2410.22888v1","abstract":"Visual Language Models (VLMs) are vulnerable to adversarial attacks,\nespecially those from adversarial images, which is however under-explored in\nliterature. To facilitate research on this critical safety problem, we first\nconstruct a new laRge-scale Adervsarial images dataset with Diverse hArmful\nResponses (RADAR), given that existing datasets are either small-scale or only\ncontain limited types of harmful responses. With the new RADAR dataset, we\nfurther develop a novel and effective iN-time Embedding-based AdveRSarial Image\nDEtection (NEARSIDE) method, which exploits a single vector that distilled from\nthe hidden states of VLMs, which we call the attacking direction, to achieve\nthe detection of adversarial images against benign ones in the input. Extensive\nexperiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the\neffectiveness, efficiency, and cross-model transferrability of our proposed\nmethod. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE"},{"date":"2024-10","title":"Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies","author":"Suchir Salhan, Richard Diehl Martinez, Z\u00e9bulon Goriely, and Paula Buttery","link":"http://arxiv.org/abs/2410.22886v1","abstract":"Curriculum Learning has been a popular strategy to improve the cognitive\nplausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge.\nHowever, it has not led to considerable improvements over non-curriculum\nmodels. We assess whether theoretical linguistic acquisition theories can be\nused to specify more fine-grained curriculum learning strategies, creating\nage-ordered corpora of Child-Directed Speech for four typologically distant\nlanguage families to implement SSLMs and acquisition-inspired curricula\ncross-lingually. Comparing the success of three objective curricula (Growing,\nInwards and MMM) that precisely replicate the predictions of acquisition\ntheories on a standard SSLM architecture, we find fine-grained\nacquisition-inspired curricula can outperform non-curriculum baselines and\nperformance benefits of curricula strategies in SSLMs can be derived by\nspecifying fine-grained language-specific curricula that precisely replicate\nlanguage acquisition theories."},{"date":"2024-10","title":"Stealing User Prompts from Mixture of Experts","author":"Itay Yona, Ilia Shumailov, Jamie Hayes, and Nicholas Carlini","link":"http://arxiv.org/abs/2410.22884v1","abstract":"Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities."},{"date":"2024-10","title":"Eliciting Critical Reasoning in Retrieval-Augmented Language Models via Contrastive Explanations","author":"Leonardo Ranaldi, Marco Valentino, and Andr\u00e8 Freitas","link":"http://arxiv.org/abs/2410.22874v1","abstract":"Retrieval-augmented generation (RAG) has emerged as a critical mechanism in\ncontemporary NLP to support Large Language Models(LLMs) in systematically\naccessing richer factual context. However, the integration of RAG mechanisms\nbrings its inherent challenges, as LLMs need to deal with potentially noisy\ncontexts. Recent studies have shown that LLMs still struggle to critically\nanalyse RAG-based in-context information, a limitation that may lead to\nincorrect inferences and hallucinations. In this paper, we investigate how to\nelicit critical reasoning in RAG via contrastive explanations. In particular,\nwe propose Contrastive-RAG (C-RAG), a framework that (i) retrieves relevant\ndocuments given a query, (ii) selects and exemplifies relevant passages, and\n(iii) generates explanations that explicitly contrast the relevance of the\npassages to (iv) support the final answer. We show the impact of C-RAG building\ncontrastive reasoning demonstrations from LLMs to instruct smaller models for\nretrieval-augmented tasks. Extensive experiments demonstrate that C-RAG\nimproves state-of-the-art RAG models while (a) requiring significantly fewer\nprompts and demonstrations and (b) being robust to perturbations in the\nretrieved documents."},{"date":"2024-10","title":"Danoliteracy of Generative, Large Language Models","author":"S\u00f8ren Vejlgaard Holm, Lars Kai Hansen, and Martin Carsten Nielsen","link":"http://arxiv.org/abs/2410.22839v1","abstract":"The language technology moonshot moment of Generative, Large Language Models\n(GLLMs) was not limited to English: These models brought a surge of\ntechnological applications, investments and hype to low-resource languages as\nwell. However, the capabilities of these models in languages such as Danish\nwere until recently difficult to verify beyond qualitative demonstrations due\nto a lack of applicable evaluation corpora. We present a GLLM benchmark to\nevaluate Danoliteracy, a measure of Danish language and cultural competency,\nacross eight diverse scenarios such Danish citizenship tests and abstractive\nsocial media question answering. This limited-size benchmark is found to\nproduce a robust ranking that correlates to human feedback at $\\rho \\sim 0.8$\nwith GPT-4 and Claude Opus models achieving the highest rankings. Analyzing\nthese model results across scenarios, we find one strong underlying factor\nexplaining $95\\%$ of scenario performance variance for GLLMs in Danish,\nsuggesting a $g$ factor of model consistency in language adaption."},{"date":"2024-10","title":"SFDFusion: An Efficient Spatial-Frequency Domain Fusion Network for Infrared and Visible Image Fusion","author":"Kun Hu, Qingle Zhang, Maoxun Yuan, and Yitian Zhang","link":"http://arxiv.org/abs/2410.22837v1","abstract":"Infrared and visible image fusion aims to utilize the complementary\ninformation from two modalities to generate fused images with prominent targets\nand rich texture details. Most existing algorithms only perform pixel-level or\nfeature-level fusion from different modalities in the spatial domain. They\nusually overlook the information in the frequency domain, and some of them\nsuffer from inefficiency due to excessively complex structures. To tackle these\nchallenges, this paper proposes an efficient Spatial-Frequency Domain Fusion\n(SFDFusion) network for infrared and visible image fusion. First, we propose a\nDual-Modality Refinement Module (DMRM) to extract complementary information.\nThis module extracts useful information from both the infrared and visible\nmodalities in the spatial domain and enhances fine-grained spatial details.\nNext, to introduce frequency domain information, we construct a Frequency\nDomain Fusion Module (FDFM) that transforms the spatial domain to the frequency\ndomain through Fast Fourier Transform (FFT) and then integrates frequency\ndomain information. Additionally, we design a frequency domain fusion loss to\nprovide guidance for the fusion process. Extensive experiments on public\ndatasets demonstrate that our method produces fused images with significant\nadvantages in various fusion metrics and visual effects. Furthermore, our\nmethod demonstrates high efficiency in image fusion and good performance on\ndownstream detection tasks, thereby satisfying the real-time demands of\nadvanced visual tasks."},{"date":"2024-10","title":"HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models","author":"Yucheng Zhang, Qinfeng Li, Tianyu Du, Xuhong Zhang, Xinkui Zhao, Zhengwen Feng, and Jianwei Yin","link":"http://arxiv.org/abs/2410.22832v1","abstract":"Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge, making them adaptable and\ncost-effective for various applications. However, the growing reliance on these\nsystems also introduces potential security risks. In this work, we reveal a\nnovel vulnerability, the retrieval prompt hijack attack (HijackRAG), which\nenables attackers to manipulate the retrieval mechanisms of RAG systems by\ninjecting malicious texts into the knowledge database. When the RAG system\nencounters target questions, it generates the attacker's pre-determined answers\ninstead of the correct ones, undermining the integrity and trustworthiness of\nthe system. We formalize HijackRAG as an optimization problem and propose both\nblack-box and white-box attack strategies tailored to different levels of the\nattacker's knowledge. Extensive experiments on multiple benchmark datasets show\nthat HijackRAG consistently achieves high attack success rates, outperforming\nexisting baseline attacks. Furthermore, we demonstrate that the attack is\ntransferable across different retriever models, underscoring the widespread\nrisk it poses to RAG systems. Lastly, our exploration of various defense\nmechanisms reveals that they are insufficient to counter HijackRAG, emphasizing\nthe urgent need for more robust security measures to protect RAG systems in\nreal-world deployments."},{"date":"2024-10","title":"Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis","author":"Zhiyuan Min, Yawei Luo, Jianwen Sun, and Yi Yang","link":"http://arxiv.org/abs/2410.22817v2","abstract":"Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from\nsparse-view observations in a feed-forward inference manner, eliminating the\nneed for scene-specific retraining required in conventional 3DGS. However,\nexisting methods rely heavily on epipolar priors, which can be unreliable in\ncomplex realworld scenes, particularly in non-overlapping and occluded regions.\nIn this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based\nmodel for generalizable novel view synthesis that operates independently of\nepipolar line constraints. To enhance multiview feature extraction with 3D\nperception, we employ a selfsupervised Vision Transformer (ViT) with cross-view\ncompletion pre-training on large-scale datasets. Additionally, we introduce an\nIterative Cross-view Gaussians Alignment method to ensure consistent depth\nscales across different views. Our eFreeSplat represents an innovative approach\nfor generalizable novel view synthesis. Different from the existing pure\ngeometry-free methods, eFreeSplat focuses more on achieving epipolar-free\nfeature matching and encoding by providing 3D priors through cross-view\npretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks\nusing the RealEstate10K and ACID datasets. Extensive experiments demonstrate\nthat eFreeSplat surpasses state-of-the-art baselines that rely on epipolar\npriors, achieving superior geometry reconstruction and novel view synthesis\nquality. Project page: https://tatakai1.github.io/efreesplat/."},{"date":"2024-10","title":"Universality of the $\u03c0^2/6$ Pathway in Avoiding Model Collapse","author":"Apratim Dey, and David Donoho","link":"http://arxiv.org/abs/2410.22812v1","abstract":"Researchers in empirical machine learning recently spotlighted their fears of\nso-called Model Collapse. They imagined a discard workflow, where an initial\ngenerative model is trained with real data, after which the real data are\ndiscarded, and subsequently, the model generates synthetic data on which a new\nmodel is trained. They came to the conclusion that models degenerate as\nmodel-fitting generations proceed. However, other researchers considered an\naugment workflow, where the original real data continue to be used in each\ngeneration of training, augmented by synthetic data from models fit in all\nearlier generations. Empirical results on canonical datasets and learning\nprocedures confirmed the occurrence of model collapse under the discard\nworkflow and avoidance of model collapse under the augment workflow. Under the\naugment workflow, theoretical evidence also confirmed avoidance in particular\ninstances; specifically, Gerstgrasser et al. (2024) found that for classical\nLinear Regression, test risk at any later generation is bounded by a moderate\nmultiple, viz. pi-squared-over-6 of the test risk of training with the original\nreal data alone. Some commentators questioned the generality of theoretical\nconclusions based on the generative model assumed in Gerstgrasser et al.\n(2024): could similar conclusions be reached for other task/model pairings? In\nthis work, we demonstrate the universality of the pi-squared-over-6 augment\nrisk bound across a large family of canonical statistical models, offering key\ninsights into exactly why collapse happens under the discard workflow and is\navoided under the augment workflow. In the process, we provide a framework that\nis able to accommodate a large variety of workflows (beyond discard and\naugment), thereby enabling an experimenter to judge the comparative merits of\nmultiple different workflows by simulating a simple Gaussian process."},{"date":"2024-10","title":"Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation","author":"Yang Zhang, Juntao You, Yimeng Bai, Jizhi Zhang, Keqin Bao, Wenjie Wang, and Tat-Seng Chua","link":"http://arxiv.org/abs/2410.22809v1","abstract":"Recent advancements in recommender systems have focused on leveraging Large\nLanguage Models (LLMs) to improve user preference modeling, yielding promising\noutcomes. However, current LLM-based approaches struggle to fully leverage user\nbehavior sequences, resulting in suboptimal preference modeling for\npersonalized recommendations. In this study, we propose a novel Counterfactual\nFine-Tuning (CFT) method to address this issue by explicitly emphasizing the\nrole of behavior sequences when generating recommendations. Specifically, we\nemploy counterfactual reasoning to identify the causal effects of behavior\nsequences on model output and introduce a task that directly fits the\nground-truth labels based on these effects, achieving the goal of explicit\nemphasis. Additionally, we develop a token-level weighting mechanism to adjust\nthe emphasis strength for different item tokens, reflecting the diminishing\ninfluence of behavior sequences from earlier to later tokens during predicting\nan item. Extensive experiments on real-world datasets demonstrate that CFT\neffectively improves behavior sequence modeling. Our codes are available at\nhttps://github.com/itsmeyjt/CFT."},{"date":"2024-10","title":"DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection","author":"Yoto Fujita, Yoshiaki Bando, Keisuke Imoto, Masaki Onishi, and Kazuyoshi Yoshii","link":"http://arxiv.org/abs/2410.22803v1","abstract":"This paper describes sound event localization and detection (SELD) for\nspatial audio recordings captured by firstorder ambisonics (FOA) microphones.\nIn this task, one may train a deep neural network (DNN) using FOA data\nannotated with the classes and directions of arrival (DOAs) of sound events.\nHowever, the performance of this approach is severely bounded by the amount of\nannotated data. To overcome this limitation, we propose a novel method of\npretraining the feature extraction part of the DNN in a self-supervised manner.\nWe use spatial audio-visual recordings abundantly available as virtual reality\ncontents. Assuming that sound objects are concurrently observed by the FOA\nmicrophones and the omni-directional camera, we jointly train audio and visual\nencoders with contrastive learning such that the audio and visual embeddings of\nthe same recording and DOA are made close. A key feature of our method is that\nthe DOA-wise audio embeddings are jointly extracted from the raw audio data,\nwhile the DOA-wise visual embeddings are separately extracted from the local\nvisual crops centered on the corresponding DOA. This encourages the latent\nfeatures of the audio encoder to represent both the classes and DOAs of sound\nevents. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows\nnon-annotated audio-visual recordings of 100 hours reduced the error score of\nSELD from 36.4 pts to 34.9 pts."},{"date":"2024-10","title":"Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation","author":"Chengkai Huang, Shoujin Wang, Xianzhi Wang, and Lina Yao","link":"http://arxiv.org/abs/2410.22790v1","abstract":"Sequential recommender systems (SRSs) aim to predict the subsequent items\nwhich may interest users via comprehensively modeling users' complex preference\nembedded in the sequence of user-item interactions. However, most of existing\nSRSs often model users' single low-level preference based on item ID\ninformation while ignoring the high-level preference revealed by item attribute\ninformation, such as item category. Furthermore, they often utilize limited\nsequence context information to predict the next item while overlooking richer\ninter-item semantic relations. To this end, in this paper, we proposed a novel\nhierarchical preference modeling framework to substantially model the complex\nlow- and high-level preference dynamics for accurate sequential recommendation.\nSpecifically, in the framework, a novel dual-transformer module and a novel\ndual contrastive learning scheme have been designed to discriminatively learn\nusers' low- and high-level preference and to effectively enhance both low- and\nhigh-level preference learning respectively. In addition, a novel\nsemantics-enhanced context embedding module has been devised to generate more\ninformative context embedding for further improving the recommendation\nperformance. Extensive experiments on six real-world datasets have demonstrated\nboth the superiority of our proposed method over the state-of-the-art ones and\nthe rationality of our design."},{"date":"2024-10","title":"A New Particle Pusher with Hadronic Interactions for Modeling Multimessenger Emission from Compact Objects","author":"Minghao Zou, Hayk Hakobyan, Rostom Mbarek, Bart Ripperda, Fabio Bacchini, and Lorenzo Sironi","link":"http://arxiv.org/abs/2410.22781v1","abstract":"We propose novel numerical schemes based on the Boris method in curved\nspacetime, incorporating both hadronic and radiative interactions for the first\ntime. Once the proton has lost significant energy due to radiative and hadronic\nlosses, and its gyroradius has decreased below typical scales on which the\nelectromagnetic field varies, we apply a guiding center approximation (GCA). We\nfundamentally simulate collision processes either with a Monte-Carlo method or,\nwhere applicable, as a continuous energy loss, contingent on the local optical\ndepth. To test our algorithm for the first time combining the effects of\nelectromagnetic, gravitational, and radiation fields including hadronic\ninteractions, we simulate highly relativistic protons traveling through various\nelectromagnetic fields and proton backgrounds. We provide unit tests in various\nspatially dependent electromagnetic and gravitational fields and background\nphoton and proton distributions, comparing the trajectory against analytic\nresults. We propose that our method can be used to analyze hadronic\ninteractions in black hole accretion disks, jets, and coronae to study the\nneutrino abundance from active galactic nuclei."},{"date":"2024-10","title":"Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models","author":"Arash Marioriyad, Parham Rezaei, Mahdieh Soleymani Baghshah, and Mohammad Hossein Rohban","link":"http://arxiv.org/abs/2410.22775v1","abstract":"Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E,\nhave shown remarkable proficiency in producing high-quality, realistic, and\nnatural images from textual descriptions. However, these models sometimes fail\nto accurately capture all the details specified in the input prompts,\nparticularly concerning entities, attributes, and spatial relationships. This\nissue becomes more pronounced when the prompt contains novel or complex\ncompositions, leading to what are known as compositional generation failure\nmodes. Recently, a new open-source diffusion-based T2I model, FLUX, has been\nintroduced, demonstrating strong performance in high-quality image generation.\nAdditionally, autoregressive T2I models like LlamaGen have claimed competitive\nvisual quality performance compared to diffusion-based models. In this study,\nwe evaluate the compositional generation capabilities of these newly introduced\nmodels against established models using the T2I-CompBench benchmark. Our\nfindings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on\npar with state-of-the-art diffusion models for compositional generation tasks\nunder the same criteria, such as model size and inference time. On the other\nhand, the open-source diffusion-based model FLUX exhibits compositional\ngeneration capabilities comparable to the state-of-the-art closed-source model\nDALL-E3."},{"date":"2024-10","title":"Unfolding Target Detection with State Space Model","author":"Luca Jiang-Tao Yu, and Chenshu Wu","link":"http://arxiv.org/abs/2410.22774v1","abstract":"Target detection is a fundamental task in radar sensing, serving as the\nprecursor to any further processing for various applications. Numerous\ndetection algorithms have been proposed. Classical methods based on signal\nprocessing, e.g., the most widely used CFAR, are challenging to tune and\nsensitive to environmental conditions. Deep learning-based methods can be more\naccurate and robust, yet usually lack interpretability and physical relevance.\nIn this paper, we introduce a novel method that combines signal processing and\ndeep learning by unfolding the CFAR detector with a state space model\narchitecture. By reserving the CFAR pipeline yet turning its sophisticated\nconfigurations into trainable parameters, our method achieves high detection\nperformance without manual parameter tuning, while preserving model\ninterpretability. We implement a lightweight model of only 260K parameters and\nconduct real-world experiments for human target detection using FMCW radars.\nThe results highlight the remarkable performance of the proposed method,\noutperforming CFAR and its variants by 10X in detection rate and false alarm\nrate. Our code is open-sourced here: https://github.com/aiot-lab/NeuroDet."},{"date":"2024-10","title":"InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models","author":"Hao Li, Xiaogeng Liu, and Chaowei Xiao","link":"http://arxiv.org/abs/2410.22770v1","abstract":"Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard."},{"date":"2024-10","title":"st-DTPM: Spatial-Temporal Guided Diffusion Transformer Probabilistic Model for Delayed Scan PET Image Prediction","author":"Ran Hong, Yuxia Huang, Lei Liu, Zhonghui Wu, Bingxuan Li, Xuemei Wang, and Qiegen Liu","link":"http://arxiv.org/abs/2410.22732v1","abstract":"PET imaging is widely employed for observing biological metabolic activities\nwithin the human body. However, numerous benign conditions can cause increased\nuptake of radiopharmaceuticals, confounding differentiation from malignant\ntumors. Several studies have indicated that dual-time PET imaging holds promise\nin distinguishing between malignant and benign tumor processes. Nevertheless,\nthe hour-long distribution period of radiopharmaceuticals post-injection\ncomplicates the determination of optimal timing for the second scan, presenting\nchallenges in both practical applications and research. Notably, we have\nidentified that delay time PET imaging can be framed as an image-to-image\nconversion problem. Motivated by this insight, we propose a novel\nspatial-temporal guided diffusion transformer probabilistic model (st-DTPM) to\nsolve dual-time PET imaging prediction problem. Specifically, this architecture\nleverages the U-net framework that integrates patch-wise features of CNN and\npixel-wise relevance of Transformer to obtain local and global information. And\nthen employs a conditional DDPM model for image synthesis. Furthermore, on\nspatial condition, we concatenate early scan PET images and noisy PET images on\nevery denoising step to guide the spatial distribution of denoising sampling.\nOn temporal condition, we convert diffusion time steps and delay time to a\nuniversal time vector, then embed it to each layer of model architecture to\nfurther improve the accuracy of predictions. Experimental results demonstrated\nthe superiority of our method over alternative approaches in preserving image\nquality and structural information, thereby affirming its efficacy in\npredictive task."},{"date":"2024-10","title":"One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks","author":"Ji Guo, Wenbo Jiang, Rui Zhang, Guoming Lu, and Hongwei Li","link":"http://arxiv.org/abs/2410.22725v2","abstract":"Recently, the success of Text-to-Image (T2I) models has led to the rise of\nnumerous third-party platforms, which claim to provide cheaper API services and\nmore flexibility in model options. However, this also raises a new security\nconcern: Are these third-party services truly offering the models they claim?\nTo address this problem, we propose the first T2I model verification method\nnamed Text-to-Image Model Verification via Non-Transferable Adversarial Attacks\n(TVN). The non-transferability of adversarial examples means that these\nexamples are only effective on a target model and ineffective on other models,\nthereby allowing for the verification of the target model. TVN utilizes the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine\nsimilarity of a prompt's text encoding, generating non-transferable adversarial\nprompts. By calculating the CLIP-text scores between the non-transferable\nadversarial prompts without perturbations and the images, we can verify if the\nmodel matches the claimed target model, based on a 3-sigma threshold. The\nexperiments showed that TVN performed well in both closed-set and open-set\nscenarios, achieving a verification accuracy of over 90\\%. Moreover, the\nadversarial prompts generated by TVN significantly reduced the CLIP-text scores\nof the target model, while having little effect on other models."},{"date":"2024-10","title":"Community search signatures as foundation features for human-centered geospatial modeling","author":"Mimi Sun, Chaitanya Kamath, Mohit Agarwal, Arbaaz Muslim, Hector Yee, David Schottlander, Shailesh Bavadekar, Niv Efron, Shravya Shetty, and Gautam Prasad","link":"http://arxiv.org/abs/2410.22721v1","abstract":"Aggregated relative search frequencies offer a unique composite signal\nreflecting people's habits, concerns, interests, intents, and general\ninformation needs, which are not found in other readily available datasets.\nTemporal search trends have been successfully used in time series modeling\nacross a variety of domains such as infectious diseases, unemployment rates,\nand retail sales. However, most existing applications require curating\nspecialized datasets of individual keywords, queries, or query clusters, and\nthe search data need to be temporally aligned with the outcome variable of\ninterest. We propose a novel approach for generating an aggregated and\nanonymized representation of search interest as foundation features at the\ncommunity level for geospatial modeling. We benchmark these features using\nspatial datasets across multiple domains. In zip codes with a population\ngreater than 3000 that cover over 95% of the contiguous US population, our\nmodels for predicting missing values in a 20% set of holdout counties achieve\nan average $R^2$ score of 0.74 across 21 health variables, and 0.80 across 6\ndemographic and environmental variables. Our results demonstrate that these\nsearch features can be used for spatial predictions without strict temporal\nalignment, and that the resulting models outperform spatial interpolation and\nstate of the art methods using satellite imagery features."},{"date":"2024-10","title":"Amplitude Expansion Phase Field Crystal (APFC) Modeling based Efficient Dislocation Simulations using Fourier Pseudospectral Method","author":"Xinyi Wei, Yangshuai Wang, Kai Jiang, and Lei Zhang","link":"http://arxiv.org/abs/2410.22720v2","abstract":"Crystalline defects play a critical role in determining the properties of\ncrystalline solids, underscoring the need for accurate computational methods to\nstudy them. Lattice deformation in dislocation simulations, which involves\nchanges in atomic positions, can be described either microscopically by\nspecific atomic configurations or macroscopically by continuum elasticity, each\nwith inherent limitations. The complex amplitude expansion of the phase field\ncrystal (APFC) model provides a mesoscopic approach that bridges these scales.\nIn this paper, we introduce a Fourier pseudospectral method for efficiently\nsolving the APFC model in the context of crystalline defect simulations. This\nstudy marks the first application of the Fourier pseudospectral method to the\nAPFC model. The method fully exploits the system's periodicity and facilitates\nthe implementation of periodic boundary conditions, thanks to its high accuracy\nand computational efficiency. Numerical experiments conducted on\ntwo-dimensional triangular lattices and three-dimensional body-centered cubic\nlattices for edge dislocation geometry optimization have produced strain field\nimages that align well with by continuum elasticity predictions. The findings\ndemonstrate the potential of the APFC model to accurately capture the complex\nstrain fields associated with dislocations at the mesoscopic scales, a key step\ntoward modeling more intricate crystalline defect structures and dynamics."},{"date":"2024-10","title":"LoFLAT: Local Feature Matching using Focused Linear Attention Transformer","author":"Naijian Cao, Renjie He, Yuchao Dai, and Mingyi He","link":"http://arxiv.org/abs/2410.22710v1","abstract":"Local feature matching is an essential technique in image matching and plays\na critical role in a wide range of vision-based applications. However, existing\nTransformer-based detector-free local feature matching methods encounter\nchallenges due to the quadratic computational complexity of attention\nmechanisms, especially at high resolutions. However, while existing\nTransformer-based detector-free local feature matching methods have reduced\ncomputational costs using linear attention mechanisms, they still struggle to\ncapture detailed local interactions, which affects the accuracy and robustness\nof precise local correspondences. In order to enhance representations of\nattention mechanisms while preserving low computational complexity, we propose\nthe LoFLAT, a novel Local Feature matching using Focused Linear Attention\nTransformer in this paper. Our LoFLAT consists of three main modules: the\nFeature Extraction Module, the Feature Transformer Module, and the Matching\nModule. Specifically, the Feature Extraction Module firstly uses ResNet and a\nFeature Pyramid Network to extract hierarchical features. The Feature\nTransformer Module further employs the Focused Linear Attention to refine\nattention distribution with a focused mapping function and to enhance feature\ndiversity with a depth-wise convolution. Finally, the Matching Module predicts\naccurate and robust matches through a coarse-to-fine strategy. Extensive\nexperimental evaluations demonstrate that the proposed LoFLAT outperforms the\nLoFTR method in terms of both efficiency and accuracy."},{"date":"2024-10","title":"FilterViT and DropoutViT: Lightweight Vision Transformer Models for Efficient Attention Mechanisms","author":"Bohang Sun","link":"http://arxiv.org/abs/2410.22709v1","abstract":"In this study, we introduce FilterViT, an enhanced version of MobileViT,\nwhich leverages an attention-based mechanism for early-stage downsampling.\nTraditional QKV operations on high-resolution feature maps are computationally\nintensive due to the abundance of tokens. To address this, we propose a filter\nattention mechanism using a convolutional neural network (CNN) to generate an\nimportance mask, focusing attention on key image regions. The method\nsignificantly reduces computational complexity while maintaining\ninterpretability, as it highlights essential image areas. Experimental results\nshow that FilterViT achieves substantial gains in both efficiency and accuracy\ncompared to other models. We also introduce DropoutViT, a variant that uses a\nstochastic approach for pixel selection, further enhancing robustness."},{"date":"2024-10","title":"Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization","author":"Kento Kawaharazuka, Yoshiki Obinata, Naoaki Kanazawa, Kei Okada, and Masayuki Inaba","link":"http://arxiv.org/abs/2410.22707v1","abstract":"State recognition of the environment and objects, such as the open/closed\nstate of doors and the on/off of lights, is indispensable for robots that\nperform daily life support and security tasks. Until now, state recognition\nmethods have been based on training neural networks from manual annotations,\npreparing special sensors for the recognition, or manually programming to\nextract features from point clouds or raw images. In contrast, we propose a\nrobotic state recognition method using a pre-trained vision-language model,\nwhich is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several\nkinds of language prompts in advance, calculate the similarity between these\nprompts and the current image by ITR, and perform state recognition. By\napplying the optimal weighting to each prompt using black-box optimization,\nstate recognition can be performed with higher accuracy. Experiments show that\nthis theory enables a variety of state recognitions by simply preparing\nmultiple prompts without retraining neural networks or manual programming. In\naddition, since only prompts and their weights need to be prepared for each\nrecognizer, there is no need to prepare multiple models, which facilitates\nresource management. It is possible to recognize the open/closed state of\ntransparent doors, the state of whether water is running or not from a faucet,\nand even the qualitative state of whether a kitchen is clean or not, which have\nbeen challenging so far, through language."},{"date":"2024-10","title":"Multi-Task Interactive Robot Fleet Learning with Visual World Models","author":"Huihan Liu, Yu Zhang, Vaarij Betala, Evan Zhang, James Liu, Crystal Ding, and Yuke Zhu","link":"http://arxiv.org/abs/2410.22689v1","abstract":"Recent advancements in large-scale multi-task robot learning offer the\npotential for deploying robot fleets in household and industrial settings,\nenabling them to perform diverse tasks across various environments. However,\nAI-enabled robots often face challenges with generalization and robustness when\nexposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a\nmulti-task interactive robot fleet learning framework to address these\nchallenges. Sirius-Fleet monitors robot performance during deployment and\ninvolves humans to correct the robot's actions when necessary. We employ a\nvisual world model to predict the outcomes of future actions and build anomaly\npredictors to predict whether they will likely result in anomalies. As the\nrobot autonomy improves, the anomaly predictors automatically adapt their\nprediction criteria, leading to fewer requests for human intervention and\ngradually reducing human workload over time. Evaluations on large-scale\nbenchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task\npolicy performance and monitoring accuracy. We demonstrate Sirius-Fleet's\nperformance in both RoboCasa in simulation and Mutex in the real world, two\ndiverse, large-scale multi-task benchmarks. More information is available on\nthe project website: https://ut-austin-rpl.github.io/sirius-fleet"},{"date":"2024-10","title":"Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings","author":"Yashvir S. Grewal, Edwin V. Bonilla, and Thang D. Bui","link":"http://arxiv.org/abs/2410.22685v1","abstract":"Accurately quantifying uncertainty in large language models (LLMs) is crucial\nfor their reliable deployment, especially in high-stakes applications. Current\nstate-of-the-art methods for measuring semantic uncertainty in LLMs rely on\nstrict bidirectional entailment criteria between multiple generated responses\nand also depend on sequence likelihoods. While effective, these approaches\noften overestimate uncertainty due to their sensitivity to minor wording\ndifferences, additional correct information, and non-important words in the\nsequence. We propose a novel approach that leverages semantic embeddings to\nachieve smoother and more robust estimation of semantic uncertainty in LLMs. By\ncapturing semantic similarities without depending on sequence likelihoods, our\nmethod inherently reduces any biases introduced by irrelevant words in the\nanswers. Furthermore, we introduce an amortised version of our approach by\nexplicitly modelling semantics as latent variables in a joint probabilistic\nmodel. This allows for uncertainty estimation in the embedding space with a\nsingle forward pass, significantly reducing computational overhead compared to\nexisting multi-pass methods. Experiments across multiple question-answering\ndatasets and frontier LLMs demonstrate that our embedding-based methods provide\nmore accurate and nuanced uncertainty quantification than traditional\napproaches."},{"date":"2024-10","title":"Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion","author":"Ji Guo, Hongwei Li, Wenbo Jiang, and Guoming Lu","link":"http://arxiv.org/abs/2410.22678v1","abstract":"Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetworks (CNN) across various computer vision tasks. However, akin to CNN, ViTs\nare vulnerable to backdoor attacks, where the adversary embeds the backdoor\ninto the victim model, causing it to make wrong predictions about testing\nsamples containing a specific trigger. Existing backdoor attacks against ViTs\nhave the limitation of failing to strike an optimal balance between attack\nstealthiness and attack effectiveness.\n  In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB)\ntargeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively\nerodes pixels in areas of maximal attention gradient, embedding a covert\nbackdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves\nan optimal balance between attack stealthiness and attack effectiveness,\nensuring the trigger remains invisible to human detection while preserving the\nmodel's accuracy on clean samples. Extensive experimental evaluations across\nvarious ViT architectures and datasets confirm the effectiveness of AGEB,\nachieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data\nAccuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated,\ndemonstrating minimal visual discrepancies between the clean and the triggered\nimages."},{"date":"2024-10","title":"Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models","author":"Garry Kuwanto, Chaitanya Agarwal, Genta Indra Winata, and Derry Tanti Wijaya","link":"http://arxiv.org/abs/2410.22660v1","abstract":"Code-switching, the phenomenon of alternating between two or more languages\nin a single conversation, presents unique challenges for Natural Language\nProcessing (NLP). Most existing research focuses on either syntactic\nconstraints or neural generation, with few efforts to integrate linguistic\ntheory with large language models (LLMs) for generating natural code-switched\ntext. In this paper, we introduce EZSwitch, a novel framework that combines\nEquivalence Constraint Theory (ECT) with LLMs to produce linguistically valid\nand fluent code-switched text. We evaluate our method using both human\njudgments and automatic metrics, demonstrating a significant improvement in the\nquality of generated code-switching sentences compared to baseline LLMs. To\naddress the lack of suitable evaluation metrics, we conduct a comprehensive\ncorrelation study of various automatic metrics against human scores, revealing\nthat current metrics often fail to capture the nuanced fluency of code-switched\ntext. Additionally, we create CSPref, a human preference dataset based on human\nratings and analyze model performance across ``hard`` and ``easy`` examples.\nOur findings indicate that incorporating linguistic constraints into LLMs leads\nto more robust and human-aligned generation, paving the way for scalable\ncode-switching text generation across diverse language pairs."},{"date":"2024-10","title":"Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation","author":"Daehee Lee, Minjong Yoo, Woo Kyung Kim, Wonje Choi, and Honguk Woo","link":"http://arxiv.org/abs/2410.22658v1","abstract":"Continual Imitation Learning (CiL) involves extracting and accumulating task\nknowledge from demonstrations across multiple stages and tasks to achieve a\nmulti-task policy. With recent advancements in foundation models, there has\nbeen a growing interest in adapter-based CiL approaches, where adapters are\nestablished parameter-efficiently for tasks newly demonstrated. While these\napproaches isolate parameters for specific tasks and tend to mitigate\ncatastrophic forgetting, they limit knowledge sharing among different\ndemonstrations. We introduce IsCiL, an adapter-based CiL framework that\naddresses this limitation of knowledge sharing by incrementally learning\nshareable skills from different demonstrations, thus enabling sample-efficient\ntask adaptation using the skills particularly in non-stationary CiL\nenvironments. In IsCiL, demonstrations are mapped into the state embedding\nspace, where proper skills can be retrieved upon input states through\nprototype-based memory. These retrievable skills are incrementally learned on\ntheir corresponding adapters. Our CiL experiments with complex tasks in\nFranka-Kitchen and Meta-World demonstrate robust performance of IsCiL in both\ntask adaptation and sample-efficiency. We also show a simple extension of IsCiL\nfor task unlearning scenarios."},{"date":"2024-10","title":"Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem","author":"Jin Huang, Xinyu Li, Liang Gao, Qihao Liu, and Yue Teng","link":"http://arxiv.org/abs/2410.22657v1","abstract":"Heuristic dispatching rules (HDRs) are widely regarded as effective methods\nfor solving dynamic job shop scheduling problems (DJSSP) in real-world\nproduction environments. However, their performance is highly\nscenario-dependent, often requiring expert customization. To address this,\ngenetic programming (GP) and gene expression programming (GEP) have been\nextensively used for automatic algorithm design. Nevertheless, these approaches\noften face challenges due to high randomness in the search process and limited\ngeneralization ability, hindering the application of trained dispatching rules\nto new scenarios or dynamic environments. Recently, the integration of large\nlanguage models (LLMs) with evolutionary algorithms has opened new avenues for\nprompt engineering and automatic algorithm design. To enhance the capabilities\nof LLMs in automatic HDRs design, this paper proposes a novel population\nself-evolutionary (SeEvo) method, a general search framework inspired by the\nself-reflective design strategies of human experts. The SeEvo method\naccelerates the search process and enhances exploration capabilities.\nExperimental results show that the proposed SeEvo method outperforms GP, GEP,\nend-to-end deep reinforcement learning methods, and more than 10 common HDRs\nfrom the literature, particularly in unseen and dynamic scenarios."},{"date":"2024-10","title":"WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting","author":"Aobo Liang, and Yan Sun","link":"http://arxiv.org/abs/2410.22649v1","abstract":"In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."},{"date":"2024-10","title":"SleepNetZero: Zero-Burden Zero-Shot Reliable Sleep Staging With Neural Networks Based on Ballistocardiograms","author":"Shuzhen Li, Yuxin Chen, Xuesong Chen, Ruiyang Gao, Yupeng Zhang, Chao Yu, Yunfei Li, Ziyi Ye, Weijun Huang, Hongliang Yi, Yue Leng, and Yi Wu","link":"http://arxiv.org/abs/2410.22646v1","abstract":"Sleep monitoring plays a crucial role in maintaining good health, with sleep\nstaging serving as an essential metric in the monitoring process. Traditional\nmethods, utilizing medical sensors like EEG and ECG, can be effective but often\npresent challenges such as unnatural user experience, complex deployment, and\nhigh costs. Ballistocardiography~(BCG), a type of piezoelectric sensor signal,\noffers a non-invasive, user-friendly, and easily deployable alternative for\nlong-term home monitoring. However, reliable BCG-based sleep staging is\nchallenging due to the limited sleep monitoring data available for BCG. A\nrestricted training dataset prevents the model from generalization across\npopulations. Additionally, transferring to BCG faces difficulty ensuring model\nrobustness when migrating from other data sources. To address these issues, we\nintroduce SleepNetZero, a zero-shot learning based approach for sleep staging.\nTo tackle the generalization challenge, we propose a series of BCG feature\nextraction methods that align BCG components with corresponding respiratory,\ncardiac, and movement channels in PSG. This allows models to be trained on\nlarge-scale PSG datasets that are diverse in population. For the migration\nchallenge, we employ data augmentation techniques, significantly enhancing\ngeneralizability. We conducted extensive training and testing on large\ndatasets~(12393 records from 9637 different subjects), achieving an accuracy of\n0.803 and a Cohen's Kappa of 0.718. ZeroSleepNet was also deployed in real\nprototype~(monitoring pads) and tested in actual hospital settings~(265 users),\ndemonstrating an accuracy of 0.697 and a Cohen's Kappa of 0.589. To the best of\nour knowledge, this work represents the first known reliable BCG-based sleep\nstaging effort and marks a significant step towards in-home health monitoring."},{"date":"2024-10","title":"Consistency Diffusion Bridge Models","author":"Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, and Jun Zhu","link":"http://arxiv.org/abs/2410.22637v2","abstract":"Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space."},{"date":"2024-10","title":"CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation","author":"Ziyang Gong, Zhixiang Wei, Di Wang, Xianzheng Ma, Hongruixuan Chen, Yuru Jia, Yupeng Deng, Zhenming Ji, Xiangwei Zhu, Naoto Yokoya, Jing Zhang, Bo Du, and Liangpei Zhang","link":"http://arxiv.org/abs/2410.22629v2","abstract":"The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods."},{"date":"2024-10","title":"FISC: Federated Domain Generalization via Interpolative Style Transfer and Contrastive Learning","author":"Dung Thuy Nguyen, Taylor T. Johnson, and Kevin Leach","link":"http://arxiv.org/abs/2410.22622v1","abstract":"Federated Learning (FL) shows promise in preserving privacy and enabling\ncollaborative learning. However, most current solutions focus on private data\ncollected from a single domain. A significant challenge arises when client data\ncomes from diverse domains (i.e., domain shift), leading to poor performance on\nunseen domains. Existing Federated Domain Generalization approaches address\nthis problem but assume each client holds data for an entire domain, limiting\ntheir practicality in real-world scenarios with domain-based heterogeneity and\nclient sampling.\n  To overcome this, we introduce FISC, a novel FL domain generalization\nparadigm that handles more complex domain distributions across clients. FISC\nenables learning across domains by extracting an interpolative style from local\nstyles and employing contrastive learning. This strategy gives clients\nmulti-domain representations and unbiased convergent targets. Empirical results\non multiple datasets, including PACS, Office-Home, and IWildCam, show FISC\noutperforms state-of-the-art (SOTA) methods. Our method achieves accuracy\nimprovements ranging from 3.64% to 57.22% on unseen domains. Our code is\navailable at https://anonymous.4open.science/r/FISC-AAAI-16107."},{"date":"2024-10","title":"Efficient Feature Extraction and Classification Architecture for MRI-Based Brain Tumor Detection","author":"Plabon Paul, Md. Nazmul Islam, Fazle Rafsani, Pegah Khorasani, and Shovito Barua Soumma","link":"http://arxiv.org/abs/2410.22619v1","abstract":"Uncontrolled cell division in the brain is what gives rise to brain tumors.\nIf the tumor size increases by more than half, there is little hope for the\npatient's recovery. This emphasizes the need of rapid and precise brain tumor\ndiagnosis. When it comes to analyzing, diagnosing, and planning therapy for\nbrain tumors, MRI imaging plays a crucial role. A brain tumor's development\nhistory is crucial information for doctors to have. When it comes to\ndistinguishing between human soft tissues, MRI scans are superior. In order to\nget reliable classification results from MRI scans quickly, deep learning is\none of the most practical methods. Early human illness diagnosis has been\ndemonstrated to be more accurate when deep learning methods are used. In the\ncase of diagnosing a brain tumor, when even a little misdiagnosis might have\nserious consequences, accuracy is especially important. Disclosure of brain\ntumors in medical images is still a difficult task. Brain MRIs are notoriously\nimprecise in revealing the presence or absence of tumors. Using MRI scans of\nthe brain, a Convolutional Neural Network (CNN) was trained to identify the\npresence of a tumor in this research. Results from the CNN model showed an\naccuracy of 99.17%. The CNN model's characteristics were also retrieved. In\norder to evaluate the CNN model's capability for processing images, we applied\nthe features via the following machine learning models: KNN, Logistic\nregression, SVM, Random Forest, Naive Bayes, and Perception. CNN and machine\nlearning models were also evaluated using the standard metrics of Precision,\nRecall, Specificity, and F1 score. The significance of the doctor's diagnosis\nenhanced the accuracy of the CNN model's assistance in identifying the\nexistence of tumor and treating the patient."},{"date":"2024-10","title":"CoGS: Model Agnostic Causality Constrained Counterfactual Explanations using goal-directed ASP","author":"Sopam Dasgupta, Joaqu\u00edn Arias, Elmer Salazar, and Gopal Gupta","link":"http://arxiv.org/abs/2410.22615v1","abstract":"Machine learning models are increasingly used in critical areas such as loan\napprovals and hiring, yet they often function as black boxes, obscuring their\ndecision-making processes. Transparency is crucial, as individuals need\nexplanations to understand decisions, primarily if the decisions result in an\nundesired outcome. Our work introduces CoGS (Counterfactual Generation with\ns(CASP)), a model-agnostic framework capable of generating counterfactual\nexplanations for classification models. CoGS leverages the goal-directed Answer\nSet Programming system s(CASP) to compute realistic and causally consistent\nmodifications to feature values, accounting for causal dependencies between\nthem. By using rule-based machine learning algorithms (RBML), notably the\nFOLD-SE algorithm, CoGS extracts the underlying logic of a statistical model to\ngenerate counterfactual solutions. By tracing a step-by-step path from an\nundesired outcome to a desired one, CoGS offers interpretable and actionable\nexplanations of the changes required to achieve the desired outcome. We present\ndetails of the CoGS framework along with its evaluation."},{"date":"2024-10","title":"Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse","author":"Seung Hyun Cheon, Anneke Wernerfelt, Sorelle A. Friedler, and Berk Ustun","link":"http://arxiv.org/abs/2410.22598v1","abstract":"Machine learning models are often used to automate or support decisions in\napplications such as lending and hiring. In such settings, consumer protection\nrules mandate that we provide a list of \"principal reasons\" to consumers who\nreceive adverse decisions. In practice, lenders and employers identify\nprincipal reasons by returning the top-scoring features from a feature\nattribution method. In this work, we study how such practices align with one of\nthe underlying goals of consumer protection - recourse - i.e., educating\nindividuals on how they can attain a desired outcome. We show that standard\nattribution methods can mislead individuals by highlighting reasons without\nrecourse - i.e., by presenting consumers with features that cannot be changed\nto achieve recourse. We propose to address these issues by scoring features on\nthe basis of responsiveness - i.e., the probability that an individual can\nattain a desired outcome by changing a specific feature. We develop efficient\nmethods to compute responsiveness scores for any model and any dataset under\ncomplex actionability constraints. We present an extensive empirical study on\nthe responsiveness of explanations in lending and demonstrate how\nresponsiveness scores can be used to construct feature-highlighting\nexplanations that lead to recourse and mitigate harm by flagging instances with\nfixed predictions."},{"date":"2024-10","title":"Are Large-Language Models Graph Algorithmic Reasoners?","author":"Alexander K Taylor, Anthony Cuturrufo, Vishal Yathish, Mingyu Derek Ma, and Wei Wang","link":"http://arxiv.org/abs/2410.22597v1","abstract":"We seek to address a core challenge facing current Large Language Models\n(LLMs). LLMs have demonstrated superior performance in many tasks, yet continue\nto struggle with reasoning problems on explicit graphs that require multiple\nsteps. To address this gap, we introduce a novel benchmark designed to evaluate\nLLM performance on classical algorithmic reasoning tasks on explicit graphs.\nOur benchmark encompasses five fundamental algorithms: Breadth-First Search\n(BFS) and Depth-First Search (DFS) for connectivity, Dijkstra's algorithm and\nFloyd-Warshall algorithm for all nodes shortest path, and Prim's Minimum\nSpanning Tree (MST-Prim's) algorithm. Through extensive experimentation, we\nassess the capabilities of state-of-the-art LLMs in executing these algorithms\nstep-by-step and systematically evaluate their performance at each stage. Our\nfindings highlight the persistent challenges LLMs face in this domain and\nunderscore the necessity for advanced prompting techniques and algorithmic\ninstruction to enhance their graph reasoning abilities. This work presents\nMAGMA, the first comprehensive benchmark focused on LLMs completing classical\ngraph algorithms, and provides a critical step toward understanding and\nimproving their structured problem-solving skills."},{"date":"2024-10","title":"GRADE: Quantifying Sample Diversity in Text-to-Image Models","author":"Royi Rassin, Aviv Slobodkin, Shauli Ravfogel, Yanai Elazar, and Yoav Goldberg","link":"http://arxiv.org/abs/2410.22592v1","abstract":"Text-to-image (T2I) models are remarkable at generating realistic images\nbased on textual descriptions. However, textual prompts are inherently\nunderspecified: they do not specify all possible attributes of the required\nimage. This raises two key questions: Do T2I models generate diverse outputs on\nunderspecified prompts? How can we automatically measure diversity? We propose\nGRADE: Granular Attribute Diversity Evaluation, an automatic method for\nquantifying sample diversity. GRADE leverages the world knowledge embedded in\nlarge language models and visual question-answering systems to identify\nrelevant concept-specific axes of diversity (e.g., ``shape'' and ``color'' for\nthe concept ``cookie''). It then estimates frequency distributions of concepts\nand their attributes and quantifies diversity using (normalized) entropy. GRADE\nachieves over 90% human agreement while exhibiting weak correlation to commonly\nused diversity metrics. We use GRADE to measure the overall diversity of 12 T2I\nmodels using 400 concept-attribute pairs, revealing that all models display\nlimited variation. Further, we find that these models often exhibit default\nbehaviors, a phenomenon where the model consistently generates concepts with\nthe same attributes (e.g., 98% of the cookies are round). Finally, we\ndemonstrate that a key reason for low diversity is due to underspecified\ncaptions in training data. Our work proposes a modern, semantically-driven\napproach to measure sample diversity and highlights the stunning homogeneity in\noutputs by T2I models."},{"date":"2024-10","title":"Memory and Friction: From the Nanoscale to the Macroscale","author":"Benjamin A. Dalton, Anton Klimek, Henrik Kiefer, Florian N. Br\u00fcnig, H\u00e9l\u00e8ne Colinet, Lucas Tepper, Amir Abbasi, and Roland R. Netz","link":"http://arxiv.org/abs/2410.22588v1","abstract":"Friction is a phenomenon that manifests across all spatial and temporal\nscales, from the molecular to the macroscopic scale. It describes the\ndissipation of energy from the motion of particles or abstract reaction\ncoordinates and arises in the transition from a detailed molecular-level\ndescription to a simplified, coarse-grained model. It has long been understood\nthat time-dependent (non-Markovian) friction effects are critical for\ndescribing the dynamics of many systems, but that they are notoriously\ndifficult to evaluate for complex physical, chemical, and biological systems.\nIn recent years, the development of advanced numerical friction extraction\ntechniques and methods to simulate the generalized Langevin equation have\nenabled exploration of the role of time-dependent friction across all scales.\nWe discuss recent applications of these friction extraction techniques and the\ngrowing understanding of the role of friction in complex equilibrium and\nnon-equilibrium dynamic many-body systems."},{"date":"2024-10","title":"Pre-Trained Vision Models as Perception Backbones for Safety Filters in Autonomous Driving","author":"Yuxuan Yang, and Hussein Sibai","link":"http://arxiv.org/abs/2410.22585v1","abstract":"End-to-end vision-based autonomous driving has achieved impressive success,\nbut safety remains a major concern. The safe control problem has been addressed\nin low-dimensional settings using safety filters, e.g., those based on control\nbarrier functions. Designing safety filters for vision-based controllers in the\nhigh-dimensional settings of autonomous driving can similarly alleviate the\nsafety problem, but is significantly more challenging. In this paper, we\naddress this challenge by using frozen pre-trained vision representation models\nas perception backbones to design vision-based safety filters, inspired by\nthese models' success as backbones of robotic control policies. We empirically\nevaluate the offline performance of four common pre-trained vision models in\nthis context. We try three existing methods for training safety filters for\nblack-box dynamics, as the dynamics over representation spaces are not known.\nWe use the DeepAccident dataset that consists of action-annotated videos from\nmultiple cameras on vehicles in CARLA simulating real accident scenarios. Our\nresults show that the filters resulting from our approach are competitive with\nthe ones that are given the ground truth state of the ego vehicle and its\nenvironment."},{"date":"2024-10","title":"BENCHAGENTS: Automated Benchmark Creation with Agent Interaction","author":"Natasha Butt, Varun Chandrasekaran, Neel Joshi, Besmira Nushi, and Vidhisha Balachandran","link":"http://arxiv.org/abs/2410.22584v1","abstract":"Evaluations are limited by benchmark availability. As models evolve, there is\na need to create benchmarks that can measure progress on new generative\ncapabilities. However, creating new benchmarks through human annotations is\nslow and expensive, restricting comprehensive evaluations for any capability.\nWe introduce BENCHAGENTS, a framework that methodically leverages large\nlanguage models (LLMs) to automate benchmark creation for complex capabilities\nwhile inherently ensuring data and metric quality. BENCHAGENTS decomposes the\nbenchmark creation process into planning, generation, data verification, and\nevaluation, each of which is executed by an LLM agent. These agents interact\nwith each other and utilize human-in-the-loop feedback from benchmark\ndevelopers to explicitly improve and flexibly control data diversity and\nquality. We use BENCHAGENTS to create benchmarks to evaluate capabilities\nrelated to planning and constraint satisfaction during text generation. We then\nuse these benchmarks to study seven state-of-the-art models and extract new\ninsights on common failure modes and model differences."},{"date":"2024-10","title":"Improved Patch Denoising Diffusion Probabilistic Models for Magnetic Resonance Fingerprinting","author":"Perla Mayo, Carolin M. Pirkl, Alin Achim, Bjoern H. Menze, and Mohammad Golbabaee","link":"http://arxiv.org/abs/2410.23318v1","abstract":"Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach."},{"date":"2024-10","title":"Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents","author":"Jaekyeom Kim, Dong-Ki Kim, Lajanugen Logeswaran, Sungryull Sohn, and Honglak Lee","link":"http://arxiv.org/abs/2410.22552v1","abstract":"In this paper, we introduce Auto-Intent, a method to adapt a pre-trained\nlarge language model (LLM) as an agent for a target domain without direct\nfine-tuning, where we empirically focus on web navigation tasks. Our approach\nfirst discovers the underlying intents from target domain demonstrations\nunsupervisedly, in a highly compact form (up to three words). With the\nextracted intents, we train our intent predictor to predict the next intent\ngiven the agent's past observations and actions. In particular, we propose a\nself-exploration approach where top-k probable intent predictions are provided\nas a hint to the pre-trained LLM agent, which leads to enhanced decision-making\ncapabilities. Auto-Intent substantially improves the performance of GPT-{3.5,\n4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation\nbenchmarks from Mind2Web and online navigation tasks from WebArena with its\ncross-benchmark generalization from Mind2Web."},{"date":"2024-10","title":"Bayesian shared parameter joint models for heterogeneous populations","author":"Sida Chen, Danilo Alvares, Marco Palma, and Jessica K. Barrett","link":"http://arxiv.org/abs/2410.22534v1","abstract":"Joint models (JMs) for longitudinal and time-to-event data are an important\nclass of biostatistical models in health and medical research. When the study\npopulation consists of heterogeneous subgroups, the standard JM may be\ninadequate and lead to misleading results. Joint latent class models (JLCMs)\nand their variants have been proposed to incorporate latent class structures\ninto JMs. JLCMs are useful for identifying latent subgroup structures,\nobtaining a more nuanced understanding of the relationships between\nlongitudinal outcomes, and improving prediction performance. We consider the\ngeneric form of JLCM, which poses significant computational challenges for both\nfrequentist and Bayesian approaches due to the numerical intractability and\nmultimodality of the associated model's likelihood or posterior. Focusing on\nthe less explored Bayesian paradigm, we propose a new Bayesian inference\nframework to tackle key limitations in the existing method. Our algorithm\nleverages state-of-the-art Markov chain Monte Carlo techniques and parallel\ncomputing for parameter estimation and model selection. Through a simulation\nstudy, we demonstrate the feasibility and superiority of our proposed method\nover the existing approach. Our simulations also generate important\ncomputational insights and practical guidance for implementing such complex\nmodels. We illustrate our method using data from the PAQUID prospective cohort\nstudy, where we jointly investigate the association between a repeatedly\nmeasured cognitive score and the risk of dementia and the latent class\nstructure defined from the longitudinal outcomes."},{"date":"2024-10","title":"Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models","author":"Rishabh Adiga, Besmira Nushi, and Varun Chandrasekaran","link":"http://arxiv.org/abs/2410.22517v1","abstract":"We explore the internal mechanisms of how bias emerges in large language\nmodels (LLMs) when provided with ambiguous comparative prompts: inputs that\ncompare or enforce choosing between two or more entities without providing\nclear context for preference. Most approaches for bias mitigation focus on\neither post-hoc analysis or data augmentation. However, these are transient\nsolutions, without addressing the root cause: the model itself. Numerous prior\nworks show the influence of the attention module towards steering generations.\nWe believe that analyzing attention is also crucial for understanding bias, as\nit provides insight into how the LLM distributes its focus across different\nentities and how this contributes to biased decisions. To this end, we first\nintroduce a metric to quantify the LLM's preference for one entity over\nanother. We then propose $\\texttt{ATLAS}$ (Attention-based Targeted Layer\nAnalysis and Scaling), a technique to localize bias to specific layers of the\nLLM by analyzing attention scores and then reduce bias by scaling attention in\nthese biased layers. To evaluate our method, we conduct experiments across 3\ndatasets (BBQ, Crows-Pairs, and WinoGender) using $\\texttt{GPT-2 XL}$ (1.5B),\n$\\texttt{GPT-J}$ (6B), $\\texttt{LLaMA-2}$ (7B) and $\\texttt{LLaMA-3}$ (8B). Our\nexperiments demonstrate that bias is concentrated in the later layers,\ntypically around the last third. We also show how $\\texttt{ATLAS}$ effectively\nmitigates bias through targeted interventions without compromising downstream\nperformance and an average increase of only 0.82% in perplexity when the\nintervention is applied. We see an average improvement of 0.28 points in the\nbias score across all the datasets."},{"date":"2024-10","title":"Temporal and Spatial Super Resolution with Latent Diffusion Model in Medical MRI images","author":"Vishal Dubey","link":"http://arxiv.org/abs/2410.23898v1","abstract":"Super Resolution (SR) plays a critical role in computer vision, particularly\nin medical imaging, where hardware and acquisition time constraints often\nresult in low spatial and temporal resolution. While diffusion models have been\napplied for both spatial and temporal SR, few studies have explored their use\nfor joint spatial and temporal SR, particularly in medical imaging. In this\nwork, we address this gap by proposing to use a Latent Diffusion Model (LDM)\ncombined with a Vector Quantised GAN (VQGAN)-based encoder-decoder architecture\nfor joint super resolution. We frame SR as an image denoising problem, focusing\non improving both spatial and temporal resolution in medical images. Using the\ncardiac MRI dataset from the Data Science Bowl Cardiac Challenge, consisting of\n2D cine images with a spatial resolution of 256x256 and 8-14 slices per\ntime-step, we demonstrate the effectiveness of our approach. Our LDM model\nachieves Peak Signal to Noise Ratio (PSNR) of 30.37, Structural Similarity\nIndex (SSIM) of 0.7580, and Learned Perceptual Image Patch Similarity (LPIPS)\nof 0.2756, outperforming simple baseline method by 5% in PSNR, 6.5% in SSIM,\n39% in LPIPS. Our LDM model generates images with high fidelity and perceptual\nquality with 15 diffusion steps. These results suggest that LDMs hold promise\nfor advancing super resolution in medical imaging, potentially enhancing\ndiagnostic accuracy and patient outcomes. Code link is also shared."},{"date":"2024-10","title":"VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration","author":"Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, and Panpan Xu","link":"http://arxiv.org/abs/2410.23317v1","abstract":"Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."},{"date":"2024-10","title":"Performance of the Segment Anything Model in Various RFI/Events Detection in Radio Astronomy","author":"Yanbin Yang, Feiyu Zhao, Ruxi Liang, Quan Guo, Junhua Gu, Yan Huang, and Yun Yu","link":"http://arxiv.org/abs/2410.22497v1","abstract":"The emerging era of big data in radio astronomy demands more efficient and\nhigher-quality processing of observational data. While deep learning methods\nhave been applied to tasks such as automatic radio frequency interference (RFI)\ndetection, these methods often face limitations, including dependence on\ntraining data and poor generalization, which are also common issues in other\ndeep learning applications within astronomy. In this study, we investigate the\nuse of the open-source image recognition and segmentation model, Segment\nAnything Model (SAM), and its optimized version, HQ-SAM, due to their\nimpressive generalization capabilities. We evaluate these models across various\ntasks, including RFI detection and solar radio burst (SRB) identification. For\nRFI detection, HQ-SAM (SAM) shows performance that is comparable to or even\nsuperior to the SumThreshold method, especially with large-area broadband RFI\ndata. In the search for SRBs, HQ-SAM demonstrates strong recognition abilities\nfor Type II and Type III bursts. Overall, with its impressive generalization\ncapability, SAM (HQ-SAM) can be a promising candidate for further optimization\nand application in RFI and event detection tasks in radio astronomy."},{"date":"2024-10","title":"Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation","author":"Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, and Serge Belongie","link":"http://arxiv.org/abs/2410.22489v1","abstract":"Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to\nsegment novel categories with minimal annotated support samples. While existing\nFS-PCS methods have shown promise, they primarily focus on unimodal point cloud\ninputs, overlooking the potential benefits of leveraging multimodal\ninformation. In this paper, we address this gap by introducing a cost-free\nmultimodal FS-PCS setup, utilizing textual labels and the potentially available\n2D image modality. Under this easy-to-achieve setup, we present the MultiModal\nFew-Shot SegNet (MM-FSS), a model effectively harnessing complementary\ninformation from multiple modalities. MM-FSS employs a shared backbone with two\nheads to extract intermodal and unimodal visual features, and a pretrained text\nencoder to generate text embeddings. To fully exploit the multimodal\ninformation, we propose a Multimodal Correlation Fusion (MCF) module to\ngenerate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module\nto refine the correlations using text-aware semantic guidance. Additionally, we\npropose a simple yet effective Test-time Adaptive Cross-modal Calibration\n(TACC) technique to mitigate training bias, further improving generalization.\nExperimental results on S3DIS and ScanNet datasets demonstrate significant\nperformance improvements achieved by our method. The efficacy of our approach\nindicates the benefits of leveraging commonly-ignored free modalities for\nFS-PCS, providing valuable insights for future research. The code is available\nat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot ."},{"date":"2024-10","title":"Bayesian Counterfactual Prediction Models for HIV Care Retention with Incomplete Outcome and Covariate Information","author":"Arman Oganisian, Joseph Hogan, Edwin Sang, Allison DeLong, Ben Mosong, Hamish Fraser, and Ann Mwangi","link":"http://arxiv.org/abs/2410.22481v1","abstract":"Like many chronic diseases, human immunodeficiency virus (HIV) is managed\nover time at regular clinic visits. At each visit, patient features are\nassessed, treatments are prescribed, and a subsequent visit is scheduled. There\nis a need for data-driven methods for both predicting retention and\nrecommending scheduling decisions that optimize retention. Prediction models\ncan be useful for estimating retention rates across a range of scheduling\noptions. However, training such models with electronic health records (EHR)\ninvolves several complexities. First, formal causal inference methods are\nneeded to adjust for observed confounding when estimating retention rates under\ncounterfactual scheduling decisions. Second, competing events such as death\npreclude retention, while censoring events render retention missing. Third,\ninconsistent monitoring of features such as viral load and CD4 count lead to\ncovariate missingness. This paper presents an all-in-one approach for both\npredicting HIV retention and optimizing scheduling while accounting for these\ncomplexities. We formulate and identify causal retention estimands in terms of\npotential return-time under a hypothetical scheduling decision. Flexible\nBayesian approaches are used to model the observed return-time distribution\nwhile accounting for competing and censoring events and form posterior point\nand uncertainty estimates for these estimands. We address the urgent need for\ndata-driven decision support in HIV care by applying our method to EHR from the\nAcademic Model Providing Access to Healthcare (AMPATH) - a consortium of\nclinics that treat HIV in Western Kenya."},{"date":"2024-10","title":"Image2Struct: Benchmarking Structure Extraction for Vision-Language Models","author":"Josselin Somerville Roberts, Tony Lee, Chi Heem Wong, Michihiro Yasunaga, Yifan Mai, and Percy Liang","link":"http://arxiv.org/abs/2410.22456v1","abstract":"We introduce Image2Struct, a benchmark to evaluate vision-language models\n(VLMs) on extracting structure from images. Our benchmark 1) captures\nreal-world use cases, 2) is fully automatic and does not require human\njudgment, and 3) is based on a renewable stream of fresh data. In Image2Struct,\nVLMs are prompted to generate the underlying structure (e.g., LaTeX code or\nHTML) from an input image (e.g., webpage screenshot). The structure is then\nrendered to produce an output image (e.g., rendered webpage), which is compared\nagainst the input image to produce a similarity score. This round-trip\nevaluation allows us to quantitatively evaluate VLMs on tasks with multiple\nvalid structures. We create a pipeline that downloads fresh data from active\nonline communities upon execution and evaluates the VLMs without human\nintervention. We introduce three domains (Webpages, LaTeX, and Musical Scores)\nand use five image metrics (pixel similarity, cosine similarity between the\nInception vectors, learned perceptual image patch similarity, structural\nsimilarity index measure, and earth mover similarity) that allow efficient and\nautomatic comparison between pairs of images. We evaluate Image2Struct on 14\nprominent VLMs and find that scores vary widely, indicating that Image2Struct\ncan differentiate between the performances of different VLMs. Additionally, the\nbest score varies considerably across domains (e.g., 0.402 on sheet music vs.\n0.830 on LaTeX equations), indicating that Image2Struct contains tasks of\nvarying difficulty. For transparency, we release the full results at\nhttps://crfm.stanford.edu/helm/image2struct/v1.0.1/."},{"date":"2024-10","title":"Explainable convolutional neural network model provides an alternative genome-wide association perspective on mutations in SARS-CoV-2","author":"Parisa Hatami, Richard Annan, Luis Urias Miranda, Jane Gorman, Mengjun Xie, Letu Qingge, and Hong Qin","link":"http://arxiv.org/abs/2410.22452v1","abstract":"Identifying mutations of SARS-CoV-2 strains associated with their phenotypic\nchanges is critical for pandemic prediction and prevention. We compared an\nexplainable convolutional neural network (CNN) and the traditional genome-wide\nassociation study (GWAS) on the mutations associated with WHO labels of\nSARS-CoV-2, a proxy for virulence phenotypes. We trained a CNN classification\nmodel that can predict genomic sequences into Variants of Concern (VOCs), and\nthen applied Shapley Additive explanations (SHAP) model to identify mutations\nthat are important for the correct predictions. For comparison, we performed\ntraditional GWAS to identify mutations associated with VOCs. Comparison of the\ntwo approaches shows that the explainable neural network approach can more\neffectively reveal known nucleotide substitutions associated with VOCs, such as\nthose in the spike gene regions. Our results suggest that explainable neural\nnetworks for genomic sequences offer a promising alternative to the traditional\ngenome wide analysis approaches."},{"date":"2024-10","title":"Do Large Language Models Align with Core Mental Health Counseling Competencies?","author":"Viet Cuong Nguyen, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha Thirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, Heather J. Soled, Michael L. Birnbaum, Srijan Kumar, and Munmun De Choudhury","link":"http://arxiv.org/abs/2410.22446v1","abstract":"The rapid evolution of Large Language Models (LLMs) offers promising\npotential to alleviate the global scarcity of mental health professionals.\nHowever, LLMs' alignment with essential mental health counseling competencies\nremains understudied. We introduce CounselingBench, a novel NCMHCE-based\nbenchmark evaluating LLMs across five key mental health counseling\ncompetencies. Testing 22 general-purpose and medical-finetuned LLMs, we find\nfrontier models exceed minimum thresholds but fall short of expert-level\nperformance, with significant variations: they excel in Intake, Assessment &\nDiagnosis yet struggle with Core Counseling Attributes and Professional\nPractice & Ethics. Medical LLMs surprisingly underperform generalist models\naccuracy-wise, while at the same time producing slightly higher-quality\njustifications but making more context-related errors. Our findings highlight\nthe complexities of developing AI systems for mental health counseling,\nparticularly for competencies requiring empathy and contextual understanding.\nWe found that frontier LLMs perform at a level exceeding the minimal required\nlevel of aptitude for all key mental health counseling competencies, but fall\nshort of expert-level performance, and that current medical LLMs do not\nsignificantly improve upon generalist models in mental health counseling\ncompetencies. This underscores the critical need for specialized, mental health\ncounseling-specific fine-tuned LLMs that rigorously aligns with core\ncompetencies combined with appropriate human supervision before any responsible\nreal-world deployment can be considered."},{"date":"2024-10","title":"Embedding Watermarks in Diffusion Process for Model Intellectual Property Protection","author":"Jijia Yang, Sen Peng, and Xiaohua Jia","link":"http://arxiv.org/abs/2410.22445v1","abstract":"In practical application, the widespread deployment of diffusion models often\nnecessitates substantial investment in training. As diffusion models find\nincreasingly diverse applications, concerns about potential misuse highlight\nthe imperative for robust intellectual property protection. Current protection\nstrategies either employ backdoor-based methods, integrating a watermark task\nas a simpler training objective with the main model task, or embedding\nwatermarks directly into the final output samples. However, the former approach\nis fragile compared to existing backdoor defense techniques, while the latter\nfundamentally alters the expected output. In this work, we introduce a novel\nwatermarking framework by embedding the watermark into the whole diffusion\nprocess, and theoretically ensure that our final output samples contain no\nadditional information. Furthermore, we utilize statistical algorithms to\nverify the watermark from internally generated model samples without\nnecessitating triggers as conditions. Detailed theoretical analysis and\nexperimental validation demonstrate the effectiveness of our proposed method."},{"date":"2024-10","title":"Point cloud-based diffusion models for the Electron-Ion Collider","author":"Jack Y. Araz, Vinicius Mikuni, Felix Ringer, Nobuo Sato, Fernando Torales Acosta, and Richard Whitehill","link":"http://arxiv.org/abs/2410.22421v1","abstract":"At high-energy collider experiments, generative models can be used for a wide\nrange of tasks, including fast detector simulations, unfolding, searches of\nphysics beyond the Standard Model, and inference tasks. In particular, it has\nbeen demonstrated that score-based diffusion models can generate high-fidelity\nand accurate samples of jets or collider events. This work expands on previous\ngenerative models in three distinct ways. First, our model is trained to\ngenerate entire collider events, including all particle species with complete\nkinematic information. We quantify how well the model learns event-wide\nconstraints such as the conservation of momentum and discrete quantum numbers.\nWe focus on the events at the future Electron-Ion Collider, but we expect that\nour results can be extended to proton-proton and heavy-ion collisions. Second,\nprevious generative models often relied on image-based techniques. The sparsity\nof the data can negatively affect the fidelity and sampling time of the model.\nWe address these issues using point clouds and a novel architecture combining\nedge creation with transformer modules called Point Edge Transformers. Third,\nwe adapt the foundation model OmniLearn, to generate full collider events. This\napproach may indicate a transition toward adapting and fine-tuning foundation\nmodels for downstream tasks instead of training new models from scratch."},{"date":"2024-10","title":"Hypothesis tests and model parameter estimation on data sets with missing correlation information","author":"Lukas Koch","link":"http://arxiv.org/abs/2410.22333v1","abstract":"Ideally, all analyses of normally distributed data should include the full\ncovariance information between all data points. In practice, the full\ncovariance matrix between all data points is not always available. Either\nbecause a result was published without a covariance matrix, or because one\ntries to combine multiple results from separate publications. For simple\nhypothesis tests, it is possible to define robust test statistics that will\nbehave conservatively in the presence on unknown correlations. For model\nparameter fits, one can inflate the variance by factor to ensure that things\nremain conservative at least up to a chosen confidence level. This paper\ndescribes a class of robust test statistics for simply hypothesis tests, as\nwell as an algorithm to determine the necessary inflation factor model\nparameter fits. It then presents some example applications of the methods to\nreal neutrino interaction data and model comparisons."},{"date":"2024-10","title":"Enhancing Code Annotation Reliability: Generative AI's Role in Comment Quality Assessment Models","author":"Seetharam Killivalavan, and Durairaj Thenmozhi","link":"http://arxiv.org/abs/2410.22323v1","abstract":"This paper explores a novel method for enhancing binary classification models\nthat assess code comment quality, leveraging Generative Artificial Intelligence\nto elevate model performance. By integrating 1,437 newly generated code-comment\npairs, labeled as \"Useful\" or \"Not Useful\" and sourced from various GitHub\nrepositories, into an existing C-language dataset of 9,048 pairs, we\ndemonstrate substantial model improvements. Using an advanced Large Language\nModel, our approach yields a 5.78% precision increase in the Support Vector\nMachine (SVM) model, improving from 0.79 to 0.8478, and a 2.17% recall boost in\nthe Artificial Neural Network (ANN) model, rising from 0.731 to 0.7527. These\nresults underscore Generative AI's value in advancing code comment\nclassification models, offering significant potential for enhanced accuracy in\nsoftware development and quality control. This study provides a promising\noutlook on the integration of generative techniques for refining machine\nlearning models in practical software engineering settings."},{"date":"2024-10","title":"EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast Histopathology Classification: A Comprehensive Approach","author":"Naren Sengodan","link":"http://arxiv.org/abs/2410.22392v1","abstract":"Breast cancer histopathology image classification is crucial for early cancer\ndetection, offering the potential to reduce mortality rates through timely\ndiagnosis. This paper introduces a novel approach integrating Hybrid\nEfficientNet models with advanced attention mechanisms, including Convolutional\nBlock Attention Module (CBAM), Self-Attention, and Deformable Attention, to\nenhance feature extraction and focus on critical image regions. We evaluate the\nperformance of our models across multiple magnification scales using publicly\navailable histopathological datasets. Our method achieves significant\nimprovements, with accuracy reaching 98.42% at 400X magnification, surpassing\nseveral state-of-the-art models, including VGG and ResNet architectures. The\nresults are validated using metrics such as accuracy, F1-score, precision, and\nrecall, demonstrating the clinical potential of our model in improving\ndiagnostic accuracy. Furthermore, the proposed method shows increased\ncomputational efficiency, making it suitable for integration into real-time\ndiagnostic workflows."},{"date":"2024-10","title":"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks","author":"Thomas Schmied, Thomas Adler, Vihang Patil, Maximilian Beck, Korbinian P\u00f6ppel, Johannes Brandstetter, G\u00fcnter Klambauer, Razvan Pascanu, and Sepp Hochreiter","link":"http://arxiv.org/abs/2410.22391v1","abstract":"In recent years, there has been a trend in the field of Reinforcement\nLearning (RL) towards large action models trained offline on large-scale\ndatasets via sequence modeling. Existing models are primarily based on the\nTransformer architecture, which result in powerful agents. However, due to slow\ninference times, Transformer-based approaches are impractical for real-time\napplications, such as robotics. Recently, modern recurrent architectures, such\nas xLSTM and Mamba, have been proposed that exhibit parallelization benefits\nduring training similar to the Transformer architecture while offering fast\ninference. In this work, we study the aptitude of these modern recurrent\narchitectures for large action models. Consequently, we propose a Large\nRecurrent Action Model (LRAM) with an xLSTM at its core that comes with\nlinear-time inference complexity and natural sequence length extrapolation\nabilities. Experiments on 432 tasks from 6 domains show that LRAM compares\nfavorably to Transformers in terms of performance and speed."},{"date":"2024-10","title":"Natural Language Inference Improves Compositionality in Vision-Language Models","author":"Paola Cascante-Bonilla, Yu Hou, Yang Trista Cao, Hal Daum\u00e9 III, and Rachel Rudinger","link":"http://arxiv.org/abs/2410.22315v1","abstract":"Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data)."},{"date":"2024-10","title":"An Efficient Approach to Generate Safe Drivable Space by LiDAR-Camera-HDmap Fusion","author":"Minghao Ning, Ahmad Reza Alghooneh, Chen Sun, Ruihe Zhang, Pouya Panahandeh, Steven Tuer, Ehsan Hashemi, and Amir Khajepour","link":"http://arxiv.org/abs/2410.22314v1","abstract":"In this paper, we propose an accurate and robust perception module for\nAutonomous Vehicles (AVs) for drivable space extraction. Perception is crucial\nin autonomous driving, where many deep learning-based methods, while accurate\non benchmark datasets, fail to generalize effectively, especially in diverse\nand unpredictable environments. Our work introduces a robust easy-to-generalize\nperception module that leverages LiDAR, camera, and HD map data fusion to\ndeliver a safe and reliable drivable space in all weather conditions. We\npresent an adaptive ground removal and curb detection method integrated with HD\nmap data for enhanced obstacle detection reliability. Additionally, we propose\nan adaptive DBSCAN clustering algorithm optimized for precipitation noise, and\na cost-effective LiDAR-camera frustum association that is resilient to\ncalibration discrepancies. Our comprehensive drivable space representation\nincorporates all perception data, ensuring compatibility with vehicle\ndimensions and road regulations. This approach not only improves generalization\nand efficiency, but also significantly enhances safety in autonomous vehicle\noperations. Our approach is tested on a real dataset and its reliability is\nverified during the daily (including harsh snowy weather) operation of our\nautonomous shuttle, WATonoBus"},{"date":"2024-10","title":"Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving","author":"Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang","link":"http://arxiv.org/abs/2410.22313v1","abstract":"End-to-end autonomous driving demonstrates strong planning capabilities with\nlarge-scale data but still struggles in complex, rare scenarios due to limited\ncommonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene\nunderstanding and reasoning. The path forward lies in merging the strengths of\nboth approaches. Previous methods using LVLMs to predict trajectories or\ncontrol signals yield suboptimal results, as LVLMs are not well-suited for\nprecise numerical predictions. This paper presents Senna, an autonomous driving\nsystem combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E).\nSenna decouples high-level planning from low-level trajectory prediction.\nSenna-VLM generates planning decisions in natural language, while Senna-E2E\npredicts precise trajectories. Senna-VLM utilizes a multi-image encoding\napproach and multi-view prompts for efficient scene understanding. Besides, we\nintroduce planning-oriented QAs alongside a three-stage training strategy,\nwhich enhances Senna-VLM's planning performance while preserving commonsense.\nExtensive experiments on two datasets show that Senna achieves state-of-the-art\nplanning performance. Notably, with pre-training on a large-scale dataset\nDriveX and fine-tuning on nuScenes, Senna significantly reduces average\nplanning error by 27.12% and collision rate by 33.33% over model without\npre-training. We believe Senna's cross-scenario generalization and\ntransferability are essential for achieving fully autonomous driving. Code and\nmodels will be released at https://github.com/hustvl/Senna."},{"date":"2024-10","title":"Effective Guidance for Model Attention with Simple Yes-no Annotations","author":"Seongmin Lee, Ali Payani, Duen Horng, and Chau","link":"http://arxiv.org/abs/2410.22312v1","abstract":"Modern deep learning models often make predictions by focusing on irrelevant\nareas, leading to biased performance and limited generalization. Existing\nmethods aimed at rectifying model attention require explicit labels for\nirrelevant areas or complex pixel-wise ground truth attention maps. We present\nCRAYON (Correcting Reasoning with Annotations of Yes Or No), offering\neffective, scalable, and practical solutions to rectify model attention using\nsimple yes-no annotations. CRAYON empowers classical and modern model\ninterpretation techniques to identify and guide model reasoning:\nCRAYON-ATTENTION directs classic interpretations based on saliency maps to\nfocus on relevant image regions, while CRAYON-PRUNING removes irrelevant\nneurons identified by modern concept-based methods to mitigate their influence.\nThrough extensive experiments with both quantitative and human evaluation, we\nshowcase CRAYON's effectiveness, scalability, and practicality in refining\nmodel attention. CRAYON achieves state-of-the-art performance, outperforming 12\nmethods across 3 benchmark datasets, surpassing approaches that require more\ncomplex annotations."},{"date":"2024-10","title":"SVIP: Towards Verifiable Inference of Open-source Large Language Models","author":"Yifan Sun, Yuhang Li, Yue Zhang, Yuchen Jin, and Huan Zhang","link":"http://arxiv.org/abs/2410.22307v1","abstract":"Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification."},{"date":"2024-10","title":"Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention","author":"Haomeng Zhang, Chiao-An Yang, and Raymond A. Yeh","link":"http://arxiv.org/abs/2410.22306v1","abstract":"Multi-object 3D Grounding involves locating 3D boxes based on a given query\nphrase from a point cloud. It is a challenging and significant task with\nnumerous applications in visual understanding, human-computer interaction, and\nrobotics. To tackle this challenge, we introduce D-LISA, a two-stage approach\nincorporating three innovations. First, a dynamic vision module that enables a\nvariable and learnable number of box proposals. Second, a dynamic camera\npositioning that extracts features for each proposal. Third, a\nlanguage-informed spatial attention module that better reasons over the\nproposals to output the final prediction. Empirically, experiments show that\nour method outperforms the state-of-the-art methods on multi-object 3D\ngrounding by 12.8% (absolute) and is competitive in single-object 3D grounding."},{"date":"2024-10","title":"Emotion-Guided Image to Music Generation","author":"Souraja Kundu, Saket Singh, and Yuji Iwahori","link":"http://arxiv.org/abs/2410.22299v1","abstract":"Generating music from images can enhance various applications, including\nbackground music for photo slideshows, social media experiences, and video\ncreation. This paper presents an emotion-guided image-to-music generation\nframework that leverages the Valence-Arousal (VA) emotional space to produce\nmusic that aligns with the emotional tone of a given image. Unlike previous\nmodels that rely on contrastive learning for emotional consistency, the\nproposed approach directly integrates a VA loss function to enable accurate\nemotional alignment. The model employs a CNN-Transformer architecture,\nfeaturing pre-trained CNN image feature extractors and three Transformer\nencoders to capture complex, high-level emotional features from MIDI music.\nThree Transformer decoders refine these features to generate musically and\nemotionally consistent MIDI sequences. Experimental results on a newly curated\nemotionally paired image-MIDI dataset demonstrate the proposed model's superior\nperformance across metrics such as Polyphony Rate, Pitch Entropy, Groove\nConsistency, and loss convergence."},{"date":"2024-10","title":"Active Event Alignment for Monocular Distance Estimation","author":"Nan Cai, and Pia Bideau","link":"http://arxiv.org/abs/2410.22280v1","abstract":"Event cameras provide a natural and data efficient representation of visual\ninformation, motivating novel computational strategies towards extracting\nvisual information. Inspired by the biological vision system, we propose a\nbehavior driven approach for object-wise distance estimation from event camera\ndata. This behavior-driven method mimics how biological systems, like the human\neye, stabilize their view based on object distance: distant objects require\nminimal compensatory rotation to stay in focus, while nearby objects demand\ngreater adjustments to maintain alignment. This adaptive strategy leverages\nnatural stabilization behaviors to estimate relative distances effectively.\nUnlike traditional vision algorithms that estimate depth across the entire\nimage, our approach targets local depth estimation within a specific region of\ninterest. By aligning events within a small region, we estimate the angular\nvelocity required to stabilize the image motion. We demonstrate that, under\ncertain assumptions, the compensatory rotational flow is inversely proportional\nto the object's distance. The proposed approach achieves new state-of-the-art\naccuracy in distance estimation - a performance gain of 16% on EVIMO2. EVIMO2\nevent sequences comprise complex camera motion and substantial variance in\ndepth of static real world scenes."},{"date":"2024-10","title":"Leveraging Reverberation and Visual Depth Cues for Sound Event Localization and Detection with Distance Estimation","author":"Davide Berghi, and Philip J. B. Jackson","link":"http://arxiv.org/abs/2410.22271v1","abstract":"This report describes our systems submitted for the DCASE2024 Task 3\nchallenge: Audio and Audiovisual Sound Event Localization and Detection with\nSource Distance Estimation (Track B). Our main model is based on the\naudio-visual (AV) Conformer, which processes video and audio embeddings\nextracted with ResNet50 and with an audio encoder pre-trained on SELD,\nrespectively. This model outperformed the audio-visual baseline of the\ndevelopment set of the STARSS23 dataset by a wide margin, halving its DOAE and\nimproving the F1 by more than 3x. Our second system performs a temporal\nensemble from the outputs of the AV-Conformer. We then extended the model with\nfeatures for distance estimation, such as direct and reverberant signal\ncomponents extracted from the omnidirectional audio channel, and depth maps\nextracted from the video frames. While the new system improved the RDE of our\nprevious model by about 3 percentage points, it achieved a lower F1 score. This\nmay be caused by sound classes that rarely appear in the training set and that\nthe more complex system does not detect, as analysis can determine. To overcome\nthis problem, our fourth and final system consists of an ensemble strategy\ncombining the predictions of the other three. Many opportunities to refine the\nsystem and training strategy can be tested in future ablation experiments, and\nlikely achieve incremental performance gains for this audio-visual task."}]