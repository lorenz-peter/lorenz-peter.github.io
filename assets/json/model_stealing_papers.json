[{"date":"2024-10","title":"Stealing User Prompts from Mixture of Experts","author":"Itay Yona, Ilia Shumailov, Jamie Hayes, and Nicholas Carlini","link":"http://arxiv.org/abs/2410.22884v1","abstract":"Mixture-of-Experts (MoE) models improve the efficiency and scalability of\ndense language models by routing each token to a small number of experts in\neach layer. In this paper, we show how an adversary that can arrange for their\nqueries to appear in the same batch of examples as a victim's queries can\nexploit Expert-Choice-Routing to fully disclose a victim's prompt. We\nsuccessfully demonstrate the effectiveness of this attack on a two-layer\nMixtral model, exploiting the tie-handling behavior of the torch.topk CUDA\nimplementation. Our results show that we can extract the entire prompt using\n$O({VM}^2)$ queries (with vocabulary size $V$ and prompt length $M$) or 100\nqueries on average per token in the setting we consider. This is the first\nattack to exploit architectural flaws for the purpose of extracting user\nprompts, introducing a new class of LLM vulnerabilities."},{"date":"2024-10","title":"Remote Timing Attacks on Efficient Language Model Inference","author":"Nicholas Carlini, and Milad Nasr","link":"http://arxiv.org/abs/2410.17175v1","abstract":"Scaling up language models has significantly increased their capabilities.\nBut larger models are slower models, and so there is now an extensive body of\nwork (e.g., speculative sampling or parallel decoding) that improves the\n(average case) efficiency of language model generation. But these techniques\nintroduce data-dependent timing characteristics. We show it is possible to\nexploit these timing differences to mount a timing attack. By monitoring the\n(encrypted) network traffic between a victim user and a remote language model,\nwe can learn information about the content of messages by noting when responses\nare faster or slower. With complete black-box access, on open source systems we\nshow how it is possible to learn the topic of a user's conversation (e.g.,\nmedical advice vs. coding assistance) with 90%+ precision, and on production\nsystems like OpenAI's ChatGPT and Anthropic's Claude we can distinguish between\nspecific messages or infer the user's language. We further show that an active\nadversary can leverage a boosting attack to recover PII placed in messages\n(e.g., phone numbers or credit card numbers) for open source systems. We\nconclude with potential defenses and directions for future work."},{"date":"2024-10","title":"Persistent Pre-Training Poisoning of LLMs","author":"Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tram\u00e8r, and Daphne Ippolito","link":"http://arxiv.org/abs/2410.13722v1","abstract":"Large language models are pre-trained on uncurated text datasets consisting\nof trillions of tokens scraped from the Web. Prior work has shown that: (1)\nweb-scraped pre-training datasets can be practically poisoned by malicious\nactors; and (2) adversaries can compromise language models after poisoning\nfine-tuning datasets. Our work evaluates for the first time whether language\nmodels can also be compromised during pre-training, with a focus on the\npersistence of pre-training attacks after models are fine-tuned as helpful and\nharmless chatbots (i.e., after SFT and DPO). We pre-train a series of LLMs from\nscratch to measure the impact of a potential poisoning adversary under four\ndifferent attack objectives (denial-of-service, belief manipulation,\njailbreaking, and prompt stealing), and across a wide range of model sizes\n(from 600M to 7B). Our main result is that poisoning only 0.1% of a model's\npre-training dataset is sufficient for three out of four attacks to measurably\npersist through post-training. Moreover, simple attacks like denial-of-service\npersist through post-training with a poisoning rate of only 0.001%."},{"date":"2024-10","title":"Polynomial Time Cryptanalytic Extraction of Deep Neural Networks in the Hard-Label Setting","author":"Nicholas Carlini, Jorge Ch\u00e1vez-Saab, Anna Hambitzer, Francisco Rodr\u00edguez-Henr\u00edquez, and Adi Shamir","link":"http://arxiv.org/abs/2410.05750v1","abstract":"Deep neural networks (DNNs) are valuable assets, yet their public\naccessibility raises security concerns about parameter extraction by malicious\nactors. Recent work by Carlini et al. (crypto'20) and Canales-Mart\\'inez et al.\n(eurocrypt'24) has drawn parallels between this issue and block cipher key\nextraction via chosen plaintext attacks. Leveraging differential cryptanalysis,\nthey demonstrated that all the weights and biases of black-box ReLU-based DNNs\ncould be inferred using a polynomial number of queries and computational time.\nHowever, their attacks relied on the availability of the exact numeric value of\noutput logits, which allowed the calculation of their derivatives. To overcome\nthis limitation, Chen et al. (asiacrypt'24) tackled the more realistic\nhard-label scenario, where only the final classification label (e.g., \"dog\" or\n\"car\") is accessible to the attacker. They proposed an extraction method\nrequiring a polynomial number of queries but an exponential execution time. In\naddition, their approach was applicable only to a restricted set of\narchitectures, could deal only with binary classifiers, and was demonstrated\nonly on tiny neural networks with up to four neurons split among up to two\nhidden layers. This paper introduces new techniques that, for the first time,\nachieve cryptanalytic extraction of DNN parameters in the most challenging\nhard-label setting, using both a polynomial number of queries and polynomial\ntime. We validate our approach by extracting nearly one million parameters from\na DNN trained on the CIFAR-10 dataset, comprising 832 neurons in four hidden\nlayers. Our results reveal the surprising fact that all the weights of a\nReLU-based DNN can be efficiently determined by analyzing only the geometric\nshape of its decision boundaries."},{"date":"2024-08","title":"DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model","author":"Mona Sheikh Zeinoddin, Chiara Lena, Jiongqi Qu, Luca Carlini, Mattia Magro, Seunghoi Kim, Elena De Momi, Sophia Bano, Matthew Grech-Sollars, Evangelos Mazomenos, Daniel C. Alexander, Danail Stoyanov, Matthew J. Clarkson, and Mobarakol Islam","link":"http://arxiv.org/abs/2408.17433v2","abstract":"Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D\nreconstruction and visualization. While foundation models like Depth Anything\nModels (DAM) show promise, directly applying them to surgery often yields\nsuboptimal results. Fully fine-tuning on limited surgical data can cause\noverfitting and catastrophic forgetting, compromising model robustness and\ngeneralization. Although Low-Rank Adaptation (LoRA) addresses some adaptation\nissues, its uniform parameter distribution neglects the inherent feature\nhierarchy, where earlier layers, learning more general features, require more\nparameters than later ones. To tackle this issue, we introduce Depth Anything\nin Robotic Endoscopic Surgery (DARES), a novel approach that employs a new\nadaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to\nperform self-supervised monocular depth estimation in RAS scenes. To enhance\nlearning efficiency, we introduce Vector-LoRA by integrating more parameters in\nearlier layers and gradually decreasing parameters in later layers. We also\ndesign a reprojection loss based on the multi-scale SSIM error to enhance depth\nperception by better tailoring the foundation model to the specific\nrequirements of the surgical environment. The proposed method is validated on\nthe SCARED dataset and demonstrates superior performance over recent\nstate-of-the-art self-supervised monocular depth estimation techniques,\nachieving an improvement of 13.3% in the absolute relative error metric. The\ncode and pre-trained weights are available at\nhttps://github.com/mobarakol/DARES."},{"date":"2024-07","title":"ImPORTance -- Machine Learning-Driven Analysis of Global Port Significance and Network Dynamics for Improved Operational Efficiency","author":"Emanuele Carlini, Domenico Di Gangi, Vinicius Monteiro de Lira, Hanna Kavalionak, Gabriel Spadon, and Amilcar Soares","link":"http://arxiv.org/abs/2407.09571v2","abstract":"Seaports play a crucial role in the global economy, and researchers have\nsought to understand their significance through various studies. In this paper,\nwe aim to explore the common characteristics shared by important ports by\nanalyzing the network of connections formed by vessel movement among them. To\naccomplish this task, we adopt a bottom-up network construction approach that\ncombines three years' worth of AIS (Automatic Identification System) data from\naround the world, constructing a Ports Network that represents the connections\nbetween different ports. Through such representation, we use machine learning\nto measure the relative significance of different port features. Our model\nexamined such features and revealed that geographical characteristics and the\ndepth of the port are indicators of a port's significance to the Ports Network.\nAccordingly, this study employs a data-driven approach and utilizes machine\nlearning to provide a comprehensive understanding of the factors contributing\nto ports' importance. The outcomes of our work are aimed to inform\ndecision-making processes related to port development, resource allocation, and\ninfrastructure planning in the industry."},{"date":"2024-06","title":"Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI","author":"Robert H\u00f6nig, Javier Rando, Nicholas Carlini, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2406.12027v1","abstract":"Artists are increasingly concerned about advancements in image generation\nmodels that can closely replicate their unique artistic styles. In response,\nseveral protection tools against style mimicry have been developed that\nincorporate small adversarial perturbations into artworks published online. In\nthis work, we evaluate the effectiveness of popular protections -- with\nmillions of downloads -- and show they only provide a false sense of security.\nWe find that low-effort and \"off-the-shelf\" techniques, such as image\nupscaling, are sufficient to create robust mimicry methods that significantly\ndegrade existing protections. Through a user study, we demonstrate that all\nexisting protections can be easily bypassed, leaving artists vulnerable to\nstyle mimicry. We caution that tools based on adversarial perturbations cannot\nreliably protect artists from the misuse of generative AI, and urge the\ndevelopment of alternative non-technological solutions."},{"date":"2024-05","title":"Cutting through buggy adversarial example defenses: fixing 1 line of code breaks Sabre","author":"Nicholas Carlini","link":"http://arxiv.org/abs/2405.03672v3","abstract":"Sabre is a defense to adversarial examples that was accepted at IEEE S&P\n2024. We first reveal significant flaws in the evaluation that point to clear\nsigns of gradient masking. We then show the cause of this gradient masking: a\nbug in the original evaluation code. By fixing a single line of code in the\noriginal repository, we reduce Sabre's robust accuracy to 0%. In response to\nthis, the authors modify the defense and introduce a new defense component not\ndescribed in the original paper. But this fix contains a second bug; modifying\none more line of code reduces robust accuracy to below baseline levels. After\nwe released the first version of our paper online, the authors introduced\nanother change to the defense; by commenting out one line of code during attack\nwe reduce the robust accuracy to 0% again."},{"date":"2024-04","title":"Graph Neural Networks and Reinforcement Learning for Proactive Application Image Placement","author":"Antonios Makris, Theodoros Theodoropoulos, Evangelos Psomakelis, Emanuele Carlini, Matteo Mordacchini, Patrizio Dazzi, and Konstantinos Tserpes","link":"http://arxiv.org/abs/2407.00007v1","abstract":"The shift from Cloud Computing to a Cloud-Edge continuum presents new\nopportunities and challenges for data-intensive and interactive applications.\nEdge computing has garnered a lot of attention from both industry and academia\nin recent years, emerging as a key enabler for meeting the increasingly strict\ndemands of Next Generation applications. In Edge computing the computations are\nplaced closer to the end-users, to facilitate low-latency and high-bandwidth\napplications and services. However, the distributed, dynamic, and heterogeneous\nnature of Edge computing, presents a significant challenge for service\nplacement. A critical aspect of Edge computing involves managing the placement\nof applications within the network system to minimize each application's\nruntime, considering the resources available on system devices and the\ncapabilities of the system's network. The placement of application images must\nbe proactively planned to minimize image tranfer time, and meet the strict\ndemands of the applications. In this regard, this paper proposes an approach\nfor proactive image placement that combines Graph Neural Networks and\nactor-critic Reinforcement Learning, which is evaluated empirically and\ncompared against various solutions. The findings indicate that although the\nproposed approach may result in longer execution times in certain scenarios, it\nconsistently achieves superior outcomes in terms of application placement."},{"date":"2024-04","title":"Forcing Diffuse Distributions out of Language Models","author":"Yiming Zhang, Avi Schwarzschild, Nicholas Carlini, Zico Kolter, and Daphne Ippolito","link":"http://arxiv.org/abs/2404.10859v2","abstract":"Despite being trained specifically to follow user instructions, today's\ninstructiontuned language models perform poorly when instructed to produce\nrandom outputs. For example, when prompted to pick a number uniformly between\none and ten Llama-2-13B-chat disproportionately favors the number five, and\nwhen tasked with picking a first name at random, Mistral-7B-Instruct chooses\nAvery 40 times more often than we would expect based on the U.S. population.\nWhen these language models are used for real-world tasks where diversity of\noutputs is crucial, such as language model assisted dataset construction, their\ninability to produce diffuse distributions over valid choices is a major\nhurdle. In this work, we propose a fine-tuning method that encourages language\nmodels to output distributions that are diffuse over valid outcomes. The\nmethods we introduce generalize across a variety of tasks and distributions and\nmake large language models practical for synthetic dataset generation with\nlittle human intervention."},{"date":"2024-04","title":"Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models","author":"Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, and Nicholas Carlini","link":"http://arxiv.org/abs/2404.01231v1","abstract":"It is commonplace to produce application-specific models by fine-tuning large\npre-trained models using a small bespoke dataset. The widespread availability\nof foundation model checkpoints on the web poses considerable risks, including\nthe vulnerability to backdoor attacks. In this paper, we unveil a new\nvulnerability: the privacy backdoor attack. This black-box privacy attack aims\nto amplify the privacy leakage that arises when fine-tuning a model: when a\nvictim fine-tunes a backdoored model, their training data will be leaked at a\nsignificantly higher rate than if they had fine-tuned a typical model. We\nconduct extensive experiments on various datasets and models, including both\nvision-language models (CLIP) and large language models, demonstrating the\nbroad applicability and effectiveness of such an attack. Additionally, we carry\nout multiple ablation studies with different fine-tuning methods and inference\nstrategies to thoroughly analyze this new threat. Our findings highlight a\ncritical privacy concern within the machine learning community and call for a\nreevaluation of safety protocols in the use of open-source pre-trained models."},{"date":"2024-03","title":"Diffusion Denoising as a Certified Defense against Clean-label Poisoning","author":"Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin","link":"http://arxiv.org/abs/2403.11981v1","abstract":"We present a certified defense to clean-label poisoning attacks. These\nattacks work by injecting a small number of poisoning samples (e.g., 1%) that\ncontain $p$-norm bounded adversarial perturbations into the training data to\ninduce a targeted misclassification of a test-time input. Inspired by the\nadversarial robustness achieved by $denoised$ $smoothing$, we show how an\noff-the-shelf diffusion model can sanitize the tampered training data. We\nextensively test our defense against seven clean-label poisoning attacks and\nreduce their attack success to 0-16% with only a negligible drop in the test\ntime accuracy. We compare our defense with existing countermeasures against\nclean-label poisoning, showing that the defense reduces the attack success the\nmost and offers the best model utility. Our results highlight the need for\nfuture work on developing stronger clean-label attacks and using our certified\nyet practical defense as a strong baseline to evaluate these attacks."},{"date":"2024-03","title":"Stealing Part of a Production Language Model","author":"Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Itay Yona, Eric Wallace, David Rolnick, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2403.06634v2","abstract":"We introduce the first model-stealing attack that extracts precise,\nnontrivial information from black-box production language models like OpenAI's\nChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding\nprojection layer (up to symmetries) of a transformer model, given typical API\naccess. For under \\$20 USD, our attack extracts the entire projection matrix of\nOpenAI's Ada and Babbage language models. We thereby confirm, for the first\ntime, that these black-box models have a hidden dimension of 1024 and 2048,\nrespectively. We also recover the exact hidden dimension size of the\ngpt-3.5-turbo model, and estimate it would cost under $2,000 in queries to\nrecover the entire projection matrix. We conclude with potential defenses and\nmitigations, and discuss the implications of possible future work that could\nextend our attack."},{"date":"2024-02","title":"Query-Based Adversarial Prompt Generation","author":"Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tram\u00e8r, and Milad Nasr","link":"http://arxiv.org/abs/2402.12329v1","abstract":"Recent work has shown it is possible to construct adversarial examples that\ncause an aligned language model to emit harmful strings or perform harmful\nbehavior. Existing attacks work either in the white-box setting (with full\naccess to the model weights), or through transferability: the phenomenon that\nadversarial examples crafted on one model often remain effective on other\nmodels. We improve on prior work with a query-based attack that leverages API\naccess to a remote language model to construct adversarial examples that cause\nthe model to emit harmful strings with (much) higher probability than with\ntransfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety\nclassifier; we can cause GPT-3.5 to emit harmful strings that current transfer\nattacks fail at, and we can evade the safety classifier with nearly 100%\nprobability."},{"date":"2023-12","title":"Hunting imaging biomarkers in pulmonary fibrosis: Benchmarks of the AIIB23 challenge","author":"Yang Nan, Xiaodan Xing, Shiyi Wang, Zeyu Tang, Federico N Felder, Sheng Zhang, Roberta Eufrasia Ledda, Xiaoliu Ding, Ruiqi Yu, Weiping Liu, Feng Shi, Tianyang Sun, Zehong Cao, Minghui Zhang, Yun Gu, Hanxiao Zhang, Jian Gao, Pingyu Wang, Wen Tang, Pengxin Yu, Han Kang, Junqiang Chen, Xing Lu, Boyu Zhang, Michail Mamalakis, Francesco Prinzi, Gianluca Carlini, Lisa Cuneo, Abhirup Banerjee, Zhaohu Xing, Lei Zhu, Zacharia Mesbah, Dhruv Jain, Tsiry Mayet, Hongyu Yuan, Qing Lyu, Abdul Qayyum, Moona Mazher, Athol Wells, Simon LF Walsh, and Guang Yang","link":"http://arxiv.org/abs/2312.13752v2","abstract":"Airway-related quantitative imaging biomarkers are crucial for examination,\ndiagnosis, and prognosis in pulmonary diseases. However, the manual delineation\nof airway trees remains prohibitively time-consuming. While significant efforts\nhave been made towards enhancing airway modelling, current public-available\ndatasets concentrate on lung diseases with moderate morphological variations.\nThe intricate honeycombing patterns present in the lung tissues of fibrotic\nlung disease patients exacerbate the challenges, often leading to various\nprediction errors. To address this issue, the 'Airway-Informed Quantitative CT\nImaging Biomarker for Fibrotic Lung Disease 2023' (AIIB23) competition was\norganized in conjunction with the official 2023 International Conference on\nMedical Image Computing and Computer Assisted Intervention (MICCAI). The airway\nstructures were meticulously annotated by three experienced radiologists.\nCompetitors were encouraged to develop automatic airway segmentation models\nwith high robustness and generalization abilities, followed by exploring the\nmost correlated QIB of mortality prediction. A training set of 120\nhigh-resolution computerised tomography (HRCT) scans were publicly released\nwith expert annotations and mortality status. The online validation set\nincorporated 52 HRCT scans from patients with fibrotic lung disease and the\noffline test set included 140 cases from fibrosis and COVID-19 patients. The\nresults have shown that the capacity of extracting airway trees from patients\nwith fibrotic lung disease could be enhanced by introducing voxel-wise weighted\ngeneral union loss and continuity loss. In addition to the competitive image\nbiomarkers for prognosis, a strong airway-derived biomarker (Hazard ratio>1.5,\np<0.0001) was revealed for survival prognostication compared with existing\nclinical measurements, clinician assessment and AI-based biomarkers."},{"date":"2023-12","title":"Initialization Matters for Adversarial Transfer Learning","author":"Andong Hua, Jindong Gu, Zhiyu Xue, Nicholas Carlini, Eric Wong, and Yao Qin","link":"http://arxiv.org/abs/2312.05716v2","abstract":"With the prevalence of the Pretraining-Finetuning paradigm in transfer\nlearning, the robustness of downstream tasks has become a critical concern. In\nthis work, we delve into adversarial robustness in transfer learning and reveal\nthe critical role of initialization, including both the pretrained model and\nthe linear head. First, we discover the necessity of an adversarially robust\npretrained model. Specifically, we reveal that with a standard pretrained\nmodel, Parameter-Efficient Finetuning (PEFT) methods either fail to be\nadversarially robust or continue to exhibit significantly degraded adversarial\nrobustness on downstream tasks, even with adversarial training during\nfinetuning. Leveraging a robust pretrained model, surprisingly, we observe that\na simple linear probing can outperform full finetuning and other PEFT methods\nwith random initialization on certain datasets. We further identify that linear\nprobing excels in preserving robustness from the robust pretraining. Based on\nthis, we propose Robust Linear Initialization (RoLI) for adversarial\nfinetuning, which initializes the linear head with the weights obtained by\nadversarial linear probing to maximally inherit the robustness from\npretraining. Across five different image classification datasets, we\ndemonstrate the effectiveness of RoLI and achieve new state-of-the-art results.\nOur code is available at \\url{https://github.com/DongXzz/RoLI}."},{"date":"2023-11","title":"Scalable Extraction of Training Data from (Production) Language Models","author":"Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee","link":"http://arxiv.org/abs/2311.17035v1","abstract":"This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization."},{"date":"2023-09","title":"Privacy Side Channels in Machine Learning Systems","author":"Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2309.05610v2","abstract":"Most current approaches for protecting privacy in machine learning (ML)\nassume that models exist in a vacuum. Yet, in reality, these models are part of\nlarger systems that include components for training data filtering, output\nmonitoring, and more. In this work, we introduce privacy side channels: attacks\nthat exploit these system-level components to extract private information at\nfar higher rates than is otherwise possible for standalone models. We propose\nfour categories of side channels that span the entire ML lifecycle (training\ndata filtering, input preprocessing, output post-processing, and query\nfiltering) and allow for enhanced membership inference, data extraction, and\neven novel threats such as extraction of users' test queries. For example, we\nshow that deduplicating training data before applying differentially-private\ntraining creates a side-channel that completely invalidates any provable\nprivacy guarantees. We further show that systems which block language models\nfrom regenerating training data can be exploited to exfiltrate private keys\ncontained in the training set--even if the model did not memorize these keys.\nTaken together, our results demonstrate the need for a holistic, end-to-end\nprivacy analysis of machine learning systems."},{"date":"2023-09","title":"Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System","author":"Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, and Yun William Yu","link":"http://arxiv.org/abs/2309.04858v1","abstract":"Neural language models are increasingly deployed into APIs and websites that\nallow a user to pass in a prompt and receive generated text. Many of these\nsystems do not reveal generation parameters. In this paper, we present methods\nto reverse-engineer the decoding method used to generate text (i.e., top-$k$ or\nnucleus sampling). Our ability to discover which decoding strategy was used has\nimplications for detecting generated text. Additionally, the process of\ndiscovering the decoding strategy can reveal biases caused by selecting\ndecoding settings which severely truncate a model's predicted distributions. We\nperform our attack on several families of open-source language models, as well\nas on production systems (e.g., ChatGPT)."},{"date":"2023-08","title":"Identifying and Mitigating the Security Risks of Generative AI","author":"Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, and Diyi Yang","link":"http://arxiv.org/abs/2308.14840v4","abstract":"Every major technical invention resurfaces the dual-use dilemma -- the new\ntechnology has the potential to be used for good as well as for harm.\nGenerative AI (GenAI) techniques, such as large language models (LLMs) and\ndiffusion models, have shown remarkable capabilities (e.g., in-context\nlearning, code-completion, and text-to-image generation and editing). However,\nGenAI can be used just as well by attackers to generate new attacks and\nincrease the velocity and efficacy of existing attacks.\n  This paper reports the findings of a workshop held at Google (co-organized by\nStanford University and the University of Wisconsin-Madison) on the dual-use\ndilemma posed by GenAI. This paper is not meant to be comprehensive, but is\nrather an attempt to synthesize some of the interesting findings from the\nworkshop. We discuss short-term and long-term goals for the community on this\ntopic. We hope this paper provides both a launching point for a discussion on\nthis important topic as well as interesting problems that the research\ncommunity can work to address."},{"date":"2023-08","title":"Revisiting N-CNN for Clinical Practice","author":"Leonardo Antunes Ferreira, Lucas Pereira Carlini, Gabriel de Almeida S\u00e1 Coutrin, Tatiany Marcondes Heideirich, Marina Carvalho de Moraes Barros, Ruth Guinsburg, and Carlos Eduardo Thomaz","link":"http://arxiv.org/abs/2308.05877v1","abstract":"This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by\noptimizing its hyperparameters and evaluating how they affect its\nclassification metrics, explainability and reliability, discussing their\npotential impact in clinical practice. We have chosen hyperparameters that do\nnot modify the original N-CNN architecture, but mainly modify its learning rate\nand training regularization. The optimization was done by evaluating the\nimprovement in F1 Score for each hyperparameter individually, and the best\nhyperparameters were chosen to create a Tuned N-CNN. We also applied soft\nlabels derived from the Neonatal Facial Coding System, proposing a novel\napproach for training facial expression classification models for neonatal pain\nassessment. Interestingly, while the Tuned N-CNN results point towards\nimprovements in classification metrics and explainability, these improvements\ndid not directly translate to calibration performance. We believe that such\ninsights might have the potential to contribute to the development of more\nreliable pain evaluation tools for newborns, aiding healthcare professionals in\ndelivering appropriate interventions and improving patient outcomes."},{"date":"2023-07","title":"Universal and Transferable Adversarial Attacks on Aligned Language Models","author":"Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson","link":"http://arxiv.org/abs/2307.15043v2","abstract":"Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks."},{"date":"2023-07","title":"Backdoor Attacks for In-Context Learning with Language Models","author":"Nikhil Kandpal, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas Carlini","link":"http://arxiv.org/abs/2307.14692v1","abstract":"Because state-of-the-art language models are expensive to train, most\npractitioners must make use of one of the few publicly available language\nmodels or language model APIs. This consolidation of trust increases the\npotency of backdoor attacks, where an adversary tampers with a machine learning\nmodel in order to make it perform some malicious behavior on inputs that\ncontain a predefined backdoor trigger. We show that the in-context learning\nability of large language models significantly complicates the question of\ndeveloping backdoor attacks, as a successful backdoor must work against various\nprompting strategies and should not affect the model's general purpose\ncapabilities. We design a new attack for eliciting targeted misclassification\nwhen language models are prompted to perform a particular target task and\ndemonstrate the feasibility of this attack by backdooring multiple large\nlanguage models ranging in size from 1.3 billion to 6 billion parameters.\nFinally we study defenses to mitigate the potential harms of our attack: for\nexample, while in the white-box setting we show that fine-tuning models for as\nfew as 500 steps suffices to remove the backdoor behavior, in the black-box\nsetting we are unable to develop a successful defense that relies on prompt\nengineering alone."},{"date":"2023-07","title":"A LLM Assisted Exploitation of AI-Guardian","author":"Nicholas Carlini","link":"http://arxiv.org/abs/2307.15008v1","abstract":"Large language models (LLMs) are now highly capable at a diverse range of\ntasks. This paper studies whether or not GPT-4, one such LLM, is capable of\nassisting researchers in the field of adversarial machine learning. As a case\nstudy, we evaluate the robustness of AI-Guardian, a recent defense to\nadversarial examples published at IEEE S&P 2023, a top computer security\nconference. We completely break this defense: the proposed scheme does not\nincrease robustness compared to an undefended baseline.\n  We write none of the code to attack this model, and instead prompt GPT-4 to\nimplement all attack algorithms following our instructions and guidance. This\nprocess was surprisingly effective and efficient, with the language model at\ntimes producing code from ambiguous instructions faster than the author of this\npaper could have done. We conclude by discussing (1) the warning signs present\nin the evaluation that suggested to us AI-Guardian would be broken, and (2) our\nexperience with designing attacks and performing novel research using the most\nrecent advances in language modeling."},{"date":"2023-07","title":"Effective Prompt Extraction from Language Models","author":"Yiming Zhang, Nicholas Carlini, and Daphne Ippolito","link":"http://arxiv.org/abs/2307.06865v3","abstract":"The text generated by large language models is commonly controlled by\nprompting, where a prompt prepended to a user's query guides the model's\noutput. The prompts used by companies to guide their models are often treated\nas secrets, to be hidden from the user making the query. They have even been\ntreated as commodities to be bought and sold on marketplaces. However,\nanecdotal reports have shown adversarial users employing prompt extraction\nattacks to recover these prompts. In this paper, we present a framework for\nsystematically measuring the effectiveness of these attacks. In experiments\nwith 3 different sources of prompts and 11 underlying large language models, we\nfind that simple text-based attacks can in fact reveal prompts with high\nprobability. Our framework determines with high precision whether an extracted\nprompt is the actual secret prompt, rather than a model hallucination. Prompt\nextraction from real systems such as Claude 3 and ChatGPT further suggest that\nsystem prompts can be revealed by an adversary despite existing defenses in\nplace."},{"date":"2023-06","title":"Are aligned neural networks adversarially aligned?","author":"Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, and Ludwig Schmidt","link":"http://arxiv.org/abs/2306.15447v2","abstract":"Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study adversarial alignment, and ask to what\nextent these models remain aligned when interacting with an adversarial user\nwho constructs worst-case inputs (adversarial examples). These inputs are\ndesigned to cause the model to emit harmful content that would otherwise be\nprohibited. We show that existing NLP-based optimization attacks are\ninsufficiently powerful to reliably attack aligned text models: even when\ncurrent NLP-based attacks fail, we can find adversarial inputs with brute\nforce. As a result, the failure of current attacks should not be seen as proof\nthat aligned text models remain aligned under adversarial inputs.\n  However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models."},{"date":"2023-06","title":"Evading Black-box Classifiers Without Breaking Eggs","author":"Edoardo Debenedetti, Nicholas Carlini, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2306.02895v2","abstract":"Decision-based evasion attacks repeatedly query a black-box classifier to\ngenerate adversarial examples. Prior work measures the cost of such attacks by\nthe total number of queries made to the classifier. We argue this metric is\nflawed. Most security-critical machine learning systems aim to weed out \"bad\"\ndata (e.g., malware, harmful content, etc). Queries to such systems carry a\nfundamentally asymmetric cost: queries detected as \"bad\" come at a higher cost\nbecause they trigger additional security filters, e.g., usage throttling or\naccount suspension. Yet, we find that existing decision-based attacks issue a\nlarge number of \"bad\" queries, which likely renders them ineffective against\nsecurity-critical systems. We then design new attacks that reduce the number of\nbad queries by $1.5$-$7.3\\times$, but often at a significant increase in total\n(non-bad) queries. We thus pose it as an open problem to build black-box\nattacks that are more effective under realistic cost metrics."},{"date":"2023-03","title":"Students Parrot Their Teachers: Membership Inference on Model Distillation","author":"Matthew Jagielski, Milad Nasr, Christopher Choquette-Choo, Katherine Lee, and Nicholas Carlini","link":"http://arxiv.org/abs/2303.03446v1","abstract":"Model distillation is frequently proposed as a technique to reduce the\nprivacy leakage of machine learning. These empirical privacy defenses rely on\nthe intuition that distilled ``student'' models protect the privacy of training\ndata, as they only interact with this data indirectly through a ``teacher''\nmodel. In this work, we design membership inference attacks to systematically\nstudy the privacy provided by knowledge distillation to both the teacher and\nstudent training sets. Our new attacks show that distillation alone provides\nonly limited privacy across a number of domains. We explain the success of our\nattacks on distillation by showing that membership inference attacks on a\nprivate dataset can succeed even if the target model is *never* queried on any\nactual training points, but only on inputs whose predictions are highly\ninfluenced by training data. Finally, we show that our attacks are strongest\nwhen student and teacher sets are similar, or when the attacker can poison the\nteacher set."},{"date":"2023-02","title":"Randomness in ML Defenses Helps Persistent Attackers and Hinders Evaluators","author":"Keane Lucas, Matthew Jagielski, Florian Tram\u00e8r, Lujo Bauer, and Nicholas Carlini","link":"http://arxiv.org/abs/2302.13464v1","abstract":"It is becoming increasingly imperative to design robust ML defenses. However,\nrecent work has found that many defenses that initially resist state-of-the-art\nattacks can be broken by an adaptive adversary. In this work we take steps to\nsimplify the design of defenses and argue that white-box defenses should eschew\nrandomness when possible. We begin by illustrating a new issue with the\ndeployment of randomized defenses that reduces their security compared to their\ndeterministic counterparts. We then provide evidence that making defenses\ndeterministic simplifies robustness evaluation, without reducing the\neffectiveness of a truly robust defense. Finally, we introduce a new defense\nevaluation framework that leverages a defense's deterministic nature to better\nevaluate its adversarial robustness."},{"date":"2023-02","title":"Poisoning Web-Scale Training Datasets is Practical","author":"Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2302.10149v2","abstract":"Deep learning models are often trained on distributed, web-scale datasets\ncrawled from the internet. In this paper, we introduce two new dataset\npoisoning attacks that intentionally introduce malicious examples to a model's\nperformance. Our attacks are immediately practical and could, today, poison 10\npopular datasets. Our first attack, split-view poisoning, exploits the mutable\nnature of internet content to ensure a dataset annotator's initial view of the\ndataset differs from the view downloaded by subsequent clients. By exploiting\nspecific invalid trust assumptions, we show how we could have poisoned 0.01% of\nthe LAION-400M or COYO-700M datasets for just $60 USD. Our second attack,\nfrontrunning poisoning, targets web-scale datasets that periodically snapshot\ncrowd-sourced content -- such as Wikipedia -- where an attacker only needs a\ntime-limited window to inject malicious examples. In light of both attacks, we\nnotify the maintainers of each affected dataset and recommended several\nlow-overhead defenses."},{"date":"2023-02","title":"Tight Auditing of Differentially Private Machine Learning","author":"Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski, Nicholas Carlini, and Andreas Terzis","link":"http://arxiv.org/abs/2302.07956v1","abstract":"Auditing mechanisms for differential privacy use probabilistic means to\nempirically estimate the privacy level of an algorithm. For private machine\nlearning, existing auditing mechanisms are tight: the empirical privacy\nestimate (nearly) matches the algorithm's provable privacy guarantee. But these\nauditing techniques suffer from two limitations. First, they only give tight\nestimates under implausible worst-case assumptions (e.g., a fully adversarial\ndataset). Second, they require thousands or millions of training runs to\nproduce non-trivial statistical estimates of the privacy leakage.\n  This work addresses both issues. We design an improved auditing scheme that\nyields tight privacy estimates for natural (not adversarially crafted) datasets\n-- if the adversary can see all model updates during training. Prior auditing\nworks rely on the same assumption, which is permitted under the standard\ndifferential privacy threat model. This threat model is also applicable, e.g.,\nin federated learning settings. Moreover, our auditing scheme requires only two\ntraining runs (instead of thousands) to produce tight privacy estimates, by\nadapting recent advances in tight composition theorems for differential\nprivacy. We demonstrate the utility of our improved auditing schemes by\nsurfacing implementation bugs in private machine learning code that eluded\nprior auditing techniques."},{"date":"2023-02","title":"Search for low mass dark matter in DarkSide-50: the bayesian network approach","author":"The DarkSide-50 Collaboration, :, P. Agnes, I. F. M. Albuquerque, T. Alexander, A. K. Alton, M. Ave, H. O. Back, G. Batignani, K. Biery, V. Bocci, W. M. Bonivento, B. Bottino, S. Bussino, M. Cadeddu, M. Cadoni, F. Calaprice, A. Caminata, M. D. Campos, N. Canci, M. Caravati, N. Cargioli, M. Cariello, M. Carlini, V. Cataudella, P. Cavalcante, S. Cavuoti, S. Chashin, A. Chepurnov, C. Cical\u00f2, G. Covone, D. D'Angelo, S. Davini, A. De Candia, S. De Cecco, G. De Filippis, G. De Rosa, A. V. Derbin, A. Devoto, M. D'Incecco, C. Dionisi, F. Dordei, M. Downing, D. D'Urso, M. Fairbairn, G. Fiorillo, D. Franco, F. Gabriele, C. Galbiati, C. Ghiano, C. Giganti, G. K. Giovanetti, A. M. Goretti, G. Grilli di Cortona, A. Grobov, M. Gromov, M. Guan, M. Gulino, B. R. Hackett, K. Herner, T. Hessel, B. Hosseini, F. Hubaut, E. V. Hungerford, An. Ianni, V. Ippolito, K. Keeter, C. L. Kendziora, M. Kimura, I. Kochanek, D. Korablev, G. Korga, A. Kubankin, M. Kuss, M. La Commara, M. Lai, X. Li, M. Lissia, G. Longo, O. Lychagina, I. N. Machulin, L. P. Mapelli, S. M. Mari, J. Maricic, A. Messina, R. Milincic, J. Monroe, M. Morrocchi, X. Mougeot, V. N. Muratova, P. Musico, A. O. Nozdrina, A. Oleinik, F. Ortica, L. Pagani, M. Pallavicini, L. Pandola, E. Pantic, E. Paoloni, K. Pelczar, N. Pelliccia, S. Piacentini, A. Pocar, D. M. Poehlmann, S. Pordes, S. S. Poudel, P. Pralavorio, D. D. Price, F. Ragusa, M. Razeti, A. Razeto, A. L. Renshaw, M. Rescigno, J. Rode, A. Romani, D. Sablone, O. Samoylov, E. Sandford, W. Sands, S. Sanfilippo, C. Savarese, B. Schlitzer, D. A. Semenov, A. Shchagin, A. Sheshukov, M. D. Skorokhvatov, O. Smirnov, A. Sotnikov, S. Stracka, Y. Suvorov, R. Tartaglia, G. Testera, A. Tonazzo, E. V. Unzhakov, A. Vishneva, R. B. Vogelaar, M. Wada, H. Wang, Y. Wang, S. Westerdale, M. M. Wojcik, X. Xiao, C. Yang, and G. Zuzel","link":"http://arxiv.org/abs/2302.01830v2","abstract":"We present a novel approach for the search of dark matter in the DarkSide-50\nexperiment, relying on Bayesian Networks. This method incorporates the detector\nresponse model into the likelihood function, explicitly maintaining the\nconnection with the quantity of interest. No assumptions about the linearity of\nthe problem or the shape of the probability distribution functions are\nrequired, and there is no need to morph signal and background spectra as a\nfunction of nuisance parameters. By expressing the problem in terms of Bayesian\nNetworks, we have developed an inference algorithm based on a Markov Chain\nMonte Carlo to calculate the posterior probability. A clever description of the\ndetector response model in terms of parametric matrices allows us to study the\nimpact of systematic variations of any parameter on the final results. Our\napproach not only provides the desired information on the parameter of\ninterest, but also potential constraints on the response model. Our results are\nconsistent with recent published analyses and further refine the parameters of\nthe detector response model."},{"date":"2023-02","title":"Effective Robustness against Natural Distribution Shifts for Models with Different Training Data","author":"Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, and Yao Qin","link":"http://arxiv.org/abs/2302.01381v2","abstract":"\"Effective robustness\" measures the extra out-of-distribution (OOD)\nrobustness beyond what can be predicted from the in-distribution (ID)\nperformance. Existing effective robustness evaluations typically use a single\ntest set such as ImageNet to evaluate the ID accuracy. This becomes problematic\nwhen evaluating models trained on different data distributions, e.g., comparing\nmodels trained on ImageNet vs. zero-shot language-image pre-trained models\ntrained on LAION. In this paper, we propose a new evaluation metric to evaluate\nand compare the effective robustness of models trained on different data. To do\nthis, we control for the accuracy on multiple ID test sets that cover the\ntraining distributions for all the evaluated models. Our new evaluation metric\nprovides a better estimate of effective robustness when there are models with\ndifferent training data. It may also explain the surprising effective\nrobustness gains of zero-shot CLIP-like models exhibited in prior works that\nused ImageNet as the only ID test set, while the gains diminish under our new\nevaluation. Additional artifacts including interactive visualizations are\nprovided at https://shizhouxing.github.io/effective-robustness."},{"date":"2023-01","title":"Extracting Training Data from Diffusion Models","author":"Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\u00e8r, Borja Balle, Daphne Ippolito, and Eric Wallace","link":"http://arxiv.org/abs/2301.13188v1","abstract":"Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have\nattracted significant attention due to their ability to generate high-quality\nsynthetic images. In this work, we show that diffusion models memorize\nindividual images from their training data and emit them at generation time.\nWith a generate-and-filter pipeline, we extract over a thousand training\nexamples from state-of-the-art models, ranging from photographs of individual\npeople to trademarked company logos. We also train hundreds of diffusion models\nin various settings to analyze how different modeling and data decisions affect\nprivacy. Overall, our results show that diffusion models are much less private\nthan prior generative models such as GANs, and that mitigating these\nvulnerabilities may require new advances in privacy-preserving training."},{"date":"2022-12","title":"Publishing Efficient On-device Models Increases Adversarial Vulnerability","author":"Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin","link":"http://arxiv.org/abs/2212.13700v1","abstract":"Recent increases in the computational demands of deep neural networks (DNNs)\nhave sparked interest in efficient deep learning mechanisms, e.g., quantization\nor pruning. These mechanisms enable the construction of a small, efficient\nversion of commercial-scale models with comparable accuracy, accelerating their\ndeployment to resource-constrained devices.\n  In this paper, we study the security considerations of publishing on-device\nvariants of large-scale models. We first show that an adversary can exploit\non-device models to make attacking the large models easier. In evaluations\nacross 19 DNNs, by exploiting the published on-device models as a transfer\nprior, the adversarial vulnerability of the original commercial-scale models\nincreases by up to 100x. We then show that the vulnerability increases as the\nsimilarity between a full-scale and its efficient model increase. Based on the\ninsights, we propose a defense, $similarity$-$unpairing$, that fine-tunes\non-device models with the objective of reducing the similarity. We evaluated\nour defense on all the 19 DNNs and found that it reduces the transferability up\nto 90% and the number of queries required by a factor of 10-100x. Our results\nsuggest that further research is needed on the security (or even privacy)\nthreats caused by publishing those efficient siblings."},{"date":"2022-12","title":"Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining","author":"Florian Tram\u00e8r, Gautam Kamath, and Nicholas Carlini","link":"http://arxiv.org/abs/2212.06470v3","abstract":"The performance of differentially private machine learning can be boosted\nsignificantly by leveraging the transfer learning capabilities of non-private\nmodels pretrained on large public datasets. We critically review this approach.\n  We primarily question whether the use of large Web-scraped datasets should be\nviewed as differential-privacy-preserving. We caution that publicizing these\nmodels pretrained on Web data as \"private\" could lead to harm and erode the\npublic's trust in differential privacy as a meaningful definition of privacy.\n  Beyond the privacy considerations of using public data, we further question\nthe utility of this paradigm. We scrutinize whether existing machine learning\nbenchmarks are appropriate for measuring the ability of pretrained models to\ngeneralize to sensitive domains, which may be poorly represented in public Web\ndata. Finally, we notice that pretraining has been especially impactful for the\nlargest available models -- models sufficiently large to prohibit end users\nrunning them on their own devices. Thus, deploying such models today could be a\nnet loss for privacy, as it would require (private) data to be outsourced to a\nmore compute-powerful third party.\n  We conclude by discussing potential paths forward for the field of private\nlearning, as public pretraining becomes more popular and powerful."},{"date":"2022-10","title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy","author":"Daphne Ippolito, Florian Tram\u00e8r, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, and Nicholas Carlini","link":"http://arxiv.org/abs/2210.17546v3","abstract":"Studying data memorization in neural language models helps us understand the\nrisks (e.g., to privacy or copyright) associated with models regurgitating\ntraining data and aids in the development of countermeasures. Many prior works\n-- and some recently deployed defenses -- focus on \"verbatim memorization\",\ndefined as a model generation that exactly matches a substring from the\ntraining set. We argue that verbatim memorization definitions are too\nrestrictive and fail to capture more subtle forms of memorization.\nSpecifically, we design and implement an efficient defense that perfectly\nprevents all verbatim memorization. And yet, we demonstrate that this \"perfect\"\nfilter does not prevent the leakage of training data. Indeed, it is easily\ncircumvented by plausible and minimally modified \"style-transfer\" prompts --\nand in some cases even the non-modified original prompts -- to extract\nmemorized information. We conclude by discussing potential alternative\ndefinitions and why defining memorization is a difficult yet crucial open\nquestion for neural language models."},{"date":"2022-10","title":"Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems","author":"Chawin Sitawarin, Florian Tram\u00e8r, and Nicholas Carlini","link":"http://arxiv.org/abs/2210.03297v2","abstract":"Decision-based attacks construct adversarial examples against a machine\nlearning (ML) model by making only hard-label queries. These attacks have\nmainly been applied directly to standalone neural networks. However, in\npractice, ML models are just one component of a larger learning system. We find\nthat by adding a single preprocessor in front of a classifier, state-of-the-art\nquery-based attacks are up to 7$\\times$ less effective at attacking a\nprediction pipeline than at attacking the model alone. We explain this\ndiscrepancy by the fact that most preprocessors introduce some notion of\ninvariance to the input space. Hence, attacks that are unaware of this\ninvariance inevitably waste a large number of queries to re-discover or\novercome it. We, therefore, develop techniques to (i) reverse-engineer the\npreprocessor and then (ii) use this extracted information to attack the\nend-to-end system. Our preprocessors extraction method requires only a few\nhundred queries, and our preprocessor-aware attacks recover the same efficacy\nas when attacking the model alone. The code can be found at\nhttps://github.com/google-research/preprocessor-aware-black-box-attack."},{"date":"2022-09","title":"No Free Lunch in \"Privacy for Free: How does Dataset Condensation Help Privacy\"","author":"Nicholas Carlini, Vitaly Feldman, and Milad Nasr","link":"http://arxiv.org/abs/2209.14987v1","abstract":"New methods designed to preserve data privacy require careful scrutiny.\nFailure to preserve privacy is hard to detect, and yet can lead to catastrophic\nresults when a system implementing a ``privacy-preserving'' method is attacked.\nA recent work selected for an Outstanding Paper Award at ICML 2022 (Dong et\nal., 2022) claims that dataset condensation (DC) significantly improves data\nprivacy when training machine learning models. This claim is supported by\ntheoretical analysis of a specific dataset condensation technique and an\nempirical evaluation of resistance to some existing membership inference\nattacks.\n  In this note we examine the claims in the work of Dong et al. (2022) and\ndescribe major flaws in the empirical evaluation of the method and its\ntheoretical analysis. These flaws imply that their work does not provide\nstatistically significant evidence that DC improves the privacy of training ML\nmodels over a naive baseline. Moreover, previously published results show that\nDP-SGD, the standard approach to privacy preserving ML, simultaneously gives\nbetter accuracy and achieves a (provably) lower membership attack success rate."},{"date":"2022-09","title":"Part-Based Models Improve Adversarial Robustness","author":"Chawin Sitawarin, Kornrapat Pongmala, Yizheng Chen, Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/2209.09117v2","abstract":"We show that combining human prior knowledge with end-to-end learning can\nimprove the robustness of deep neural networks by introducing a part-based\nmodel for object classification. We believe that the richer form of annotation\nhelps guide neural networks to learn more robust features without requiring\nmore samples or larger models. Our model combines a part segmentation model\nwith a tiny classifier and is trained end-to-end to simultaneously segment\nobjects into parts and then classify the segmented object. Empirically, our\npart-based models achieve both higher accuracy and higher adversarial\nrobustness than a ResNet-50 baseline on all three datasets. For instance, the\nclean accuracy of our part models is up to 15 percentage points higher than the\nbaseline's, given the same level of robustness. Our experiments indicate that\nthese models also reduce texture bias and yield better robustness against\ncommon corruptions and spurious correlations. The code is publicly available at\nhttps://github.com/chawins/adv-part-model."},{"date":"2022-09","title":"Sensitivity projections for a dual-phase argon TPC optimized for light dark matter searches through the ionization channel","author":"P. Agnes, I. Ahmad, S. Albergo, I. F. M. Albuquerque, T. Alexander, A. K. Alton, P. Amaudruz, M. Atzori Corona, D. J. Auty, M. Ave, I. Ch. Avetisov, R. I. Avetisov, O. Azzolini, H. O. Back, Z. Balmforth, V. Barbarian, A. Barrado Olmedo, P. Barrillon, A. Basco, G. Batignani, E. Berzin, A. Bondar, W. M. Bonivento, E. Borisova, B. Bottino, M. G. Boulay, G. Buccino, S. Bussino, J. Busto, A. Buzulutskov, M. Cadeddu, M. Cadoni, A. Caminata, N. Canci, A. Capra, S. Caprioli, M. Caravati, M. C\u00e1rdenas-Montes, N. Cargioli, M. Carlini, P. Castello, V. Cataudella, P. Cavalcante, S. Cavuoti, S. Cebrian, J. M. Cela Ruiz, S. Chashin, A. Chepurnov, E Chyhyrynets, C. Cical\u00f2, L. Cifarelli, D. Cintas, V. Cocco, E. Conde Vilda, L. Consiglio, S. Copello, G. Covone, S. Cross, P. Czudak, M. D'Aniello, S. D'Auria, M. D. Da Rocha Rolo, O. Dadoun, M. Daniel, S. Davini, A. De Candia, S. De Cecco, A. De Falco, G. De Filippis, D. De Gruttola, S. De Pasquale, G. De Rosa, G. Dellacasa, A. V. Derbin, A. Devoto, F. Di Capua, L. Di Noto, P. Di Stefano, C. Dionisi, G. Dolganov, F. Dordei, L. Doria, T. Erjavec, M. Fernandez Diaz, G. Fiorillo, A. Franceschi, P. Franchini, D. Franco, E. Frolov, N. Funicello, F. Gabriele, D. Gahan, C. Galbiati, G. Gallina, G. Gallus, M. Garbini, P. Garcia Abia, A. Gendotti, C. Ghiano, R. A. Giampaolo, C. Giganti, M. A. Giorgi, G. K. Giovanetti, V. Goicoechea Casanueva, A. Gola, D. Gorman, R. Graciani Diaz, G. Grauso, G. Grilli di Cortona, A. Grobov, M. Gromov, M. Guan, M. Guerzoni, M. Gulino, C. Guo, B. R. Hackett, J. B. Hall, A. L. Hallin, A. Hamer, H. Helton, M. Haranczyk, T. Hessel, S. Hill, S. Horikawa, F. Hubaut, T. Hugues, E. V. Hungerford, An. Ianni, V. Ippolito, C. Jillings, P. Kachru, A. A. Kemp, C. L. Kendziora, G. Keppel, A. V. Khomyakov, M. Kimura, I. Kochanek, K. Kondo, G. Korga, S. Koulosousas, A. Kubankin, M. Kuss, M. Ku\u017aniak, M. La Commara, M. Lai, E. Le Guirriec, E. Leason, X. Li, L. Lidey, J. Lipp, M. Lissia, G. Longo, L. Luzzi, O. Macfadyen, I. N. Machulin, I. Manthos, L. Mapelli, A. Margotti, S. M. Mari, C. Mariani, J. Maricic, A. Marini, M. Mart\u00ednez, C. J. Martoff, A. Masoni, K. Mavrokoridis, A. Mazzi, A. B. McDonald, A. Messina, R. Milincic, A. Moggi, A. Moharana, J. Monroe, M. Morrocchi, E. N. Mozhevitina, T. Mr\u00f3z, V. N. Muratova, C. Muscas, P. Musico, R. Nania, T. Napolitano, M. Nessi, G. Nieradka, K. Nikolopoulos, I. Nikulin, J. Nowak, K. Olchansky, A. Oleinik, V. Oleynikov, P. Organtini, A. Ortiz de Sol\u00f3rzano, L. Pagani, M. Pallavicini, L. Pandola, E. Pantic, E. Paoloni, G. Paternoster, P. A. Pegoraro, K. Pelczar, C. Pellegrino, F. Perotti, V. Pesudo, S. Piacentini, F. Pietropaolo, N. Pino, C. Pira, A. Pocar, D. M. Poehlmann, S. Pordes, P. Pralavorio, D. Price, F. Raffaelli, F. Ragusa, Y. Ramachers, A. Ramirez, M. Razeti, A. Razeto, A. L. Renshaw, M. Rescigno, F. Resnati, F. Retiere, L. P. Rignanese, C. Ripoli, A. Rivetti, A. Roberts, C. Roberts, J. Rode, G. Rogers, L. Romero, M. Rossi, A. Rubbia, S. Sadashivajois, T. R. Saffold, O. Samoylov, E. Sandford, S. Sanfilippo, D. Santone, R. Santorelli, C. Savarese, E. Scapparone, G. Scioli, D. A. Semenov, A. Shchagin, A. Sheshukov, M. Simeone, P. Skensved, M. D. Skorokhvatov, O. Smirnov, T. Smirnova, B. Smith, A. Sokolov, M. Spangenberg, R. Stefanizzi, A. Steri, S. Stracka, V. Strickland, M. Stringer, S. Sulis, A. Sung, Y. Suvorov, A. M. Szelc, C. T\u00fcrko\u011f, R. Tartaglia, A. Taylor, J. Taylor, S. Tedesco, G. Testera, K. Thieme, T. N. Thorpe, A. Tonazzo, S. Torres-Lara, A. Tricomi, E. V. Unzhakov, T. Vallivilayil John, M. Van Uffelen, T. Viant, S. Viel, A. Vishneva, R. B. Vogelaar, J. Vossebeld, M. Wada, M. B. Walczak, Y. Wang, S. Westerdale, R. J. Wheadon, L. Williams, I. Wingerter-Seez, R. Wojaczy\u0144ski, Ma. M. Wojcik, Ma. Wojcik, T. Wright, Y. Xie, C. Yang, A. Zabihi, P. Zakhary, A. Zani, A. Zichichi, G. Zuzel, and M. P. Zykova","link":"http://arxiv.org/abs/2209.01177v2","abstract":"Dark matter lighter than 10 GeV/c$^2$ encompasses a promising range of\ncandidates. A conceptual design for a new detector, DarkSide-LowMass, is\npresented, based on the DarkSide-50 detector and progress toward DarkSide-20k,\noptimized for a low-threshold electron-counting measurement. Sensitivity to\nlight dark matter is explored for various potential energy thresholds and\nbackground rates. These studies show that DarkSide-LowMass can achieve\nsensitivity to light dark matter down to the solar neutrino floor for GeV-scale\nmasses and significant sensitivity down to 10 MeV/c$^2$ considering the Migdal\neffect or interactions with electrons. Requirements for optimizing the\ndetector's sensitivity are explored, as are potential sensitivity gains from\nmodeling and mitigating spurious electron backgrounds that may dominate the\nsignal at the lowest energies."},{"date":"2022-06","title":"Measuring Forgetting of Memorized Training Examples","author":"Matthew Jagielski, Om Thakkar, Florian Tram\u00e8r, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Chiyuan Zhang","link":"http://arxiv.org/abs/2207.00099v2","abstract":"Machine learning models exhibit two seemingly contradictory phenomena:\ntraining data memorization, and various forms of forgetting. In memorization,\nmodels overfit specific training examples and become susceptible to privacy\nattacks. In forgetting, examples which appeared early in training are forgotten\nby the end. In this work, we connect these phenomena. We propose a technique to\nmeasure to what extent models \"forget\" the specifics of training examples,\nbecoming less susceptible to privacy attacks on examples they have not seen\nrecently. We show that, while non-convex models can memorize data forever in\nthe worst-case, standard image, speech, and language models empirically do\nforget examples over time. We identify nondeterminism as a potential\nexplanation, showing that deterministically trained models do not forget. Our\nresults suggest that examples seen early when training with extremely large\ndatasets - for instance those examples used to pre-train a model - may observe\nprivacy benefits at the expense of examples seen later."},{"date":"2022-06","title":"Increasing Confidence in Adversarial Robustness Evaluations","author":"Roland S. Zimmermann, Wieland Brendel, Florian Tramer, and Nicholas Carlini","link":"http://arxiv.org/abs/2206.13991v1","abstract":"Hundreds of defenses have been proposed to make deep neural networks robust\nagainst minimal (adversarial) input perturbations. However, only a handful of\nthese defenses held up their claims because correctly evaluating robustness is\nextremely challenging: Weak attacks often fail to find adversarial examples\neven if they unknowingly exist, thereby making a vulnerable network look\nrobust. In this paper, we propose a test to identify weak attacks, and thus\nweak defense evaluations. Our test slightly modifies a neural network to\nguarantee the existence of an adversarial example for every sample.\nConsequentially, any correct attack must succeed in breaking this modified\nnetwork. For eleven out of thirteen previously-published defenses, the original\nevaluation of the defense fails our test, while stronger attacks that break\nthese defenses pass it. We hope that attack unit tests - such as ours - will be\na major component in future robustness evaluations and increase confidence in\nan empirical field that is currently riddled with skepticism."},{"date":"2022-06","title":"(Certified!!) Adversarial Robustness for Free!","author":"Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J. Zico Kolter","link":"http://arxiv.org/abs/2206.10550v2","abstract":"In this paper we show how to achieve state-of-the-art certified adversarial\nrobustness to 2-norm bounded perturbations by relying exclusively on\noff-the-shelf pretrained models. To do so, we instantiate the denoised\nsmoothing approach of Salman et al. 2020 by combining a pretrained denoising\ndiffusion probabilistic model and a standard high-accuracy classifier. This\nallows us to certify 71% accuracy on ImageNet under adversarial perturbations\nconstrained to be within an 2-norm of 0.5, an improvement of 14 percentage\npoints over the prior certified SoTA using any approach, or an improvement of\n30 percentage points over denoised smoothing. We obtain these results using\nonly pretrained diffusion models and image classifiers, without requiring any\nfine tuning or retraining of model parameters."},{"date":"2022-06","title":"The Privacy Onion Effect: Memorization is Relative","author":"Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer","link":"http://arxiv.org/abs/2206.10469v2","abstract":"Machine learning models trained on private datasets have been shown to leak\ntheir private data. While recent work has found that the average data point is\nrarely leaked, the outlier samples are frequently subject to memorization and,\nconsequently, privacy leakage. We demonstrate and analyse an Onion Effect of\nmemorization: removing the \"layer\" of outlier points that are most vulnerable\nto a privacy attack exposes a new layer of previously-safe points to the same\nattack. We perform several experiments to study this effect, and understand why\nit occurs. The existence of this effect has various consequences. For example,\nit suggests that proposals to defend against memorization without training with\nrigorous privacy guarantees are unlikely to be effective. Further, it suggests\nthat privacy-enhancing technologies such as machine unlearning could actually\nharm the privacy of other users."},{"date":"2022-03","title":"Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets","author":"Florian Tram\u00e8r, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski, Sanghyun Hong, and Nicholas Carlini","link":"http://arxiv.org/abs/2204.00032v2","abstract":"We introduce a new class of attacks on machine learning models. We show that\nan adversary who can poison a training dataset can cause models trained on this\ndataset to leak significant private details of training points belonging to\nother parties. Our active inference attacks connect two independent lines of\nwork targeting the integrity and privacy of machine learning training data.\n  Our attacks are effective across membership inference, attribute inference,\nand data extraction. For example, our targeted attacks can poison <0.1% of the\ntraining dataset to boost the performance of inference attacks by 1 to 2 orders\nof magnitude. Further, an adversary who controls a significant fraction of the\ntraining data (e.g., 50%) can launch untargeted attacks that enable 8x more\nprecise inference on all other users' otherwise-private data points.\n  Our results cast doubts on the relevance of cryptographic privacy guarantees\nin multiparty computation protocols for machine learning, if parties can\narbitrarily select their share of training data."},{"date":"2022-02","title":"Debugging Differential Privacy: A Case Study for Privacy Auditing","author":"Florian Tramer, Andreas Terzis, Thomas Steinke, Shuang Song, Matthew Jagielski, and Nicholas Carlini","link":"http://arxiv.org/abs/2202.12219v2","abstract":"Differential Privacy can provide provable privacy guarantees for training\ndata in machine learning. However, the presence of proofs does not preclude the\npresence of errors. Inspired by recent advances in auditing which have been\nused for estimating lower bounds on differentially private algorithms, here we\nshow that auditing can also be used to find flaws in (purportedly)\ndifferentially private schemes. In this case study, we audit a recent open\nsource implementation of a differentially private deep learning algorithm and\nfind, with 99.99999999% confidence, that the implementation does not satisfy\nthe claimed differential privacy guarantee."},{"date":"2022-02","title":"Quantifying Memorization Across Neural Language Models","author":"Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang","link":"http://arxiv.org/abs/2202.07646v3","abstract":"Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations."},{"date":"2021-12","title":"First Determination of the 27Al Neutron Distribution Radius from a Parity-Violating Electron Scattering Measurement","author":"QWeak Collaboration, D. Androic, D. S. Armstrong, K. Bartlett, R. S. Beminiwattha, J. Benesch, F. Benmokhtar, J. Birchall, R. D. Carlini, J. C. Cornejo, S. Covrig Dusa, M. M. Dalton, C. A. Davis, W. Deconinck, J. F. Dowd, J. A. Dunne, D. Dutta, W. S. Duvall, M. Elaasar, W. R. Falk, J. M. Finn, T. Forest, C. Gal, D. Gaskell, M. T. W. Gericke, V. M. Gray, K. Grimm, F. Guo, J. R. Hoskins, D. C. Jones, M. K. Jones, M. Kargiantoulakis, P. M. King, E. Korkmaz, S. Kowalski, J. Leacock, J. Leckey, A. R. Lee, J. H. Lee, L. Lee, S. MacEwan, D. Mack, J. A. Magee, R. Mahurin, J. Mammei, J. W. Martin, M. J. McHugh, D. Meekins, J. Mei, K. E. Mesick, R. Michaels, A. Micherdzinska, A. Mkrtchyan, H. Mkrtchyan, A. Narayan, L. Z. Ndukum, V. Nelyubin, Nuruzzaman, W. T. H van Oers, V. F. Owen, S. A. Page, J. Pan, K. D. Paschke, S. K. Phillips, M. L. Pitt, R. W. Radloff, J. F. Rajotte, W. D. Ramsay, J. Roche, B. Sawatzky, T. Seva, M. H. Shabestari, R. Silwal, N. Simicevic, G. R. Smith, P. Solvignon, D. T. Spayde, A. Subedi, R. Subedi, R. Suleiman, V. Tadevosyan, W. A. Tobias, V. Tvaskis, B. Waidyawansa, P. Wang, S. P. Wells, S. A. Wood, S. Yang, P. Zang, S. Zhamkochyan, M. E. Christy, C. J. Horowitz, F. J. Fattoyev, and Z. Lin","link":"http://arxiv.org/abs/2112.15412v2","abstract":"We report the first measurement of the parity-violating elastic electron\nscattering asymmetry on 27Al. The 27Al elastic asymmetry is A_PV = 2.16 +- 0.11\n(stat) +- 0.16 (syst) ppm, and was measured at <Q^2> =0.02357 +- 0.0001 GeV^2,\n<theta_lab> = 7.61 +- 0.02 degrees, and <E_lab> = 1.157 GeV with the Qweak\napparatus at Jefferson Lab. Predictions using a simple Born approximation as\nwell as more sophisticated distorted-wave calculations are in good agreement\nwith this result. From this asymmetry the 27Al neutron radius R_n = 2.89 +-\n0.12 fm was determined using a many-models correlation technique. The\ncorresponding neutron skin thickness R_n-R_p = -0.04 +- 0.12 fm is small, as\nexpected for a light nucleus with a neutron excess of only 1. This result thus\nserves as a successful benchmark for electroweak determinations of neutron\nradii on heavier nuclei. A tree-level approach was used to extract the 27Al\nweak radius R_w = 3.00 +- 0.15 fm, and the weak skin thickness R_wk - R_ch =\n-0.04 +- 0.15 fm. The weak form factor at this Q^2 is F_wk = 0.39 +- 0.04."},{"date":"2021-12","title":"Counterfactual Memorization in Neural Language Models","author":"Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tram\u00e8r, and Nicholas Carlini","link":"http://arxiv.org/abs/2112.12938v2","abstract":"Modern neural language models that are widely used in various NLP tasks risk\nmemorizing sensitive information from their training data. Understanding this\nmemorization is important in real world applications and also from a\nlearning-theoretical perspective. An open question in previous studies of\nlanguage model memorization is how to filter out \"common\" memorization. In\nfact, most memorization criteria strongly correlate with the number of\noccurrences in the training set, capturing memorized familiar phrases, public\nknowledge, templated texts, or other repeated data. We formulate a notion of\ncounterfactual memorization which characterizes how a model's predictions\nchange if a particular document is omitted during training. We identify and\nstudy counterfactually-memorized training examples in standard text datasets.\nWe estimate the influence of each memorized training example on the validation\nset and on generated texts, showing how this can provide direct evidence of the\nsource of memorization at test time."},{"date":"2021-12","title":"Membership Inference Attacks From First Principles","author":"Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer","link":"http://arxiv.org/abs/2112.03570v2","abstract":"A membership inference attack allows an adversary to query a trained machine\nlearning model to predict whether or not a particular example was contained in\nthe model's training dataset. These attacks are currently evaluated using\naverage-case \"accuracy\" metrics that fail to characterize whether the attack\ncan confidently identify any members of the training set. We argue that attacks\nshould instead be evaluated by computing their true-positive rate at low (e.g.,\n<0.1%) false-positive rates, and find most prior attacks perform poorly when\nevaluated in this way. To address this we develop a Likelihood Ratio Attack\n(LiRA) that carefully combines multiple ideas from the literature. Our attack\nis 10x more powerful at low false-positive rates, and also strictly dominates\nprior attacks on existing metrics."},{"date":"2021-09","title":"Unsolved Problems in ML Safety","author":"Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt","link":"http://arxiv.org/abs/2109.13916v5","abstract":"Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), reducing inherent model hazards\n(\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions."},{"date":"2021-08","title":"NeuraCrypt is not private","author":"Nicholas Carlini, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, and Florian Tramer","link":"http://arxiv.org/abs/2108.07256v1","abstract":"NeuraCrypt (Yara et al. arXiv 2021) is an algorithm that converts a sensitive\ndataset to an encoded dataset so that (1) it is still possible to train machine\nlearning models on the encoded data, but (2) an adversary who has access only\nto the encoded dataset can not learn much about the original sensitive dataset.\nWe break NeuraCrypt privacy claims, by perfectly solving the authors' public\nchallenge, and by showing that NeuraCrypt does not satisfy the formal privacy\ndefinitions posed in the original paper. Our attack consists of a series of\nboosting steps that, coupled with various design flaws, turns a 1% attack\nadvantage into a 100% complete break of the scheme."},{"date":"2021-07","title":"A study of events with photoelectric emission in the DarkSide-50 liquid argon Time Projection Chamber","author":"The DarkSide-50 Collaboration, :, P. Agnes, I. F. M. Albuquerque, T. Alexander, A. K. Alton, M. Ave, H. O. Back, G. Batignani, K. Biery, V. Bocci, W. M. Bonivento, B. Bottino, S. Bussino, M. Cadeddu, M. Cadoni, F. Calaprice, A. Caminata, N. Canci, M. Caravati, M. Cariello, M. Carlini, M. Carpinelli, S. Catalanotti, V. Cataudella, P. Cavalcante, S. Cavuoti, A. Chepurnov, C. Cicalo, A. G. Cocco, G. Covone, D. D'Angelo, S. Davini, A. De Candia, S. De Cecco, G. De Filippis, G. De Rosa, A. V. Derbin, A. Devoto, M. D'Incecco, C. Dionisi, F. Dordei, M. Downing, D. D'Urso, G. Fiorillo, D. Franco, F. Gabriele, C. Galbiati, C. Ghiano, C. Giganti, G. K. Giovanetti, O. Gorchakov, A. M. Goretti, A. Grobov, M. Gromov, M. Guan, Y. Guardincerri, M. Gulino, B. R. Hackett, K. Herner, B. Hosseini, F. Hubaut, E. V. Hungerford, An. Ianni, V. Ippolito, K. Keeter, C. L. Kendziora, I. Kochanek, D. Korablev, G. Korga, A. Kubankin, M. Kuss, M. La Commara, M. Lai, X. Li, M. Lissia, G. Longo, I. N. Machulin, L. P. Mapelli, S. M. Mari, J. Maricic, C. J. Martoff, A. Messina, P. D. Meyers, R. Milincic, M. Morrocchi, V. N. Muratova, P. Musico, A. Navrer Agasson, A. O. Nozdrina, A. Oleinik, F. Ortica, L. Pagani, M. Pallavicini, L. Pandola, E. Pantic, E. Paoloni, K. Pelczar, N. Pelliccia, E. Picciau, A. Pocar, S. Pordes, S. S. Poudel, P. Pralavorio, F. Ragusa, M. Razeti, A. Razeto, A. L. Renshaw, M. Rescigno, J. Rode, A. Romani, D. Sablone, O. Samoylov, W. Sands, S. Sanfilippo, C. Savarese, B. Schlitzer, D. A. Semenov, A. Shchagin, A. Sheshukov, M. D. Skorokhvatov, O. Smirnov, A. Sotnikov, S. Stracka, Y. Suvorov, R. Tartaglia, G. Testera, A. Tonazzo, E. V. Unzhakov, A. Vishneva, R. B. Vogelaar, M. Wada, H. Wang, Y. Wang, S. Westerdale, Ma. M. Wojcik, X. Xiao, C. Yang, and G. Zuzel","link":"http://arxiv.org/abs/2107.08015v2","abstract":"Finding unequivocal evidence of dark matter interactions in a particle\ndetector is a major objective of physics research. Liquid argon time projection\nchambers offer a path to probe Weakly Interacting Massive Particles scattering\ncross sections on nucleus down to the so-called neutrino floor, in a mass range\nfrom few GeV's to hundredths of TeV's. Based on the successful operation of the\nDarkSide-50 detector at LNGS, a new and more sensitive experiment,\nDarkSide-20k, has been designed and is now under construction. A thorough\nunderstanding of the DarkSide-50 detector response and, therefore, of all kind\nof observed events, is essential for an optimal design of the new experiment.\nIn this paper, we report on a particular set of events, which were not used for\ndark matter searches. Namely, standard two-pulse scintillation-ionization\nsignals accompanied by a small amplitude third pulse, originating from single\nor few electrons, in a time window of less than a maximum drift time. We\ncompare our findings to those of a recent paper of the LUX Collaboration\n(D.S.Akerib et al. Phys.Rev.D 102, 092004). Indeed, both experiments observe\nevents related to photoionization of the cathode. From the measured rate of\nthese events, we estimate for the first time the quantum efficiency of the\ntetraphenyl butadiene deposited on the DarkSide-50 cathode at wavelengths\naround 128 nm, in liquid argon. Also, both experiments observe events likely\nrelated to photoionization of impurities in the liquid. The probability of\nphotoelectron emission per unit length turns out to be one order of magnitude\nsmaller in DarkSide-50 than in LUX. This result, together with the much larger\nmeasured electron lifetime, coherently hints toward a lower concentration of\ncontaminants in DarkSide-50 than in LUX."},{"date":"2021-07","title":"TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence","author":"Davide Bacciu, Siranush Akarmazyan, Eric Armengaud, Manlio Bacco, George Bravos, Calogero Calandra, Emanuele Carlini, Antonio Carta, Pietro Cassara, Massimo Coppola, Charalampos Davalas, Patrizio Dazzi, Maria Carmela Degennaro, Daniele Di Sarli, J\u00fcrgen Dobaj, Claudio Gallicchio, Sylvain Girbal, Alberto Gotta, Riccardo Groppo, Vincenzo Lomonaco, Georg Macher, Daniele Mazzei, Gabriele Mencagli, Dimitrios Michail, Alessio Micheli, Roberta Peroglio, Salvatore Petroni, Rosaria Potenza, Farank Pourdanesh, Christos Sardianos, Konstantinos Tserpes, Fulvio Tagliab\u00f2, Jakob Valtl, Iraklis Varlamis, and Omar Veledar","link":"http://arxiv.org/abs/2107.06543v1","abstract":"This paper discusses the perspective of the H2020 TEACHING project on the\nnext generation of autonomous applications running in a distributed and highly\nheterogeneous environment comprising both virtual and physical resources\nspanning the edge-cloud continuum. TEACHING puts forward a human-centred vision\nleveraging the physiological, emotional, and cognitive state of the users as a\ndriver for the adaptation and optimization of the autonomous applications. It\ndoes so by building a distributed, embedded and federated learning system\ncomplemented by methods and tools to enforce its dependability, security and\nprivacy preservation. The paper discusses the main concepts of the TEACHING\napproach and singles out the main AI-related research challenges associated\nwith it. Further, we provide a discussion of the design choices for the\nTEACHING system to tackle the aforementioned challenges"},{"date":"2021-07","title":"Deduplicating Training Data Makes Language Models Better","author":"Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini","link":"http://arxiv.org/abs/2107.06499v2","abstract":"We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets."},{"date":"2021-06","title":"Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent","author":"Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang, and Nicholas Carlini","link":"http://arxiv.org/abs/2106.15023v1","abstract":"Evading adversarial example detection defenses requires finding adversarial\nexamples that must simultaneously (a) be misclassified by the model and (b) be\ndetected as non-adversarial. We find that existing attacks that attempt to\nsatisfy multiple simultaneous constraints often over-optimize against one\nconstraint at the cost of satisfying another. We introduce Orthogonal Projected\nGradient Descent, an improved attack technique to generate adversarial examples\nthat avoids this problem by orthogonalizing the gradients when running standard\ngradient-based attacks. We use our technique to evade four state-of-the-art\ndetection defenses, reducing their accuracy to 0% while maintaining a 0%\ndetection rate."},{"date":"2021-06","title":"Data Poisoning Won't Save You From Facial Recognition","author":"Evani Radiya-Dixit, Sanghyun Hong, Nicholas Carlini, and Florian Tram\u00e8r","link":"http://arxiv.org/abs/2106.14851v2","abstract":"Data poisoning has been proposed as a compelling defense against facial\nrecognition models trained on Web-scraped pictures. Users can perturb images\nthey post online, so that models will misclassify future (unperturbed)\npictures. We demonstrate that this strategy provides a false sense of security,\nas it ignores an inherent asymmetry between the parties: users' pictures are\nperturbed once and for all before being published (at which point they are\nscraped) and must thereafter fool all future models -- including models trained\nadaptively against the users' past attacks, or models that use technologies\ndiscovered after the attack. We evaluate two systems for poisoning attacks\nagainst large-scale facial recognition, Fawkes (500'000+ downloads) and LowKey.\nWe demonstrate how an \"oblivious\" model trainer can simply wait for future\ndevelopments in computer vision to nullify the protection of pictures collected\nin the past. We further show that an adversary with black-box access to the\nattack can (i) train a robust model that resists the perturbations of collected\npictures and (ii) detect poisoned pictures uploaded online. We caution that\nfacial recognition poisoning will not admit an \"arms race\" between attackers\nand defenders. Once perturbed pictures are scraped, the attack cannot be\nchanged so any future successful defense irrevocably undermines users' privacy."},{"date":"2021-06","title":"Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples","author":"Maura Pintor, Luca Demetrio, Angelo Sotgiu, Ambra Demontis, Nicholas Carlini, Battista Biggio, and Fabio Roli","link":"http://arxiv.org/abs/2106.09947v3","abstract":"Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nrobustness by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by: (i) categorizing attack failures based on how they affect the\noptimization of gradient-based attacks, while also unveiling two novel failures\naffecting many popular attack implementations and past evaluations; (ii)\nproposing six novel indicators of failure, to automatically detect the presence\nof such failures in the attack optimization process; and (iii) suggesting a\nsystematic protocol to apply the corresponding fixes. Our extensive\nexperimental analysis, involving more than 15 models in 3 distinct application\ndomains, shows that our indicators of failure can be used to debug and improve\ncurrent adversarial robustness evaluations, thereby providing a first concrete\nstep towards automatizing and systematizing them. Our open-source code is\navailable at: https://github.com/pralab/IndicatorsOfAttackFailure."},{"date":"2021-06","title":"Poisoning and Backdooring Contrastive Learning","author":"Nicholas Carlini, and Andreas Terzis","link":"http://arxiv.org/abs/2106.09667v2","abstract":"Multimodal contrastive learning methods like CLIP train on noisy and\nuncurated training datasets. This is cheaper than labeling datasets manually,\nand even improves out-of-distribution robustness. We show that this practice\nmakes backdoor and poisoning attacks a significant threat. By poisoning just\n0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual\nCaptions dataset), we can cause the model to misclassify test images by\noverlaying a small patch. Targeted poisoning attacks, whereby the model\nmisclassifies a particular test input with an adversarially-desired label, are\neven easier requiring control of 0.0001% of the dataset (e.g., just three out\nof the 3 million images). Our attacks call into question whether training on\nnoisy and uncurated Internet scrapes is desirable."},{"date":"2021-06","title":"AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation","author":"David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin","link":"http://arxiv.org/abs/2106.04732v2","abstract":"We extend semi-supervised learning to the problem of domain adaptation to\nlearn significantly higher-accuracy models that train on one data distribution\nand test on a different one. With the goal of generality, we introduce\nAdaMatch, a method that unifies the tasks of unsupervised domain adaptation\n(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation\n(SSDA). In an extensive experimental study, we compare its behavior with\nrespective state-of-the-art techniques from SSL, SSDA, and UDA on vision\nclassification tasks. We find AdaMatch either matches or significantly exceeds\nthe state-of-the-art in each case using the same hyper-parameters regardless of\nthe dataset or task. For example, AdaMatch nearly doubles the accuracy compared\nto that of the prior state-of-the-art on the UDA task for DomainNet and even\nexceeds the accuracy of the prior state-of-the-art obtained with pre-training\nby 6.4% when AdaMatch is trained completely from scratch. Furthermore, by\nproviding AdaMatch with just one labeled example per class from the target\ndomain (i.e., the SSDA setting), we increase the target accuracy by an\nadditional 6.1%, and with 5 labeled examples, by 13.6%."},{"date":"2021-06","title":"Handcrafted Backdoors in Deep Neural Networks","author":"Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin","link":"http://arxiv.org/abs/2106.04690v2","abstract":"When machine learning training is outsourced to third parties, $backdoor$\n$attacks$ become practical as the third party who trains the model may act\nmaliciously to inject hidden behaviors into the otherwise accurate model. Until\nnow, the mechanism to inject backdoors has been limited to $poisoning$. We\nargue that a supply-chain attacker has more attack techniques available by\nintroducing a $handcrafted$ attack that directly manipulates a model's weights.\nThis direct modification gives our attacker more degrees of freedom compared to\npoisoning, and we show it can be used to evade many backdoor detection or\nremoval defenses effectively. Across four datasets and four network\narchitectures our backdoor attacks maintain an attack success rate above 96%.\nOur results suggest that further research is needed for understanding the\ncomplete space of supply-chain backdoor attacks."},{"date":"2021-05","title":"Poisoning the Unlabeled Dataset of Semi-Supervised Learning","author":"Nicholas Carlini","link":"http://arxiv.org/abs/2105.01622v2","abstract":"Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n  We study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n  We find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack."},{"date":"2021-01","title":"On the Evolution of Syntactic Information Encoded by BERT's Contextualized Representations","author":"Laura P\u00e9rez-Mayos, Roberto Carlini, Miguel Ballesteros, and Leo Wanner","link":"http://arxiv.org/abs/2101.11492v2","abstract":"The adaptation of pretrained language models to solve supervised tasks has\nbecome a baseline in NLP, and many recent works have focused on studying how\nlinguistic information is encoded in the pretrained sentence representations.\nAmong other information, it has been shown that entire syntax trees are\nimplicitly embedded in the geometry of such models. As these models are often\nfine-tuned, it becomes increasingly important to understand how the encoded\nknowledge evolves along the fine-tuning. In this paper, we analyze the\nevolution of the embedded syntax trees along the fine-tuning process of BERT\nfor six different tasks, covering all levels of the linguistic structure.\nExperimental results show that the encoded syntactic information is forgotten\n(PoS tagging), reinforced (dependency and constituency parsing) or preserved\n(semantics-related tasks) in different ways along the fine-tuning process\ndepending on the task."},{"date":"2021-01","title":"Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning","author":"Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini","link":"http://arxiv.org/abs/2101.04535v1","abstract":"Differentially private (DP) machine learning allows us to train models on\nprivate data while limiting data leakage. DP formalizes this data leakage\nthrough a cryptographic game, where an adversary must predict if a model was\ntrained on a dataset D, or a dataset D' that differs in just one example.If\nobserving the training algorithm does not meaningfully increase the adversary's\nodds of successfully guessing which dataset the model was trained on, then the\nalgorithm is said to be differentially private. Hence, the purpose of privacy\nanalysis is to upper bound the probability that any adversary could\nsuccessfully guess which dataset the model was trained on.In our paper, we\ninstantiate this hypothetical adversary in order to establish lower bounds on\nthe probability that this distinguishing game can be won. We use this adversary\nto evaluate the importance of the adversary capabilities allowed in the privacy\nanalysis of DP training algorithms.For DP-SGD, the most common method for\ntraining neural networks with differential privacy, our lower bounds are tight\nand match the theoretical upper bound. This implies that in order to prove\nbetter upper bounds, it will be necessary to make use of additional\nassumptions. Fortunately, we find that our attacks are significantly weaker\nwhen additional (realistic)restrictions are put in place on the adversary's\ncapabilities.Thus, in the practical setting common to many real-world\ndeployments, there is a gap between our lower bounds and the upper bounds\nprovided by the analysis: differential privacy is conservative and adversaries\nmay not be able to leak as much information as suggested by the theoretical\nbound."},{"date":"2020-12","title":"Extracting Training Data from Large Language Models","author":"Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel","link":"http://arxiv.org/abs/2012.07805v2","abstract":"It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models."},{"date":"2020-11","title":"Is Private Learning Possible with Instance Encoding?","author":"Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, and Florian Tramer","link":"http://arxiv.org/abs/2011.05315v2","abstract":"A private machine learning algorithm hides as much as possible about its\ntraining data while still preserving accuracy. In this work, we study whether a\nnon-private learning algorithm can be made private by relying on an\ninstance-encoding mechanism that modifies the training inputs before feeding\nthem to a normal learner. We formalize both the notion of instance encoding and\nits privacy by providing two attack models. We first prove impossibility\nresults for achieving a (stronger) model. Next, we demonstrate practical\nattacks in the second (weaker) attack model on InstaHide, a recent proposal by\nHuang, Song, Li and Arora [ICML'20] that aims to use instance encoding for\nprivacy."},{"date":"2020-09","title":"Erratum Concerning the Obfuscated Gradients Attack on Stochastic Activation Pruning","author":"Guneet S. Dhillon, and Nicholas Carlini","link":"http://arxiv.org/abs/2010.00071v1","abstract":"Stochastic Activation Pruning (SAP) (Dhillon et al., 2018) is a defense to\nadversarial examples that was attacked and found to be broken by the\n\"Obfuscated Gradients\" paper (Athalye et al., 2018). We discover a flaw in the\nre-implementation that artificially weakens SAP. When SAP is applied properly,\nthe proposed attack is not effective. However, we show that a new use of the\nBPDA attack technique can still reduce the accuracy of SAP to 0.1%."},{"date":"2020-09","title":"A Partial Break of the Honeypots Defense to Catch Adversarial Attacks","author":"Nicholas Carlini","link":"http://arxiv.org/abs/2009.10975v1","abstract":"A recent defense proposes to inject \"honeypots\" into neural networks in order\nto detect adversarial attacks. We break the baseline version of this defense by\nreducing the detection true positive rate to 0\\% and the detection AUC to 0.02,\nmaintaining the original distortion bounds. The authors of the original paper\nhave amended the defense in their CCS'20 paper to mitigate this attacks. To aid\nfurther research, we release the complete 2.5 hour keystroke-by-keystroke\nscreen recording of our attack process at\nhttps://nicholas.carlini.com/code/ccs_honeypot_break."},{"date":"2020-07","title":"Label-Only Membership Inference Attacks","author":"Christopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot","link":"http://arxiv.org/abs/2007.14321v3","abstract":"Membership inference attacks are one of the simplest forms of privacy leakage\nfor machine learning models: given a data point and model, determine whether\nthe point was used to train the model. Existing membership inference attacks\nexploit models' abnormal confidence when queried on their training data. These\nattacks do not apply if the adversary only gets access to models' predicted\nlabels, without a confidence measure. In this paper, we introduce label-only\nmembership inference attacks. Instead of relying on confidence scores, our\nattacks evaluate the robustness of a model's predicted labels under\nperturbations to obtain a fine-grained membership signal. These perturbations\ninclude common data augmentations or adversarial examples. We empirically show\nthat our label-only membership inference attacks perform on par with prior\nattacks that required access to model confidences. We further demonstrate that\nlabel-only attacks break multiple defenses against membership inference attacks\nthat (implicitly or explicitly) rely on a phenomenon we call confidence\nmasking. These defenses modify a model's confidence scores in order to thwart\nattacks, but leave the model's predicted labels unchanged. Our label-only\nattacks demonstrate that confidence-masking is not a viable defense strategy\nagainst membership inference. Finally, we investigate worst-case label-only\nattacks, that infer membership for a small number of outlier data points. We\nshow that label-only attacks also match confidence-based attacks in this\nsetting. We find that training models with differential privacy and (strong) L2\nregularization are the only known defense strategies that successfully prevents\nall attacks. This remains true even when the differential privacy budget is too\nhigh to offer meaningful provable guarantees."},{"date":"2020-07","title":"Measuring Robustness to Natural Distribution Shifts in Image Classification","author":"Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt","link":"http://arxiv.org/abs/2007.00644v2","abstract":"We study how robust current ImageNet models are to distribution shifts\narising from natural variations in datasets. Most research on robustness\nfocuses on synthetic image perturbations (noise, simulated weather artifacts,\nadversarial examples, etc.), which leaves open how robustness on synthetic\ndistribution shift relates to distribution shift arising in real data. Informed\nby an evaluation of 204 ImageNet models in 213 different test conditions, we\nfind that there is often little to no transfer of robustness from current\nsynthetic to natural distribution shift. Moreover, most current techniques\nprovide no robustness to the natural distribution shifts in our testbed. The\nmain exception is training on larger and more diverse datasets, which in\nmultiple cases increases robustness, but is still far from closing the\nperformance gaps. Our results indicate that distribution shifts arising in real\ndata are currently an open research problem. We provide our testbed and data as\na resource for future work at https://modestyachts.github.io/imagenet-testbed/ ."},{"date":"2020-04","title":"Evading Deepfake-Image Detectors with White- and Black-Box Attacks","author":"Nicholas Carlini, and Hany Farid","link":"http://arxiv.org/abs/2004.00622v1","abstract":"It is now possible to synthesize highly realistic images of people who don't\nexist. Such content has, for example, been implicated in the creation of\nfraudulent social-media profiles responsible for dis-information campaigns.\nSignificant efforts are, therefore, being deployed to detect\nsynthetically-generated content. One popular forensic approach trains a neural\nnetwork to distinguish real from synthetic content.\n  We show that such forensic classifiers are vulnerable to a range of attacks\nthat reduce the classifier to near-0% accuracy. We develop five attack case\nstudies on a state-of-the-art classifier that achieves an area under the ROC\ncurve (AUC) of 0.95 on almost all existing image generators, when only trained\non one generator. With full access to the classifier, we can flip the lowest\nbit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb\n1% of the image area to reduce the classifier's AUC to 0.08; or add a single\nnoise pattern in the synthesizer's latent space to reduce the classifier's AUC\nto 0.17. We also develop a black-box attack that, with no access to the target\nclassifier, reduces the AUC to 0.22. These attacks reveal significant\nvulnerabilities of certain image-forensic classifiers."},{"date":"2020-03","title":"Mechanical Ventilator Milano (MVM): A Novel Mechanical Ventilator Designed for Mass Scale Production in Response to the COVID-19 Pandemic","author":"C. Galbiati, A. Abba, P. Agnes, P. Amaudruz, M. Arba, F. Ardellier-Desages, C. Badia, G. Batignani, G. Bellani, G. Bianchi, D. Bishop, V. Bocci, W. Bonivento, B. Bottino, M. Bouchard, S. Brice, G. Buccino, S. Bussino, A. Caminata, A. Capra, M. Caravati, M. Carlini, L. Carrozzi, J. M. Cela, B. Celano, C. Charette, S. Coelli, M. Constable, V. Cocco, G. Croci, S. Cudmore, A. Dal Molin, S. D'Auria, G. D'Avenio, J. DeRuiter, S. De Cecco, L. De Lauretis, M. Del Tutto, A. Devoto, T. Dinon, E. Druszkiewicz, A. Fabbri, F. Ferroni, G. Fiorillo, R. Ford, G. Foti, D. Franco, F. Gabriele, P. Garcia Abia, L. S. Giarratana, J. Givoletti, Mi. Givoletti, G. Gorini, E. Gramellini, G. Grosso, F. Guescini, E. Guetre, T. Hadden, J. Hall, A. Heavey, G. Hersak, N. Hessey, An. Ianni, C. Ienzi, V. Ippolito, C. L. Kendziora, M. King, A. Kittmer, I. Kochanek, R. Kruecken, M. La Commara, G. Leblond, X. Li, C. Lim, T. Lindner, T. Lombardi, T. Long, P. Lu, G. Lukhanin, G. Magni, R. Maharaj, M. Malosio, C. Mapelli, P. Maqueo, P. Margetak, S. M. Mari, L. Martin, N. Massacret, A. McDonald, D. Minuzzo, T. A. Mohayai, L. Molinari Tosatti, C. Moretti, A. Muraro, F. Nati, A. J. Noble, A. Norrick, K. Olchanski, I. Palumbo, R. Paoletti, N. Paoli, C. Pearson, C. Pellegrino, V. Pesudo, A. Pocar, M. Pontesilli, R. Pordes, S. Pordes, A. Prini, O. Putignano, J. L. Raaf, M. Razeti, A. Razeto, D. Reed, A. Renshaw, M. Rescigno, F. Retiere, L. P. Rignanese, J. Rode, L. J. Romualdez, R. Santorelli, D. Sablone, E. Scapparone, T. Schaubel, B. Shaw, A. S. Slutsky, B. Smith, N. J. T. Smith, P. Spagnolo, F. Spinella, A. Stenzler, A. Steri, L. Stiaccini, C. Stoughton, P. Stringari, M. Tardocchi, R. Tartaglia, G. Testera, C. Tintori, A. Tonazzo, J. Tseng, E. Viscione, F. Vivaldi, M. Wada, H. Wang, S. Westerdale, S. Yue, and A. Zardoni","link":"http://arxiv.org/abs/2003.10405v3","abstract":"Presented here is the design of the Mechanical Ventilator Milano (MVM), a\nnovel mechanical ventilator designed for rapid mass production in response to\nthe COVID-19 pandemic to address the urgent shortage of intensive therapy\nventilators in many countries, and the growing difficulty in procuring these\ndevices through normal supply chains across borders. This ventilator is an\nelectro-mechanical equivalent of the old and reliable Manley Ventilator, and is\nable to operate in both pressure-controlled and pressure-supported ventilation\nmodes. MVM is optimized for the COVID-19 emergency, thanks to the collaboration\nwith medical doctors in the front line. MVM is designed for large-scale\nproduction in a short amount of time and at a limited cost, as it relays on\noff-the-shelf components, readily available worldwide. Operation of the MVM\nrequires only a source of compressed oxygen (or compressed medical air) and\nelectrical power. Initial tests of a prototype device with a breathing\nsimulator are also presented. Further tests and developments are underway. At\nthis stage the MVM is not yet a certified medical device but certification is\nin progress."},{"date":"2020-03","title":"Cryptanalytic Extraction of Neural Network Models","author":"Nicholas Carlini, Matthew Jagielski, and Ilya Mironov","link":"http://arxiv.org/abs/2003.04884v2","abstract":"We argue that the machine learning problem of model extraction is actually a\ncryptanalytic problem in disguise, and should be studied as such. Given oracle\naccess to a neural network, we introduce a differential attack that can\nefficiently steal the parameters of the remote model up to floating point\nprecision. Our attack relies on the fact that ReLU neural networks are\npiecewise linear functions, and thus queries at the critical points reveal\ninformation about the model parameters.\n  We evaluate our attack on multiple neural network models and extract models\nthat are 2^20 times more precise and require 100x fewer queries than prior\nwork. For example, we extract a 100,000 parameter neural network trained on the\nMNIST digit recognition task with 2^21.5 queries in under an hour, such that\nthe extracted model agrees with the oracle on all inputs up to a worst-case\nerror of 2^-25, or a model with 4,000 parameters in 2^18.5 queries with\nworst-case error of 2^-40.4. Code is available at\nhttps://github.com/google-research/cryptanalytic-model-extraction."},{"date":"2020-02","title":"On Adaptive Attacks to Adversarial Example Defenses","author":"Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry","link":"http://arxiv.org/abs/2002.08347v2","abstract":"Adaptive attacks have (rightfully) become the de facto standard for\nevaluating defenses to adversarial examples. We find, however, that typical\nadaptive evaluations are incomplete. We demonstrate that thirteen defenses\nrecently published at ICLR, ICML and NeurIPS---and chosen for illustrative and\npedagogical purposes---can be circumvented despite attempting to perform\nevaluations using adaptive attacks. While prior evaluation papers focused\nmainly on the end result---showing that a defense was ineffective---this paper\nfocuses on laying out the methodology and the approach necessary to perform an\nadaptive attack. We hope that these analyses will serve as guidance on how to\nproperly perform adaptive attacks against defenses to adversarial examples, and\nthus will allow the community to make further progress in building more robust\nmodels."},{"date":"2020-02","title":"Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations","author":"Florian Tram\u00e8r, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and J\u00f6rn-Henrik Jacobsen","link":"http://arxiv.org/abs/2002.04599v2","abstract":"Adversarial examples are malicious inputs crafted to induce\nmisclassification. Commonly studied sensitivity-based adversarial examples\nintroduce semantically-small changes to an input that result in a different\nmodel prediction. This paper studies a complementary failure mode,\ninvariance-based adversarial examples, that introduce minimal semantic changes\nthat modify an input's true label yet preserve the model's prediction. We\ndemonstrate fundamental tradeoffs between these two types of adversarial\nexamples.\n  We show that defenses against sensitivity-based attacks actively harm a\nmodel's accuracy on invariance-based attacks, and that new approaches are\nneeded to resist both attack types. In particular, we break state-of-the-art\nadversarially-trained and certifiably-robust models by generating small\nperturbations that the models are (provably) robust to, yet that change an\ninput's class according to human labelers. Finally, we formally show that the\nexistence of excessively invariant classifiers arises from the presence of\noverly-robust predictive features in standard datasets."},{"date":"2020-01","title":"FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence","author":"Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel","link":"http://arxiv.org/abs/2001.07685v2","abstract":"Semi-supervised learning (SSL) provides an effective means of leveraging\nunlabeled data to improve a model's performance. In this paper, we demonstrate\nthe power of a simple combination of two common SSL methods: consistency\nregularization and pseudo-labeling. Our algorithm, FixMatch, first generates\npseudo-labels using the model's predictions on weakly-augmented unlabeled\nimages. For a given image, the pseudo-label is only retained if the model\nproduces a high-confidence prediction. The model is then trained to predict the\npseudo-label when fed a strongly-augmented version of the same image. Despite\nits simplicity, we show that FixMatch achieves state-of-the-art performance\nacross a variety of standard semi-supervised learning benchmarks, including\n94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just\n4 labels per class. Since FixMatch bears many similarities to existing SSL\nmethods that achieve worse performance, we carry out an extensive ablation\nstudy to tease apart the experimental factors that are most important to\nFixMatch's success. We make our code available at\nhttps://github.com/google-research/fixmatch."},{"date":"2019-11","title":"ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring","author":"David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel","link":"http://arxiv.org/abs/1911.09785v2","abstract":"We improve the recently-proposed \"MixMatch\" semi-supervised learning\nalgorithm by introducing two new techniques: distribution alignment and\naugmentation anchoring. Distribution alignment encourages the marginal\ndistribution of predictions on unlabeled data to be close to the marginal\ndistribution of ground-truth labels. Augmentation anchoring feeds multiple\nstrongly augmented versions of an input into the model and encourages each\noutput to be close to the prediction for a weakly-augmented version of the same\ninput. To produce strong augmentations, we propose a variant of AutoAugment\nwhich learns the augmentation policy while the model is being trained. Our new\nalgorithm, dubbed ReMixMatch, is significantly more data-efficient than prior\nwork, requiring between $5\\times$ and $16\\times$ less data to reach the same\naccuracy. For example, on CIFAR-10 with 250 labeled examples we reach $93.73\\%$\naccuracy (compared to MixMatch's accuracy of $93.58\\%$ with $4{,}000$ examples)\nand a median accuracy of $84.92\\%$ with just four labels per class. We make our\ncode and data open-source at https://github.com/google-research/remixmatch."},{"date":"2019-10","title":"Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications","author":"Nicholas Carlini, \u00dalfar Erlingsson, and Nicolas Papernot","link":"http://arxiv.org/abs/1910.13427v1","abstract":"We develop techniques to quantify the degree to which a given (training or\ntesting) example is an outlier in the underlying distribution. We evaluate five\nmethods to score examples in a dataset by how well-represented the examples\nare, for different plausible definitions of \"well-represented\", and apply these\nto four common datasets: MNIST, Fashion-MNIST, CIFAR-10, and ImageNet. Despite\nbeing independent approaches, we find all five are highly correlated,\nsuggesting that the notion of being well-represented can be quantified. Among\nother uses, we find these methods can be combined to identify (a) prototypical\nexamples (that match human expectations); (b) memorized training examples; and,\n(c) uncommon submodes of the dataset. Further, we show how we can utilize our\nmetrics to determine an improved ordering for curriculum learning, and impact\nadversarial robustness. We release all metric values on training and test sets\nwe studied."},{"date":"2019-09","title":"High Accuracy and High Fidelity Extraction of Neural Networks","author":"Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot","link":"http://arxiv.org/abs/1909.01838v2","abstract":"In a model extraction attack, an adversary steals a copy of a remotely\ndeployed machine learning model, given oracle prediction access. We taxonomize\nmodel extraction attacks around two objectives: *accuracy*, i.e., performing\nwell on the underlying learning task, and *fidelity*, i.e., matching the\npredictions of the remote victim classifier on any input.\n  To extract a high-accuracy model, we develop a learning-based attack\nexploiting the victim to supervise the training of an extracted model. Through\nanalytical and empirical arguments, we then explain the inherent limitations\nthat prevent any learning-based strategy from extracting a truly high-fidelity\nmodel---i.e., extracting a functionally-equivalent model whose predictions are\nidentical to those of the victim model on all possible inputs. Addressing these\nlimitations, we expand on prior work to develop the first practical\nfunctionally-equivalent extraction attack for direct extraction (i.e., without\ntraining) of a model's weights.\n  We perform experiments both on academic datasets and a state-of-the-art image\nclassifier trained with 1 billion proprietary images. In addition to broadening\nthe scope of model extraction research, our work demonstrates the practicality\nof model extraction attacks against production-grade systems."},{"date":"2019-07","title":"Stateful Detection of Black-Box Adversarial Attacks","author":"Steven Chen, Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1907.05587v1","abstract":"The problem of adversarial examples, evasion attacks on machine learning\nclassifiers, has proven extremely difficult to solve. This is true even when,\nas is the case in many practical settings, the classifier is hosted as a remote\nservice and so the adversary does not have direct access to the model\nparameters.\n  This paper argues that in such settings, defenders have a much larger space\nof actions than have been previously explored. Specifically, we deviate from\nthe implicit assumption made by prior work that a defense must be a stateless\nfunction that operates on individual examples, and explore the possibility for\nstateful defenses.\n  To begin, we develop a defense designed to detect the process of adversarial\nexample generation. By keeping a history of the past queries, a defender can\ntry to identify when a sequence of queries appears to be for the purpose of\ngenerating an adversarial example. We then introduce query blinding, a new\nclass of attacks designed to bypass defenses that rely on such a defense\napproach.\n  We believe that expanding the study of adversarial examples from stateless\nclassifiers to stateful systems is not only more realistic for many black-box\nsettings, but also gives the defender a much-needed advantage in responding to\nthe adversary."},{"date":"2019-05","title":"A critique of the DeepSec Platform for Security Analysis of Deep Learning Models","author":"Nicholas Carlini","link":"http://arxiv.org/abs/1905.07112v1","abstract":"At IEEE S&P 2019, the paper \"DeepSec: A Uniform Platform for Security\nAnalysis of Deep Learning Model\" aims to to \"systematically evaluate the\nexisting adversarial attack and defense methods.\" While the paper's goals are\nlaudable, it fails to achieve them and presents results that are fundamentally\nflawed and misleading. We explain the flaws in the DeepSec work, along with how\nits analysis fails to meaningfully evaluate the various attacks and defenses.\nSpecifically, DeepSec (1) evaluates each defense obliviously, using attacks\ncrafted against undefended models; (2) evaluates attacks and defenses using\nincorrect implementations that greatly under-estimate their effectiveness; (3)\nevaluates the robustness of each defense as an average, not based on the most\neffective attack against that defense; (4) performs several statistical\nanalyses incorrectly and fails to report variance; and, (5) as a result of\nthese errors draws invalid conclusions and makes sweeping generalizations."},{"date":"2019-05","title":"MixMatch: A Holistic Approach to Semi-Supervised Learning","author":"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel","link":"http://arxiv.org/abs/1905.02249v2","abstract":"Semi-supervised learning has proven to be a powerful paradigm for leveraging\nunlabeled data to mitigate the reliance on large labeled datasets. In this\nwork, we unify the current dominant approaches for semi-supervised learning to\nproduce a new algorithm, MixMatch, that works by guessing low-entropy labels\nfor data-augmented unlabeled examples and mixing labeled and unlabeled data\nusing MixUp. We show that MixMatch obtains state-of-the-art results by a large\nmargin across many datasets and labeled data amounts. For example, on CIFAR-10\nwith 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by\na factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a\ndramatically better accuracy-privacy trade-off for differential privacy.\nFinally, we perform an ablation study to tease apart which components of\nMixMatch are most important for its success."},{"date":"2019-03","title":"MLSys: The New Frontier of Machine Learning Systems","author":"Alexander Ratner, Dan Alistarh, Gustavo Alonso, David G. Andersen, Peter Bailis, Sarah Bird, Nicholas Carlini, Bryan Catanzaro, Jennifer Chayes, Eric Chung, Bill Dally, Jeff Dean, Inderjit S. Dhillon, Alexandros Dimakis, Pradeep Dubey, Charles Elkan, Grigori Fursin, Gregory R. Ganger, Lise Getoor, Phillip B. Gibbons, Garth A. Gibson, Joseph E. Gonzalez, Justin Gottschlich, Song Han, Kim Hazelwood, Furong Huang, Martin Jaggi, Kevin Jamieson, Michael I. Jordan, Gauri Joshi, Rania Khalaf, Jason Knight, Jakub Kone\u010dn\u00fd, Tim Kraska, Arun Kumar, Anastasios Kyrillidis, Aparna Lakshmiratan, Jing Li, Samuel Madden, H. Brendan McMahan, Erik Meijer, Ioannis Mitliagkas, Rajat Monga, Derek Murray, Kunle Olukotun, Dimitris Papailiopoulos, Gennady Pekhimenko, Theodoros Rekatsinas, Afshin Rostamizadeh, Christopher R\u00e9, Christopher De Sa, Hanie Sedghi, Siddhartha Sen, Virginia Smith, Alex Smola, Dawn Song, Evan Sparks, Ion Stoica, Vivienne Sze, Madeleine Udell, Joaquin Vanschoren, Shivaram Venkataraman, Rashmi Vinayak, Markus Weimer, Andrew Gordon Wilson, Eric Xing, Matei Zaharia, Ce Zhang, and Ameet Talwalkar","link":"http://arxiv.org/abs/1904.03257v3","abstract":"Machine learning (ML) techniques are enjoying rapidly increasing adoption.\nHowever, designing and implementing the systems that support ML models in\nreal-world deployments remains a significant obstacle, in large part due to the\nradically different development and deployment profile of modern ML methods,\nand the range of practical concerns that come with broader adoption. We propose\nto foster a new systems machine learning research community at the intersection\nof the traditional systems and ML communities, focused on topics such as\nhardware systems for ML, software systems for ML, and ML optimized for metrics\nbeyond predictive accuracy. To do this, we describe a new conference, MLSys,\nthat explicitly targets research at the intersection of systems and machine\nlearning with a program committee split evenly between experts in systems and\nML, and an explicit focus on topics at the intersection of the two."},{"date":"2019-03","title":"Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness","author":"J\u00f6rn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tram\u00e8r, and Nicolas Papernot","link":"http://arxiv.org/abs/1903.10484v1","abstract":"Adversarial examples are malicious inputs crafted to cause a model to\nmisclassify them. Their most common instantiation, \"perturbation-based\"\nadversarial examples introduce changes to the input that leave its true label\nunchanged, yet result in a different model prediction. Conversely,\n\"invariance-based\" adversarial examples insert changes to the input that leave\nthe model's prediction unaffected despite the underlying input's label having\nchanged.\n  In this paper, we demonstrate that robustness to perturbation-based\nadversarial examples is not only insufficient for general robustness, but\nworse, it can also increase vulnerability of the model to invariance-based\nadversarial examples. In addition to analytical constructions, we empirically\nstudy vision classifiers with state-of-the-art robustness to perturbation-based\nadversaries constrained by an $\\ell_p$ norm. We mount attacks that exploit\nexcessive model invariance in directions relevant to the task, which are able\nto find adversarial examples within the $\\ell_p$ ball. In fact, we find that\nclassifiers trained to be $\\ell_p$-norm robust are more vulnerable to\ninvariance-based adversarial examples than their undefended counterparts.\n  Excessive invariance is not limited to models trained to be robust to\nperturbation-based $\\ell_p$-norm adversaries. In fact, we argue that the term\nadversarial example is used to capture a series of model limitations, some of\nwhich may not have been discovered yet. Accordingly, we call for a set of\nprecise definitions that taxonomize and address each of these shortcomings in\nlearning."},{"date":"2019-03","title":"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition","author":"Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, and Colin Raffel","link":"http://arxiv.org/abs/1903.10346v2","abstract":"Adversarial examples are inputs to machine learning models designed by an\nadversary to cause an incorrect output. So far, adversarial examples have been\nstudied most extensively in the image domain. In this domain, adversarial\nexamples can be constructed by imperceptibly modifying images to cause\nmisclassification, and are practical in the physical world. In contrast,\ncurrent targeted adversarial examples applied to speech recognition systems\nhave neither of these properties: humans can easily identify the adversarial\nperturbations, and they are not effective when played over-the-air. This paper\nmakes advances on both of these fronts. First, we develop effectively\nimperceptible audio adversarial examples (verified through a human study) by\nleveraging the psychoacoustic principle of auditory masking, while retaining\n100% targeted success rate on arbitrary full-sentence targets. Next, we make\nprogress towards physical-world over-the-air audio adversarial examples by\nconstructing perturbations which remain effective even after applying realistic\nsimulated environmental distortions."},{"date":"2019-02","title":"On Evaluating Adversarial Robustness","author":"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin","link":"http://arxiv.org/abs/1902.06705v2","abstract":"Correctly evaluating defenses against adversarial examples has proven to be\nextremely difficult. Despite the significant amount of recent work attempting\nto design defenses that withstand adaptive attacks, few have succeeded; most\npapers that propose defenses are quickly shown to be incorrect.\n  We believe a large contributing factor is the difficulty of performing\nsecurity evaluations. In this paper, we discuss the methodological foundations,\nreview commonly accepted best practices, and suggest new methods for evaluating\ndefenses to adversarial examples. We hope that both researchers developing\ndefenses as well as readers and reviewers who wish to understand the\ncompleteness of an evaluation consider our advice in order to avoid common\npitfalls."},{"date":"2019-02","title":"Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?","author":"Nicholas Carlini","link":"http://arxiv.org/abs/1902.02322v1","abstract":"No."},{"date":"2019-01","title":"Adversarial Examples Are a Natural Consequence of Test Error in Noise","author":"Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk","link":"http://arxiv.org/abs/1901.10513v1","abstract":"Over the last few years, the phenomenon of adversarial examples ---\nmaliciously constructed inputs that fool trained machine learning models ---\nhas captured the attention of the research community, especially when the\nadversary is restricted to small modifications of a correctly handled input.\nLess surprisingly, image classifiers also lack human-level performance on\nrandomly corrupted images, such as images with additive Gaussian noise. In this\npaper we provide both empirical and theoretical evidence that these are two\nmanifestations of the same underlying phenomenon, establishing close\nconnections between the adversarial robustness and corruption robustness\nresearch programs. This suggests that improving adversarial robustness should\ngo hand in hand with improving performance in the presence of more general and\nrealistic image corruptions. Based on our results we recommend that future\nadversarial defenses consider evaluating the robustness of their methods to\ndistributional shift with benchmarks such as Imagenet-C."},{"date":"2018-12","title":"The Hitchhiker guide to: Secant Varieties and Tensor Decomposition","author":"Alessandra Bernardi, Enrico Carlini, Maria Virginia Catalisano, Alessandro Gimigliano, and Alessandro Oneto","link":"http://arxiv.org/abs/1812.10267v1","abstract":"We consider here the problem, which is quite classical in Algebraic geometry,\nof studying the secant varieties of a projective variety $X$. The case we\nconcentrate on is when $X$ is a Veronese variety, a Grassmannian or a Segre\nvariety. Not only these varieties are among the ones that have been most\nclassically studied, but a strong motivation in taking them into consideration\nis the fact that they parameterize, respectively, symmetric, skew-symmetric and\ngeneral tensors, which are decomposable, and their secant varieties give a\nstratification of tensors via tensor rank. We collect here most of the known\nresults and the open problems on this fascinating subject."},{"date":"2018-09","title":"Unrestricted Adversarial Examples","author":"Tom B. Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Goodfellow","link":"http://arxiv.org/abs/1809.08352v1","abstract":"We introduce a two-player contest for evaluating the safety and robustness of\nmachine learning systems, with a large prize pool. Unlike most prior work in ML\nrobustness, which studies norm-constrained adversaries, we shift our focus to\nunconstrained adversaries. Defenders submit machine learning models, and try to\nachieve high accuracy and coverage on non-adversarial data while making no\nconfident mistakes on adversarial inputs. Attackers try to subvert defenses by\nfinding arbitrary unambiguous inputs where the model assigns an incorrect label\nwith high confidence. We propose a simple unambiguous dataset (\"bird-or-\nbicycle\") to use as part of this contest. We hope this contest will help to\nmore comprehensively evaluate the worst-case adversarial risk of machine\nlearning models."},{"date":"2018-07","title":"Variational approach to the optimal control of coherently driven, open quantum system dynamics","author":"Vasco Cavina, Andrea Mari, Alberto Carlini, and Vittorio Giovannetti","link":"http://arxiv.org/abs/1807.07450v2","abstract":"Quantum coherence inherently affects the dynamics and the performances of a\nquantum machine. Coherent control can, at least in principle, enhance the work\nextraction and boost the velocity of evolution in an open quantum system. Using\nadvanced tools from the calculus of variations and reformulating the control\nproblem in the instantaneous Hamiltonian eigenframe, we develop a general\ntechnique for minimizing a wide class of cost functionals when the external\ncontrol has access to full rotations of the system Hamiltonian. The method is\nthen applied both to time and heat loss minimization problems and explicitly\nsolved in the case of a two level system in contact with either bosonic or\nfermionic thermal environments."},{"date":"2018-04","title":"Hilbert functions of schemes of double and reduced points","author":"Enrico Carlini, Maria Virginia Catalisano, Elena Guardo, and Adam Van Tuyl","link":"http://arxiv.org/abs/1804.10277v3","abstract":"It remains an open problem to classify the Hilbert functions of double points\nin $\\mathbb{P}^2$. Given a valid Hilbert function $H$ of a zero-dimensional\nscheme in $\\mathbb{P}^2$, we show how to construct a set of fat points $Z\n\\subseteq \\mathbb{P}^2$ of double and reduced points such that $H_Z$, the\nHilbert function of $Z$, is the same as $H$. In other words, we show that any\nvalid Hilbert function $H$ of a zero-dimensional scheme is the Hilbert function\nof a set of a positive number of double points and some reduced points. For\nsome families of valid Hilbert functions, we are also able to show that $H$ is\nthe Hilbert function of only double points. In addition, we give necessary and\nsufficient conditions for the Hilbert function of a scheme of a double points,\nor double points plus one additional reduced point, to be the Hilbert function\nof points with support on a star configuration of lines."},{"date":"2018-04","title":"On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses","author":"Anish Athalye, and Nicholas Carlini","link":"http://arxiv.org/abs/1804.03286v1","abstract":"Neural networks are known to be vulnerable to adversarial examples. In this\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\nfind they are ineffective: when applying existing techniques, we can reduce the\naccuracy of the defended models to 0%."},{"date":"2018-02","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks","author":"Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song","link":"http://arxiv.org/abs/1802.08232v3","abstract":"This paper describes a testing methodology for quantitatively assessing the\nrisk that rare or unique training-data sequences are unintentionally memorized\nby generative sequence models---a common type of machine-learning model.\nBecause such models are sometimes trained on sensitive data (e.g., the text of\nusers' private messages), this methodology can benefit privacy by allowing\ndeep-learning practitioners to select means of training that minimize such\nmemorization.\n  In experiments, we show that unintended memorization is a persistent,\nhard-to-avoid issue that can have serious consequences. Specifically, for\nmodels trained without consideration of memorization, we describe new,\nefficient procedures that can extract unique, secret sequences, such as credit\ncard numbers. We show that our testing strategy is a practical and easy-to-use\nfirst line of defense, e.g., by describing its application to quantitatively\nlimit data exposure in Google's Smart Compose, a commercial text-completion\nneural network trained on millions of users' email messages."},{"date":"2018-02","title":"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples","author":"Anish Athalye, Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1802.00420v4","abstract":"We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers."},{"date":"2018-01","title":"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text","author":"Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1801.01944v2","abstract":"We construct targeted audio adversarial examples on automatic speech\nrecognition. Given any audio waveform, we can produce another that is over\n99.9% similar, but transcribes as any phrase we choose (recognizing up to 50\ncharacters per second of audio). We apply our white-box iterative\noptimization-based attack to Mozilla's implementation DeepSpeech end-to-end,\nand show it has a 100% success rate. The feasibility of this attack introduce a\nnew domain to study adversarial examples."},{"date":"2017-11","title":"MagNet and \"Efficient Defenses Against Adversarial Attacks\" are Not Robust to Adversarial Examples","author":"Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1711.08478v1","abstract":"MagNet and \"Efficient Defenses...\" were recently proposed as a defense to\nadversarial examples. We find that we can construct adversarial examples that\ndefeat these defenses with only a slight increase in distortion."},{"date":"2017-09","title":"Provably Minimally-Distorted Adversarial Examples","author":"Nicholas Carlini, Guy Katz, Clark Barrett, and David L. Dill","link":"http://arxiv.org/abs/1709.10207v2","abstract":"The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2."},{"date":"2017-06","title":"Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong","author":"Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song","link":"http://arxiv.org/abs/1706.04701v1","abstract":"Ongoing research has proposed several methods to defend neural networks\nagainst adversarial examples, many of which researchers have shown to be\nineffective. We ask whether a strong defense can be created by combining\nmultiple (possibly weak) defenses. To answer this question, we study three\ndefenses that follow this approach. Two of these are recently proposed defenses\nthat intentionally combine components designed to work well together. A third\ndefense combines three independent defenses. For all the components of these\ndefenses and the combined defenses themselves, we show that an adaptive\nadversary can create adversarial examples successfully with low distortion.\nThus, our work implies that ensemble of weak defenses is not sufficient to\nprovide strong defense against adversarial examples."},{"date":"2017-05","title":"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods","author":"Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1705.07263v2","abstract":"Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses."},{"date":"2017-04","title":"The Monte Carlo simulation of the Borexino detector","author":"M. Agostini, K. Altenmuller, S. Appel, V. Atroshchenko, Z. Bagdasarian, D. Basilico, G. Bellini, J. Benziger, D. Bick, G. Bonfini, L. Borodikhina, D. Bravo, B. Caccianiga, F. Calaprice, A. Caminata, S. Caprioli, M. Carlini, P. Cavalcante, A. Chepurnov, K. Choi, D. D'Angelo, S. Davini, A. Derbin, X. F. Ding, L. Di Noto, I. Drachnev, K. Fomenko, A. Formozov, D. Franco, F. Froborg, F. Gabriele, C. Galbiati, C. Ghiano, M. Giammarchi, M. Goeger-Neff, A. Goretti, M. Gromov, C. Hagner, T. Houdy, E. Hungerford, Aldo Ianni, Andrea Ianni, A. Jany, D. Jeschke, V. Kobychev, D. Korablev, G. Korga, D. Kryn, M. Laubenstein, E. Litvinovich, F. Lombardi, P. Lombardi, L. Ludhova, G. Lukyanchenko, I. Machulin, G. Manuzio, S. Marcocci, J. Martyn, E. Meroni, M. Meyer, L. Miramonti, M. Misiaszek, V. Muratova, B. Neumair, L. Oberauer, B. Opitz, F. Ortica, M. Pallavicini, L. Papp, A. Pocar, G. Ranucci, A. Re, A. Romani, R. Roncin, N. Rossi, S. Schonert, D. Semenov, P. Shakina, M. Skorokhvatov, O. Smirnov, A. Sotnikov, L. F. F. Stokes, Y. Suvorov, R. Tartaglia, G. Testera, J. Thurn, M. Toropova, E. Unzhakov, A. Vishneva, R. B. Vogelaar, F. von Feilitzsch, H. Wang, S. Weinz, M. Wojcik, M. Wurm, Z. Yokley, O. Zaimidoroga, S. Zavatarelli, K. Zuber, and G. Zuzel","link":"http://arxiv.org/abs/1704.02291v1","abstract":"We describe the Monte Carlo (MC) simulation package of the Borexino detector\nand discuss the agreement of its output with data. The Borexino MC 'ab initio'\nsimulates the energy loss of particles in all detector components and generates\nthe resulting scintillation photons and their propagation within the liquid\nscintillator volume. The simulation accounts for absorption, reemission, and\nscattering of the optical photons and tracks them until they either are\nabsorbed or reach the photocathode of one of the photomultiplier tubes. Photon\ndetection is followed by a comprehensive simulation of the readout electronics\nresponse. The algorithm proceeds with a detailed simulation of the electronics\nchain. The MC is tuned using data collected with radioactive calibration\nsources deployed inside and around the scintillator volume. The simulation\nreproduces the energy response of the detector, its uniformity within the\nfiducial scintillator volume relevant to neutrino physics, and the time\ndistribution of detected photons to better than 1% between 100 keV and several\nMeV. The techniques developed to simulate the Borexino detector and their level\nof refinement are of possible interest to the neutrino community, especially\nfor current and future large-volume liquid scintillator experiments such as\nKamland-Zen, SNO+, and Juno."},{"date":"2016-10","title":"Technical Report on the CleverHans v2.1.0 Adversarial Examples Library","author":"Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, Rujun Long, and Patrick McDaniel","link":"http://arxiv.org/abs/1610.00768v6","abstract":"CleverHans is a software library that provides standardized reference\nimplementations of adversarial example construction techniques and adversarial\ntraining. The library may be used to develop more robust machine learning\nmodels and to provide standardized benchmarks of models' performance in the\nadversarial setting. Benchmarks constructed without a standardized\nimplementation of adversarial example construction are not comparable to each\nother, because a good result may indicate a robust model or it may merely\nindicate a weak implementation of the adversarial example construction\nprocedure.\n  This technical report is structured as follows. Section 1 provides an\noverview of adversarial examples in machine learning and of the CleverHans\nsoftware. Section 2 presents the core functionalities of the library: namely\nthe attacks based on adversarial examples and defenses to improve the\nrobustness of machine learning models to these attacks. Section 3 describes how\nto report benchmark results using the library. Section 4 describes the\nversioning system."},{"date":"2016-08","title":"Towards Evaluating the Robustness of Neural Networks","author":"Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1608.04644v2","abstract":"Neural networks provide state-of-the-art results for most machine learning\ntasks. Unfortunately, neural networks are vulnerable to adversarial examples:\ngiven an input $x$ and any target classification $t$, it is possible to find a\nnew input $x'$ that is similar to $x$ but classified as $t$. This makes it\ndifficult to apply neural networks in security-critical areas. Defensive\ndistillation is a recently proposed approach that can take an arbitrary neural\nnetwork, and increase its robustness, reducing the success rate of current\nattacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$.\n  In this paper, we demonstrate that defensive distillation does not\nsignificantly increase the robustness of neural networks by introducing three\nnew attack algorithms that are successful on both distilled and undistilled\nneural networks with $100\\%$ probability. Our attacks are tailored to three\ndistance metrics used previously in the literature, and when compared to\nprevious adversarial example generation algorithms, our attacks are often much\nmore effective (and never worse). Furthermore, we propose using high-confidence\nadversarial examples in a simple transferability test we show can also be used\nto break defensive distillation. We hope our attacks will be used as a\nbenchmark in future defense attempts to create neural networks that resist\nadversarial examples."},{"date":"2016-07","title":"Defensive Distillation is Not Robust to Adversarial Examples","author":"Nicholas Carlini, and David Wagner","link":"http://arxiv.org/abs/1607.04311v1","abstract":"We show that defensive distillation is not secure: it is no more resistant to\ntargeted misclassification attacks than unprotected neural networks."},{"date":"2015-12","title":"Real and complex Waring rank of reducible cubic forms","author":"Enrico Carlini, Cheng Guo, and Emanuele Ventura","link":"http://arxiv.org/abs/1512.04905v1","abstract":"In this paper, we study the real and the complex Waring rank of reducible\ncubic forms. In particular, we compute the complex rank of all reducible cubic\nforms. In the real case, for all reducible cubics, we either compute or bound\nthe real rank depending on the signature of the degree two factor."},{"date":"2014-11","title":"The MOLLER Experiment: An Ultra-Precise Measurement of the Weak Mixing Angle Using M\u00f8ller Scattering","author":"MOLLER Collaboration, J. Benesch, P. Brindza, R. D. Carlini, J-P. Chen, E. Chudakov, S. Covrig, M. M. Dalton, A. Deur, D. Gaskell, A. Gavalya, J. Gomez, D. W. Higinbotham, C. Keppel, D. Meekins, R. Michaels, B. Moffit, Y. Roblin, R. Suleiman, R. Wines, B. Wojtsekhowski, G. Cates, D. Crabb, D. Day, K. Gnanvo, D. Keller, N. Liyanage, V. V. Nelyubin, H. Nguyen, B. Norum, K. Paschke, V. Sulkosky, J. Zhang, X. Zheng, J. Birchall, P. Blunden, M. T. W. Gericke, W. R. Falk, L. Lee, J. Mammei, S. A. Page, W. T. H. van Oers, K. Dehmelt, A. Deshpande, N. Feege, T. K. Hemmick, K. S. Kumar, T. Kutz, R. Miskimen, M. J. Ramsey-Musolf, S. Riordan, N. Hirlinger Saylor, J. Bessuille, E. Ihloff, J. Kelsey, S. Kowalski, R. Silwal, G. De Cataldo, R. De Leo, D. Di Bari, L. Lagamba, E. NappiV. Bellini, F. Mammoliti, F. Noto, M. L. Sperduto, C. M. Sutera, P. Cole, T. A. Forest, M. Khandekar, D. McNulty, K. Aulenbacher, S. Baunack, F. Maas, V. Tioukine, R. Gilman, K. Myers, R. Ransome, A. Tadepalli, R. Beniniwattha, R. Holmes, P. Souder, D. S. Armstrong, T. D. Averett, W. Deconinck, W. Duvall, A. Lee, M. L. Pitt, J. A. Dunne, D. Dutta, L. El Fassi, F. De Persio, F. Meddi, G. M. Urciuoli, E. Cisbani, C. Fanelli, F. Garibaldi, K. Johnston, N. Simicevic, S. Wells, P. M. King, J. Roche, J. Arrington, P. E. Reimer, G. Franklin, B. Quinn, A. Ahmidouch, S. Danagoulian, O. Glamazdin, R. Pomatsalyuk, R. Mammei, J. W. Martin, T. Holmstrom, J. Erler, Yu. G. Kolomensky, J. Napolitano, K. A. Aniol, W. D. Ramsay, E. Korkmaz, D. T. Spayde, F. Benmokhtar, A. Del Dotto, R. Perrino, S. Barkanova, A. Aleksejevs, and J. Singh","link":"http://arxiv.org/abs/1411.4088v2","abstract":"The physics case and an experimental overview of the MOLLER (Measurement Of a\nLepton Lepton Electroweak Reaction) experiment at the 12 GeV upgraded Jefferson\nLab are presented. A highlight of the Fundamental Symmetries subfield of the\n2007 NSAC Long Range Plan was the SLAC E158 measurement of the parity-violating\nasymmetry $A_{PV}$ in polarized electron-electron (M{\\o}ller) scattering. The\nproposed MOLLER experiment will improve on this result by a factor of five,\nyielding the most precise measurement of the weak mixing angle at low or high\nenergy anticipated over the next decade. This new result would be sensitive to\nthe interference of the electromagnetic amplitude with new neutral current\namplitudes as weak as $\\sim 10^{-3}\\cdot G_F$ from as yet undiscovered dynamics\nbeyond the Standard Model. The resulting discovery reach is unmatched by any\nproposed experiment measuring a flavor- and CP-conserving process over the next\ndecade, and yields a unique window to new physics at MeV and multi-TeV scales,\ncomplementary to direct searches at high energy colliders such as the Large\nHadron Collider (LHC). The experiment takes advantage of the unique opportunity\nprovided by the upgraded electron beam energy, luminosity, and stability at\nJefferson Laboratory and the extensive experience accumulated in the community\nafter a round of recent successfully completed parity-violating electron\nscattering experiments"},{"date":"2014-05","title":"Progress on the symmetric Strassen conjecture","author":"Enrico Carlini, Maria Virginia Catalisano, and Luca Chiantini","link":"http://arxiv.org/abs/1405.3721v1","abstract":"Let F and G be homogeneous polynomials in disjoint sets of variables. We\nprove that the Waring rank is additive, thus proving the symmetric Strassen\nconjecture, when either F or G is a power, or F and G have two variables, or\neither F or G has small rank."},{"date":"2013-09","title":"Toric ideals with linear components: an algebraic interpretation of clustering the cells of a contingency table","author":"Enrico Carlini, and Fabio Rapallo","link":"http://arxiv.org/abs/1309.7622v1","abstract":"In this paper we show that the agglomeration of rows or columns of a\ncontingency table with a hierarchical clustering algorithm yields statistical\nmodels defined through toric ideals. In particular, starting from the classical\nindependence model, the agglomeration process adds a linear part to the toric\nideal generated by the $2 \\times 2$ minors."},{"date":"2013-07","title":"Speeding up and slowing down the relaxation of a qubit by optimal control","author":"Victor Mukherjee, Alberto Carlini, Andrea Mari, Tommaso Caneva, Simone Montangero, Tommaso Calarco, Rosario Fazio, and Vittorio Giovannetti","link":"http://arxiv.org/abs/1307.7964v1","abstract":"We consider a two-level quantum system prepared in an arbitrary initial state\nand relaxing to a steady state due to the action of a Markovian dissipative\nchannel. We study how optimal control can be used for speeding up or slowing\ndown the relaxation towards the fixed point of the dynamics. We analytically\nderive the optimal relaxation times for different quantum channels in the ideal\nansatz of unconstrained quantum control (a magnetic field of infinite\nstrength). We also analyze the situation in which the control Hamiltonian is\nbounded by a finite threshold. As byproducts of our analysis we find that: (i)\nif the qubit is initially in a thermal state hotter than the environmental\nbath, quantum control cannot speed up its natural cooling rate; (ii) if the\nqubit is initially in a thermal state colder than the bath, it can reach the\nfixed point of the dynamics in finite time if a strong control field is\napplied; (iii) in the presence of unconstrained quantum control it is possible\nto keep the evolved state indefinitely and arbitrarily close to special initial\nstates which are far away from the fixed points of the dynamics."},{"date":"2011-03","title":"The G0 Experiment: Apparatus for Parity-Violating Electron Scattering Measurements at Forward and Backward Angles","author":"G0 Collaboration, D. Androic, D. S. Armstrong, J. Arvieux, R. Asaturyan, T. D. Averett, S. L. Bailey, G. Batigne, D. H. Beck, E. J. Beise, J. Benesch, F. Benmokhtar, L. Bimbot, J. Birchall, A. Biselli, P. Bosted, H. Breuer, P. Brindza, C. L. Capuano, R. D. Carlini, R. Carr, N. Chant, Y. -C. Chao, R. Clark, A. Coppens, S. D. Covrig, A. Cowley, D. Dale, C. A. Davis, C. Ellis, W. R. Falk, H. Fenker, J. M. Finn, T. Forest, G. Franklin, R. Frascaria, C. Furget, D. Gaskell, M. T. W. Gericke, J. Grames, K. A. Griffioen, K. Grimm, G. Guillard, B. Guillon, H. Guler, K. Gustafsson, L. Hannelius, J. Hansknecht, R. D. Hasty, A. M. Hawthorne Allen, T. Horn, T. M. Ito, K. Johnston, M. Jones, P. Kammel, R. Kazimi, P. M. King, A. Kolarkar, E. Korkmaz, W. Korsch, S. Kox, J. Kuhn, J. Lachniet, R. Laszewski, L. Lee, J. Lenoble, E. Liatard, J. Liu, A. Lung, G. A. MacLachlan, J. Mammei, D. Marchand, J. W. Martin, D. J. Mack, K. W. McFarlane, D. W. McKee, R. D. McKeown, F. Merchez, M. Mihovilovic, A. Micherdzinska, H. Mkrtchyan, B. Moffit, M. Morlet, M. Muether, J. Musson, K. Nakahara, R. Neveling, S. Niccolai, D. Nilsson, S. Ong, S. A. Page, V. Papavassiliou, S. F. Pate, S. K. Phillips, P. Pillot, M. L. Pitt, M. Poelker, T. A. Porcelli, G. Quemener, B. P. Quinn, W. D. Ramsay, A. W. Rauf, J. -S. Real, T. Ries, J. Roche P. Roos, G. A. Rutledge, J. Schaub, J. Secrest, T. Seva, N. Simicevic, G. R. Smith, D. T. Spayde, S. Stepanyan, M. Stutzman, R. Suleiman, V. Tadevosyan, R. Tieulent, J. van de Wiele, W. T. H. van Oers, M. Versteegen, E. Voutier, W. F. Vulcan, S. P. Wells, G. Warren, S. E. Williamson, R. J. Woo, S. A. Wood, C. Yan, J. Yun, and V. Zeps","link":"http://arxiv.org/abs/1103.0761v1","abstract":"In the G0 experiment, performed at Jefferson Lab, the parity-violating\nelastic scattering of electrons from protons and quasi-elastic scattering from\ndeuterons is measured in order to determine the neutral weak currents of the\nnucleon. Asymmetries as small as 1 part per million in the scattering of a\npolarized electron beam are determined using a dedicated apparatus. It consists\nof specialized beam-monitoring and control systems, a cryogenic hydrogen (or\ndeuterium) target, and a superconducting, toroidal magnetic spectrometer\nequipped with plastic scintillation and aerogel Cerenkov detectors, as well as\nfast readout electronics for the measurement of individual events. The overall\ndesign and performance of this experimental system is discussed."},{"date":"2005-07","title":"The Geometry of Statistical Models for Two-Way Contingency Tables with Fixed Odds Ratios","author":"Enrico Carlini, and Fabio Rapallo","link":"http://arxiv.org/abs/math/0507529v2","abstract":"We study the geometric structure of the statistical models for two-by-two\ncontingency tables. One or two odds ratios are fixed and the corresponding\nmodels are shown to be a portion of a ruled quadratic surface or a segment.\nSome pointers to the general case of two-way contingency tables are also given\nand an application to case-control studies is presented."},{"date":"2002-02","title":"Quantum learning and universal quantum matching machine","author":"Masahide Sasaki, and Alberto Carlini","link":"http://arxiv.org/abs/quant-ph/0202173v1","abstract":"Suppose that three kinds of quantum systems are given in some unknown states\n$\\ket f^{\\otimes N}$, $\\ket{g_1}^{\\otimes K}$, and $\\ket{g_2}^{\\otimes K}$, and\nwe want to decide which \\textit{template} state $\\ket{g_1}$ or $\\ket{g_2}$,\neach representing the feature of the pattern class ${\\cal C}_1$ or ${\\cal\nC}_2$, respectively, is closest to the input \\textit{feature} state $\\ket f$.\nThis is an extension of the pattern matching problem into the quantum domain.\nAssuming that these states are known a priori to belong to a certain parametric\nfamily of pure qubit systems, we derive two kinds of matching strategies. The\nfirst is a semiclassical strategy which is obtained by the natural extension of\nconventional matching strategies and consists of a two-stage procedure:\nidentification (estimation) of the unknown template states to design the\nclassifier (\\textit{learning} process to train the classifier) and\nclassification of the input system into the appropriate pattern class based on\nthe estimated results. The other is a fully quantum strategy without any\nintermediate measurement which we might call as the {\\it universal quantum\nmatching machine}. We present the Bayes optimal solutions for both strategies\nin the case of K=1, showing that there certainly exists a fully quantum\nmatching procedure which is strictly superior to the straightforward\nsemiclassical extension of the conventional matching strategy based on the\nlearning process."},{"date":"2001-02","title":"x- and xi-scaling of the Nuclear Structure Function at Large x","author":"J. Arrington, C. S. Armstrong, T. Averett, O. K. Baker, L. de Bever, C. W. Bochna, W. Boeglin, B. Bray, R. D. Carlini, G. Collins, C. Cothran, D. Crabb, D. Day, J. A. Dunne, D. Dutta, R. Ent, B. W. Filippone, A. Honegger, E. W. Hughes, J. Jensen, J. Jourdan, C. E. Keppel, D. M. Koltenuk, R. Lindgren, A. Lung, D. J Mack, J. McCarthy, R. D. McKeown, D. Meekins, J. H. Mitchell, H. G. Mkrtchyan, G. Niculescu, I. Niculescu, T. Petitjean, O. Rondon, I. Sick, C. Smith, B. Terburg, W. F. Vulcan, S. A. Wood, C. Yan, J. Zhao, and B. Zihlmann","link":"http://arxiv.org/abs/nucl-ex/0102004v1","abstract":"Inclusive electron scattering data are presented for ^2H and Fe targets at an\nincident electron energy of 4.045 GeV for a range of momentum transfers from\nQ^2 = 1 to 7 (GeV/c)^2. Data were taken at Jefferson Laboratory for low values\nof energy loss, corresponding to values of Bjorken x greater than or near 1.\nThe structure functions do not show scaling in x in this range, where inelastic\nscattering is not expected to dominate the cross section. The data do show\nscaling, however, in the Nachtmann variable \\xi. This scaling may be the result\nof Bloom Gilman duality in the nucleon structure function combined with the\nFermi motion of the nucleons in the nucleus. The resulting extension of scaling\nto larger values of \\xi opens up the possibility of accessing nuclear structure\nfunctions in the high-x region at lower values of Q^2 than previously believed."},{"date":"2000-11","title":"Optimal phase estimation and square root measurement","author":"M. Sasaki, A. Carlini, and A. Chefles","link":"http://arxiv.org/abs/quant-ph/0011057v1","abstract":"We present an optimal strategy having finite outcomes for estimating a single\nparameter of the displacement operator on an arbitrary finite dimensional\nsystem using a finite number of identical samples. Assuming the uniform {\\it a\npriori} distribution for the displacement parameter, an optimal strategy can be\nconstructed by making the {\\it square root measurement} based on uniformly\ndistributed sample points. This type of measurement automatically ensures the\nglobal maximality of the figure of merit, that is, the so called average score\nor fidelity. Quantum circuit implementations for the optimal strategies are\nprovided in the case of a two dimensional system."},{"date":"1994-06","title":"Fundamental Constants and the Problem of Time","author":"A. Carlini, and J. Greensite","link":"http://arxiv.org/abs/gr-qc/9406044v1","abstract":"We point out that for a large class of parametrized theories, there is a\nconstant in the constrained Hamiltonian which drops out of the classical\nequations of motion in configuration space. Examples include the mass of a\nrelativistic particle in free fall, the tension of the Nambu string, and\nNewton's constant for the case of pure gravity uncoupled to matter or other\nfields. In the general case, the classically irrelevant constant is\nproportional to the ratio of the kinetic and potential terms in the\nHamiltonian. It is shown that this ratio can be reinterpreted as an {\\it\nunconstrained} Hamiltonian, which generates the usual classical equations of\nmotion. At the quantum level, this immediately suggests a resolution of the\n\"problem of time\" in quantum gravity. We then make contact with a recently\nproposed transfer matrix formulation of quantum gravity and discuss the\nsemiclassical limit. In this formulation, it is argued that a physical state\ncan obey a (generalized) Poincar\\'e algebra of constraints, and still be an\napproximate eigenstate of 3-geometry. Solutions of the quantum evolution\nequations for certain minisuperspace examples are presented. An implication of\nour proposal is the existence of a small, inherent uncertainty in the\nphenomenological value of Planck's constant."}]