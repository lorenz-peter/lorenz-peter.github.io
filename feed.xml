<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lorenz-peter.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lorenz-peter.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-15T15:34:00+00:00</updated><id>https://lorenz-peter.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal Website </subtitle><entry><title type="html">What your Statistics Professor did not tell you about Hypothesis Testing and how you can use it for…</title><link href="https://lorenz-peter.github.io/blog/2025/what-your-statistics-professor-did-not-tell-you-about-hypothesis-testing-and-how-you-can-use-it-for/" rel="alternate" type="text/html" title="What your Statistics Professor did not tell you about Hypothesis Testing and how you can use it for…"/><published>2025-10-14T19:20:12+00:00</published><updated>2025-10-14T19:20:12+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/what-your-statistics-professor-did-not-tell-you-about-hypothesis-testing-and-how-you-can-use-it-for</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/what-your-statistics-professor-did-not-tell-you-about-hypothesis-testing-and-how-you-can-use-it-for/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Proposal AI infernece + Agentic AI</title><link href="https://lorenz-peter.github.io/blog/2025/ai_inference_agentic/" rel="alternate" type="text/html" title="Proposal AI infernece + Agentic AI"/><published>2025-08-24T16:40:16+00:00</published><updated>2025-08-24T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/ai_inference_agentic</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/ai_inference_agentic/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Agentic AI is a term describing that an AI is considered as a whole system and can think and act in an environment autonomously. One prominent example is that Agentic AI is an evolution of conversational AI, enabling systems to act more autonomously and proactively. While traditional conversational AI focuses on natural language interactions, agentic AI adds the capability for systems to set goals, make decisions, and take actions to achieve those goals with limited human intervention.</p> <p>One essential component of these AI systems is Large language models (LLMs). LLMs are usually in the backend, which needs to be trained (pre-trained) and then tuned to a specific task (post-training) [5]. Pre-training and post-training take a lot of effort in data and training, and computing power, and as a result, the models become larger and larger.</p> <p>In 2024, Snell et al. [1] stated that “Scaling LLM Test-Time to Compute Optimally Can Be More Effective than Scaling Model Parameters”. Brown et al. [2] went a step further: At inference, a model should make more than one attempt at a problem. Similarly, Hao et al. [3] introduced “COCONUT” (Chain of Continuous Thought).</p> <p><img src="assets/proposal_inference/image.png" alt="image"/></p> <p>Fig.: Inference will become increasingly important in the future. At inference (aka test-time), the model could make many decisions that have not been discovered. (Image source: https://upaspro.com/inference-time-scaling-vs-training-compute)</p> <p>Recently, a first attempt at AI safety assessment has been evaluated [4] on closed-source models. They also proposed a reinforcement-based approach to mitigate safety issues. For mitigating AI safety issues can be mitigated with post-training [5]. [7] states that expanding reasoning with safety improves generalization. [8] found out that increasing inference-time computation improves the adversarial robustness of reasoning LLMs in many cases in terms of reducing the rate of successful attacks.</p> <h2 id="research-question">Research Question</h2> <p>What is the AI safety risk in inference scaling in Agentic AI? E.g., mistakes. How can those be mitigated? How can AI be human-aligned? Research Aims and Objectives Investigate current methods (post-training and at inference): Evaluate them and generate a dataset, if needed. Develop an evaluation environment, e.g., extend AgentBench. Based on the evaluations and benchmarks, develop your method to mitigate AI safety issues. (Probably too futuristic: evaluate and align multi-agent systems. Agent A does not have the same alignment as agent B.)</p> <h2 id="references">References</h2> <p>[1] Snell C, Lee J, Xu K, Kumar A. Scaling LLM test-time to compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. 2024 Aug 6. [2] Brown B, Juravsky J, Ehrlich R, Clark R, Le QV, Ré C, Mirhoseini A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. 2024 Jul 31. [3] Hao S, Sukhbaatar S, Su D, Li X, Hu Z, Weston J, Tian Y. Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769. 2024 Dec 9. [4] Qiu R, Li G, Wei T, He J, Tong H. Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance. arXiv preprint arXiv:2506.06444. 2025 Jun 6. [5] Kumar K, Ashraf T, Thawakar O, Anwer RM, Cholakkal H, Shah M, Yang MH, Torr PH, Khan FS, Khan S. Llm post-training: A deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321. 2025 Feb 28. [7] Kumarage T, Mehrabi N, Ramakrishna A, Zhao X, Zemel R, Chang KW, Galstyan A, Gupta R, Peris C. Towards safety reasoning in LLMs: AI-agentic deliberation for policy-embedded CoT data creation. arXiv preprint arXiv:2505.21784. 2025 May 27. [8] Zaremba W, Nitishinskaya E, Barak B, Lin S, Toyer S, Yu Y, Dias R, Wallace E, Xiao K, Heidecke J, Glaese A. Trading inference-time compute for adversarial robustness. arXiv preprint arXiv:2501.18841. 2025 Jan 31. [9] Lu S, Bigoulaeva I, Sachdeva R, Madabushi HT, Gurevych I. Are emergent abilities in large language models just in-context learning?. arXiv preprint arXiv:2309.01809. 2023 Sep 4.</p>]]></content><author><name></name></author><category term="tech"/><category term="inference,"/><category term="agentic"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">WizardMath - LLM reasnoning for math problem solving</title><link href="https://lorenz-peter.github.io/blog/2025/wizardmath/" rel="alternate" type="text/html" title="WizardMath - LLM reasnoning for math problem solving"/><published>2025-08-07T16:40:16+00:00</published><updated>2025-08-07T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/wizardmath</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/wizardmath/"><![CDATA[<p>Paper Review</p> <p><strong>Table of Contents</strong></p> <ul> <li><a href="#summary2">Summary 2</a></li> <li><a href="#analysisreport2">Analysis Report 2</a></li> <li><a href="#1context2">1. Context 2</a></li> <li><a href="#2technicalcontributionsandinnovations3">2. Technical Contributions and Innovations 3</a></li> <li><a href="#21mathevolinstructdiversifiedsyntheticdatageneration5">2.1 Math Evol-Instruct: Diversified Synthetic Data Generation 5</a></li> <li><a href="#22reinforcementlearningwithmultifacetedrewardmodels5">2.2 Reinforcement Learning with Multi-Faceted Reward Models 5</a></li> <li><a href="#23automatedandscalabledatalabeling6">2.3 Automated and Scalable Data Labeling 6</a></li> <li><a href="#3empiricalfindingsandindustryimpact6">3. Empirical Findings and Industry Impact 6</a></li> <li><a href="#31stateoftheartresults6">3.1 State-of-the-Art Results 6</a></li> <li><a href="#32crossmodelgeneralization7">3.2 Cross-Model Generalization 7</a></li> <li><a href="#33ablationandanalysis8">3.3 Ablation and Analysis 8</a></li> <li><a href="#4positionintheacademicandindustriallandscape9">4. Position in the Academic and Industrial Landscape 9</a></li> <li><a href="#41advancingopensourcellmcapabilities9">4.1 Advancing Open-Source LLM Capabilities 9</a></li> <li><a href="#42impactonaiineducationscientificcomputingandengineering9">4.2 Impact on AI in Education, Scientific Computing, and Engineering 9</a></li> <li><a href="#43broadertrendsandinsights9">4.3 Broader Trends and Insights 9</a></li> <li><a href="#5strengthslimitationsandoutlook10">5. Strengths, Limitations, and Outlook 10</a></li> <li><a href="#51strengths10">5.1 Strengths 10</a></li> <li><a href="#52limitations10">5.2 Limitations 10</a></li> <li><a href="#6conclusion10">6. Conclusion 10</a></li> <li><a href="#references11">References 11</a></li> <li><a href="#appendix11">Appendix 11</a></li> <li><a href="#a1googlescholaranalysis11">A1. Google Scholar Analysis 11</a></li> <li><a href="#a2reviewssummarization12">A2. Reviews Summarization 12</a></li> </ul> <h1 id="summary">Summary</h1> <ul> <li>Link: <a href="https://wizardlm.github.io/WizardMath">https://wizardlm.github.io/WizardMath</a></li> <li>HF: <a href="https://huggingface.co/WizardLMTeam/WizardMath-70B-V1.0">https://huggingface.co/WizardLMTeam/WizardMath-70B-V1.0</a></li> <li>Code: <a href="https://github.com/nlpxucan/WizardLM/tree/main/WizardMath">https://github.com/nlpxucan/WizardLM/tree/main/WizardMath</a></li> <li>Reviews at ICLR’25: <a href="https://openreview.net/forum?id=mMPMHWOdOy">https://openreview.net/forum?id=mMPMHWOdOy</a></li> </ul> <p><strong>WizardMath</strong> represents one of the most significant efforts in advancing the mathematical reasoning capabilities of large language models (LLMs). While proprietary models, such as GPT-4, have demonstrated strong performance across various natural language tasks, including mathematics, most open-source LLMs have lagged, particularly in complex, multistep quantitative reasoning tasks.</p> <p>This paper directly addresses that gap by introducing a comprehensive framework—<strong>Reinforcement Learning from Evol-Instruct Feedback (RLEIF)</strong>—which boosts both performance and data efficiency for mathematical problem-solving, primarily focusing on freely available model families like Llama and Mistral. <br/> The Evol-Instruct method was introduced in the WizardLM approach, and the authors extended it to the mathematical domain. They created a robust mathematical supervised fine-tuning (SFT) dataset. The introduction of new reward models—combining an Instruction Reward Model (IRM) and Process Reward Model (PRM)—for reinforcement learning in mathematics improves performance in the experiments.</p> <p>WizardMath Mistral 7B outperforms existing open-source models. The experimental results show the effectiveness of different model sizes (number of billion parameters) and WizardMath 70B even exceeds proprietary models, those as strong proprietary models such as GPT-3.5-Turbo, Claude 2, Gemini Pro, and GPT-4-early-version. <br/> It should be noted that open-source models are usually (except Llama or DeepSeekMath) not necessarily pre-trained on mathematical datasets, and how proprietary models are trained is a secret or only partially disclosed. Further, in the result Table 1 (see [1]), it should be considered that WizardMath is a model trained with both SFT and reinforcement learning (RL), whereas typically other models are not fine-tuned via RL. It would be interesting to see how the other models might also benefit comparably from RL for a more insightful evaluation. Moreover, WizardMath relies on the proprietary GPT-4 model for PRM, which makes it challenging to fully agree with the authors’ characterization of WizardMath as a fair comparison to open-source models.</p> <p>While not explicitly stated in the paper, recent trends include the use of WizardMath to enhance agentic AI, enabling autonomous problem-solving and decision-making in complex environments. This aligns with industries requiring verifiable, step-by-step reasoning for high-stakes decisions, driving progress in fields like robotics and scientific research.</p> <p>In summary, WizardMath is a milestone for open-source mathematical LLMs, demonstrating that process-supervised RL with diversified, AI-evolved math data can vault open models into (and beyond) the realm previously dominated by closed, expensive alternatives.</p> <h1 id="analysis-report">Analysis Report</h1> <h2 id="1-context">1. Context</h2> <p>Large-scale language models (LLMs) have gained considerable attention and become the preferred solution for a wide range of natural language processing (NLP) tasks, such as open-domain conversations, coding, and mathematics.</p> <p>LLMs are typically pre-trained on vast amounts of internet data and then fine-tuned using specialized instruction data and techniques. This approach enables them to achieve cutting-edge zero-shot performance across multiple benchmarks.</p> <p>This trend also gradually stimulates the releases of open-source models (Mistral, Alpaca, Vicuna, WizardLM [2]).</p> <p>Closed-source models, like those developed by OpenAI, often perform better in complex, multi-step reasoning tasks due to their more specialized training, optimizations, and proprietary techniques [3]. They tend to have access to more tailored datasets and advanced methods for fine-tuning, which can enhance their performance in areas like multi-step mathematical and scientific reasoning [3,4].</p> <p>The authors emphasize that they excluded external Python tools (such as ToRA [5], MAmmoTH [6], or OpenMathInstruct-2 [7]) from their research. ToRA, for example, focuses not only on Chain-of-Thought (CoT) reasoning [5], but also on a system with Agentic AI, where tools are provided to enhance reasoning. In contrast, WizardMath’s contribution is centered on the reasoning process itself, rather than on the system.</p> <p>In 2024, ToRA published its results and is outperforming the WizardMath results from the arXiv version of 2023:</p> <p><em>Notably, TORA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. TORACODE-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4’s CoT result, and is competitive with GPT-4 solving problems with programs.</em></p> <p>There is no direct comparison of the current WizardMath publication and ToRA, which makes the impression that WizardMath won’t be able to at the moment. WizardMath explicitly writes that they only outperform GPT-4 earlier versions.</p> <h2 id="2-technical-contributions-and-innovations">2. Technical Contributions and Innovations</h2> <p>In this section, we will discuss the evol-instruct, reinforcement learning, and automated and scalable approach.</p> <p>Note: In Appendix A2 - remarks by the reviewers: <br/> <em>While the empirical results are strong, some reviewers see the novelty as moderate because both the Evol-Instruct framework and PRM methodology existed previously—the paper “basically took Evol-Instruct and PRM and used them to train a model”.</em></p> <p>In Figure 1 (copied from [1]), the 3 steps of the WizardMath method are illustrated.</p> <p><img src="assets/wizardmath/yNZ_Image_1.png" alt="yNZ_Image_1"/></p> <p>As illustrated in Figure 1, the authors propose a novel method called Reinforcement Learning from Evol-Instruct Feedback (RLEIF). This approach aims to generate diverse math instruction data through a newly introduced framework, Math Evol-Instruct, which incorporates both downward and upward evolution processes. The downward evolution generates grade school math problems, while the upward evolution tackles more challenging high school-level math. (More in Section 2.1)</p> <p>In contrast to WizardLM [2] and WizardCoder [8], which primarily focus on the Supervised Fine-Tuning (SFT) stage and are prone to learning hallucinated information from the teacher model, the authors of this study introduce the innovative use of a Process-Reward Model (PRM). This model addresses the issue of false positives that can arise during the problem-solving process.</p> <p>To prevent the instruction evolution from becoming uncontrollable, the authors also introduce an Instruction Reward Model (IRM). The IRM is designed to evaluate the quality of the evolved instructions, while the PRM provides feedback on each reasoning step during the solution process. These two reward models are trained using existing research [9-12]. (More in Section 2.2)</p> <p>The training process begins by fine-tuning large language models (LLMs) with the evolved math data. Subsequently, GPT-4 is employed to produce a ranking order of the instructions and assess the correctness of each reasoning step. The LLMs are then optimized to incorporate this feedback into the reward models. Finally, a Step-by-Step Proximal Policy Optimization (PPO) [13] approach is used to train the model, WizardMath, ensuring it adapts to the evolving instructions while maintaining accuracy.</p> <h2 id="21-math-evol-instruct-diversified-synthetic-data-generation">2.1 Math Evol-Instruct: Diversified Synthetic Data Generation</h2> <ul> <li> <p>The <strong>Evol-Instruct</strong> [1, 2] methodology employs upward and downward evolution to create diverse and complex math problems.</p> </li> <li> <p><strong>Upward Evolution</strong> increases the complexity and constraints of existing problems, leading to more challenging questions.</p> </li> <li> <p><strong>Downward Evolution</strong> (a novel addition) simplifies problems, creating easier variants. This complements the upward approach, providing a controllable span of complexity, simulating a real educational curriculum from basic to advanced challenges.</p> </li> <li> <p><strong>Significance:</strong> This approach automates large-scale, high-quality, diverse instruction data creation, a crucial factor for training models to generalize to unfamiliar or harder tasks—a recognized bottleneck in LM development.</p> </li> <li> <p><strong>Weakness</strong>:</p> </li> <li> <p>In the related work, there could have been a discussion of an alternative, such as [15], where smaller LLMs can be used for instruction evolving.</p> </li> <li> <p>Only GPT-4 is used for Evol-Instruct, which is known for advanced reasoning abilities. It is a bit questionable if the authors can state it as an open-source approach and comparison is correct.</p> </li> </ul> <h2 id="22-reinforcement-learning-with-multi-faceted-reward-models">2.2 Reinforcement Learning with Multi-Faceted Reward Models</h2> <ul> <li> <p>RLEIF harmonizes two reward models:</p> </li> <li> <p><strong>Instruction Reward Model (IRM):</strong> Automatically assesses the <em>quality</em> of instructions regarding clarity, completeness, and difficulty. IRM is trained on the rankings provided by GPT-4.</p> </li> <li> <p><strong>Process-supervised Reward Model (PRM) **[9]</strong>:** Judges the <em>correctness</em> of each reasoning step in model-generated solutions, trained with step-level feedback (also GPT-4-labeled). (see Tables 4 and 5)</p> </li> </ul> <p><img src="wizardmath/LGV_Image_3.png" alt="Enter image alt description"/></p> <ul> <li><strong>Role:</strong> Unlike previous outcome-only reward models, PRM ensures the model does not learn to “game” the metric by producing correct answers via erroneous steps (the “false positive” problem). Both IRM and PRM improve RL alignment, leading to more reliable intermediate reasoning:</li> </ul> <p><img src="wizardmath/xho_Image_4.png" alt="Enter image alt description"/></p> <ul> <li> <p><strong>Significance:</strong> This step-by-step, process-focused alignment is rapidly becoming a technical frontier, moving beyond “does it get the final answer?” to “does it reason correctly, as a human would be expected to?”</p> </li> <li> <p><strong>Weakness:</strong></p> </li> <li> <p>Proprietary GPT-4 is used.</p> </li> </ul> <h2 id="23-automated-and-scalable-data-labeling">2.3 Automated and Scalable Data Labeling</h2> <p>By leveraging proprietary model GPT-4 for both problem evolution and annotation, the pipeline achieves full automation and scalability, avoiding costly and inconsistent manual data curation. This is especially important for math, where high-quality annotation requires significant expertise.</p> <h2 id="3-empirical-findings-and-industry-impact">3. Empirical Findings and Industry Impact</h2> <h2 id="31-state-of-the-art-results">3.1 State-of-the-Art Results</h2> <ul> <li> <p><strong>WizardMath</strong>, particularly the 70B scale models, routinely beats the best open-source alternatives (like MetaMath, MathScale, and Xwin-Math) and even previous iterations of the most popular proprietary LLMs (like GPT-3.5 and GPT-4-early) on the GSM8k and MATH benchmarks, which cover everything from elementary school math to difficult high school math. (see Table 1 in [1]) \</p> </li> <li> <p><strong>Data Efficiency:</strong></p> </li> <li> <p>WizardMath achieves higher accuracy with less synthesized data than major competitors. The evolutionary method is shown to produce more “efficient” data—learning curves demonstrate higher accuracy at a smaller data scale compared to other synthesis methods:</p> </li> </ul> <p><img src="wizardmath/dF6_Image_5.png" alt="Enter image alt description"/></p> <ul> <li>MathFusion [16] is a later approach that needs fewer samples than WizardMath (version 2023, no comparison with version 2025). More investigation in this direction for data efficiency is recommended:</li> </ul> <p><img src="wizardmath/qSU_Image_6.png" alt="Enter image alt description"/></p> <h2 id="32-cross-model-generalization">3.2 Cross-Model Generalization</h2> <ul> <li> <p>RLEIF’s improvements are robust across several backbone architectures (GPT-2, Llama, Mistral, Qwen, DeepSeek), demonstrating general applicability, not just model-specific tuning.</p> </li> <li> <p><strong>Out-of-domain Generalization:</strong> WizardMath exhibits strong performance on OOD datasets (e.g., MWPBench), indicating better “real-world” applicability.</p> </li> </ul> <p><img src="wizardmath/oQs_Image_7.png" alt="Enter image alt description"/></p> <h2 id="33-ablation-and-analysis">3.3 Ablation and Analysis</h2> <ul> <li>Both <strong>downward and upward evolution</strong> make significant, complementary contributions to performance.</li> </ul> <p><img src="wizardmath/OZj_Image_8.png" alt="Enter image alt description"/></p> <ul> <li> <p>Process supervision (PRM) and instruction quality assessment (IRM) together provide substantive RL improvements over SFT alone or SFT+RL with only one reward model.</p> </li> <li> <p>PRM labeled purely by open-source models (e.g., Llama) still achieves strong results, suggesting cost-effective alternatives to GPT-4 for future scaling.</p> </li> </ul> <p><img src="wizardmath/R4Z_Image_9.png" alt="Enter image alt description"/></p> <p><img src="wizardmath/Zmi_Image_10.png" alt="Enter image alt description"/></p> <h2 id="4-position-in-the-academic-and-industrial-landscape">4. Position in the Academic and Industrial Landscape</h2> <h2 id="41-advancing-open-source-llm-capabilities">4.1 Advancing Open-Source LLM Capabilities</h2> <ul> <li>The gap between proprietary (closed) and open-source models has had serious real-world implications for democratizing advanced AI capabilities. WizardMath provides a clear path to bridge this gap, particularly in quantitative and STEM domains, historically open-source LLMs’ achilles heel.</li> </ul> <h2 id="42-impact-on-ai-in-education-scientific-computing-and-engineering">4.2 Impact on AI in Education, Scientific Computing, and Engineering</h2> <ul> <li> <p>High-quality mathematical reasoning unlocks applications in:</p> </li> <li> <p><strong>Education:</strong> Personalized, adaptive tutoring and automated grading.</p> </li> <li> <p><strong>Scientific Research:</strong> Automated theorem proving, scientific literature understanding.</p> </li> <li> <p><strong>Engineering:</strong> Automated verification, technical documentation, and modeling assistance.</p> </li> </ul> <h2 id="43-broader-trends-and-insights">4.3 Broader Trends and Insights</h2> <ul> <li> <p>The “process supervision” trend aligns with how high-stakes industries (finance, healthcare, law) require not just answers but verified, auditable, stepwise logic—WizardMath directly answers this need.</p> </li> <li> <p>The automatic, AI-driven data synthesis and reward labeling pipeline foreshadows even greater scaling and adaptation to new subdomains (e.g., physics, chemistry) as public LLMs become increasingly capable.</p> </li> <li> <p>Agentic reasoning: WizardMath’s advanced mathematical reasoning, combined with reinforcement learning, enhances agentic AI by enabling autonomous problem-solving and decision-making in complex environments. As AI agents become more capable, they will be able to adapt, optimize, and model real-world scenarios with greater efficiency, driving progress in fields like robotics, scientific research, and AI-driven innovation. This aligns with broader trends in industries requiring not only accurate answers but also verifiable, step-by-step reasoning to ensure reliable, high-stakes decision-making.</p> </li> </ul> <h2 id="5-strengths-limitations-and-outlook">5. Strengths, Limitations, and Outlook</h2> <h2 id="51-strengths">5.1 Strengths</h2> <ul> <li> <p><strong>Unified, robust training pipeline:</strong> From data synthesis to multi-level RL rewards—all modular, scalable, and largely automated.</p> </li> <li> <p><strong>Significant, well-documented gains:</strong> Comprehensive benchmarking across multiple datasets and model backbones.</p> </li> <li> <p><strong>Ablation and contamination control</strong>: The study carefully looks at leakage, duplication, and simple connections.</p> </li> </ul> <h2 id="52-limitations">5.2 Limitations</h2> <ul> <li> <p>Heavy reliance on GPT-4 for initial data evolution and annotation, though partially mitigated by later open-source experiments.</p> </li> <li> <p>Focus is primarily on core mathematical reasoning—extension to truly symbolic logic (full theorem proving, proof assistants) is not covered.</p> </li> <li> <p>Some reliance on synthetic data means caution must be used when deploying in areas requiring deeper domain knowledge (university-level, research mathematics), though early generalization results are promising.</p> </li> <li> <p>Multimodality. This approach can be easily extended to multimodality. Evol-Instruct has already shown capable results [14].</p> </li> </ul> <h2 id="6-conclusion">6. Conclusion</h2> <p>WizardMath represents a significant advancement in the domain of open-source mathematical large language models (LLMs). It illustrates that process-supervised reinforcement learning (RL) combined with a diverse, AI-generated mathematical data set can propel open models to achieve, and even exceed, the performance levels traditionally seen in closed, highly costly alternatives. Its exceptional performance across core benchmarks and generalization tasks, alongside a robust and scalable training pipeline, underscores both novel scientific insights and technical expertise, thereby setting a new benchmark within the field.</p> <p>In the rapidly evolving AI landscape, the framework introduced by WizardMath not only pushes the boundaries of the current state-of-the-art but also establishes critical paradigms—such as process-based supervision, data-driven evolution, and automated reward modeling. These are poised to serve as foundational elements for future developments in language, mathematics, and broader scientific disciplines.</p> <h1 id="references">References</h1> <p>[1] WizardMath: <a href="https://openreview.net/forum?id=mMPMHWOdOy">https://openreview.net/forum?id=mMPMHWOdOy</a> <br/> [2] WizardLM: <a href="https://arxiv.org/abs/2304.12244">https://arxiv.org/abs/2304.12244</a> <br/> [3] LLM Math Reasoning - Progresses and challenges: <a href="https://arxiv.org/pdf/2402.00157">https://arxiv.org/pdf/2402.00157</a> <br/> [4] On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey <a href="https://arxiv.org/pdf/2406.15126">https://arxiv.org/pdf/2406.15126</a> <br/> [5] ToRA <a href="https://github.com/microsoft/ToRA">https://github.com/microsoft/ToRA</a> <br/> [6] MAmmoth <a href="https://tiger-ai-lab.github.io/MAmmoTH">https://tiger-ai-lab.github.io/MAmmoTH</a> <br/> [7] OpenMathInstruct-2 <a href="https://arxiv.org/abs/2410.01560">https://arxiv.org/abs/2410.01560</a> <br/> [8] WizardCoder <a href="https://wizardlm.github.io/WizardCoder">https://wizardlm.github.io/WizardCoder</a> <br/> [9] Let’s verify step by step <a href="https://openreview.net/pdf?id=v8L0pN6EOi">https://openreview.net/pdf?id=v8L0pN6EOi</a> <br/> [10] Solving math word problems with process- and outcome-based feedback <a href="https://arxiv.org/abs/2211.14275">https://arxiv.org/abs/2211.14275</a> <br/> [11] Math-Shepherd <a href="https://arxiv.org/abs/2312.08935">https://arxiv.org/abs/2312.08935</a> <br/> [12] Common 7B Language Models Already Possess Strong Math Capabilities <a href="https://arxiv.org/pdf/2403.04706">https://arxiv.org/pdf/2403.04706</a> <br/> [13] PPO <a href="https://docs.pytorch.org/tutorials/intermediate/reinforcement_ppo.html">https://docs.pytorch.org/tutorials/intermediate/reinforcement_ppo.html</a> <br/> [14] MMEVOL <a href="https://aclanthology.org/2025.findings-acl.1009">https://aclanthology.org/2025.findings-acl.1009</a> <br/> [15] Smaller Language Models Are Better Instruction Evolvers <a href="https://arxiv.org/pdf/2412.11231v1">https://arxiv.org/pdf/2412.11231v1</a> <br/> [16] Mathfusion <a href="https://arxiv.org/abs/2503.16212">https://arxiv.org/abs/2503.16212</a></p> <h1 id="appendix">Appendix</h1> <h2 id="a1-google-scholar-analysis">A1. Google Scholar Analysis</h2> <p>The first version of this paper appeared on arXiv in 2023. Today, it has been cited around 512 times.</p> <p><img src="wizardmath/Bll_Image_11.png" alt="Enter image alt description"/></p> <p>Cited by: <a href="https://scholar.google.com/scholar?cites=9916633631554786614&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en">scholar.google.com/scholar?cites=9916633631554786614&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en</a></p> <p>When clicking on the recent papers in “cited by”, it can be observed that affiliations are at top-tier research institutes and industry. The mix of both implies the practical importance of this paper. Here are some examples copied from “cited by”:</p> <ul> <li> <p><a href="https://arxiv.org/pdf/2504.09037">https://arxiv.org/pdf/2504.09037</a> (Salesforce, NUS, NTU)</p> </li> <li> <p><a href="https://arxiv.org/pdf/2501.17703">https://arxiv.org/pdf/2501.17703</a> (Department of Computer Science, University of Waterloo, CMU, Vector Institute)</p> </li> <li> <p><a href="https://arxiv.org/pdf/2506.09038">https://arxiv.org/pdf/2506.09038</a> (FAIR Meta)</p> </li> <li> <p><a href="https://arxiv.org/pdf/2503.02324">https://arxiv.org/pdf/2503.02324</a> (University of Hong Kong, Ant Group)</p> </li> <li> <p><a href="https://aclanthology.org/2025.acl-long.42.pdf">https://aclanthology.org/2025.acl-long.42.pdf</a> (Tsinghua University, New York University, Tencent)</p> </li> </ul> <p>In a broader sense, Wizardmath is interesting for LLM reasoning that enables logical inference, problem-solving, and decision-making. From the system’s point of view, it is an integral part of agentic AI<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> .</p> <h2 id="a2-reviews-summarization">A2. Reviews Summarization</h2> <p>The reviews of “WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct” highlight both substantial strengths and notable weaknesses in the work.</p> <p><strong>What reviewers found good:</strong></p> <ul> <li> <p><strong>Strong empirical results:</strong> Reviewers consistently praised the paper for achieving impressive gains in mathematical reasoning, often exceeding those of strong proprietary models such as GPT-4 and Claude2. One reviewer noted that improvements from training on Math Evol-Instruct were “more than 10 points,” and that surpassing proprietary models is significant and attention-worthy. \</p> </li> <li> <p><strong>Thorough experiments and ablations:</strong> The paper presents exhaustive experiments over a wide range of model scales (from 100M to 70B parameters), using different base models, and compares against several math-specific baselines. The ablation studies and analyses are seen as solid evidence of the method’s advantages and scalability. \</p> </li> <li> <p><strong>Novel methodological components:</strong> The introduction of new reward models—an Instruction Reward Model (IRM) and Process Reward Model (PRM)—for reinforcement learning in mathematics is considered a valuable innovation. Experimentally, integrating IRM with PRM is shown to robustly improve performance. \</p> </li> <li> <p><strong>Scalability and automation:</strong> The model training pipeline is largely automated using AI, including fully automated instruction evolution and reward data generation, which reviewers noted as a scalable and adaptable feature that could potentially be applied beyond math (e.g., in coding domains). \</p> </li> <li> <p><strong>Clear practical impact:</strong> The experiments demonstrate consistent, robust improvements over open and closed alternatives, which reviewers interpret as deliverable value to the NLP and AI research community. \</p> </li> </ul> <p><strong>What reviewers found bad or problematic:</strong></p> <ul> <li> <p><strong>Unfair baseline comparisons:</strong> Multiple reviewers criticized the fairness of comparisons in the results tables. Specifically, WizardMath is a model trained with both supervised fine-tuning (SFT) and reinforcement learning (RL), whereas many baselines have only undergone SFT. They argue that other models might also benefit comparably from RL, and comparisons should be more rigorously controlled, isolating the effects of SFT and RL. \</p> </li> <li> <p><strong>Reliance on GPT-4 labeling for PRM:</strong> Some concerns using GPT-4 for annotating process reward model (PRM) training data may not scale well to much larger datasets and could limit generalizability, though reviewers acknowledge that this enables full automation and seems effective in practice. \</p> </li> <li> <p><strong>Presentation and clarity:</strong> The presentation was described as “messy” and at times unclear. Some technical terms and methodological concepts, like “Evol-Instruct”, were referenced before explanation, certain dataset descriptions (like grade school vs high school tasks) appeared and disappeared without clear discussion, and main tables (such as Table 1) omitted important baseline scores. There were also typos and confusing figures (e.g., Figure 1). \</p> </li> <li> <p><strong>Marginality of contribution:</strong> While the empirical results are strong, some reviewers see the novelty as moderate because both the Evol-Instruct framework and PRM methodology existed previously—the paper “basically took Evol-Instruct and PRM and used them to train a model”. They suggest a broader contribution could be achieved by applying the method to additional domains (such as code). \</p> </li> <li> <p><strong>Reporting gaps:</strong> Reviewers noted that some performance information, such as base model scores for certain variants, is missing from the tables, making it difficult to appreciate the incremental gains attributable to the proposed method. \</p> </li> </ul> <p><strong>Other notes:</strong></p> <ul> <li>The authors, in their responses, elaborated on these points, providing clarification, improved explanations, and expanded ablation studies to address review concerns. They also showed openness to expanding their comparisons and clarifying their figures and methodology in future versions.</li> </ul> <p>In summary, the paper is recognized for its strong empirical performance, especially in surpassing major baselines and proprietary models, and for its scalable, automated approach using new reward models. The main criticisms center on comparative rigor, clarity/presentation, and perceived marginality in core methodological novelty.</p> <h2 id="notes">Notes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://en.wikipedia.org/wiki/Agentic_AI <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="tech"/><category term="foundation"/><category term="models,"/><category term="reasoning,"/><category term="llms"/><summary type="html"><![CDATA[Reasoning capability comparison of open-source and closed source models in math.]]></summary></entry><entry><title type="html">Post-training</title><link href="https://lorenz-peter.github.io/blog/2025/post-training/" rel="alternate" type="text/html" title="Post-training"/><published>2025-05-03T16:40:16+00:00</published><updated>2025-05-03T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/post-training</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/post-training/"><![CDATA[<p>Post-training strategies like <strong>RLHF (Reinforcement Learning from Human Feedback)</strong> and <strong>DPO (Direct Preference Optimization)</strong> can also be applied to <strong>image-generation models</strong> (e.g., Stable Diffusion, DALL·E, Midjourney) to improve alignment, safety, and aesthetic quality. However, since images are non-textual, the methods differ slightly from those used in language models. Below is how these techniques work for image-generation AI:</p> <hr/> <h2 id="1-key-challenges-for-image-models-vs-text-models"><strong>1. Key Challenges for Image Models vs. Text Models</strong></h2> <p>| <strong>Aspect</strong> | <strong>Text Models (LLMs)</strong> | <strong>Image Models (Diffusion/VAEs)</strong> | |———————|———————-|——————————-| | <strong>Output Type</strong> | Discrete tokens | Continuous pixel space | | <strong>Preference Feedback</strong> | Easier (rank text responses) | Harder (subjective, multi-dimensional) | | <strong>Reward Modeling</strong> | Predict text quality | Predict aesthetics, safety, faithfulness | | <strong>RL Fine-Tuning</strong> | PPO on text sequences | Requires pixel-space optimization |</p> <hr/> <h2 id="2-how-rlhf-works-for-image-models"><strong>2. How RLHF Works for Image Models</strong></h2> <h3 id="step-1-supervised-fine-tuning-sft"><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></h3> <ul> <li>Train the base model (e.g., Stable Diffusion) on high-quality, curated images.</li> <li>Helps the model generate better initial outputs before alignment.</li> </ul> <h3 id="step-2-reward-model-training"><strong>Step 2: Reward Model Training</strong></h3> <ul> <li>Collect <strong>human preference data</strong> by showing users multiple generated images and asking: <ul> <li><em>Which image is more aesthetically pleasing?</em></li> <li><em>Which image better follows the prompt?</em></li> <li><em>Which image is safer (no harmful content)?</em></li> </ul> </li> <li>Train a <strong>reward model</strong> (e.g., a neural network) to predict human preferences.</li> </ul> <h3 id="step-3-rl-fine-tuning-ppo-or-diffusion-policy-optimization"><strong>Step 3: RL Fine-Tuning (PPO or Diffusion Policy Optimization)</strong></h3> <ul> <li>Use <strong>Reinforcement Learning (RL)</strong> to fine-tune the image generator to maximize the reward score.</li> <li>Unlike text models, optimizing in <strong>pixel space</strong> is computationally expensive, so alternatives include: <ul> <li><strong>Latent-space optimization</strong> (e.g., fine-tuning Stable Diffusion’s latent space).</li> <li><strong>Denoising Diffusion Policy Optimization (DDPO)</strong> (a variant of PPO for diffusion models).</li> </ul> </li> </ul> <h4 id="example-improving-aesthetics-with-rlhf"><strong>Example: Improving Aesthetics with RLHF</strong></h4> <ul> <li>A model like <strong>DALL·E 3</strong> may use RLHF to ensure: <ul> <li>Generated images match prompts more accurately.</li> <li>Images are more visually appealing (better lighting, composition).</li> <li>Avoids generating harmful/NSFW content.</li> </ul> </li> </ul> <hr/> <h2 id="3-how-dpo-works-for-image-models"><strong>3. How DPO Works for Image Models</strong></h2> <p>Since DPO eliminates the need for a separate reward model, it can be more efficient for image alignment.</p> <h3 id="step-1-collect-preference-data"><strong>Step 1: Collect Preference Data</strong></h3> <ul> <li>Humans rank pairs of images <strong>(A, B)</strong> based on: <ul> <li><strong>Prompt faithfulness</strong> (does it match the text?).</li> <li><strong>Aesthetics</strong> (which looks better?).</li> <li><strong>Safety</strong> (which is less harmful?).</li> </ul> </li> </ul> <h3 id="step-2-direct-optimization"><strong>Step 2: Direct Optimization</strong></h3> <ul> <li>Instead of training a reward model, <strong>DPO directly adjusts the image generator’s weights</strong> to increase the likelihood of preferred images over dispreferred ones.</li> <li>Works well for <strong>diffusion models</strong> since it avoids unstable RL training.</li> </ul> <h4 id="example-reducing-bias-with-dpo"><strong>Example: Reducing Bias with DPO</strong></h4> <ul> <li>If a model generates stereotypical images (e.g., “CEO” always as a man), DPO can: <ul> <li>Downweight biased images in training.</li> <li>Upsample diverse, fairer generations.</li> </ul> </li> </ul> <hr/> <h2 id="4-other-post-training-strategies-for-image-models"><strong>4. Other Post-Training Strategies for Image Models</strong></h2> <h3 id="a-rejection-sampling-best-of-n-filtering"><strong>A. Rejection Sampling (Best-of-N Filtering)</strong></h3> <ul> <li>Generate <strong>multiple images</strong>, then pick the <strong>best one</strong> using: <ul> <li>A <strong>safety classifier</strong> (e.g., NSFW filter).</li> <li>A <strong>reward model</strong> (e.g., aesthetic scorer).</li> </ul> </li> <li>Used in <strong>Midjourney v6</strong> to improve output quality.</li> </ul> <h3 id="b-adversarial-training-red-teaming"><strong>B. Adversarial Training (Red-Teaming)</strong></h3> <ul> <li>Test the model with <strong>malicious prompts</strong> (e.g., requests for violent images).</li> <li>Fine-tune the model to <strong>refuse harmful generations</strong>.</li> </ul> <h3 id="c-human-in-the-loop-refinement"><strong>C. Human-in-the-Loop Refinement</strong></h3> <ul> <li>Platforms like <strong>Stable Diffusion XL</strong> allow users to <strong>rate images</strong>, which are then used for further fine-tuning.</li> </ul> <hr/> <h2 id="5-applications--trade-offs"><strong>5. Applications &amp; Trade-offs</strong></h2> <p>| <strong>Goal</strong> | <strong>Method</strong> | <strong>Pros</strong> | <strong>Cons</strong> | |———————–|——————–|———————————-|———————————-| | <strong>Better Aesthetics</strong> | RLHF + Reward Model | High-quality outputs | Computationally expensive | | <strong>Prompt Faithfulness</strong> | DPO | Simpler, no reward model needed | Needs large preference dataset | | <strong>Safety/NSFW Filtering</strong> | Rejection Sampling | Easy to implement | Limited to filtering, not training | | <strong>Bias Mitigation</strong> | DPO + Fairness Data | Reduces stereotypes | May reduce creativity |</p> <hr/> <h2 id="6-real-world-examples"><strong>6. Real-World Examples</strong></h2> <ul> <li><strong>OpenAI’s DALL·E 3</strong> – Uses RLHF to improve prompt adherence and safety.</li> <li><strong>Stable Diffusion XL</strong> – Leverages human feedback for better aesthetics.</li> <li><strong>Midjourney</strong> – Uses ranking systems to refine style and quality.</li> </ul> <hr/> <h3 id="conclusion"><strong>Conclusion</strong></h3> <ul> <li><strong>RLHF and DPO can align image models</strong> with human preferences, but <strong>pixel-space optimization is harder than text</strong>.</li> <li><strong>DPO is more efficient</strong> than RLHF for images but requires good preference data.</li> <li><strong>Hybrid approaches (RLHF + filters)</strong> are common in production systems.</li> </ul> <p>Would you like details on implementing this for a specific image model (e.g., Stable Diffusion)?</p>]]></content><author><name></name></author><category term="tech"/><category term="foundation"/><category term="models,"/><category term="post-training"/><summary type="html"><![CDATA[foundation models]]></summary></entry><entry><title type="html">Ml_breadth</title><link href="https://lorenz-peter.github.io/blog/2025/ml_breadth/" rel="alternate" type="text/html" title="Ml_breadth"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/ml_breadth</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/ml_breadth/"><![CDATA[<h1 id="ml-breath">ML Breath</h1> <h2 id="training-and-optimization">Training and Optimization:</h2> <h3 id="adaptive-gradient-approaches-regularization-and-overfitting-loss-functions">Adaptive gradient approaches, Regularization and overfitting, loss functions</h3> <h3 id="bayesian-vs-maximum-likelihood-estimation">Bayesian v/s maximum likelihood estimation,</h3> <h3 id="dealing-with-class-imbalance-k-fold-cross-validation-bias-and-variance">dealing with class imbalance, K-fold cross validation, bias, and variance</h3> <p>###Evaluation metrics: Accuracy Precision, Recall Area under ROC R-squared: R-squared, also known as the coefficient of determination, is a statistical measure used in regression analysis to assess the goodness of fit of a model. Essentially, it indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Mean average precision (MAP) Mean reciprocal rank Equal Error rate: For biometric systems - authentication.. Binary. he Equal Error Rate is the point where the False Acceptance Rate (FAR) and the False Rejection Rate (FRR) are equal. In other words, it is the operating point where the system makes an equal number of errors in falsely accepting legitimate users (FAR) and falsely rejecting authorized users (FRR). Lower: better performance A/B testing fundamentals Supervised Learning Linear &amp; Logistic regression Naive Bayes classifier Bagging &amp; Boosting K-nearest neighbors Trees Neural Networks Support Vector Machines Random Forests, Gradient Boosted trees, kernel methods, Stochastic Gradient Descent (SGD), Sequence Modeling, Bayesian linear regression, Gaussian Processes, Concepts of overfitting and under fitting, Regularization and evaluation metrics for classification and regression problems Unsupervised Learning Clustering algorithms, k-Means clustering, Anomaly detection, Markov methods, DBSCAN, Self-organizing maps, Deep Belief Nets, Expectation Maximization (EM), Gaussian Mixture Models (GMM) and Evaluation metrics for clustering problems</p> <p>Probabilistic graphical models Bayesian Network, Markov Networks, Variational inference, Markov chain, Monte Carlo methods, Latent Dirichlet Allocation (LDA), Inference methods such as Belief Propagation, Gibbs Sampling</p> <p>Dimensionality reduction Auto encoders, t-SNE, Principal Component Analysis (PCA), Singular Value Decomposition (SVD), Spectral Clustering and Matrix Factorization</p> <p>Sequential models Hidden Markov model (HMM), Conditional random fields (CRF), Recurrent Neural Network (RNN), Natural Language processing applications such as Named Entity Recognition (NER) and Parts of Speech (POS) tagging Reinforcement Learning State–action–reward–state–action (SARSA), explore-exploit techniques, multi-armed bandits epsilon greed UCB, Thompson Sampling Q-learning, and Deep Q-Networks (DQNs). Applied to domains such as retail, Speech, NLP, Vision, robotics etc.</p> <p>Deep Neural Networks / Deep Learning Feed forward Neural Networks Convolutional Neural Networks Backpropagation Recurrent Neural Networks (RNNs) Long Short Term Memory (LSTM) networks GAN Attention Dropout Vanishing gradient Activation Functions</p> <p>Natural Language Processing Statistical Language Modelling Latent Dirichlet allocation (LDA) Named Entity Recognition (NER) Word Embedding Word2Vec Sentiment Analysis BERT: Bidirectional Encoder Transformer. ULMFiT</p> <p>Image and Computer Vision Object Detection Image recognition Pattern recognition FaceNet CNN YOLO</p>]]></content><author><name></name></author><summary type="html"><![CDATA[ML Breath]]></summary></entry><entry><title type="html">DICE</title><link href="https://lorenz-peter.github.io/blog/2025/dice/" rel="alternate" type="text/html" title="DICE"/><published>2025-03-28T16:40:16+00:00</published><updated>2025-03-28T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/dice</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/dice/"><![CDATA[<p>To solve the wide variety of dice problems efficiently, it’s helpful to recognize common patterns and techniques. Below is a categorized summary of key patterns and strategies that appear frequently in the problems you’ve shared. By identifying which pattern a problem falls into, you can minimize effort and apply the appropriate method.</p> <p><a href="https://www.madandmoonly.com/doctormatt/mathematics/dice1.pdf">Source</a></p> <hr/> <h3 id="1-expected-number-of-rolls-until-a-condition-is-met"><strong>1. Expected Number of Rolls Until a Condition is Met</strong></h3> <p><strong>Pattern</strong>: Problems ask for the average number of rolls needed to achieve a specific outcome (e.g., rolling a 6, getting all faces, etc.).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Geometric Distribution</strong>: For a single event (e.g., rolling a 6), the expected number of rolls is ( \frac{1}{p} ) (e.g., 6 rolls for a fair die).</li> <li><strong>Markov Chains/Recursion</strong>: For multi-step conditions (e.g., “until two 6’s appear in a row”), set up recursive equations or use states to model the problem.</li> <li><strong>Coupon Collector’s Problem</strong>: For “collecting all faces,” the expected number of rolls is ( n \sum_{k=1}^n \frac{1}{k} ) (e.g., 14.7 for a 6-sided die).</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 1, 2, 3, 9, 10, 11, 19, 25, 26.</p> <hr/> <h3 id="2-probability-of-specific-sequences-or-outcomes"><strong>2. Probability of Specific Sequences or Outcomes</strong></h3> <p><strong>Pattern</strong>: Questions about the probability of sequences (e.g., non-decreasing rolls), subsets (e.g., all faces appearing), or sums (e.g., sum is prime).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Inclusion-Exclusion Principle</strong>: For “at least one” or “all faces” problems (e.g., Problem 5: probability all faces appear in ( n ) rolls).</li> <li><strong>Generating Functions</strong>: For sums, use polynomials to model dice outcomes (e.g., ((x + x^2 + \dots + x^6)^n) for ( n ) dice).</li> <li><strong>Dynamic Programming/Recursion</strong>: For sequences (e.g., non-decreasing rolls), build up probabilities step-by-step.</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 5, 6, 7, 15, 16, 20, 22, 28, 29, 30.</p> <hr/> <h3 id="3-non-standard-dice-problems"><strong>3. Non-Standard Dice Problems</strong></h3> <p><strong>Pattern</strong>: Questions about biased dice, custom dice, or alternative definitions (e.g., sums, re-rolling rules).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Linear Algebra</strong>: For non-transitive dice (e.g., Efron’s dice), construct probability matrices.</li> <li><strong>Renumbering/Transformation</strong>: For “find dice with the same sum probabilities,” use generating functions or brute-force search.</li> <li><strong>Conditional Probability</strong>: For re-rolling rules (e.g., Problem 44: sum with rerolls on 6s).</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 48, 49, 50, 51, 53, 54.</p> <hr/> <h3 id="4-sum-related-problems"><strong>4. Sum-Related Problems</strong></h3> <p><strong>Pattern</strong>: Focus on sums of dice (e.g., distribution, stopping rules, or hitting a target sum).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Convolution</strong>: For sums of multiple dice, convolve their probability distributions.</li> <li><strong>Recursion</strong>: For “sum reaching ( n )” (e.g., Problem 36: ( E_n = 1 + \frac{1}{6} \sum_{k=1}^6 E_{n-k} )).</li> <li><strong>Markov Chains</strong>: For sums modulo ( n ) (e.g., Problem 37: expected rolls until sum is divisible by ( n )).</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 29, 31, 32, 34, 36, 37, 38, 39, 40.</p> <hr/> <h3 id="5-optimal-stopping-strategies"><strong>5. Optimal Stopping Strategies</strong></h3> <p><strong>Pattern</strong>: Decide when to stop rolling to maximize score or minimize loss (e.g., “stop when the current roll is higher than the expected future rolls”).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Backward Induction</strong>: Calculate expected values from the end of the game (e.g., Problem 14: stop if roll &gt; future expectation).</li> <li><strong>Dynamic Programming</strong>: For multi-stage decisions (e.g., Problem 72: stop before rolling a repeated face).</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 14, 72, 73, 74, 77.</p> <hr/> <h3 id="6-markov-chains-and-state-transitions"><strong>6. Markov Chains and State Transitions</strong></h3> <p><strong>Pattern</strong>: Problems where the outcome depends on previous states (e.g., runs, consecutive differences, or memory-based conditions).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Transition Matrices</strong>: Model states and transitions (e.g., Problem 25: consecutive differences).</li> <li><strong>Absorbing States</strong>: For “game ends when X happens” (e.g., Problem 26: rolls until all faces are distinct).</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 4, 6, 7, 25, 26, 35.</p> <hr/> <h3 id="7-asymptotic-behavior-and-approximations"><strong>7. Asymptotic Behavior and Approximations</strong></h3> <p><strong>Pattern</strong>: Questions about limits or large numbers (e.g., “probability all faces appear equally as ( n \to \infty )”).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Stirling’s Approximation</strong>: For factorials in large ( n ) (e.g., Problem 24: equal face counts in ( 6k ) rolls).</li> <li><strong>Central Limit Theorem</strong>: For sums of many dice, approximate with normal distributions.</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 24, 36, 39.</p> <hr/> <h3 id="8-symmetry-and-uniformity"><strong>8. Symmetry and Uniformity</strong></h3> <p><strong>Pattern</strong>: Exploit symmetry to simplify calculations (e.g., identical dice, uniform distributions).</p> <ul> <li><strong>Key Techniques</strong>: <ul> <li><strong>Symmetry Arguments</strong>: e.g., Problem 37: expected rolls until sum is divisible by ( n ) is ( n ), due to uniformity.</li> <li><strong>Renumbering Faces</strong>: For custom dice, relabel to match standard probabilities.</li> </ul> </li> </ul> <p><strong>Example Problems</strong>: 37, 52, 54.</p> <hr/> <h3 id="how-to-apply-these-patterns"><strong>How to Apply These Patterns</strong></h3> <ol> <li><strong>Classify the Problem</strong>: Identify which category the problem falls into (e.g., expected rolls, sum probabilities).</li> <li><strong>Choose the Technique</strong>: Use the corresponding method (e.g., recursion for expected rolls, generating functions for sums).</li> <li><strong>Simplify with Symmetry</strong>: Look for symmetries or uniform distributions to reduce complexity.</li> <li><strong>Verify Edge Cases</strong>: Check small ( n ) or trivial cases to ensure correctness.</li> </ol> <p>By recognizing these patterns, you can tackle most dice problems systematically. For example:</p> <ul> <li><strong>Problem 9 (Coupon Collector)</strong>: Use the harmonic series formula.</li> <li><strong>Problem 29 (Identical Sum Probabilities)</strong>: Use generating functions to compare coefficients.</li> <li><strong>Problem 72 (Optimal Stopping)</strong>: Use backward induction to find the stopping rule.</li> </ul> <p>Let me know if you’d like a deeper dive into any specific pattern!</p>]]></content><author><name></name></author><category term="statistics"/><category term="statistics"/><summary type="html"><![CDATA[To solve the wide variety of dice problems efficiently, it’s helpful to recognize common patterns and techniques. Below is a categorized summary of key patterns and strategies that appear frequently in the problems you’ve shared. By identifying which pattern a problem falls into, you can minimize effort and apply the appropriate method.]]></summary></entry><entry><title type="html">MLSD</title><link href="https://lorenz-peter.github.io/blog/2025/MLSD/" rel="alternate" type="text/html" title="MLSD"/><published>2025-03-28T16:40:16+00:00</published><updated>2025-03-28T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/MLSD</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/MLSD/"><![CDATA[<p>The main source is the book <a href="https://bytebytego.com/intro/machine-learning-system-design-interview">MLSD</a> by Alex Xu and Ali Aminian.</p> <ul> <li><a href="https://github.com/jS5t3r/Machine-Learning-Interviews/blob/main/src/MLSD/ml-system-design.md">Template</a></li> <li><a href="http://patrickhalina.com/posts/ml-systems-design-interview-guide/">Guide</a></li> <li><a href="https://lorenz-peter.github.io/blog/2025/recommender_systems">Recommender Systems</a></li> <li><a href="https://excalidraw.com/">Drawing Tool - Excalidraw</a></li> <li><a href="https://www.teamblind.com/post/Meta-MLE-E6-ML-System-Design-Interview-XaoxCs0c/41332921">Post</a></li> <li><a href="https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/">Scaling</a></li> </ul>]]></content><author><name></name></author><category term="Recommender"/><category term="System"/><category term="MLSD"/><summary type="html"><![CDATA[The main source is the book MLSD by Alex Xu and Ali Aminian.]]></summary></entry><entry><title type="html">Imbalanced Data</title><link href="https://lorenz-peter.github.io/blog/2025/imbalanced-data/" rel="alternate" type="text/html" title="Imbalanced Data"/><published>2025-03-28T16:40:16+00:00</published><updated>2025-03-28T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/imbalanced-data</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/imbalanced-data/"><![CDATA[<p><a href="https://aman.ai/primers/ai/data-imbalance">Source</a></p> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#overview">Overview</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#data-based-methods">Data-based Methods</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#oversampling-techniques">Oversampling Techniques</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#undersampling-techniques">Undersampling Techniques</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#hybrid-approaches">Hybrid Approaches</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#stratified-splitting">Stratified Splitting</a></li> </ul> </li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#loss-function-based-methods">Loss Function-based Methods</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#focal-loss">Focal Loss</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#weighted-loss-functions">Weighted Loss Functions</a></li> </ul> </li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#model-based-methods">Model-based Methods</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#benefits-of-ensemble-methods-for-class-imbalance">Benefits of Ensemble Methods for Class Imbalance</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#bagging">Bagging</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#boosting">Boosting</a></li> </ul> </li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#metrics-based-methods">Metrics-based Methods</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#evaluation-metrics">Evaluation Metrics</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#additional-diagnostic-tools-confusion-matrix-and-correlation-coefficients">Additional Diagnostic Tools (Confusion Matrix and Correlation Coefficients)</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#calibration-metrics">Calibration Metrics</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#practical-recommendations">Practical Recommendations</a></li> </ul> </li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#faqs">FAQs</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#how-do-ensemble-methods-help-with-class-imbalance">How Do Ensemble Methods Help with Class Imbalance?</a> <ul> <li><a href="https://aman.ai/primers/ai/data-imbalance/#bagging-methods-eg-random-forest">Bagging Methods (e.g., Random Forest)</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#boosting-methods-eg-adaboost-gradient-boosting-xgboost">Boosting Methods (e.g., AdaBoost, Gradient Boosting, XGBoost)</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#ensemble-of-resampled-datasets">Ensemble of Resampled Datasets</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#cost-sensitive-learning-with-ensembles">Cost-Sensitive Learning with Ensembles</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#hybrid-approaches-1">Hybrid Approaches</a></li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#key-advantages-of-using-ensembles-for-class-imbalance">Key Advantages of Using Ensembles for Class Imbalance</a></li> </ul> </li> </ul> </li> <li><a href="https://aman.ai/primers/ai/data-imbalance/#citation">Citation</a></li> </ul> <h2 id="overview">Overview</h2> <ul> <li>Class imbalance arises when one or more classes in a dataset are significantly underrepresented compared to others, often leading to biased predictions by machine learning models. Models trained on imbalanced datasets may perform well for the majority class but fail to adequately capture patterns for minority classes. Addressing class imbalance is essential to ensure fair, accurate, and generalized predictions.</li> <li>Below, we explore detailed techniques to handle class imbalance at the data, model, and metrics levels.</li> </ul> <h2 id="data-based-methods">Data-based Methods</h2> <ul> <li>Data-based methods aim to modify the dataset to balance the class distribution before training. By transforming the data, these approaches directly impact the availability and representativeness of minority classes in the training process. Common techniques include oversampling, undersampling, hybrid methods, and data transformation.</li> </ul> <h3 id="oversampling-techniques">Oversampling Techniques</h3> <ul> <li> <p>Oversampling involves increasing the representation of minority classes in the dataset by duplicating or generating synthetic samples.</p> </li> <li><strong>Random Oversampling</strong>: <ul> <li>Randomly duplicates existing samples from the minority class until the class sizes are balanced.</li> <li>While simple to implement, random oversampling can lead to overfitting, as it replicates the same samples multiple times without introducing new information.</li> </ul> </li> <li><strong>Advanced Variants</strong>: <ul> <li><strong>Augmentation-Based Oversampling</strong>: <ul> <li>Generates variability in minority class data by applying transformations like rotation, cropping, flipping, noise addition, or color jittering.</li> <li>Particularly effective in image and text data, where augmentation introduces realistic variations without requiring additional data collection.</li> </ul> </li> </ul> </li> <li><strong>SMOTE Variants (Synthetic Minority Oversampling Technique)</strong>: <ul> <li>SMOTE generates synthetic data points by interpolating between existing minority class samples and their nearest neighbors. Advanced SMOTE variants include: <ul> <li><strong>K-Means SMOTE</strong>: <ul> <li>Applies k-means clustering to the data and generates synthetic samples from minority clusters, ensuring the synthetic data aligns better with the data’s natural structure.</li> </ul> </li> <li><strong>SMOTE-Tomek</strong>: <ul> <li>Combines SMOTE with Tomek links (pairs of nearest-neighbor samples from different classes) to oversample the minority class while removing borderline and noisy samples.</li> </ul> </li> <li><strong>SVM-SMOTE</strong>: <ul> <li>Focuses on generating synthetic samples near the support vectors of a Support Vector Machine (SVM) classifier, emphasizing the critical decision boundaries.</li> </ul> </li> </ul> </li> </ul> </li> <li><strong>GAN-Based Data Augmentation/Oversampling</strong>: <ul> <li><strong>Conditional GANs (cGANs)</strong>: <ul> <li>Uses Generative Adversarial Networks (GANs) conditioned on class labels to create realistic and diverse synthetic samples for the minority class.</li> <li>Particularly useful for high-dimensional, complex datasets like images, time-series, or text, where traditional oversampling might fail to capture nuanced patterns.</li> </ul> </li> <li><strong>CycleGANs</strong> for domain-specific augmentation (e.g., converting aerial images to street views).</li> <li><strong>Variational Autoencoders (VAEs)</strong> to generate synthetic tabular data.</li> </ul> </li> </ul> <h3 id="undersampling-techniques">Undersampling Techniques</h3> <ul> <li> <p>Undersampling reduces the size of the majority class to achieve a balanced dataset. This approach is effective when the majority class has redundant or non-informative samples.</p> </li> <li><strong>Edited Nearest Neighbors (ENN)</strong>: <ul> <li>Removes samples from the majority class that are misclassified by their k-nearest neighbors, ensuring that noisy and overlapping samples are eliminated, leading to cleaner decision boundaries.</li> </ul> </li> <li><strong>Tomek Links</strong>: <ul> <li>Identifies pairs of samples from different classes that are each other’s nearest neighbors and removes the majority class samples from these pairs.</li> <li>Improves class separability by reducing overlap between classes.</li> </ul> </li> <li><strong>Cluster-Based Undersampling</strong>: <ul> <li>Uses clustering algorithms (e.g., k-means) to identify representative samples from the majority class, reducing redundancy while retaining critical patterns.</li> <li>Preserves the diversity of majority class data, preventing loss of important information.</li> </ul> </li> </ul> <h3 id="hybrid-approaches">Hybrid Approaches</h3> <ul> <li> <p>Hybrid approaches combine oversampling and undersampling techniques to leverage the benefits of both.</p> </li> <li><strong>SMOTE + ENN</strong>: <ul> <li>Applies SMOTE to oversample the minority class and ENN to remove noisy or overlapping samples from the majority class.</li> <li>Results in a balanced and clean dataset, especially for datasets with significant noise or class overlap.</li> </ul> </li> <li><strong>ADASYN + Cluster Centroids</strong>: <ul> <li>ADASYN (Adaptive Synthetic Sampling) generates synthetic samples for harder-to-classify minority samples, while cluster centroids reduce the size of the majority class by representing clusters as single samples.</li> <li>Ensures balanced yet simplified data for training.</li> </ul> </li> </ul> <h3 id="stratified-splitting">Stratified Splitting</h3> <ul> <li>Stratified splitting ensures that the class distribution in the training, validation, and test splits matches the original dataset. This prevents data leakage and ensures that the model’s performance metrics are evaluated fairly across all classes.</li> <li>Implementation: <ul> <li>Tools like Python’s <code class="language-plaintext highlighter-rouge">scikit-learn</code> provide a <code class="language-plaintext highlighter-rouge">stratify</code> parameter in the <code class="language-plaintext highlighter-rouge">train_test_split</code> function to maintain class proportions across splits.</li> </ul> </li> </ul> <h2 id="loss-function-based-methods">Loss Function-based Methods</h2> <ul> <li>These methods modify the loss function to penalize misclassification of minority classes more heavily, improving model sensitivity to underrepresented data.</li> </ul> <h3 id="focal-loss">Focal Loss</h3> <ul> <li>Tailored for extreme class imbalance, Focal Loss emphasizes harder-to-classify samples by down-weighting easy samples. L=−α(1−pt)γlog⁡(pt). <ul> <li>Parameters: <ul> <li>α: Balances class contributions to the loss.</li> <li>γ: Focuses training on hard samples, making the model more sensitive to the minority class.</li> </ul> </li> </ul> </li> </ul> <h3 id="weighted-loss-functions">Weighted Loss Functions</h3> <ul> <li>Assigns higher weights to minority classes, increasing their influence on the loss and model updates.</li> <li>Example: wc=Nnc, <ul> <li>where N is the total number of samples and nc is the number of samples in class c.</li> </ul> </li> <li>Widely supported in frameworks like TensorFlow, PyTorch, and scikit-learn.</li> </ul> <h2 id="model-based-methods">Model-based Methods</h2> <ul> <li>Model-based methods adapt algorithms to emphasize minority classes, often through paradigms like ensemble approaches.</li> <li>Ensemble methods combine predictions from multiple models to improve overall performance. They are particularly effective for handling class imbalance, as they can adjust focus on minority classes during training through techniques like sampling and weighting.</li> </ul> <h3 id="benefits-of-ensemble-methods-for-class-imbalance">Benefits of Ensemble Methods for Class Imbalance</h3> <ol> <li><strong>Improved Generalization</strong>: <ul> <li>By combining multiple models, ensemble methods reduce the risk of bias toward the majority class and ensure robust performance across classes.</li> </ul> </li> <li><strong>Flexible Sampling</strong>: <ul> <li>Bagging and boosting can be combined with oversampling, undersampling, or hybrid sampling strategies, enhancing their adaptability to class imbalance.</li> </ul> </li> <li><strong>Customizable Weighting</strong>: <ul> <li>Most ensemble frameworks, particularly boosting methods, allow for class-based weighting in their objectives, enabling precise control over class contributions to the final model.</li> </ul> </li> <li><strong>Enhanced Decision Boundaries</strong>: <ul> <li>Ensembles, especially boosting methods, refine decision boundaries iteratively, ensuring minority class regions are not overlooked.</li> </ul> </li> </ol> <ul> <li>In practice, the choice between bagging and boosting depends on the dataset and model goals: <ul> <li><strong>Bagging</strong> is better for reducing overfitting and leveraging parallelism.</li> <li><strong>Boosting</strong> excels in capturing complex patterns, particularly for skewed distributions, with the added advantage of class weighting options in frameworks like XGBoost and LightGBM.</li> </ul> </li> </ul> <h3 id="bagging">Bagging</h3> <ul> <li> <p>Bagging (Bootstrap Aggregating) reduces variance and improves model robustness by training multiple models on different subsets of data sampled with replacement. In the context of class imbalance, bagging methods help by:</p> </li> <li><strong>Boosting Minority Representation</strong>: <ul> <li>Resampling techniques, such as oversampling the minority class or undersampling the majority class, can be applied within each bootstrap sample.</li> <li>Ensures that minority classes are adequately represented in the training data for each model.</li> </ul> </li> <li><strong>Random Forest</strong>: <ul> <li>As a bagging method, Random Forest can handle class imbalance by: <ul> <li>Adjusting the class distribution in each bootstrap sample.</li> <li>Assigning class weights inversely proportional to their frequencies during tree construction.</li> </ul> </li> </ul> </li> <li><strong>Class-Specific Aggregation</strong>: <ul> <li>Combines predictions across multiple models, often weighting minority class predictions higher to correct for imbalance.</li> </ul> </li> </ul> <h3 id="boosting">Boosting</h3> <ul> <li> <p>Boosting sequentially trains models, focusing on samples that previous models misclassified. It is inherently suited to handling class imbalance due to its iterative adjustment of sample weights.</p> </li> <li><strong>Focusing on Hard-to-Classify Samples</strong>: <ul> <li>Boosting algorithms (e.g., AdaBoost, Gradient Boosting) assign higher weights to misclassified samples, often aligning with minority class instances in imbalanced datasets.</li> </ul> </li> <li><strong>Specialized Boosting Variants</strong>: <ul> <li><strong>SMOTEBoost</strong>: <ul> <li>Integrates SMOTE oversampling with boosting. At each iteration, synthetic samples are generated for the minority class, ensuring better representation.</li> </ul> </li> <li><strong>RUSBoost</strong>: <ul> <li>Combines Random Undersampling (RUS) with boosting to reduce majority class dominance while maintaining minority class focus.</li> </ul> </li> </ul> </li> <li><strong>Class Weight Support</strong>: <ul> <li>Many modern boosting frameworks, such as <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong>, allow specifying <strong>class weights</strong> directly in their loss functions.</li> <li>Class weights allow the algorithm to penalize misclassifications of minority class samples more heavily, improving balance. Put simply, this feature prioritizes the minority class by increasing its contribution to the optimization objective, further mitigating imbalance effects.</li> </ul> </li> </ul> <h2 id="metrics-based-methods">Metrics-based Methods</h2> <ul> <li>Metrics play a crucial role in evaluating machine learning models, particularly for imbalanced datasets, where standard accuracy measures can be misleading. By adopting specialized metrics, practitioners can ensure that the performance evaluation aligns with the goals of addressing class imbalance and prioritizing minority class outcomes.</li> </ul> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <ul> <li><strong>Precision, Recall, and F1-Score</strong>: <ul> <li>These metrics go beyond overall accuracy by focusing on specific aspects of model performance, particularly for the minority class: <ul> <li><strong>Precision</strong>: <ul> <li>Represents the proportion of correctly identified positive samples out of all samples predicted as positive.</li> <li>Useful in scenarios where false positives are costly, such as fraud detection. Precision=TPTP+FP.</li> </ul> </li> <li><strong>Recall</strong>: <ul> <li>Measures the proportion of actual positives correctly identified by the model.</li> <li>Crucial for applications like medical diagnosis, where missing positive cases can have severe consequences. Recall=TPTP+FN.</li> </ul> </li> <li><strong>F1-Score</strong>: <ul> <li>The harmonic mean of Precision and Recall, balancing their trade-offs.</li> <li>Provides a single, interpretable metric to assess a model’s focus on minority classes. F1=2⋅Precision⋅RecallPrecision+Recall.</li> </ul> </li> <li><strong>AUC-PR (Precision-Recall Curve)</strong>: <ul> <li>Evaluates performance across different thresholds by plotting Precision against Recall.</li> <li>More sensitive to the minority class than ROC-AUC because it avoids the dilution caused by the dominant majority class.</li> </ul> </li> </ul> </li> </ul> </li> <li><strong>Class-Specific Metrics</strong>: <ul> <li>Evaluate metrics like Precision, Recall, and F1 for each class separately, offering a detailed understanding of how the model performs for the minority class versus the majority class.</li> </ul> </li> </ul> <h3 id="additional-diagnostic-tools-confusion-matrix-and-correlation-coefficients">Additional Diagnostic Tools (Confusion Matrix and Correlation Coefficients)</h3> <ul> <li><strong>Confusion Matrix Analysis</strong>: <ul> <li>Provides a granular view of model performance across all prediction outcomes, including true/false positives and negatives.</li> <li>Enables targeted optimization for minority classes by identifying patterns in errors.</li> </ul> </li> <li><strong>Matthews Correlation Coefficient (MCC)</strong>: <ul> <li>A comprehensive metric for binary classification that considers true and false positives and negatives.</li> <li>Particularly robust for imbalanced datasets as it evaluates all four quadrants of the confusion matrix, providing a balanced measure. MCC=TP⋅TN−FP⋅FN(TP+FP)(TP+FN)(TN+FP)(TN+FN).</li> </ul> </li> <li><strong>Cohen’s Kappa</strong>: <ul> <li>Measures agreement between predicted and actual labels, adjusted for chance.</li> <li>Effective for class imbalance, as it accounts for the disparity in class proportions. Kappa=Po−Pe1−Pe,</li> <li>where (P_o) is the observed agreement and (P_e) is the agreement expected by chance.</li> </ul> </li> </ul> <h3 id="calibration-metrics">Calibration Metrics</h3> <ul> <li><strong>Brier Score</strong>: <ul> <li>Assesses the accuracy of probabilistic predictions by penalizing confidence in incorrect predictions and rewarding well-calibrated probabilities.</li> <li>Especially relevant for imbalanced datasets, where models often exhibit calibration issues due to overconfidence in the majority class. Brier Score=1N∑i=1N(fi−yi)2, where (f_i) is the predicted probability for sample (i) and (y_i) is the actual label.</li> </ul> </li> <li><strong>Expected Calibration Error (ECE)</strong>: <ul> <li>Measures the difference between predicted probabilities and actual outcomes, providing insight into the reliability of a model’s probabilistic outputs.</li> </ul> </li> </ul> <h3 id="practical-recommendations">Practical Recommendations</h3> <ol> <li><strong>Data-Level Techniques</strong>: <ul> <li>Employ <strong>SMOTE + Tomek Links</strong> to oversample the minority class and remove overlapping samples that introduce noise.</li> </ul> </li> <li><strong>Algorithm Adjustments</strong>: <ul> <li>Train a <strong>Weighted XGBoost</strong> model with <strong>Focal Loss</strong> to dynamically focus on difficult samples, especially in imbalanced datasets.</li> </ul> </li> <li><strong>Evaluation</strong>: <ul> <li>Prioritize metrics that emphasize the minority class, such as <strong>Precision-Recall curves</strong>, <strong>F1-Scores</strong>, and <strong>MCC</strong>.</li> </ul> </li> <li><strong>Calibration</strong>: <ul> <li>Validate model outputs with <strong>Brier Score</strong> and calibration plots to ensure reliable probabilistic predictions.</li> </ul> </li> </ol> <ul> <li>Class imbalance can be addressed effectively by leveraging a combination of these methods, tuned to the problem’s specific needs.</li> </ul> <h2 id="faqs">FAQs</h2> <h3 id="how-do-ensemble-methods-help-with-class-imbalance">How Do Ensemble Methods Help with Class Imbalance?</h3> <ul> <li>Ensemble methods are effective tools for addressing class imbalance, as they combine multiple models to improve overall performance, reduce overfitting, and mitigate the bias toward majority classes. By amplifying the signal from minority class data and leveraging the diversity of models, these methods enhance prediction accuracy and fairness across all classes. When paired with complementary techniques such as resampling, adjusting class weights, or generating synthetic data, ensemble methods can yield even more robust results in handling imbalanced datasets.</li> </ul> <h4 id="bagging-methods-eg-random-forest">Bagging Methods (e.g., Random Forest)</h4> <ul> <li><strong>How It Helps:</strong> <ul> <li>Bagging trains multiple models on different bootstrapped (randomly sampled with replacement) subsets of the data.</li> <li>You can apply techniques like oversampling the minority class or undersampling the majority class within each bootstrapped sample to improve representation of minority classes.</li> <li>Random Forests average predictions across trees, which helps mitigate the bias introduced by imbalanced data.</li> </ul> </li> <li><strong>Advantages:</strong> <ul> <li>Reduces variance and prevents overfitting.</li> <li>Can handle imbalance if combined with balanced sampling strategies.</li> </ul> </li> </ul> <h4 id="boosting-methods-eg-adaboost-gradient-boosting-xgboost">Boosting Methods (e.g., AdaBoost, Gradient Boosting, XGBoost)</h4> <ul> <li><strong>How It Helps:</strong> <ul> <li>Boosting focuses on correcting the mistakes of previous models by assigning higher weights to misclassified instances.</li> <li>In the case of imbalanced datasets, boosting naturally places more emphasis on minority class samples, as they are more likely to be misclassified in early iterations.</li> <li>Many boosting frameworks (e.g., XGBoost, LightGBM) allow specifying <strong>class weights</strong>, which further prioritize the minority class.</li> </ul> </li> <li><strong>Advantages:</strong> <ul> <li>Effective at focusing on hard-to-classify samples (often minority class).</li> <li>Customizable with parameters like learning rate and class weights.</li> </ul> </li> </ul> <h4 id="ensemble-of-resampled-datasets">Ensemble of Resampled Datasets</h4> <ul> <li><strong>How It Helps:</strong> <ul> <li>Build multiple models, each trained on a dataset that has been resampled to balance the classes.</li> <li>For example: <ul> <li><strong>Over-sampling:</strong> Duplicate samples of the minority class.</li> <li><strong>Under-sampling:</strong> Reduce samples of the majority class.</li> </ul> </li> <li>Combine predictions using voting or averaging to reduce individual model biases.</li> </ul> </li> <li><strong>Advantages:</strong> <ul> <li>Balances class representation while maintaining diversity among models.</li> <li>Reduces overfitting to the majority class.</li> </ul> </li> </ul> <h4 id="cost-sensitive-learning-with-ensembles">Cost-Sensitive Learning with Ensembles</h4> <ul> <li><strong>How It Helps:</strong> <ul> <li>Modify the objective function of ensemble models to include misclassification costs.</li> <li>Penalize misclassifications of the minority class more heavily, forcing the model to focus on getting those predictions right.</li> <li>Many frameworks, such as XGBoost, support custom loss functions that incorporate class imbalance.</li> </ul> </li> <li><strong>Advantages:</strong> <ul> <li>Directly addresses the imbalance by prioritizing the minority class.</li> <li>Avoids the need for resampling.</li> </ul> </li> </ul> <h4 id="hybrid-approaches-1">Hybrid Approaches</h4> <ul> <li><strong>How It Helps:</strong> <ul> <li>Combine ensemble methods with other imbalance techniques, such as SMOTE (Synthetic Minority Oversampling Technique).</li> <li>For example: <ul> <li>Use SMOTE to generate synthetic samples for the minority class, then train a Random Forest or XGBoost model.</li> </ul> </li> </ul> </li> <li><strong>Advantages:</strong> <ul> <li>Leverages the strengths of both resampling and ensemble learning.</li> <li>Can yield high performance even for severely imbalanced datasets.</li> </ul> </li> </ul> <h4 id="key-advantages-of-using-ensembles-for-class-imbalance">Key Advantages of Using Ensembles for Class Imbalance</h4> <ul> <li><strong>Improved Robustness:</strong> Ensembles aggregate predictions, reducing the likelihood of bias from a single model.</li> <li><strong>Focus on Hard Cases:</strong> Methods like boosting inherently focus on hard-to-classify samples, which are often from the minority class.</li> <li><strong>Flexibility:</strong> Many ensemble methods can integrate class weights or cost-sensitive learning to handle imbalance directly.</li> <li><strong>Versatility:</strong> Ensembles can be combined with other preprocessing or algorithmic approaches for greater effectiveness.</li> </ul> <h2 id="citation">Citation</h2> <p>If you found our work useful, please cite it as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Chadha2020DataImbalance,
  title   = {Data Imbalance},
  author  = {Chadha, Aman},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="Recommender"/><category term="System"/><category term="MLSD"/><summary type="html"><![CDATA[Source]]></summary></entry><entry><title type="html">Recommender Systems</title><link href="https://lorenz-peter.github.io/blog/2025/recommender-systems/" rel="alternate" type="text/html" title="Recommender Systems"/><published>2025-03-28T16:40:16+00:00</published><updated>2025-03-28T16:40:16+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/recommender-systems</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/recommender-systems/"><![CDATA[<p><a href="https://aman.ai/recsys/architectures/#deep-factorization-machine--deepfm-2017">Source</a></p> <ul> <li><a href="https://aman.ai/recsys/architectures/#overview">Overview</a></li> <li><a href="https://aman.ai/recsys/architectures/#deep-neural-network-models-for-recommender-systems">Deep Neural Network Models for Recommender Systems</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#advantages-of-dnn-models-for-recommender-systems">Advantages of DNN Models for Recommender Systems</a></li> <li><a href="https://aman.ai/recsys/architectures/#scenarios-where-deep-learning-may-not-be-effective">Scenarios Where Deep Learning May Not be Effective</a></li> <li><a href="https://aman.ai/recsys/architectures/#common-variations-of-neural-building-blocks">Common Variations of Neural Building Blocks</a></li> <li><a href="https://aman.ai/recsys/architectures/#applications-in-recommendation-systems">Applications in Recommendation Systems</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#wide-and-deep-2016">Wide and Deep (2016)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#background-cross-features">Background: Cross Features</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#what-are-feature-crosses-and-why-are-they-important">What are Feature Crosses and Why are They Important?</a></li> <li><a href="https://aman.ai/recsys/architectures/#what-are-the-challenges-in-learning-feature-crosses">What are the Challenges in Learning Feature Crosses?</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#motivation">Motivation</a></li> <li><a href="https://aman.ai/recsys/architectures/#architecture">Architecture</a></li> <li><a href="https://aman.ai/recsys/architectures/#example">Example</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#input-to-the-wide-component">Input to the Wide Component</a></li> <li><a href="https://aman.ai/recsys/architectures/#input-to-the-deep-component">Input to the Deep Component</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#combining-inputs-in-wide--deep-model">Combining Inputs in Wide &amp; Deep Model</a></li> <li><a href="https://aman.ai/recsys/architectures/#results">Results</a></li> <li><a href="https://aman.ai/recsys/architectures/#summary">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#factorization-machines--fm-2010">Factorization Machines / FM (2010)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-1">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#deep-factorization-machine--deepfm-2017">Deep Factorization Machine / DeepFM (2017)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-2">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#neural-collaborative-filtering--ncf-2017">Neural Collaborative Filtering / NCF (2017)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-3">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#deep-and-cross-networks--dcn-2017">Deep and Cross Networks / DCN (2017)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#forming-higher-order-feature-interactions">Forming Higher-Order Feature Interactions</a></li> <li><a href="https://aman.ai/recsys/architectures/#handling-sparse-features-through-embedding-layers">Handling Sparse Features Through Embedding Layers</a></li> <li><a href="https://aman.ai/recsys/architectures/#explicit-feature-crossing-and-polynomial-degree">Explicit Feature Crossing and Polynomial Degree</a></li> <li><a href="https://aman.ai/recsys/architectures/#input-and-output-to-each-component">Input and Output to Each Component</a></li> <li><a href="https://aman.ai/recsys/architectures/#cross-network-layers">Cross Network Layers</a></li> <li><a href="https://aman.ai/recsys/architectures/#deep-network-layers">Deep Network Layers</a></li> <li><a href="https://aman.ai/recsys/architectures/#results-1">Results</a></li> <li><a href="https://aman.ai/recsys/architectures/#summary-4">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#autoint-2019">AutoInt (2019)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-5">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#dlrm-2019">DLRM (2019)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-6">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#dcn-v2-2020">DCN V2 (2020)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#dcn-vs-dcn-v2">DCN vs. DCN V2</a></li> <li><a href="https://aman.ai/recsys/architectures/#low-rank-techniques-in-dcn-v2">Low-Rank Techniques in DCN V2</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#dcn-limitations-in-scalability">DCN Limitations in Scalability</a></li> <li><a href="https://aman.ai/recsys/architectures/#low-rank-approximations">Low-Rank Approximations</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#mixture-of-experts-architecture">Mixture-of-Experts Architecture</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#enhancing-expressiveness-with-mixture-of-experts">Enhancing Expressiveness with Mixture-of-Experts</a></li> <li><a href="https://aman.ai/recsys/architectures/#advantages-of-mixture-of-experts">Advantages of Mixture-of-Experts</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#model-structure-parallel-dcn-vs-stacked-and-parallel-dcn-v2">Model Structure: Parallel (DCN) vs. Stacked and Parallel (DCN V2)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#dcn">DCN</a></li> <li><a href="https://aman.ai/recsys/architectures/#dcn-v2">DCN V2</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#summary-of-key-differences">Summary of Key Differences</a></li> <li><a href="https://aman.ai/recsys/architectures/#summary-7">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#dhen-2022">DHEN (2022)</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#summary-8">Summary</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#gdcn-2023">GDCN (2023)</a></li> <li><a href="https://aman.ai/recsys/architectures/#graph-neural-networks-based-recsys-architectures">Graph Neural Networks-based RecSys Architectures</a></li> <li><a href="https://aman.ai/recsys/architectures/#two-towers-in-recsys">Two Towers in RecSys</a> <ul> <li><a href="https://aman.ai/recsys/architectures/#split-network">Split Network</a></li> </ul> </li> <li><a href="https://aman.ai/recsys/architectures/#summary-9">Summary</a></li> <li><a href="https://aman.ai/recsys/architectures/#comparative-analysis">Comparative Analysis</a></li> <li><a href="https://aman.ai/recsys/architectures/#references">References</a></li> </ul> <h2 id="overview">Overview</h2> <ul> <li>This primer explores some of the most popular architectures used in recommender systems, focusing on how these systems process and utilize different types of features for generating recommendations.</li> <li>The plot below <a href="https://paperswithcode.com/sota/click-through-rate-prediction-on-criteo">(source)</a> is a visual representation of the models and architectures for the task of Click-Through Rate Prediction on the Criteo dataset. With this use-case as our poster child, we will discuss the inner workings of some of the major model architectures listed in the plot.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/CTR.jpg" alt=""/></p> <h2 id="deep-neural-network-models-for-recommender-systems">Deep Neural Network Models for Recommender Systems</h2> <ul> <li>Deep neural network (DNN) models have become a cornerstone in modern recommendation systems due to their ability to capture complex patterns and deliver highly personalized and accurate recommendations.</li> <li>Leveraging variations of artificial neural networks (ANNs), DNNs excel in modeling intricate, non-linear relationships and generalizing beyond traditional linear approaches. Their strength lies in integrating diverse data types—such as image, audio, or textual content—and capturing sequential behaviors, such as user interactions over time, to provide contextually relevant recommendations.</li> <li>While computationally expensive and requiring significant data and expertise, the transformative capabilities of DNNs, especially when applied to large-scale datasets, make them indispensable for building state-of-the-art recommendation systems.</li> </ul> <h3 id="advantages-of-dnn-models-for-recommender-systems">Advantages of DNN Models for Recommender Systems</h3> <ol> <li><strong>Better Generalization Beyond Linear Models</strong>: <ul> <li>DNN models excel in learning non-linear relationships, which enables them to model user-item interactions with greater complexity than traditional linear approaches.</li> </ul> </li> <li><strong>Unified Representation of Heterogeneous Signals</strong>: <ul> <li>DNNs can integrate various forms of data into a unified representation by modeling different facets of users and items. For example, convolutional neural networks (CNNs) can process image, audio, or textual content as side information to enrich item embeddings, leading to more personalized and accurate recommendations.</li> </ul> </li> <li><strong>Scalability with Large-Scale Data</strong>: <ul> <li>DNN models are particularly effective when applied to large-scale datasets. They leverage the volume and variety of data to learn richer, more nuanced representations and interactions.</li> </ul> </li> <li><strong>Exploitation of Sequential Information</strong>: <ul> <li>Sequential patterns in user behavior, such as viewing, purchasing, or search history, can be captured using models like Long Short-Term Memory (LSTM) networks. These models predict the next action by analyzing sequences of past actions, improving the contextual relevance of recommendations.</li> </ul> </li> </ol> <h3 id="scenarios-where-deep-learning-may-not-be-effective">Scenarios Where Deep Learning May Not be Effective</h3> <ul> <li> <p>Despite their advantages, DNN models are not always the optimal choice, especially in scenarios where well-tuned machine learning (ML) baselines can outperform them:</p> <ol> <li><strong>Sparse Data</strong>: <ul> <li>In situations where user-item interactions are sparse or there is limited data available for training, DNNs struggle to learn meaningful patterns. Traditional techniques like matrix factorization or simpler ML algorithms can be more effective and less prone to overfitting.</li> </ul> </li> <li><strong>Low-Volume Data</strong>: <ul> <li>DNNs require large amounts of data to train effectively. When datasets are small, models like gradient-boosted trees or logistic regression, which require less data and are less computationally intensive, can outperform DNNs. Additionally, DNNs are more prone to overfitting in low-volume data scenarios, leading to poor generalization on unseen data.</li> </ul> </li> <li><strong>Homogeneous Data</strong>: <ul> <li>When the data is relatively uniform or lacks diverse signals (e.g., only numerical features without contextual data like images or text), the added complexity of DNNs is often unnecessary. Well-tuned linear models or tree-based methods can achieve comparable performance with lower computational overhead.</li> </ul> </li> </ol> </li> </ul> <h3 id="common-variations-of-neural-building-blocks">Common Variations of Neural Building Blocks</h3> <ol> <li><strong>Feedforward Neural Networks (FFNNs):</strong> <ul> <li>FNNs are a type of ANN where information flows in a unidirectional manner from one layer to the next.</li> <li>Multilayer perceptrons (MLPs) are specific types of FNNs that consist of at least three layers: an input layer, one or more hidden layers, and an output layer.</li> <li>MLPs are versatile and can be applied to a wide range of scenarios.</li> </ul> </li> <li><strong>Convolutional Neural Networks (CNNs):</strong> <ul> <li>CNNs, known for their prowess in image processing tasks, employ convolutional operations to extract meaningful features from input data.</li> <li>In recommendation systems, CNNs can be used to process side information such as item images, audio previews, or text descriptions.</li> </ul> </li> <li><strong>Recurrent Neural Networks (RNNs):</strong> <ul> <li>Designed for sequential data, RNNs capture temporal dependencies and are used for tasks such as predicting user behavior based on past actions.</li> <li>Variants like LSTMs are particularly adept at handling longer-term dependencies in sequences, and model sequential recommendations.</li> </ul> </li> </ol> <h3 id="applications-in-recommendation-systems">Applications in Recommendation Systems</h3> <ul> <li>Deep learning models build upon traditional recommendation techniques, such as collaborative filtering, by employing embeddings to represent categorical variables like users or items. These embeddings place similar users or items closer together in a vector space, facilitating better predictions. For instance, a deep learning approach to collaborative filtering can learn user and item embeddings from their interaction history using neural networks.</li> <li>These models benefit from advanced network architectures and optimization algorithms, which enable efficient training on large datasets.</li> </ul> <h2 id="wide-and-deep-2016">Wide and Deep (2016)</h2> <p><img src="https://aman.ai/recsys/assets/architectures/wd.webp" alt=""/></p> <ul> <li>While <a href="https://aman.ai/recsys/architectures/#neural-collaborative-filtering--ncf-2017">neural collaborative filtering (NCF)</a> revolutionized the domain of recommender systems, it lacks an important ingredient that turned out to be extremely important for the success of recommenders: cross features. The idea of cross features was first popularized in Google’s 2016 paper <a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep Learning for Recommender Systems</a> by Cheng et al.</li> <li><strong>Wide and Deep model architectures</strong> in recommender systems combine a linear model for the “wide” part, which captures <a href="https://aman.ai/recsys/architectures/#what-are-feature-crosses-and-why-are-they-important">cross-feature interactions</a> that models nonlinear interactions between the original features, with a NCF model for the “deep” part, which learns complex feature relationships and interactions. This hybrid approach balances memorization and generalization by capturing both specific feature combinations and broader patterns in the data.</li> </ul> <h3 id="background-cross-features">Background: Cross Features</h3> <h4 id="what-are-feature-crosses-and-why-are-they-important">What are Feature Crosses and Why are They Important?</h4> <ul> <li>A cross feature is a second-order feature (i.e., a cross-product transformation) that is created by “crossing” two sparse/categorical features (using the multiplication operation), thus modeling the interactive effects between the two features. Cross features capture nonlinear interactions between the original features, allowing the model to account for relationships that linear models would miss. In real-world problems, features often interact, meaning the effect of one feature on the output depends on the value of another feature.</li> <li>By modeling these interactions, cross features allow recommender systems to capture more complex relationships in the data, improving recommendations and ultimately, user engagement.</li> <li>For example, in an ad-click prediction system, consider the device type and time of day as two features. Their interaction could significantly affect the likelihood of a user clicking on an ad. For instance, users may be more likely to click on ads from their mobile device during evening hours when they are casually browsing, compared to when they are at work on a computer during the day. Such nonlinear interactions between these original features can be effectively modeled and captured through cross features, enabling the system to make more accurate predictions.</li> <li> <p>As another example, in the Google Play Store, first-order features include the impressed app, or the list of user-installed apps. These two can be combined to create powerful cross features, such as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;span&gt;AND&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;user_installed_app&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'netflix'&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;impression_app&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'hulu'&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;
</code></pre></div> </div> <ul> <li>which is 1 if the user has Netflix installed <em>and</em> the impressed app is Hulu.</li> </ul> </li> <li> <p>Cross features can also be more coarse-grained, such as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> &lt;span&gt;AND&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;user_installed_category&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'video'&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;impression_category&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'video'&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;
</code></pre></div> </div> <ul> <li>which is 1 if the user installed video apps before <em>and</em> the impressed app is a video app as well. The authors argue that adding cross features of different granularities enables both memorization (from more granular crosses) and generalization (from less granular crosses).</li> </ul> </li> <li>As another example (<a href="https://www.tensorflow.org/recommenders/examples/dcn">source</a>), imagine we are building a recommender system to sell a “blender” to customers. A customer’s past purchase history, such as <code class="language-plaintext highlighter-rouge">purchased_bananas</code> and <code class="language-plaintext highlighter-rouge">purchased_cooking_books</code>, or geographic features, are single features. If a customer has purchased both bananas and cooking books, then this customer will more likely click on the recommended blender. The combination of <code class="language-plaintext highlighter-rouge">purchased_bananas</code> and <code class="language-plaintext highlighter-rouge">purchased_cooking_books</code> is referred to as a feature cross, which provides additional interaction information beyond the individual features.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/Cross.gif" alt=""/></p> <h4 id="what-are-the-challenges-in-learning-feature-crosses">What are the Challenges in Learning Feature Crosses?</h4> <ul> <li>In web-scale applications, data is mostly categorical, leading to a large and sparse feature space. Identifying effective feature crosses in this setting often requires manual feature engineering or exhaustive search.</li> <li>Traditional feed-forward multilayer perceptron (MLP) models are universal function approximators; however, they cannot efficiently approximate even 2nd or 3rd-order feature crosses (<a href="https://arxiv.org/pdf/2008.13535.pdf">Wang et al. (2020)</a>, <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/18fa88ad519f25dc4860567e19ab00beff3f01cb.pdf">Beutel et al. (2018)</a>).</li> </ul> <h3 id="motivation">Motivation</h3> <ul> <li>Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. However, memorization and generalization are both important for recommender systems. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank.</li> </ul> <blockquote> <p>The Wide and Deep architecture demonstrated the critical importance of cross features, that is, second-order features that are created by crossing two of the original features. It combines a wide (and shallow) module for cross features with a deep (and narrow) module much like <a href="https://aman.ai/recsys/architectures/#neural-collaborative-filtering-2017">NCF</a>. It seeks to obtain the best of both worlds by combining the unique strengths of wide and deep models, i.e., memorization and generalization respectively, thus enabling better recommendations.</p> </blockquote> <ul> <li>Wide and Deep learning jointly train wide linear models and deep neural networks – to combine the benefits of memorization and generalization for recommender systems. Wide linear models can effectively memorize sparse feature interactions using cross-product feature transformations, while deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings.</li> </ul> <h3 id="architecture">Architecture</h3> <blockquote> <p><strong>Wide part:</strong> The wide part of the model is a generalized linear model that takes into account cross-product feature transformations, in addition to the original features. The cross-product transformations capture interactions between categorical features. For example, if you were building a real estate recommendation system, you might include a cross-product transformation of <code class="language-plaintext highlighter-rouge">city=San Francisco</code> AND <code class="language-plaintext highlighter-rouge">type=condo</code>. These cross-product transformations can effectively capture specific, niche rules, offering the model the benefit of <strong>memorization</strong>.</p> </blockquote> <blockquote> <p><strong>Deep part:</strong> The deep part of the model is a feed-forward neural network that takes all features as input, both categorical and continuous. However, categorical features are typically transformed into embeddings first, as neural networks work with continuous data. The deep part of the model excels at generalizing patterns from the data to unseen examples, offering the model the benefit of <strong>generalization</strong>.</p> </blockquote> <ul> <li>As a recap, a Generalized Linear Model (GLM) is a flexible generalization of ordinary linear regression that allows for response/outcome variables to have error distribution models other than a normal distribution. GLMs are used to model relationships between a response/outcome variable and one or more predictor variables. Examples of GLMs include logistic regression (used for binary outcomes like pass/fail), Poisson regression (for count data), and linear regression (for continuous data with a normal distribution).</li> <li>As an example <a href="https://blog.research.google/2016/06/wide-deep-learning-better-together-with.html">(source)</a>, say you’re trying to offer food/beverage recommendations based on an input query. People looking for specific items like “iced decaf latte with nonfat milk” really mean it. Just because it’s pretty close to “hot latte with whole milk” in the embedding space doesn’t mean it’s an acceptable alternative. Similarly, there are millions of these rules where the transitivity (a relation between three elements such that if it holds between the first and second and it also holds between the second and third, it must necessarily hold between the first and third) of embeddings may actually do more harm than good.</li> <li>On the other hand, queries that are more exploratory like “seafood” or “italian food” may be open to more generalization and discovering a diverse set of related items.</li> </ul> <blockquote> <p>Building upon the food recommendation example earlier, as shown in the graph below <a href="https://blog.research.google/2016/06/wide-deep-learning-better-together-with.html">(source)</a>, sparse features like <code class="language-plaintext highlighter-rouge">query="fried chicken"</code> and <code class="language-plaintext highlighter-rouge">item="chicken fried rice"</code> are used in both the wide part (left) and the deep part (right) of the model.</p> </blockquote> <p><img src="https://aman.ai/images/papers/Wide&amp;Deep2.jpg" alt=""/></p> <ul> <li>For the wide component utilizing a generalized linear model, cross-product transformations are carried out on the binary features (e.g., <code class="language-plaintext highlighter-rouge">AND(gender=female, language=en)</code>) is 1 if and only if the constituent features (<code class="language-plaintext highlighter-rouge">gender=female</code> and <code class="language-plaintext highlighter-rouge">language=en</code>) are all 1, and 0 otherwise. This captures the interactions between the binary features, and adds non-linearity to the generalized linear model.</li> <li>For the deep component utilizing a feed-forward neural network, each of the sparse, high-dimensional categorical features are first converted into a low-dimensional, dense real-valued vector, often referred to as an embedding vector. The dimensionality of the embeddings are usually on the order of O(10) to O(100). The embedding vectors are initialized randomly and then the values are trained to minimize the final loss function during model training.</li> <li>During training, the prediction errors are backpropagated to both sides to train the model parameters, i.e., the two models function as one “cohesive” architecture and are trained jointly with the same loss function.</li> <li>The figure below from the paper shows how Wide and Deep models form a sweet middle compared to just Wide and just Deep models:</li> </ul> <p><img src="https://aman.ai/images/papers/Wide&amp;Deep.jpg" alt=""/></p> <ul> <li>Thus, the key architectural choice in Wide and Deep is to have both a wide module, which is a linear model that takes all cross features directly as inputs, and a deep module, which is essentially an <a href="https://aman.ai/recsys/architectures/#neural-collaborative-filtering-2017">NCF</a>, and then combine both modules into a single output task head that learns from user/app engagements. The architectural diagram below <a href="https://medium.com/better-ml/recsys-model-serving-model-architectures-serving-1b5f038848bd">(source)</a> showcases this structure.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/4.jpg" alt=""/></p> <ul> <li>By combining these two components, Wide and Deep models aim to achieve a balance between memorization and generalization, which can be particularly useful in recommendation systems, where both aspects can be important. The wide part can capture specific item combinations that a particular user might like (based on historical data), while the deep part can generalize from user behavior to recommend items that the user hasn’t interacted with yet but might find appealing based on their broader preferences. Put simply, Wide and Deep architectures combine a deep neural network component for capturing complex patterns and a wide component using a generalized linear model that models feature interactions explicitly. This allows the model to learn both deep representations and exploit feature interactions, providing a balance between memorization and generalization.</li> <li> <p>In the Wide &amp; Deep Learning model, both the wide and deep components handle sparse features, but in different ways:</p> <ol> <li><strong>Wide Component</strong>: <ul> <li>The wide component is a generalized linear model that uses raw input features and transformed features.</li> <li>An important transformation in the wide component is the cross-product transformation. This is particularly useful for binary features, where a cross-product transformation like <code class="language-plaintext highlighter-rouge">AND(gender=female language=en)</code> is 1 if and only if both constituent features are 1, and 0 otherwise.</li> <li>Such transformations capture the interactions between binary features and add non-linearity to the generalized linear model.</li> </ul> </li> <li><strong>Deep Component</strong>: <ul> <li>The deep component is a feed-forward neural network.</li> <li>For handling categorical features, which are often sparse and high-dimensional, the deep component first converts these features into low-dimensional, dense real-valued vectors, commonly referred to as embedding vectors. The dimensionality of these embeddings usually ranges from 10 to 100.</li> <li>These dense embedding vectors are then fed into the hidden layers of the neural network. The embeddings are initialized randomly and trained to minimize the final loss function during model training.</li> </ul> </li> <li><strong>Combined Model</strong>: <ul> <li>The wide and deep components are combined using a weighted sum of their output log odds, which is then fed to a common logistic loss function for joint training.</li> <li>In this combined model, the wide part focuses on memorization (exploiting explicit feature interactions), while the deep part focuses on generalization (learning implicit feature representations).</li> <li>The combined model ensures that both sparse and dense features are effectively utilized, with sparse features often transformed into dense representations for efficient processing in the deep neural network.</li> </ul> </li> </ol> </li> </ul> <h3 id="example">Example</h3> <ul> <li>As an example, let’s consider a music recommendation app using the Wide and Deep Learning model, the input features for both the wide and deep components would be tailored to capture different aspects of user preferences and characteristics of the music items. Let’s consider what these inputs might look like.</li> </ul> <h4 id="input-to-the-wide-component">Input to the Wide Component</h4> <ul> <li> <p>The wide component would primarily use sparse, categorical features, possibly transformed to capture specific interactions:</p> <ol> <li><strong>User Features</strong>: Demographics (age, gender, location), user ID, historical user behavior (e.g., genres listened to frequently, favorite artists).</li> <li><strong>Music Item Features</strong>: Music genre, artist ID, album ID, release year.</li> <li><strong>Cross-Product Transformations</strong>: Combinations of categorical features that are believed to interact in meaningful ways. For instance, “user’s favorite genre = pop” AND “music genre = pop”, or “user’s location = USA” AND “artist’s origin = USA”. These cross-products help capture interaction effects that are specifically relevant to music recommendations.</li> </ol> </li> </ul> <h4 id="input-to-the-deep-component">Input to the Deep Component</h4> <ul> <li> <p>The deep component would use both dense and sparse features, with sparse features transformed into dense embeddings:</p> <ol> <li><strong>User Features (as Embeddings)</strong>: Embeddings for user ID, embedding vectors for historical preferences (like a vector summarizing genres listened to), demographics if treated as categorical.</li> <li><strong>Music Item Features (as Embeddings)</strong>: Embeddings for music genre, artist ID, album ID. These embeddings capture the nuanced relationships in the music domain.</li> <li><strong>Additional Dense Features</strong>: If available, numerical features like the number of times a song has been played, user’s average listening duration, or ratings given by the user.</li> </ol> </li> <li> <p>The embeddings created to serve as the input to the Dense component are “learned embeddings” or “trainable embeddings,” as they are learned directly from the data during the training process of the model.</p> </li> <li> <p>Here’s a Python code snippet using TensorFlow to illustrate how a categorical feature (like user IDs) is embedded:</p> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;span&gt;import&lt;/span&gt; &lt;span&gt;tensorflow&lt;/span&gt; &lt;span&gt;as&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;

&lt;span&gt;# Assuming we have 10,000 unique users and we want to embed them into a 64-dimensional space
&lt;/span&gt;&lt;span&gt;num_unique_users&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;10000&lt;/span&gt;
&lt;span&gt;embedding_dimension&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;64&lt;/span&gt;

&lt;span&gt;# Create an input layer for user IDs (assuming user IDs are integers ranging from 0 to 9999)
&lt;/span&gt;&lt;span&gt;user_id_input&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;keras&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;Input&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;shape&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;,),&lt;/span&gt; &lt;span&gt;dtype&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'int32'&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Create an embedding layer
&lt;/span&gt;&lt;span&gt;user_embedding_layer&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;keras&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;layers&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;Embedding&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;input_dim&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;num_unique_users&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; 
                                                 &lt;span&gt;output_dim&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;embedding_dimension&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; 
                                                 &lt;span&gt;input_length&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; 
                                                 &lt;span&gt;name&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'user_embedding'&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Apply the embedding layer to the user ID input
&lt;/span&gt;&lt;span&gt;user_embedding&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;user_embedding_layer&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;user_id_input&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Flatten the embedding output to feed into a dense layer
&lt;/span&gt;&lt;span&gt;user_embedding_flattened&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;keras&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;layers&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;Flatten&lt;/span&gt;&lt;span&gt;()(&lt;/span&gt;&lt;span&gt;user_embedding&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Add a dense layer (more layers can be added as needed)
&lt;/span&gt;&lt;span&gt;dense_layer&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;keras&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;layers&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;Dense&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;128&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;activation&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'relu'&lt;/span&gt;&lt;span&gt;)(&lt;/span&gt;&lt;span&gt;user_embedding_flattened&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Create a model
&lt;/span&gt;&lt;span&gt;model&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; &lt;span&gt;tf&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;keras&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;Model&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;inputs&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;user_id_input&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;outputs&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;dense_layer&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;

&lt;span&gt;# Compile the model
&lt;/span&gt;&lt;span&gt;model&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;compile&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;optimizer&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'adam'&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;loss&lt;/span&gt;&lt;span&gt;=&lt;/span&gt;&lt;span&gt;'mse'&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;  &lt;span&gt;# Adjust the loss based on your specific task
&lt;/span&gt;
&lt;span&gt;# Model summary
&lt;/span&gt;&lt;span&gt;model&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;span&gt;summary&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;
</code></pre></div></div> <p>In this code:</p> <ul> <li>We first define the number of unique users (<code class="language-plaintext highlighter-rouge">num_unique_users</code>) and the dimensionality of the embedding space (<code class="language-plaintext highlighter-rouge">embedding_dimension</code>).</li> <li>An input layer is created to accept user IDs.</li> <li>An embedding layer (<code class="language-plaintext highlighter-rouge">tf.keras.layers.Embedding</code>) is added to transform each user ID into a 64-dimensional vector. This layer is set to be trainable, meaning its weights (the embeddings) are learned during training.</li> <li>The embedding layer’s output is then flattened and passed through a dense layer for further processing.</li> <li> <p>The model is compiled with an optimizer and loss function, which should be chosen based on the specific task (e.g., classification, regression).</p> </li> <li>This code example demonstrates how to create trainable embeddings for a categorical feature within a neural network using TensorFlow. These embeddings are specifically tailored to the data and task at hand, learning to represent each category (in this case, user IDs) in a way that is useful for the model’s predictive task.</li> </ul> <h3 id="combining-inputs-in-wide--deep-model">Combining Inputs in Wide &amp; Deep Model</h3> <ul> <li><strong>Joint Model</strong>: The wide and deep components are joined in a unified model. The wide component helps with memorization of explicit feature interactions (especially useful for categorical data), while the deep component contributes to generalization by learning implicit patterns and relationships in the data.</li> <li><strong>Feature Transformation</strong>: Sparse features are more straightforwardly handled in the wide part through cross-product transformations, while in the deep part, they are typically converted into dense embeddings.</li> <li> <p><strong>Model Training</strong>: Both parts are trained jointly, allowing the model to leverage the strengths of both memorization and generalization.</p> </li> <li>As an example, in a music recommendation app, this combination allows the model to not only consider obvious interactions (like a user’s past preferences for certain genres or artists) but also to uncover more subtle patterns and relationships within the data, which might not be immediately apparent but are influential in determining a user’s music preferences.</li> </ul> <h3 id="results">Results</h3> <ul> <li>They productionized and evaluated the system on Google Play Store, a massive-scale commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide and Deep significantly increased app acquisitions compared with wide-only and deep-only models.</li> <li>Compared to a deep-only model, Wide and Deep improved acquisitions in the Google Play store by 1%. Consider that Google makes tens of billions in revenue each year from its Play Store, and it’s easy to see how impactful Wide and Deep was.</li> </ul> <h3 id="summary">Summary</h3> <ul> <li><strong>Architecture</strong>: The Wide and Deep model in recommendation systems incorporates cross features, particularly in the “wide” component of the model. The wide part is designed for memorization and uses linear models with cross-product feature transformations, effectively capturing interactions between categorical features. This is crucial for learning specific, rule-based information, which complements the “deep” part of the model that focuses on generalization through deep neural networks. By combining these approaches, Wide and Deep models effectively capture both simple, rule-based patterns and complex, non-linear relationships within the data.</li> <li><strong>Pros:</strong> Balances memorization (wide component) and generalization (deep component), capturing both complex patterns and explicit feature interactions.</li> <li><strong>Cons:</strong> Increased model complexity and potential challenges in training and optimization.</li> <li><strong>Advantages:</strong> Improved performance by leveraging both deep representations and explicit feature interactions.</li> <li><strong>Example Use Case:</strong> E-commerce platforms where a combination of user behavior and item features plays a crucial role in recommendations.</li> <li><strong>Phase:</strong> Ranking.</li> <li><strong>Recommendation Workflow:</strong> Given it’s complexity, the deep and wide architecture is suitable for the ranking phase. The wide component can capture explicit feature interactions and enhance the candidate generation process. The deep component allows for learning complex patterns and interactions, improving the ranking of candidate items based on user-item preferences.</li> </ul> <h2 id="factorization-machines--fm-2010">Factorization Machines / FM (2010)</h2> <ul> <li>Factorization Machines (FM), introduced in <a href="https://analyticsconsultores.com.mx/wp-content/uploads/2019/03/Factorization-Machines-Steffen-Rendle-Osaka-University-2010.pdf">Rendle (2010)</a>, extend the traditional matrix factorization (MF) model to handle high-dimensional sparse input data and model pairwise interactions between features.</li> </ul> <blockquote> <p>Unlike MF, which is limited to user-item interaction matrices, the FM architecture is particularly effective for incorporating high-dimensional side information/contextual features, (such as user demographics, item attributes, or temporal context) along with sparse user-item interactions.</p> </blockquote> <ul> <li> <p>The key innovation of FM lies in its ability to generalize matrix factorization by learning pairwise feature interactions across any set of features, not just users and items. It achieves this through a latent factorization of feature interactions, representing each feature as a low-dimensional embedding. For example, in a movie recommendation system, FM can model interactions like <code class="language-plaintext highlighter-rouge">user-movie</code>, <code class="language-plaintext highlighter-rouge">user-genre</code>, or <code class="language-plaintext highlighter-rouge">user-age</code> in a unified framework.</p> </li> <li> <p>FM models second-order interactions between features in a latent space, making it more expressive than linear models while being computationally efficient. It computes these interactions efficiently using factorized parameters, avoiding the computational cost of explicitly enumerating all feature pairs, which would otherwise be infeasible in large-scale systems.</p> </li> <li> <p>FM works exceptionally well with sparse, high-dimensional data, a common trait in recommendation systems and click-through rate prediction tasks. Its design allows it to handle such data effectively, ensuring scalability without sacrificing predictive accuracy.</p> </li> <li> <p>Unlike traditional matrix factorization, FM can incorporate arbitrary features beyond user and item IDs, including metadata or contextual signals. This flexibility makes it suitable for a wide range of applications where additional information can enhance predictions.</p> </li> <li> <p>The FM model predicts the interaction score y^ for a given feature vector x as:</p> <p>y^=w0+∑i=1nwixi+∑i=1n∑j=i+1n⟨vi,vj⟩xixj</p> <ul> <li>where, <ul> <li>w0: Global bias term.</li> <li>wi: Weight for each feature xi.</li> <li>⟨vi,vj⟩: Dot product of latent vectors vi and vj, representing the interaction between features xi and xj.</li> </ul> </li> </ul> </li> <li> <p>The following figure from the paper shows an example of sparse, real-valued feature vectors x, created from transactions in a recommendation system. Each row represents a feature vector x(i) with its corresponding target y(i) (e.g., a user’s rating of a movie):</p> <ul> <li><strong>Blue Columns:</strong> Indicator variables for the active user.</li> <li><strong>Red Columns:</strong> Indicator variables for the active item (e.g., the movie being rated).</li> <li><strong>Yellow Columns:</strong> Implicit indicators for other items the user has rated (e.g., related movies).</li> <li><strong>Green Column:</strong> A real-valued feature representing the time (e.g., in months).</li> <li><strong>Brown Columns:</strong> Indicator variables for the last item the user rated before the active one.</li> <li><strong>Rightmost Column:</strong> The target variable (e.g., the user’s rating).</li> <li>This example demonstrates FM’s ability to process diverse types of contextual and historical information, enabling it to learn complex patterns and relationships from these interactions.</li> </ul> </li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/FM.jpg" alt=""/></p> <ul> <li>FM’s structure makes it ideal for handling datasets where feature interactions are critical but the data is sparse, such as in recommender systems, advertising, and search.</li> </ul> <h3 id="summary-1">Summary</h3> <ul> <li><strong>Pros:</strong> <ul> <li>Handles both structured and unstructured features.</li> <li>Scalable for high-dimensional sparse datasets.</li> <li>Provides a unified framework for incorporating contextual information.</li> </ul> </li> <li><strong>Cons:</strong> <ul> <li>Limited to modeling pairwise interactions; higher-order interactions require extensions or integration with deep learning models.</li> <li>May not capture non-linear interactions as effectively as deep learning-based approaches.</li> </ul> </li> <li><strong>Example Use Case:</strong> CTR prediction, recommendation systems, and search ranking.</li> <li><strong>Phase:</strong> Candidate Generation, Ranking.</li> <li><strong>Recommendation Workflow:</strong> FM is commonly used in the ranking phase, where it effectively models pairwise feature interactions for personalized recommendations or ad targeting.</li> </ul> <h2 id="deep-factorization-machine--deepfm-2017">Deep Factorization Machine / DeepFM (2017)</h2> <ul> <li>Similar to Google’s <a href="https://aman.ai/recsys/architectures/#deep-and-cross-networks--dcn-2017">DCN</a>, Huawei’s DeepFM, as introduced in <a href="https://arxiv.org/abs/1703.04247">Guo et al. (2017)</a>, also replaces manual feature engineering in the wide component of the Wide and Deep model with a specialized neural network that learns cross features. However, unlike DCN, the wide component in DeepFM is not a cross neural network but instead utilizes a factorization machine (FM) layer.</li> </ul> <blockquote> <p>What is the role of the FM layer? It computes the dot products of all pairs of embeddings. For example, in a movie recommender system with four id-features as inputs, such as user id, movie id, actor ids, and director id, the FM layer calculates six dot products. These correspond to the combinations <code class="language-plaintext highlighter-rouge">user-movie</code>, <code class="language-plaintext highlighter-rouge">user-actor</code>, <code class="language-plaintext highlighter-rouge">user-director</code>, <code class="language-plaintext highlighter-rouge">movie-actor</code>, <code class="language-plaintext highlighter-rouge">movie-director</code>, and <code class="language-plaintext highlighter-rouge">actor-director</code>. The output from the FM layer is concatenated with the output from the deep component and passed through a sigmoid layer to generate the model’s predictions.</p> </blockquote> <blockquote> <p>It is important to note that DeepFM, much like <a href="https://aman.ai/recsys/architectures/#deep-and-cross-networks--dcn-2017">DCN</a>, employs a brute-force method by considering all possible feature combinations uniformly (i.e., calculating all pairwise interactions). In contrast, more recent approaches such as <a href="https://aman.ai/recsys/architectures/#autoint-2019">AutoInt</a> utilize self-attention mechanisms to automatically determine the most relevant feature interactions, effectively identifying which interactions are most significant (and ignoring others by setting their attention weights to zero).</p> </blockquote> <ul> <li>The figure below, taken from the paper, illustrates the architecture of DeepFM. Both the wide and deep components share the same raw feature vector as input, allowing DeepFM to simultaneously learn both low- and high-order feature interactions from the input data. Notably, in the figure, there is a circle labeled “+” in the FM layer alongside the inner products. This functions as a skip connection, directly passing the concatenated inputs to the output unit.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/dfm.webp" alt=""/></p> <ul> <li> <p>The authors demonstrate that DeepFM outperforms several competitors, including Google’s Wide and Deep model, by more than 0.42% in Logloss on internal company data.</p> </li> <li> <p>DeepFM replaces the cross neural network in DCN with factorization machines, specifically employing dot products for feature interactions.</p> </li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/5.webp" alt=""/></p> <ul> <li>DeepFM integrates FM with deep neural networks. The FM component models pairwise feature interactions, while the deep neural network captures higher-order feature interactions. This combined architecture effectively exploits both linear and non-linear relationships between features.</li> </ul> <h3 id="summary-2">Summary</h3> <ul> <li><strong>Pros:</strong> Combines the benefits of FM and deep neural networks, capturing both pairwise and higher-order feature interactions. In other words, accurate modeling of both linear and non-linear relationships between features, providing a comprehensive understanding of feature interactions.</li> <li><strong>Cons:</strong> <ul> <li>DeepFM creates feature crosses in a brute-force way, simply by considering all possible combinations. This is not only inefficient, it could also create feature crosses that aren’t helpful at all, and just make the model overfit.</li> <li>Increased model complexity and potential challenges in training and optimization.</li> </ul> </li> <li><strong>Example Use Case:</strong> Click-through rate prediction in online advertising or personalized recommendation systems.</li> <li><strong>Phase:</strong> Candidate Generation, Ranking.</li> <li><strong>Recommendation Workflow:</strong> DeepFM is commonly utilized in both the candidate generation and ranking phases. It combines the strengths of factorization machines and deep neural networks. In the candidate generation phase, DeepFM can capture pairwise feature interactions efficiently. In the ranking phase, it can leverage deep neural networks to model higher-order feature interactions and improve the ranking of candidate items.</li> </ul> <h2 id="neural-collaborative-filtering--ncf-2017">Neural Collaborative Filtering / NCF (2017)</h2> <ul> <li>The integration of deep learning into recommender systems witnessed a significant breakthrough with the introduction of Neural Collaborative Filtering (NCF), introduced in <a href="https://arxiv.org/abs/1708.05031">He et. al (2017)</a> from NUS Singapore, Columbia University, Shandong University, and Texas A&amp;M University.</li> <li>This innovative approach marked a departure from the (then standard) matrix factorization method. Prior to NCF, the gold standard in recommender systems was matrix factorization, which relied on learning latent vectors (a.k.a. embeddings) for both users and items, and then generate recommendations for a user by taking the dot product between the user vector and the item vectors. The closer the dot product is to 1, the better the match. As such, matrix factorization can be simply viewed as a linear model of latent factors.</li> </ul> <blockquote> <p>The key idea behind NCF is to substitute the inner product in matrix factorization with a neural network architecture to that can learn an arbitrary non-linear function from data. To supercharge the learning process of the user-item interaction function with non-linearities, they concatenated user and item embeddings, and then fed them into a multi-layer perceptron (MLP) with a single task head predicting user engagement, like clicks. Both MLP weights and embedding weights (which user/item IDs are mapped to) were learned through backpropagation of loss gradients during model training.</p> </blockquote> <ul> <li>The hypothesis underpinning NCF posits that user-item interactions are non-linear, contrary to the linear assumption in matrix factorization.</li> <li>The figure below from the paper illustrates the neural collaborative filtering framework.</li> </ul> <p><img src="https://aman.ai/images/papers/NCF.jpg" alt=""/></p> <ul> <li>NCF proved the value of replacing (then standard) linear matrix factorization algorithms with a neural network. With a relatively simply 4-layer neural network, NCF proved that there’s immense value of applying deep neural networks in recommender systems, marking the pivotal transition away from matrix factorization and towards deep recommenders. They were able to beat the best matrix factorization algorithms at the time by 5% hit rate on the Movielens and Pinterest benchmark datasets. Empirical evidence showed that using deeper layers of neural networks offers better recommendation performance.</li> <li>Despite its revolutionary impact, NCF lacked an important ingredient that turned out to be extremely important for the success of recommenders: cross features, a concept popularized by the <a href="https://aman.ai/recsys/architectures/#wide-and-deep-2016">Wide &amp; Deep</a> paper described above.</li> </ul> <h3 id="summary-3">Summary</h3> <ul> <li>NCF proved the value of replacing (then standard) linear matrix factorization algorithms with a neural network.</li> <li>The NCF framework, which is both generic and capable of expressing and generalizing matrix factorization, utilized a multi-layer perceptron to imbue the model with non-linear capabilities.</li> <li>With a relatively simple 4-layer neural network, they were able to beat the best matrix factorization algorithms at the time by 5% hit rate on the Movielens and Pinterest benchmark datasets.</li> </ul> <h2 id="deep-and-cross-networks--dcn-2017">Deep and Cross Networks / DCN (2017)</h2> <ul> <li>Wide and Deep has proven the significance of cross features, however it has a huge downside: the cross features need to be manually engineered, which is a tedious process that requires engineering resources, infrastructure, and domain expertise. Cross features à la Wide and Deep are expensive. They don’t scale.</li> <li>The key idea of Deep and Cross Networks (DCN), introduced in a <a href="https://arxiv.org/abs/1708.05123">Wang et al. (2017)</a> by Google is to replace the wide component in Wide and Deep with a “cross neural network,” a neural network dedicated to learning cross features of arbitrarily high order (as opposed to second-order/pairwise features in Wide and Deep Networks). However, note that DCN (similar to <a href="https://aman.ai/recsys/architectures/#deep-factorization-machine--deepfm-2017">DeepFM</a>) learns this in a brute-force manner simply by considering all possible combinations uniformly (i.e., it calculates all pair-wise interactions), while newer implementations such as <a href="https://aman.ai/recsys/architectures/#autoint-2019">AutoInt</a> leverage self-attention to automatically determine the most informative feature interactions, i.e., which feature interactions to pay the most attention to (and which to ignore by setting the attention weights to zero).</li> <li>Similar to Huawei’s <a href="https://aman.ai/recsys/architectures/#deep-factorization-machine--deepfm-2017">DeepFM</a>, introduced in <a href="https://arxiv.org/abs/1703.04247">Guo et al. (2017)</a>, DCN also replaces manual feature engineering in the wide component of Wide and Deep with a dedicated cross neural network that learns cross features. However, unlike DeepFM, the wide component is a cross neural network, instead of a so-called factorization machine layer.</li> <li> <p>DCN was designed to learn explicit and bounded-degree cross features more effectively. It starts with an input layer (typically an embedding layer), followed by a cross network containing multiple cross layers that models explicit feature interactions, and then combines with a deep network that models implicit feature interactions.</p> <ul> <li><strong>Cross Network</strong>: This is the core of DCN, explicitly applying feature crossing at each layer, where the highest polynomial degree increases with layer depth. The cross network layers efficiently capture feature interactions by combining linear transformations, feature interactions, and residual connections. The following figure shows the (i+1)th cross layer.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/i1cross.png" alt=""/></p> <ul> <li> <p><strong>Deep Network</strong>: A traditional deep feed-forward network, consisting of fully-connected layers that use weights, biases, and non-linear activation functions to learn abstract representations and complex patterns in the data.</p> </li> <li> <p><strong>DCN Combination</strong>: The deep and cross networks are combined to form DCN. This can be done either by placing them in parallel, as shown in the figure below.</p> </li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/dc.webp" alt=""/></p> </li> <li>What makes a cross neural network different from a standard MLP? As a reminder, in an MLP, each neuron in the next layer is a linear combination of all neurons in the previous layer, plus a bias term:</li> </ul> <p>xl+1=bl+1+W⋅xl</p> <ul> <li> <p>The Cross Network helps in better generalizing on sparse features by learning explicit bounded-degree feature interactions. This is particularly useful for sparse data, where traditional deep learning models might struggle due to the high dimensionality and lack of explicit feature interaction modeling.</p> </li> <li> <p>By contrast, in the cross neural network the next layer is constructed by forming second-order (i.e., pairwise) combinations of the previous layer’s features:</p> </li> </ul> <p>xl+1=bl+1+xl+xl⋅W⋅xlT</p> <ul> <li>At the input, sparse features are transformed into dense vectors through an embedding layer while dense features are normalized. These processed features are then combined into a single vector x0, which includes the stacked embedding vectors for the sparse features and the normalized dense features. This combined vector is then fed into the network.</li> <li>Hence, a cross neural network of depth L will learn cross features in the form of polynomials of degrees up to L.</li> </ul> <blockquote> <p>The deeper the neural network, the higher the order of interactions that are learned.</p> </blockquote> <ul> <li>The unified wide and cross model architecture is training jointly with mean squared error (MSE) as it’s loss function.</li> <li> <p>For model evaluation, the Root Mean Squared Error (RMSE, the lower the better) is reported per <a href="https://www.tensorflow.org/recommenders/examples/dcn">TensorFlow: Deep &amp; Cross Network (DCN)</a>.</p> </li> <li>The Deep and Cross Network (DCN) introduces a novel approach to handling feature interactions and dealing with sparse features. Let’s break down how DCN accomplishes these tasks:</li> </ul> <h3 id="forming-higher-order-feature-interactions">Forming Higher-Order Feature Interactions</h3> <ul> <li><strong>Mechanism of the Cross Network</strong>: <ul> <li> <p>In a standard MLP, each neuron in a layer is a linear combination of all neurons from the previous layer. As delineated above, this is mathematically expressed as,</p> <p>xl+1=bl+1+W⋅xl</p> <ul> <li>where xl is the input from the previous layer, W is the weight matrix, and bl+1 is the bias.</li> </ul> </li> <li> <p>However, in the Cross Network of DCN, the idea is to explicitly form higher-order interactions of features.</p> </li> </ul> </li> <li><strong>Second-Order Combinations</strong>: <ul> <li> <p>In the Cross Network, the next layer is created by incorporating second-order (i.e., pairwise) combinations of the previous layer’s features. As delineated above, this is mathematically expressed as,</p> <p>xl+1=bl+1+xl+xl⋅W⋅xlT</p> </li> <li>This approach allows the network to automatically learn complex feature interactions (cross features) that are higher than first-order, which would be impossible in a standard MLP without manual feature engineering. Specifically, in a standard MLP, feature interactions aren’t explicitly learned unless they’re manually engineered, meaning that the model would rely on domain experts to create new features that represent interactions between the original inputs. This is both labor-intensive and non-scalable.</li> <li>However, in DCN’s Cross Network, these feature interactions—specifically higher-order ones—are learned automatically by the model itself. This removes the need for manual feature engineering, allowing the model to capture complex relationships between features more effectively and without human intervention, especially in high-dimensional or sparse data scenarios.</li> </ul> </li> </ul> <h3 id="handling-sparse-features-through-embedding-layers">Handling Sparse Features Through Embedding Layers</h3> <ul> <li> <p><strong>Sparse to Dense Transformation</strong>: Neural networks generally work better with dense input data. However, in many real-world applications, features are often sparse (like categorical data). DCN addresses this challenge by transforming sparse features into dense vectors through an embedding layer for dense embedding generation.</p> </li> <li> <p><strong>Embedding Process</strong>: Dense embedding generation enables sparse, high-dimensional data (like one-hot encoded vectors) are converted into a lower-dimensional, continuous, and dense vector. Each unique category in the sparse feature is mapped to a dense vector, and these vectors are learned during the training process. This transformation is crucial because it enables the network to work with a dense representation of the data, which is more efficient and effective for learning complex patterns.</p> </li> </ul> <h3 id="explicit-feature-crossing-and-polynomial-degree">Explicit Feature Crossing and Polynomial Degree</h3> <ul> <li> <p><strong>Explicit Feature Crossing</strong>: The Cross Network in DCN explicitly handles feature crossing at each layer, directly modeling interactions between different features instead of relying on the deep network to implicitly capture these interactions.</p> </li> <li> <p><strong>Increasing Polynomial Degree with Depth</strong>: As the Cross Network’s depth increases, the polynomial degree of feature interactions grows, allowing the model to capture more complex interactions (higher-order feature combinations).</p> </li> </ul> <blockquote> <p>Essentially, DCN learns polynomials of features (i.e., higher order features), where the degree increases with the network’s depth.</p> </blockquote> <ul> <li> <p><strong>Bounded-Degree Cross Features</strong>: The design of the Cross Network controls the degree of these polynomials through the network depth. This helps prevent excessive complexity, avoiding overfitting and ensuring computational efficiency.</p> </li> <li> <p><strong>Handling Sparse Features</strong>: DCN’s Cross Network forms higher-order feature interactions by explicitly crossing features at each layer while embedding sparse features into dense vectors, making them suitable for neural network processing. This enables automatic and efficient learning of complex feature interactions without manual feature engineering.</p> </li> <li> <p><strong>Integrating Outputs</strong>: The outputs from the Cross Network and the Deep Network are concatenated to combine their strengths.</p> </li> <li> <p><strong>Final Prediction</strong>: The concatenated vector is fed into a logits layer, which combines explicit feature interactions and deep learned representations to make the final prediction (e.g., for classification tasks).</p> </li> </ul> <h3 id="input-and-output-to-each-component">Input and Output to Each Component</h3> <ul> <li><strong>Input to Cross and Deep Networks:</strong> Both networks take the same input vector, which is a combination of dense embeddings (from sparse features) and normalized dense features. Put siomply, embedded sparse features are concatenated with dense features and offered as input to both networks.</li> <li> <p><strong>Output:</strong> The outputs of both networks are combined in the Combination Layer for the final model output.</p> </li> <li>Based on the paper, the architecture and composition of each layer in the Cross and Deep Networks of the DCN are as follows:</li> </ul> <h3 id="cross-network-layers">Cross Network Layers</h3> <ul> <li>Each layer in the Cross Network is defined by the following formula: xl+1=x0xlTwl+bl+xl <ul> <li><strong>Inputs and Outputs</strong>: xl and xl+1 are the outputs from the lth and (l+1)th cross layers respectively, represented as column vectors.</li> <li><strong>Weight and Bias Parameters</strong>: Each layer has its own weight (wl) and bias (bl) parameters, which are learned during training.</li> <li><strong>Feature Crossing Function</strong>: The feature crossing function is represented by f(xl,wl,bl), and it is designed to fit the residual of xl+1−xl. This function captures interactions between the features.</li> <li><strong>Residual Connection</strong>: Each layer adds back its input after the feature crossing, which helps in preserving the information and building upon the previous layer’s output.</li> </ul> </li> </ul> <h3 id="deep-network-layers">Deep Network Layers</h3> <ul> <li>Each layer in the Deep Network is structured as a standard fully-connected layer and is defined by the following formula: hl+1=f(wlhl+bl) <ul> <li><strong>Inputs and Outputs</strong>: hl and hl+1 are the lth and (l+1)th hidden layers’ outputs respectively.</li> <li><strong>Weight and Bias Parameters</strong>: Similar to the cross layer, each deep layer has its own weight matrix (wl) and bias vector (bl).</li> <li><strong>Activation Function</strong>: The function f(⋅) is typically a non-linear activation function, such as ReLU (Rectified Linear Unit), which introduces non-linearity into the model, allowing it to learn complex patterns in the data.</li> </ul> </li> </ul> <h3 id="results-1">Results</h3> <ul> <li>Compared to a model with just the deep component, DCN has a 0.1% statistically significant lower logloss on the Criteo display ads benchmark dataset. And that’s without any manual feature engineering, as in Wide and Deep! (It would have been nice to see a comparison between DCN and Wide and Deep. However, the authors of DCN did not have a good method to manually create cross features for the Criteo dataset, and hence skipped this comparison.)</li> <li>The DCN architecture includes a cross network component that captures cross-feature interactions. It combines a deep network with cross layers, allowing the model to learn explicit feature interactions and capture non-linear relationships between features.</li> </ul> <h3 id="summary-4">Summary</h3> <ul> <li>DCN showed that we can get even more performance gains by replacing manual engineering of cross features with an algorithmic approach that automatically creates all possible feature crosses up to any arbitrary order. Compared to Wide &amp; Deep, DCN achieved 0.1% lower logloss on the Criteo display ads benchmark dataset.</li> <li><strong>Pros:</strong> Captures explicit high-order feature interactions and non-linear relationships through cross layers, allowing for improved modeling of complex patterns.</li> <li><strong>Cons:</strong> <ul> <li>DCN creates feature crosses in a brute-force way, simply by considering all possible combinations. This is not only inefficient, it could also create feature crosses that aren’t helpful at all, and just make the model overfit.</li> <li>More complex than simple feed-forward networks.</li> <li>May not perform well on tasks where feature interactions aren’t important.</li> <li>Increased model complexity, potential overfitting on sparse data.</li> </ul> </li> <li><strong>Use case:</strong> Useful for tasks where high-order feature interactions are critical, such as CTR prediction and ranking tasks.</li> <li><strong>Example Use Case:</strong> Advertising platforms where understanding the interactions between user characteristics and ad features is essential for personalized ad targeting.</li> <li><strong>Phase:</strong> Ranking, Final Ranking.</li> <li><strong>Recommendation Workflow:</strong> The deep and cross architecture is typically applied in the ranking phase and the final ranking phase. The deep and cross network captures explicit feature interactions and non-linear relationships, enabling accurate ranking of candidate items based on user preferences. It contributes to the final ranking of candidate items, leveraging its ability to model complex patterns and interactions.</li> </ul> <h2 id="autoint-2019">AutoInt (2019)</h2> <ul> <li>Proposed in <a href="https://arxiv.org/abs/1810.11921">AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks</a> by Song et al. from from Peking University and Mila-Quebec AI Institute, and HEC Montreal in CIKM 2019.</li> <li>The paper introduces AutoInt (short for “automated feature interaction learning”), a novel method for efficiently learning high-order feature interactions in an automated way. Developed to address the inefficiencies and overfitting problems in existing models like DCN and DeepFM, which create feature crosses in a brute-force manner, AutoInt leverages self-attention to determine the most informative feature interactions.</li> <li>AutoInt employs a multi-head self-attentive neural network with residual connections, designed to explicitly model feature interactions in a 16-dimensional embedding space. It overcomes the limitations of prior models by focusing on relevant feature combinations, avoiding unnecessary and unhelpful feature crosses.</li> <li><strong>Processing Steps</strong>: <ol> <li><strong>Input Layer</strong>: Represents user profiles and item attributes as sparse vectors.</li> <li><strong>Embedding Layer</strong>: Projects each feature into a 16-dimensional space.</li> <li><strong>Interacting Layer</strong>: Utilizes several multi-head self-attention layers to automatically identify the most informative feature interactions. The attention mechanism is based on dot product for its effectiveness in capturing feature interactions.</li> <li><strong>Output Layer</strong>: Uses the learned feature interactions for CTR estimation.</li> </ol> </li> <li>The goal of AutoInt is to map the original sparse and high-dimensional feature vector into low-dimensional spaces and meanwhile model the high-order feature interactions. As shown in the below figure, AutoInt takes the sparse feature vector x as input, followed by an embedding layer that projects all features (i.e., both categorical and numerical features) into the same low-dimensional space. Next, embeddings of all fields are fed into a novel interacting layer, which is implemented as a multi-head self-attentive neural network. For each interacting layer, high-order features are combined through the attention mechanism, and different kinds of combinations can be evaluated with the multi-head mechanisms, which map the features into different subspaces. By stacking multiple interacting layers, different orders of combinatorial features can be modeled. The output of the final interacting layer is the low-dimensional representation of the input feature, which models the high-order combinatorial features and is further used for estimating the clickthrough rate through a sigmoid function. The figure below from the paper shows an overview of AutoInt.</li> </ul> <p><img src="https://aman.ai/images/papers/autoint.png" alt=""/></p> <ul> <li>The figure below from the paper illustrates the input and embedding layer, where both categorical and numerical fields are represented by low-dimensional dense vectors.</li> </ul> <p><img src="https://aman.ai/images/papers/autoint2.jpg" alt=""/></p> <ul> <li>AutoInt demonstrates superior performance over competitors like Wide and Deep and DeepFM on benchmark datasets like MovieLens and Criteo, thanks to its efficient handling of feature interactions.</li> <li>The technical innovations in AutoInt consist of: (i) introduction of multi-head self-attention to learn which cross features really matter, replacing the brute-force generation of all possible feature crosses, and (ii) the model’s ability to learn important feature crosses such as <code class="language-plaintext highlighter-rouge">Genre-Gender</code>, <code class="language-plaintext highlighter-rouge">Genre-Age</code>, and <code class="language-plaintext highlighter-rouge">RequestTime-ReleaseTime</code>, which are crucial for accurate CTR prediction.</li> <li>AutoInt showcases efficiency in processing large-scale, sparse, high-dimensional data, with a stack of 3 attention layers, each having 2 heads. The attention mechanism improves model explainability by highlighting relevant feature interactions, as exemplified in the attention matrix learned on the MovieLens dataset.</li> <li>AutoInt addresses the need for a model that is both powerful in capturing complex interactions and interpretable in its recommendations, without the inefficiency and overfitting issues seen in models that generate feature crosses in a brute-force manner.</li> </ul> <h3 id="summary-5">Summary</h3> <ul> <li>The primary concept in DCN and DeepFM involved generating feature crosses through brute-force methods by considering all possible combinations. This approach is not only inefficient but also risks creating feature crosses that offer no meaningful value, leading to model overfitting.</li> <li>What is required, therefore, is a method to automatically identify which feature interactions are significant and which can be disregarded. The solution, as you might expect, is self-attention.</li> </ul> <blockquote> <p>AutoInt introduces the concept of multi-head self-attention within recommender systems: instead of generating all possible pairwise feature crosses through brute force, attention mechanisms are employed to discern which feature crosses are truly relevant.</p> </blockquote> <ul> <li>This was the key innovation behind AutoInt, short for “automated feature interaction learning,” as proposed by <a href="https://arxiv.org/abs/1810.11921">Song et al. (2019)</a> from Peking University, China. Specifically, the authors first project each feature into a 16-dimensional embedding space, and then pass these embeddings through a stack of multi-head self-attention layers, which automatically identify the most informative feature interactions. The inputs to the key, query, and value matrices are simply the list of all feature embeddings, and the attention function is a dot product, chosen for its simplicity and effectiveness in capturing feature interactions.</li> <li>Although this may sound complex, there is no real mystery—just a series of matrix multiplications. For instance, the attention matrix learned by one of the attention heads in AutoInt for the MovieLens benchmark dataset is shown below:</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/aint2.webp" alt=""/></p> <ul> <li>The model learns that feature crosses such as <code class="language-plaintext highlighter-rouge">Genre-Gender</code>, <code class="language-plaintext highlighter-rouge">Genre-Age</code>, and <code class="language-plaintext highlighter-rouge">RequestTime-ReleaseTime</code> are important, highlighted in green. This makes sense, as men and women typically have different movie preferences, and children often prefer different films compared to adults. The <code class="language-plaintext highlighter-rouge">RequestTime-ReleaseTime</code> feature cross captures the movie’s freshness at the time of the training instance.</li> <li>By utilizing a stack of three attention layers, each with two heads, the authors of AutoInt were able to outperform several competitors, including Wide and Deep and DeepFM, on the MovieLens and Criteo benchmark datasets.</li> </ul> <h2 id="dlrm-2019">DLRM (2019)</h2> <ul> <li>Let’s fast-forward by a year to Meta’s DLRM (“deep learning for recommender systems”) architecture, proposed in <a href="https://aman.ai/recsys/architectures/](https://arxiv.org/abs/1906.00091)">Naumov et al. (2019)</a>, another important milestone in recommender system modeling.</li> <li>This paper by Naumov et al. from Facebook in 2019 introduces the DLRM (deep learning for recommender systems) architecture, a significant development in recommender system modeling, which was open-sourced in both PyTorch and Caffe2 frameworks.</li> <li>Contrary to the “deep learning” part in it’s name, DLRM represents a progression from the DeepFM architecture, maintaining the FM (factorization machine) component while discarding the deep neural network part. The fundamental hypothesis of DLRM is that interactions are paramount in recommender systems, which can be modeled using shallow MLPs (and complex deep learning components are thus not essential).</li> <li>The DLRM model handles continuous (dense) and categorical (sparse) features that describe users and products. DLRM exercises a wide range of hardware and system components, such as memory capacity and bandwidth, as well as communication and compute resources as shown in the figure below from the paper.</li> </ul> <p><img src="https://aman.ai/images/papers/dlrm.jpg" alt=""/></p> <ul> <li>The figure below from the paper shows the overall structure of DLRM.</li> </ul> <p><img src="https://aman.ai/images/papers/dlrm2.jpg" alt=""/></p> <ul> <li>DLRM uniquely handles both continuous (dense) and categorical (sparse) features that describe users and products, projecting them into a shared embedding space. These features are then passed through MLPs before and after computing pairwise feature interactions (dot products). This method significantly differs from other neural network-based recommendation models in its explicit computation of feature interactions and treatment of each embedded feature vector as a single unit, contrasting with approaches like Deep and Cross which consider each element in the feature vector separately.</li> </ul> <blockquote> <p>DLRM shows that interactions are all you need: it’s akin to using just the FM component of DeepFM but with MLPs added before and after the interactions to increase modeling capacity.</p> </blockquote> <ul> <li>The architecture of DLRM includes multiple MLPs, which are added to increase the model’s capacity and expressiveness, enabling it to model more complex interactions. This aspect is critical as it allows for fitting data with higher precision, given adequate parameters and depth in the MLPs.</li> <li>Compared to other DL-based approaches to recommendation, DLRM differs in two ways. First, it computes the feature interactions explicitly while limiting the order of interaction to pairwise interactions. Second, DLRM treats each embedded feature vector (corresponding to categorical features) as a single unit, whereas other methods (such as Deep and Cross) treat each element in the feature vector as a new unit that should yield different cross terms. These design choices help reduce computational/memory cost while maintaining competitive accuracy.</li> <li>A key contribution of DLRM is its specialized parallelization scheme, which utilizes model parallelism on the embedding tables to manage memory constraints and exploits data parallelism in the fully-connected layers for computational scalability. This approach is particularly effective for systems with diverse hardware and system components, like memory capacity and bandwidth, as well as communication and compute resources.</li> <li>The paper demonstrates that DLRM surpasses the performance of the DCN model on the Criteo dataset, validating the authors’ hypothesis about the predominance of feature interactions. Moreover, DLRM has been characterized for its performance on the Big Basin AI platform, proving its utility as a benchmark for future algorithmic experimentation, system co-design, and benchmarking in the field of deep learning-based recommendation models.</li> <li><a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/">Facebook AI</a> post.</li> </ul> <h3 id="summary-6">Summary</h3> <ul> <li>The key idea behind DLRM is to take the approach from DeepFM but only keep the FM part, not the Deep part, and expand on top of that. The underlying hypothesis is that the interactions of features are really all that matter in recommender systems. “Interactions are all you need!”, you may say.</li> <li>The deep component is not really needed. DLRM uses a bunch of MLPs to model feature interactions. Under the hood, DLRM projects all sparse and dense features into the same embedding space, passes them through MLPs (blue triangles in the above figure), computes all pairs of feature interactions (the cloud), and finally passes this interaction signal through another MLP (the top blue triangle). The interactions here are simply dot products, just like in DeepFM.</li> <li>The key difference to the DeepFM’s “FM” though is the addition of all these MLPs, the blue triangles. Why do we need those? Because they’re adding modeling capacity and expressiveness, allowing us to model more complex interactions. After all, one of the most important rules in neural networks is that given enough parameters, MLPs with sufficient depth and width can fit data to arbitrary precision!</li> <li>In the paper, the authors show that DLRM beats DCN on the Criteo dataset. The authors’ hypothesis proved to be true. Interactions, it seems, may really be all you need.</li> </ul> <h2 id="dcn-v2-2020">DCN V2 (2020)</h2> <ul> <li>Proposed in <a href="https://arxiv.org/abs/2008.13535">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</a> by Wang et al. from Google, DCN V2 is an enhanced version of the Deep &amp; Cross Network (DCN), designed to effectively learn feature interactions in large-scale learning to rank (LTR) systems.</li> <li>The paper addresses DCN’s limited expressiveness in learning predictive feature interactions, especially in web-scale systems with extensive training data.</li> <li>DCN V2 is focused on the efficient and effective learning of predictive feature interactions, a crucial aspect of applications like search recommendation systems and computational advertising. It tackles the inefficiency of traditional methods, including manual identification of feature crosses and reliance on deep neural networks (DNNs) for higher-order feature crosses.</li> <li>The embedding layer in DCN V2 processes both categorical (sparse) and dense features, supporting various embedding sizes, essential for industrial-scale applications with diverse vocabulary sizes.</li> <li>The core of DCN V2 is its cross layers, which explicitly create feature crosses. These layers are based on a base layer with original features, utilizing learned weight matrices and bias vectors for each cross layer.</li> <li>The figure below from the paper visualizes a cross layer.</li> </ul> <p><img src="https://aman.ai/images/papers/DCNV2_2.jpg" alt=""/></p> <ul> <li>As shown in the figure below, DCN V2 employs a novel architecture that combines a cross network with a deep network. This combination is realized through two architectures: a stacked structure where the cross network output feeds into the deep network, and a parallel structure where outputs from both networks are concatenated. The cross operation in these layers is represented as xl+1=x0⊙(Wlxl+bl)+xl.</li> </ul> <p><img src="https://aman.ai/images/papers/DCNV2_1.jpg" alt=""/></p> <blockquote> <p>A key feature of DCN V2 is the use of low-rank techniques to approximate feature crosses in a subspace, improving performance and reducing latency. This is further enhanced by a Mixture-of-Experts architecture, which decomposes the matrix into multiple smaller sub-spaces aggregated through a gating mechanism.</p> </blockquote> <ul> <li>DCN V2 demonstrates superior performance in extensive studies and comparisons with state-of-the-art algorithms on benchmark datasets like Criteo and MovieLens-1M. It offers significant gains in offline accuracy and online business metrics in Google’s web-scale LTR systems.</li> <li>The paper also delves into polynomial approximation from both bitwise and feature-wise perspectives, illustrating how DCN V2 creates feature interactions up to a certain order with a given number of cross layers, thus being more expressive than the original DCN.</li> </ul> <h3 id="dcn-vs-dcn-v2">DCN vs. DCN V2</h3> <ul> <li>DCN focuses on explicit low-order feature interaction modeling through cross networks but faces limitations in scalability and memory efficiency as interaction complexity increases.</li> <li>DCN V2 overcomes the limitations of DCN by enhancing the scalability and efficiency by incorporating low-rank techniques and Mixture-of-Experts architectures. These enhancements make DCN V2 suitable for large-scale, real-time applications with significant memory and computational optimizations.</li> </ul> <h3 id="low-rank-techniques-in-dcn-v2">Low-Rank Techniques in DCN V2</h3> <h4 id="dcn-limitations-in-scalability">DCN Limitations in Scalability</h4> <ul> <li>DCN captures nonlinear interactions using a Cross Network, where interaction complexity is tied to the number of cross layers. As the network depth increases, the number of parameters grows significantly, leading to inefficiencies in handling higher-order feature interactions, especially in large-scale systems.</li> <li> <p>The Cross Network formula in DCN is as follows:</p> <p>xl+1=x0⋅(Wl⋅xl+bl)+xl</p> <ul> <li>Here, Wl and bl represent the weight matrix and bias vector for layer l, while x0 is the original input, and xl is the input to the current cross layer.</li> </ul> </li> <li>While this structure allows DCN to model nonlinear interactions between features, its scalability is constrained by the computational cost of increasing parameters, making it less efficient for modeling arbitrary high-order interactions.</li> </ul> <h4 id="low-rank-approximations">Low-Rank Approximations</h4> <ul> <li> <p>DCN V2 addresses these scalability issues with low-rank approximations in the Cross Network. The weight matrix Wl is factorized to reduce computational complexity:</p> <p>Wl≈Ul⋅VlT</p> <ul> <li>where Ul and Vl are lower-dimensional matrices.</li> </ul> </li> <li> <p>This factorization reduces computational overhead from O(d2) to O(d×r), where r is the rank, significantly improving efficiency for large datasets.</p> </li> <li> <p>Low-rank approximations enable DCN V2 to model higher-order feature interactions with lower memory and computational requirements, making it better suited for web-scale, real-time systems.</p> </li> </ul> <h3 id="mixture-of-experts-architecture">Mixture-of-Experts Architecture</h3> <h4 id="enhancing-expressiveness-with-mixture-of-experts">Enhancing Expressiveness with Mixture-of-Experts</h4> <ul> <li> <p>DCN V2 introduces a Mixture-of-Experts architecture to enhance its expressiveness. This approach dynamically selects which “expert” network to activate based on input, allowing different subspaces to specialize in capturing specific feature interactions.</p> <p>xl+1=∑i=1KGi(xl)⋅Ei(xl)+xl</p> <ul> <li>where, Gi(xl) is a gating function that determines which expert Ei(xl) to activate.</li> </ul> </li> <li> <p>By assigning specific feature interactions to dedicated experts, the Mixture-of-Experts framework allows DCN V2 to model complex, higher-order interactions without significantly increasing computational cost.</p> </li> </ul> <h4 id="advantages-of-mixture-of-experts">Advantages of Mixture-of-Experts</h4> <ul> <li>The architecture ensures that feature crosses are handled flexibly and efficiently. Unlike DCN, where feature interactions are bounded by the fixed structure of the Cross Network, DCN V2 leverages the gating mechanism to adaptively allocate computational resources to relevant subspaces.</li> <li>This dynamic expert selection enables DCN V2 to scale effectively for industrial applications, where both accuracy and speed are critical.</li> </ul> <h3 id="model-structure-parallel-dcn-vs-stacked-and-parallel-dcn-v2">Model Structure: Parallel (DCN) vs. Stacked and Parallel (DCN V2)</h3> <ul> <li>DCN V2 builds on the strengths of DCN by making the cross network more expressive and scalable, particularly through low-rank techniques and flexible model architectures. This makes DCN V2 better suited for large-scale, web-based recommendation systems while maintaining efficiency.</li> </ul> <h4 id="dcn">DCN</h4> <ul> <li>The model structure in DCN consists of two parallel networks: <ul> <li><strong>Deep Network (DNN)</strong>: The DNN is responsible for capturing implicit feature interactions, which are complex and nonlinear. The deep network uses multiple fully connected layers, allowing the model to learn intricate relationships between features that are not easily captured by simple feature crossing.</li> <li><strong>Cross Network</strong>: This part of the model is designed to capture explicit feature interactions up to a fixed degree (bounded by the number of layers in the cross network). The cross layers systematically apply feature crosses at each level, combining the original features with the output of the previous cross layer to form higher-degree feature interactions. The cross network is particularly efficient in modeling lower-order feature crosses without the need for manual feature engineering.</li> <li><strong>Parallel Structure</strong>: In DCN, both the DNN and cross network operate in parallel. The input features are passed through both networks, and their respective outputs are concatenated in the final logits layer for prediction. This parallel approach is effective at capturing both implicit and explicit interactions in the data, allowing DCN to perform well without requiring exhaustive feature engineering.</li> <li><strong>Drawback</strong>: However, this structure might be limiting in cases where the sequential dependency between explicit and implicit features is important. The model does not allow for deep interactions between the cross network’s explicit crosses and the deep network’s implicit learning, as both networks run independently.</li> </ul> </li> </ul> <h4 id="dcn-v2">DCN V2</h4> <ul> <li>DCN V2 enhances the flexibility of the model by introducing two ways of combining the deep network and cross network: stacked and parallel structures. <ul> <li><strong>Stacked Structure</strong>: In the stacked architecture, the cross network is applied first to generate explicit feature crosses, and the output of the cross network is then fed into the deep network to learn higher-order implicit interactions. This stacked approach allows the deep network to build upon the explicitly crossed features, enabling a richer, more nuanced learning process. The stacked structure is especially useful in situations where the interactions between explicit feature crosses and deeper, more implicit interactions need to be modeled sequentially. By first capturing simpler, bounded-degree feature crosses in the cross network, the deep network can then focus on learning more complex, high-order interactions that depend on these explicit crosses.</li> <li><strong>Parallel Structure</strong>: Similar to the original DCN, DCN V2 also supports a parallel structure where both the deep network and cross network operate simultaneously. In this approach, the features are processed by both networks concurrently, and their outputs are concatenated for final prediction. This structure is particularly useful for datasets where implicit and explicit interactions are relatively independent, and combining them at the end provides a comprehensive understanding of the data.</li> <li><strong>Combination Layer</strong>: In both the stacked and parallel setups, DCN V2 uses a combination layer to aggregate the outputs of the cross network and deep network before passing them to the final output layer (often a logits layer). Depending on the architecture chosen, the combination can take the form of either a sequential concatenation (in the stacked case) or a direct concatenation of both network outputs (in the parallel case).</li> <li><strong>Flexibility and Adaptation</strong>: This added flexibility enables DCN V2 to better adapt to different types of datasets and tasks. For instance, if the dataset contains feature interactions that are primarily simple and can be captured by bounded-degree crosses, the stacked structure allows the model to first handle these simpler interactions and then apply deep learning for more complex patterns. Alternatively, if the dataset benefits from learning both types of interactions concurrently, the parallel structure can be used. This versatility makes DCN V2 highly customizable and better suited for diverse real-world applications.</li> <li><strong>Efficiency</strong>: Although the stacked structure adds more depth and complexity to the model, DCN V2 remains computationally efficient by leveraging low-rank techniques and Mixture-of-Experts in the cross layers, ensuring that the additional depth does not significantly increase computational cost or inference time.</li> </ul> </li> <li><strong>Stacked vs. Parallel</strong>: The choice between stacked and parallel structures in DCN V2 depends on the specific requirements of the task at hand: <ul> <li>The stacked structure is more suited for tasks where feature crosses learned by the cross network can directly inform and enrich the implicit interactions learned by the deep network. This sequential dependency enhances the ability to capture more complex feature relationships that depend on simpler interactions.</li> <li>The parallel structure works better for tasks where the explicit and implicit interactions are more independent and do not require one to build on the other. This allows for concurrent learning of different types of interactions, potentially improving the speed and efficiency of learning.</li> </ul> </li> </ul> <h3 id="summary-of-key-differences">Summary of Key Differences</h3> <ul> <li>Here’s a table that provides a detailed comparison, incorporating the technical aspects of both models, and highlights how DCN V2 overcomes the limitations of DCN to provide a more scalable, efficient, and production-ready solution.</li> </ul> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>DCN</strong></th> <th><strong>DCN V2</strong></th> </tr> </thead> <tbody> <tr> <td>Cross Features’ Expressiveness</td> <td>Captures nonlinear interactions through cross layers, with expressiveness limited by network depth.</td> <td>Enhanced expressiveness with low-rank techniques and Mixture-of-Experts for higher-order interactions.</td> </tr> <tr> <td>Scalability</td> <td>Limited scalability as parameter complexity increases with deeper layers.</td> <td>Improved scalability using low-rank factorization, optimizing for large-scale datasets.</td> </tr> <tr> <td>Efficiency</td> <td>Efficiency decreases with growing interaction complexity due to higher computational cost.</td> <td>Reduces complexity from O(d2) to O(d×r) using low-rank approximations, improving efficiency.</td> </tr> <tr> <td>Model Structure</td> <td>Parallel structure where Cross Network and DNN run independently.</td> <td>Offers both stacked and parallel structures, enabling richer interaction modeling with flexibility.</td> </tr> <tr> <td>Handling Higher-Order Interactions</td> <td>Limited by the depth of the Cross Network, with increasing computational overhead.</td> <td>Capable of modeling complex, higher-order interactions efficiently through Mixture-of-Experts.</td> </tr> <tr> <td>Flexibility</td> <td>Fixed structure with limited adaptability to different tasks or datasets.</td> <td>Flexible with stacked and parallel setups, adaptable to various datasets and interaction complexities.</td> </tr> <tr> <td>Suitable Applications</td> <td>Best suited for smaller systems with limited interaction complexities.</td> <td>Optimized for large-scale, real-time systems, with memory and computational efficiency.</td> </tr> </tbody> </table> <h3 id="summary-7">Summary</h3> <ul> <li>Proposed in <a href="https://arxiv.org/abs/2008.13535">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems</a> by Wang et al. from Google. An enhanced version of the Deep &amp; Cross Network (DCN), DCN V2, effectively learns feature interactions in large-scale learning to rank (LTR) systems.</li> <li>DCN V2 addresses the limitations of the original DCN, particularly in web-scale systems with vast amounts of training data, where DCN exhibited limited expressiveness in its cross network for learning predictive feature interactions.</li> <li>The paper focuses on efficient and effective learning of predictive feature interactions, crucial in applications like search recommendation systems and computational advertising. Traditional approaches often involve manual identification of feature crosses or rely on deep neural networks (DNNs), which can be inefficient for higher-order feature crosses.</li> <li>DCN V2 includes an embedding layer that processes both categorical (sparse) and dense features. It supports different embedding sizes, crucial for industrial-scale applications with varying vocabulary sizes.</li> <li>The core of DCN V2 is its cross layers, which create explicit feature crosses. These layers are built upon a base layer containing original features and use learned weight matrices and bias vectors for each cross layer.</li> <li>DCN V2’s effectiveness is demonstrated through extensive studies and comparisons with state-of-the-art algorithms on benchmark datasets like Criteo and MovieLens-1M. It outperforms these algorithms and offers significant offline accuracy and online business metrics gains in Google’s web-scale LTR systems.</li> <li>In summary, the key change in DCN V2’s cross network that enhances its expressiveness is the incorporation of low-rank matrices in the cross layers. This approach optimizes the computation of feature interactions, making the network more efficient and scalable, especially for complex, high-dimensional datasets. The use of low-rank matrices allows the network to capture complex feature interactions (including higher-order interactions) more effectively without the computational burden of full-rank operations.</li> </ul> <h2 id="dhen-2022">DHEN (2022)</h2> <ul> <li>Learning feature interactions is important to the model performance of online advertising services. As a result, extensive efforts have been devoted to designing effective architectures to learn feature interactions. However, they observe that the practical performance of those designs can vary from dataset to dataset, even when the order of interactions claimed to be captured is the same. That indicates different designs may have different advantages and the interactions captured by them have non-overlapping information.</li> <li>Proposed in <a href="https://arxiv.org/abs/2203.11014">DHEN: A Deep and Hierarchical Ensemble Network for Large-Scale Click-Through Rate Prediction</a>, this paper by Zhang et al. from Meta introduces DHEN (Deep and Hierarchical Ensemble Network), a novel architecture designed for large-scale Click-Through Rate (CTR) prediction. The significance of DHEN lies in its ability to learn feature interactions effectively, a crucial aspect in the performance of online advertising services. Recognizing that different interaction models offer varying advantages and capture non-overlapping information, DHEN integrates a hierarchical ensemble framework with diverse interaction modules, including AdvancedDLRM, self-attention, Linear, Deep Cross Net, and Convolution. These modules enable DHEN to learn a hierarchy of interactions across different orders, addressing the limitations and variable performance of previous models on different datasets.</li> <li>The following figure from the paper shows a two-layer two-module hierarchical ensemble (left) and its expanded details (right). A general DHEN can be expressed as a mixture of multiple high-order interactions. Dense feature input for the interaction modules are omitted in this figure for clarity.</li> </ul> <p><img src="https://aman.ai/images/papers/DHEN.jpg" alt=""/></p> <ul> <li>In CTR prediction tasks, the feature inputs usually contain discrete categorical terms (sparse features) and numerical values (dense features). DHEN uses the same feature processing layer in DLRM, which is shown in the figure below. The sparse lookup tables map the categorical terms to a list of “static” numerical embeddings. Specifically, each categorical term is assigned a trainable d-dimensional vector as its feature representation. On the other hand, the numerical values are processed by dense layers. Dense layers compose of several Multi-layer Perceptions (MLPs) from which an output of a d-dimensional vector is computed. After a concatenation of the output from sparse lookup table and dense layer, the final output of the feature processing layer X0∈Rd×m can be expressed as X0=(x01,x02,…,x0m), where m is the number of the output embeddings and d is the embedding dimension.</li> </ul> <p><img src="https://aman.ai/images/papers/DHEN2.jpg" alt=""/></p> <ul> <li>A key technical advancement in this work is the development of a co-designed training system tailored for DHEN’s complex, multi-layer structure. This system introduces the Hybrid Sharded Data Parallel, a novel distributed training paradigm. This approach not only caters to the deeper structure of DHEN but also significantly enhances training efficiency, achieving up to 1.2x better throughput compared to existing models.</li> <li>Empirical evaluations on large-scale datasets for CTR prediction tasks have demonstrated the effectiveness of DHEN. The model showed an improvement of 0.27% in Normalized Entropy (NE) gain over state-of-the-art models, underlining its practical effectiveness. The paper also discusses improvements in training throughput and scaling efficiency, highlighting the system-level optimizations that make DHEN particularly adept at handling large and complex datasets in the realm of online advertising.n the Normalized Entropy (NE) of prediction and 1.2x better training throughput than state-of-the-art baseline, demonstrating their effectiveness in practice.</li> </ul> <h3 id="summary-8">Summary</h3> <ul> <li>In contrast to DCN, the feature interactions in DLRM are restricted to second-order (i.e., pairwise) interactions only: they are simply dot products of all pairs of embeddings. Referring back to the movie example (with features such as user, movie, actors, director), second-order interactions would include user-movie, user-actor, user-director, movie-actor, movie-director, and actor-director. A third-order interaction would involve combinations like user-movie-director, actor-actor-user, director-actor-user, and so forth.</li> <li>For instance, certain users may favor movies directed by Steven Spielberg that feature Tom Hanks, necessitating a cross feature to account for such preferences. Unfortunately, standard DLRM does not accommodate such interactions, representing a significant limitation.</li> <li>This is where DHEN, short for “Deep Hierarchical Ensemble Network”, comes in. Proposed in <a href="https://arxiv.org/abs/2203.11014">Zhang et al. (2022)</a>, the core concept of DHEN is to establish a “hierarchy” of cross features that deepens with the number of DHEN layers, allowing for third, fourth, and even higher-order interactions.</li> <li>At a high level, DHEN operates as follows: suppose we have two input features entering DHEN, which we denote as A and B. A 1-layer DHEN module would generate an entire hierarchy of cross features, incorporating both the features themselves and second-order interactions, such as:</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;span&gt;A&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;B&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;
</code></pre></div></div> <ul> <li>where, “x” does not signify a singular interaction but represents a combination of the following five interactions: <ul> <li>dot product,</li> <li>self-attention (similar to AutoInt),</li> <li>convolution,</li> <li>linear: y=Wx, or</li> <li>the cross module from DCN.</li> </ul> </li> <li>Adding another layer introduces further complexity:</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;span&gt;A&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxAxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxAxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxBxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;AxBxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;
&lt;span&gt;B&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxBxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxBxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxAxB&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;BxAxA&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;
</code></pre></div></div> <ul> <li>In this case, “x” represents one of five interactions, culminating in 62 distinct signals. DHEN is indeed formidable, and its computational complexity, due to its recursive nature, is quite challenging. To manage this complexity, the authors of the DHEN paper developed a new distributed training approach called “Hybrid Sharded Data Parallel”, which delivers a 1.2X increase in throughput compared to the then state-of-the-art distributed learning algorithm.</li> <li>Most notably, DHEN proves effective: in their experiments on internal click-through rate data, the authors report a 0.27% improvement in NE compared to DLRM when using a stack of 8 DHEN layers. While such a seemingly small improvement in NE might raise questions about whether it justifies the significant increase in complexity, at Meta’s scale, it likely does.</li> <li>DHEN does not merely represent an incremental improvement over DLRM; it introduces a comprehensive hierarchy of feature interactions, comprising dot products, AutoInt-like self-attention, convolution, linear processing, and DCN-like crossing, replacing DLRM’s simpler dot product approach.</li> </ul> <h2 id="gdcn-2023">GDCN (2023)</h2> <ul> <li>Proposed in the paper <a href="https://arxiv.org/abs/2311.04635">Towards Deeper, Lighter, and Interpretable Cross Network for CTR Prediction</a> by Wang et al. (2023) from Fudan University and Microsoft Research Asia in CIKM ‘23. The paper introduces the Gated Deep Cross Network (GDCN) and the Field-level Dimension Optimization (FDO) approach. GDCN aims to address significant challenges in Click-Through Rate (CTR) prediction for recommender systems and online advertising, specifically the automatic capture of high-order feature interactions, interpretability issues, and the redundancy of parameters in existing methods.</li> <li>GDCN is inspired by DCN V2 and consists of an embedding layer, a Gated Cross Network (GCN), and a Deep Neural Network (DNN). The GCN forms its core structure, which captures explicit bounded-degree high-order feature crosses/interactions. The GCN employs an information gate in each cross layer (representing a higher order interaction) to dynamically filter and amplify important interactions. This gate controls the information flow, ensuring that the model focuses on relevant interactions. This approach not only allows for deeper feature crossing but also adds a layer of interpretability by identifying crucial interactions, thus modelling implicit feature crosses.</li> <li>GDCN is a generalization of DCN V2, offering dynamic instance-based interpretability and the ability to utilize deeper cross features without a loss in performance.</li> </ul> <blockquote> <p>The unique selling point of DCN V2 is that it treats all cross features equally, while GDCN uses information gates for fine-grained control over feature importance.</p> </blockquote> <ul> <li>GDCN transforms high-dimensional, sparse input into low-dimensional, dense representations. Unlike most CTR models, GDCN allows arbitrary embedding dimensions.</li> <li>Two structures are proposed: GDCN-S (stacked) and GDCN-P (parallel). GDCN-S feeds the output of GCN into a DNN, while GDCN-P feeds the input vector in parallel into GCN and DNN, concatenating their outputs.</li> <li>Alongside GDCN, the FDO approach focuses on optimizing the dimensions of each field in the embedding layer based on their importance. FDO addresses the issue of redundant parameters by learning independent dimensions for each field based on its intrinsic importance. This approach allows for a more efficient allocation of embedding dimensions, reducing unnecessary parameters and enhancing enhancing efficiency without compromising performance. FDO uses methods like PCA to determine optimal dimensions and only needs to be done once, with the dimensions applicable to subsequent model updates.</li> <li>The following figure shows the architecture of the GDCN-S and GDCN-P. ⊗ is the cross operation (a.k.a, the gated cross layer).</li> </ul> <p><img src="https://aman.ai/images/papers/GDCN1.jpg" alt=""/></p> <ul> <li>The following figure visualizes the gated cross layer. ⊙ is elementwise/Hadamard product, and × is matrix multiplication.</li> </ul> <p><img src="https://aman.ai/images/papers/GDCN2.jpg" alt=""/></p> <ul> <li>Results indicate that GDCN, especially when paired with the FDO approach, outperforms state-of-the-art methods in terms of prediction performance, interpretability, and efficiency. GDCN was evaluated on five datasets (Criteo, Avazu, Malware, Frappe, ML-tag) using metrics like AUC and Logloss, showcasing the effectiveness and superiority of GDCN in capturing deeper high-order interactions. These experiments also demonstrate the interpretability of the GCN model and the successful parameter reduction achieved by the FDO approach. The datasets underwent preprocessing like feature removal for infrequent items and normalization. The comparison included various classes of CTR models and demonstrated GDCN’s effectiveness in handling high-order feature interactions without the drawbacks of overfitting or performance degradation observed in other models. GDCN achieves comparable or better performance with only a fraction (about 23%) of the original model parameters.</li> <li>In summary, GDCN addresses the limitations of existing CTR prediction models by offering a more interpretable, efficient, and effective approach to handling high-order feature interactions, supported by the innovative use of information gates and dimension optimization techniques.</li> </ul> <h2 id="graph-neural-networks-based-recsys-architectures">Graph Neural Networks-based RecSys Architectures</h2> <ul> <li>Graph Neural Networks (GNN) architectures utilize graph structures to capture relationships between users, items, and their interactions. GNNs propagate information through the user-item interaction graph, enabling the model to learn user and item representations that incorporate relational dependencies. This is particularly useful in scenarios with rich graph-based data. <ul> <li><strong>Pros:</strong> Captures relational dependencies and propagates information through graph structures, enabling better modeling of complex relationships. <ul> <li><strong>Cons:</strong> Requires graph-based data and potentially higher computational resources for training and inference.</li> <li><strong>Advantages:</strong> Improved recommendations by incorporating the rich relational information among users, items, and their interactions.</li> <li><strong>Example Use Case:</strong> Social recommendation systems, where user-user connections or item-item relationships play a significant role in personalized recommendations.</li> </ul> </li> <li><strong>Phase:</strong> Candidate Generation, Ranking, Retrieval.</li> <li><strong>Recommendation Workflow:</strong> GNN architectures are suitable for multiple phases of the recommendation workflow. In the candidate generation phase, GNNs can leverage graph structures to capture relational dependencies and generate potential candidate items. In the ranking phase, GNNs can learn user and item embeddings that incorporate relational information, leading to improved ranking. In the retrieval phase, GNNs can assist in efficient retrieval of relevant items based on their graph-based representations.</li> </ul> </li> <li>For a detailed overview of GNNs in RecSys, please refer to the <a href="https://aman.ai/recsys/GNN">GNN primer</a>.</li> </ul> <h2 id="two-towers-in-recsys">Two Towers in RecSys</h2> <ul> <li>One of the most prevalent architectures in personalization and recommendation systems (RecSys) is the two-tower network. This network architecture typically comprises two towers: the user tower (U) and the candidate tower (C). These towers generate dense vector representations (embeddings) of the user and the candidate, respectively. The final layer of the network combines these embeddings using either a dot product or cosine similarity function.</li> <li>Consider the computational costs involved: if the cost of executing the user tower is u, the candidate tower is c, and the dot product is d, then the total cost of ranking N candidates for a single user is N∗(u+c+d). Since the user representation is fixed and computed once, the cost reduces to u+N∗(c+d). Moreover, caching the embeddings can further reduce the cost to u+N∗d+k, where k represents additional fixed overheads. <a href="https://medium.com/better-ml/recsys-model-serving-model-architectures-serving-1b5f038848bd">(source)</a></li> <li>The following image illustrates this concept <a href="https://medium.com/better-ml/recsys-model-serving-model-architectures-serving-1b5f038848bd">(source)</a>:</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/2.webp" alt=""/></p> <ul> <li>The two-tower architecture consists of two distinct branches: a query tower (user tower) and a candidate tower (item tower). The query tower learns the user’s representation based on their history, while the candidate tower learns item representations from item features. The two towers are combined at the final stage to produce recommendations. <ul> <li><strong>Pros:</strong> This approach explicitly models user and item representations separately, facilitating a better understanding of user preferences and item features.</li> <li><strong>Cons:</strong> It requires additional computation to learn and combine the representations from both the query and candidate towers.</li> <li><strong>Advantages:</strong> This method enhances personalization by learning user and item representations separately, allowing for more granular preference capture.</li> <li><strong>Example Use Case:</strong> This architecture is particularly effective in personalized recommendation systems where understanding both the user’s past behavior and item characteristics is crucial.</li> <li><strong>Phase:</strong> Candidate Generation, Ranking.</li> <li><strong>Recommendation Workflow:</strong> The two-tower architecture is commonly used in the candidate generation and ranking phases. During candidate generation, it allows for the independent processing of user and item features, generating separate representations. In the ranking phase, these representations are merged to assess the relevance of candidate items to the user’s preferences.</li> </ul> </li> <li>The two-tower model gained formal recognition in the machine learning community through Huawei’s 2019 <a href="https://www.researchgate.net/publication/335771749_PAL_a_position-bias_aware_learning_framework_for_CTR_prediction_in_live_recommender_systems">PAL</a> paper. This model was designed to address biases in ranking models, particularly position bias in recommendation systems.</li> <li>The two-tower architecture typically includes one tower for learning relevance (user/item interactions) and another for learning biases (such as position bias). These towers are combined, either multiplicatively or additively, to generate the final output.</li> <li><strong>Examples of notable two-tower implementations:</strong> <ul> <li>Huawei’s PAL model employs a multiplicative approach to combine the outputs of the two towers, addressing position bias within their app store.</li> <li>YouTube’s “Watch Next” paper introduced an additive two-tower model, which not only mitigates position bias but also considers other selection biases by incorporating features like device type.</li> </ul> </li> <li>The two-tower model has demonstrated significant improvements in recommendation systems. For instance, Huawei’s PAL model improved click-through and conversion rates by approximately 25%. YouTube’s model, by integrating a shallow tower for bias learning, also showed increased engagement metrics.</li> <li><strong>Challenges and considerations:</strong> <ul> <li>A primary challenge in two-tower models is ensuring that both towers learn independently during training, as relevance can interfere with the learning of position bias.</li> <li>Techniques such as Dropout have been employed to reduce over-reliance on certain features, such as position, and to enhance generalization.-</li> </ul> </li> <li>Overall, the two-tower model is recognized as an effective approach for building unbiased ranking models in recommender systems. It remains a promising area of research, with significant potential for further development.</li> </ul> <h3 id="split-network">Split Network</h3> <ul> <li>A split network is a generalized version of a two tower network. The same optimization of embedding lookup holds here as well. Instead of a dot product, a simple neural network could be used to produce output.</li> <li>The image below <a href="https://medium.com/better-ml/recsys-model-serving-model-architectures-serving-1b5f038848bd">(source)</a> showcases this.</li> </ul> <p><img src="https://aman.ai/recsys/assets/architectures/3.webp" alt=""/></p> <ul> <li>In a split network architecture, different components of the recommendation model are split and processed separately. For example, the user and item features may be processed independently and combined in a later stage. This allows for parallel processing and efficient handling of large-scale recommender systems. <ul> <li><strong>Pros:</strong> Enables parallel processing, efficient handling of large-scale systems, and flexibility in designing and optimizing different components separately.</li> <li><strong>Cons:</strong> Requires additional coordination and synchronization between the split components, potentially increasing complexity.</li> <li><strong>Advantages:</strong> Scalability, flexibility, and improved performance in handling large-scale recommender systems.</li> <li><strong>Example Use Case:</strong> Recommendation systems with a massive number of users and items, where parallel processing is crucial for efficient computation.</li> <li><strong>Phase:</strong> Candidate Generation, Ranking, Final Ranking.</li> <li><strong>Recommendation Workflow:</strong> The split network architecture can be utilized in various phases. During the candidate generation phase, the split network can be used to process user and item features independently, allowing efficient retrieval of potential candidate items. In the ranking phase, the split network can be employed to learn representations and capture interactions between the user and candidate items. Finally, in the final ranking phase, the split network can contribute to the overall ranking of the candidate items based on learned representations.</li> </ul> </li> </ul> <h2 id="summary-9">Summary</h2> <ul> <li>Neural Collaborative Filtering (NCF) represents a pioneering approach in recommender systems. It was one of the initial studies to replace the then-standard linear matrix factorization algorithms with neural networks, thus facilitating the integration of deep learning into recommender systems.</li> <li>The Wide &amp; Deep model underscored the significance of cross features—specifically, second-order features formed by intersecting two original features. This model effectively combines a broad, shallow module for handling cross features with a deep module, paralleling the approach of NCF.</li> <li>Deep and Cross Neural Network (DCN) was among the first to transition from manually engineered cross features to an algorithmic method capable of autonomously generating all potential feature crosses to any desired order.</li> <li>Deep Factorization Machine (DeepFM) shares conceptual similarities with DCN. However, it distinctively substitutes the cross layers in DCN with factorization machines, or more specifically, dot products.</li> <li>Automatic Interactions (AutoInt) brought multi-head self-attention mechanisms, previously known in Large Language Models (LLMs), into the domain of feature interaction. This technique moves away from brute-force generation of all possible feature interactions, which can lead to model overfitting on noisy feature crosses. Instead, it employs attention mechanisms to enable the model to selectively focus on the most relevant feature interactions.</li> <li>Deep Learning Recommendation Model (DLRM) marked a departure from previous models by discarding the deep module. It relies solely on an interaction layer that computes dot products, akin to the factorization machine component in DeepFM, followed by a Multi-Layer Perceptron (MLP). This model emphasizes the sufficiency of interaction layers alone.</li> <li>Deep Hierarchical Embedding Network (DHEN) builds upon the DLRM framework by replacing the conventional dot product with a sophisticated hierarchy of feature interactions, including dot product, convolution, self-attention akin to AutoInt, and crossing features similar to those in DCN.</li> <li>Gated Deep Cross Network (GDCN) enhances Click-Through Rate (CTR) prediction in recommender systems by improving interpretability, efficiency, and handling of high-order feature interactions.</li> <li>The Two Tower model in recommender systems, known for its separate user and candidate towers, optimizes personalized recommendations and addresses biases like position bias, representing an evolving and powerful approach in building unbiased ranking models.</li> </ul> <h2 id="comparative-analysis">Comparative Analysis</h2> <ul> <li>This table offers a detailed comparative analysis table summarizing the key characteristics of various recommender system architectures in terms of features, advantages, limitations, use cases, and applicable phases in recommendation workflows.</li> </ul> <table> <thead> <tr> <th><strong>Technique</strong></th> <th><strong>Key Features</strong></th> <th><strong>Advantages</strong></th> <th><strong>Limitations</strong></th> <th><strong>Example Use Cases</strong></th> <th><strong>Phase</strong></th> </tr> </thead> <tbody> <tr> <td>Wide and Deep (2016)</td> <td>Combines a wide linear model for memorization with a deep neural network for generalization. Cross features essential.</td> <td>Balances memorization and generalization. Captures both specific rules and complex patterns.</td> <td>Requires manual engineering of cross features, increasing complexity.</td> <td>E-commerce, app recommendations</td> <td>Ranking</td> </tr> <tr> <td>Factorization Machines (FM, 2010)</td> <td>Models pairwise feature interactions using embeddings.</td> <td>Handles sparse data effectively. Efficient computation of pairwise interactions.</td> <td>Limited to pairwise interactions. Struggles with non-linear, higher-order interactions.</td> <td>CTR prediction, basic recommendations</td> <td>Ranking, Candidate Generation</td> </tr> <tr> <td>DeepFM (2017)</td> <td>Integrates FM with deep networks for higher-order feature interactions.</td> <td>Combines FM’s efficiency with deep learning’s capability to model non-linear interactions.</td> <td>Brute-force feature cross generation increases complexity and potential overfitting.</td> <td>Online advertising, CTR prediction</td> <td>Ranking, Candidate Generation</td> </tr> <tr> <td>NCF (2017)</td> <td>Substitutes matrix factorization’s dot product with neural networks for modeling user-item interactions.</td> <td>Adds non-linearity, beating matrix factorization in benchmarks.</td> <td>Lacks explicit cross-feature modeling (critical for some domains).</td> <td>Personalized recommendations</td> <td>Ranking</td> </tr> <tr> <td>DCN (2017)</td> <td>Introduces Cross Network for explicit feature interactions of bounded order.</td> <td>Automates cross-feature learning. Improves efficiency for sparse data.</td> <td>Limited scalability and expressiveness as complexity grows with depth.</td> <td>Display advertising, e-commerce</td> <td>Ranking</td> </tr> <tr> <td>DCN V2 (2020)</td> <td>Enhances DCN with low-rank approximations and Mixture-of-Experts. Supports stacked/parallel structures.</td> <td>More scalable and expressive. Models higher-order interactions effectively.</td> <td>Complexity and additional computational cost.</td> <td>Large-scale ranking systems</td> <td>Ranking</td> </tr> <tr> <td>AutoInt (2019)</td> <td>Uses multi-head self-attention for feature interaction modeling.</td> <td>Selectively focuses on relevant feature interactions, improving efficiency and reducing overfitting.</td> <td>Requires significant resources for training self-attention mechanisms.</td> <td>CTR prediction, movie recommendations</td> <td>Ranking</td> </tr> <tr> <td>DLRM (2019)</td> <td>Focuses solely on feature interaction via dot products, with MLPs for capacity.</td> <td>Simple and computationally efficient. Optimized for hardware scalability.</td> <td>Limited to second-order (pairwise) interactions.</td> <td>Web-scale personalization</td> <td>Ranking, Candidate Generation</td> </tr> <tr> <td>DHEN (2022)</td> <td>Builds hierarchical feature interaction using diverse modules (e.g., self-attention, convolution).</td> <td>Captures higher-order feature interactions. Strong performance gains in large-scale systems.</td> <td>High computational and implementation complexity.</td> <td>Online advertising</td> <td>Ranking, Candidate Generation</td> </tr> <tr> <td>GDCN (2023)</td> <td>Adds gated mechanisms to cross networks for fine-grained interaction control. Supports dimension optimization.</td> <td>Efficient parameter usage, interpretable results, captures deeper high-order interactions.</td> <td>Requires careful tuning of gates and dimensions.</td> <td>CTR prediction</td> <td>Ranking, Final Ranking</td> </tr> <tr> <td>Graph Neural Networks (GNN)</td> <td>Propagates information in user-item interaction graphs.</td> <td>Effectively models complex relational data. Captures dependencies among users and items.</td> <td>Computationally intensive. Requires graph-based data.</td> <td>Social recommendations, collaborative filtering</td> <td>Candidate Generation, Ranking</td> </tr> <tr> <td>Two Towers (2019)</td> <td>Separate user and item towers generate embeddings, combined via dot product or cosine similarity.</td> <td>Simplifies user-item representation learning. Efficient inference through caching.</td> <td>May struggle with learning complex interactions due to separation of towers.</td> <td>Personalized search, e-commerce</td> <td>Candidate Generation, Ranking</td> </tr> <tr> <td>Split Network</td> <td>Generalization of Two Towers using neural networks to combine user and item features.</td> <td>Flexible and scalable. Suitable for large-scale systems.</td> <td>Requires efficient synchronization between split components.</td> <td>Large-scale recommendations</td> <td>Candidate Generation, Ranking</td> </tr> </tbody> </table> <h2 id="references">References</h2> <ul> <li>Samuel Flender’s <a href="https://mlfrontiers.substack.com/p/a-tour-of-the-recommender-system">ML Frontiers</a></li> <li><a href="https://medium.com/better-ml/recsys-model-serving-model-architectures-serving-1b5f038848bd">RecSys model architectures and serving paridigms</a></li> <li><a href="https://blog.research.google/2016/06/wide-deep-learning-better-together-with.html">Wide &amp; Deep Learning: Better Together with TensorFlow</a></li> <li><a href="https://www.tensorflow.org/recommenders/examples/dcn">Deep &amp; Cross Network (DCN)</a></li> </ul>]]></content><author><name></name></author><category term="Recommender"/><category term="System"/><category term="MLSD"/><summary type="html"><![CDATA[Source]]></summary></entry><entry><title type="html">Standard AI Libraries Alternatives</title><link href="https://lorenz-peter.github.io/blog/2025/standard-ai-libraries-alternatives/" rel="alternate" type="text/html" title="Standard AI Libraries Alternatives"/><published>2025-03-23T01:47:22+00:00</published><updated>2025-03-23T01:47:22+00:00</updated><id>https://lorenz-peter.github.io/blog/2025/standard-ai-libraries-alternatives</id><content type="html" xml:base="https://lorenz-peter.github.io/blog/2025/standard-ai-libraries-alternatives/"><![CDATA[]]></content><author><name></name></author></entry></feed>